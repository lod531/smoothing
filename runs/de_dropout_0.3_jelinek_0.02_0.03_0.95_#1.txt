Sender: LSF System <lsfadmin@eu-g2-09>
Subject: Job 208722370: <de_dropout_0.3_jelinek_0.02_0.03_0.95_#1> in cluster <euler> Done

Job <de_dropout_0.3_jelinek_0.02_0.03_0.95_#1> was submitted from host <eu-login-24> by user <andriusb> in cluster <euler> at Tue Mar 15 16:18:08 2022
Job was executed on host(s) <eu-g2-09>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 16:32:50 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 16:32:50 2022
Terminated at Wed Mar 16 15:45:36 2022
Results reported at Wed Mar 16 15:45:36 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/de --save-dir /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.02,0.03, 0.95)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   87647.13 sec.
    Max Memory :                                 3721 MB
    Average Memory :                             2882.39 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16279.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   83565 sec.
    Turnaround time :                            84448 sec.

The output (if any) follows:

2022-03-15 16:32:58 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/de', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02,0.03, 0.95)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 16:32:58 | INFO | fairseq.tasks.language_modeling | dictionary: 34528 types
2022-03-15 16:32:59 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
Calculating frequency stats:
  0%|          | 0/45920 [00:00<?, ?it/s]  0%|          | 74/45920 [00:00<01:02, 737.79it/s]  0%|          | 163/45920 [00:00<00:55, 826.29it/s]  1%|          | 246/45920 [00:00<01:15, 606.58it/s]  1%|          | 327/45920 [00:00<01:12, 632.38it/s]  1%|          | 426/45920 [00:00<01:01, 740.01it/s]  1%|          | 506/45920 [00:00<01:00, 756.74it/s]  1%|▏         | 588/45920 [00:00<00:58, 775.23it/s]  1%|▏         | 671/45920 [00:00<00:58, 768.84it/s]  2%|▏         | 750/45920 [00:01<01:17, 581.13it/s]  2%|▏         | 824/45920 [00:01<01:12, 618.05it/s]  2%|▏         | 892/45920 [00:01<01:11, 631.99it/s]  2%|▏         | 985/45920 [00:01<01:03, 709.76it/s]  2%|▏         | 1079/45920 [00:01<00:58, 771.72it/s]  3%|▎         | 1165/45920 [00:01<00:56, 796.49it/s]  3%|▎         | 1264/45920 [00:01<00:52, 848.38it/s]  3%|▎         | 1353/45920 [00:01<00:51, 859.22it/s]  3%|▎         | 1450/45920 [00:01<00:50, 888.53it/s]  3%|▎         | 1540/45920 [00:02<00:49, 890.16it/s]  4%|▎         | 1630/45920 [00:02<00:55, 802.53it/s]  4%|▎         | 1713/45920 [00:02<00:56, 778.86it/s]  4%|▍         | 1818/45920 [00:02<00:51, 852.43it/s]  4%|▍         | 1913/45920 [00:02<00:50, 879.54it/s]  4%|▍         | 2003/45920 [00:02<00:49, 880.40it/s]  5%|▍         | 2093/45920 [00:02<00:51, 844.88it/s]  5%|▍         | 2179/45920 [00:02<00:53, 814.39it/s]  5%|▍         | 2264/45920 [00:02<00:52, 823.96it/s]  5%|▌         | 2348/45920 [00:03<01:01, 706.22it/s]  5%|▌         | 2431/45920 [00:03<00:58, 737.35it/s]  5%|▌         | 2517/45920 [00:03<00:56, 764.68it/s]  6%|▌         | 2596/45920 [00:03<00:57, 751.46it/s]  6%|▌         | 2697/45920 [00:03<00:52, 822.45it/s]  6%|▌         | 2781/45920 [00:03<00:55, 775.32it/s]  6%|▋         | 2889/45920 [00:03<00:50, 858.47it/s]  6%|▋         | 2980/45920 [00:03<00:49, 866.45it/s]  7%|▋         | 3068/45920 [00:03<00:56, 758.48it/s]  7%|▋         | 3147/45920 [00:04<00:57, 739.62it/s]  7%|▋         | 3231/45920 [00:04<00:55, 764.91it/s]  7%|▋         | 3311/45920 [00:04<00:55, 774.02it/s]  7%|▋         | 3395/45920 [00:04<00:53, 792.15it/s]  8%|▊         | 3476/45920 [00:04<00:54, 774.46it/s]  8%|▊         | 3555/45920 [00:04<00:55, 761.48it/s]  8%|▊         | 3632/45920 [00:04<01:06, 632.10it/s]  8%|▊         | 3712/45920 [00:04<01:02, 673.01it/s]  8%|▊         | 3792/45920 [00:05<01:04, 654.51it/s]  8%|▊         | 3882/45920 [00:05<00:58, 717.26it/s]  9%|▊         | 3957/45920 [00:05<01:04, 654.71it/s]  9%|▉         | 4026/45920 [00:05<01:03, 663.75it/s]  9%|▉         | 4095/45920 [00:05<01:06, 628.49it/s]  9%|▉         | 4160/45920 [00:05<01:07, 614.78it/s]  9%|▉         | 4242/45920 [00:05<01:17, 540.22it/s]  9%|▉         | 4314/45920 [00:05<01:11, 578.46it/s] 10%|▉         | 4404/45920 [00:05<01:04, 641.94it/s] 10%|▉         | 4495/45920 [00:06<00:58, 710.47it/s] 10%|█         | 4594/45920 [00:06<00:53, 766.37it/s] 10%|█         | 4673/45920 [00:06<00:59, 692.81it/s] 10%|█         | 4774/45920 [00:06<00:53, 774.91it/s] 11%|█         | 4855/45920 [00:06<00:52, 781.35it/s] 11%|█         | 4944/45920 [00:06<00:50, 811.35it/s] 11%|█         | 5033/45920 [00:06<00:49, 833.19it/s] 11%|█         | 5118/45920 [00:06<00:54, 752.69it/s] 11%|█▏        | 5196/45920 [00:06<00:55, 727.82it/s] 11%|█▏        | 5271/45920 [00:07<00:56, 715.93it/s] 12%|█▏        | 5365/45920 [00:07<00:52, 774.97it/s] 12%|█▏        | 5466/45920 [00:07<00:48, 834.35it/s] 12%|█▏        | 5551/45920 [00:07<00:55, 721.13it/s] 12%|█▏        | 5635/45920 [00:07<00:53, 751.49it/s] 12%|█▏        | 5713/45920 [00:07<00:55, 723.40it/s] 13%|█▎        | 5788/45920 [00:07<00:59, 679.69it/s] 13%|█▎        | 5884/45920 [00:07<00:53, 751.00it/s] 13%|█▎        | 5962/45920 [00:08<00:56, 706.02it/s] 13%|█▎        | 6037/45920 [00:08<00:55, 717.46it/s] 13%|█▎        | 6111/45920 [00:08<00:58, 685.67it/s] 13%|█▎        | 6188/45920 [00:08<00:56, 707.57it/s] 14%|█▎        | 6260/45920 [00:08<00:57, 686.42it/s] 14%|█▍        | 6353/45920 [00:08<00:52, 749.39it/s] 14%|█▍        | 6456/45920 [00:08<00:48, 821.93it/s] 14%|█▍        | 6540/45920 [00:08<00:48, 819.17it/s] 14%|█▍        | 6623/45920 [00:08<00:53, 735.83it/s] 15%|█▍        | 6699/45920 [00:09<00:56, 695.26it/s] 15%|█▍        | 6796/45920 [00:09<00:51, 765.89it/s] 15%|█▍        | 6875/45920 [00:09<00:51, 756.13it/s] 15%|█▌        | 6973/45920 [00:09<00:47, 817.02it/s] 15%|█▌        | 7060/45920 [00:09<00:46, 830.85it/s] 16%|█▌        | 7174/45920 [00:09<00:42, 919.22it/s] 16%|█▌        | 7268/45920 [00:09<00:50, 770.15it/s] 16%|█▌        | 7350/45920 [00:09<00:53, 725.66it/s] 16%|█▌        | 7427/45920 [00:09<00:52, 735.26it/s] 16%|█▋        | 7504/45920 [00:10<00:52, 727.09it/s] 17%|█▋        | 7579/45920 [00:10<01:03, 602.24it/s] 17%|█▋        | 7644/45920 [00:10<01:20, 473.10it/s] 17%|█▋        | 7721/45920 [00:10<01:11, 532.96it/s] 17%|█▋        | 7782/45920 [00:10<01:10, 543.20it/s] 17%|█▋        | 7878/45920 [00:10<00:59, 642.90it/s] 17%|█▋        | 7970/45920 [00:10<00:53, 713.77it/s] 18%|█▊        | 8047/45920 [00:10<00:53, 709.27it/s] 18%|█▊        | 8135/45920 [00:11<00:50, 750.62it/s] 18%|█▊        | 8213/45920 [00:11<00:51, 732.95it/s] 18%|█▊        | 8306/45920 [00:11<00:47, 784.01it/s] 18%|█▊        | 8403/45920 [00:11<00:44, 835.78it/s] 18%|█▊        | 8489/45920 [00:11<00:50, 742.54it/s] 19%|█▊        | 8568/45920 [00:11<00:49, 754.60it/s] 19%|█▉        | 8654/45920 [00:11<00:47, 780.66it/s] 19%|█▉        | 8734/45920 [00:11<00:51, 728.33it/s] 19%|█▉        | 8823/45920 [00:11<00:48, 768.83it/s] 19%|█▉        | 8902/45920 [00:12<00:48, 759.80it/s] 20%|█▉        | 8980/45920 [00:12<00:51, 714.24it/s] 20%|█▉        | 9077/45920 [00:12<00:47, 783.56it/s] 20%|█▉        | 9157/45920 [00:12<00:51, 718.16it/s] 20%|██        | 9248/45920 [00:12<00:47, 769.01it/s] 20%|██        | 9327/45920 [00:12<00:48, 750.38it/s] 21%|██        | 9416/45920 [00:12<00:46, 788.77it/s] 21%|██        | 9497/45920 [00:12<00:46, 791.64it/s] 21%|██        | 9578/45920 [00:13<00:49, 735.34it/s] 21%|██        | 9653/45920 [00:13<00:55, 655.80it/s] 21%|██        | 9725/45920 [00:13<00:53, 671.36it/s] 21%|██▏       | 9801/45920 [00:13<00:52, 693.91it/s] 22%|██▏       | 9883/45920 [00:13<00:56, 640.74it/s] 22%|██▏       | 9979/45920 [00:13<00:49, 722.23it/s] 22%|██▏       | 10054/45920 [00:13<00:50, 705.53it/s] 22%|██▏       | 10131/45920 [00:13<00:55, 647.73it/s] 22%|██▏       | 10215/45920 [00:13<00:51, 697.19it/s] 22%|██▏       | 10318/45920 [00:14<00:45, 784.67it/s] 23%|██▎       | 10399/45920 [00:14<00:45, 785.25it/s] 23%|██▎       | 10507/45920 [00:14<00:40, 866.88it/s] 23%|██▎       | 10596/45920 [00:14<00:47, 743.92it/s] 23%|██▎       | 10675/45920 [00:14<00:50, 695.29it/s] 23%|██▎       | 10768/45920 [00:14<00:46, 752.76it/s] 24%|██▎       | 10855/45920 [00:14<00:49, 701.83it/s] 24%|██▍       | 10929/45920 [00:14<00:50, 692.50it/s] 24%|██▍       | 11019/45920 [00:15<00:46, 744.74it/s] 24%|██▍       | 11096/45920 [00:15<00:48, 716.66it/s] 24%|██▍       | 11189/45920 [00:15<00:44, 773.25it/s] 25%|██▍       | 11270/45920 [00:15<00:44, 782.90it/s] 25%|██▍       | 11356/45920 [00:15<00:43, 793.13it/s] 25%|██▍       | 11437/45920 [00:15<00:48, 709.23it/s] 25%|██▌       | 11511/45920 [00:15<00:48, 705.00it/s] 25%|██▌       | 11612/45920 [00:15<00:43, 787.41it/s] 25%|██▌       | 11693/45920 [00:15<00:49, 685.26it/s] 26%|██▌       | 11765/45920 [00:16<00:56, 609.04it/s] 26%|██▌       | 11830/45920 [00:16<00:55, 612.50it/s] 26%|██▌       | 11913/45920 [00:16<00:50, 666.83it/s] 26%|██▌       | 11983/45920 [00:16<00:53, 635.23it/s] 26%|██▋       | 12062/45920 [00:16<00:50, 675.63it/s] 26%|██▋       | 12132/45920 [00:16<00:54, 615.32it/s] 27%|██▋       | 12213/45920 [00:16<00:50, 665.64it/s] 27%|██▋       | 12301/45920 [00:16<00:46, 721.18it/s] 27%|██▋       | 12403/45920 [00:16<00:41, 803.26it/s] 27%|██▋       | 12496/45920 [00:17<00:39, 837.65it/s] 27%|██▋       | 12582/45920 [00:17<00:40, 815.96it/s] 28%|██▊       | 12678/45920 [00:17<00:38, 854.80it/s] 28%|██▊       | 12765/45920 [00:17<00:42, 778.34it/s] 28%|██▊       | 12845/45920 [00:17<00:50, 656.03it/s] 28%|██▊       | 12941/45920 [00:17<00:45, 729.85it/s] 28%|██▊       | 13019/45920 [00:17<00:46, 705.28it/s] 29%|██▊       | 13112/45920 [00:17<00:43, 762.01it/s] 29%|██▊       | 13199/45920 [00:18<00:41, 790.92it/s] 29%|██▉       | 13292/45920 [00:18<00:39, 826.54it/s] 29%|██▉       | 13377/45920 [00:18<00:42, 761.11it/s] 29%|██▉       | 13467/45920 [00:18<00:40, 798.42it/s] 30%|██▉       | 13549/45920 [00:18<00:42, 765.70it/s] 30%|██▉       | 13628/45920 [00:18<00:42, 768.24it/s] 30%|██▉       | 13706/45920 [00:18<00:42, 751.10it/s] 30%|███       | 13797/45920 [00:18<00:40, 793.62it/s] 30%|███       | 13893/45920 [00:18<00:38, 840.77it/s] 30%|███       | 13978/45920 [00:19<00:45, 696.96it/s] 31%|███       | 14053/45920 [00:19<00:45, 706.42it/s] 31%|███       | 14127/45920 [00:19<00:48, 654.17it/s] 31%|███       | 14196/45920 [00:19<00:49, 643.37it/s] 31%|███       | 14263/45920 [00:19<00:52, 603.64it/s] 31%|███       | 14349/45920 [00:19<00:47, 665.87it/s] 31%|███▏      | 14428/45920 [00:19<00:45, 696.22it/s] 32%|███▏      | 14517/45920 [00:19<00:41, 748.26it/s] 32%|███▏      | 14594/45920 [00:19<00:47, 660.16it/s] 32%|███▏      | 14677/45920 [00:20<00:51, 605.59it/s] 32%|███▏      | 14762/45920 [00:20<00:46, 663.12it/s] 32%|███▏      | 14836/45920 [00:20<00:46, 674.42it/s] 33%|███▎      | 14925/45920 [00:20<00:42, 726.87it/s] 33%|███▎      | 15000/45920 [00:20<00:42, 728.31it/s] 33%|███▎      | 15075/45920 [00:20<00:53, 580.80it/s] 33%|███▎      | 15168/45920 [00:20<00:46, 661.71it/s] 33%|███▎      | 15268/45920 [00:20<00:44, 685.36it/s] 33%|███▎      | 15344/45920 [00:21<00:43, 701.15it/s] 34%|███▎      | 15418/45920 [00:21<00:47, 637.64it/s] 34%|███▎      | 15492/45920 [00:21<00:45, 663.17it/s] 34%|███▍      | 15579/45920 [00:21<00:42, 717.39it/s] 34%|███▍      | 15654/45920 [00:21<00:45, 658.30it/s] 34%|███▍      | 15749/45920 [00:21<00:41, 734.04it/s] 34%|███▍      | 15826/45920 [00:21<00:44, 678.16it/s] 35%|███▍      | 15897/45920 [00:21<00:43, 685.91it/s] 35%|███▍      | 15979/45920 [00:22<00:41, 719.54it/s] 35%|███▍      | 16058/45920 [00:22<00:40, 738.44it/s] 35%|███▌      | 16134/45920 [00:22<00:42, 708.40it/s] 35%|███▌      | 16220/45920 [00:22<00:39, 750.66it/s] 35%|███▌      | 16297/45920 [00:22<00:48, 615.70it/s] 36%|███▌      | 16396/45920 [00:22<00:41, 708.87it/s] 36%|███▌      | 16474/45920 [00:22<00:40, 724.96it/s] 36%|███▌      | 16551/45920 [00:22<00:45, 647.25it/s] 36%|███▋      | 16664/45920 [00:22<00:38, 754.91it/s] 36%|███▋      | 16751/45920 [00:23<00:37, 784.74it/s] 37%|███▋      | 16833/45920 [00:23<00:45, 637.21it/s] 37%|███▋      | 16904/45920 [00:23<00:45, 633.24it/s] 37%|███▋      | 16972/45920 [00:23<00:47, 605.53it/s] 37%|███▋      | 17075/45920 [00:23<00:40, 711.12it/s] 37%|███▋      | 17168/45920 [00:23<00:37, 767.31it/s] 38%|███▊      | 17249/45920 [00:23<00:41, 690.46it/s] 38%|███▊      | 17322/45920 [00:23<00:42, 672.58it/s] 38%|███▊      | 17400/45920 [00:24<00:40, 700.20it/s] 38%|███▊      | 17489/45920 [00:24<00:38, 743.59it/s] 38%|███▊      | 17596/45920 [00:24<00:34, 829.86it/s] 39%|███▊      | 17681/45920 [00:24<00:34, 812.06it/s] 39%|███▊      | 17764/45920 [00:24<00:35, 797.98it/s] 39%|███▉      | 17845/45920 [00:24<00:36, 778.95it/s] 39%|███▉      | 17930/45920 [00:24<00:35, 790.97it/s] 39%|███▉      | 18010/45920 [00:24<00:41, 668.25it/s] 39%|███▉      | 18098/45920 [00:24<00:38, 722.25it/s] 40%|███▉      | 18199/45920 [00:25<00:34, 793.44it/s] 40%|███▉      | 18287/45920 [00:25<00:33, 816.53it/s] 40%|████      | 18371/45920 [00:25<00:33, 820.44it/s] 40%|████      | 18455/45920 [00:25<00:37, 732.38it/s] 40%|████      | 18532/45920 [00:25<00:39, 693.79it/s] 41%|████      | 18604/45920 [00:25<00:51, 529.16it/s] 41%|████      | 18696/45920 [00:25<00:44, 614.64it/s] 41%|████      | 18766/45920 [00:25<00:45, 600.98it/s] 41%|████      | 18870/45920 [00:26<00:38, 707.03it/s] 41%|████▏     | 18961/45920 [00:26<00:35, 758.99it/s] 42%|████▏     | 19057/45920 [00:26<00:33, 810.19it/s] 42%|████▏     | 19143/45920 [00:26<00:34, 781.49it/s] 42%|████▏     | 19225/45920 [00:26<00:35, 750.71it/s] 42%|████▏     | 19314/45920 [00:26<00:36, 727.62it/s] 42%|████▏     | 19401/45920 [00:26<00:34, 762.03it/s] 42%|████▏     | 19479/45920 [00:26<00:36, 714.65it/s] 43%|████▎     | 19590/45920 [00:26<00:32, 817.75it/s] 43%|████▎     | 19675/45920 [00:27<00:31, 825.11it/s] 43%|████▎     | 19760/45920 [00:27<00:32, 816.50it/s] 43%|████▎     | 19854/45920 [00:27<00:30, 844.72it/s] 43%|████▎     | 19940/45920 [00:27<00:32, 795.62it/s] 44%|████▎     | 20031/45920 [00:27<00:33, 783.66it/s] 44%|████▍     | 20111/45920 [00:27<00:37, 689.36it/s] 44%|████▍     | 20215/45920 [00:27<00:33, 771.02it/s] 44%|████▍     | 20305/45920 [00:27<00:31, 804.30it/s] 44%|████▍     | 20388/45920 [00:28<00:32, 790.45it/s] 45%|████▍     | 20469/45920 [00:28<00:32, 791.44it/s] 45%|████▍     | 20550/45920 [00:28<00:33, 761.93it/s] 45%|████▍     | 20647/45920 [00:28<00:30, 819.49it/s] 45%|████▌     | 20731/45920 [00:28<00:31, 801.93it/s] 45%|████▌     | 20816/45920 [00:28<00:30, 813.81it/s] 46%|████▌     | 20899/45920 [00:28<00:33, 755.04it/s] 46%|████▌     | 20976/45920 [00:28<00:33, 745.88it/s] 46%|████▌     | 21052/45920 [00:28<00:37, 656.60it/s] 46%|████▌     | 21120/45920 [00:29<00:39, 635.10it/s] 46%|████▌     | 21186/45920 [00:29<00:38, 640.85it/s] 46%|████▋     | 21268/45920 [00:29<00:35, 687.52it/s] 46%|████▋     | 21338/45920 [00:29<00:38, 636.87it/s] 47%|████▋     | 21433/45920 [00:29<00:34, 718.25it/s] 47%|████▋     | 21507/45920 [00:29<00:34, 703.32it/s] 47%|████▋     | 21607/45920 [00:29<00:30, 785.40it/s] 47%|████▋     | 21688/45920 [00:29<00:32, 757.23it/s] 47%|████▋     | 21765/45920 [00:29<00:33, 710.60it/s] 48%|████▊     | 21838/45920 [00:30<00:37, 641.28it/s] 48%|████▊     | 21904/45920 [00:30<00:38, 629.06it/s] 48%|████▊     | 21995/45920 [00:30<00:34, 701.17it/s] 48%|████▊     | 22067/45920 [00:30<00:38, 626.60it/s] 48%|████▊     | 22145/45920 [00:30<00:35, 665.72it/s] 48%|████▊     | 22252/45920 [00:30<00:30, 772.26it/s] 49%|████▊     | 22337/45920 [00:30<00:30, 775.40it/s] 49%|████▉     | 22437/45920 [00:30<00:28, 837.51it/s] 49%|████▉     | 22523/45920 [00:30<00:28, 824.80it/s] 49%|████▉     | 22607/45920 [00:31<00:28, 822.25it/s] 49%|████▉     | 22691/45920 [00:31<00:30, 764.91it/s] 50%|████▉     | 22788/45920 [00:31<00:28, 817.89it/s] 50%|████▉     | 22884/45920 [00:31<00:26, 855.57it/s] 50%|█████     | 22971/45920 [00:31<00:29, 771.97it/s] 50%|█████     | 23076/45920 [00:31<00:27, 846.06it/s] 50%|█████     | 23163/45920 [00:31<00:29, 771.41it/s] 51%|█████     | 23243/45920 [00:31<00:29, 773.27it/s] 51%|█████     | 23323/45920 [00:31<00:29, 778.62it/s] 51%|█████     | 23409/45920 [00:32<00:28, 780.44it/s] 51%|█████     | 23489/45920 [00:32<00:30, 745.77it/s] 51%|█████▏    | 23565/45920 [00:32<00:35, 623.58it/s] 51%|█████▏    | 23645/45920 [00:32<00:33, 665.63it/s] 52%|█████▏    | 23721/45920 [00:32<00:32, 686.27it/s] 52%|█████▏    | 23793/45920 [00:32<00:32, 671.52it/s] 52%|█████▏    | 23887/45920 [00:32<00:29, 743.86it/s] 52%|█████▏    | 23964/45920 [00:32<00:34, 644.86it/s] 52%|█████▏    | 24059/45920 [00:33<00:30, 720.69it/s] 53%|█████▎    | 24135/45920 [00:33<00:31, 695.19it/s] 53%|█████▎    | 24208/45920 [00:33<00:30, 701.14it/s] 53%|█████▎    | 24281/45920 [00:33<00:32, 660.78it/s] 53%|█████▎    | 24371/45920 [00:33<00:29, 721.77it/s] 53%|█████▎    | 24464/45920 [00:33<00:27, 773.21it/s] 53%|█████▎    | 24554/45920 [00:33<00:26, 808.06it/s] 54%|█████▎    | 24666/45920 [00:33<00:23, 893.67it/s] 54%|█████▍    | 24757/45920 [00:33<00:24, 866.19it/s] 54%|█████▍    | 24845/45920 [00:34<00:25, 839.31it/s] 54%|█████▍    | 24930/45920 [00:34<00:25, 832.48it/s] 54%|█████▍    | 25014/45920 [00:34<00:29, 711.59it/s] 55%|█████▍    | 25091/45920 [00:34<00:28, 725.15it/s] 55%|█████▍    | 25166/45920 [00:34<00:28, 725.67it/s] 55%|█████▍    | 25246/45920 [00:34<00:27, 745.72it/s] 55%|█████▌    | 25339/45920 [00:34<00:25, 797.06it/s] 55%|█████▌    | 25420/45920 [00:34<00:25, 794.66it/s] 56%|█████▌    | 25515/45920 [00:34<00:24, 838.93it/s] 56%|█████▌    | 25600/45920 [00:35<00:26, 768.53it/s] 56%|█████▌    | 25679/45920 [00:35<00:29, 697.83it/s] 56%|█████▌    | 25756/45920 [00:35<00:28, 714.49it/s] 56%|█████▋    | 25856/45920 [00:35<00:25, 791.48it/s] 56%|█████▋    | 25938/45920 [00:35<00:25, 795.51it/s] 57%|█████▋    | 26027/45920 [00:35<00:24, 816.79it/s] 57%|█████▋    | 26110/45920 [00:35<00:26, 753.06it/s] 57%|█████▋    | 26187/45920 [00:35<00:29, 672.49it/s] 57%|█████▋    | 26285/45920 [00:35<00:26, 750.43it/s] 57%|█████▋    | 26363/45920 [00:36<00:26, 741.21it/s] 58%|█████▊    | 26446/45920 [00:36<00:26, 748.48it/s] 58%|█████▊    | 26525/45920 [00:36<00:26, 726.90it/s] 58%|█████▊    | 26614/45920 [00:36<00:25, 771.22it/s] 58%|█████▊    | 26695/45920 [00:36<00:24, 773.51it/s] 58%|█████▊    | 26785/45920 [00:36<00:23, 808.69it/s] 59%|█████▊    | 26867/45920 [00:36<00:26, 706.73it/s] 59%|█████▊    | 26941/45920 [00:36<00:26, 711.69it/s] 59%|█████▉    | 27030/45920 [00:36<00:24, 760.28it/s] 59%|█████▉    | 27112/45920 [00:37<00:24, 776.61it/s] 59%|█████▉    | 27192/45920 [00:37<00:34, 542.96it/s] 59%|█████▉    | 27273/45920 [00:37<00:31, 599.01it/s] 60%|█████▉    | 27361/45920 [00:37<00:27, 665.16it/s] 60%|█████▉    | 27438/45920 [00:37<00:26, 691.12it/s] 60%|█████▉    | 27514/45920 [00:37<00:27, 671.73it/s] 60%|██████    | 27599/45920 [00:37<00:25, 715.90it/s] 60%|██████    | 27679/45920 [00:37<00:25, 717.29it/s] 60%|██████    | 27754/45920 [00:38<00:26, 692.13it/s] 61%|██████    | 27826/45920 [00:38<00:30, 594.53it/s] 61%|██████    | 27889/45920 [00:38<00:31, 578.43it/s] 61%|██████    | 27950/45920 [00:38<00:31, 562.13it/s] 61%|██████    | 28046/45920 [00:38<00:27, 649.72it/s] 61%|██████    | 28113/45920 [00:38<00:28, 625.67it/s] 61%|██████▏   | 28177/45920 [00:38<00:29, 605.47it/s] 62%|██████▏   | 28251/45920 [00:38<00:27, 640.74it/s] 62%|██████▏   | 28325/45920 [00:39<00:26, 667.35it/s] 62%|██████▏   | 28408/45920 [00:39<00:24, 709.86it/s] 62%|██████▏   | 28499/45920 [00:39<00:22, 760.01it/s] 62%|██████▏   | 28576/45920 [00:39<00:23, 743.74it/s] 62%|██████▏   | 28651/45920 [00:39<00:23, 734.04it/s] 63%|██████▎   | 28725/45920 [00:39<00:24, 695.39it/s] 63%|██████▎   | 28810/45920 [00:39<00:23, 736.84it/s] 63%|██████▎   | 28887/45920 [00:39<00:22, 745.65it/s] 63%|██████▎   | 28965/45920 [00:39<00:22, 745.70it/s] 63%|██████▎   | 29047/45920 [00:39<00:22, 765.38it/s] 63%|██████▎   | 29124/45920 [00:40<00:22, 756.49it/s] 64%|██████▎   | 29200/45920 [00:40<00:22, 754.42it/s] 64%|██████▍   | 29276/45920 [00:40<00:27, 613.14it/s] 64%|██████▍   | 29343/45920 [00:40<00:27, 602.70it/s] 64%|██████▍   | 29407/45920 [00:40<00:27, 608.83it/s] 64%|██████▍   | 29488/45920 [00:40<00:24, 661.49it/s] 64%|██████▍   | 29559/45920 [00:40<00:24, 673.47it/s] 65%|██████▍   | 29637/45920 [00:40<00:23, 698.47it/s] 65%|██████▍   | 29709/45920 [00:40<00:23, 704.50it/s] 65%|██████▍   | 29781/45920 [00:41<00:26, 618.71it/s] 65%|██████▌   | 29856/45920 [00:41<00:24, 652.87it/s] 65%|██████▌   | 29924/45920 [00:41<00:25, 627.31it/s] 65%|██████▌   | 30000/45920 [00:41<00:24, 663.04it/s] 65%|██████▌   | 30068/45920 [00:41<00:24, 651.60it/s] 66%|██████▌   | 30150/45920 [00:41<00:22, 698.20it/s] 66%|██████▌   | 30221/45920 [00:41<00:23, 673.36it/s] 66%|██████▌   | 30290/45920 [00:41<00:24, 641.94it/s] 66%|██████▌   | 30363/45920 [00:41<00:23, 664.75it/s] 66%|██████▋   | 30439/45920 [00:42<00:22, 677.62it/s] 66%|██████▋   | 30536/45920 [00:42<00:20, 756.63it/s] 67%|██████▋   | 30616/45920 [00:42<00:20, 755.80it/s] 67%|██████▋   | 30694/45920 [00:42<00:19, 762.22it/s] 67%|██████▋   | 30775/45920 [00:42<00:19, 776.09it/s] 67%|██████▋   | 30864/45920 [00:42<00:18, 808.22it/s] 67%|██████▋   | 30946/45920 [00:42<00:19, 782.41it/s] 68%|██████▊   | 31038/45920 [00:42<00:18, 820.72it/s] 68%|██████▊   | 31133/45920 [00:42<00:17, 857.34it/s] 68%|██████▊   | 31220/45920 [00:43<00:19, 738.86it/s] 68%|██████▊   | 31297/45920 [00:43<00:19, 744.97it/s] 68%|██████▊   | 31379/45920 [00:43<00:19, 765.15it/s] 69%|██████▊   | 31468/45920 [00:43<00:21, 662.53it/s] 69%|██████▊   | 31539/45920 [00:43<00:25, 558.89it/s] 69%|██████▉   | 31616/45920 [00:43<00:23, 605.36it/s] 69%|██████▉   | 31682/45920 [00:43<00:23, 601.00it/s] 69%|██████▉   | 31746/45920 [00:43<00:24, 588.04it/s] 69%|██████▉   | 31808/45920 [00:44<00:25, 556.87it/s] 69%|██████▉   | 31882/45920 [00:44<00:23, 601.21it/s] 70%|██████▉   | 31944/45920 [00:44<00:23, 604.42it/s] 70%|██████▉   | 32022/45920 [00:44<00:21, 651.43it/s] 70%|██████▉   | 32110/45920 [00:44<00:19, 714.73it/s] 70%|███████   | 32193/45920 [00:44<00:18, 728.90it/s] 70%|███████   | 32267/45920 [00:44<00:19, 708.32it/s] 70%|███████   | 32345/45920 [00:44<00:18, 727.85it/s] 71%|███████   | 32423/45920 [00:44<00:18, 742.78it/s] 71%|███████   | 32504/45920 [00:44<00:17, 762.21it/s] 71%|███████   | 32581/45920 [00:45<00:17, 745.86it/s] 71%|███████   | 32676/45920 [00:45<00:16, 801.85it/s] 71%|███████▏  | 32757/45920 [00:45<00:19, 669.21it/s] 72%|███████▏  | 32835/45920 [00:45<00:18, 692.78it/s] 72%|███████▏  | 32935/45920 [00:45<00:16, 773.47it/s] 72%|███████▏  | 33016/45920 [00:45<00:16, 763.18it/s] 72%|███████▏  | 33100/45920 [00:45<00:16, 779.10it/s] 72%|███████▏  | 33180/45920 [00:45<00:16, 765.77it/s] 72%|███████▏  | 33266/45920 [00:46<00:16, 784.13it/s] 73%|███████▎  | 33366/45920 [00:46<00:14, 845.46it/s] 73%|███████▎  | 33452/45920 [00:46<00:15, 831.09it/s] 73%|███████▎  | 33540/45920 [00:46<00:14, 844.69it/s] 73%|███████▎  | 33625/45920 [00:46<00:14, 820.14it/s] 73%|███████▎  | 33708/45920 [00:46<00:15, 803.87it/s] 74%|███████▎  | 33796/45920 [00:46<00:14, 824.30it/s] 74%|███████▍  | 33879/45920 [00:46<00:16, 728.53it/s] 74%|███████▍  | 33956/45920 [00:46<00:16, 733.71it/s] 74%|███████▍  | 34051/45920 [00:46<00:14, 791.76it/s] 74%|███████▍  | 34132/45920 [00:47<00:14, 787.44it/s] 75%|███████▍  | 34212/45920 [00:47<00:16, 689.08it/s] 75%|███████▍  | 34295/45920 [00:47<00:16, 722.86it/s] 75%|███████▍  | 34380/45920 [00:47<00:15, 756.41it/s] 75%|███████▌  | 34458/45920 [00:47<00:17, 647.59it/s] 75%|███████▌  | 34549/45920 [00:47<00:15, 713.69it/s] 75%|███████▌  | 34642/45920 [00:47<00:14, 770.48it/s] 76%|███████▌  | 34723/45920 [00:47<00:15, 743.02it/s] 76%|███████▌  | 34801/45920 [00:48<00:15, 737.80it/s] 76%|███████▌  | 34877/45920 [00:48<00:14, 743.33it/s] 76%|███████▌  | 34953/45920 [00:48<00:14, 740.52it/s] 76%|███████▋  | 35028/45920 [00:48<00:16, 645.78it/s] 76%|███████▋  | 35099/45920 [00:48<00:16, 657.13it/s] 77%|███████▋  | 35167/45920 [00:48<00:18, 589.08it/s] 77%|███████▋  | 35253/45920 [00:48<00:16, 657.97it/s] 77%|███████▋  | 35347/45920 [00:48<00:14, 731.17it/s] 77%|███████▋  | 35423/45920 [00:48<00:15, 698.28it/s] 77%|███████▋  | 35509/45920 [00:49<00:14, 737.89it/s] 77%|███████▋  | 35585/45920 [00:49<00:15, 656.86it/s] 78%|███████▊  | 35679/45920 [00:49<00:14, 729.65it/s] 78%|███████▊  | 35755/45920 [00:49<00:14, 721.62it/s] 78%|███████▊  | 35832/45920 [00:49<00:13, 731.30it/s] 78%|███████▊  | 35924/45920 [00:49<00:12, 782.72it/s] 78%|███████▊  | 36004/45920 [00:49<00:13, 755.16it/s] 79%|███████▊  | 36081/45920 [00:49<00:14, 694.11it/s] 79%|███████▉  | 36167/45920 [00:49<00:13, 738.37it/s] 79%|███████▉  | 36248/45920 [00:50<00:12, 757.49it/s] 79%|███████▉  | 36326/45920 [00:50<00:14, 682.44it/s] 79%|███████▉  | 36397/45920 [00:50<00:14, 671.77it/s] 79%|███████▉  | 36486/45920 [00:50<00:12, 729.02it/s] 80%|███████▉  | 36571/45920 [00:50<00:13, 708.50it/s] 80%|███████▉  | 36644/45920 [00:50<00:13, 681.87it/s] 80%|███████▉  | 36714/45920 [00:50<00:14, 634.34it/s] 80%|████████  | 36795/45920 [00:50<00:13, 680.13it/s] 80%|████████  | 36865/45920 [00:51<00:13, 676.87it/s] 80%|████████  | 36934/45920 [00:51<00:13, 649.19it/s] 81%|████████  | 37000/45920 [00:51<00:13, 638.83it/s] 81%|████████  | 37099/45920 [00:51<00:12, 731.15it/s] 81%|████████  | 37182/45920 [00:51<00:11, 752.96it/s] 81%|████████  | 37272/45920 [00:51<00:10, 792.04it/s] 81%|████████▏ | 37352/45920 [00:51<00:11, 744.58it/s] 82%|████████▏ | 37433/45920 [00:51<00:11, 757.98it/s] 82%|████████▏ | 37510/45920 [00:51<00:11, 758.82it/s] 82%|████████▏ | 37587/45920 [00:51<00:11, 700.65it/s] 82%|████████▏ | 37659/45920 [00:52<00:12, 686.87it/s] 82%|████████▏ | 37736/45920 [00:52<00:11, 708.00it/s] 82%|████████▏ | 37825/45920 [00:52<00:10, 758.90it/s] 83%|████████▎ | 37908/45920 [00:52<00:10, 779.10it/s] 83%|████████▎ | 37998/45920 [00:52<00:10, 748.81it/s] 83%|████████▎ | 38074/45920 [00:52<00:12, 639.21it/s] 83%|████████▎ | 38141/45920 [00:52<00:12, 643.35it/s] 83%|████████▎ | 38223/45920 [00:52<00:11, 687.58it/s] 83%|████████▎ | 38297/45920 [00:53<00:10, 698.30it/s] 84%|████████▎ | 38371/45920 [00:53<00:10, 709.18it/s] 84%|████████▎ | 38448/45920 [00:53<00:10, 721.77it/s] 84%|████████▍ | 38540/45920 [00:53<00:09, 778.78it/s] 84%|████████▍ | 38619/45920 [00:53<00:11, 623.23it/s] 84%|████████▍ | 38687/45920 [00:53<00:11, 608.42it/s] 84%|████████▍ | 38752/45920 [00:53<00:11, 612.97it/s] 85%|████████▍ | 38816/45920 [00:53<00:11, 619.83it/s] 85%|████████▍ | 38880/45920 [00:53<00:11, 614.47it/s] 85%|████████▍ | 38956/45920 [00:54<00:10, 654.81it/s] 85%|████████▍ | 39030/45920 [00:54<00:10, 676.23it/s] 85%|████████▌ | 39101/45920 [00:54<00:09, 685.43it/s] 85%|████████▌ | 39186/45920 [00:54<00:09, 733.04it/s] 85%|████████▌ | 39260/45920 [00:54<00:10, 615.77it/s] 86%|████████▌ | 39331/45920 [00:54<00:10, 640.17it/s] 86%|████████▌ | 39398/45920 [00:54<00:10, 620.38it/s] 86%|████████▌ | 39485/45920 [00:54<00:09, 686.81it/s] 86%|████████▌ | 39556/45920 [00:54<00:09, 688.70it/s] 86%|████████▋ | 39627/45920 [00:55<00:09, 646.76it/s] 86%|████████▋ | 39694/45920 [00:55<00:10, 618.05it/s] 87%|████████▋ | 39768/45920 [00:55<00:09, 647.35it/s] 87%|████████▋ | 39835/45920 [00:55<00:09, 653.29it/s] 87%|████████▋ | 39924/45920 [00:55<00:08, 719.89it/s] 87%|████████▋ | 39997/45920 [00:55<00:08, 693.94it/s] 87%|████████▋ | 40068/45920 [00:55<00:08, 682.26it/s] 87%|████████▋ | 40146/45920 [00:55<00:08, 708.47it/s] 88%|████████▊ | 40224/45920 [00:55<00:07, 721.44it/s] 88%|████████▊ | 40297/45920 [00:56<00:08, 676.09it/s] 88%|████████▊ | 40369/45920 [00:56<00:08, 688.13it/s] 88%|████████▊ | 40446/45920 [00:56<00:07, 711.15it/s] 88%|████████▊ | 40527/45920 [00:56<00:07, 732.87it/s] 88%|████████▊ | 40601/45920 [00:56<00:07, 699.20it/s] 89%|████████▊ | 40692/45920 [00:56<00:06, 758.78it/s] 89%|████████▉ | 40772/45920 [00:56<00:06, 763.27it/s] 89%|████████▉ | 40849/45920 [00:56<00:06, 727.29it/s] 89%|████████▉ | 40937/45920 [00:56<00:06, 762.46it/s] 89%|████████▉ | 41015/45920 [00:56<00:06, 766.09it/s] 90%|████████▉ | 41106/45920 [00:57<00:06, 800.69it/s] 90%|████████▉ | 41187/45920 [00:57<00:06, 691.15it/s] 90%|████████▉ | 41259/45920 [00:57<00:06, 691.43it/s] 90%|█████████ | 41331/45920 [00:57<00:06, 683.04it/s] 90%|█████████ | 41401/45920 [00:57<00:06, 687.58it/s] 90%|█████████ | 41478/45920 [00:57<00:06, 708.63it/s] 90%|█████████ | 41550/45920 [00:57<00:07, 608.25it/s] 91%|█████████ | 41647/45920 [00:57<00:06, 701.05it/s] 91%|█████████ | 41733/45920 [00:57<00:05, 742.97it/s] 91%|█████████ | 41831/45920 [00:58<00:05, 808.52it/s] 91%|█████████▏| 41935/45920 [00:58<00:04, 873.95it/s] 92%|█████████▏| 42025/45920 [00:58<00:04, 870.15it/s] 92%|█████████▏| 42114/45920 [00:58<00:04, 847.56it/s] 92%|█████████▏| 42200/45920 [00:58<00:05, 719.47it/s] 92%|█████████▏| 42276/45920 [00:58<00:05, 696.76it/s] 92%|█████████▏| 42349/45920 [00:58<00:05, 694.61it/s] 92%|█████████▏| 42439/45920 [00:58<00:04, 744.70it/s] 93%|█████████▎| 42525/45920 [00:58<00:04, 772.21it/s] 93%|█████████▎| 42611/45920 [00:59<00:04, 796.06it/s] 93%|█████████▎| 42692/45920 [00:59<00:04, 776.34it/s] 93%|█████████▎| 42771/45920 [00:59<00:04, 705.81it/s] 93%|█████████▎| 42844/45920 [00:59<00:04, 709.20it/s] 93%|█████████▎| 42917/45920 [00:59<00:04, 714.92it/s] 94%|█████████▎| 42990/45920 [00:59<00:04, 695.40it/s] 94%|█████████▍| 43061/45920 [00:59<00:04, 594.00it/s] 94%|█████████▍| 43146/45920 [00:59<00:04, 657.44it/s] 94%|█████████▍| 43217/45920 [01:00<00:04, 671.31it/s] 94%|█████████▍| 43301/45920 [01:00<00:03, 715.92it/s] 94%|█████████▍| 43385/45920 [01:00<00:03, 748.71it/s] 95%|█████████▍| 43462/45920 [01:00<00:03, 700.58it/s] 95%|█████████▍| 43534/45920 [01:00<00:03, 703.20it/s] 95%|█████████▍| 43606/45920 [01:00<00:03, 673.23it/s] 95%|█████████▌| 43705/45920 [01:00<00:02, 759.79it/s] 95%|█████████▌| 43783/45920 [01:00<00:02, 751.23it/s] 96%|█████████▌| 43877/45920 [01:00<00:02, 803.35it/s] 96%|█████████▌| 43962/45920 [01:00<00:02, 804.15it/s] 96%|█████████▌| 44047/45920 [01:01<00:02, 813.29it/s] 96%|█████████▌| 44138/45920 [01:01<00:02, 804.08it/s] 96%|█████████▋| 44219/45920 [01:01<00:02, 628.78it/s] 96%|█████████▋| 44306/45920 [01:01<00:02, 683.35it/s] 97%|█████████▋| 44380/45920 [01:01<00:02, 687.87it/s] 97%|█████████▋| 44470/45920 [01:01<00:01, 743.32it/s] 97%|█████████▋| 44558/45920 [01:01<00:01, 780.60it/s] 97%|█████████▋| 44639/45920 [01:01<00:01, 743.42it/s] 97%|█████████▋| 44716/45920 [01:02<00:01, 741.62it/s] 98%|█████████▊| 44792/45920 [01:02<00:01, 645.00it/s] 98%|█████████▊| 44860/45920 [01:02<00:01, 612.29it/s] 98%|█████████▊| 44926/45920 [01:02<00:01, 624.31it/s] 98%|█████████▊| 45002/45920 [01:02<00:01, 648.20it/s] 98%|█████████▊| 45069/45920 [01:02<00:01, 540.44it/s] 98%|█████████▊| 45149/45920 [01:02<00:01, 603.56it/s] 98%|█████████▊| 45227/45920 [01:02<00:01, 640.68it/s] 99%|█████████▊| 45295/45920 [01:03<00:00, 644.22it/s] 99%|█████████▉| 45376/45920 [01:03<00:00, 688.36it/s] 99%|█████████▉| 45474/45920 [01:03<00:00, 769.77it/s] 99%|█████████▉| 45560/45920 [01:03<00:00, 792.64it/s] 99%|█████████▉| 45641/45920 [01:03<00:00, 637.77it/s]100%|█████████▉| 45723/45920 [01:03<00:00, 679.62it/s]100%|█████████▉| 45818/45920 [01:03<00:00, 749.92it/s]100%|█████████▉| 45900/45920 [01:03<00:00, 698.75it/s]100%|██████████| 45920/45920 [01:03<00:00, 718.96it/s]

gathering stats for n=1
  0%|          | 0/45920 [00:00<?, ?it/s]  1%|          | 296/45920 [00:00<00:15, 2950.27it/s]  1%|▏         | 609/45920 [00:00<00:14, 3049.30it/s]  2%|▏         | 914/45920 [00:00<00:16, 2731.70it/s]  3%|▎         | 1269/45920 [00:00<00:14, 3027.29it/s]  4%|▎         | 1620/45920 [00:00<00:13, 3180.41it/s]  4%|▍         | 1967/45920 [00:00<00:13, 3275.03it/s]  5%|▌         | 2297/45920 [00:00<00:13, 3225.03it/s]  6%|▌         | 2622/45920 [00:00<00:13, 3178.75it/s]  6%|▋         | 2974/45920 [00:00<00:13, 3281.08it/s]  7%|▋         | 3304/45920 [00:01<00:13, 3198.70it/s]  8%|▊         | 3625/45920 [00:01<00:13, 3033.72it/s]  9%|▊         | 3931/45920 [00:01<00:13, 3002.81it/s]  9%|▉         | 4233/45920 [00:01<00:14, 2869.09it/s] 10%|▉         | 4522/45920 [00:01<00:14, 2864.50it/s] 11%|█         | 4837/45920 [00:01<00:13, 2945.93it/s] 11%|█         | 5157/45920 [00:01<00:13, 3018.20it/s] 12%|█▏        | 5471/45920 [00:01<00:13, 3045.85it/s] 13%|█▎        | 5777/45920 [00:01<00:13, 2973.55it/s] 13%|█▎        | 6076/45920 [00:02<00:13, 2970.27it/s] 14%|█▍        | 6403/45920 [00:02<00:12, 3056.52it/s] 15%|█▍        | 6710/45920 [00:02<00:12, 3060.26it/s] 15%|█▌        | 7051/45920 [00:02<00:12, 3157.95it/s] 16%|█▌        | 7368/45920 [00:02<00:12, 3141.27it/s] 17%|█▋        | 7683/45920 [00:02<00:13, 2837.40it/s] 17%|█▋        | 8015/45920 [00:02<00:12, 2965.00it/s] 18%|█▊        | 8334/45920 [00:02<00:12, 3028.01it/s] 19%|█▉        | 8663/45920 [00:02<00:12, 3101.22it/s] 20%|█▉        | 8977/45920 [00:02<00:12, 2998.66it/s] 20%|██        | 9304/45920 [00:03<00:11, 3073.35it/s] 21%|██        | 9619/45920 [00:03<00:12, 3025.01it/s] 22%|██▏       | 9924/45920 [00:03<00:12, 2980.90it/s] 22%|██▏       | 10224/45920 [00:03<00:12, 2944.21it/s] 23%|██▎       | 10581/45920 [00:03<00:11, 3124.13it/s] 24%|██▎       | 10895/45920 [00:03<00:11, 3047.08it/s] 24%|██▍       | 11224/45920 [00:03<00:11, 3114.90it/s] 25%|██▌       | 11537/45920 [00:03<00:11, 3094.45it/s] 26%|██▌       | 11848/45920 [00:03<00:11, 2965.20it/s] 26%|██▋       | 12146/45920 [00:04<00:11, 2915.95it/s] 27%|██▋       | 12504/45920 [00:04<00:10, 3104.84it/s] 28%|██▊       | 12817/45920 [00:04<00:10, 3061.50it/s] 29%|██▊       | 13154/45920 [00:04<00:10, 3148.65it/s] 29%|██▉       | 13493/45920 [00:04<00:10, 3213.67it/s] 30%|███       | 13816/45920 [00:04<00:10, 3172.63it/s] 31%|███       | 14134/45920 [00:04<00:10, 3021.36it/s] 31%|███▏      | 14438/45920 [00:04<00:10, 2987.56it/s] 32%|███▏      | 14738/45920 [00:04<00:10, 2881.19it/s] 33%|███▎      | 15036/45920 [00:04<00:10, 2900.68it/s] 33%|███▎      | 15328/45920 [00:05<00:10, 2880.73it/s] 34%|███▍      | 15620/45920 [00:05<00:10, 2864.38it/s] 35%|███▍      | 15932/45920 [00:05<00:10, 2936.13it/s] 35%|███▌      | 16232/45920 [00:05<00:10, 2953.84it/s] 36%|███▌      | 16528/45920 [00:05<00:10, 2900.25it/s] 37%|███▋      | 16824/45920 [00:05<00:09, 2915.78it/s] 37%|███▋      | 17139/45920 [00:05<00:09, 2981.67it/s] 38%|███▊      | 17471/45920 [00:05<00:09, 3075.15it/s] 39%|███▉      | 17819/45920 [00:05<00:08, 3193.67it/s] 40%|███▉      | 18139/45920 [00:05<00:08, 3120.18it/s] 40%|████      | 18452/45920 [00:06<00:08, 3113.13it/s] 41%|████      | 18764/45920 [00:06<00:09, 2947.26it/s] 42%|████▏     | 19134/45920 [00:06<00:08, 3161.45it/s] 42%|████▏     | 19453/45920 [00:06<00:08, 3141.06it/s] 43%|████▎     | 19778/45920 [00:06<00:08, 3169.46it/s] 44%|████▍     | 20099/45920 [00:06<00:08, 3174.98it/s] 44%|████▍     | 20432/45920 [00:06<00:07, 3212.56it/s] 45%|████▌     | 20798/45920 [00:06<00:07, 3308.69it/s] 46%|████▌     | 21130/45920 [00:06<00:07, 3144.11it/s] 47%|████▋     | 21447/45920 [00:07<00:07, 3082.27it/s] 47%|████▋     | 21757/45920 [00:07<00:07, 3081.73it/s] 48%|████▊     | 22067/45920 [00:07<00:08, 2958.14it/s] 49%|████▉     | 22426/45920 [00:07<00:07, 3132.15it/s] 50%|████▉     | 22747/45920 [00:07<00:07, 3153.30it/s] 50%|█████     | 23104/45920 [00:07<00:07, 3258.87it/s] 51%|█████     | 23432/45920 [00:07<00:07, 3188.74it/s] 52%|█████▏    | 23752/45920 [00:07<00:07, 2977.49it/s] 52%|█████▏    | 24053/45920 [00:07<00:07, 2937.45it/s] 53%|█████▎    | 24349/45920 [00:08<00:07, 2857.40it/s] 54%|█████▍    | 24698/45920 [00:08<00:06, 3034.22it/s] 54%|█████▍    | 25004/45920 [00:08<00:07, 2965.62it/s] 55%|█████▌    | 25323/45920 [00:08<00:06, 3028.74it/s] 56%|█████▌    | 25638/45920 [00:08<00:06, 3055.67it/s] 57%|█████▋    | 25962/45920 [00:08<00:06, 3106.68it/s] 57%|█████▋    | 26274/45920 [00:08<00:06, 3071.24it/s] 58%|█████▊    | 26584/45920 [00:08<00:06, 3078.57it/s] 59%|█████▊    | 26922/45920 [00:08<00:05, 3166.78it/s] 59%|█████▉    | 27240/45920 [00:08<00:06, 3001.40it/s] 60%|██████    | 27567/45920 [00:09<00:05, 3078.02it/s] 61%|██████    | 27877/45920 [00:09<00:05, 3018.11it/s] 61%|██████▏   | 28181/45920 [00:09<00:06, 2911.28it/s] 62%|██████▏   | 28499/45920 [00:09<00:05, 2984.62it/s] 63%|██████▎   | 28808/45920 [00:09<00:05, 3014.61it/s] 63%|██████▎   | 29116/45920 [00:09<00:05, 3031.95it/s] 64%|██████▍   | 29421/45920 [00:09<00:05, 2905.79it/s] 65%|██████▍   | 29744/45920 [00:09<00:05, 2992.90it/s] 65%|██████▌   | 30045/45920 [00:09<00:05, 2978.49it/s] 66%|██████▌   | 30349/45920 [00:09<00:05, 2989.68it/s] 67%|██████▋   | 30684/45920 [00:10<00:04, 3093.88it/s] 68%|██████▊   | 31010/45920 [00:10<00:04, 3138.15it/s] 68%|██████▊   | 31361/45920 [00:10<00:04, 3247.80it/s] 69%|██████▉   | 31687/45920 [00:10<00:04, 2989.65it/s] 70%|██████▉   | 31991/45920 [00:10<00:04, 2963.75it/s] 70%|███████   | 32329/45920 [00:10<00:04, 3070.17it/s] 71%|███████   | 32656/45920 [00:10<00:04, 3126.64it/s] 72%|███████▏  | 32974/45920 [00:10<00:04, 3137.08it/s] 73%|███████▎  | 33312/45920 [00:10<00:03, 3204.96it/s] 73%|███████▎  | 33657/45920 [00:11<00:03, 3271.46it/s] 74%|███████▍  | 33986/45920 [00:11<00:03, 3259.34it/s] 75%|███████▍  | 34313/45920 [00:11<00:03, 3172.55it/s] 75%|███████▌  | 34635/45920 [00:11<00:03, 3181.94it/s] 76%|███████▌  | 34955/45920 [00:11<00:03, 3185.64it/s] 77%|███████▋  | 35275/45920 [00:11<00:03, 2987.72it/s] 77%|███████▋  | 35577/45920 [00:11<00:03, 2984.14it/s] 78%|███████▊  | 35911/45920 [00:11<00:03, 3081.98it/s] 79%|███████▉  | 36232/45920 [00:11<00:03, 3116.98it/s] 80%|███████▉  | 36545/45920 [00:11<00:03, 3111.16it/s] 80%|████████  | 36858/45920 [00:12<00:03, 3010.36it/s] 81%|████████  | 37184/45920 [00:12<00:02, 3081.98it/s] 82%|████████▏ | 37494/45920 [00:12<00:02, 3079.71it/s] 82%|████████▏ | 37809/45920 [00:12<00:02, 3076.49it/s] 83%|████████▎ | 38118/45920 [00:12<00:02, 3001.47it/s] 84%|████████▎ | 38423/45920 [00:12<00:02, 3014.74it/s] 84%|████████▍ | 38725/45920 [00:12<00:02, 2930.54it/s] 85%|████████▍ | 39025/45920 [00:12<00:02, 2950.54it/s] 86%|████████▌ | 39321/45920 [00:12<00:02, 2932.11it/s] 86%|████████▋ | 39615/45920 [00:12<00:02, 2930.07it/s] 87%|████████▋ | 39919/45920 [00:13<00:02, 2962.34it/s] 88%|████████▊ | 40226/45920 [00:13<00:01, 2991.43it/s] 88%|████████▊ | 40526/45920 [00:13<00:01, 2972.83it/s] 89%|████████▉ | 40842/45920 [00:13<00:01, 3028.05it/s] 90%|████████▉ | 41154/45920 [00:13<00:01, 3055.33it/s] 90%|█████████ | 41460/45920 [00:13<00:01, 3002.89it/s] 91%|█████████ | 41793/45920 [00:13<00:01, 3099.13it/s] 92%|█████████▏| 42130/45920 [00:13<00:01, 3169.02it/s] 92%|█████████▏| 42448/45920 [00:13<00:01, 3143.56it/s] 93%|█████████▎| 42774/45920 [00:14<00:00, 3176.18it/s] 94%|█████████▍| 43092/45920 [00:14<00:00, 3097.90it/s] 95%|█████████▍| 43403/45920 [00:14<00:00, 3094.85it/s] 95%|█████████▌| 43718/45920 [00:14<00:00, 3110.16it/s] 96%|█████████▌| 44048/45920 [00:14<00:00, 3166.03it/s] 97%|█████████▋| 44365/45920 [00:14<00:00, 3080.45it/s] 97%|█████████▋| 44683/45920 [00:14<00:00, 3101.29it/s] 98%|█████████▊| 44994/45920 [00:14<00:00, 3007.18it/s] 99%|█████████▊| 45296/45920 [00:14<00:00, 2938.12it/s] 99%|█████████▉| 45620/45920 [00:14<00:00, 3020.16it/s]100%|██████████| 45920/45920 [00:15<00:00, 3054.33it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 462.79it/s]2022-03-15 16:34:22 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(34528, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=34528, bias=False)
  )
)
2022-03-15 16:34:22 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 16:34:22 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 16:34:22 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 16:34:22 | INFO | fairseq_cli.train | num. shared model params: 36,592,640 (num. trained: 36,592,640)
2022-03-15 16:34:22 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 16:34:22 | INFO | fairseq.data.data_utils | loaded 2,294 examples from: data-bin/de/valid
2022-03-15 16:34:22 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 16:34:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:22 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 16:34:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 16:34:22 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 16:34:22 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-15 16:34:22 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-15 16:34:22 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 16:34:22 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
2022-03-15 16:34:22 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 16:34:22 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 16:34:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 16:34:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:34:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 16:34:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 16:39:48 | INFO | train_inner | epoch 001:    104 / 392 loss=14.705, ppl=26712.3, wps=21681.8, ups=0.33, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.151, loss_scale=8, train_wall=301, gb_free=9.6, wall=326
2022-03-15 16:44:55 | INFO | train_inner | epoch 001:    204 / 392 loss=13.163, ppl=9171.57, wps=21365.7, ups=0.33, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.722, loss_scale=16, train_wall=282, gb_free=9.6, wall=633
2022-03-15 16:50:04 | INFO | train_inner | epoch 001:    304 / 392 loss=12.224, ppl=4783.96, wps=21229.3, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.455, loss_scale=32, train_wall=284, gb_free=9.6, wall=942
2022-03-15 16:54:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 16:55:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.58 | ppl 3061.35 | wps 33328.5 | wpb 511.9 | bsz 1 | num_updates 388
2022-03-15 16:55:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-15 16:55:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 16:55:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 16:55:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.58) (writing took 2.4670074820169248 seconds)
2022-03-15 16:55:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 16:55:13 | INFO | train | epoch 001 | loss 13.002 | ppl 8202.17 | wps 20681.4 | ups 0.32 | wpb 65404.5 | bsz 127.7 | num_updates 388 | lr 4.85903e-05 | gnorm 0.946 | loss_scale 64 | train_wall 1114 | gb_free 9.6 | wall 1251
KL Stats: Epoch 1 Divergences: Uniform: 0.6982834114577697 Unigram: 0.8934114434183179
2022-03-15 16:55:13 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 16:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 16:55:50 | INFO | train_inner | epoch 002:     12 / 392 loss=11.741, ppl=3423.04, wps=18792.4, ups=0.29, wpb=65022.4, bsz=127, num_updates=400, lr=5.009e-05, gnorm=0.39, loss_scale=64, train_wall=280, gb_free=9.6, wall=1288
2022-03-15 16:59:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:01:03 | INFO | train_inner | epoch 002:    113 / 392 loss=11.508, ppl=2913.11, wps=20919, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.376, loss_scale=32, train_wall=288, gb_free=9.6, wall=1601
2022-03-15 17:06:14 | INFO | train_inner | epoch 002:    213 / 392 loss=11.253, ppl=2440.03, wps=21058.6, ups=0.32, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.377, loss_scale=64, train_wall=286, gb_free=9.6, wall=1912
2022-03-15 17:10:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:11:26 | INFO | train_inner | epoch 002:    314 / 392 loss=10.957, ppl=1987.22, wps=21025.6, ups=0.32, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.439, loss_scale=32, train_wall=287, gb_free=9.6, wall=2224
2022-03-15 17:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:16:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.439 | ppl 1388.09 | wps 32719.5 | wpb 511.9 | bsz 1 | num_updates 778 | best_loss 10.439
2022-03-15 17:16:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 778 updates
2022-03-15 17:16:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 17:16:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 17:16:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 2 @ 778 updates, score 10.439) (writing took 2.2744335900060833 seconds)
2022-03-15 17:16:06 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 17:16:06 | INFO | train | epoch 002 | loss 11.145 | ppl 2264.44 | wps 20367.3 | ups 0.31 | wpb 65405.2 | bsz 127.7 | num_updates 778 | lr 9.73306e-05 | gnorm 0.403 | loss_scale 32 | train_wall 1114 | gb_free 9.6 | wall 2503
KL Stats: Epoch 2 Divergences: Uniform: 1.4639421266699915 Unigram: 0.5688864606272428
2022-03-15 17:16:06 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 17:16:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:17:15 | INFO | train_inner | epoch 003:     22 / 392 loss=10.672, ppl=1631.73, wps=18639.6, ups=0.29, wpb=65029.1, bsz=127, num_updates=800, lr=0.00010008, gnorm=0.438, loss_scale=32, train_wall=283, gb_free=9.6, wall=2573
2022-03-15 17:18:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:22:32 | INFO | train_inner | epoch 003:    124 / 392 loss=10.424, ppl=1374.17, wps=20685.2, ups=0.32, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.491, loss_scale=16, train_wall=292, gb_free=9.6, wall=2890
2022-03-15 17:27:44 | INFO | train_inner | epoch 003:    224 / 392 loss=10.216, ppl=1189.37, wps=20995.1, ups=0.32, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.531, loss_scale=32, train_wall=287, gb_free=9.6, wall=3202
2022-03-15 17:32:58 | INFO | train_inner | epoch 003:    324 / 392 loss=10.026, ppl=1042.47, wps=20837.6, ups=0.32, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.56, loss_scale=64, train_wall=290, gb_free=9.6, wall=3516
2022-03-15 17:33:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:36:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:37:07 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.67 | ppl 814.66 | wps 32803.4 | wpb 511.9 | bsz 1 | num_updates 1167 | best_loss 9.67
2022-03-15 17:37:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1167 updates
2022-03-15 17:37:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 17:37:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 17:37:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 3 @ 1167 updates, score 9.67) (writing took 2.324097241042182 seconds)
2022-03-15 17:37:10 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 17:37:10 | INFO | train | epoch 003 | loss 10.183 | ppl 1162.61 | wps 20127.6 | ups 0.31 | wpb 65404.8 | bsz 127.7 | num_updates 1167 | lr 0.000145946 | gnorm 0.528 | loss_scale 32 | train_wall 1125 | gb_free 9.6 | wall 3767
KL Stats: Epoch 3 Divergences: Uniform: 1.9779916767932857 Unigram: 1.3132684725207877
2022-03-15 17:37:10 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 17:37:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:38:54 | INFO | train_inner | epoch 004:     33 / 392 loss=9.853, ppl=924.69, wps=18283.6, ups=0.28, wpb=65025.8, bsz=127, num_updates=1200, lr=0.00015007, gnorm=0.557, loss_scale=32, train_wall=289, gb_free=9.6, wall=3872
2022-03-15 17:43:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:44:13 | INFO | train_inner | epoch 004:    134 / 392 loss=9.697, ppl=829.96, wps=20542, ups=0.31, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.616, loss_scale=32, train_wall=294, gb_free=9.6, wall=4191
2022-03-15 17:49:30 | INFO | train_inner | epoch 004:    234 / 392 loss=9.562, ppl=755.64, wps=20681.1, ups=0.32, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.628, loss_scale=32, train_wall=292, gb_free=9.6, wall=4508
2022-03-15 17:50:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:54:51 | INFO | train_inner | epoch 004:    335 / 392 loss=9.432, ppl=690.72, wps=20435.8, ups=0.31, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.644, loss_scale=32, train_wall=296, gb_free=9.6, wall=4829
2022-03-15 17:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:58:27 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.095 | ppl 546.76 | wps 33482.5 | wpb 511.9 | bsz 1 | num_updates 1557 | best_loss 9.095
2022-03-15 17:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1557 updates
2022-03-15 17:58:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 17:58:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 17:58:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 4 @ 1557 updates, score 9.095) (writing took 2.532383252051659 seconds)
2022-03-15 17:58:29 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 17:58:29 | INFO | train | epoch 004 | loss 9.548 | ppl 748.6 | wps 19930.9 | ups 0.3 | wpb 65405.2 | bsz 127.7 | num_updates 1557 | lr 0.000194686 | gnorm 0.63 | loss_scale 64 | train_wall 1141 | gb_free 9.6 | wall 5047
KL Stats: Epoch 4 Divergences: Uniform: 2.3927394953951806 Unigram: 1.7147965452043394
2022-03-15 17:58:29 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 17:58:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:59:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:00:45 | INFO | train_inner | epoch 005:     44 / 392 loss=9.283, ppl=622.93, wps=18348.8, ups=0.28, wpb=65025.8, bsz=127, num_updates=1600, lr=0.00020006, gnorm=0.675, loss_scale=32, train_wall=288, gb_free=9.6, wall=5183
2022-03-15 18:05:54 | INFO | train_inner | epoch 005:    144 / 392 loss=9.141, ppl=564.49, wps=21189.6, ups=0.32, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.655, loss_scale=32, train_wall=285, gb_free=9.6, wall=5492
2022-03-15 18:06:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:07:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:11:12 | INFO | train_inner | epoch 005:    246 / 392 loss=9.009, ppl=515.35, wps=20657.8, ups=0.32, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.689, loss_scale=16, train_wall=292, gb_free=9.6, wall=5810
2022-03-15 18:16:27 | INFO | train_inner | epoch 005:    346 / 392 loss=8.884, ppl=472.36, wps=20784.9, ups=0.32, wpb=65532.7, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.704, loss_scale=32, train_wall=290, gb_free=9.6, wall=6125
2022-03-15 18:18:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:19:29 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.562 | ppl 377.87 | wps 32702.1 | wpb 511.9 | bsz 1 | num_updates 1946 | best_loss 8.562
2022-03-15 18:19:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1946 updates
2022-03-15 18:19:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 18:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 18:19:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 5 @ 1946 updates, score 8.562) (writing took 2.354639583034441 seconds)
2022-03-15 18:19:31 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 18:19:31 | INFO | train | epoch 005 | loss 9.009 | ppl 515.29 | wps 20162.3 | ups 0.31 | wpb 65404.8 | bsz 127.7 | num_updates 1946 | lr 0.000243301 | gnorm 0.682 | loss_scale 32 | train_wall 1123 | gb_free 9.6 | wall 6309
KL Stats: Epoch 5 Divergences: Uniform: 2.713054259399163 Unigram: 1.9896246913738618
2022-03-15 18:19:31 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 18:19:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:21:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:22:22 | INFO | train_inner | epoch 006:     55 / 392 loss=8.742, ppl=428.05, wps=18324.7, ups=0.28, wpb=65029.1, bsz=127, num_updates=2000, lr=0.00025005, gnorm=0.678, loss_scale=32, train_wall=288, gb_free=9.6, wall=6480
2022-03-15 18:27:36 | INFO | train_inner | epoch 006:    155 / 392 loss=8.622, ppl=393.92, wps=20846, ups=0.32, wpb=65532.7, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.671, loss_scale=32, train_wall=289, gb_free=9.6, wall=6794
2022-03-15 18:28:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:32:58 | INFO | train_inner | epoch 006:    256 / 392 loss=8.522, ppl=367.72, wps=20389.4, ups=0.31, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.69, loss_scale=32, train_wall=296, gb_free=9.6, wall=7116
2022-03-15 18:33:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:38:16 | INFO | train_inner | epoch 006:    357 / 392 loss=8.428, ppl=344.35, wps=20600.7, ups=0.31, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.666, loss_scale=16, train_wall=293, gb_free=9.6, wall=7434
2022-03-15 18:40:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:40:42 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.178 | ppl 289.64 | wps 32654.6 | wpb 511.9 | bsz 1 | num_updates 2335 | best_loss 8.178
2022-03-15 18:40:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2335 updates
2022-03-15 18:40:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 18:40:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 18:40:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 6 @ 2335 updates, score 8.178) (writing took 2.4384850839851424 seconds)
2022-03-15 18:40:45 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 18:40:45 | INFO | train | epoch 006 | loss 8.536 | ppl 371.17 | wps 19980.1 | ups 0.31 | wpb 65404.8 | bsz 127.7 | num_updates 2335 | lr 0.000291917 | gnorm 0.676 | loss_scale 32 | train_wall 1134 | gb_free 9.6 | wall 7583
KL Stats: Epoch 6 Divergences: Uniform: 3.0135383068059185 Unigram: 2.2142467437417253
2022-03-15 18:40:45 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 18:40:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:44:07 | INFO | train_inner | epoch 007:     65 / 392 loss=8.322, ppl=319.94, wps=18510.6, ups=0.28, wpb=65029.1, bsz=127, num_updates=2400, lr=0.00030004, gnorm=0.67, loss_scale=32, train_wall=285, gb_free=9.6, wall=7785
2022-03-15 18:44:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:49:24 | INFO | train_inner | epoch 007:    166 / 392 loss=8.228, ppl=299.81, wps=20686.2, ups=0.32, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.641, loss_scale=16, train_wall=292, gb_free=9.6, wall=8102
2022-03-15 18:53:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:54:41 | INFO | train_inner | epoch 007:    267 / 392 loss=8.155, ppl=284.97, wps=20652.7, ups=0.32, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.644, loss_scale=16, train_wall=292, gb_free=9.6, wall=8419
2022-03-15 18:59:52 | INFO | train_inner | epoch 007:    367 / 392 loss=8.081, ppl=270.72, wps=21118.1, ups=0.32, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.624, loss_scale=16, train_wall=286, gb_free=9.6, wall=8729
2022-03-15 19:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:01:46 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.874 | ppl 234.63 | wps 32884.2 | wpb 511.9 | bsz 1 | num_updates 2725 | best_loss 7.874
2022-03-15 19:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2725 updates
2022-03-15 19:01:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 19:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 19:01:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 7 @ 2725 updates, score 7.874) (writing took 2.5761925419792533 seconds)
2022-03-15 19:01:49 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 19:01:49 | INFO | train | epoch 007 | loss 8.171 | ppl 288.23 | wps 20181 | ups 0.31 | wpb 65405.2 | bsz 127.7 | num_updates 2725 | lr 0.000340657 | gnorm 0.643 | loss_scale 32 | train_wall 1125 | gb_free 9.6 | wall 8847
KL Stats: Epoch 7 Divergences: Uniform: 3.247746221421864 Unigram: 2.3818201597083233
2022-03-15 19:01:49 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 19:01:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:05:43 | INFO | train_inner | epoch 008:     75 / 392 loss=7.979, ppl=252.24, wps=18519.2, ups=0.28, wpb=65025.8, bsz=127, num_updates=2800, lr=0.00035003, gnorm=0.629, loss_scale=32, train_wall=284, gb_free=9.6, wall=9081
2022-03-15 19:08:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:11:04 | INFO | train_inner | epoch 008:    176 / 392 loss=7.924, ppl=242.91, wps=20372.1, ups=0.31, wpb=65532.7, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.62, loss_scale=32, train_wall=296, gb_free=9.6, wall=9402
2022-03-15 19:13:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:16:35 | INFO | train_inner | epoch 008:    277 / 392 loss=7.86, ppl=232.33, wps=19812.1, ups=0.3, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.613, loss_scale=16, train_wall=305, gb_free=9.6, wall=9733
2022-03-15 19:20:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:22:05 | INFO | train_inner | epoch 008:    378 / 392 loss=7.813, ppl=224.9, wps=19857.4, ups=0.3, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.604, loss_scale=16, train_wall=304, gb_free=9.6, wall=10063
2022-03-15 19:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:23:29 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.613 | ppl 195.83 | wps 31673.3 | wpb 511.9 | bsz 1 | num_updates 3114 | best_loss 7.613
2022-03-15 19:23:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3114 updates
2022-03-15 19:23:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 19:23:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 19:23:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 8 @ 3114 updates, score 7.613) (writing took 2.6304473659256473 seconds)
2022-03-15 19:23:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 19:23:32 | INFO | train | epoch 008 | loss 7.881 | ppl 235.65 | wps 19524.4 | ups 0.3 | wpb 65404.8 | bsz 127.7 | num_updates 3114 | lr 0.000389272 | gnorm 0.614 | loss_scale 16 | train_wall 1161 | gb_free 9.6 | wall 10150
KL Stats: Epoch 8 Divergences: Uniform: 3.4142948509799704 Unigram: 2.4985561257598636
2022-03-15 19:23:32 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 19:23:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:28:15 | INFO | train_inner | epoch 009:     86 / 392 loss=7.696, ppl=207.39, wps=17579.8, ups=0.27, wpb=65029.1, bsz=127, num_updates=3200, lr=0.00040002, gnorm=0.615, loss_scale=16, train_wall=301, gb_free=9.6, wall=10433
2022-03-15 19:31:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:33:44 | INFO | train_inner | epoch 009:    187 / 392 loss=7.662, ppl=202.53, wps=19929.7, ups=0.3, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.601, loss_scale=16, train_wall=303, gb_free=9.6, wall=10762
2022-03-15 19:38:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:39:16 | INFO | train_inner | epoch 009:    288 / 392 loss=7.613, ppl=195.82, wps=19736.1, ups=0.3, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.597, loss_scale=16, train_wall=306, gb_free=9.6, wall=11094
2022-03-15 19:44:47 | INFO | train_inner | epoch 009:    388 / 392 loss=7.555, ppl=188.04, wps=19802.4, ups=0.3, wpb=65532.7, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.595, loss_scale=16, train_wall=305, gb_free=9.6, wall=11425
2022-03-15 19:44:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:45:38 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.361 | ppl 164.36 | wps 31817.6 | wpb 511.9 | bsz 1 | num_updates 3504 | best_loss 7.361
2022-03-15 19:45:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3504 updates
2022-03-15 19:45:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 19:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 19:45:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 9 @ 3504 updates, score 7.361) (writing took 2.3898970389273018 seconds)
2022-03-15 19:45:41 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 19:45:41 | INFO | train | epoch 009 | loss 7.626 | ppl 197.56 | wps 19195.6 | ups 0.29 | wpb 65405.2 | bsz 127.7 | num_updates 3504 | lr 0.000438012 | gnorm 0.602 | loss_scale 16 | train_wall 1185 | gb_free 9.6 | wall 11479
KL Stats: Epoch 9 Divergences: Uniform: 3.5394285432382864 Unigram: 2.5929699844542067
2022-03-15 19:45:41 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 19:45:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:46:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:50:56 | INFO | train_inner | epoch 010:     97 / 392 loss=7.44, ppl=173.65, wps=17626.7, ups=0.27, wpb=65029.1, bsz=127, num_updates=3600, lr=0.00045001, gnorm=0.604, loss_scale=16, train_wall=300, gb_free=9.6, wall=11794
2022-03-15 19:56:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:56:24 | INFO | train_inner | epoch 010:    198 / 392 loss=7.411, ppl=170.18, wps=19999.6, ups=0.31, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.587, loss_scale=16, train_wall=302, gb_free=9.6, wall=12121
2022-03-15 20:01:50 | INFO | train_inner | epoch 010:    298 / 392 loss=7.37, ppl=165.41, wps=20062.4, ups=0.31, wpb=65532.7, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.596, loss_scale=16, train_wall=301, gb_free=9.6, wall=12448
2022-03-15 20:03:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:07:36 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.153 | ppl 142.35 | wps 31426.6 | wpb 511.9 | bsz 1 | num_updates 3893 | best_loss 7.153
2022-03-15 20:07:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3893 updates
2022-03-15 20:07:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 20:07:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 20:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 10 @ 3893 updates, score 7.153) (writing took 3.540184176992625 seconds)
2022-03-15 20:07:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 20:07:40 | INFO | train | epoch 010 | loss 7.388 | ppl 167.47 | wps 19290.3 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 3893 | lr 0.000486628 | gnorm 0.595 | loss_scale 16 | train_wall 1175 | gb_free 9.6 | wall 12797
KL Stats: Epoch 10 Divergences: Uniform: 3.6447402037686536 Unigram: 2.6750045527990522
2022-03-15 20:07:40 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 20:07:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:08:03 | INFO | train_inner | epoch 011:      7 / 392 loss=7.326, ppl=160.45, wps=17457.6, ups=0.27, wpb=65029.1, bsz=127, num_updates=3900, lr=0.000487503, gnorm=0.601, loss_scale=16, train_wall=302, gb_free=9.6, wall=12821
2022-03-15 20:11:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:13:35 | INFO | train_inner | epoch 011:    108 / 392 loss=7.224, ppl=149.47, wps=19710.2, ups=0.3, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.587, loss_scale=16, train_wall=307, gb_free=9.6, wall=13153
2022-03-15 20:18:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:19:07 | INFO | train_inner | epoch 011:    209 / 392 loss=7.195, ppl=146.54, wps=19729.9, ups=0.3, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.581, loss_scale=16, train_wall=306, gb_free=9.6, wall=13485
2022-03-15 20:24:35 | INFO | train_inner | epoch 011:    309 / 392 loss=7.176, ppl=144.61, wps=19986.1, ups=0.3, wpb=65532.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.583, loss_scale=16, train_wall=302, gb_free=9.6, wall=13813
2022-03-15 20:26:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:29:50 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.996 | ppl 127.66 | wps 31261.3 | wpb 511.9 | bsz 1 | num_updates 4282 | best_loss 6.996
2022-03-15 20:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4282 updates
2022-03-15 20:29:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 20:29:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 20:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 11 @ 4282 updates, score 6.996) (writing took 2.3713817049283534 seconds)
2022-03-15 20:29:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 20:29:52 | INFO | train | epoch 011 | loss 7.187 | ppl 145.72 | wps 19089.6 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 4282 | lr 0.000483255 | gnorm 0.585 | loss_scale 16 | train_wall 1189 | gb_free 9.6 | wall 14130
KL Stats: Epoch 11 Divergences: Uniform: 3.723390159319833 Unigram: 2.7386615537210885
2022-03-15 20:29:52 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 20:29:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:30:53 | INFO | train_inner | epoch 012:     18 / 392 loss=7.127, ppl=139.78, wps=17226.1, ups=0.26, wpb=65029.1, bsz=127, num_updates=4300, lr=0.000482243, gnorm=0.582, loss_scale=16, train_wall=308, gb_free=9.6, wall=14191
2022-03-15 20:36:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:36:29 | INFO | train_inner | epoch 012:    119 / 392 loss=7.048, ppl=132.34, wps=19514.5, ups=0.3, wpb=65532.7, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.558, loss_scale=16, train_wall=310, gb_free=9.6, wall=14526
2022-03-15 20:42:01 | INFO | train_inner | epoch 012:    219 / 392 loss=7.036, ppl=131.22, wps=19707.6, ups=0.3, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.576, loss_scale=16, train_wall=307, gb_free=9.6, wall=14859
2022-03-15 20:44:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:47:35 | INFO | train_inner | epoch 012:    320 / 392 loss=7.008, ppl=128.72, wps=19646.3, ups=0.3, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.556, loss_scale=16, train_wall=308, gb_free=9.6, wall=15193
2022-03-15 20:51:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:52:09 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.872 | ppl 117.17 | wps 31451.9 | wpb 511.9 | bsz 1 | num_updates 4672 | best_loss 6.872
2022-03-15 20:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4672 updates
2022-03-15 20:52:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 20:52:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 20:52:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 12 @ 4672 updates, score 6.872) (writing took 2.405767117976211 seconds)
2022-03-15 20:52:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 20:52:11 | INFO | train | epoch 012 | loss 7.026 | ppl 130.32 | wps 19047.7 | ups 0.29 | wpb 65405.2 | bsz 127.7 | num_updates 4672 | lr 0.000462646 | gnorm 0.566 | loss_scale 32 | train_wall 1195 | gb_free 9.6 | wall 15469
KL Stats: Epoch 12 Divergences: Uniform: 3.7908926090521566 Unigram: 2.789423009078106
2022-03-15 20:52:12 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 20:52:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:52:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:53:47 | INFO | train_inner | epoch 013:     29 / 392 loss=6.971, ppl=125.46, wps=17445.4, ups=0.27, wpb=65029.1, bsz=127, num_updates=4700, lr=0.000461266, gnorm=0.572, loss_scale=16, train_wall=303, gb_free=9.6, wall=15565
2022-03-15 20:59:17 | INFO | train_inner | epoch 013:    129 / 392 loss=6.92, ppl=121.12, wps=19855.7, ups=0.3, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.549, loss_scale=16, train_wall=304, gb_free=9.6, wall=15895
2022-03-15 21:01:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:04:51 | INFO | train_inner | epoch 013:    230 / 392 loss=6.903, ppl=119.72, wps=19652.8, ups=0.3, wpb=65532.7, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.537, loss_scale=16, train_wall=308, gb_free=9.6, wall=16229
2022-03-15 21:09:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:10:24 | INFO | train_inner | epoch 013:    331 / 392 loss=6.9, ppl=119.43, wps=19648.4, ups=0.3, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.555, loss_scale=16, train_wall=308, gb_free=9.6, wall=16562
2022-03-15 21:13:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:14:24 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.797 | ppl 111.18 | wps 31426.8 | wpb 511.9 | bsz 1 | num_updates 5061 | best_loss 6.797
2022-03-15 21:14:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5061 updates
2022-03-15 21:14:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 21:14:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 21:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 13 @ 5061 updates, score 6.797) (writing took 2.346957854926586 seconds)
2022-03-15 21:14:27 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 21:14:27 | INFO | train | epoch 013 | loss 6.903 | ppl 119.71 | wps 19055.5 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 5061 | lr 0.00044451 | gnorm 0.549 | loss_scale 16 | train_wall 1191 | gb_free 9.6 | wall 16805
KL Stats: Epoch 13 Divergences: Uniform: 3.8408307812411597 Unigram: 2.830907043839417
2022-03-15 21:14:27 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 21:14:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:16:36 | INFO | train_inner | epoch 014:     39 / 392 loss=6.848, ppl=115.21, wps=17521.7, ups=0.27, wpb=65025.8, bsz=127, num_updates=5100, lr=0.000442807, gnorm=0.547, loss_scale=16, train_wall=302, gb_free=9.6, wall=16933
2022-03-15 21:17:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:22:08 | INFO | train_inner | epoch 014:    140 / 392 loss=6.799, ppl=111.35, wps=19692.7, ups=0.3, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.542, loss_scale=16, train_wall=307, gb_free=9.6, wall=17266
2022-03-15 21:24:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:27:43 | INFO | train_inner | epoch 014:    241 / 392 loss=6.814, ppl=112.48, wps=19601, ups=0.3, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.543, loss_scale=16, train_wall=308, gb_free=9.6, wall=17601
2022-03-15 21:32:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:33:19 | INFO | train_inner | epoch 014:    342 / 392 loss=6.81, ppl=112.2, wps=19477.5, ups=0.3, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.539, loss_scale=16, train_wall=310, gb_free=9.6, wall=17937
2022-03-15 21:36:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:36:44 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.719 | ppl 105.32 | wps 31396.6 | wpb 511.9 | bsz 1 | num_updates 5450 | best_loss 6.719
2022-03-15 21:36:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5450 updates
2022-03-15 21:36:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 21:36:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 21:36:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 14 @ 5450 updates, score 6.719) (writing took 2.299410120001994 seconds)
2022-03-15 21:36:46 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 21:36:46 | INFO | train | epoch 014 | loss 6.806 | ppl 111.86 | wps 18992.2 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 5450 | lr 0.000428353 | gnorm 0.543 | loss_scale 16 | train_wall 1196 | gb_free 9.6 | wall 18144
KL Stats: Epoch 14 Divergences: Uniform: 3.880281710731016 Unigram: 2.862521548291334
2022-03-15 21:36:46 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 21:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:39:32 | INFO | train_inner | epoch 015:     50 / 392 loss=6.758, ppl=108.23, wps=17454.3, ups=0.27, wpb=65029.1, bsz=127, num_updates=5500, lr=0.000426401, gnorm=0.544, loss_scale=16, train_wall=304, gb_free=9.6, wall=18310
2022-03-15 21:41:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:45:08 | INFO | train_inner | epoch 015:    151 / 392 loss=6.731, ppl=106.19, wps=19510.6, ups=0.3, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.529, loss_scale=16, train_wall=310, gb_free=9.6, wall=18646
2022-03-15 21:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:50:44 | INFO | train_inner | epoch 015:    252 / 392 loss=6.729, ppl=106.09, wps=19471.9, ups=0.3, wpb=65532.7, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.533, loss_scale=16, train_wall=311, gb_free=9.6, wall=18982
2022-03-15 21:56:16 | INFO | train_inner | epoch 015:    352 / 392 loss=6.725, ppl=105.79, wps=19766.9, ups=0.3, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.538, loss_scale=32, train_wall=306, gb_free=9.6, wall=19314
2022-03-15 21:56:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:58:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:59:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.676 | ppl 102.25 | wps 31391 | wpb 511.9 | bsz 1 | num_updates 5839 | best_loss 6.676
2022-03-15 21:59:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5839 updates
2022-03-15 21:59:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 21:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 21:59:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 15 @ 5839 updates, score 6.676) (writing took 2.3745858049951494 seconds)
2022-03-15 21:59:09 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 21:59:09 | INFO | train | epoch 015 | loss 6.725 | ppl 105.8 | wps 18948 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 5839 | lr 0.000413838 | gnorm 0.536 | loss_scale 16 | train_wall 1199 | gb_free 9.6 | wall 19487
KL Stats: Epoch 15 Divergences: Uniform: 3.9156105608705936 Unigram: 2.8943965076394274
2022-03-15 21:59:09 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 21:59:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:02:31 | INFO | train_inner | epoch 016:     61 / 392 loss=6.674, ppl=102.09, wps=17334.3, ups=0.27, wpb=65029.1, bsz=127, num_updates=5900, lr=0.000411693, gnorm=0.545, loss_scale=16, train_wall=306, gb_free=9.6, wall=19689
2022-03-15 22:04:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:08:03 | INFO | train_inner | epoch 016:    162 / 392 loss=6.659, ppl=101.06, wps=19711.9, ups=0.3, wpb=65532.7, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.538, loss_scale=16, train_wall=307, gb_free=9.6, wall=20021
2022-03-15 22:12:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:13:38 | INFO | train_inner | epoch 016:    263 / 392 loss=6.661, ppl=101.21, wps=19558.4, ups=0.3, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.545, loss_scale=16, train_wall=309, gb_free=9.6, wall=20356
2022-03-15 22:19:11 | INFO | train_inner | epoch 016:    363 / 392 loss=6.653, ppl=100.66, wps=19694.7, ups=0.3, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.534, loss_scale=32, train_wall=307, gb_free=9.6, wall=20689
2022-03-15 22:19:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:20:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:21:25 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.623 | ppl 98.55 | wps 31330.8 | wpb 511.9 | bsz 1 | num_updates 6228 | best_loss 6.623
2022-03-15 22:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6228 updates
2022-03-15 22:21:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 22:21:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 22:21:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 16 @ 6228 updates, score 6.623) (writing took 2.454512858064845 seconds)
2022-03-15 22:21:28 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 22:21:28 | INFO | train | epoch 016 | loss 6.658 | ppl 100.96 | wps 19005.5 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 6228 | lr 0.000400706 | gnorm 0.539 | loss_scale 16 | train_wall 1194 | gb_free 9.6 | wall 20826
KL Stats: Epoch 16 Divergences: Uniform: 3.9462886821020797 Unigram: 2.9179487377079814
2022-03-15 22:21:28 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 22:21:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:25:27 | INFO | train_inner | epoch 017:     72 / 392 loss=6.611, ppl=97.76, wps=17308.1, ups=0.27, wpb=65025.8, bsz=127, num_updates=6300, lr=0.00039841, gnorm=0.529, loss_scale=16, train_wall=306, gb_free=9.6, wall=21065
2022-03-15 22:27:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:31:02 | INFO | train_inner | epoch 017:    173 / 392 loss=6.598, ppl=96.89, wps=19565, ups=0.3, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.543, loss_scale=16, train_wall=309, gb_free=9.6, wall=21400
2022-03-15 22:35:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:36:34 | INFO | train_inner | epoch 017:    274 / 392 loss=6.603, ppl=97.19, wps=19730.7, ups=0.3, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.532, loss_scale=16, train_wall=306, gb_free=9.6, wall=21732
2022-03-15 22:42:05 | INFO | train_inner | epoch 017:    374 / 392 loss=6.604, ppl=97.25, wps=19773.4, ups=0.3, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.529, loss_scale=16, train_wall=306, gb_free=9.6, wall=22063
2022-03-15 22:42:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:43:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:43:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.574 | ppl 95.29 | wps 31130.1 | wpb 511.9 | bsz 1 | num_updates 6617 | best_loss 6.574
2022-03-15 22:43:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6617 updates
2022-03-15 22:43:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 22:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 22:43:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 17 @ 6617 updates, score 6.574) (writing took 2.3888271319447085 seconds)
2022-03-15 22:43:47 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 22:43:47 | INFO | train | epoch 017 | loss 6.599 | ppl 96.95 | wps 18998.4 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 6617 | lr 0.000388749 | gnorm 0.534 | loss_scale 16 | train_wall 1194 | gb_free 9.6 | wall 22165
KL Stats: Epoch 17 Divergences: Uniform: 3.9716618723813815 Unigram: 2.938252329557307
2022-03-15 22:43:47 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 22:43:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:48:24 | INFO | train_inner | epoch 018:     83 / 392 loss=6.54, ppl=93.08, wps=17184.4, ups=0.26, wpb=65029.1, bsz=127, num_updates=6700, lr=0.000386334, gnorm=0.539, loss_scale=16, train_wall=308, gb_free=9.6, wall=22442
2022-03-15 22:50:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:53:59 | INFO | train_inner | epoch 018:    184 / 392 loss=6.549, ppl=93.64, wps=19578, ups=0.3, wpb=65532.7, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.527, loss_scale=16, train_wall=309, gb_free=9.6, wall=22777
2022-03-15 22:57:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:59:35 | INFO | train_inner | epoch 018:    285 / 392 loss=6.554, ppl=93.95, wps=19510.4, ups=0.3, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.529, loss_scale=16, train_wall=310, gb_free=9.6, wall=23112
2022-03-15 23:05:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:05:06 | INFO | train_inner | epoch 018:    386 / 392 loss=6.557, ppl=94.14, wps=19741.8, ups=0.3, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.533, loss_scale=16, train_wall=306, gb_free=9.6, wall=23444
2022-03-15 23:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:06:05 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.549 | ppl 93.64 | wps 31356 | wpb 511.9 | bsz 1 | num_updates 7006 | best_loss 6.549
2022-03-15 23:06:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7006 updates
2022-03-15 23:06:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 23:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 23:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 18 @ 7006 updates, score 6.549) (writing took 2.3984965858981013 seconds)
2022-03-15 23:06:07 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 23:06:07 | INFO | train | epoch 018 | loss 6.548 | ppl 93.58 | wps 18980.2 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 7006 | lr 0.000377803 | gnorm 0.532 | loss_scale 16 | train_wall 1196 | gb_free 9.6 | wall 23505
KL Stats: Epoch 18 Divergences: Uniform: 3.9927416547028662 Unigram: 2.956703486724044
2022-03-15 23:06:07 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 23:06:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:11:19 | INFO | train_inner | epoch 019:     94 / 392 loss=6.486, ppl=89.64, wps=17449.6, ups=0.27, wpb=65029.1, bsz=127, num_updates=7100, lr=0.000375293, gnorm=0.525, loss_scale=16, train_wall=303, gb_free=9.6, wall=23817
2022-03-15 23:12:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:16:52 | INFO | train_inner | epoch 019:    195 / 392 loss=6.504, ppl=90.79, wps=19678.7, ups=0.3, wpb=65532.7, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.534, loss_scale=16, train_wall=307, gb_free=9.6, wall=24150
2022-03-15 23:20:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:22:21 | INFO | train_inner | epoch 019:    296 / 392 loss=6.51, ppl=91.13, wps=19905.9, ups=0.3, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.536, loss_scale=16, train_wall=304, gb_free=9.6, wall=24479
2022-03-15 23:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:28:16 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.52 | ppl 91.75 | wps 31316.2 | wpb 511.9 | bsz 1 | num_updates 7396 | best_loss 6.52
2022-03-15 23:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7396 updates
2022-03-15 23:28:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 23:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 23:28:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 19 @ 7396 updates, score 6.52) (writing took 2.3246932280017063 seconds)
2022-03-15 23:28:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-15 23:28:18 | INFO | train | epoch 019 | loss 6.503 | ppl 90.69 | wps 19166.4 | ups 0.29 | wpb 65405.2 | bsz 127.7 | num_updates 7396 | lr 0.000367707 | gnorm 0.53 | loss_scale 32 | train_wall 1187 | gb_free 9.6 | wall 24836
KL Stats: Epoch 19 Divergences: Uniform: 4.012942364990897 Unigram: 2.973521460661906
2022-03-15 23:28:18 | INFO | fairseq.trainer | begin training epoch 20
2022-03-15 23:28:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:28:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:28:35 | INFO | train_inner | epoch 020:      5 / 392 loss=6.51, ppl=91.16, wps=17405.6, ups=0.27, wpb=65029.1, bsz=127, num_updates=7400, lr=0.000367607, gnorm=0.527, loss_scale=16, train_wall=304, gb_free=9.6, wall=24853
2022-03-15 23:34:03 | INFO | train_inner | epoch 020:    105 / 392 loss=6.455, ppl=87.75, wps=19985.9, ups=0.3, wpb=65532.7, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.523, loss_scale=16, train_wall=302, gb_free=9.6, wall=25181
2022-03-15 23:35:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:39:36 | INFO | train_inner | epoch 020:    206 / 392 loss=6.461, ppl=88.12, wps=19663.5, ups=0.3, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.539, loss_scale=16, train_wall=307, gb_free=9.6, wall=25514
2022-03-15 23:42:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:45:08 | INFO | train_inner | epoch 020:    307 / 392 loss=6.469, ppl=88.58, wps=19747.7, ups=0.3, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.528, loss_scale=16, train_wall=306, gb_free=9.6, wall=25846
2022-03-15 23:49:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:50:24 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.487 | ppl 89.73 | wps 31495.3 | wpb 511.9 | bsz 1 | num_updates 7785 | best_loss 6.487
2022-03-15 23:50:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7785 updates
2022-03-15 23:50:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 23:50:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-15 23:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 20 @ 7785 updates, score 6.487) (writing took 2.365609303000383 seconds)
2022-03-15 23:50:26 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-15 23:50:26 | INFO | train | epoch 020 | loss 6.463 | ppl 88.24 | wps 19160.9 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 7785 | lr 0.000358402 | gnorm 0.53 | loss_scale 32 | train_wall 1184 | gb_free 9.6 | wall 26164
KL Stats: Epoch 20 Divergences: Uniform: 4.033137665853904 Unigram: 2.988575219950979
2022-03-15 23:50:26 | INFO | fairseq.trainer | begin training epoch 21
2022-03-15 23:50:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:50:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:51:19 | INFO | train_inner | epoch 021:     16 / 392 loss=6.461, ppl=88.08, wps=17534.2, ups=0.27, wpb=65029.1, bsz=127, num_updates=7800, lr=0.000358057, gnorm=0.532, loss_scale=16, train_wall=302, gb_free=9.6, wall=26217
2022-03-15 23:56:47 | INFO | train_inner | epoch 021:    116 / 392 loss=6.417, ppl=85.46, wps=19983.2, ups=0.3, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.526, loss_scale=16, train_wall=302, gb_free=9.6, wall=26545
2022-03-15 23:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:02:18 | INFO | train_inner | epoch 021:    217 / 392 loss=6.417, ppl=85.47, wps=19803.8, ups=0.3, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.532, loss_scale=16, train_wall=305, gb_free=9.6, wall=26876
2022-03-16 00:06:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:07:52 | INFO | train_inner | epoch 021:    318 / 392 loss=6.435, ppl=86.54, wps=19597, ups=0.3, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.529, loss_scale=16, train_wall=308, gb_free=9.6, wall=27210
2022-03-16 00:11:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:12:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.456 | ppl 87.79 | wps 31509.2 | wpb 511.9 | bsz 1 | num_updates 8174 | best_loss 6.456
2022-03-16 00:12:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8174 updates
2022-03-16 00:12:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 00:12:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 00:12:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 21 @ 8174 updates, score 6.456) (writing took 2.6532443610485643 seconds)
2022-03-16 00:12:38 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-16 00:12:38 | INFO | train | epoch 021 | loss 6.427 | ppl 86.04 | wps 19102.2 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 8174 | lr 0.00034977 | gnorm 0.532 | loss_scale 16 | train_wall 1188 | gb_free 9.6 | wall 27496
KL Stats: Epoch 21 Divergences: Uniform: 4.049903634705283 Unigram: 3.0013479044767504
2022-03-16 00:12:38 | INFO | fairseq.trainer | begin training epoch 22
2022-03-16 00:12:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:14:03 | INFO | train_inner | epoch 022:     26 / 392 loss=6.422, ppl=85.73, wps=17537.9, ups=0.27, wpb=65025.8, bsz=127, num_updates=8200, lr=0.000349215, gnorm=0.532, loss_scale=16, train_wall=302, gb_free=9.6, wall=27581
2022-03-16 00:14:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:19:31 | INFO | train_inner | epoch 022:    127 / 392 loss=6.38, ppl=83.28, wps=19962, ups=0.3, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.523, loss_scale=16, train_wall=303, gb_free=9.6, wall=27909
2022-03-16 00:22:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:25:05 | INFO | train_inner | epoch 022:    228 / 392 loss=6.401, ppl=84.52, wps=19618.6, ups=0.3, wpb=65532.7, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.532, loss_scale=16, train_wall=308, gb_free=9.6, wall=28243
2022-03-16 00:29:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:30:38 | INFO | train_inner | epoch 022:    329 / 392 loss=6.402, ppl=84.57, wps=19694.1, ups=0.3, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.526, loss_scale=16, train_wall=307, gb_free=9.6, wall=28576
2022-03-16 00:33:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:34:28 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.44 | ppl 86.81 | wps 34042.9 | wpb 511.9 | bsz 1 | num_updates 8563 | best_loss 6.44
2022-03-16 00:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8563 updates
2022-03-16 00:34:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 00:34:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 00:34:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 22 @ 8563 updates, score 6.44) (writing took 2.3383361699525267 seconds)
2022-03-16 00:34:30 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-16 00:34:30 | INFO | train | epoch 022 | loss 6.394 | ppl 84.09 | wps 19387.6 | ups 0.3 | wpb 65404.8 | bsz 127.7 | num_updates 8563 | lr 0.000341733 | gnorm 0.527 | loss_scale 16 | train_wall 1173 | gb_free 9.6 | wall 28808
KL Stats: Epoch 22 Divergences: Uniform: 4.067439369001983 Unigram: 3.0165752871888394
2022-03-16 00:34:30 | INFO | fairseq.trainer | begin training epoch 23
2022-03-16 00:34:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:36:23 | INFO | train_inner | epoch 023:     37 / 392 loss=6.379, ppl=83.2, wps=18832.7, ups=0.29, wpb=65029.1, bsz=127, num_updates=8600, lr=0.000340997, gnorm=0.534, loss_scale=16, train_wall=280, gb_free=9.6, wall=28921
2022-03-16 00:36:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:41:30 | INFO | train_inner | epoch 023:    138 / 392 loss=6.353, ppl=81.76, wps=21391.4, ups=0.33, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.527, loss_scale=16, train_wall=281, gb_free=9.6, wall=29228
2022-03-16 00:43:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:46:31 | INFO | train_inner | epoch 023:    239 / 392 loss=6.366, ppl=82.48, wps=21755.6, ups=0.33, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.532, loss_scale=16, train_wall=277, gb_free=9.6, wall=29529
2022-03-16 00:49:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:51:37 | INFO | train_inner | epoch 023:    340 / 392 loss=6.375, ppl=83.02, wps=21404.3, ups=0.33, wpb=65532.7, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.524, loss_scale=16, train_wall=281, gb_free=9.6, wall=29835
2022-03-16 00:54:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:54:51 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.418 | ppl 85.52 | wps 33856 | wpb 511.9 | bsz 1 | num_updates 8952 | best_loss 6.418
2022-03-16 00:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 8952 updates
2022-03-16 00:54:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 00:54:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 00:54:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 23 @ 8952 updates, score 6.418) (writing took 2.3735070560360327 seconds)
2022-03-16 00:54:53 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-16 00:54:53 | INFO | train | epoch 023 | loss 6.364 | ppl 82.38 | wps 20803.6 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 8952 | lr 0.000334226 | gnorm 0.53 | loss_scale 16 | train_wall 1086 | gb_free 9.6 | wall 30031
KL Stats: Epoch 23 Divergences: Uniform: 4.081230218955464 Unigram: 3.027510497598861
2022-03-16 00:54:53 | INFO | fairseq.trainer | begin training epoch 24
2022-03-16 00:54:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:56:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:57:23 | INFO | train_inner | epoch 024:     49 / 392 loss=6.354, ppl=81.81, wps=18829.4, ups=0.29, wpb=65029.1, bsz=127, num_updates=9000, lr=0.000333333, gnorm=0.54, loss_scale=16, train_wall=280, gb_free=9.6, wall=30180
2022-03-16 01:02:21 | INFO | train_inner | epoch 024:    149 / 392 loss=6.324, ppl=80.14, wps=21924.8, ups=0.33, wpb=65532.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.539, loss_scale=16, train_wall=274, gb_free=9.6, wall=30479
2022-03-16 01:03:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:05:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:07:33 | INFO | train_inner | epoch 024:    251 / 392 loss=6.345, ppl=81.3, wps=21014.3, ups=0.32, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.525, loss_scale=8, train_wall=287, gb_free=9.6, wall=30791
2022-03-16 01:12:32 | INFO | train_inner | epoch 024:    351 / 392 loss=6.345, ppl=81.29, wps=21934.6, ups=0.33, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.527, loss_scale=16, train_wall=274, gb_free=9.6, wall=31090
2022-03-16 01:14:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:15:08 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.406 | ppl 84.81 | wps 33607.2 | wpb 511.9 | bsz 1 | num_updates 9341 | best_loss 6.406
2022-03-16 01:15:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9341 updates
2022-03-16 01:15:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 01:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 01:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 24 @ 9341 updates, score 6.406) (writing took 2.346192194032483 seconds)
2022-03-16 01:15:11 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-16 01:15:11 | INFO | train | epoch 024 | loss 6.337 | ppl 80.84 | wps 20901.5 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 9341 | lr 0.000327192 | gnorm 0.532 | loss_scale 16 | train_wall 1080 | gb_free 9.6 | wall 31248
KL Stats: Epoch 24 Divergences: Uniform: 4.096394649036056 Unigram: 3.040026569105785
2022-03-16 01:15:11 | INFO | fairseq.trainer | begin training epoch 25
2022-03-16 01:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:18:11 | INFO | train_inner | epoch 025:     59 / 392 loss=6.308, ppl=79.23, wps=19195.3, ups=0.3, wpb=65025.8, bsz=127, num_updates=9400, lr=0.000326164, gnorm=0.532, loss_scale=16, train_wall=273, gb_free=9.6, wall=31429
2022-03-16 01:19:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:23:18 | INFO | train_inner | epoch 025:    160 / 392 loss=6.305, ppl=79.07, wps=21303.5, ups=0.33, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.531, loss_scale=16, train_wall=283, gb_free=9.6, wall=31736
2022-03-16 01:23:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:28:17 | INFO | train_inner | epoch 025:    261 / 392 loss=6.319, ppl=79.83, wps=21957.2, ups=0.34, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.532, loss_scale=8, train_wall=274, gb_free=9.6, wall=32035
2022-03-16 01:33:21 | INFO | train_inner | epoch 025:    361 / 392 loss=6.322, ppl=79.98, wps=21519.3, ups=0.33, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.531, loss_scale=16, train_wall=280, gb_free=9.6, wall=32339
2022-03-16 01:34:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:35:32 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.392 | ppl 83.98 | wps 33650.9 | wpb 511.9 | bsz 1 | num_updates 9731 | best_loss 6.392
2022-03-16 01:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9731 updates
2022-03-16 01:35:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 01:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 01:35:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 25 @ 9731 updates, score 6.392) (writing took 2.3583575470838696 seconds)
2022-03-16 01:35:34 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-16 01:35:34 | INFO | train | epoch 025 | loss 6.311 | ppl 79.42 | wps 20845.2 | ups 0.32 | wpb 65405.2 | bsz 127.7 | num_updates 9731 | lr 0.000320569 | gnorm 0.531 | loss_scale 16 | train_wall 1086 | gb_free 9.6 | wall 32472
KL Stats: Epoch 25 Divergences: Uniform: 4.1089766814766016 Unigram: 3.0500073892812214
2022-03-16 01:35:34 | INFO | fairseq.trainer | begin training epoch 26
2022-03-16 01:35:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:37:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:39:08 | INFO | train_inner | epoch 026:     70 / 392 loss=6.284, ppl=77.93, wps=18759.4, ups=0.29, wpb=65029.1, bsz=127, num_updates=9800, lr=0.000319438, gnorm=0.53, loss_scale=16, train_wall=281, gb_free=9.6, wall=32686
2022-03-16 01:43:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:43:48 | INFO | train_inner | epoch 026:    171 / 392 loss=6.283, ppl=77.89, wps=23449, ups=0.36, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.521, loss_scale=16, train_wall=255, gb_free=9.6, wall=32966
2022-03-16 01:48:12 | INFO | train_inner | epoch 026:    271 / 392 loss=6.292, ppl=78.33, wps=24772.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.525, loss_scale=16, train_wall=241, gb_free=9.6, wall=33230
2022-03-16 01:49:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:52:39 | INFO | train_inner | epoch 026:    372 / 392 loss=6.302, ppl=78.89, wps=24542.1, ups=0.37, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.533, loss_scale=16, train_wall=243, gb_free=9.6, wall=33497
2022-03-16 01:53:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:54:05 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.368 | ppl 82.6 | wps 36863.4 | wpb 511.9 | bsz 1 | num_updates 10120 | best_loss 6.368
2022-03-16 01:54:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10120 updates
2022-03-16 01:54:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 01:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 01:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 26 @ 10120 updates, score 6.368) (writing took 2.2874700620304793 seconds)
2022-03-16 01:54:07 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 01:54:07 | INFO | train | epoch 026 | loss 6.288 | ppl 78.12 | wps 22857.5 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 10120 | lr 0.000314347 | gnorm 0.527 | loss_scale 16 | train_wall 982 | gb_free 9.6 | wall 33585
KL Stats: Epoch 26 Divergences: Uniform: 4.121908558573278 Unigram: 3.058994189864278
2022-03-16 01:54:07 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 01:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:55:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:57:42 | INFO | train_inner | epoch 027:     81 / 392 loss=6.256, ppl=76.44, wps=21466.5, ups=0.33, wpb=65025.8, bsz=127, num_updates=10200, lr=0.000313112, gnorm=0.541, loss_scale=16, train_wall=242, gb_free=9.6, wall=33800
2022-03-16 02:01:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:02:09 | INFO | train_inner | epoch 027:    182 / 392 loss=6.257, ppl=76.46, wps=24543.6, ups=0.37, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.526, loss_scale=16, train_wall=243, gb_free=9.6, wall=34067
2022-03-16 02:06:34 | INFO | train_inner | epoch 027:    282 / 392 loss=6.276, ppl=77.5, wps=24767, ups=0.38, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=34332
2022-03-16 02:06:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:11:01 | INFO | train_inner | epoch 027:    383 / 392 loss=6.285, ppl=78, wps=24520.9, ups=0.37, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.534, loss_scale=8, train_wall=243, gb_free=9.6, wall=34599
2022-03-16 02:11:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:11:58 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.353 | ppl 81.74 | wps 36880 | wpb 511.9 | bsz 1 | num_updates 10509 | best_loss 6.353
2022-03-16 02:11:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10509 updates
2022-03-16 02:11:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 02:11:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 02:12:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 27 @ 10509 updates, score 6.353) (writing took 2.2782999200280756 seconds)
2022-03-16 02:12:00 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 02:12:00 | INFO | train | epoch 027 | loss 6.266 | ppl 76.98 | wps 23717 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 10509 | lr 0.000308475 | gnorm 0.533 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 34658
KL Stats: Epoch 27 Divergences: Uniform: 4.131602412095142 Unigram: 3.069047819774675
2022-03-16 02:12:00 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 02:12:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:16:01 | INFO | train_inner | epoch 028:     91 / 392 loss=6.226, ppl=74.87, wps=21694.5, ups=0.33, wpb=65025.8, bsz=127, num_updates=10600, lr=0.000307148, gnorm=0.539, loss_scale=16, train_wall=239, gb_free=9.6, wall=34899
2022-03-16 02:18:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:20:28 | INFO | train_inner | epoch 028:    192 / 392 loss=6.239, ppl=75.55, wps=24551.3, ups=0.37, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.535, loss_scale=16, train_wall=243, gb_free=9.6, wall=35166
2022-03-16 02:24:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:24:54 | INFO | train_inner | epoch 028:    293 / 392 loss=6.257, ppl=76.5, wps=24563.7, ups=0.37, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.526, loss_scale=16, train_wall=243, gb_free=9.6, wall=35432
2022-03-16 02:29:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:29:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.351 | ppl 81.6 | wps 36933 | wpb 511.9 | bsz 1 | num_updates 10899 | best_loss 6.351
2022-03-16 02:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10899 updates
2022-03-16 02:29:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 02:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 02:29:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 28 @ 10899 updates, score 6.351) (writing took 2.251727184979245 seconds)
2022-03-16 02:29:51 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 02:29:51 | INFO | train | epoch 028 | loss 6.246 | ppl 75.88 | wps 23812.8 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 10899 | lr 0.000302905 | gnorm 0.534 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 35729
KL Stats: Epoch 28 Divergences: Uniform: 4.144884065350257 Unigram: 3.0769769223484493
2022-03-16 02:29:51 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 02:29:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:29:54 | INFO | train_inner | epoch 029:      1 / 392 loss=6.26, ppl=76.64, wps=21706.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=10900, lr=0.000302891, gnorm=0.541, loss_scale=16, train_wall=239, gb_free=9.6, wall=35732
2022-03-16 02:30:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:34:21 | INFO | train_inner | epoch 029:    102 / 392 loss=6.2, ppl=73.52, wps=24530.6, ups=0.37, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.527, loss_scale=16, train_wall=243, gb_free=9.6, wall=35999
2022-03-16 02:36:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:38:48 | INFO | train_inner | epoch 029:    203 / 392 loss=6.221, ppl=74.6, wps=24552.8, ups=0.37, wpb=65532.7, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.537, loss_scale=16, train_wall=243, gb_free=9.6, wall=36266
2022-03-16 02:42:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:43:15 | INFO | train_inner | epoch 029:    304 / 392 loss=6.235, ppl=75.33, wps=24539.6, ups=0.37, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.536, loss_scale=16, train_wall=243, gb_free=9.6, wall=36533
2022-03-16 02:47:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:47:41 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.324 | ppl 80.14 | wps 36910.6 | wpb 511.9 | bsz 1 | num_updates 11288 | best_loss 6.324
2022-03-16 02:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11288 updates
2022-03-16 02:47:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 02:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 02:47:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 29 @ 11288 updates, score 6.324) (writing took 2.2463792508933693 seconds)
2022-03-16 02:47:43 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 02:47:43 | INFO | train | epoch 029 | loss 6.226 | ppl 74.85 | wps 23736.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 11288 | lr 0.00029764 | gnorm 0.534 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 36801
KL Stats: Epoch 29 Divergences: Uniform: 4.1536523144661714 Unigram: 3.084249765834869
2022-03-16 02:47:43 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 02:47:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:48:15 | INFO | train_inner | epoch 030:     12 / 392 loss=6.243, ppl=75.75, wps=21681.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=11300, lr=0.000297482, gnorm=0.538, loss_scale=16, train_wall=239, gb_free=9.6, wall=36833
2022-03-16 02:48:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:52:42 | INFO | train_inner | epoch 030:    113 / 392 loss=6.193, ppl=73.14, wps=24509.4, ups=0.37, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.527, loss_scale=16, train_wall=243, gb_free=9.6, wall=37100
2022-03-16 02:54:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:55:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:57:12 | INFO | train_inner | epoch 030:    215 / 392 loss=6.202, ppl=73.64, wps=24279.1, ups=0.37, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.538, loss_scale=8, train_wall=246, gb_free=9.6, wall=37370
2022-03-16 03:01:37 | INFO | train_inner | epoch 030:    315 / 392 loss=6.219, ppl=74.47, wps=24767.4, ups=0.38, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.524, loss_scale=16, train_wall=241, gb_free=9.6, wall=37635
2022-03-16 03:04:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:05:33 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.317 | ppl 79.72 | wps 36871.4 | wpb 511.9 | bsz 1 | num_updates 11677 | best_loss 6.317
2022-03-16 03:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11677 updates
2022-03-16 03:05:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:05:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:05:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 30 @ 11677 updates, score 6.317) (writing took 2.2647069100057706 seconds)
2022-03-16 03:05:36 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 03:05:36 | INFO | train | epoch 030 | loss 6.207 | ppl 73.89 | wps 23720.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 11677 | lr 0.00029264 | gnorm 0.53 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 37874
KL Stats: Epoch 30 Divergences: Uniform: 4.163952567061448 Unigram: 3.092649072687085
2022-03-16 03:05:36 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 03:05:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:06:37 | INFO | train_inner | epoch 031:     23 / 392 loss=6.205, ppl=73.79, wps=21687.4, ups=0.33, wpb=65025.8, bsz=127, num_updates=11700, lr=0.000292353, gnorm=0.537, loss_scale=16, train_wall=239, gb_free=9.6, wall=37935
2022-03-16 03:07:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:11:04 | INFO | train_inner | epoch 031:    124 / 392 loss=6.167, ppl=71.86, wps=24523.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.532, loss_scale=16, train_wall=243, gb_free=9.6, wall=38202
2022-03-16 03:13:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:14:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:15:34 | INFO | train_inner | epoch 031:    226 / 392 loss=6.206, ppl=73.82, wps=24271.4, ups=0.37, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.532, loss_scale=8, train_wall=246, gb_free=9.6, wall=38472
2022-03-16 03:19:59 | INFO | train_inner | epoch 031:    326 / 392 loss=6.197, ppl=73.35, wps=24775, ups=0.38, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.536, loss_scale=16, train_wall=241, gb_free=9.6, wall=38737
2022-03-16 03:22:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:22:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:23:26 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.306 | ppl 79.15 | wps 36923.9 | wpb 511.9 | bsz 1 | num_updates 12065 | best_loss 6.306
2022-03-16 03:23:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12065 updates
2022-03-16 03:23:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:23:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 31 @ 12065 updates, score 6.306) (writing took 2.2560153279919177 seconds)
2022-03-16 03:23:29 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 03:23:29 | INFO | train | epoch 031 | loss 6.191 | ppl 73.06 | wps 23654.6 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 12065 | lr 0.000287896 | gnorm 0.536 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 38946
KL Stats: Epoch 31 Divergences: Uniform: 4.1725968458767 Unigram: 3.0991481356369186
2022-03-16 03:23:29 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 03:23:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:25:02 | INFO | train_inner | epoch 032:     35 / 392 loss=6.191, ppl=73.07, wps=21467.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=12100, lr=0.00028748, gnorm=0.538, loss_scale=8, train_wall=242, gb_free=9.6, wall=39039
2022-03-16 03:29:26 | INFO | train_inner | epoch 032:    135 / 392 loss=6.162, ppl=71.61, wps=24742.3, ups=0.38, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.54, loss_scale=16, train_wall=241, gb_free=9.6, wall=39304
2022-03-16 03:33:51 | INFO | train_inner | epoch 032:    235 / 392 loss=6.177, ppl=72.33, wps=24756.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.529, loss_scale=16, train_wall=241, gb_free=9.6, wall=39569
2022-03-16 03:34:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:38:19 | INFO | train_inner | epoch 032:    336 / 392 loss=6.179, ppl=72.48, wps=24468.9, ups=0.37, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.538, loss_scale=16, train_wall=244, gb_free=9.6, wall=39837
2022-03-16 03:40:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:40:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:41:21 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.299 | ppl 78.76 | wps 36902.1 | wpb 511.9 | bsz 1 | num_updates 12455 | best_loss 6.299
2022-03-16 03:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12455 updates
2022-03-16 03:41:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:41:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 32 @ 12455 updates, score 6.299) (writing took 2.239164710044861 seconds)
2022-03-16 03:41:23 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 03:41:23 | INFO | train | epoch 032 | loss 6.175 | ppl 72.26 | wps 23744.8 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 12455 | lr 0.000283353 | gnorm 0.536 | loss_scale 16 | train_wall 944 | gb_free 9.6 | wall 40021
KL Stats: Epoch 32 Divergences: Uniform: 4.183663543508683 Unigram: 3.1071254771753036
2022-03-16 03:41:23 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 03:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:43:23 | INFO | train_inner | epoch 033:     45 / 392 loss=6.171, ppl=72.03, wps=21400.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=12500, lr=0.000282843, gnorm=0.533, loss_scale=16, train_wall=243, gb_free=9.6, wall=40141
2022-03-16 03:46:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:47:51 | INFO | train_inner | epoch 033:    146 / 392 loss=6.145, ppl=70.75, wps=24461.1, ups=0.37, wpb=65532.7, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.528, loss_scale=16, train_wall=244, gb_free=9.6, wall=40409
2022-03-16 03:52:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:52:18 | INFO | train_inner | epoch 033:    247 / 392 loss=6.169, ppl=71.93, wps=24530.7, ups=0.37, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.539, loss_scale=16, train_wall=243, gb_free=9.6, wall=40676
2022-03-16 03:52:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:56:45 | INFO | train_inner | epoch 033:    348 / 392 loss=6.168, ppl=71.91, wps=24514.9, ups=0.37, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.539, loss_scale=8, train_wall=243, gb_free=9.6, wall=40943
2022-03-16 03:58:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:59:14 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.292 | ppl 78.34 | wps 36911.9 | wpb 511.9 | bsz 1 | num_updates 12844 | best_loss 6.292
2022-03-16 03:59:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12844 updates
2022-03-16 03:59:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:59:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 03:59:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 33 @ 12844 updates, score 6.292) (writing took 2.26742396899499 seconds)
2022-03-16 03:59:17 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 03:59:17 | INFO | train | epoch 033 | loss 6.16 | ppl 71.52 | wps 23691.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 12844 | lr 0.000279029 | gnorm 0.535 | loss_scale 16 | train_wall 944 | gb_free 9.6 | wall 41095
KL Stats: Epoch 33 Divergences: Uniform: 4.193469454780196 Unigram: 3.1147066271025565
2022-03-16 03:59:17 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 03:59:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:01:45 | INFO | train_inner | epoch 034:     56 / 392 loss=6.147, ppl=70.86, wps=21677.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=12900, lr=0.000278423, gnorm=0.544, loss_scale=16, train_wall=239, gb_free=9.6, wall=41243
2022-03-16 04:04:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:06:13 | INFO | train_inner | epoch 034:    157 / 392 loss=6.136, ppl=70.31, wps=24513.6, ups=0.37, wpb=65532.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.53, loss_scale=16, train_wall=243, gb_free=9.6, wall=41510
2022-03-16 04:10:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:10:40 | INFO | train_inner | epoch 034:    258 / 392 loss=6.152, ppl=71.09, wps=24537.5, ups=0.37, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.531, loss_scale=16, train_wall=243, gb_free=9.6, wall=41778
2022-03-16 04:11:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:15:07 | INFO | train_inner | epoch 034:    359 / 392 loss=6.158, ppl=71.42, wps=24534.7, ups=0.37, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.533, loss_scale=8, train_wall=243, gb_free=9.6, wall=42045
2022-03-16 04:16:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:17:07 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.276 | ppl 77.51 | wps 36881.3 | wpb 511.9 | bsz 1 | num_updates 13233 | best_loss 6.276
2022-03-16 04:17:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13233 updates
2022-03-16 04:17:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 04:17:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 04:17:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 34 @ 13233 updates, score 6.276) (writing took 2.311241763061844 seconds)
2022-03-16 04:17:09 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 04:17:09 | INFO | train | epoch 034 | loss 6.146 | ppl 70.8 | wps 23718.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 13233 | lr 0.000274898 | gnorm 0.534 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 42167
KL Stats: Epoch 34 Divergences: Uniform: 4.198799692589989 Unigram: 3.1192243583737804
2022-03-16 04:17:09 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 04:17:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:20:07 | INFO | train_inner | epoch 035:     67 / 392 loss=6.121, ppl=69.6, wps=21653.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=13300, lr=0.000274204, gnorm=0.539, loss_scale=16, train_wall=239, gb_free=9.6, wall=42345
2022-03-16 04:23:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:24:34 | INFO | train_inner | epoch 035:    168 / 392 loss=6.122, ppl=69.63, wps=24536.6, ups=0.37, wpb=65532.7, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.535, loss_scale=16, train_wall=243, gb_free=9.6, wall=42612
2022-03-16 04:28:59 | INFO | train_inner | epoch 035:    268 / 392 loss=6.145, ppl=70.78, wps=24774.5, ups=0.38, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=42877
2022-03-16 04:29:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:33:26 | INFO | train_inner | epoch 035:    369 / 392 loss=6.147, ppl=70.85, wps=24544.4, ups=0.37, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.536, loss_scale=16, train_wall=243, gb_free=9.6, wall=43144
2022-03-16 04:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:35:00 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.269 | ppl 77.13 | wps 36885.9 | wpb 511.9 | bsz 1 | num_updates 13623 | best_loss 6.269
2022-03-16 04:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13623 updates
2022-03-16 04:35:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 04:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 04:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 35 @ 13623 updates, score 6.269) (writing took 2.2597659430466592 seconds)
2022-03-16 04:35:02 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 04:35:02 | INFO | train | epoch 035 | loss 6.132 | ppl 70.14 | wps 23784.2 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 13623 | lr 0.000270934 | gnorm 0.537 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 43240
KL Stats: Epoch 35 Divergences: Uniform: 4.2092010103428095 Unigram: 3.126385806407527
2022-03-16 04:35:02 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 04:35:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:36:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:38:28 | INFO | train_inner | epoch 036:     78 / 392 loss=6.1, ppl=68.61, wps=21476.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=13700, lr=0.000270172, gnorm=0.537, loss_scale=16, train_wall=242, gb_free=9.6, wall=43446
2022-03-16 04:41:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:42:56 | INFO | train_inner | epoch 036:    179 / 392 loss=6.116, ppl=69.34, wps=24538.7, ups=0.37, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.533, loss_scale=16, train_wall=243, gb_free=9.6, wall=43713
2022-03-16 04:47:20 | INFO | train_inner | epoch 036:    279 / 392 loss=6.128, ppl=69.92, wps=24788.2, ups=0.38, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=43978
2022-03-16 04:47:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:51:47 | INFO | train_inner | epoch 036:    380 / 392 loss=6.139, ppl=70.47, wps=24551.3, ups=0.37, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.538, loss_scale=16, train_wall=243, gb_free=9.6, wall=44245
2022-03-16 04:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:52:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.27 | ppl 77.18 | wps 36864.1 | wpb 511.9 | bsz 1 | num_updates 14012 | best_loss 6.269
2022-03-16 04:52:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14012 updates
2022-03-16 04:52:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 04:52:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 04:52:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 36 @ 14012 updates, score 6.27) (writing took 1.2781316080363467 seconds)
2022-03-16 04:52:53 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 04:52:53 | INFO | train | epoch 036 | loss 6.119 | ppl 69.51 | wps 23755.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 14012 | lr 0.000267147 | gnorm 0.536 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 44311
KL Stats: Epoch 36 Divergences: Uniform: 4.217247864664938 Unigram: 3.1343032801267836
2022-03-16 04:52:53 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 04:52:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:54:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:56:49 | INFO | train_inner | epoch 037:     89 / 392 loss=6.09, ppl=68.13, wps=21541.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=14100, lr=0.000266312, gnorm=0.54, loss_scale=16, train_wall=242, gb_free=9.6, wall=44547
2022-03-16 05:00:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:01:16 | INFO | train_inner | epoch 037:    190 / 392 loss=6.1, ppl=68.58, wps=24511.6, ups=0.37, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.534, loss_scale=16, train_wall=243, gb_free=9.6, wall=44814
2022-03-16 05:05:41 | INFO | train_inner | epoch 037:    290 / 392 loss=6.116, ppl=69.34, wps=24757, ups=0.38, wpb=65532.7, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=45079
2022-03-16 05:05:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:10:08 | INFO | train_inner | epoch 037:    391 / 392 loss=6.128, ppl=69.94, wps=24516, ups=0.37, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.542, loss_scale=16, train_wall=243, gb_free=9.6, wall=45346
2022-03-16 05:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:10:44 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.261 | ppl 76.7 | wps 36914.1 | wpb 511.9 | bsz 1 | num_updates 14401 | best_loss 6.261
2022-03-16 05:10:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14401 updates
2022-03-16 05:10:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 05:10:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 05:10:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 37 @ 14401 updates, score 6.261) (writing took 2.2526362700155005 seconds)
2022-03-16 05:10:46 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 05:10:46 | INFO | train | epoch 037 | loss 6.107 | ppl 68.94 | wps 23709.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 14401 | lr 0.000263514 | gnorm 0.537 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 45384
KL Stats: Epoch 37 Divergences: Uniform: 4.225114379213493 Unigram: 3.140112724414247
2022-03-16 05:10:46 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 05:10:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:12:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:15:11 | INFO | train_inner | epoch 038:    100 / 392 loss=6.072, ppl=67.26, wps=21480.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=14500, lr=0.000262613, gnorm=0.545, loss_scale=16, train_wall=241, gb_free=9.6, wall=45649
2022-03-16 05:18:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:19:38 | INFO | train_inner | epoch 038:    201 / 392 loss=6.091, ppl=68.17, wps=24549.6, ups=0.37, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.537, loss_scale=16, train_wall=243, gb_free=9.6, wall=45916
2022-03-16 05:24:02 | INFO | train_inner | epoch 038:    301 / 392 loss=6.106, ppl=68.89, wps=24788.1, ups=0.38, wpb=65532.7, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.534, loss_scale=32, train_wall=241, gb_free=9.6, wall=46180
2022-03-16 05:24:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:28:36 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.25 | ppl 76.12 | wps 36910.1 | wpb 511.9 | bsz 1 | num_updates 14790 | best_loss 6.25
2022-03-16 05:28:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14790 updates
2022-03-16 05:28:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 05:28:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 05:28:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 38 @ 14790 updates, score 6.25) (writing took 2.2608633040217683 seconds)
2022-03-16 05:28:38 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 05:28:38 | INFO | train | epoch 038 | loss 6.096 | ppl 68.38 | wps 23734.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 14790 | lr 0.000260025 | gnorm 0.538 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 46456
KL Stats: Epoch 38 Divergences: Uniform: 4.2308841750785 Unigram: 3.1438216998216615
2022-03-16 05:28:38 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 05:28:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:29:05 | INFO | train_inner | epoch 039:     10 / 392 loss=6.109, ppl=69, wps=21503.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=14800, lr=0.000259938, gnorm=0.543, loss_scale=16, train_wall=241, gb_free=9.6, wall=46483
2022-03-16 05:30:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:33:32 | INFO | train_inner | epoch 039:    111 / 392 loss=6.065, ppl=66.96, wps=24525.5, ups=0.37, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.539, loss_scale=16, train_wall=243, gb_free=9.6, wall=46750
2022-03-16 05:35:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:37:59 | INFO | train_inner | epoch 039:    212 / 392 loss=6.077, ppl=67.51, wps=24516.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.539, loss_scale=8, train_wall=243, gb_free=9.6, wall=47017
2022-03-16 05:42:24 | INFO | train_inner | epoch 039:    312 / 392 loss=6.098, ppl=68.48, wps=24752.5, ups=0.38, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.533, loss_scale=16, train_wall=241, gb_free=9.6, wall=47282
2022-03-16 05:42:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:45:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:46:29 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.239 | ppl 75.54 | wps 36891.8 | wpb 511.9 | bsz 1 | num_updates 15179 | best_loss 6.239
2022-03-16 05:46:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15179 updates
2022-03-16 05:46:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 05:46:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 05:46:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 39 @ 15179 updates, score 6.239) (writing took 2.2954673720523715 seconds)
2022-03-16 05:46:31 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 05:46:31 | INFO | train | epoch 039 | loss 6.084 | ppl 67.85 | wps 23706.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 15179 | lr 0.000256672 | gnorm 0.54 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 47529
KL Stats: Epoch 39 Divergences: Uniform: 4.238830304197448 Unigram: 3.148958845487085
2022-03-16 05:46:31 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 05:46:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:47:27 | INFO | train_inner | epoch 040:     21 / 392 loss=6.088, ppl=68.02, wps=21448.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=15200, lr=0.000256495, gnorm=0.549, loss_scale=8, train_wall=242, gb_free=9.6, wall=47585
2022-03-16 05:51:52 | INFO | train_inner | epoch 040:    121 / 392 loss=6.055, ppl=66.51, wps=24757.6, ups=0.38, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=47850
2022-03-16 05:54:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:56:19 | INFO | train_inner | epoch 040:    222 / 392 loss=6.073, ppl=67.34, wps=24512.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.55, loss_scale=16, train_wall=243, gb_free=9.6, wall=48117
2022-03-16 06:00:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:00:47 | INFO | train_inner | epoch 040:    323 / 392 loss=6.085, ppl=67.89, wps=24500.4, ups=0.37, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.543, loss_scale=16, train_wall=244, gb_free=9.6, wall=48385
2022-03-16 06:03:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:04:22 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.241 | ppl 75.63 | wps 36943.7 | wpb 511.9 | bsz 1 | num_updates 15569 | best_loss 6.239
2022-03-16 06:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15569 updates
2022-03-16 06:04:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 06:04:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 06:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 40 @ 15569 updates, score 6.241) (writing took 1.2687093729618937 seconds)
2022-03-16 06:04:24 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 06:04:24 | INFO | train | epoch 040 | loss 6.074 | ppl 67.37 | wps 23786.2 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 15569 | lr 0.000253437 | gnorm 0.543 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 48602
KL Stats: Epoch 40 Divergences: Uniform: 4.246780994596287 Unigram: 3.1549858332943006
2022-03-16 06:04:24 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 06:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:05:46 | INFO | train_inner | epoch 041:     31 / 392 loss=6.076, ppl=67.47, wps=21738.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=15600, lr=0.000253185, gnorm=0.542, loss_scale=16, train_wall=239, gb_free=9.6, wall=48684
2022-03-16 06:06:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:09:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:10:16 | INFO | train_inner | epoch 041:    133 / 392 loss=6.055, ppl=66.49, wps=24284.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.54, loss_scale=8, train_wall=246, gb_free=9.6, wall=48954
2022-03-16 06:14:40 | INFO | train_inner | epoch 041:    233 / 392 loss=6.064, ppl=66.91, wps=24790.8, ups=0.38, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.532, loss_scale=8, train_wall=241, gb_free=9.6, wall=49218
2022-03-16 06:19:04 | INFO | train_inner | epoch 041:    333 / 392 loss=6.077, ppl=67.49, wps=24810.2, ups=0.38, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.54, loss_scale=16, train_wall=241, gb_free=9.6, wall=49482
2022-03-16 06:20:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:21:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:22:13 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.236 | ppl 75.37 | wps 36979.9 | wpb 511.9 | bsz 1 | num_updates 15958 | best_loss 6.236
2022-03-16 06:22:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 15958 updates
2022-03-16 06:22:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 06:22:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 06:22:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 41 @ 15958 updates, score 6.236) (writing took 2.237790769082494 seconds)
2022-03-16 06:22:15 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 06:22:15 | INFO | train | epoch 041 | loss 6.064 | ppl 66.91 | wps 23747.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 15958 | lr 0.000250329 | gnorm 0.539 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 49673
KL Stats: Epoch 41 Divergences: Uniform: 4.255496339284115 Unigram: 3.1601144156670493
2022-03-16 06:22:15 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 06:22:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:22:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:24:09 | INFO | train_inner | epoch 042:     43 / 392 loss=6.056, ppl=66.51, wps=21344.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=16000, lr=0.00025, gnorm=0.551, loss_scale=8, train_wall=243, gb_free=9.6, wall=49787
2022-03-16 06:28:33 | INFO | train_inner | epoch 042:    143 / 392 loss=6.042, ppl=65.89, wps=24812.3, ups=0.38, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.536, loss_scale=16, train_wall=240, gb_free=9.6, wall=50051
2022-03-16 06:32:57 | INFO | train_inner | epoch 042:    243 / 392 loss=6.053, ppl=66.39, wps=24815.6, ups=0.38, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.537, loss_scale=16, train_wall=240, gb_free=9.6, wall=50315
2022-03-16 06:33:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:37:24 | INFO | train_inner | epoch 042:    344 / 392 loss=6.074, ppl=67.39, wps=24569.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.543, loss_scale=16, train_wall=243, gb_free=9.6, wall=50582
2022-03-16 06:39:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:40:04 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.229 | ppl 75.01 | wps 36904.3 | wpb 511.9 | bsz 1 | num_updates 16348 | best_loss 6.229
2022-03-16 06:40:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16348 updates
2022-03-16 06:40:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 06:40:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 06:40:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 42 @ 16348 updates, score 6.229) (writing took 2.2779080090112984 seconds)
2022-03-16 06:40:06 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 06:40:06 | INFO | train | epoch 042 | loss 6.055 | ppl 66.47 | wps 23820 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 16348 | lr 0.000247325 | gnorm 0.54 | loss_scale 32 | train_wall 941 | gb_free 9.6 | wall 50744
KL Stats: Epoch 42 Divergences: Uniform: 4.261868057388375 Unigram: 3.165021579901183
2022-03-16 06:40:06 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 06:40:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:40:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:42:26 | INFO | train_inner | epoch 043:     53 / 392 loss=6.043, ppl=65.94, wps=21494.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=16400, lr=0.000246932, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=50884
2022-03-16 06:46:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:46:53 | INFO | train_inner | epoch 043:    154 / 392 loss=6.035, ppl=65.57, wps=24554.5, ups=0.37, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.544, loss_scale=16, train_wall=243, gb_free=9.6, wall=51151
2022-03-16 06:51:18 | INFO | train_inner | epoch 043:    254 / 392 loss=6.05, ppl=66.24, wps=24773.4, ups=0.38, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.539, loss_scale=16, train_wall=241, gb_free=9.6, wall=51416
2022-03-16 06:52:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:55:45 | INFO | train_inner | epoch 043:    355 / 392 loss=6.063, ppl=66.84, wps=24534.7, ups=0.37, wpb=65532.7, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.541, loss_scale=16, train_wall=243, gb_free=9.6, wall=51683
2022-03-16 06:57:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:57:56 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.221 | ppl 74.6 | wps 36943.9 | wpb 511.9 | bsz 1 | num_updates 16737 | best_loss 6.221
2022-03-16 06:57:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16737 updates
2022-03-16 06:57:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 06:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 06:57:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 43 @ 16737 updates, score 6.221) (writing took 2.2907976410351694 seconds)
2022-03-16 06:57:58 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 06:57:58 | INFO | train | epoch 043 | loss 6.045 | ppl 66.05 | wps 23733.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 16737 | lr 0.000244434 | gnorm 0.542 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 51816
KL Stats: Epoch 43 Divergences: Uniform: 4.267561701832907 Unigram: 3.170562301947451
2022-03-16 06:57:58 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 06:57:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:58:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:00:47 | INFO | train_inner | epoch 044:     64 / 392 loss=6.028, ppl=65.25, wps=21490.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=16800, lr=0.000243975, gnorm=0.544, loss_scale=16, train_wall=241, gb_free=9.6, wall=51985
2022-03-16 07:04:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:05:15 | INFO | train_inner | epoch 044:    165 / 392 loss=6.021, ppl=64.95, wps=24529.1, ups=0.37, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.54, loss_scale=16, train_wall=243, gb_free=9.6, wall=52253
2022-03-16 07:09:39 | INFO | train_inner | epoch 044:    265 / 392 loss=6.049, ppl=66.21, wps=24812.6, ups=0.38, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.538, loss_scale=16, train_wall=240, gb_free=9.6, wall=52517
2022-03-16 07:10:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:14:06 | INFO | train_inner | epoch 044:    366 / 392 loss=6.046, ppl=66.09, wps=24553.3, ups=0.37, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.531, loss_scale=16, train_wall=243, gb_free=9.6, wall=52784
2022-03-16 07:15:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:15:47 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.223 | ppl 74.7 | wps 36957.9 | wpb 511.9 | bsz 1 | num_updates 17126 | best_loss 6.221
2022-03-16 07:15:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17126 updates
2022-03-16 07:15:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 07:15:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 07:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 44 @ 17126 updates, score 6.223) (writing took 1.2752665790030733 seconds)
2022-03-16 07:15:49 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 07:15:49 | INFO | train | epoch 044 | loss 6.036 | ppl 65.61 | wps 23762.6 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 17126 | lr 0.000241642 | gnorm 0.539 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 52886
KL Stats: Epoch 44 Divergences: Uniform: 4.2748176448398265 Unigram: 3.1764083491643094
2022-03-16 07:15:49 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 07:15:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:16:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:19:10 | INFO | train_inner | epoch 045:     76 / 392 loss=6.017, ppl=64.76, wps=21392.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=17200, lr=0.000241121, gnorm=0.551, loss_scale=8, train_wall=244, gb_free=9.6, wall=53088
2022-03-16 07:23:34 | INFO | train_inner | epoch 045:    176 / 392 loss=6.015, ppl=64.67, wps=24783.8, ups=0.38, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.539, loss_scale=16, train_wall=241, gb_free=9.6, wall=53352
2022-03-16 07:27:59 | INFO | train_inner | epoch 045:    276 / 392 loss=6.037, ppl=65.68, wps=24764.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=53617
2022-03-16 07:29:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:32:26 | INFO | train_inner | epoch 045:    377 / 392 loss=6.047, ppl=66.12, wps=24535.3, ups=0.37, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.532, loss_scale=16, train_wall=243, gb_free=9.6, wall=53884
2022-03-16 07:33:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:33:38 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.209 | ppl 74 | wps 36976.9 | wpb 511.9 | bsz 1 | num_updates 17515 | best_loss 6.209
2022-03-16 07:33:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17515 updates
2022-03-16 07:33:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 07:33:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 07:33:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 45 @ 17515 updates, score 6.209) (writing took 2.272840194986202 seconds)
2022-03-16 07:33:41 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 07:33:41 | INFO | train | epoch 045 | loss 6.027 | ppl 65.22 | wps 23732.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 17515 | lr 0.000238943 | gnorm 0.541 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 53959
KL Stats: Epoch 45 Divergences: Uniform: 4.280344789129443 Unigram: 3.178429534248459
2022-03-16 07:33:41 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 07:33:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:35:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:37:28 | INFO | train_inner | epoch 046:     86 / 392 loss=6.002, ppl=64.09, wps=21497.3, ups=0.33, wpb=65025.8, bsz=127, num_updates=17600, lr=0.000238366, gnorm=0.548, loss_scale=16, train_wall=241, gb_free=9.6, wall=54186
2022-03-16 07:40:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:41:55 | INFO | train_inner | epoch 046:    187 / 392 loss=6.018, ppl=64.81, wps=24540.6, ups=0.37, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.541, loss_scale=8, train_wall=243, gb_free=9.6, wall=54453
2022-03-16 07:46:20 | INFO | train_inner | epoch 046:    287 / 392 loss=6.027, ppl=65.19, wps=24772.8, ups=0.38, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.545, loss_scale=8, train_wall=241, gb_free=9.6, wall=54718
2022-03-16 07:48:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:50:48 | INFO | train_inner | epoch 046:    388 / 392 loss=6.033, ppl=65.47, wps=24474.3, ups=0.37, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.551, loss_scale=8, train_wall=244, gb_free=9.6, wall=54986
2022-03-16 07:50:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:51:31 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.213 | ppl 74.18 | wps 36956.7 | wpb 511.9 | bsz 1 | num_updates 17904 | best_loss 6.209
2022-03-16 07:51:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 17904 updates
2022-03-16 07:51:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 07:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 07:51:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 46 @ 17904 updates, score 6.213) (writing took 1.2763161539332941 seconds)
2022-03-16 07:51:32 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 07:51:32 | INFO | train | epoch 046 | loss 6.019 | ppl 64.87 | wps 23736.5 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 17904 | lr 0.000236333 | gnorm 0.547 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 55030
KL Stats: Epoch 46 Divergences: Uniform: 4.288071260303261 Unigram: 3.183878148466118
2022-03-16 07:51:33 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 07:51:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:55:46 | INFO | train_inner | epoch 047:     96 / 392 loss=5.991, ppl=63.6, wps=21782.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=18000, lr=0.000235702, gnorm=0.546, loss_scale=16, train_wall=239, gb_free=9.6, wall=55284
2022-03-16 08:00:10 | INFO | train_inner | epoch 047:    196 / 392 loss=6.005, ppl=64.2, wps=24794.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.547, loss_scale=16, train_wall=241, gb_free=9.6, wall=55548
2022-03-16 08:00:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:04:37 | INFO | train_inner | epoch 047:    297 / 392 loss=6.02, ppl=64.87, wps=24552.6, ups=0.37, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.539, loss_scale=16, train_wall=243, gb_free=9.6, wall=55815
2022-03-16 08:05:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:08:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:09:22 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.207 | ppl 73.87 | wps 36933 | wpb 511.9 | bsz 1 | num_updates 18294 | best_loss 6.207
2022-03-16 08:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18294 updates
2022-03-16 08:09:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 08:09:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 08:09:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 47 @ 18294 updates, score 6.207) (writing took 2.2661102389683947 seconds)
2022-03-16 08:09:24 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 08:09:24 | INFO | train | epoch 047 | loss 6.012 | ppl 64.52 | wps 23809.2 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 18294 | lr 0.000233801 | gnorm 0.546 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 56102
KL Stats: Epoch 47 Divergences: Uniform: 4.291596102980544 Unigram: 3.187266152402127
2022-03-16 08:09:24 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 08:09:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:09:40 | INFO | train_inner | epoch 048:      6 / 392 loss=6.03, ppl=65.32, wps=21500.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=18300, lr=0.000233762, gnorm=0.557, loss_scale=8, train_wall=241, gb_free=9.6, wall=56118
2022-03-16 08:14:05 | INFO | train_inner | epoch 048:    106 / 392 loss=5.983, ppl=63.25, wps=24759.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.548, loss_scale=16, train_wall=241, gb_free=9.6, wall=56382
2022-03-16 08:17:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:18:32 | INFO | train_inner | epoch 048:    207 / 392 loss=5.998, ppl=63.92, wps=24539.5, ups=0.37, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.544, loss_scale=16, train_wall=243, gb_free=9.6, wall=56650
2022-03-16 08:21:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:22:59 | INFO | train_inner | epoch 048:    308 / 392 loss=6.017, ppl=64.75, wps=24552.7, ups=0.37, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.543, loss_scale=8, train_wall=243, gb_free=9.6, wall=56916
2022-03-16 08:26:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:27:14 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.201 | ppl 73.59 | wps 36939 | wpb 511.9 | bsz 1 | num_updates 18684 | best_loss 6.201
2022-03-16 08:27:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18684 updates
2022-03-16 08:27:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 08:27:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 08:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 48 @ 18684 updates, score 6.201) (writing took 2.2710330260451883 seconds)
2022-03-16 08:27:16 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 08:27:16 | INFO | train | epoch 048 | loss 6.004 | ppl 64.16 | wps 23790.6 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 18684 | lr 0.000231348 | gnorm 0.546 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 57174
KL Stats: Epoch 48 Divergences: Uniform: 4.296386701077052 Unigram: 3.190680344413338
2022-03-16 08:27:16 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 08:27:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:27:59 | INFO | train_inner | epoch 049:     16 / 392 loss=6.013, ppl=64.57, wps=21670.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=18700, lr=0.000231249, gnorm=0.552, loss_scale=16, train_wall=239, gb_free=9.6, wall=57217
2022-03-16 08:32:23 | INFO | train_inner | epoch 049:    116 / 392 loss=5.976, ppl=62.96, wps=24782.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=57481
2022-03-16 08:33:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:36:50 | INFO | train_inner | epoch 049:    217 / 392 loss=5.997, ppl=63.88, wps=24517.2, ups=0.37, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.557, loss_scale=16, train_wall=243, gb_free=9.6, wall=57748
2022-03-16 08:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:41:18 | INFO | train_inner | epoch 049:    318 / 392 loss=6.006, ppl=64.29, wps=24517.1, ups=0.37, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.542, loss_scale=16, train_wall=243, gb_free=9.6, wall=58016
2022-03-16 08:44:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:45:06 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.203 | ppl 73.65 | wps 36934.8 | wpb 511.9 | bsz 1 | num_updates 19074 | best_loss 6.201
2022-03-16 08:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19074 updates
2022-03-16 08:45:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 08:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 08:45:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 49 @ 19074 updates, score 6.203) (writing took 1.5009991690749303 seconds)
2022-03-16 08:45:08 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 08:45:08 | INFO | train | epoch 049 | loss 5.996 | ppl 63.84 | wps 23797.1 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 19074 | lr 0.00022897 | gnorm 0.548 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 58246
KL Stats: Epoch 49 Divergences: Uniform: 4.303612032084656 Unigram: 3.195733510609683
2022-03-16 08:45:08 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 08:45:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:45:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:46:20 | INFO | train_inner | epoch 050:     27 / 392 loss=5.996, ppl=63.84, wps=21537, ups=0.33, wpb=65029.1, bsz=127, num_updates=19100, lr=0.000228814, gnorm=0.546, loss_scale=16, train_wall=242, gb_free=9.6, wall=58318
2022-03-16 08:48:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:50:47 | INFO | train_inner | epoch 050:    128 / 392 loss=5.968, ppl=62.59, wps=24536, ups=0.37, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.555, loss_scale=8, train_wall=243, gb_free=9.6, wall=58585
2022-03-16 08:55:11 | INFO | train_inner | epoch 050:    228 / 392 loss=5.989, ppl=63.51, wps=24794.3, ups=0.38, wpb=65532.7, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.543, loss_scale=16, train_wall=241, gb_free=9.6, wall=58849
2022-03-16 08:59:35 | INFO | train_inner | epoch 050:    328 / 392 loss=6.004, ppl=64.2, wps=24794.9, ups=0.38, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.544, loss_scale=16, train_wall=241, gb_free=9.6, wall=59113
2022-03-16 08:59:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:02:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:02:57 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.199 | ppl 73.45 | wps 36933.3 | wpb 511.9 | bsz 1 | num_updates 19463 | best_loss 6.199
2022-03-16 09:03:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19463 updates
2022-03-16 09:03:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:03:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:03:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 50 @ 19463 updates, score 6.199) (writing took 2.264146796078421 seconds)
2022-03-16 09:03:03 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 09:03:03 | INFO | train | epoch 050 | loss 5.989 | ppl 63.53 | wps 23669.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 19463 | lr 0.000226671 | gnorm 0.548 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 59321
KL Stats: Epoch 50 Divergences: Uniform: 4.309547623675402 Unigram: 3.2000686649527688
2022-03-16 09:03:03 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 09:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:04:41 | INFO | train_inner | epoch 051:     37 / 392 loss=5.996, ppl=63.83, wps=21280.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=19500, lr=0.000226455, gnorm=0.556, loss_scale=16, train_wall=241, gb_free=9.6, wall=59419
2022-03-16 09:06:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:09:08 | INFO | train_inner | epoch 051:    138 / 392 loss=5.964, ppl=62.41, wps=24537.6, ups=0.37, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.546, loss_scale=16, train_wall=243, gb_free=9.6, wall=59686
2022-03-16 09:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:13:35 | INFO | train_inner | epoch 051:    239 / 392 loss=5.981, ppl=63.15, wps=24539.2, ups=0.37, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.544, loss_scale=8, train_wall=243, gb_free=9.6, wall=59953
2022-03-16 09:17:59 | INFO | train_inner | epoch 051:    339 / 392 loss=6.001, ppl=64.05, wps=24782, ups=0.38, wpb=65532.7, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.539, loss_scale=16, train_wall=241, gb_free=9.6, wall=60217
2022-03-16 09:20:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:20:53 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.194 | ppl 73.23 | wps 36938.3 | wpb 511.9 | bsz 1 | num_updates 19853 | best_loss 6.194
2022-03-16 09:20:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 19853 updates
2022-03-16 09:20:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:20:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 51 @ 19853 updates, score 6.194) (writing took 3.200916619040072 seconds)
2022-03-16 09:20:56 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 09:20:56 | INFO | train | epoch 051 | loss 5.982 | ppl 63.21 | wps 23768.4 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 19853 | lr 0.000224433 | gnorm 0.545 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 60394
KL Stats: Epoch 51 Divergences: Uniform: 4.3142591729466675 Unigram: 3.202768852901155
2022-03-16 09:20:56 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 09:20:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:23:00 | INFO | train_inner | epoch 052:     47 / 392 loss=5.981, ppl=63.15, wps=21610.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=19900, lr=0.000224168, gnorm=0.55, loss_scale=16, train_wall=239, gb_free=9.6, wall=60518
2022-03-16 09:23:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:27:30 | INFO | train_inner | epoch 052:    148 / 392 loss=5.96, ppl=62.24, wps=24332.8, ups=0.37, wpb=65532.7, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.548, loss_scale=16, train_wall=245, gb_free=9.6, wall=60788
2022-03-16 09:29:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:32:08 | INFO | train_inner | epoch 052:    249 / 392 loss=5.976, ppl=62.95, wps=23570.2, ups=0.36, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.549, loss_scale=16, train_wall=254, gb_free=9.6, wall=61066
2022-03-16 09:33:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:36:38 | INFO | train_inner | epoch 052:    350 / 392 loss=5.993, ppl=63.7, wps=24255.8, ups=0.37, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.549, loss_scale=8, train_wall=246, gb_free=9.6, wall=61336
2022-03-16 09:38:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:39:04 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.187 | ppl 72.87 | wps 36197.2 | wpb 511.9 | bsz 1 | num_updates 20242 | best_loss 6.187
2022-03-16 09:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20242 updates
2022-03-16 09:39:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:39:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 52 @ 20242 updates, score 6.187) (writing took 2.2293822140200064 seconds)
2022-03-16 09:39:06 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 09:39:06 | INFO | train | epoch 052 | loss 5.975 | ppl 62.9 | wps 23337 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 20242 | lr 0.000222266 | gnorm 0.55 | loss_scale 8 | train_wall 959 | gb_free 9.6 | wall 61484
KL Stats: Epoch 52 Divergences: Uniform: 4.320746963143849 Unigram: 3.206594643689849
2022-03-16 09:39:06 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 09:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:41:42 | INFO | train_inner | epoch 053:     58 / 392 loss=5.958, ppl=62.14, wps=21410, ups=0.33, wpb=65029.1, bsz=127, num_updates=20300, lr=0.000221948, gnorm=0.553, loss_scale=16, train_wall=242, gb_free=9.6, wall=61640
2022-03-16 09:46:08 | INFO | train_inner | epoch 053:    158 / 392 loss=5.96, ppl=62.27, wps=24579.5, ups=0.38, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.543, loss_scale=32, train_wall=243, gb_free=9.6, wall=61906
2022-03-16 09:46:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:48:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:50:40 | INFO | train_inner | epoch 053:    260 / 392 loss=5.971, ppl=62.74, wps=24092.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.541, loss_scale=8, train_wall=248, gb_free=9.6, wall=62178
2022-03-16 09:55:07 | INFO | train_inner | epoch 053:    360 / 392 loss=5.987, ppl=63.43, wps=24570.6, ups=0.37, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.555, loss_scale=16, train_wall=243, gb_free=9.6, wall=62445
2022-03-16 09:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:57:05 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.183 | ppl 72.68 | wps 36759.5 | wpb 511.9 | bsz 1 | num_updates 20632 | best_loss 6.183
2022-03-16 09:57:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20632 updates
2022-03-16 09:57:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:57:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 09:57:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 53 @ 20632 updates, score 6.183) (writing took 2.1923864270793274 seconds)
2022-03-16 09:57:08 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 09:57:08 | INFO | train | epoch 053 | loss 5.969 | ppl 62.62 | wps 23590.6 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 20632 | lr 0.000220155 | gnorm 0.547 | loss_scale 16 | train_wall 951 | gb_free 9.6 | wall 62565
KL Stats: Epoch 53 Divergences: Uniform: 4.324449426222736 Unigram: 3.209732231100433
2022-03-16 09:57:08 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 09:57:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:00:09 | INFO | train_inner | epoch 054:     68 / 392 loss=5.95, ppl=61.83, wps=21537.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=20700, lr=0.000219793, gnorm=0.552, loss_scale=32, train_wall=241, gb_free=9.6, wall=62747
2022-03-16 10:00:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:04:38 | INFO | train_inner | epoch 054:    169 / 392 loss=5.957, ppl=62.14, wps=24372.3, ups=0.37, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.548, loss_scale=16, train_wall=245, gb_free=9.6, wall=63016
2022-03-16 10:06:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:09:07 | INFO | train_inner | epoch 054:    270 / 392 loss=5.972, ppl=62.76, wps=24359.1, ups=0.37, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.55, loss_scale=16, train_wall=245, gb_free=9.6, wall=63285
2022-03-16 10:12:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:13:37 | INFO | train_inner | epoch 054:    371 / 392 loss=5.979, ppl=63.09, wps=24301.5, ups=0.37, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.548, loss_scale=16, train_wall=246, gb_free=9.6, wall=63555
2022-03-16 10:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:15:06 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.182 | ppl 72.63 | wps 36529.4 | wpb 511.9 | bsz 1 | num_updates 21021 | best_loss 6.182
2022-03-16 10:15:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21021 updates
2022-03-16 10:15:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 10:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 10:15:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 54 @ 21021 updates, score 6.182) (writing took 2.1826701359823346 seconds)
2022-03-16 10:15:08 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 10:15:08 | INFO | train | epoch 054 | loss 5.963 | ppl 62.4 | wps 23545.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 21021 | lr 0.000218109 | gnorm 0.551 | loss_scale 16 | train_wall 950 | gb_free 9.6 | wall 63646
KL Stats: Epoch 54 Divergences: Uniform: 4.3298907158792925 Unigram: 3.2144663737483192
2022-03-16 10:15:08 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 10:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:18:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:18:41 | INFO | train_inner | epoch 055:     80 / 392 loss=5.94, ppl=61.38, wps=21328.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=21100, lr=0.0002177, gnorm=0.559, loss_scale=8, train_wall=243, gb_free=9.6, wall=63859
2022-03-16 10:23:10 | INFO | train_inner | epoch 055:    180 / 392 loss=5.95, ppl=61.83, wps=24425.7, ups=0.37, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.547, loss_scale=8, train_wall=245, gb_free=9.6, wall=64128
2022-03-16 10:27:42 | INFO | train_inner | epoch 055:    280 / 392 loss=5.966, ppl=62.5, wps=24088.2, ups=0.37, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.545, loss_scale=16, train_wall=248, gb_free=9.6, wall=64400
2022-03-16 10:29:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:31:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:32:20 | INFO | train_inner | epoch 055:    382 / 392 loss=5.97, ppl=62.67, wps=23582.8, ups=0.36, wpb=65532.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.542, loss_scale=8, train_wall=254, gb_free=9.6, wall=64678
2022-03-16 10:32:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:33:21 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.178 | ppl 72.39 | wps 35757 | wpb 511.9 | bsz 1 | num_updates 21410 | best_loss 6.178
2022-03-16 10:33:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21410 updates
2022-03-16 10:33:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 10:33:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 10:33:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 55 @ 21410 updates, score 6.178) (writing took 2.2566589899361134 seconds)
2022-03-16 10:33:23 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 10:33:23 | INFO | train | epoch 055 | loss 5.956 | ppl 62.09 | wps 23230.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 21410 | lr 0.000216118 | gnorm 0.548 | loss_scale 8 | train_wall 964 | gb_free 9.6 | wall 64741
KL Stats: Epoch 55 Divergences: Uniform: 4.334683616257571 Unigram: 3.2167977441590985
2022-03-16 10:33:23 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 10:33:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:37:30 | INFO | train_inner | epoch 056:     90 / 392 loss=5.926, ppl=60.79, wps=20939, ups=0.32, wpb=65029.1, bsz=127, num_updates=21500, lr=0.000215666, gnorm=0.554, loss_scale=8, train_wall=248, gb_free=9.6, wall=64988
2022-03-16 10:42:04 | INFO | train_inner | epoch 056:    190 / 392 loss=5.947, ppl=61.71, wps=23960.3, ups=0.37, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.552, loss_scale=16, train_wall=250, gb_free=9.6, wall=65262
2022-03-16 10:44:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:46:40 | INFO | train_inner | epoch 056:    291 / 392 loss=5.964, ppl=62.43, wps=23728.9, ups=0.36, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.548, loss_scale=16, train_wall=252, gb_free=9.6, wall=65538
2022-03-16 10:50:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:51:14 | INFO | train_inner | epoch 056:    392 / 392 loss=5.971, ppl=62.72, wps=23751.4, ups=0.37, wpb=65025.8, bsz=127, num_updates=21800, lr=0.000214176, gnorm=0.554, loss_scale=16, train_wall=250, gb_free=9.6, wall=65812
2022-03-16 10:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:51:50 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.178 | ppl 72.43 | wps 35713.6 | wpb 511.9 | bsz 1 | num_updates 21800 | best_loss 6.178
2022-03-16 10:51:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21800 updates
2022-03-16 10:51:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 10:51:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 10:51:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 56 @ 21800 updates, score 6.178) (writing took 2.1034055809723213 seconds)
2022-03-16 10:51:52 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 10:51:52 | INFO | train | epoch 056 | loss 5.951 | ppl 61.86 | wps 23005.2 | ups 0.35 | wpb 65405.2 | bsz 127.7 | num_updates 21800 | lr 0.000214176 | gnorm 0.551 | loss_scale 16 | train_wall 977 | gb_free 9.6 | wall 65850
KL Stats: Epoch 56 Divergences: Uniform: 4.340560345290617 Unigram: 3.220717668796237
2022-03-16 10:51:52 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 10:51:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:56:26 | INFO | train_inner | epoch 057:    100 / 392 loss=5.917, ppl=60.44, wps=20985.2, ups=0.32, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.557, loss_scale=16, train_wall=250, gb_free=9.6, wall=66124
2022-03-16 10:56:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:01:01 | INFO | train_inner | epoch 057:    201 / 392 loss=5.942, ppl=61.47, wps=23808.6, ups=0.36, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.548, loss_scale=16, train_wall=251, gb_free=9.6, wall=66399
2022-03-16 11:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:05:37 | INFO | train_inner | epoch 057:    302 / 392 loss=5.965, ppl=62.48, wps=23748.2, ups=0.36, wpb=65532.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.55, loss_scale=16, train_wall=252, gb_free=9.6, wall=66675
2022-03-16 11:08:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:10:18 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.176 | ppl 72.32 | wps 35472.9 | wpb 511.9 | bsz 1 | num_updates 22189 | best_loss 6.176
2022-03-16 11:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22189 updates
2022-03-16 11:10:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 11:10:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 11:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 57 @ 22189 updates, score 6.176) (writing took 2.2620217620860785 seconds)
2022-03-16 11:10:20 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 11:10:20 | INFO | train | epoch 057 | loss 5.945 | ppl 61.61 | wps 22954.9 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 22189 | lr 0.000212291 | gnorm 0.552 | loss_scale 16 | train_wall 976 | gb_free 9.6 | wall 66958
KL Stats: Epoch 57 Divergences: Uniform: 4.346193387848334 Unigram: 3.2245172438001233
2022-03-16 11:10:20 | INFO | fairseq.trainer | begin training epoch 58
2022-03-16 11:10:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:10:51 | INFO | train_inner | epoch 058:     11 / 392 loss=5.953, ppl=61.95, wps=20728.2, ups=0.32, wpb=65029.1, bsz=127, num_updates=22200, lr=0.000212238, gnorm=0.553, loss_scale=16, train_wall=251, gb_free=9.6, wall=66989
2022-03-16 11:12:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:15:25 | INFO | train_inner | epoch 058:    112 / 392 loss=5.914, ppl=60.28, wps=23915.9, ups=0.36, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.547, loss_scale=8, train_wall=250, gb_free=9.6, wall=67263
2022-03-16 11:19:56 | INFO | train_inner | epoch 058:    212 / 392 loss=5.936, ppl=61.21, wps=24184.8, ups=0.37, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.552, loss_scale=16, train_wall=247, gb_free=9.6, wall=67534
2022-03-16 11:22:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:24:32 | INFO | train_inner | epoch 058:    313 / 392 loss=5.951, ppl=61.86, wps=23779, ups=0.36, wpb=65532.7, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.553, loss_scale=8, train_wall=252, gb_free=9.6, wall=67810
2022-03-16 11:28:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:28:42 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.169 | ppl 71.94 | wps 35374.6 | wpb 511.9 | bsz 1 | num_updates 22579 | best_loss 6.169
2022-03-16 11:28:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22579 updates
2022-03-16 11:28:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 11:28:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 11:28:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 58 @ 22579 updates, score 6.169) (writing took 2.279408589936793 seconds)
2022-03-16 11:28:44 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-16 11:28:44 | INFO | train | epoch 058 | loss 5.939 | ppl 61.36 | wps 23112.2 | ups 0.35 | wpb 65405.2 | bsz 127.7 | num_updates 22579 | lr 0.000210449 | gnorm 0.552 | loss_scale 16 | train_wall 972 | gb_free 9.6 | wall 68062
KL Stats: Epoch 58 Divergences: Uniform: 4.3470769683037656 Unigram: 3.2261144164595734
2022-03-16 11:28:44 | INFO | fairseq.trainer | begin training epoch 59
2022-03-16 11:28:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:29:42 | INFO | train_inner | epoch 059:     21 / 392 loss=5.955, ppl=62.03, wps=20979.5, ups=0.32, wpb=65029.1, bsz=127, num_updates=22600, lr=0.000210352, gnorm=0.553, loss_scale=16, train_wall=247, gb_free=9.6, wall=68120
2022-03-16 11:34:16 | INFO | train_inner | epoch 059:    121 / 392 loss=5.917, ppl=60.43, wps=23907.8, ups=0.36, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.549, loss_scale=16, train_wall=250, gb_free=9.6, wall=68394
2022-03-16 11:34:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:38:52 | INFO | train_inner | epoch 059:    222 / 392 loss=5.934, ppl=61.14, wps=23697.5, ups=0.36, wpb=65532.7, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.558, loss_scale=16, train_wall=252, gb_free=9.6, wall=68670
2022-03-16 11:40:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:43:29 | INFO | train_inner | epoch 059:    323 / 392 loss=5.946, ppl=61.66, wps=23677.5, ups=0.36, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.545, loss_scale=16, train_wall=253, gb_free=9.6, wall=68947
2022-03-16 11:46:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:47:12 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.169 | ppl 71.94 | wps 35735.5 | wpb 511.9 | bsz 1 | num_updates 22968 | best_loss 6.169
2022-03-16 11:47:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 22968 updates
2022-03-16 11:47:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 11:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 11:47:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 59 @ 22968 updates, score 6.169) (writing took 2.1611309789586812 seconds)
2022-03-16 11:47:14 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-16 11:47:14 | INFO | train | epoch 059 | loss 5.934 | ppl 61.14 | wps 22923.7 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 22968 | lr 0.00020866 | gnorm 0.552 | loss_scale 16 | train_wall 978 | gb_free 9.6 | wall 69172
KL Stats: Epoch 59 Divergences: Uniform: 4.353086298153993 Unigram: 3.2302887328161027
2022-03-16 11:47:14 | INFO | fairseq.trainer | begin training epoch 60
2022-03-16 11:47:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:48:42 | INFO | train_inner | epoch 060:     32 / 392 loss=5.927, ppl=60.86, wps=20789, ups=0.32, wpb=65029.1, bsz=127, num_updates=23000, lr=0.000208514, gnorm=0.553, loss_scale=16, train_wall=250, gb_free=9.6, wall=69260
2022-03-16 11:53:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:53:18 | INFO | train_inner | epoch 060:    133 / 392 loss=5.914, ppl=60.29, wps=23755.9, ups=0.36, wpb=65532.7, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.545, loss_scale=16, train_wall=252, gb_free=9.6, wall=69536
2022-03-16 11:57:51 | INFO | train_inner | epoch 060:    233 / 392 loss=5.933, ppl=61.11, wps=23982.6, ups=0.37, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.549, loss_scale=16, train_wall=249, gb_free=9.6, wall=69809
2022-03-16 11:59:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:02:30 | INFO | train_inner | epoch 060:    334 / 392 loss=5.945, ppl=61.59, wps=23455.8, ups=0.36, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.544, loss_scale=16, train_wall=255, gb_free=9.6, wall=70088
2022-03-16 12:05:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:05:44 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.172 | ppl 72.11 | wps 35698.8 | wpb 511.9 | bsz 1 | num_updates 23358 | best_loss 6.169
2022-03-16 12:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23358 updates
2022-03-16 12:05:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 12:05:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 12:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 60 @ 23358 updates, score 6.172) (writing took 1.249352234066464 seconds)
2022-03-16 12:05:45 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-16 12:05:45 | INFO | train | epoch 060 | loss 5.929 | ppl 60.93 | wps 22959.9 | ups 0.35 | wpb 65405.2 | bsz 127.7 | num_updates 23358 | lr 0.00020691 | gnorm 0.547 | loss_scale 32 | train_wall 980 | gb_free 9.6 | wall 70283
KL Stats: Epoch 60 Divergences: Uniform: 4.358634523977346 Unigram: 3.2330661186311382
2022-03-16 12:05:45 | INFO | fairseq.trainer | begin training epoch 61
2022-03-16 12:05:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:05:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:07:43 | INFO | train_inner | epoch 061:     43 / 392 loss=5.92, ppl=60.57, wps=20819.8, ups=0.32, wpb=65029.1, bsz=127, num_updates=23400, lr=0.000206725, gnorm=0.552, loss_scale=16, train_wall=251, gb_free=9.6, wall=70401
2022-03-16 12:09:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:12:19 | INFO | train_inner | epoch 061:    144 / 392 loss=5.908, ppl=60.05, wps=23756.4, ups=0.36, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.555, loss_scale=8, train_wall=252, gb_free=9.6, wall=70677
2022-03-16 12:16:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:16:55 | INFO | train_inner | epoch 061:    245 / 392 loss=5.926, ppl=60.8, wps=23744.7, ups=0.36, wpb=65532.7, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.553, loss_scale=8, train_wall=252, gb_free=9.6, wall=70952
2022-03-16 12:21:28 | INFO | train_inner | epoch 061:    345 / 392 loss=5.937, ppl=61.28, wps=23994, ups=0.37, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.545, loss_scale=8, train_wall=249, gb_free=9.6, wall=71226
2022-03-16 12:23:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:24:10 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.165 | ppl 71.76 | wps 35437.7 | wpb 511.9 | bsz 1 | num_updates 23747 | best_loss 6.165
2022-03-16 12:24:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23747 updates
2022-03-16 12:24:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 12:24:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 12:24:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 61 @ 23747 updates, score 6.165) (writing took 2.17088607896585 seconds)
2022-03-16 12:24:13 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-16 12:24:13 | INFO | train | epoch 061 | loss 5.924 | ppl 60.71 | wps 22972.2 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 23747 | lr 0.000205209 | gnorm 0.552 | loss_scale 16 | train_wall 976 | gb_free 9.6 | wall 71390
KL Stats: Epoch 61 Divergences: Uniform: 4.361579670279896 Unigram: 3.235729157130573
2022-03-16 12:24:13 | INFO | fairseq.trainer | begin training epoch 62
2022-03-16 12:24:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:26:38 | INFO | train_inner | epoch 062:     53 / 392 loss=5.925, ppl=60.76, wps=20984.2, ups=0.32, wpb=65029.1, bsz=127, num_updates=23800, lr=0.00020498, gnorm=0.558, loss_scale=16, train_wall=247, gb_free=9.6, wall=71536
2022-03-16 12:28:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:31:14 | INFO | train_inner | epoch 062:    154 / 392 loss=5.905, ppl=59.92, wps=23738.3, ups=0.36, wpb=65532.7, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.547, loss_scale=16, train_wall=252, gb_free=9.6, wall=71812
2022-03-16 12:34:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:35:50 | INFO | train_inner | epoch 062:    255 / 392 loss=5.915, ppl=60.34, wps=23755.3, ups=0.36, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.557, loss_scale=16, train_wall=252, gb_free=9.6, wall=72087
2022-03-16 12:40:23 | INFO | train_inner | epoch 062:    355 / 392 loss=5.941, ppl=61.45, wps=23999.5, ups=0.37, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.556, loss_scale=16, train_wall=249, gb_free=9.6, wall=72361
2022-03-16 12:40:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:42:38 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.164 | ppl 71.7 | wps 35734.2 | wpb 511.9 | bsz 1 | num_updates 24136 | best_loss 6.164
2022-03-16 12:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24136 updates
2022-03-16 12:42:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 12:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 12:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 62 @ 24136 updates, score 6.164) (writing took 2.271431803004816 seconds)
2022-03-16 12:42:40 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-16 12:42:40 | INFO | train | epoch 062 | loss 5.919 | ppl 60.51 | wps 22970 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 24136 | lr 0.000203548 | gnorm 0.554 | loss_scale 16 | train_wall 976 | gb_free 9.6 | wall 72498
KL Stats: Epoch 62 Divergences: Uniform: 4.364821007291156 Unigram: 3.2385977536833837
2022-03-16 12:42:40 | INFO | fairseq.trainer | begin training epoch 63
2022-03-16 12:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:45:36 | INFO | train_inner | epoch 063:     64 / 392 loss=5.903, ppl=59.85, wps=20776.9, ups=0.32, wpb=65029.1, bsz=127, num_updates=24200, lr=0.000203279, gnorm=0.557, loss_scale=16, train_wall=250, gb_free=9.6, wall=72674
2022-03-16 12:47:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:50:12 | INFO | train_inner | epoch 063:    165 / 392 loss=5.906, ppl=59.98, wps=23747.9, ups=0.36, wpb=65532.7, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.552, loss_scale=16, train_wall=252, gb_free=9.6, wall=72949
2022-03-16 12:51:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:54:47 | INFO | train_inner | epoch 063:    266 / 392 loss=5.92, ppl=60.54, wps=23772.1, ups=0.36, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.561, loss_scale=8, train_wall=252, gb_free=9.6, wall=73225
2022-03-16 12:58:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:59:24 | INFO | train_inner | epoch 063:    367 / 392 loss=5.931, ppl=61.02, wps=23714.3, ups=0.36, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.555, loss_scale=8, train_wall=252, gb_free=9.6, wall=73502
2022-03-16 13:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:01:06 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.16 | ppl 71.5 | wps 35713.9 | wpb 511.9 | bsz 1 | num_updates 24525 | best_loss 6.16
2022-03-16 13:01:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24525 updates
2022-03-16 13:01:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 13:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 13:01:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 63 @ 24525 updates, score 6.16) (writing took 2.1694755520438775 seconds)
2022-03-16 13:01:08 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-16 13:01:08 | INFO | train | epoch 063 | loss 5.914 | ppl 60.29 | wps 22963.9 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 24525 | lr 0.000201928 | gnorm 0.557 | loss_scale 8 | train_wall 976 | gb_free 9.6 | wall 73606
KL Stats: Epoch 63 Divergences: Uniform: 4.371094207939484 Unigram: 3.2420327845624506
2022-03-16 13:01:08 | INFO | fairseq.trainer | begin training epoch 64
2022-03-16 13:01:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:04:32 | INFO | train_inner | epoch 064:     75 / 392 loss=5.895, ppl=59.52, wps=21111.7, ups=0.32, wpb=65025.8, bsz=127, num_updates=24600, lr=0.000201619, gnorm=0.555, loss_scale=8, train_wall=246, gb_free=9.6, wall=73810
2022-03-16 13:08:58 | INFO | train_inner | epoch 064:    175 / 392 loss=5.896, ppl=59.57, wps=24568.9, ups=0.37, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.557, loss_scale=16, train_wall=243, gb_free=9.6, wall=74076
2022-03-16 13:10:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:13:26 | INFO | train_inner | epoch 064:    276 / 392 loss=5.918, ppl=60.47, wps=24460.1, ups=0.37, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.56, loss_scale=16, train_wall=244, gb_free=9.6, wall=74344
2022-03-16 13:16:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:17:54 | INFO | train_inner | epoch 064:    377 / 392 loss=5.93, ppl=60.96, wps=24515.6, ups=0.37, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.553, loss_scale=16, train_wall=243, gb_free=9.6, wall=74612
2022-03-16 13:18:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:19:06 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.154 | ppl 71.23 | wps 36893.8 | wpb 511.9 | bsz 1 | num_updates 24915 | best_loss 6.154
2022-03-16 13:19:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 24915 updates
2022-03-16 13:19:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 13:19:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 13:19:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 64 @ 24915 updates, score 6.154) (writing took 2.2690143870422617 seconds)
2022-03-16 13:19:09 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-16 13:19:09 | INFO | train | epoch 064 | loss 5.909 | ppl 60.11 | wps 23607.9 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 24915 | lr 0.000200341 | gnorm 0.557 | loss_scale 16 | train_wall 950 | gb_free 9.6 | wall 74686
KL Stats: Epoch 64 Divergences: Uniform: 4.373820516446832 Unigram: 3.243762978803494
2022-03-16 13:19:09 | INFO | fairseq.trainer | begin training epoch 65
2022-03-16 13:19:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:22:54 | INFO | train_inner | epoch 065:     85 / 392 loss=5.892, ppl=59.37, wps=21633.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=25000, lr=0.0002, gnorm=0.559, loss_scale=16, train_wall=240, gb_free=9.6, wall=74912
2022-03-16 13:23:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:27:27 | INFO | train_inner | epoch 065:    186 / 392 loss=5.894, ppl=59.47, wps=24011.7, ups=0.37, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.553, loss_scale=16, train_wall=249, gb_free=9.6, wall=75185
2022-03-16 13:30:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:32:01 | INFO | train_inner | epoch 065:    287 / 392 loss=5.91, ppl=60.14, wps=23930.8, ups=0.37, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.549, loss_scale=16, train_wall=250, gb_free=9.6, wall=75459
2022-03-16 13:35:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:36:35 | INFO | train_inner | epoch 065:    388 / 392 loss=5.926, ppl=60.79, wps=23939.7, ups=0.37, wpb=65532.7, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.549, loss_scale=16, train_wall=250, gb_free=9.6, wall=75733
2022-03-16 13:36:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:37:19 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.159 | ppl 71.47 | wps 36146.4 | wpb 511.9 | bsz 1 | num_updates 25304 | best_loss 6.154
2022-03-16 13:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25304 updates
2022-03-16 13:37:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 13:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 13:37:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 65 @ 25304 updates, score 6.159) (writing took 1.2608509010169655 seconds)
2022-03-16 13:37:21 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-16 13:37:21 | INFO | train | epoch 065 | loss 5.904 | ppl 59.9 | wps 23300 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 25304 | lr 0.000198795 | gnorm 0.552 | loss_scale 16 | train_wall 961 | gb_free 9.6 | wall 75778
KL Stats: Epoch 65 Divergences: Uniform: 4.379504914627742 Unigram: 3.247387371214152
2022-03-16 13:37:21 | INFO | fairseq.trainer | begin training epoch 66
2022-03-16 13:37:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:39:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 13:41:44 | INFO | train_inner | epoch 066:     97 / 392 loss=5.88, ppl=58.89, wps=21051.9, ups=0.32, wpb=65029.1, bsz=127, num_updates=25400, lr=0.000198419, gnorm=0.563, loss_scale=8, train_wall=248, gb_free=9.6, wall=76042
2022-03-16 13:46:15 | INFO | train_inner | epoch 066:    197 / 392 loss=5.899, ppl=59.69, wps=24163.3, ups=0.37, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.554, loss_scale=16, train_wall=247, gb_free=9.6, wall=76313
2022-03-16 13:49:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 13:50:49 | INFO | train_inner | epoch 066:    298 / 392 loss=5.911, ppl=60.17, wps=23914.8, ups=0.36, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.555, loss_scale=8, train_wall=250, gb_free=9.6, wall=76587
2022-03-16 13:55:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:55:37 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.151 | ppl 71.04 | wps 36141.5 | wpb 511.9 | bsz 1 | num_updates 25694 | best_loss 6.151
2022-03-16 13:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25694 updates
2022-03-16 13:55:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 13:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 13:55:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 66 @ 25694 updates, score 6.151) (writing took 2.2508377619087696 seconds)
2022-03-16 13:55:40 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-16 13:55:40 | INFO | train | epoch 066 | loss 5.9 | ppl 59.73 | wps 23206.2 | ups 0.35 | wpb 65405.2 | bsz 127.7 | num_updates 25694 | lr 0.00019728 | gnorm 0.557 | loss_scale 8 | train_wall 967 | gb_free 9.6 | wall 76878
KL Stats: Epoch 66 Divergences: Uniform: 4.380675456380688 Unigram: 3.248241771058966
2022-03-16 13:55:40 | INFO | fairseq.trainer | begin training epoch 67
2022-03-16 13:55:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:55:56 | INFO | train_inner | epoch 067:      6 / 392 loss=5.909, ppl=60.1, wps=21168.1, ups=0.33, wpb=65025.8, bsz=127, num_updates=25700, lr=0.000197257, gnorm=0.561, loss_scale=8, train_wall=245, gb_free=9.6, wall=76894
2022-03-16 14:00:27 | INFO | train_inner | epoch 067:    106 / 392 loss=5.875, ppl=58.7, wps=24160.6, ups=0.37, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.554, loss_scale=16, train_wall=247, gb_free=9.6, wall=77165
2022-03-16 14:03:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:05:02 | INFO | train_inner | epoch 067:    207 / 392 loss=5.893, ppl=59.43, wps=23867.8, ups=0.36, wpb=65532.7, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.555, loss_scale=16, train_wall=250, gb_free=9.6, wall=77440
2022-03-16 14:09:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:09:37 | INFO | train_inner | epoch 067:    308 / 392 loss=5.907, ppl=60.02, wps=23862.3, ups=0.36, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.554, loss_scale=16, train_wall=250, gb_free=9.6, wall=77714
2022-03-16 14:10:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 14:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:13:58 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.145 | ppl 70.78 | wps 36188.8 | wpb 511.9 | bsz 1 | num_updates 26083 | best_loss 6.145
2022-03-16 14:13:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26083 updates
2022-03-16 14:13:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 14:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 14:14:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 67 @ 26083 updates, score 6.145) (writing took 2.258094114018604 seconds)
2022-03-16 14:14:00 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-16 14:14:00 | INFO | train | epoch 067 | loss 5.897 | ppl 59.57 | wps 23123.4 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 26083 | lr 0.000195804 | gnorm 0.558 | loss_scale 8 | train_wall 969 | gb_free 9.6 | wall 77978
KL Stats: Epoch 67 Divergences: Uniform: 4.383805468762533 Unigram: 3.249542813143517
2022-03-16 14:14:00 | INFO | fairseq.trainer | begin training epoch 68
2022-03-16 14:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:14:46 | INFO | train_inner | epoch 068:     17 / 392 loss=5.904, ppl=59.89, wps=20999.9, ups=0.32, wpb=65025.8, bsz=127, num_updates=26100, lr=0.00019574, gnorm=0.565, loss_scale=8, train_wall=248, gb_free=9.6, wall=78024
2022-03-16 14:19:18 | INFO | train_inner | epoch 068:    117 / 392 loss=5.876, ppl=58.72, wps=24131.7, ups=0.37, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.552, loss_scale=16, train_wall=248, gb_free=9.6, wall=78296
2022-03-16 14:23:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:23:52 | INFO | train_inner | epoch 068:    218 / 392 loss=5.886, ppl=59.16, wps=23904.4, ups=0.36, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.55, loss_scale=16, train_wall=250, gb_free=9.6, wall=78570
2022-03-16 14:28:23 | INFO | train_inner | epoch 068:    318 / 392 loss=5.9, ppl=59.7, wps=24150.3, ups=0.37, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.553, loss_scale=16, train_wall=248, gb_free=9.6, wall=78841
2022-03-16 14:29:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:32:18 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.151 | ppl 71.05 | wps 36090.7 | wpb 511.9 | bsz 1 | num_updates 26473 | best_loss 6.145
2022-03-16 14:32:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26473 updates
2022-03-16 14:32:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 14:32:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 14:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 68 @ 26473 updates, score 6.151) (writing took 1.2772807970177382 seconds)
2022-03-16 14:32:19 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-16 14:32:19 | INFO | train | epoch 068 | loss 5.892 | ppl 59.38 | wps 23207.4 | ups 0.35 | wpb 65405.2 | bsz 127.7 | num_updates 26473 | lr 0.000194356 | gnorm 0.554 | loss_scale 16 | train_wall 968 | gb_free 9.6 | wall 79077
KL Stats: Epoch 68 Divergences: Uniform: 4.38920711115035 Unigram: 3.255240122146568
2022-03-16 14:32:19 | INFO | fairseq.trainer | begin training epoch 69
2022-03-16 14:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:33:32 | INFO | train_inner | epoch 069:     27 / 392 loss=5.901, ppl=59.77, wps=21032.1, ups=0.32, wpb=65029.1, bsz=127, num_updates=26500, lr=0.000194257, gnorm=0.562, loss_scale=16, train_wall=248, gb_free=9.6, wall=79150
2022-03-16 14:35:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:38:06 | INFO | train_inner | epoch 069:    128 / 392 loss=5.872, ppl=58.57, wps=23924.6, ups=0.37, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.559, loss_scale=16, train_wall=250, gb_free=9.6, wall=79424
2022-03-16 14:41:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:42:40 | INFO | train_inner | epoch 069:    229 / 392 loss=5.883, ppl=59.01, wps=23915.2, ups=0.36, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.554, loss_scale=16, train_wall=250, gb_free=9.6, wall=79698
2022-03-16 14:47:12 | INFO | train_inner | epoch 069:    329 / 392 loss=5.906, ppl=59.95, wps=24157.1, ups=0.37, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.56, loss_scale=16, train_wall=247, gb_free=9.6, wall=79970
2022-03-16 14:47:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:49:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 14:50:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:50:36 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.143 | ppl 70.67 | wps 36140.2 | wpb 511.9 | bsz 1 | num_updates 26861 | best_loss 6.143
2022-03-16 14:50:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 26861 updates
2022-03-16 14:50:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 14:50:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt
2022-03-16 14:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_best.pt (epoch 69 @ 26861 updates, score 6.143) (writing took 2.2467873740242794 seconds)
2022-03-16 14:50:39 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-16 14:50:39 | INFO | train | epoch 069 | loss 5.888 | ppl 59.21 | wps 23080.3 | ups 0.35 | wpb 65404.5 | bsz 127.7 | num_updates 26861 | lr 0.000192947 | gnorm 0.559 | loss_scale 8 | train_wall 968 | gb_free 9.6 | wall 80177
KL Stats: Epoch 69 Divergences: Uniform: 4.390937062918177 Unigram: 3.255726627649438
2022-03-16 14:50:39 | INFO | fairseq.trainer | begin training epoch 70
2022-03-16 14:50:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:52:24 | INFO | train_inner | epoch 070:     39 / 392 loss=5.889, ppl=59.24, wps=20794.5, ups=0.32, wpb=65025.8, bsz=127, num_updates=26900, lr=0.000192807, gnorm=0.569, loss_scale=8, train_wall=250, gb_free=9.6, wall=80282
2022-03-16 14:56:56 | INFO | train_inner | epoch 070:    139 / 392 loss=5.872, ppl=58.57, wps=24160, ups=0.37, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.557, loss_scale=16, train_wall=247, gb_free=9.6, wall=80554
2022-03-16 15:01:27 | INFO | train_inner | epoch 070:    239 / 392 loss=5.889, ppl=59.25, wps=24169.9, ups=0.37, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.557, loss_scale=16, train_wall=247, gb_free=9.6, wall=80825
2022-03-16 15:02:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:06:01 | INFO | train_inner | epoch 070:    340 / 392 loss=5.893, ppl=59.41, wps=23943.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.563, loss_scale=16, train_wall=250, gb_free=9.6, wall=81098
2022-03-16 15:07:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:08:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:08:55 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.151 | ppl 71.07 | wps 36175.9 | wpb 511.9 | bsz 1 | num_updates 27251 | best_loss 6.143
2022-03-16 15:08:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27251 updates
2022-03-16 15:08:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 15:08:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 15:08:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 70 @ 27251 updates, score 6.151) (writing took 1.2635311400517821 seconds)
2022-03-16 15:08:56 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-16 15:08:56 | INFO | train | epoch 070 | loss 5.883 | ppl 59.03 | wps 23236.7 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 27251 | lr 0.000191562 | gnorm 0.56 | loss_scale 16 | train_wall 967 | gb_free 9.6 | wall 81274
KL Stats: Epoch 70 Divergences: Uniform: 4.396276608100057 Unigram: 3.2599745711935615
2022-03-16 15:08:56 | INFO | fairseq.trainer | begin training epoch 71
2022-03-16 15:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 15:11:09 | INFO | train_inner | epoch 071:     49 / 392 loss=5.876, ppl=58.72, wps=21058.5, ups=0.32, wpb=65029.1, bsz=127, num_updates=27300, lr=0.00019139, gnorm=0.558, loss_scale=16, train_wall=248, gb_free=9.6, wall=81407
2022-03-16 15:14:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:15:43 | INFO | train_inner | epoch 071:    150 / 392 loss=5.868, ppl=58.39, wps=23939.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.55, loss_scale=16, train_wall=250, gb_free=9.6, wall=81681
2022-03-16 15:18:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 15:20:17 | INFO | train_inner | epoch 071:    251 / 392 loss=5.881, ppl=58.95, wps=23940, ups=0.37, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.561, loss_scale=8, train_wall=250, gb_free=9.6, wall=81955
2022-03-16 15:24:48 | INFO | train_inner | epoch 071:    351 / 392 loss=5.896, ppl=59.54, wps=24187.5, ups=0.37, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.559, loss_scale=16, train_wall=247, gb_free=9.6, wall=82226
2022-03-16 15:26:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:27:13 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.151 | ppl 71.05 | wps 36145.1 | wpb 511.9 | bsz 1 | num_updates 27641 | best_loss 6.143
2022-03-16 15:27:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27641 updates
2022-03-16 15:27:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 15:27:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 15:27:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 71 @ 27641 updates, score 6.151) (writing took 1.2579524019965902 seconds)
2022-03-16 15:27:14 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-16 15:27:14 | INFO | train | epoch 071 | loss 5.88 | ppl 58.89 | wps 23240.4 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 27641 | lr 0.000190206 | gnorm 0.559 | loss_scale 16 | train_wall 967 | gb_free 9.6 | wall 82372
KL Stats: Epoch 71 Divergences: Uniform: 4.401271417241242 Unigram: 3.2627669346839934
2022-03-16 15:27:14 | INFO | fairseq.trainer | begin training epoch 72
2022-03-16 15:27:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 15:29:54 | INFO | train_inner | epoch 072:     59 / 392 loss=5.87, ppl=58.48, wps=21232.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=27700, lr=0.000190003, gnorm=0.567, loss_scale=16, train_wall=245, gb_free=9.6, wall=82532
2022-03-16 15:31:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:34:28 | INFO | train_inner | epoch 072:    160 / 392 loss=5.865, ppl=58.28, wps=23900.1, ups=0.36, wpb=65532.7, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.558, loss_scale=16, train_wall=250, gb_free=9.6, wall=82806
2022-03-16 15:37:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:39:03 | INFO | train_inner | epoch 072:    261 / 392 loss=5.881, ppl=58.94, wps=23888.8, ups=0.36, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.559, loss_scale=16, train_wall=250, gb_free=9.6, wall=83081
2022-03-16 15:43:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:43:37 | INFO | train_inner | epoch 072:    362 / 392 loss=5.893, ppl=59.44, wps=23909.5, ups=0.36, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.552, loss_scale=16, train_wall=250, gb_free=9.6, wall=83355
2022-03-16 15:44:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:45:32 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 6.145 | ppl 70.76 | wps 36151.3 | wpb 511.9 | bsz 1 | num_updates 28030 | best_loss 6.143
2022-03-16 15:45:32 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 15:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28030 updates
2022-03-16 15:45:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 15:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt
2022-03-16 15:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.02_0.03_0.95/checkpoint_last.pt (epoch 72 @ 28030 updates, score 6.145) (writing took 1.2956689449492842 seconds)
2022-03-16 15:45:33 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-16 15:45:33 | INFO | train | epoch 072 | loss 5.876 | ppl 58.72 | wps 23148.7 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 28030 | lr 0.000188881 | gnorm 0.558 | loss_scale 16 | train_wall 968 | gb_free 9.6 | wall 83471
2022-03-16 15:45:33 | INFO | fairseq_cli.train | done training in 83470.8 seconds
