Sender: LSF System <lsfadmin@eu-g2-08>
Subject: Job 208027552: <en_cross_entropy_dropout_0.1_#1> in cluster <euler> Done

Job <en_cross_entropy_dropout_0.1_#1> was submitted from host <eu-login-12> by user <andriusb> in cluster <euler> at Sat Mar 12 10:18:59 2022
Job was executed on host(s) <eu-g2-08>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Sat Mar 12 10:19:09 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sat Mar 12 10:19:09 2022
Terminated at Sat Mar 12 16:37:44 2022
Results reported at Sat Mar 12 16:37:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/en --save-dir /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   24126.40 sec.
    Max Memory :                                 4023 MB
    Average Memory :                             3357.41 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15977.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   22714 sec.
    Turnaround time :                            22725 sec.

The output (if any) follows:

2022-03-12 10:19:17 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/en', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-12 10:19:18 | INFO | fairseq.tasks.language_modeling | dictionary: 35768 types
2022-03-12 10:19:18 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35768, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=35768, bias=False)
  )
)
2022-03-12 10:19:18 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-12 10:19:18 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-12 10:19:18 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-12 10:19:18 | INFO | fairseq_cli.train | num. shared model params: 37,227,520 (num. trained: 37,227,520)
2022-03-12 10:19:18 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-12 10:19:18 | INFO | fairseq.data.data_utils | loaded 1,679 examples from: data-bin/en/valid
2022-03-12 10:19:21 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-12 10:19:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-12 10:19:21 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-12 10:19:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-12 10:19:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-12 10:19:21 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-12 10:19:21 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 10:19:21 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 10:19:21 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-12 10:19:22 | INFO | fairseq.data.data_utils | loaded 35,391 examples from: data-bin/en/train
2022-03-12 10:19:22 | INFO | fairseq.trainer | begin training epoch 1
2022-03-12 10:19:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 10:19:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-12 10:19:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:19:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 10:19:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 10:23:13 | INFO | train_inner | epoch 001:    104 / 347 loss=14.508, ppl=23293.4, wps=30751.1, ups=0.47, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.171, loss_scale=8, train_wall=207, gb_free=9.8, wall=232
2022-03-12 10:26:46 | INFO | train_inner | epoch 001:    204 / 347 loss=12.559, ppl=6033.91, wps=30760.9, ups=0.47, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.994, loss_scale=16, train_wall=190, gb_free=9.8, wall=445
2022-03-12 10:30:20 | INFO | train_inner | epoch 001:    304 / 347 loss=11.45, ppl=2797.62, wps=30635.8, ups=0.47, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.62, loss_scale=32, train_wall=191, gb_free=9.8, wall=659
2022-03-12 10:31:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 10:32:14 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.793 | ppl 1774.23 | wps 53088.5 | wpb 511.9 | bsz 1 | num_updates 343
2022-03-12 10:32:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 343 updates
2022-03-12 10:32:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 10:32:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 10:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 1 @ 343 updates, score 10.793) (writing took 2.163417368021328 seconds)
2022-03-12 10:32:17 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-12 10:32:17 | INFO | train | epoch 001 | loss 12.597 | ppl 6197.4 | wps 29653.2 | ups 0.45 | wpb 65403.8 | bsz 127.7 | num_updates 343 | lr 4.29664e-05 | gnorm 1.453 | loss_scale 32 | train_wall 669 | gb_free 9.8 | wall 775
2022-03-12 10:32:17 | INFO | fairseq.trainer | begin training epoch 2
2022-03-12 10:32:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 10:34:19 | INFO | train_inner | epoch 002:     57 / 347 loss=10.772, ppl=1748.47, wps=27257.7, ups=0.42, wpb=65082.7, bsz=127.1, num_updates=400, lr=5.009e-05, gnorm=0.414, loss_scale=64, train_wall=191, gb_free=9.8, wall=898
2022-03-12 10:34:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:37:58 | INFO | train_inner | epoch 002:    158 / 347 loss=10.45, ppl=1398.78, wps=29944.4, ups=0.46, wpb=65533.2, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.429, loss_scale=32, train_wall=195, gb_free=9.8, wall=1116
2022-03-12 10:39:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:41:35 | INFO | train_inner | epoch 002:    259 / 347 loss=10.147, ppl=1133.68, wps=30214.5, ups=0.46, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.51, loss_scale=32, train_wall=193, gb_free=9.8, wall=1333
2022-03-12 10:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 10:45:04 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.702 | ppl 832.97 | wps 54197.9 | wpb 511.9 | bsz 1 | num_updates 688 | best_loss 9.702
2022-03-12 10:45:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 688 updates
2022-03-12 10:45:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 10:45:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 10:45:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 2 @ 688 updates, score 9.702) (writing took 2.2104830570169725 seconds)
2022-03-12 10:45:06 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-12 10:45:06 | INFO | train | epoch 002 | loss 10.249 | ppl 1216.55 | wps 29316.4 | ups 0.45 | wpb 65404.6 | bsz 127.7 | num_updates 688 | lr 8.60828e-05 | gnorm 0.479 | loss_scale 64 | train_wall 664 | gb_free 9.8 | wall 1545
2022-03-12 10:45:06 | INFO | fairseq.trainer | begin training epoch 3
2022-03-12 10:45:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 10:45:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:45:35 | INFO | train_inner | epoch 003:     13 / 347 loss=9.83, ppl=910.33, wps=27093.9, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=700, lr=8.75825e-05, gnorm=0.587, loss_scale=32, train_wall=193, gb_free=9.8, wall=1574
2022-03-12 10:49:10 | INFO | train_inner | epoch 003:    113 / 347 loss=9.546, ppl=747.31, wps=30422.8, ups=0.46, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.688, loss_scale=32, train_wall=192, gb_free=9.8, wall=1789
2022-03-12 10:49:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:52:48 | INFO | train_inner | epoch 003:    214 / 347 loss=9.311, ppl=635.26, wps=30077, ups=0.46, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.722, loss_scale=32, train_wall=194, gb_free=9.8, wall=2007
2022-03-12 10:54:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:56:25 | INFO | train_inner | epoch 003:    315 / 347 loss=9.113, ppl=553.74, wps=30284.3, ups=0.46, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.759, loss_scale=32, train_wall=193, gb_free=9.8, wall=2223
2022-03-12 10:57:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 10:57:55 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.96 | ppl 497.87 | wps 52539 | wpb 511.9 | bsz 1 | num_updates 1032 | best_loss 8.96
2022-03-12 10:57:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1032 updates
2022-03-12 10:57:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 10:57:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 10:57:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 3 @ 1032 updates, score 8.96) (writing took 2.1869263259868603 seconds)
2022-03-12 10:57:57 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-12 10:57:57 | INFO | train | epoch 003 | loss 9.308 | ppl 633.76 | wps 29188.2 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 1032 | lr 0.000129074 | gnorm 0.73 | loss_scale 32 | train_wall 665 | gb_free 9.8 | wall 2316
2022-03-12 10:57:57 | INFO | fairseq.trainer | begin training epoch 4
2022-03-12 10:57:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 10:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:59:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 11:00:26 | INFO | train_inner | epoch 004:     70 / 347 loss=8.939, ppl=490.76, wps=26966.7, ups=0.41, wpb=65082.7, bsz=127.1, num_updates=1100, lr=0.000137573, gnorm=0.803, loss_scale=16, train_wall=193, gb_free=9.8, wall=2465
2022-03-12 11:04:00 | INFO | train_inner | epoch 004:    170 / 347 loss=8.786, ppl=441.5, wps=30648.1, ups=0.47, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.84, loss_scale=16, train_wall=190, gb_free=9.8, wall=2678
2022-03-12 11:07:36 | INFO | train_inner | epoch 004:    270 / 347 loss=8.636, ppl=397.8, wps=30339.8, ups=0.46, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.873, loss_scale=32, train_wall=192, gb_free=9.8, wall=2894
2022-03-12 11:09:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:10:41 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.46 | ppl 352.05 | wps 53561 | wpb 511.9 | bsz 1 | num_updates 1376 | best_loss 8.46
2022-03-12 11:10:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1376 updates
2022-03-12 11:10:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:10:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 4 @ 1376 updates, score 8.46) (writing took 2.1370510150154587 seconds)
2022-03-12 11:10:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-12 11:10:43 | INFO | train | epoch 004 | loss 8.709 | ppl 418.41 | wps 29364.3 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 1376 | lr 0.000172066 | gnorm 0.855 | loss_scale 32 | train_wall 661 | gb_free 9.8 | wall 3082
2022-03-12 11:10:43 | INFO | fairseq.trainer | begin training epoch 5
2022-03-12 11:10:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:11:36 | INFO | train_inner | epoch 005:     24 / 347 loss=8.499, ppl=361.88, wps=27163.2, ups=0.42, wpb=65082.7, bsz=127.1, num_updates=1400, lr=0.000175065, gnorm=0.884, loss_scale=32, train_wall=192, gb_free=9.8, wall=3134
2022-03-12 11:15:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:15:15 | INFO | train_inner | epoch 005:    125 / 347 loss=8.344, ppl=324.96, wps=29882.2, ups=0.46, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.92, loss_scale=32, train_wall=195, gb_free=9.8, wall=3353
2022-03-12 11:18:51 | INFO | train_inner | epoch 005:    225 / 347 loss=8.232, ppl=300.73, wps=30292, ups=0.46, wpb=65533.2, bsz=128, num_updates=1600, lr=0.00020006, gnorm=0.93, loss_scale=32, train_wall=193, gb_free=9.8, wall=3570
2022-03-12 11:20:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:22:31 | INFO | train_inner | epoch 005:    326 / 347 loss=8.12, ppl=278.18, wps=29855.4, ups=0.46, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.911, loss_scale=32, train_wall=196, gb_free=9.8, wall=3789
2022-03-12 11:23:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:23:35 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.034 | ppl 262.07 | wps 53953 | wpb 511.9 | bsz 1 | num_updates 1721 | best_loss 8.034
2022-03-12 11:23:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1721 updates
2022-03-12 11:23:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:23:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:23:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 5 @ 1721 updates, score 8.034) (writing took 2.069911331986077 seconds)
2022-03-12 11:23:37 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-12 11:23:37 | INFO | train | epoch 005 | loss 8.233 | ppl 300.93 | wps 29147.3 | ups 0.45 | wpb 65404.6 | bsz 127.7 | num_updates 1721 | lr 0.000215182 | gnorm 0.911 | loss_scale 32 | train_wall 669 | gb_free 9.8 | wall 3856
2022-03-12 11:23:38 | INFO | fairseq.trainer | begin training epoch 6
2022-03-12 11:23:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:25:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:26:31 | INFO | train_inner | epoch 006:     80 / 347 loss=7.961, ppl=249.12, wps=27117.3, ups=0.42, wpb=65082.7, bsz=127.1, num_updates=1800, lr=0.000225055, gnorm=0.914, loss_scale=32, train_wall=193, gb_free=9.8, wall=4029
2022-03-12 11:30:06 | INFO | train_inner | epoch 006:    180 / 347 loss=7.854, ppl=231.36, wps=30486.8, ups=0.47, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.894, loss_scale=32, train_wall=191, gb_free=9.8, wall=4244
2022-03-12 11:30:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:33:42 | INFO | train_inner | epoch 006:    281 / 347 loss=7.758, ppl=216.53, wps=30306.5, ups=0.46, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.914, loss_scale=32, train_wall=192, gb_free=9.8, wall=4460
2022-03-12 11:35:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:36:18 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.67 | ppl 203.65 | wps 55216.1 | wpb 511.9 | bsz 1 | num_updates 2065 | best_loss 7.67
2022-03-12 11:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2065 updates
2022-03-12 11:36:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:36:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 6 @ 2065 updates, score 7.67) (writing took 2.0948124429851305 seconds)
2022-03-12 11:36:20 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-12 11:36:20 | INFO | train | epoch 006 | loss 7.81 | ppl 224.44 | wps 29502.4 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 2065 | lr 0.000258173 | gnorm 0.909 | loss_scale 32 | train_wall 658 | gb_free 9.8 | wall 4619
2022-03-12 11:36:20 | INFO | fairseq.trainer | begin training epoch 7
2022-03-12 11:36:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:37:33 | INFO | train_inner | epoch 007:     35 / 347 loss=7.626, ppl=197.59, wps=28175.4, ups=0.43, wpb=65085.4, bsz=127.1, num_updates=2100, lr=0.000262548, gnorm=0.91, loss_scale=32, train_wall=185, gb_free=9.8, wall=4691
2022-03-12 11:39:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:41:04 | INFO | train_inner | epoch 007:    136 / 347 loss=7.518, ppl=183.29, wps=31076.2, ups=0.47, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.881, loss_scale=32, train_wall=187, gb_free=9.8, wall=4902
2022-03-12 11:44:31 | INFO | train_inner | epoch 007:    236 / 347 loss=7.433, ppl=172.81, wps=31564.4, ups=0.48, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.875, loss_scale=64, train_wall=184, gb_free=9.8, wall=5110
2022-03-12 11:44:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:48:02 | INFO | train_inner | epoch 007:    337 / 347 loss=7.352, ppl=163.41, wps=31063.2, ups=0.47, wpb=65533.2, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.862, loss_scale=32, train_wall=187, gb_free=9.8, wall=5321
2022-03-12 11:48:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:48:42 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.359 | ppl 164.21 | wps 55252.1 | wpb 511.9 | bsz 1 | num_updates 2410 | best_loss 7.359
2022-03-12 11:48:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2410 updates
2022-03-12 11:48:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:48:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 11:48:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 7 @ 2410 updates, score 7.359) (writing took 2.091085274994839 seconds)
2022-03-12 11:48:45 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-12 11:48:45 | INFO | train | epoch 007 | loss 7.444 | ppl 174.13 | wps 30313 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 2410 | lr 0.00030129 | gnorm 0.878 | loss_scale 32 | train_wall 640 | gb_free 9.8 | wall 5363
2022-03-12 11:48:45 | INFO | fairseq.trainer | begin training epoch 8
2022-03-12 11:48:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:50:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:51:54 | INFO | train_inner | epoch 008:     91 / 347 loss=7.207, ppl=147.79, wps=28080, ups=0.43, wpb=65082.7, bsz=127.1, num_updates=2500, lr=0.000312538, gnorm=0.867, loss_scale=32, train_wall=185, gb_free=9.8, wall=5553
2022-03-12 11:54:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:55:24 | INFO | train_inner | epoch 008:    192 / 347 loss=7.142, ppl=141.29, wps=31222, ups=0.48, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.851, loss_scale=32, train_wall=186, gb_free=9.8, wall=5763
2022-03-12 11:58:52 | INFO | train_inner | epoch 008:    292 / 347 loss=7.073, ppl=134.6, wps=31464.8, ups=0.48, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.844, loss_scale=32, train_wall=185, gb_free=9.8, wall=5971
2022-03-12 11:59:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:00:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:01:09 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.058 | ppl 133.3 | wps 54110.3 | wpb 511.9 | bsz 1 | num_updates 2754 | best_loss 7.058
2022-03-12 12:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 2754 updates
2022-03-12 12:01:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:01:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:01:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 8 @ 2754 updates, score 7.058) (writing took 2.072694920003414 seconds)
2022-03-12 12:01:11 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-12 12:01:11 | INFO | train | epoch 008 | loss 7.118 | ppl 138.9 | wps 30128.9 | ups 0.46 | wpb 65404.2 | bsz 127.7 | num_updates 2754 | lr 0.000344281 | gnorm 0.847 | loss_scale 32 | train_wall 642 | gb_free 9.8 | wall 6110
2022-03-12 12:01:11 | INFO | fairseq.trainer | begin training epoch 9
2022-03-12 12:01:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:02:50 | INFO | train_inner | epoch 009:     46 / 347 loss=6.964, ppl=124.85, wps=27390.4, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=2800, lr=0.00035003, gnorm=0.831, loss_scale=32, train_wall=191, gb_free=9.8, wall=6208
2022-03-12 12:04:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:06:24 | INFO | train_inner | epoch 009:    147 / 347 loss=6.866, ppl=116.62, wps=30578.2, ups=0.47, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.817, loss_scale=32, train_wall=191, gb_free=9.8, wall=6423
2022-03-12 12:07:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:09:56 | INFO | train_inner | epoch 009:    248 / 347 loss=6.807, ppl=111.96, wps=30890.2, ups=0.47, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.816, loss_scale=16, train_wall=188, gb_free=9.8, wall=6635
2022-03-12 12:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:13:43 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.795 | ppl 111.08 | wps 54247.1 | wpb 511.9 | bsz 1 | num_updates 3099 | best_loss 6.795
2022-03-12 12:13:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3099 updates
2022-03-12 12:13:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:13:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:13:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 9 @ 3099 updates, score 6.795) (writing took 2.1201876740087755 seconds)
2022-03-12 12:13:45 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-12 12:13:45 | INFO | train | epoch 009 | loss 6.821 | ppl 113.09 | wps 29937.7 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 3099 | lr 0.000387398 | gnorm 0.816 | loss_scale 32 | train_wall 649 | gb_free 9.8 | wall 6863
2022-03-12 12:13:45 | INFO | fairseq.trainer | begin training epoch 10
2022-03-12 12:13:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:13:47 | INFO | train_inner | epoch 010:      1 / 347 loss=6.753, ppl=107.89, wps=28208.9, ups=0.43, wpb=65082.7, bsz=127.1, num_updates=3100, lr=0.000387523, gnorm=0.799, loss_scale=32, train_wall=184, gb_free=9.8, wall=6866
2022-03-12 12:16:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:17:18 | INFO | train_inner | epoch 010:    102 / 347 loss=6.616, ppl=98.11, wps=31033.5, ups=0.47, wpb=65536, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.775, loss_scale=32, train_wall=187, gb_free=9.8, wall=7077
2022-03-12 12:19:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:20:50 | INFO | train_inner | epoch 010:    203 / 347 loss=6.596, ppl=96.76, wps=30905.1, ups=0.47, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.767, loss_scale=16, train_wall=188, gb_free=9.8, wall=7289
2022-03-12 12:24:19 | INFO | train_inner | epoch 010:    303 / 347 loss=6.552, ppl=93.81, wps=31447.9, ups=0.48, wpb=65533.2, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.752, loss_scale=32, train_wall=185, gb_free=9.8, wall=7497
2022-03-12 12:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:26:10 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.616 | ppl 98.06 | wps 55008.5 | wpb 511.9 | bsz 1 | num_updates 3444 | best_loss 6.616
2022-03-12 12:26:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3444 updates
2022-03-12 12:26:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:26:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:26:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 10 @ 3444 updates, score 6.616) (writing took 2.2548426020075567 seconds)
2022-03-12 12:26:12 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-12 12:26:12 | INFO | train | epoch 010 | loss 6.58 | ppl 95.69 | wps 30190.3 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 3444 | lr 0.000430514 | gnorm 0.761 | loss_scale 32 | train_wall 643 | gb_free 9.8 | wall 7611
2022-03-12 12:26:12 | INFO | fairseq.trainer | begin training epoch 11
2022-03-12 12:26:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:28:10 | INFO | train_inner | epoch 011:     56 / 347 loss=6.468, ppl=88.51, wps=28130.2, ups=0.43, wpb=65085.4, bsz=127.1, num_updates=3500, lr=0.000437513, gnorm=0.726, loss_scale=32, train_wall=185, gb_free=9.8, wall=7729
2022-03-12 12:28:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:31:40 | INFO | train_inner | epoch 011:    157 / 347 loss=6.412, ppl=85.13, wps=31177.5, ups=0.48, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.735, loss_scale=32, train_wall=187, gb_free=9.8, wall=7939
2022-03-12 12:32:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:33:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:35:13 | INFO | train_inner | epoch 011:    259 / 347 loss=6.39, ppl=83.84, wps=30791.3, ups=0.47, wpb=65533.2, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.692, loss_scale=16, train_wall=189, gb_free=9.8, wall=8152
2022-03-12 12:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:38:35 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.506 | ppl 90.91 | wps 55262.2 | wpb 511.9 | bsz 1 | num_updates 3788 | best_loss 6.506
2022-03-12 12:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 3788 updates
2022-03-12 12:38:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:38:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 11 @ 3788 updates, score 6.506) (writing took 2.0971724780101795 seconds)
2022-03-12 12:38:38 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-12 12:38:38 | INFO | train | epoch 011 | loss 6.399 | ppl 84.36 | wps 30194.8 | ups 0.46 | wpb 65404.2 | bsz 127.7 | num_updates 3788 | lr 0.000473505 | gnorm 0.71 | loss_scale 32 | train_wall 641 | gb_free 9.8 | wall 8356
2022-03-12 12:38:38 | INFO | fairseq.trainer | begin training epoch 12
2022-03-12 12:38:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:39:02 | INFO | train_inner | epoch 012:     12 / 347 loss=6.365, ppl=82.45, wps=28392.4, ups=0.44, wpb=65085.4, bsz=127.1, num_updates=3800, lr=0.000475005, gnorm=0.694, loss_scale=32, train_wall=183, gb_free=9.8, wall=8381
2022-03-12 12:42:32 | INFO | train_inner | epoch 012:    112 / 347 loss=6.257, ppl=76.47, wps=31322.7, ups=0.48, wpb=65533.2, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.679, loss_scale=64, train_wall=186, gb_free=9.8, wall=8590
2022-03-12 12:42:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:46:03 | INFO | train_inner | epoch 012:    213 / 347 loss=6.251, ppl=76.15, wps=31034.6, ups=0.47, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.655, loss_scale=32, train_wall=187, gb_free=9.8, wall=8801
2022-03-12 12:47:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:47:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:49:36 | INFO | train_inner | epoch 012:    315 / 347 loss=6.239, ppl=75.51, wps=30758.3, ups=0.47, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.642, loss_scale=16, train_wall=189, gb_free=9.8, wall=9014
2022-03-12 12:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:51:02 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.396 | ppl 84.24 | wps 54990.1 | wpb 511.9 | bsz 1 | num_updates 4132 | best_loss 6.396
2022-03-12 12:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4132 updates
2022-03-12 12:51:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:51:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 12:51:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 12 @ 4132 updates, score 6.396) (writing took 2.1904750800167676 seconds)
2022-03-12 12:51:04 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-12 12:51:04 | INFO | train | epoch 012 | loss 6.249 | ppl 76.08 | wps 30147.1 | ups 0.46 | wpb 65404.2 | bsz 127.7 | num_updates 4132 | lr 0.000491949 | gnorm 0.658 | loss_scale 16 | train_wall 642 | gb_free 9.8 | wall 9102
2022-03-12 12:51:04 | INFO | fairseq.trainer | begin training epoch 13
2022-03-12 12:51:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:53:25 | INFO | train_inner | epoch 013:     68 / 347 loss=6.148, ppl=70.9, wps=28361.3, ups=0.44, wpb=65085.4, bsz=127.1, num_updates=4200, lr=0.00048795, gnorm=0.638, loss_scale=32, train_wall=183, gb_free=9.8, wall=9244
2022-03-12 12:56:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:56:57 | INFO | train_inner | epoch 013:    169 / 347 loss=6.113, ppl=69.22, wps=31017.4, ups=0.47, wpb=65533.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.614, loss_scale=32, train_wall=188, gb_free=9.8, wall=9455
2022-03-12 13:00:25 | INFO | train_inner | epoch 013:    269 / 347 loss=6.12, ppl=69.54, wps=31500.1, ups=0.48, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.598, loss_scale=32, train_wall=185, gb_free=9.8, wall=9663
2022-03-12 13:01:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 13:01:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:03:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.309 | ppl 79.3 | wps 55335.1 | wpb 511.9 | bsz 1 | num_updates 4476 | best_loss 6.309
2022-03-12 13:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 4476 updates
2022-03-12 13:03:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:03:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:03:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 13 @ 4476 updates, score 6.309) (writing took 2.1117170130019076 seconds)
2022-03-12 13:03:27 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-12 13:03:27 | INFO | train | epoch 013 | loss 6.112 | ppl 69.18 | wps 30269.7 | ups 0.46 | wpb 65404.2 | bsz 127.7 | num_updates 4476 | lr 0.000472667 | gnorm 0.609 | loss_scale 16 | train_wall 639 | gb_free 9.8 | wall 9846
2022-03-12 13:03:27 | INFO | fairseq.trainer | begin training epoch 14
2022-03-12 13:03:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:04:16 | INFO | train_inner | epoch 014:     24 / 347 loss=6.075, ppl=67.41, wps=28115.7, ups=0.43, wpb=65082.7, bsz=127.1, num_updates=4500, lr=0.000471405, gnorm=0.592, loss_scale=16, train_wall=185, gb_free=9.8, wall=9895
2022-03-12 13:07:45 | INFO | train_inner | epoch 014:    124 / 347 loss=5.983, ppl=63.23, wps=31437.5, ups=0.48, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.576, loss_scale=32, train_wall=185, gb_free=9.8, wall=10103
2022-03-12 13:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 13:11:16 | INFO | train_inner | epoch 014:    225 / 347 loss=5.999, ppl=63.96, wps=31001.7, ups=0.47, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.581, loss_scale=32, train_wall=188, gb_free=9.8, wall=10315
2022-03-12 13:14:45 | INFO | train_inner | epoch 014:    325 / 347 loss=5.999, ppl=63.95, wps=31310.6, ups=0.48, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.563, loss_scale=32, train_wall=186, gb_free=9.8, wall=10524
2022-03-12 13:15:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 13:15:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:15:50 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.253 | ppl 76.26 | wps 55455.3 | wpb 511.9 | bsz 1 | num_updates 4821 | best_loss 6.253
2022-03-12 13:15:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 4821 updates
2022-03-12 13:15:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:15:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:15:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 14 @ 4821 updates, score 6.253) (writing took 2.072410728986142 seconds)
2022-03-12 13:15:52 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-12 13:15:52 | INFO | train | epoch 014 | loss 5.993 | ppl 63.68 | wps 30274.5 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 4821 | lr 0.00045544 | gnorm 0.576 | loss_scale 32 | train_wall 642 | gb_free 9.8 | wall 10591
2022-03-12 13:15:52 | INFO | fairseq.trainer | begin training epoch 15
2022-03-12 13:15:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:16:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:18:40 | INFO | train_inner | epoch 015:     80 / 347 loss=5.901, ppl=59.77, wps=27726.3, ups=0.43, wpb=65085.4, bsz=127.1, num_updates=4900, lr=0.000451754, gnorm=0.57, loss_scale=16, train_wall=188, gb_free=9.8, wall=10759
2022-03-12 13:22:13 | INFO | train_inner | epoch 015:    180 / 347 loss=5.889, ppl=59.28, wps=30777.6, ups=0.47, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.555, loss_scale=32, train_wall=190, gb_free=9.8, wall=10972
2022-03-12 13:23:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:25:49 | INFO | train_inner | epoch 015:    281 / 347 loss=5.896, ppl=59.55, wps=30413.7, ups=0.46, wpb=65533.2, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.549, loss_scale=16, train_wall=192, gb_free=9.8, wall=11187
2022-03-12 13:28:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:28:29 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.199 | ppl 73.46 | wps 53254.9 | wpb 511.9 | bsz 1 | num_updates 5166 | best_loss 6.199
2022-03-12 13:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5166 updates
2022-03-12 13:28:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:28:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 15 @ 5166 updates, score 6.199) (writing took 2.1140251510078087 seconds)
2022-03-12 13:28:31 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-12 13:28:31 | INFO | train | epoch 015 | loss 5.891 | ppl 59.33 | wps 29747.8 | ups 0.45 | wpb 65404.6 | bsz 127.7 | num_updates 5166 | lr 0.00043997 | gnorm 0.558 | loss_scale 32 | train_wall 654 | gb_free 9.8 | wall 11349
2022-03-12 13:28:31 | INFO | fairseq.trainer | begin training epoch 16
2022-03-12 13:28:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:28:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:29:45 | INFO | train_inner | epoch 016:     35 / 347 loss=5.861, ppl=58.11, wps=27463.6, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=5200, lr=0.000438529, gnorm=0.567, loss_scale=16, train_wall=190, gb_free=9.8, wall=11424
2022-03-12 13:33:18 | INFO | train_inner | epoch 016:    135 / 347 loss=5.787, ppl=55.22, wps=30768, ups=0.47, wpb=65533.2, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.547, loss_scale=32, train_wall=189, gb_free=9.8, wall=11637
2022-03-12 13:36:51 | INFO | train_inner | epoch 016:    235 / 347 loss=5.811, ppl=56.13, wps=30826.2, ups=0.47, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.54, loss_scale=32, train_wall=189, gb_free=9.8, wall=11850
2022-03-12 13:37:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 13:40:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:40:29 | INFO | train_inner | epoch 016:    337 / 347 loss=5.819, ppl=56.44, wps=30142.1, ups=0.46, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.545, loss_scale=16, train_wall=194, gb_free=9.8, wall=12067
2022-03-12 13:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:41:10 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.158 | ppl 71.42 | wps 53485.3 | wpb 511.9 | bsz 1 | num_updates 5510 | best_loss 6.158
2022-03-12 13:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 5510 updates
2022-03-12 13:41:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:41:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:41:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 16 @ 5510 updates, score 6.158) (writing took 2.0702645169803873 seconds)
2022-03-12 13:41:12 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-12 13:41:12 | INFO | train | epoch 016 | loss 5.803 | ppl 55.82 | wps 29577.4 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 5510 | lr 0.000426014 | gnorm 0.547 | loss_scale 16 | train_wall 656 | gb_free 9.8 | wall 12110
2022-03-12 13:41:12 | INFO | fairseq.trainer | begin training epoch 17
2022-03-12 13:41:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:44:24 | INFO | train_inner | epoch 017:     90 / 347 loss=5.708, ppl=52.29, wps=27647.1, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=5600, lr=0.000422577, gnorm=0.549, loss_scale=16, train_wall=188, gb_free=9.8, wall=12302
2022-03-12 13:47:56 | INFO | train_inner | epoch 017:    190 / 347 loss=5.721, ppl=52.76, wps=30950.8, ups=0.47, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.549, loss_scale=32, train_wall=188, gb_free=9.8, wall=12514
2022-03-12 13:48:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:51:31 | INFO | train_inner | epoch 017:    291 / 347 loss=5.744, ppl=53.59, wps=30387.5, ups=0.46, wpb=65533.2, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.531, loss_scale=16, train_wall=192, gb_free=9.8, wall=12730
2022-03-12 13:53:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:53:48 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.127 | ppl 69.87 | wps 53565 | wpb 511.9 | bsz 1 | num_updates 5856 | best_loss 6.127
2022-03-12 13:53:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 5856 updates
2022-03-12 13:53:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:53:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 13:53:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 17 @ 5856 updates, score 6.127) (writing took 2.0649530260125175 seconds)
2022-03-12 13:53:50 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-12 13:53:50 | INFO | train | epoch 017 | loss 5.727 | ppl 52.96 | wps 29824.8 | ups 0.46 | wpb 65405 | bsz 127.7 | num_updates 5856 | lr 0.000413237 | gnorm 0.542 | loss_scale 32 | train_wall 654 | gb_free 9.8 | wall 12869
2022-03-12 13:53:50 | INFO | fairseq.trainer | begin training epoch 18
2022-03-12 13:53:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:55:24 | INFO | train_inner | epoch 018:     44 / 347 loss=5.695, ppl=51.8, wps=28030.7, ups=0.43, wpb=65085.4, bsz=127.1, num_updates=5900, lr=0.000411693, gnorm=0.544, loss_scale=32, train_wall=185, gb_free=9.8, wall=12962
2022-03-12 13:57:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:58:59 | INFO | train_inner | epoch 018:    145 / 347 loss=5.639, ppl=49.84, wps=30476.7, ups=0.47, wpb=65533.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.542, loss_scale=16, train_wall=191, gb_free=9.8, wall=13177
2022-03-12 14:02:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:02:32 | INFO | train_inner | epoch 018:    246 / 347 loss=5.675, ppl=51.09, wps=30683, ups=0.47, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.541, loss_scale=16, train_wall=190, gb_free=9.8, wall=13391
2022-03-12 14:06:02 | INFO | train_inner | epoch 018:    346 / 347 loss=5.679, ppl=51.24, wps=31233, ups=0.48, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.532, loss_scale=16, train_wall=186, gb_free=9.8, wall=13600
2022-03-12 14:06:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:06:24 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.103 | ppl 68.73 | wps 53582.1 | wpb 511.9 | bsz 1 | num_updates 6201 | best_loss 6.103
2022-03-12 14:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 6201 updates
2022-03-12 14:06:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:06:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 18 @ 6201 updates, score 6.103) (writing took 2.147893635003129 seconds)
2022-03-12 14:06:26 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-12 14:06:26 | INFO | train | epoch 018 | loss 5.659 | ppl 50.51 | wps 29855 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 6201 | lr 0.000401577 | gnorm 0.54 | loss_scale 16 | train_wall 651 | gb_free 9.8 | wall 13625
2022-03-12 14:06:26 | INFO | fairseq.trainer | begin training epoch 19
2022-03-12 14:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:09:55 | INFO | train_inner | epoch 019:     99 / 347 loss=5.555, ppl=47.02, wps=27921.7, ups=0.43, wpb=65082.7, bsz=127.1, num_updates=6300, lr=0.00039841, gnorm=0.539, loss_scale=32, train_wall=186, gb_free=9.8, wall=13834
2022-03-12 14:10:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:13:29 | INFO | train_inner | epoch 019:    200 / 347 loss=5.597, ppl=48.4, wps=30621.2, ups=0.47, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.538, loss_scale=16, train_wall=190, gb_free=9.8, wall=14048
2022-03-12 14:16:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:17:02 | INFO | train_inner | epoch 019:    301 / 347 loss=5.623, ppl=49.27, wps=30743.8, ups=0.47, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.543, loss_scale=16, train_wall=190, gb_free=9.8, wall=14261
2022-03-12 14:18:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:18:59 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.077 | ppl 67.5 | wps 54833.7 | wpb 511.9 | bsz 1 | num_updates 6546 | best_loss 6.077
2022-03-12 14:18:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 6546 updates
2022-03-12 14:18:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:19:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:19:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 19 @ 6546 updates, score 6.077) (writing took 2.182786688004853 seconds)
2022-03-12 14:19:01 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-12 14:19:01 | INFO | train | epoch 019 | loss 5.598 | ppl 48.43 | wps 29883 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 6546 | lr 0.000390852 | gnorm 0.54 | loss_scale 16 | train_wall 651 | gb_free 9.8 | wall 14380
2022-03-12 14:19:01 | INFO | fairseq.trainer | begin training epoch 20
2022-03-12 14:19:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:20:57 | INFO | train_inner | epoch 020:     54 / 347 loss=5.555, ppl=47.03, wps=27778.2, ups=0.43, wpb=65085.4, bsz=127.1, num_updates=6600, lr=0.000389249, gnorm=0.544, loss_scale=16, train_wall=188, gb_free=9.8, wall=14495
2022-03-12 14:24:27 | INFO | train_inner | epoch 020:    154 / 347 loss=5.53, ppl=46.19, wps=31098.1, ups=0.47, wpb=65533.2, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.534, loss_scale=32, train_wall=187, gb_free=9.8, wall=14706
2022-03-12 14:26:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 14:28:03 | INFO | train_inner | epoch 020:    255 / 347 loss=5.556, ppl=47.06, wps=30397.4, ups=0.46, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.538, loss_scale=32, train_wall=192, gb_free=9.8, wall=14921
2022-03-12 14:30:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:31:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:31:37 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.071 | ppl 67.21 | wps 54178.3 | wpb 511.9 | bsz 1 | num_updates 6891 | best_loss 6.071
2022-03-12 14:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 6891 updates
2022-03-12 14:31:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:31:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:31:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 20 @ 6891 updates, score 6.071) (writing took 2.2260753749869764 seconds)
2022-03-12 14:31:39 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-12 14:31:39 | INFO | train | epoch 020 | loss 5.543 | ppl 46.62 | wps 29779.4 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 6891 | lr 0.000380942 | gnorm 0.536 | loss_scale 16 | train_wall 653 | gb_free 9.8 | wall 15138
2022-03-12 14:31:39 | INFO | fairseq.trainer | begin training epoch 21
2022-03-12 14:31:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:31:59 | INFO | train_inner | epoch 021:      9 / 347 loss=5.564, ppl=47.3, wps=27588, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=6900, lr=0.000380693, gnorm=0.532, loss_scale=16, train_wall=189, gb_free=9.8, wall=15157
2022-03-12 14:35:32 | INFO | train_inner | epoch 021:    109 / 347 loss=5.457, ppl=43.92, wps=30719.6, ups=0.47, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.536, loss_scale=32, train_wall=190, gb_free=9.8, wall=15371
2022-03-12 14:39:05 | INFO | train_inner | epoch 021:    209 / 347 loss=5.502, ppl=45.31, wps=30765.2, ups=0.47, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.539, loss_scale=32, train_wall=190, gb_free=9.8, wall=15584
2022-03-12 14:39:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:42:38 | INFO | train_inner | epoch 021:    310 / 347 loss=5.513, ppl=45.68, wps=30749.5, ups=0.47, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.537, loss_scale=16, train_wall=190, gb_free=9.8, wall=15797
2022-03-12 14:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:44:16 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.054 | ppl 66.44 | wps 54688.7 | wpb 511.9 | bsz 1 | num_updates 7237 | best_loss 6.054
2022-03-12 14:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 7237 updates
2022-03-12 14:44:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:44:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:44:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 21 @ 7237 updates, score 6.054) (writing took 2.2347056590078864 seconds)
2022-03-12 14:44:18 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-12 14:44:18 | INFO | train | epoch 021 | loss 5.493 | ppl 45.05 | wps 29803.8 | ups 0.46 | wpb 65405 | bsz 127.7 | num_updates 7237 | lr 0.000371724 | gnorm 0.538 | loss_scale 32 | train_wall 655 | gb_free 9.8 | wall 15897
2022-03-12 14:44:18 | INFO | fairseq.trainer | begin training epoch 22
2022-03-12 14:44:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:46:34 | INFO | train_inner | epoch 022:     63 / 347 loss=5.451, ppl=43.76, wps=27664.9, ups=0.43, wpb=65082.7, bsz=127.1, num_updates=7300, lr=0.000370117, gnorm=0.542, loss_scale=32, train_wall=189, gb_free=9.8, wall=16032
2022-03-12 14:48:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 14:50:07 | INFO | train_inner | epoch 022:    164 / 347 loss=5.432, ppl=43.18, wps=30640.7, ups=0.47, wpb=65533.2, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.538, loss_scale=32, train_wall=190, gb_free=9.8, wall=16246
2022-03-12 14:51:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:53:40 | INFO | train_inner | epoch 022:    265 / 347 loss=5.458, ppl=43.95, wps=30789.2, ups=0.47, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.531, loss_scale=16, train_wall=189, gb_free=9.8, wall=16459
2022-03-12 14:56:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:56:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:56:53 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.043 | ppl 65.93 | wps 53158.9 | wpb 511.9 | bsz 1 | num_updates 7581 | best_loss 6.043
2022-03-12 14:56:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 7581 updates
2022-03-12 14:56:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:56:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 14:56:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 22 @ 7581 updates, score 6.043) (writing took 2.121376210998278 seconds)
2022-03-12 14:56:55 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-12 14:56:55 | INFO | train | epoch 022 | loss 5.448 | ppl 43.64 | wps 29733 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 7581 | lr 0.000363192 | gnorm 0.538 | loss_scale 16 | train_wall 652 | gb_free 9.8 | wall 16654
2022-03-12 14:56:55 | INFO | fairseq.trainer | begin training epoch 23
2022-03-12 14:56:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:57:36 | INFO | train_inner | epoch 023:     19 / 347 loss=5.46, ppl=44.01, wps=27604.8, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=7600, lr=0.000362738, gnorm=0.547, loss_scale=16, train_wall=188, gb_free=9.8, wall=16695
2022-03-12 15:01:08 | INFO | train_inner | epoch 023:    119 / 347 loss=5.374, ppl=41.46, wps=30965.3, ups=0.47, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.54, loss_scale=16, train_wall=188, gb_free=9.8, wall=16906
2022-03-12 15:04:38 | INFO | train_inner | epoch 023:    219 / 347 loss=5.403, ppl=42.3, wps=31212.8, ups=0.48, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.543, loss_scale=32, train_wall=187, gb_free=9.8, wall=17116
2022-03-12 15:05:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:08:12 | INFO | train_inner | epoch 023:    320 / 347 loss=5.436, ppl=43.28, wps=30591.1, ups=0.47, wpb=65533.2, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.543, loss_scale=32, train_wall=191, gb_free=9.8, wall=17330
2022-03-12 15:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:09:29 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.038 | ppl 65.71 | wps 53948.2 | wpb 511.9 | bsz 1 | num_updates 7927 | best_loss 6.038
2022-03-12 15:09:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 7927 updates
2022-03-12 15:09:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:09:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:09:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 23 @ 7927 updates, score 6.038) (writing took 2.1185962969902903 seconds)
2022-03-12 15:09:31 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-12 15:09:31 | INFO | train | epoch 023 | loss 5.406 | ppl 42.4 | wps 29940.2 | ups 0.46 | wpb 65405 | bsz 127.7 | num_updates 7927 | lr 0.000355178 | gnorm 0.543 | loss_scale 32 | train_wall 651 | gb_free 9.8 | wall 17409
2022-03-12 15:09:31 | INFO | fairseq.trainer | begin training epoch 24
2022-03-12 15:09:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:10:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:12:08 | INFO | train_inner | epoch 024:     74 / 347 loss=5.361, ppl=41.11, wps=27512.4, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=8000, lr=0.000353553, gnorm=0.544, loss_scale=32, train_wall=190, gb_free=9.8, wall=17567
2022-03-12 15:15:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:15:43 | INFO | train_inner | epoch 024:    175 / 347 loss=5.359, ppl=41.03, wps=30527.5, ups=0.47, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.544, loss_scale=32, train_wall=191, gb_free=9.8, wall=17782
2022-03-12 15:19:18 | INFO | train_inner | epoch 024:    275 / 347 loss=5.388, ppl=41.86, wps=30555.3, ups=0.47, wpb=65533.2, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.542, loss_scale=32, train_wall=191, gb_free=9.8, wall=17996
2022-03-12 15:20:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:21:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:22:09 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.032 | ppl 65.44 | wps 54861 | wpb 511.9 | bsz 1 | num_updates 8271 | best_loss 6.032
2022-03-12 15:22:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 8271 updates
2022-03-12 15:22:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:22:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:22:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 24 @ 8271 updates, score 6.032) (writing took 2.0889241179975215 seconds)
2022-03-12 15:22:11 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-12 15:22:11 | INFO | train | epoch 024 | loss 5.366 | ppl 41.25 | wps 29597.7 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 8271 | lr 0.000347713 | gnorm 0.543 | loss_scale 32 | train_wall 656 | gb_free 9.8 | wall 18170
2022-03-12 15:22:11 | INFO | fairseq.trainer | begin training epoch 25
2022-03-12 15:22:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:23:12 | INFO | train_inner | epoch 025:     29 / 347 loss=5.356, ppl=40.97, wps=27723.4, ups=0.43, wpb=65082.7, bsz=127.1, num_updates=8300, lr=0.000347105, gnorm=0.544, loss_scale=32, train_wall=188, gb_free=9.8, wall=18231
2022-03-12 15:25:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:26:46 | INFO | train_inner | epoch 025:    130 / 347 loss=5.308, ppl=39.6, wps=30693.3, ups=0.47, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.541, loss_scale=32, train_wall=190, gb_free=9.8, wall=18444
2022-03-12 15:29:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:30:19 | INFO | train_inner | epoch 025:    231 / 347 loss=5.344, ppl=40.62, wps=30735.3, ups=0.47, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.547, loss_scale=32, train_wall=190, gb_free=9.8, wall=18658
2022-03-12 15:31:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:33:55 | INFO | train_inner | epoch 025:    332 / 347 loss=5.357, ppl=40.98, wps=30385.1, ups=0.46, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.543, loss_scale=16, train_wall=192, gb_free=9.8, wall=18873
2022-03-12 15:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:34:46 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.04 | ppl 65.78 | wps 54788.4 | wpb 511.9 | bsz 1 | num_updates 8615 | best_loss 6.032
2022-03-12 15:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 8615 updates
2022-03-12 15:34:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 15:34:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 15:34:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt (epoch 25 @ 8615 updates, score 6.04) (writing took 1.1769084520055912 seconds)
2022-03-12 15:34:47 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-12 15:34:47 | INFO | train | epoch 025 | loss 5.332 | ppl 40.27 | wps 29756.4 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 8615 | lr 0.0003407 | gnorm 0.544 | loss_scale 16 | train_wall 653 | gb_free 9.8 | wall 18926
2022-03-12 15:34:47 | INFO | fairseq.trainer | begin training epoch 26
2022-03-12 15:34:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:37:47 | INFO | train_inner | epoch 026:     85 / 347 loss=5.26, ppl=38.33, wps=28058.9, ups=0.43, wpb=65085.4, bsz=127.1, num_updates=8700, lr=0.000339032, gnorm=0.553, loss_scale=32, train_wall=186, gb_free=9.8, wall=19105
2022-03-12 15:41:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:41:21 | INFO | train_inner | epoch 026:    186 / 347 loss=5.297, ppl=39.31, wps=30536.5, ups=0.47, wpb=65533.2, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.547, loss_scale=32, train_wall=191, gb_free=9.8, wall=19320
2022-03-12 15:44:54 | INFO | train_inner | epoch 026:    286 / 347 loss=5.328, ppl=40.16, wps=30834.5, ups=0.47, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.547, loss_scale=32, train_wall=189, gb_free=9.8, wall=19532
2022-03-12 15:45:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:47:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:47:22 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.027 | ppl 65.23 | wps 55128.6 | wpb 511.9 | bsz 1 | num_updates 8960 | best_loss 6.027
2022-03-12 15:47:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 8960 updates
2022-03-12 15:47:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:47:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:47:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 26 @ 8960 updates, score 6.027) (writing took 2.108280415995978 seconds)
2022-03-12 15:47:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-12 15:47:25 | INFO | train | epoch 026 | loss 5.298 | ppl 39.35 | wps 29791 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 8960 | lr 0.000334077 | gnorm 0.55 | loss_scale 16 | train_wall 653 | gb_free 9.8 | wall 19683
2022-03-12 15:47:25 | INFO | fairseq.trainer | begin training epoch 27
2022-03-12 15:47:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:48:49 | INFO | train_inner | epoch 027:     40 / 347 loss=5.28, ppl=38.85, wps=27662.9, ups=0.43, wpb=65082.7, bsz=127.1, num_updates=9000, lr=0.000333333, gnorm=0.557, loss_scale=16, train_wall=189, gb_free=9.8, wall=19768
2022-03-12 15:52:22 | INFO | train_inner | epoch 027:    140 / 347 loss=5.244, ppl=37.9, wps=30851.6, ups=0.47, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.544, loss_scale=32, train_wall=189, gb_free=9.8, wall=19980
2022-03-12 15:54:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 15:55:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:55:57 | INFO | train_inner | epoch 027:    242 / 347 loss=5.278, ppl=38.8, wps=30394.2, ups=0.46, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.552, loss_scale=16, train_wall=192, gb_free=9.8, wall=20196
2022-03-12 15:59:26 | INFO | train_inner | epoch 027:    342 / 347 loss=5.303, ppl=39.48, wps=31344, ups=0.48, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.552, loss_scale=16, train_wall=186, gb_free=9.8, wall=20405
2022-03-12 15:59:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:59:56 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.023 | ppl 65.02 | wps 55480.3 | wpb 511.9 | bsz 1 | num_updates 9305 | best_loss 6.023
2022-03-12 15:59:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 9305 updates
2022-03-12 15:59:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt
2022-03-12 15:59:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_best.pt (epoch 27 @ 9305 updates, score 6.023) (writing took 2.060740443004761 seconds)
2022-03-12 15:59:58 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-12 15:59:58 | INFO | train | epoch 027 | loss 5.267 | ppl 38.5 | wps 29950.9 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 9305 | lr 0.000327825 | gnorm 0.551 | loss_scale 16 | train_wall 649 | gb_free 9.8 | wall 20437
2022-03-12 15:59:58 | INFO | fairseq.trainer | begin training epoch 28
2022-03-12 15:59:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:03:15 | INFO | train_inner | epoch 028:     95 / 347 loss=5.2, ppl=36.76, wps=28509.3, ups=0.44, wpb=65085.4, bsz=127.1, num_updates=9400, lr=0.000326164, gnorm=0.56, loss_scale=32, train_wall=182, gb_free=9.8, wall=20633
2022-03-12 16:04:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 16:05:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:06:48 | INFO | train_inner | epoch 028:    197 / 347 loss=5.226, ppl=37.43, wps=30707.3, ups=0.47, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.562, loss_scale=16, train_wall=190, gb_free=9.8, wall=20847
2022-03-12 16:10:19 | INFO | train_inner | epoch 028:    297 / 347 loss=5.268, ppl=38.54, wps=31020.1, ups=0.47, wpb=65533.2, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.549, loss_scale=32, train_wall=188, gb_free=9.8, wall=21058
2022-03-12 16:12:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 16:12:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.028 | ppl 65.27 | wps 54775 | wpb 511.9 | bsz 1 | num_updates 9650 | best_loss 6.023
2022-03-12 16:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 9650 updates
2022-03-12 16:12:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 16:12:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 16:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt (epoch 28 @ 9650 updates, score 6.028) (writing took 1.2006123730097897 seconds)
2022-03-12 16:12:27 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-12 16:12:27 | INFO | train | epoch 028 | loss 5.238 | ppl 37.74 | wps 30112.2 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 9650 | lr 0.000321911 | gnorm 0.557 | loss_scale 32 | train_wall 646 | gb_free 9.8 | wall 21186
2022-03-12 16:12:27 | INFO | fairseq.trainer | begin training epoch 29
2022-03-12 16:12:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:14:14 | INFO | train_inner | epoch 029:     50 / 347 loss=5.213, ppl=37.09, wps=27709.1, ups=0.43, wpb=65085.4, bsz=127.1, num_updates=9700, lr=0.000321081, gnorm=0.56, loss_scale=32, train_wall=189, gb_free=9.8, wall=21293
2022-03-12 16:14:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 16:17:48 | INFO | train_inner | epoch 029:    151 / 347 loss=5.197, ppl=36.67, wps=30602.2, ups=0.47, wpb=65533.2, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.563, loss_scale=32, train_wall=190, gb_free=9.8, wall=21507
2022-03-12 16:19:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 16:21:24 | INFO | train_inner | epoch 029:    252 / 347 loss=5.223, ppl=37.35, wps=30439.6, ups=0.46, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.557, loss_scale=32, train_wall=192, gb_free=9.8, wall=21722
2022-03-12 16:23:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 16:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 16:25:05 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.03 | ppl 65.33 | wps 53351.2 | wpb 511.9 | bsz 1 | num_updates 9994 | best_loss 6.023
2022-03-12 16:25:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 9994 updates
2022-03-12 16:25:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 16:25:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 16:25:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt (epoch 29 @ 9994 updates, score 6.03) (writing took 1.2134601160068996 seconds)
2022-03-12 16:25:06 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-12 16:25:06 | INFO | train | epoch 029 | loss 5.211 | ppl 37.04 | wps 29639.3 | ups 0.45 | wpb 65404.2 | bsz 127.7 | num_updates 9994 | lr 0.000316323 | gnorm 0.561 | loss_scale 32 | train_wall 655 | gb_free 9.8 | wall 21945
2022-03-12 16:25:06 | INFO | fairseq.trainer | begin training epoch 30
2022-03-12 16:25:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:25:19 | INFO | train_inner | epoch 030:      6 / 347 loss=5.242, ppl=37.84, wps=27613.6, ups=0.42, wpb=65085.4, bsz=127.1, num_updates=10000, lr=0.000316228, gnorm=0.563, loss_scale=32, train_wall=189, gb_free=9.8, wall=21958
2022-03-12 16:28:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 16:28:54 | INFO | train_inner | epoch 030:    107 / 347 loss=5.143, ppl=35.34, wps=30520.5, ups=0.47, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.564, loss_scale=32, train_wall=191, gb_free=9.8, wall=22173
2022-03-12 16:31:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:32:29 | INFO | train_inner | epoch 030:    208 / 347 loss=5.193, ppl=36.57, wps=30549, ups=0.47, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.563, loss_scale=16, train_wall=191, gb_free=9.8, wall=22387
2022-03-12 16:35:59 | INFO | train_inner | epoch 030:    308 / 347 loss=5.211, ppl=37.04, wps=31174.9, ups=0.48, wpb=65533.2, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.562, loss_scale=32, train_wall=187, gb_free=9.8, wall=22597
2022-03-12 16:37:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 16:37:41 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.036 | ppl 65.64 | wps 55031.3 | wpb 511.9 | bsz 1 | num_updates 10339 | best_loss 6.023
2022-03-12 16:37:41 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-12 16:37:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 10339 updates
2022-03-12 16:37:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 16:37:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt
2022-03-12 16:37:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/en_cross_entropy_dropout_0.1_#1/checkpoint_last.pt (epoch 30 @ 10339 updates, score 6.036) (writing took 1.3119659750373103 seconds)
2022-03-12 16:37:42 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-12 16:37:42 | INFO | train | epoch 030 | loss 5.185 | ppl 36.38 | wps 29865 | ups 0.46 | wpb 65404.6 | bsz 127.7 | num_updates 10339 | lr 0.000311 | gnorm 0.565 | loss_scale 32 | train_wall 652 | gb_free 9.8 | wall 22701
2022-03-12 16:37:42 | INFO | fairseq_cli.train | done training in 22700.0 seconds
