Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 207019729: <w103_fp16_size_0.25_jelinek_0.0_0.1_0.9_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.0_0.1_0.9_#1> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 11:53:17 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 20:37:39 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 20:37:39 2022
Terminated at Sat Mar  5 08:35:25 2022
Results reported at Sat Mar  5 08:35:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.0, 0.1, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321671 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   129411.85 sec.
    Max Memory :                                 8349 MB
    Average Memory :                             2895.87 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11651.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   129466 sec.
    Turnaround time :                            160928 sec.

The output (if any) follows:

2022-03-03 20:37:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.0, 0.1, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 20:37:48 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 20:37:53 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 686/450337 [00:00<01:05, 6835.66it/s]  0%|          | 1370/450337 [00:00<01:13, 6142.70it/s]  0%|          | 1989/450337 [00:00<01:15, 5905.27it/s]  1%|          | 2611/450337 [00:00<01:14, 6013.59it/s]  1%|          | 3289/450337 [00:00<01:11, 6276.59it/s]  1%|          | 3969/450337 [00:00<01:09, 6446.65it/s]  1%|          | 4693/450337 [00:00<01:06, 6698.26it/s]  1%|          | 5467/450337 [00:00<01:03, 7023.67it/s]  1%|▏         | 6172/450337 [00:00<01:04, 6934.60it/s]  2%|▏         | 6867/450337 [00:01<01:10, 6298.72it/s]  2%|▏         | 7542/450337 [00:01<01:08, 6425.13it/s]  2%|▏         | 8194/450337 [00:01<01:10, 6245.22it/s]  2%|▏         | 8826/450337 [00:01<01:11, 6208.21it/s]  2%|▏         | 9464/450337 [00:01<01:10, 6257.03it/s]  2%|▏         | 10131/450337 [00:01<01:09, 6376.74it/s]  2%|▏         | 10772/450337 [00:01<01:10, 6250.86it/s]  3%|▎         | 11400/450337 [00:01<01:11, 6179.17it/s]  3%|▎         | 12096/450337 [00:01<01:08, 6403.16it/s]  3%|▎         | 12739/450337 [00:02<01:10, 6218.39it/s]  3%|▎         | 13391/450337 [00:02<01:09, 6295.39it/s]  3%|▎         | 14043/450337 [00:02<01:08, 6347.47it/s]  3%|▎         | 14692/450337 [00:02<01:08, 6388.14it/s]  3%|▎         | 15357/450337 [00:02<01:07, 6463.34it/s]  4%|▎         | 16005/450337 [00:02<01:10, 6177.74it/s]  4%|▎         | 16626/450337 [00:02<01:10, 6123.02it/s]  4%|▍         | 17289/450337 [00:02<01:09, 6265.25it/s]  4%|▍         | 17955/450337 [00:02<01:07, 6372.28it/s]  4%|▍         | 18594/450337 [00:02<01:09, 6204.97it/s]  4%|▍         | 19451/450337 [00:03<01:02, 6889.36it/s]  4%|▍         | 20144/450337 [00:03<01:05, 6522.16it/s]  5%|▍         | 20803/450337 [00:03<01:06, 6425.86it/s]  5%|▍         | 21450/450337 [00:03<01:10, 6079.53it/s]  5%|▍         | 22113/450337 [00:03<01:08, 6226.87it/s]  5%|▌         | 22808/450337 [00:03<01:06, 6431.22it/s]  5%|▌         | 23478/450337 [00:03<01:05, 6504.97it/s]  5%|▌         | 24261/450337 [00:03<01:01, 6891.06it/s]  6%|▌         | 24993/450337 [00:03<01:00, 7013.99it/s]  6%|▌         | 25698/450337 [00:03<01:02, 6843.49it/s]  6%|▌         | 26386/450337 [00:04<01:05, 6440.63it/s]  6%|▌         | 27037/450337 [00:04<01:07, 6304.99it/s]  6%|▌         | 27672/450337 [00:04<01:09, 6116.68it/s]  6%|▋         | 28359/450337 [00:04<01:06, 6326.33it/s]  6%|▋         | 29067/450337 [00:04<01:04, 6541.39it/s]  7%|▋         | 29725/450337 [00:04<01:06, 6316.96it/s]  7%|▋         | 30362/450337 [00:04<01:06, 6330.26it/s]  7%|▋         | 30998/450337 [00:04<01:10, 5963.98it/s]  7%|▋         | 31674/450337 [00:04<01:07, 6183.65it/s]  7%|▋         | 32298/450337 [00:05<01:11, 5868.20it/s]  7%|▋         | 32922/450337 [00:05<01:09, 5970.59it/s]  7%|▋         | 33525/450337 [00:05<01:09, 5962.16it/s]  8%|▊         | 34150/450337 [00:05<01:08, 6043.41it/s]  8%|▊         | 34891/450337 [00:05<01:04, 6434.54it/s]  8%|▊         | 35538/450337 [00:05<01:07, 6168.87it/s]  8%|▊         | 36171/450337 [00:05<01:06, 6211.88it/s]  8%|▊         | 36796/450337 [00:05<01:06, 6202.68it/s]  8%|▊         | 37419/450337 [00:05<01:11, 5812.44it/s]  8%|▊         | 38015/450337 [00:06<01:10, 5853.63it/s]  9%|▊         | 38633/450337 [00:06<01:09, 5939.41it/s]  9%|▊         | 39296/450337 [00:06<01:06, 6139.58it/s]  9%|▉         | 39926/450337 [00:06<01:06, 6184.96it/s]  9%|▉         | 40611/450337 [00:06<01:04, 6378.40it/s]  9%|▉         | 41269/450337 [00:06<01:03, 6428.53it/s]  9%|▉         | 41914/450337 [00:06<01:07, 6046.94it/s]  9%|▉         | 42525/450337 [00:06<01:08, 5919.52it/s] 10%|▉         | 43121/450337 [00:06<01:08, 5930.44it/s] 10%|▉         | 43818/450337 [00:06<01:05, 6224.36it/s] 10%|▉         | 44444/450337 [00:07<01:05, 6160.06it/s] 10%|█         | 45071/450337 [00:07<01:05, 6188.31it/s] 10%|█         | 45723/450337 [00:07<01:04, 6282.97it/s] 10%|█         | 46474/450337 [00:07<01:00, 6645.40it/s] 10%|█         | 47140/450337 [00:07<01:00, 6620.27it/s] 11%|█         | 48111/450337 [00:07<00:53, 7533.73it/s] 11%|█         | 48866/450337 [00:07<00:55, 7218.02it/s] 11%|█         | 49646/450337 [00:07<00:54, 7384.99it/s] 11%|█         | 50388/450337 [00:07<00:59, 6707.90it/s] 11%|█▏        | 51073/450337 [00:08<01:00, 6620.57it/s] 11%|█▏        | 51744/450337 [00:08<01:00, 6611.51it/s] 12%|█▏        | 52567/450337 [00:08<00:56, 7068.71it/s] 12%|█▏        | 53281/450337 [00:08<00:57, 6870.35it/s] 12%|█▏        | 53974/450337 [00:08<00:59, 6625.37it/s] 12%|█▏        | 54642/450337 [00:08<01:03, 6224.27it/s] 12%|█▏        | 55276/450337 [00:08<01:03, 6255.81it/s] 12%|█▏        | 56014/450337 [00:08<01:00, 6561.33it/s] 13%|█▎        | 56676/450337 [00:08<01:01, 6430.30it/s] 13%|█▎        | 57323/450337 [00:09<01:05, 5959.66it/s] 13%|█▎        | 57995/450337 [00:09<01:03, 6164.70it/s] 13%|█▎        | 58778/450337 [00:09<00:59, 6623.24it/s] 13%|█▎        | 59449/450337 [00:09<01:02, 6240.25it/s] 13%|█▎        | 60083/450337 [00:09<01:02, 6196.05it/s] 13%|█▎        | 60779/450337 [00:09<01:00, 6409.42it/s] 14%|█▎        | 61542/450337 [00:09<00:57, 6751.21it/s] 14%|█▍        | 62223/450337 [00:09<01:00, 6451.00it/s] 14%|█▍        | 62907/450337 [00:09<00:59, 6556.00it/s] 14%|█▍        | 63568/450337 [00:09<01:00, 6363.41it/s] 14%|█▍        | 64209/450337 [00:10<01:01, 6235.79it/s] 14%|█▍        | 64898/450337 [00:10<01:00, 6419.85it/s] 15%|█▍        | 65543/450337 [00:10<01:01, 6221.39it/s] 15%|█▍        | 66176/450337 [00:10<01:01, 6247.86it/s] 15%|█▍        | 66907/450337 [00:10<00:58, 6554.12it/s] 15%|█▌        | 67565/450337 [00:10<01:02, 6115.23it/s] 15%|█▌        | 68184/450337 [00:10<01:03, 5995.75it/s] 15%|█▌        | 68946/450337 [00:10<00:59, 6449.80it/s] 15%|█▌        | 69604/450337 [00:10<00:58, 6486.94it/s] 16%|█▌        | 70258/450337 [00:11<00:58, 6457.49it/s] 16%|█▌        | 70907/450337 [00:11<01:00, 6293.98it/s] 16%|█▌        | 71579/450337 [00:11<00:59, 6416.77it/s] 16%|█▌        | 72224/450337 [00:11<00:59, 6393.22it/s] 16%|█▌        | 73006/450337 [00:11<00:55, 6803.06it/s] 16%|█▋        | 73726/450337 [00:11<00:54, 6918.33it/s] 17%|█▋        | 74547/450337 [00:11<00:51, 7297.59it/s] 17%|█▋        | 75279/450337 [00:11<00:53, 6960.91it/s] 17%|█▋        | 75980/450337 [00:11<00:55, 6795.75it/s] 17%|█▋        | 76663/450337 [00:11<00:55, 6754.03it/s] 17%|█▋        | 77341/450337 [00:12<00:56, 6625.00it/s] 17%|█▋        | 78026/450337 [00:12<00:55, 6683.35it/s] 17%|█▋        | 78696/450337 [00:12<00:58, 6304.08it/s] 18%|█▊        | 79332/450337 [00:12<01:02, 5897.89it/s] 18%|█▊        | 79963/450337 [00:12<01:01, 6006.34it/s] 18%|█▊        | 80570/450337 [00:12<01:01, 5974.82it/s] 18%|█▊        | 81331/450337 [00:12<00:57, 6434.31it/s] 18%|█▊        | 81980/450337 [00:12<00:58, 6257.77it/s] 18%|█▊        | 82640/450337 [00:12<00:57, 6349.93it/s] 19%|█▊        | 83348/450337 [00:13<00:55, 6557.33it/s] 19%|█▊        | 84027/450337 [00:13<00:55, 6625.01it/s] 19%|█▉        | 84692/450337 [00:13<00:55, 6565.52it/s] 19%|█▉        | 85351/450337 [00:13<00:58, 6197.22it/s] 19%|█▉        | 86049/450337 [00:13<00:56, 6410.15it/s] 19%|█▉        | 86695/450337 [00:13<00:56, 6420.72it/s] 19%|█▉        | 87399/450337 [00:13<00:55, 6593.83it/s] 20%|█▉        | 88095/450337 [00:13<00:54, 6696.32it/s] 20%|█▉        | 88767/450337 [00:13<00:58, 6224.72it/s] 20%|█▉        | 89398/450337 [00:13<00:59, 6094.35it/s] 20%|█▉        | 90013/450337 [00:14<00:59, 6044.81it/s] 20%|██        | 90637/450337 [00:14<00:58, 6100.29it/s] 20%|██        | 91308/450337 [00:14<00:57, 6272.77it/s] 20%|██        | 91954/450337 [00:14<00:56, 6323.42it/s] 21%|██        | 92666/450337 [00:14<00:54, 6555.88it/s] 21%|██        | 93324/450337 [00:14<00:56, 6306.18it/s] 21%|██        | 93958/450337 [00:14<00:56, 6264.43it/s] 21%|██        | 94695/450337 [00:14<00:54, 6581.96it/s] 21%|██        | 95356/450337 [00:14<00:55, 6436.04it/s] 21%|██▏       | 96002/450337 [00:15<00:55, 6350.45it/s] 22%|██▏       | 96898/450337 [00:15<00:49, 7099.01it/s] 22%|██▏       | 97612/450337 [00:15<00:53, 6563.84it/s] 22%|██▏       | 98305/450337 [00:15<00:52, 6658.16it/s] 22%|██▏       | 98979/450337 [00:15<00:54, 6409.30it/s] 22%|██▏       | 99627/450337 [00:15<00:56, 6224.43it/s] 22%|██▏       | 100255/450337 [00:15<00:56, 6160.93it/s] 22%|██▏       | 100893/450337 [00:15<00:56, 6220.41it/s] 23%|██▎       | 101518/450337 [00:15<00:56, 6215.05it/s] 23%|██▎       | 102194/450337 [00:15<00:54, 6370.13it/s] 23%|██▎       | 102833/450337 [00:16<00:54, 6336.10it/s] 23%|██▎       | 103468/450337 [00:16<00:55, 6234.81it/s] 23%|██▎       | 104093/450337 [00:16<00:57, 5990.08it/s] 23%|██▎       | 104784/450337 [00:16<00:55, 6252.42it/s] 23%|██▎       | 105413/450337 [00:16<00:55, 6239.02it/s] 24%|██▎       | 106062/450337 [00:16<00:54, 6310.45it/s] 24%|██▎       | 106695/450337 [00:16<00:54, 6275.62it/s] 24%|██▍       | 107324/450337 [00:16<00:56, 6096.72it/s] 24%|██▍       | 108001/450337 [00:16<00:54, 6285.21it/s] 24%|██▍       | 108632/450337 [00:17<00:56, 6064.06it/s] 24%|██▍       | 109241/450337 [00:17<00:57, 5981.12it/s] 24%|██▍       | 109901/450337 [00:17<00:55, 6152.20it/s] 25%|██▍       | 110565/450337 [00:17<00:53, 6294.08it/s] 25%|██▍       | 111264/450337 [00:17<00:52, 6486.05it/s] 25%|██▍       | 111915/450337 [00:17<00:52, 6395.47it/s] 25%|██▌       | 112597/450337 [00:17<00:51, 6515.62it/s] 25%|██▌       | 113250/450337 [00:17<00:53, 6351.45it/s] 25%|██▌       | 113887/450337 [00:17<00:54, 6206.21it/s] 25%|██▌       | 114510/450337 [00:17<00:54, 6112.03it/s] 26%|██▌       | 115149/450337 [00:18<00:54, 6188.03it/s] 26%|██▌       | 115769/450337 [00:18<00:56, 5923.82it/s] 26%|██▌       | 116365/450337 [00:18<00:56, 5932.84it/s] 26%|██▌       | 116977/450337 [00:18<00:55, 5980.20it/s] 26%|██▌       | 117577/450337 [00:18<00:55, 5978.53it/s] 26%|██▌       | 118178/450337 [00:18<00:55, 5986.20it/s] 26%|██▋       | 118834/450337 [00:18<00:53, 6144.14it/s] 27%|██▋       | 119450/450337 [00:18<00:53, 6143.86it/s] 27%|██▋       | 120182/450337 [00:18<00:50, 6491.45it/s] 27%|██▋       | 120843/450337 [00:18<00:50, 6518.84it/s] 27%|██▋       | 121496/450337 [00:19<00:52, 6230.52it/s] 27%|██▋       | 122122/450337 [00:19<00:52, 6235.33it/s] 27%|██▋       | 122777/450337 [00:19<00:51, 6327.15it/s] 27%|██▋       | 123412/450337 [00:19<00:54, 6050.90it/s] 28%|██▊       | 124063/450337 [00:19<00:52, 6175.23it/s] 28%|██▊       | 124684/450337 [00:19<00:53, 6090.09it/s] 28%|██▊       | 125330/450337 [00:19<00:52, 6195.80it/s] 28%|██▊       | 126024/450337 [00:19<00:50, 6413.67it/s] 28%|██▊       | 126719/450337 [00:19<00:49, 6571.52it/s] 28%|██▊       | 127378/450337 [00:20<00:50, 6338.65it/s] 28%|██▊       | 128082/450337 [00:20<00:49, 6534.30it/s] 29%|██▊       | 128738/450337 [00:20<00:51, 6280.06it/s] 29%|██▊       | 129420/450337 [00:20<00:49, 6434.25it/s] 29%|██▉       | 130067/450337 [00:20<00:51, 6176.12it/s] 29%|██▉       | 130689/450337 [00:20<00:52, 6095.36it/s] 29%|██▉       | 131338/450337 [00:20<00:51, 6201.63it/s] 29%|██▉       | 131961/450337 [00:20<00:56, 5626.12it/s] 29%|██▉       | 132687/450337 [00:20<00:52, 6070.56it/s] 30%|██▉       | 133354/450337 [00:21<00:50, 6236.97it/s] 30%|██▉       | 134026/450337 [00:21<00:49, 6372.73it/s] 30%|██▉       | 134757/450337 [00:21<00:47, 6640.21it/s] 30%|███       | 135529/450337 [00:21<00:45, 6952.40it/s] 30%|███       | 136230/450337 [00:21<00:46, 6781.39it/s] 30%|███       | 137007/450337 [00:21<00:44, 7067.70it/s] 31%|███       | 137718/450337 [00:21<00:45, 6921.08it/s] 31%|███       | 138414/450337 [00:21<00:46, 6739.85it/s] 31%|███       | 139091/450337 [00:21<00:47, 6615.94it/s] 31%|███       | 139764/450337 [00:21<00:46, 6647.81it/s] 31%|███       | 140431/450337 [00:22<00:49, 6286.81it/s] 31%|███▏      | 141090/450337 [00:22<00:48, 6364.94it/s] 31%|███▏      | 141731/450337 [00:22<00:48, 6339.28it/s] 32%|███▏      | 142368/450337 [00:22<00:50, 6151.01it/s] 32%|███▏      | 142986/450337 [00:22<00:50, 6087.68it/s] 32%|███▏      | 143735/450337 [00:22<00:47, 6481.22it/s] 32%|███▏      | 144386/450337 [00:22<00:47, 6416.66it/s] 32%|███▏      | 145155/450337 [00:22<00:45, 6781.36it/s] 32%|███▏      | 145836/450337 [00:22<00:45, 6741.40it/s] 33%|███▎      | 146512/450337 [00:23<00:46, 6496.37it/s] 33%|███▎      | 147165/450337 [00:23<00:47, 6367.85it/s] 33%|███▎      | 147804/450337 [00:23<00:50, 6001.54it/s] 33%|███▎      | 148435/450337 [00:23<00:49, 6084.26it/s] 33%|███▎      | 149123/450337 [00:23<00:47, 6306.89it/s] 33%|███▎      | 149758/450337 [00:23<00:48, 6166.11it/s] 33%|███▎      | 150378/450337 [00:23<00:49, 6058.94it/s] 34%|███▎      | 150987/450337 [00:23<00:50, 5968.95it/s] 34%|███▎      | 151642/450337 [00:23<00:48, 6134.06it/s] 34%|███▍      | 152258/450337 [00:23<00:50, 5864.02it/s] 34%|███▍      | 152978/450337 [00:24<00:47, 6240.03it/s] 34%|███▍      | 153607/450337 [00:24<00:47, 6253.72it/s] 34%|███▍      | 154236/450337 [00:24<00:47, 6196.10it/s] 34%|███▍      | 154858/450337 [00:24<00:47, 6192.21it/s] 35%|███▍      | 155549/450337 [00:24<00:46, 6401.12it/s] 35%|███▍      | 156236/450337 [00:24<00:44, 6538.44it/s] 35%|███▍      | 156892/450337 [00:24<00:45, 6384.38it/s] 35%|███▍      | 157604/450337 [00:24<00:44, 6597.93it/s] 35%|███▌      | 158332/450337 [00:24<00:42, 6797.23it/s] 35%|███▌      | 159014/450337 [00:25<00:45, 6396.84it/s] 35%|███▌      | 159660/450337 [00:25<00:45, 6352.95it/s] 36%|███▌      | 160308/450337 [00:25<00:45, 6380.60it/s] 36%|███▌      | 161046/450337 [00:25<00:43, 6662.63it/s] 36%|███▌      | 161716/450337 [00:25<00:46, 6257.34it/s] 36%|███▌      | 162398/450337 [00:25<00:44, 6412.99it/s] 36%|███▌      | 163139/450337 [00:25<00:43, 6675.91it/s] 36%|███▋      | 163812/450337 [00:25<00:45, 6266.29it/s] 37%|███▋      | 164455/450337 [00:25<00:45, 6308.15it/s] 37%|███▋      | 165168/450337 [00:25<00:43, 6542.34it/s] 37%|███▋      | 165879/450337 [00:26<00:42, 6701.44it/s] 37%|███▋      | 166554/450337 [00:26<00:42, 6698.21it/s] 37%|███▋      | 167256/450337 [00:26<00:41, 6785.89it/s] 37%|███▋      | 167937/450337 [00:26<00:44, 6358.09it/s] 37%|███▋      | 168597/450337 [00:26<00:43, 6419.24it/s] 38%|███▊      | 169244/450337 [00:26<00:43, 6396.32it/s] 38%|███▊      | 169888/450337 [00:26<00:44, 6236.81it/s] 38%|███▊      | 170515/450337 [00:26<00:46, 6071.84it/s] 38%|███▊      | 171125/450337 [00:26<00:46, 5990.21it/s] 38%|███▊      | 171790/450337 [00:27<00:45, 6178.65it/s] 38%|███▊      | 172410/450337 [00:27<00:45, 6112.10it/s] 38%|███▊      | 173042/450337 [00:27<00:44, 6165.73it/s] 39%|███▊      | 173681/450337 [00:27<00:44, 6227.40it/s] 39%|███▊      | 174305/450337 [00:27<00:45, 6115.64it/s] 39%|███▉      | 175033/450337 [00:27<00:42, 6455.52it/s] 39%|███▉      | 175681/450337 [00:27<00:44, 6126.52it/s] 39%|███▉      | 176331/450337 [00:27<00:43, 6227.67it/s] 39%|███▉      | 176958/450337 [00:27<00:45, 6068.60it/s] 39%|███▉      | 177568/450337 [00:27<00:47, 5690.10it/s] 40%|███▉      | 178184/450337 [00:28<00:46, 5819.17it/s] 40%|███▉      | 178861/450337 [00:28<00:44, 6086.99it/s] 40%|███▉      | 179510/450337 [00:28<00:43, 6202.09it/s] 40%|████      | 180135/450337 [00:28<00:45, 5993.25it/s] 40%|████      | 180802/450337 [00:28<00:43, 6184.59it/s] 40%|████      | 181425/450337 [00:28<00:44, 6051.75it/s] 40%|████      | 182034/450337 [00:28<00:44, 5967.86it/s] 41%|████      | 182649/450337 [00:28<00:44, 6018.15it/s] 41%|████      | 183253/450337 [00:28<00:44, 5970.48it/s] 41%|████      | 183852/450337 [00:29<00:46, 5675.71it/s] 41%|████      | 184460/450337 [00:29<00:45, 5790.27it/s] 41%|████      | 185113/450337 [00:29<00:44, 5997.50it/s] 41%|████      | 185716/450337 [00:29<00:45, 5752.81it/s] 41%|████▏     | 186313/450337 [00:29<00:45, 5805.96it/s] 42%|████▏     | 186939/450337 [00:29<00:44, 5932.24it/s] 42%|████▏     | 187608/450337 [00:29<00:42, 6153.23it/s] 42%|████▏     | 188226/450337 [00:29<00:42, 6120.28it/s] 42%|████▏     | 188925/450337 [00:29<00:41, 6370.73it/s] 42%|████▏     | 189564/450337 [00:29<00:42, 6182.90it/s] 42%|████▏     | 190207/450337 [00:30<00:41, 6254.27it/s] 42%|████▏     | 190852/450337 [00:30<00:41, 6311.60it/s] 43%|████▎     | 191485/450337 [00:30<00:42, 6148.85it/s] 43%|████▎     | 192181/450337 [00:30<00:40, 6381.60it/s] 43%|████▎     | 192850/450337 [00:30<00:39, 6470.72it/s] 43%|████▎     | 193499/450337 [00:30<00:39, 6448.07it/s] 43%|████▎     | 194178/450337 [00:30<00:39, 6545.83it/s] 43%|████▎     | 194834/450337 [00:30<00:40, 6340.85it/s] 43%|████▎     | 195486/450337 [00:30<00:39, 6391.70it/s] 44%|████▎     | 196127/450337 [00:30<00:40, 6250.40it/s] 44%|████▎     | 196754/450337 [00:31<00:41, 6141.26it/s] 44%|████▍     | 197782/450337 [00:31<00:34, 7328.64it/s] 44%|████▍     | 198520/450337 [00:31<00:38, 6619.22it/s] 44%|████▍     | 199198/450337 [00:31<00:38, 6548.45it/s] 44%|████▍     | 199864/450337 [00:31<00:38, 6442.31it/s] 45%|████▍     | 200516/450337 [00:31<00:40, 6156.41it/s] 45%|████▍     | 201152/450337 [00:31<00:40, 6210.84it/s] 45%|████▍     | 201778/450337 [00:31<00:40, 6130.29it/s] 45%|████▍     | 202399/450337 [00:31<00:40, 6148.15it/s] 45%|████▌     | 203060/450337 [00:32<00:39, 6281.30it/s] 45%|████▌     | 203791/450337 [00:32<00:37, 6581.52it/s] 45%|████▌     | 204452/450337 [00:32<00:39, 6274.81it/s] 46%|████▌     | 205084/450337 [00:32<00:39, 6196.54it/s] 46%|████▌     | 205707/450337 [00:32<00:39, 6179.78it/s] 46%|████▌     | 206390/450337 [00:32<00:38, 6363.28it/s] 46%|████▌     | 207029/450337 [00:32<00:39, 6171.36it/s] 46%|████▌     | 207649/450337 [00:32<00:39, 6111.07it/s] 46%|████▌     | 208262/450337 [00:32<00:40, 6037.12it/s] 46%|████▋     | 208867/450337 [00:33<00:41, 5807.70it/s] 47%|████▋     | 209459/450337 [00:33<00:41, 5830.82it/s] 47%|████▋     | 210190/450337 [00:33<00:38, 6256.09it/s] 47%|████▋     | 210841/450337 [00:33<00:37, 6327.88it/s] 47%|████▋     | 211476/450337 [00:33<00:37, 6324.11it/s] 47%|████▋     | 212110/450337 [00:33<00:38, 6177.71it/s] 47%|████▋     | 212864/450337 [00:33<00:36, 6573.06it/s] 47%|████▋     | 213524/450337 [00:33<00:37, 6318.61it/s] 48%|████▊     | 214160/450337 [00:33<00:38, 6201.64it/s] 48%|████▊     | 214783/450337 [00:33<00:39, 6031.69it/s] 48%|████▊     | 215389/450337 [00:34<00:38, 6029.09it/s] 48%|████▊     | 215994/450337 [00:34<00:38, 6021.62it/s] 48%|████▊     | 216598/450337 [00:34<00:39, 5907.95it/s] 48%|████▊     | 217219/450337 [00:34<00:38, 5991.40it/s] 48%|████▊     | 217913/450337 [00:34<00:37, 6265.72it/s] 49%|████▊     | 218558/450337 [00:34<00:36, 6314.82it/s] 49%|████▊     | 219246/450337 [00:34<00:35, 6481.20it/s] 49%|████▉     | 219896/450337 [00:34<00:37, 6216.95it/s] 49%|████▉     | 220521/450337 [00:34<00:37, 6056.89it/s] 49%|████▉     | 221261/450337 [00:35<00:35, 6439.58it/s] 49%|████▉     | 221909/450337 [00:35<00:37, 6151.30it/s] 49%|████▉     | 222529/450337 [00:35<00:38, 5871.67it/s] 50%|████▉     | 223182/450337 [00:35<00:37, 6046.62it/s] 50%|████▉     | 223792/450337 [00:35<00:39, 5727.93it/s] 50%|████▉     | 224545/450337 [00:35<00:36, 6225.57it/s] 50%|█████     | 225197/450337 [00:35<00:35, 6308.41it/s] 50%|█████     | 225987/450337 [00:35<00:33, 6763.47it/s] 50%|█████     | 226670/450337 [00:35<00:34, 6571.10it/s] 50%|█████     | 227333/450337 [00:35<00:34, 6480.57it/s] 51%|█████     | 227985/450337 [00:36<00:35, 6241.98it/s] 51%|█████     | 228613/450337 [00:36<00:36, 6075.49it/s] 51%|█████     | 229224/450337 [00:36<00:36, 6074.61it/s] 51%|█████     | 229834/450337 [00:36<00:38, 5703.57it/s] 51%|█████     | 230494/450337 [00:36<00:36, 5948.07it/s] 51%|█████▏    | 231132/450337 [00:36<00:36, 6070.51it/s] 51%|█████▏    | 231744/450337 [00:36<00:36, 5970.94it/s] 52%|█████▏    | 232361/450337 [00:36<00:36, 6028.11it/s] 52%|█████▏    | 233087/450337 [00:36<00:34, 6383.38it/s] 52%|█████▏    | 233813/450337 [00:37<00:32, 6624.08it/s] 52%|█████▏    | 234537/450337 [00:37<00:31, 6803.08it/s] 52%|█████▏    | 235220/450337 [00:37<00:31, 6737.27it/s] 52%|█████▏    | 235896/450337 [00:37<00:32, 6571.51it/s] 53%|█████▎    | 236555/450337 [00:37<00:34, 6161.83it/s] 53%|█████▎    | 237196/450337 [00:37<00:34, 6231.16it/s] 53%|█████▎    | 237824/450337 [00:37<00:34, 6131.63it/s] 53%|█████▎    | 238580/450337 [00:37<00:32, 6536.96it/s] 53%|█████▎    | 239238/450337 [00:37<00:33, 6392.72it/s] 53%|█████▎    | 239881/450337 [00:37<00:34, 6151.61it/s] 53%|█████▎    | 240669/450337 [00:38<00:31, 6640.28it/s] 54%|█████▎    | 241370/450337 [00:38<00:30, 6741.94it/s] 54%|█████▎    | 242049/450337 [00:38<00:31, 6687.44it/s] 54%|█████▍    | 242721/450337 [00:38<00:31, 6538.21it/s] 54%|█████▍    | 243378/450337 [00:38<00:32, 6402.96it/s] 54%|█████▍    | 244021/450337 [00:38<00:32, 6355.70it/s] 54%|█████▍    | 244685/450337 [00:38<00:31, 6432.37it/s] 54%|█████▍    | 245330/450337 [00:38<00:32, 6388.53it/s] 55%|█████▍    | 246040/450337 [00:38<00:30, 6594.75it/s] 55%|█████▍    | 246836/450337 [00:39<00:29, 6993.48it/s] 55%|█████▍    | 247537/450337 [00:39<00:29, 6767.96it/s] 55%|█████▌    | 248217/450337 [00:39<00:31, 6514.05it/s] 55%|█████▌    | 248960/450337 [00:39<00:29, 6773.12it/s] 55%|█████▌    | 249689/450337 [00:39<00:28, 6921.02it/s] 56%|█████▌    | 250385/450337 [00:39<00:30, 6598.54it/s] 56%|█████▌    | 251067/450337 [00:39<00:29, 6659.89it/s] 56%|█████▌    | 251752/450337 [00:39<00:29, 6709.13it/s] 56%|█████▌    | 252426/450337 [00:39<00:30, 6569.07it/s] 56%|█████▌    | 253086/450337 [00:39<00:31, 6338.29it/s] 56%|█████▋    | 253780/450337 [00:40<00:30, 6509.31it/s] 56%|█████▋    | 254434/450337 [00:40<00:30, 6377.15it/s] 57%|█████▋    | 255088/450337 [00:40<00:30, 6419.07it/s] 57%|█████▋    | 255733/450337 [00:40<00:30, 6425.17it/s] 57%|█████▋    | 256402/450337 [00:40<00:29, 6498.21it/s] 57%|█████▋    | 257070/450337 [00:40<00:29, 6546.58it/s] 57%|█████▋    | 257790/450337 [00:40<00:28, 6739.98it/s] 57%|█████▋    | 258465/450337 [00:40<00:31, 6005.55it/s] 58%|█████▊    | 259081/450337 [00:40<00:32, 5966.97it/s] 58%|█████▊    | 259707/450337 [00:41<00:31, 6042.61it/s] 58%|█████▊    | 260417/450337 [00:41<00:29, 6343.03it/s] 58%|█████▊    | 261116/450337 [00:41<00:28, 6526.25it/s] 58%|█████▊    | 261775/450337 [00:41<00:29, 6414.82it/s] 58%|█████▊    | 262421/450337 [00:41<00:30, 6138.36it/s] 58%|█████▊    | 263040/450337 [00:41<00:30, 6122.42it/s] 59%|█████▊    | 263694/450337 [00:41<00:29, 6234.78it/s] 59%|█████▊    | 264321/450337 [00:41<00:31, 5989.16it/s] 59%|█████▉    | 264972/450337 [00:41<00:30, 6134.60it/s] 59%|█████▉    | 265606/450337 [00:41<00:29, 6193.24it/s] 59%|█████▉    | 266228/450337 [00:42<00:30, 5982.82it/s] 59%|█████▉    | 266830/450337 [00:42<00:30, 5932.91it/s] 59%|█████▉    | 267465/450337 [00:42<00:30, 6052.05it/s] 60%|█████▉    | 268072/450337 [00:42<00:30, 6054.37it/s] 60%|█████▉    | 268733/450337 [00:42<00:29, 6217.05it/s] 60%|█████▉    | 269411/450337 [00:42<00:28, 6383.58it/s] 60%|█████▉    | 270051/450337 [00:42<00:28, 6251.46it/s] 60%|██████    | 270692/450337 [00:42<00:28, 6291.58it/s] 60%|██████    | 271323/450337 [00:42<00:28, 6253.46it/s] 60%|██████    | 272009/450337 [00:43<00:27, 6431.88it/s] 61%|██████    | 272808/450337 [00:43<00:25, 6887.26it/s] 61%|██████    | 273500/450337 [00:43<00:25, 6891.98it/s] 61%|██████    | 274190/450337 [00:43<00:25, 6834.60it/s] 61%|██████    | 274875/450337 [00:43<00:27, 6433.36it/s] 61%|██████    | 275524/450337 [00:43<00:28, 6098.52it/s] 61%|██████▏   | 276152/450337 [00:43<00:28, 6148.43it/s] 61%|██████▏   | 276772/450337 [00:43<00:29, 5941.72it/s] 62%|██████▏   | 277409/450337 [00:43<00:28, 6057.77it/s] 62%|██████▏   | 278081/450337 [00:43<00:27, 6242.31it/s] 62%|██████▏   | 278782/450337 [00:44<00:26, 6459.51it/s] 62%|██████▏   | 279431/450337 [00:44<00:27, 6204.66it/s] 62%|██████▏   | 280099/450337 [00:44<00:26, 6340.52it/s] 62%|██████▏   | 280737/450337 [00:44<00:27, 6202.96it/s] 62%|██████▏   | 281370/450337 [00:44<00:27, 6238.14it/s] 63%|██████▎   | 281996/450337 [00:44<00:26, 6240.28it/s] 63%|██████▎   | 282622/450337 [00:44<00:27, 6090.78it/s] 63%|██████▎   | 283293/450337 [00:44<00:26, 6269.86it/s] 63%|██████▎   | 283955/450337 [00:44<00:26, 6367.75it/s] 63%|██████▎   | 284594/450337 [00:45<00:26, 6267.91it/s] 63%|██████▎   | 285234/450337 [00:45<00:26, 6303.76it/s] 63%|██████▎   | 285866/450337 [00:45<00:26, 6307.29it/s] 64%|██████▎   | 286498/450337 [00:45<00:26, 6160.04it/s] 64%|██████▍   | 287116/450337 [00:45<00:26, 6098.50it/s] 64%|██████▍   | 287760/450337 [00:45<00:26, 6193.93it/s] 64%|██████▍   | 288467/450337 [00:45<00:25, 6446.84it/s] 64%|██████▍   | 289146/450337 [00:45<00:24, 6546.24it/s] 64%|██████▍   | 289802/450337 [00:45<00:26, 6097.38it/s] 64%|██████▍   | 290419/450337 [00:45<00:27, 5900.98it/s] 65%|██████▍   | 291072/450337 [00:46<00:26, 6070.73it/s] 65%|██████▍   | 291713/450337 [00:46<00:25, 6164.86it/s] 65%|██████▍   | 292334/450337 [00:46<00:25, 6133.80it/s] 65%|██████▌   | 292950/450337 [00:46<00:26, 6006.28it/s] 65%|██████▌   | 293626/450337 [00:46<00:25, 6219.02it/s] 65%|██████▌   | 294251/450337 [00:46<00:25, 6220.25it/s] 66%|██████▌   | 294971/450337 [00:46<00:23, 6507.39it/s] 66%|██████▌   | 295624/450337 [00:46<00:24, 6444.38it/s] 66%|██████▌   | 296270/450337 [00:46<00:24, 6401.99it/s] 66%|██████▌   | 296924/450337 [00:46<00:23, 6440.32it/s] 66%|██████▌   | 297569/450337 [00:47<00:24, 6282.69it/s] 66%|██████▌   | 298258/450337 [00:47<00:23, 6447.43it/s] 66%|██████▋   | 298904/450337 [00:47<00:23, 6330.55it/s] 67%|██████▋   | 299613/450337 [00:47<00:23, 6552.02it/s] 67%|██████▋   | 300270/450337 [00:47<00:23, 6446.83it/s] 67%|██████▋   | 300916/450337 [00:47<00:23, 6242.07it/s] 67%|██████▋   | 301543/450337 [00:47<00:23, 6221.92it/s] 67%|██████▋   | 302167/450337 [00:47<00:24, 6023.25it/s] 67%|██████▋   | 302870/450337 [00:47<00:23, 6309.53it/s] 67%|██████▋   | 303538/450337 [00:48<00:22, 6403.92it/s] 68%|██████▊   | 304205/450337 [00:48<00:22, 6475.68it/s] 68%|██████▊   | 304855/450337 [00:48<00:22, 6354.77it/s] 68%|██████▊   | 305492/450337 [00:48<00:23, 6197.18it/s] 68%|██████▊   | 306138/450337 [00:48<00:22, 6271.23it/s] 68%|██████▊   | 306767/450337 [00:48<00:23, 6047.67it/s] 68%|██████▊   | 307488/450337 [00:48<00:22, 6380.59it/s] 68%|██████▊   | 308130/450337 [00:48<00:23, 6023.62it/s] 69%|██████▊   | 308738/450337 [00:48<00:24, 5885.17it/s] 69%|██████▉   | 309706/450337 [00:48<00:20, 6951.75it/s] 69%|██████▉   | 310411/450337 [00:49<00:20, 6731.46it/s] 69%|██████▉   | 311092/450337 [00:49<00:21, 6483.68it/s] 69%|██████▉   | 311747/450337 [00:49<00:22, 6177.39it/s] 69%|██████▉   | 312390/450337 [00:49<00:22, 6242.31it/s] 70%|██████▉   | 313074/450337 [00:49<00:21, 6410.45it/s] 70%|██████▉   | 313796/450337 [00:49<00:20, 6630.88it/s] 70%|██████▉   | 314481/450337 [00:49<00:20, 6687.19it/s] 70%|██████▉   | 315153/450337 [00:49<00:21, 6166.79it/s] 70%|███████   | 315817/450337 [00:49<00:21, 6295.66it/s] 70%|███████   | 316455/450337 [00:50<00:22, 5985.89it/s] 70%|███████   | 317115/450337 [00:50<00:21, 6154.86it/s] 71%|███████   | 317738/450337 [00:50<00:22, 5827.14it/s] 71%|███████   | 318421/450337 [00:50<00:21, 6099.33it/s] 71%|███████   | 319039/450337 [00:50<00:21, 6115.67it/s] 71%|███████   | 319693/450337 [00:50<00:20, 6225.06it/s] 71%|███████   | 320320/450337 [00:50<00:21, 6101.31it/s] 71%|███████▏  | 320976/450337 [00:50<00:20, 6230.92it/s] 71%|███████▏  | 321658/450337 [00:50<00:20, 6399.28it/s] 72%|███████▏  | 322301/450337 [00:51<00:21, 6040.73it/s] 72%|███████▏  | 322911/450337 [00:51<00:21, 6050.15it/s] 72%|███████▏  | 323557/450337 [00:51<00:20, 6161.58it/s] 72%|███████▏  | 324230/450337 [00:51<00:19, 6327.00it/s] 72%|███████▏  | 324879/450337 [00:51<00:19, 6373.69it/s] 72%|███████▏  | 325519/450337 [00:51<00:21, 5833.05it/s] 72%|███████▏  | 326143/450337 [00:51<00:20, 5940.79it/s] 73%|███████▎  | 326897/450337 [00:51<00:19, 6395.52it/s] 73%|███████▎  | 327589/450337 [00:51<00:18, 6546.89it/s] 73%|███████▎  | 328381/450337 [00:51<00:17, 6943.02it/s] 73%|███████▎  | 329081/450337 [00:52<00:17, 6858.33it/s] 73%|███████▎  | 329771/450337 [00:52<00:18, 6446.30it/s] 73%|███████▎  | 330423/450337 [00:52<00:19, 6059.27it/s] 74%|███████▎  | 331105/450337 [00:52<00:19, 6267.42it/s] 74%|███████▎  | 331740/450337 [00:52<00:19, 6127.94it/s] 74%|███████▍  | 332359/450337 [00:52<00:19, 6129.52it/s] 74%|███████▍  | 332996/450337 [00:52<00:18, 6190.93it/s] 74%|███████▍  | 333619/450337 [00:52<00:19, 6132.10it/s] 74%|███████▍  | 334235/450337 [00:52<00:18, 6128.54it/s] 74%|███████▍  | 334924/450337 [00:53<00:18, 6349.56it/s] 75%|███████▍  | 335569/450337 [00:53<00:18, 6363.77it/s] 75%|███████▍  | 336222/450337 [00:53<00:17, 6399.58it/s] 75%|███████▍  | 336863/450337 [00:53<00:17, 6324.18it/s] 75%|███████▍  | 337590/450337 [00:53<00:17, 6600.72it/s] 75%|███████▌  | 338251/450337 [00:53<00:17, 6572.03it/s] 75%|███████▌  | 338971/450337 [00:53<00:16, 6749.37it/s] 75%|███████▌  | 339647/450337 [00:53<00:16, 6516.46it/s] 76%|███████▌  | 340301/450337 [00:53<00:18, 5931.45it/s] 76%|███████▌  | 340956/450337 [00:53<00:17, 6098.90it/s] 76%|███████▌  | 341618/450337 [00:54<00:17, 6244.39it/s] 76%|███████▌  | 342258/450337 [00:54<00:17, 6279.84it/s] 76%|███████▌  | 342913/450337 [00:54<00:16, 6354.66it/s] 76%|███████▋  | 343571/450337 [00:54<00:16, 6420.04it/s] 76%|███████▋  | 344247/450337 [00:54<00:16, 6519.96it/s] 77%|███████▋  | 344902/450337 [00:54<00:16, 6338.62it/s] 77%|███████▋  | 345539/450337 [00:54<00:17, 6135.31it/s] 77%|███████▋  | 346156/450337 [00:54<00:17, 5848.38it/s] 77%|███████▋  | 346850/450337 [00:54<00:16, 6154.46it/s] 77%|███████▋  | 347491/450337 [00:55<00:16, 6227.43it/s] 77%|███████▋  | 348118/450337 [00:55<00:16, 6188.63it/s] 77%|███████▋  | 348740/450337 [00:55<00:16, 6089.33it/s] 78%|███████▊  | 349365/450337 [00:55<00:16, 6131.60it/s] 78%|███████▊  | 350230/450337 [00:55<00:14, 6868.85it/s] 78%|███████▊  | 350920/450337 [00:55<00:15, 6584.24it/s] 78%|███████▊  | 351583/450337 [00:55<00:15, 6548.69it/s] 78%|███████▊  | 352241/450337 [00:55<00:16, 6022.14it/s] 78%|███████▊  | 352853/450337 [00:55<00:16, 5880.63it/s] 79%|███████▊  | 353531/450337 [00:55<00:15, 6127.57it/s] 79%|███████▊  | 354151/450337 [00:56<00:16, 5983.41it/s] 79%|███████▉  | 354871/450337 [00:56<00:15, 6326.37it/s] 79%|███████▉  | 355509/450337 [00:56<00:15, 6084.00it/s] 79%|███████▉  | 356149/450337 [00:56<00:15, 6172.98it/s] 79%|███████▉  | 356771/450337 [00:56<00:15, 6156.39it/s] 79%|███████▉  | 357390/450337 [00:56<00:16, 5780.84it/s] 80%|███████▉  | 358072/450337 [00:56<00:15, 6066.35it/s] 80%|███████▉  | 358697/450337 [00:56<00:14, 6115.18it/s] 80%|███████▉  | 359331/450337 [00:56<00:14, 6179.27it/s] 80%|███████▉  | 359953/450337 [00:57<00:14, 6178.87it/s] 80%|████████  | 360674/450337 [00:57<00:13, 6476.17it/s] 80%|████████  | 361358/450337 [00:57<00:13, 6583.67it/s] 80%|████████  | 362100/450337 [00:57<00:12, 6831.42it/s] 81%|████████  | 362785/450337 [00:57<00:12, 6818.24it/s] 81%|████████  | 363468/450337 [00:57<00:12, 6702.81it/s] 81%|████████  | 364143/450337 [00:57<00:12, 6709.70it/s] 81%|████████  | 364833/450337 [00:57<00:12, 6759.48it/s] 81%|████████  | 365525/450337 [00:57<00:12, 6804.72it/s] 81%|████████▏ | 366216/450337 [00:57<00:12, 6828.87it/s] 81%|████████▏ | 366900/450337 [00:58<00:13, 6413.80it/s] 82%|████████▏ | 367569/450337 [00:58<00:12, 6488.47it/s] 82%|████████▏ | 368222/450337 [00:58<00:13, 6007.36it/s] 82%|████████▏ | 368923/450337 [00:58<00:12, 6281.75it/s] 82%|████████▏ | 369575/450337 [00:58<00:12, 6340.60it/s] 82%|████████▏ | 370233/450337 [00:58<00:12, 6408.51it/s] 82%|████████▏ | 370879/450337 [00:58<00:12, 6145.27it/s] 82%|████████▏ | 371506/450337 [00:58<00:12, 6175.36it/s] 83%|████████▎ | 372128/450337 [00:58<00:12, 6109.06it/s] 83%|████████▎ | 372770/450337 [00:59<00:12, 6194.55it/s] 83%|████████▎ | 373392/450337 [00:59<00:12, 6176.97it/s] 83%|████████▎ | 374033/450337 [00:59<00:12, 6243.33it/s] 83%|████████▎ | 374766/450337 [00:59<00:11, 6555.15it/s] 83%|████████▎ | 375423/450337 [00:59<00:11, 6314.66it/s] 84%|████████▎ | 376057/450337 [00:59<00:12, 6093.55it/s] 84%|████████▎ | 376670/450337 [00:59<00:12, 5925.97it/s] 84%|████████▍ | 377294/450337 [00:59<00:12, 6013.24it/s] 84%|████████▍ | 377948/450337 [00:59<00:11, 6158.93it/s] 84%|████████▍ | 378575/450337 [00:59<00:11, 6191.14it/s] 84%|████████▍ | 379228/450337 [01:00<00:11, 6288.56it/s] 84%|████████▍ | 379966/450337 [01:00<00:10, 6610.66it/s] 85%|████████▍ | 380629/450337 [01:00<00:10, 6574.04it/s] 85%|████████▍ | 381363/450337 [01:00<00:10, 6800.55it/s] 85%|████████▍ | 382044/450337 [01:00<00:10, 6328.67it/s] 85%|████████▍ | 382684/450337 [01:00<00:11, 6037.38it/s] 85%|████████▌ | 383299/450337 [01:00<00:11, 6066.87it/s] 85%|████████▌ | 383911/450337 [01:00<00:11, 6012.42it/s] 85%|████████▌ | 384691/450337 [01:00<00:10, 6512.06it/s] 86%|████████▌ | 385347/450337 [01:01<00:09, 6510.14it/s] 86%|████████▌ | 386002/450337 [01:01<00:10, 6156.51it/s] 86%|████████▌ | 386636/450337 [01:01<00:10, 6205.45it/s] 86%|████████▌ | 387441/450337 [01:01<00:09, 6735.53it/s] 86%|████████▌ | 388120/450337 [01:01<00:09, 6598.34it/s] 86%|████████▋ | 388784/450337 [01:01<00:09, 6366.51it/s] 86%|████████▋ | 389482/450337 [01:01<00:09, 6537.31it/s] 87%|████████▋ | 390168/450337 [01:01<00:09, 6629.99it/s] 87%|████████▋ | 390834/450337 [01:01<00:09, 6514.27it/s] 87%|████████▋ | 391488/450337 [01:01<00:09, 6109.11it/s] 87%|████████▋ | 392105/450337 [01:02<00:09, 6047.69it/s] 87%|████████▋ | 392714/450337 [01:02<00:09, 5998.54it/s] 87%|████████▋ | 393317/450337 [01:02<00:09, 5927.34it/s] 87%|████████▋ | 393987/450337 [01:02<00:09, 6144.54it/s] 88%|████████▊ | 394622/450337 [01:02<00:08, 6203.79it/s] 88%|████████▊ | 395245/450337 [01:02<00:08, 6188.02it/s] 88%|████████▊ | 395907/450337 [01:02<00:08, 6306.77it/s] 88%|████████▊ | 396539/450337 [01:02<00:08, 6117.22it/s] 88%|████████▊ | 397153/450337 [01:02<00:08, 5937.18it/s] 88%|████████▊ | 397786/450337 [01:03<00:08, 6045.16it/s] 88%|████████▊ | 398503/450337 [01:03<00:08, 6370.72it/s] 89%|████████▊ | 399174/450337 [01:03<00:07, 6469.93it/s] 89%|████████▉ | 399906/450337 [01:03<00:07, 6718.85it/s] 89%|████████▉ | 400580/450337 [01:03<00:07, 6573.43it/s] 89%|████████▉ | 401240/450337 [01:03<00:07, 6399.27it/s] 89%|████████▉ | 401882/450337 [01:03<00:08, 6039.40it/s] 89%|████████▉ | 402491/450337 [01:03<00:08, 5673.69it/s] 90%|████████▉ | 403123/450337 [01:03<00:08, 5849.57it/s] 90%|████████▉ | 403780/450337 [01:03<00:07, 6048.36it/s] 90%|████████▉ | 404391/450337 [01:04<00:07, 5933.73it/s] 90%|████████▉ | 405047/450337 [01:04<00:07, 6111.24it/s] 90%|█████████ | 405662/450337 [01:04<00:07, 5996.26it/s] 90%|█████████ | 406298/450337 [01:04<00:07, 6096.97it/s] 90%|█████████ | 406910/450337 [01:04<00:07, 5977.01it/s] 90%|█████████ | 407510/450337 [01:04<00:07, 5964.91it/s] 91%|█████████ | 408108/450337 [01:04<00:07, 5844.62it/s] 91%|█████████ | 408801/450337 [01:04<00:06, 6155.91it/s] 91%|█████████ | 409446/450337 [01:04<00:06, 6241.72it/s] 91%|█████████ | 410072/450337 [01:05<00:06, 6108.69it/s] 91%|█████████ | 410726/450337 [01:05<00:06, 6229.56it/s] 91%|█████████▏| 411351/450337 [01:05<00:06, 6070.06it/s] 91%|█████████▏| 411979/450337 [01:05<00:06, 6130.88it/s] 92%|█████████▏| 412594/450337 [01:05<00:06, 5717.86it/s] 92%|█████████▏| 413172/450337 [01:05<00:06, 5522.42it/s] 92%|█████████▏| 413847/450337 [01:05<00:06, 5859.26it/s] 92%|█████████▏| 414439/450337 [01:05<00:06, 5571.01it/s] 92%|█████████▏| 415003/450337 [01:05<00:06, 5582.77it/s] 92%|█████████▏| 415659/450337 [01:05<00:05, 5853.36it/s] 92%|█████████▏| 416249/450337 [01:06<00:06, 5665.49it/s] 93%|█████████▎| 416895/450337 [01:06<00:05, 5887.74it/s] 93%|█████████▎| 417531/450337 [01:06<00:05, 6023.60it/s] 93%|█████████▎| 418203/450337 [01:06<00:05, 6222.71it/s] 93%|█████████▎| 418829/450337 [01:06<00:05, 6133.12it/s] 93%|█████████▎| 419445/450337 [01:06<00:05, 6133.00it/s] 93%|█████████▎| 420080/450337 [01:06<00:04, 6191.50it/s] 93%|█████████▎| 420723/450337 [01:06<00:04, 6260.55it/s] 94%|█████████▎| 421350/450337 [01:06<00:04, 5882.95it/s] 94%|█████████▎| 421960/450337 [01:07<00:04, 5940.12it/s] 94%|█████████▍| 422669/450337 [01:07<00:04, 6269.40it/s] 94%|█████████▍| 423300/450337 [01:07<00:04, 6214.64it/s] 94%|█████████▍| 423932/450337 [01:07<00:04, 6240.18it/s] 94%|█████████▍| 424558/450337 [01:07<00:04, 6070.55it/s] 94%|█████████▍| 425168/450337 [01:07<00:04, 6058.52it/s] 95%|█████████▍| 425803/450337 [01:07<00:03, 6136.59it/s] 95%|█████████▍| 426545/450337 [01:07<00:03, 6513.54it/s] 95%|█████████▍| 427198/450337 [01:07<00:03, 6508.31it/s] 95%|█████████▌| 427850/450337 [01:07<00:03, 6425.09it/s] 95%|█████████▌| 428606/450337 [01:08<00:03, 6756.95it/s] 95%|█████████▌| 429283/450337 [01:08<00:03, 6626.67it/s] 95%|█████████▌| 429947/450337 [01:08<00:03, 6539.83it/s] 96%|█████████▌| 430602/450337 [01:08<00:03, 6379.07it/s] 96%|█████████▌| 431242/450337 [01:08<00:03, 6032.15it/s] 96%|█████████▌| 431850/450337 [01:08<00:03, 5856.89it/s] 96%|█████████▌| 432730/450337 [01:08<00:02, 6680.27it/s] 96%|█████████▌| 433406/450337 [01:08<00:02, 6566.07it/s] 96%|█████████▋| 434069/450337 [01:08<00:02, 6357.71it/s] 97%|█████████▋| 434710/450337 [01:09<00:02, 6053.31it/s] 97%|█████████▋| 435321/450337 [01:09<00:02, 6037.08it/s] 97%|█████████▋| 435972/450337 [01:09<00:02, 6164.35it/s] 97%|█████████▋| 436592/450337 [01:09<00:02, 6133.79it/s] 97%|█████████▋| 437292/450337 [01:09<00:02, 6362.74it/s] 97%|█████████▋| 437931/450337 [01:09<00:01, 6285.87it/s] 97%|█████████▋| 438562/450337 [01:09<00:01, 6207.96it/s] 98%|█████████▊| 439184/450337 [01:09<00:01, 6007.79it/s] 98%|█████████▊| 439787/450337 [01:09<00:01, 5930.78it/s] 98%|█████████▊| 440382/450337 [01:09<00:01, 5931.05it/s] 98%|█████████▊| 441002/450337 [01:10<00:01, 6008.82it/s] 98%|█████████▊| 441643/450337 [01:10<00:01, 6123.00it/s] 98%|█████████▊| 442319/450337 [01:10<00:01, 6301.19it/s] 98%|█████████▊| 442950/450337 [01:10<00:01, 6177.58it/s] 99%|█████████▊| 443639/450337 [01:10<00:01, 6378.70it/s] 99%|█████████▊| 444278/450337 [01:10<00:00, 6371.38it/s] 99%|█████████▉| 444916/450337 [01:10<00:00, 5857.82it/s] 99%|█████████▉| 445547/450337 [01:10<00:00, 5982.60it/s] 99%|█████████▉| 446152/450337 [01:10<00:00, 5942.69it/s] 99%|█████████▉| 446751/450337 [01:11<00:00, 5924.57it/s] 99%|█████████▉| 447447/450337 [01:11<00:00, 6223.56it/s] 99%|█████████▉| 448073/450337 [01:11<00:00, 6163.96it/s]100%|█████████▉| 448779/450337 [01:11<00:00, 6426.00it/s]100%|█████████▉| 449424/450337 [01:11<00:00, 6164.52it/s]100%|█████████▉| 450044/450337 [01:11<00:00, 6147.07it/s]100%|██████████| 450337/450337 [01:11<00:00, 6291.78it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1928/450337 [00:00<00:23, 19276.62it/s]  1%|          | 4013/450337 [00:00<00:22, 20199.36it/s]  1%|▏         | 6270/450337 [00:00<00:20, 21280.37it/s]  2%|▏         | 8399/450337 [00:00<00:21, 20376.99it/s]  2%|▏         | 10443/450337 [00:00<00:21, 20351.51it/s]  3%|▎         | 12482/450337 [00:00<00:21, 20184.72it/s]  3%|▎         | 14522/450337 [00:00<00:21, 20250.90it/s]  4%|▎         | 16549/450337 [00:00<00:21, 20151.37it/s]  4%|▍         | 18566/450337 [00:00<00:21, 20155.23it/s]  5%|▍         | 20753/450337 [00:01<00:20, 20675.79it/s]  5%|▌         | 22822/450337 [00:01<00:20, 20455.11it/s]  6%|▌         | 25137/450337 [00:01<00:19, 21262.95it/s]  6%|▌         | 27266/450337 [00:01<00:20, 20479.98it/s]  7%|▋         | 29321/450337 [00:01<00:20, 20441.87it/s]  7%|▋         | 31370/450337 [00:01<00:20, 20072.34it/s]  7%|▋         | 33382/450337 [00:01<00:21, 19686.79it/s]  8%|▊         | 35441/450337 [00:01<00:20, 19937.34it/s]  8%|▊         | 37438/450337 [00:01<00:21, 19589.27it/s]  9%|▊         | 39400/450337 [00:01<00:21, 19562.80it/s]  9%|▉         | 41488/450337 [00:02<00:20, 19940.87it/s] 10%|▉         | 43485/450337 [00:02<00:20, 19427.53it/s] 10%|█         | 45602/450337 [00:02<00:20, 19933.48it/s] 11%|█         | 48023/450337 [00:02<00:18, 21188.84it/s] 11%|█         | 50168/450337 [00:02<00:18, 21262.87it/s] 12%|█▏        | 52308/450337 [00:02<00:18, 21303.13it/s] 12%|█▏        | 54442/450337 [00:02<00:19, 20737.86it/s] 13%|█▎        | 56521/450337 [00:02<00:19, 20720.01it/s] 13%|█▎        | 58597/450337 [00:02<00:19, 20563.26it/s] 13%|█▎        | 60656/450337 [00:02<00:19, 20302.72it/s] 14%|█▍        | 62842/450337 [00:03<00:18, 20755.04it/s] 14%|█▍        | 64920/450337 [00:03<00:18, 20581.44it/s] 15%|█▍        | 66987/450337 [00:03<00:18, 20598.90it/s] 15%|█▌        | 69049/450337 [00:03<00:18, 20441.10it/s] 16%|█▌        | 71095/450337 [00:03<00:18, 20364.72it/s] 16%|█▋        | 73322/450337 [00:03<00:18, 20928.54it/s] 17%|█▋        | 75497/450337 [00:03<00:17, 21168.70it/s] 17%|█▋        | 77651/450337 [00:03<00:17, 21278.65it/s] 18%|█▊        | 79780/450337 [00:03<00:18, 20238.47it/s] 18%|█▊        | 81865/450337 [00:04<00:18, 20411.78it/s] 19%|█▊        | 84019/450337 [00:04<00:17, 20736.46it/s] 19%|█▉        | 86100/450337 [00:04<00:17, 20479.93it/s] 20%|█▉        | 88266/450337 [00:04<00:17, 20817.96it/s] 20%|██        | 90353/450337 [00:04<00:18, 19945.58it/s] 21%|██        | 92479/450337 [00:04<00:17, 20323.02it/s] 21%|██        | 94520/450337 [00:04<00:17, 20077.48it/s] 21%|██▏       | 96761/450337 [00:04<00:17, 20753.82it/s] 22%|██▏       | 98843/450337 [00:04<00:17, 20350.41it/s] 22%|██▏       | 100884/450337 [00:04<00:17, 20071.33it/s] 23%|██▎       | 102939/450337 [00:05<00:17, 20208.72it/s] 23%|██▎       | 104964/450337 [00:05<00:17, 19974.42it/s] 24%|██▍       | 106964/450337 [00:05<00:17, 19839.09it/s] 24%|██▍       | 108950/450337 [00:05<00:17, 19596.82it/s] 25%|██▍       | 111102/450337 [00:05<00:16, 20158.80it/s] 25%|██▌       | 113121/450337 [00:05<00:16, 20005.77it/s] 26%|██▌       | 115124/450337 [00:05<00:16, 19940.27it/s] 26%|██▌       | 117120/450337 [00:05<00:17, 19519.70it/s] 26%|██▋       | 119115/450337 [00:05<00:16, 19635.05it/s] 27%|██▋       | 121251/450337 [00:05<00:16, 20132.68it/s] 27%|██▋       | 123267/450337 [00:06<00:16, 19691.74it/s] 28%|██▊       | 125264/450337 [00:06<00:16, 19770.29it/s] 28%|██▊       | 127364/450337 [00:06<00:16, 20126.06it/s] 29%|██▊       | 129425/450337 [00:06<00:15, 20263.37it/s] 29%|██▉       | 131454/450337 [00:06<00:16, 19845.10it/s] 30%|██▉       | 133442/450337 [00:06<00:16, 19638.20it/s] 30%|███       | 135751/450337 [00:06<00:15, 20645.13it/s] 31%|███       | 137910/450337 [00:06<00:14, 20919.88it/s] 31%|███       | 140006/450337 [00:06<00:14, 20718.80it/s] 32%|███▏      | 142081/450337 [00:06<00:15, 20317.66it/s] 32%|███▏      | 144166/450337 [00:07<00:14, 20470.79it/s] 33%|███▎      | 146383/450337 [00:07<00:14, 20955.18it/s] 33%|███▎      | 148481/450337 [00:07<00:14, 20143.57it/s] 33%|███▎      | 150504/450337 [00:07<00:15, 19925.79it/s] 34%|███▍      | 152502/450337 [00:07<00:15, 19717.54it/s] 34%|███▍      | 154529/450337 [00:07<00:14, 19877.43it/s] 35%|███▍      | 156630/450337 [00:07<00:14, 20208.68it/s] 35%|███▌      | 158789/450337 [00:07<00:14, 20613.43it/s] 36%|███▌      | 160853/450337 [00:07<00:14, 20450.46it/s] 36%|███▌      | 162929/450337 [00:08<00:13, 20539.91it/s] 37%|███▋      | 164985/450337 [00:08<00:13, 20467.93it/s] 37%|███▋      | 167178/450337 [00:08<00:13, 20898.55it/s] 38%|███▊      | 169269/450337 [00:08<00:13, 20485.78it/s] 38%|███▊      | 171320/450337 [00:08<00:14, 19918.12it/s] 38%|███▊      | 173317/450337 [00:08<00:13, 19831.27it/s] 39%|███▉      | 175303/450337 [00:08<00:13, 19759.71it/s] 39%|███▉      | 177281/450337 [00:08<00:13, 19520.37it/s] 40%|███▉      | 179235/450337 [00:08<00:13, 19369.23it/s] 40%|████      | 181173/450337 [00:08<00:13, 19323.39it/s] 41%|████      | 183110/450337 [00:09<00:13, 19335.60it/s] 41%|████      | 185044/450337 [00:09<00:13, 19042.21it/s] 42%|████▏     | 186950/450337 [00:09<00:13, 18917.96it/s] 42%|████▏     | 189016/450337 [00:09<00:13, 19428.98it/s] 42%|████▏     | 191097/450337 [00:09<00:13, 19837.59it/s] 43%|████▎     | 193103/450337 [00:09<00:12, 19900.88it/s] 43%|████▎     | 195105/450337 [00:09<00:12, 19935.95it/s] 44%|████▍     | 197333/450337 [00:09<00:12, 20629.55it/s] 44%|████▍     | 199397/450337 [00:09<00:12, 20470.57it/s] 45%|████▍     | 201445/450337 [00:09<00:12, 19859.02it/s] 45%|████▌     | 203553/450337 [00:10<00:12, 20209.54it/s] 46%|████▌     | 205578/450337 [00:10<00:12, 19934.01it/s] 46%|████▌     | 207575/450337 [00:10<00:12, 19786.68it/s] 47%|████▋     | 209556/450337 [00:10<00:12, 19287.87it/s] 47%|████▋     | 211645/450337 [00:10<00:12, 19751.89it/s] 47%|████▋     | 213677/450337 [00:10<00:11, 19914.32it/s] 48%|████▊     | 215672/450337 [00:10<00:12, 19477.11it/s] 48%|████▊     | 217674/450337 [00:10<00:11, 19627.89it/s] 49%|████▉     | 219728/450337 [00:10<00:11, 19889.76it/s] 49%|████▉     | 221722/450337 [00:10<00:11, 19899.30it/s] 50%|████▉     | 223714/450337 [00:11<00:11, 19285.41it/s] 50%|█████     | 225999/450337 [00:11<00:11, 20322.93it/s] 51%|█████     | 228038/450337 [00:11<00:11, 19964.95it/s] 51%|█████     | 230040/450337 [00:11<00:11, 19331.89it/s] 52%|█████▏    | 231995/450337 [00:11<00:11, 19391.66it/s] 52%|█████▏    | 234228/450337 [00:11<00:10, 20245.09it/s] 52%|█████▏    | 236292/450337 [00:11<00:10, 20356.77it/s] 53%|█████▎    | 238333/450337 [00:11<00:10, 20186.08it/s] 53%|█████▎    | 240355/450337 [00:11<00:10, 20101.07it/s] 54%|█████▍    | 242550/450337 [00:12<00:10, 20643.04it/s] 54%|█████▍    | 244617/450337 [00:12<00:10, 20525.97it/s] 55%|█████▍    | 246809/450337 [00:12<00:09, 20935.40it/s] 55%|█████▌    | 248905/450337 [00:12<00:09, 20922.58it/s] 56%|█████▌    | 251007/450337 [00:12<00:09, 20950.86it/s] 56%|█████▌    | 253103/450337 [00:12<00:09, 20647.00it/s] 57%|█████▋    | 255170/450337 [00:12<00:09, 20573.97it/s] 57%|█████▋    | 257284/450337 [00:12<00:09, 20739.37it/s] 58%|█████▊    | 259359/450337 [00:12<00:09, 19785.38it/s] 58%|█████▊    | 261521/450337 [00:12<00:09, 20310.13it/s] 59%|█████▊    | 263561/450337 [00:13<00:09, 19854.25it/s] 59%|█████▉    | 265554/450337 [00:13<00:09, 19744.21it/s] 59%|█████▉    | 267534/450337 [00:13<00:09, 19531.09it/s] 60%|█████▉    | 269545/450337 [00:13<00:09, 19697.77it/s] 60%|██████    | 271519/450337 [00:13<00:09, 19708.68it/s] 61%|██████    | 273814/450337 [00:13<00:08, 20664.10it/s] 61%|██████▏   | 275884/450337 [00:13<00:08, 20038.25it/s] 62%|██████▏   | 277894/450337 [00:13<00:08, 19850.31it/s] 62%|██████▏   | 279902/450337 [00:13<00:08, 19916.55it/s] 63%|██████▎   | 281922/450337 [00:13<00:08, 19997.20it/s] 63%|██████▎   | 283937/450337 [00:14<00:08, 20037.49it/s] 64%|██████▎   | 285966/450337 [00:14<00:08, 20112.13it/s] 64%|██████▍   | 287979/450337 [00:14<00:08, 19892.10it/s] 64%|██████▍   | 289970/450337 [00:14<00:08, 19594.93it/s] 65%|██████▍   | 291941/450337 [00:14<00:08, 19626.39it/s] 65%|██████▌   | 293905/450337 [00:14<00:08, 19492.11it/s] 66%|██████▌   | 296018/450337 [00:14<00:07, 19974.43it/s] 66%|██████▌   | 298050/450337 [00:14<00:07, 20068.26it/s] 67%|██████▋   | 300120/450337 [00:14<00:07, 20254.72it/s] 67%|██████▋   | 302147/450337 [00:15<00:07, 19705.85it/s] 68%|██████▊   | 304269/450337 [00:15<00:07, 20146.24it/s] 68%|██████▊   | 306288/450337 [00:15<00:07, 19929.38it/s] 68%|██████▊   | 308284/450337 [00:15<00:07, 19487.68it/s] 69%|██████▉   | 310544/450337 [00:15<00:06, 20393.53it/s] 69%|██████▉   | 312589/450337 [00:15<00:06, 20304.52it/s] 70%|██████▉   | 314691/450337 [00:15<00:06, 20507.00it/s] 70%|███████   | 316745/450337 [00:15<00:06, 19950.72it/s] 71%|███████   | 318745/450337 [00:15<00:06, 19543.79it/s] 71%|███████   | 320721/450337 [00:15<00:06, 19606.03it/s] 72%|███████▏  | 322685/450337 [00:16<00:06, 19613.26it/s] 72%|███████▏  | 324700/450337 [00:16<00:06, 19770.22it/s] 73%|███████▎  | 326679/450337 [00:16<00:06, 19535.03it/s] 73%|███████▎  | 328996/450337 [00:16<00:05, 20606.14it/s] 74%|███████▎  | 331060/450337 [00:16<00:05, 19945.19it/s] 74%|███████▍  | 333061/450337 [00:16<00:05, 19714.19it/s] 74%|███████▍  | 335086/450337 [00:16<00:05, 19859.37it/s] 75%|███████▍  | 337148/450337 [00:16<00:05, 20078.95it/s] 75%|███████▌  | 339188/450337 [00:16<00:05, 20171.13it/s] 76%|███████▌  | 341208/450337 [00:16<00:05, 19761.93it/s] 76%|███████▌  | 343233/450337 [00:17<00:05, 19903.63it/s] 77%|███████▋  | 345306/450337 [00:17<00:05, 20145.42it/s] 77%|███████▋  | 347323/450337 [00:17<00:05, 19835.91it/s] 78%|███████▊  | 349309/450337 [00:17<00:05, 19779.56it/s] 78%|███████▊  | 351532/450337 [00:17<00:04, 20502.82it/s] 79%|███████▊  | 353585/450337 [00:17<00:04, 19766.73it/s] 79%|███████▉  | 355569/450337 [00:17<00:04, 19711.18it/s] 79%|███████▉  | 357545/450337 [00:17<00:04, 19418.47it/s] 80%|███████▉  | 359561/450337 [00:17<00:04, 19632.63it/s] 80%|████████  | 361704/450337 [00:17<00:04, 20159.37it/s] 81%|████████  | 363879/450337 [00:18<00:04, 20626.56it/s] 81%|████████▏ | 366061/450337 [00:18<00:04, 20978.39it/s] 82%|████████▏ | 368162/450337 [00:18<00:04, 20111.18it/s] 82%|████████▏ | 370283/450337 [00:18<00:03, 20428.83it/s] 83%|████████▎ | 372334/450337 [00:18<00:03, 19951.46it/s] 83%|████████▎ | 374449/450337 [00:18<00:03, 20298.00it/s] 84%|████████▎ | 376485/450337 [00:18<00:03, 19927.70it/s] 84%|████████▍ | 378483/450337 [00:18<00:03, 19800.96it/s] 85%|████████▍ | 380626/450337 [00:18<00:03, 20273.25it/s] 85%|████████▍ | 382657/450337 [00:19<00:03, 19961.87it/s] 85%|████████▌ | 384724/450337 [00:19<00:03, 20167.64it/s] 86%|████████▌ | 386744/450337 [00:19<00:03, 19905.31it/s] 86%|████████▋ | 388868/450337 [00:19<00:03, 20294.49it/s] 87%|████████▋ | 390972/450337 [00:19<00:02, 20506.98it/s] 87%|████████▋ | 393025/450337 [00:19<00:02, 19680.87it/s] 88%|████████▊ | 395001/450337 [00:19<00:02, 19680.10it/s] 88%|████████▊ | 396975/450337 [00:19<00:02, 19659.08it/s] 89%|████████▊ | 399066/450337 [00:19<00:02, 20026.37it/s] 89%|████████▉ | 401126/450337 [00:19<00:02, 20190.92it/s] 90%|████████▉ | 403148/450337 [00:20<00:02, 19433.46it/s] 90%|████████▉ | 405099/450337 [00:20<00:02, 19419.36it/s] 90%|█████████ | 407047/450337 [00:20<00:02, 19399.54it/s] 91%|█████████ | 409103/450337 [00:20<00:02, 19740.25it/s] 91%|█████████▏| 411081/450337 [00:20<00:02, 19487.39it/s] 92%|█████████▏| 413033/450337 [00:20<00:01, 18802.46it/s] 92%|█████████▏| 414920/450337 [00:20<00:01, 18578.95it/s] 93%|█████████▎| 416866/450337 [00:20<00:01, 18823.17it/s] 93%|█████████▎| 418887/450337 [00:20<00:01, 19224.44it/s] 93%|█████████▎| 420881/450337 [00:20<00:01, 19433.97it/s] 94%|█████████▍| 422828/450337 [00:21<00:01, 19350.32it/s] 94%|█████████▍| 424766/450337 [00:21<00:01, 19291.25it/s] 95%|█████████▍| 426875/450337 [00:21<00:01, 19822.61it/s] 95%|█████████▌| 428887/450337 [00:21<00:01, 19904.77it/s] 96%|█████████▌| 430894/450337 [00:21<00:00, 19952.88it/s] 96%|█████████▌| 433062/450337 [00:21<00:00, 20465.59it/s] 97%|█████████▋| 435110/450337 [00:21<00:00, 19793.75it/s] 97%|█████████▋| 437114/450337 [00:21<00:00, 19864.81it/s] 98%|█████████▊| 439105/450337 [00:21<00:00, 19789.21it/s] 98%|█████████▊| 441087/450337 [00:22<00:00, 19484.24it/s] 98%|█████████▊| 443153/450337 [00:22<00:00, 19822.94it/s] 99%|█████████▉| 445138/450337 [00:22<00:00, 19361.15it/s] 99%|█████████▉| 447078/450337 [00:22<00:00, 19096.02it/s]100%|█████████▉| 449113/450337 [00:22<00:00, 19458.83it/s]100%|██████████| 450337/450337 [00:22<00:00, 20032.65it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 16.29it/s]2022-03-03 20:39:35 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 20:39:35 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 20:39:35 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 20:39:35 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 20:39:35 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 20:39:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 20:39:35 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 20:39:35 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 20:39:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:35 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 20:39:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:35 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 20:39:35 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 20:39:35 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_last.pt
2022-03-03 20:39:35 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_last.pt
2022-03-03 20:39:35 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 20:39:35 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 20:39:36 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 20:39:36 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 20:39:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 20:39:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 20:39:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 20:40:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:40:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 20:47:40 | INFO | train_inner | epoch 001:    105 / 393 loss=16.919, ppl=123953, wps=14674.3, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.424, loss_scale=4, train_wall=479, gb_free=10.1, wall=485
2022-03-03 20:55:06 | INFO | train_inner | epoch 001:    205 / 393 loss=14.473, ppl=22734.6, wps=14682.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.566, loss_scale=4, train_wall=441, gb_free=10.1, wall=931
2022-03-03 21:02:33 | INFO | train_inner | epoch 001:    305 / 393 loss=12.376, ppl=5314.29, wps=14676.6, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.065, loss_scale=4, train_wall=442, gb_free=10.1, wall=1378
2022-03-03 21:09:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:09:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.481 | ppl 1429.49 | wps 33639.8 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 21:09:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 21:09:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 21:09:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 21:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.481) (writing took 4.842015285976231 seconds)
2022-03-03 21:09:15 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 21:09:15 | INFO | train | epoch 001 | loss 13.76 | ppl 13871.6 | wps 14584 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.707 | loss_scale 4 | train_wall 1749 | gb_free 10.1 | wall 1780
2022-03-03 21:09:15 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 21:09:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:10:09 | INFO | train_inner | epoch 002:     12 / 393 loss=10.868, ppl=1868.38, wps=14318.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.626, loss_scale=4, train_wall=439, gb_free=10.1, wall=1833
2022-03-03 21:17:35 | INFO | train_inner | epoch 002:    112 / 393 loss=10.267, ppl=1232.32, wps=14691.3, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.473, loss_scale=4, train_wall=441, gb_free=10.1, wall=2279
2022-03-03 21:25:01 | INFO | train_inner | epoch 002:    212 / 393 loss=9.979, ppl=1009.45, wps=14691.8, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.513, loss_scale=8, train_wall=441, gb_free=10.1, wall=2725
2022-03-03 21:32:27 | INFO | train_inner | epoch 002:    312 / 393 loss=9.744, ppl=857.68, wps=14683.3, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.567, loss_scale=8, train_wall=441, gb_free=10.1, wall=3172
2022-03-03 21:38:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:38:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.431 | ppl 690.44 | wps 33542.2 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.431
2022-03-03 21:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 21:38:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 21:38:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 21:38:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.431) (writing took 4.519660695455968 seconds)
2022-03-03 21:38:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 21:38:38 | INFO | train | epoch 002 | loss 9.923 | ppl 970.9 | wps 14594.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.535 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 3542
2022-03-03 21:38:38 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 21:38:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:40:03 | INFO | train_inner | epoch 003:     19 / 393 loss=9.537, ppl=742.91, wps=14325.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.636, loss_scale=8, train_wall=439, gb_free=10.1, wall=3627
2022-03-03 21:47:29 | INFO | train_inner | epoch 003:    119 / 393 loss=9.325, ppl=641.38, wps=14686.2, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.719, loss_scale=8, train_wall=441, gb_free=10.1, wall=4073
2022-03-03 21:54:55 | INFO | train_inner | epoch 003:    219 / 393 loss=9.158, ppl=571.4, wps=14683.9, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.804, loss_scale=8, train_wall=441, gb_free=10.1, wall=4520
2022-03-03 22:02:21 | INFO | train_inner | epoch 003:    319 / 393 loss=9.009, ppl=515.28, wps=14680.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.822, loss_scale=16, train_wall=442, gb_free=10.1, wall=4966
2022-03-03 22:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:07:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.808 | ppl 448.05 | wps 33570.3 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 8.808
2022-03-03 22:07:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 22:07:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 22:08:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 22:08:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 3 @ 1174 updates, score 8.808) (writing took 4.469796071760356 seconds)
2022-03-03 22:08:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 22:08:01 | INFO | train | epoch 003 | loss 9.125 | ppl 558.41 | wps 14591.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.782 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 5305
2022-03-03 22:08:01 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 22:08:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:09:57 | INFO | train_inner | epoch 004:     26 / 393 loss=8.859, ppl=464.28, wps=14323.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.813, loss_scale=16, train_wall=440, gb_free=10.1, wall=5422
2022-03-03 22:17:23 | INFO | train_inner | epoch 004:    126 / 393 loss=8.705, ppl=417.34, wps=14683.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.78, loss_scale=16, train_wall=441, gb_free=10.1, wall=5868
2022-03-03 22:24:50 | INFO | train_inner | epoch 004:    226 / 393 loss=8.595, ppl=386.81, wps=14681.6, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.823, loss_scale=16, train_wall=441, gb_free=10.1, wall=6314
2022-03-03 22:32:16 | INFO | train_inner | epoch 004:    326 / 393 loss=8.494, ppl=360.65, wps=14681.1, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.829, loss_scale=16, train_wall=441, gb_free=10.1, wall=6761
2022-03-03 22:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:37:20 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.345 | ppl 325.14 | wps 33601.4 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.345
2022-03-03 22:37:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 22:37:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 22:37:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 22:37:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.345) (writing took 4.549973887391388 seconds)
2022-03-03 22:37:24 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 22:37:24 | INFO | train | epoch 004 | loss 8.576 | ppl 381.56 | wps 14589.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.817 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 7069
2022-03-03 22:37:24 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 22:37:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:39:52 | INFO | train_inner | epoch 005:     33 / 393 loss=8.362, ppl=329.02, wps=14323.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.813, loss_scale=32, train_wall=440, gb_free=10.1, wall=7216
2022-03-03 22:47:18 | INFO | train_inner | epoch 005:    133 / 393 loss=8.24, ppl=302.36, wps=14684.9, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.814, loss_scale=32, train_wall=441, gb_free=10.1, wall=7662
2022-03-03 22:54:44 | INFO | train_inner | epoch 005:    233 / 393 loss=8.16, ppl=286.01, wps=14684.8, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.826, loss_scale=32, train_wall=441, gb_free=10.1, wall=8109
2022-03-03 22:55:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 23:02:15 | INFO | train_inner | epoch 005:    334 / 393 loss=8.069, ppl=268.52, wps=14538.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.803, loss_scale=16, train_wall=446, gb_free=10.1, wall=8560
2022-03-03 23:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:06:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.999 | ppl 255.87 | wps 33879.7 | wpb 2034.1 | bsz 4 | num_updates 1959 | best_loss 7.999
2022-03-03 23:06:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1959 updates
2022-03-03 23:06:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 23:06:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 23:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 5 @ 1959 updates, score 7.999) (writing took 4.498166335746646 seconds)
2022-03-03 23:06:47 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 23:06:47 | INFO | train | epoch 005 | loss 8.144 | ppl 282.94 | wps 14554.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 1959 | lr 0.000244926 | gnorm 0.807 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 8832
2022-03-03 23:06:47 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 23:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:09:50 | INFO | train_inner | epoch 006:     41 / 393 loss=7.958, ppl=248.57, wps=14325, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.772, loss_scale=16, train_wall=440, gb_free=10.1, wall=9015
2022-03-03 23:17:17 | INFO | train_inner | epoch 006:    141 / 393 loss=7.852, ppl=231.08, wps=14685.3, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.784, loss_scale=16, train_wall=441, gb_free=10.1, wall=9461
2022-03-03 23:24:43 | INFO | train_inner | epoch 006:    241 / 393 loss=7.79, ppl=221.34, wps=14682.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.746, loss_scale=16, train_wall=441, gb_free=10.1, wall=9908
2022-03-03 23:32:09 | INFO | train_inner | epoch 006:    341 / 393 loss=7.728, ppl=211.94, wps=14681, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.772, loss_scale=16, train_wall=442, gb_free=10.1, wall=10354
2022-03-03 23:36:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:36:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.735 | ppl 212.97 | wps 33661.4 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 7.735
2022-03-03 23:36:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 23:36:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 23:36:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-03 23:36:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 6 @ 2352 updates, score 7.735) (writing took 4.4050117721781135 seconds)
2022-03-03 23:36:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 23:36:10 | INFO | train | epoch 006 | loss 7.785 | ppl 220.59 | wps 14591 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.763 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 10595
2022-03-03 23:36:10 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 23:36:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:39:45 | INFO | train_inner | epoch 007:     48 / 393 loss=7.615, ppl=196.04, wps=14329.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.758, loss_scale=32, train_wall=440, gb_free=10.1, wall=10809
2022-03-03 23:47:11 | INFO | train_inner | epoch 007:    148 / 393 loss=7.521, ppl=183.62, wps=14680.7, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.728, loss_scale=32, train_wall=442, gb_free=10.1, wall=11256
2022-03-03 23:51:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 23:54:42 | INFO | train_inner | epoch 007:    249 / 393 loss=7.486, ppl=179.25, wps=14537.4, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.742, loss_scale=16, train_wall=446, gb_free=10.1, wall=11707
2022-03-04 00:02:08 | INFO | train_inner | epoch 007:    349 / 393 loss=7.433, ppl=172.76, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.717, loss_scale=16, train_wall=441, gb_free=10.1, wall=12153
2022-03-04 00:05:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:05:29 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.525 | ppl 184.22 | wps 33731.7 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 7.525
2022-03-04 00:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-04 00:05:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 00:05:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 00:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 7 @ 2744 updates, score 7.525) (writing took 4.366517557762563 seconds)
2022-03-04 00:05:34 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 00:05:34 | INFO | train | epoch 007 | loss 7.481 | ppl 178.69 | wps 14554.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.736 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 12358
2022-03-04 00:05:34 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 00:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:09:44 | INFO | train_inner | epoch 008:     56 / 393 loss=7.333, ppl=161.22, wps=14327.8, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.724, loss_scale=16, train_wall=440, gb_free=10.1, wall=12608
2022-03-04 00:17:10 | INFO | train_inner | epoch 008:    156 / 393 loss=7.245, ppl=151.66, wps=14680.7, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.703, loss_scale=16, train_wall=442, gb_free=10.1, wall=13055
2022-03-04 00:24:36 | INFO | train_inner | epoch 008:    256 / 393 loss=7.224, ppl=149.52, wps=14679.3, ups=0.22, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.696, loss_scale=16, train_wall=442, gb_free=10.1, wall=13501
2022-03-04 00:32:03 | INFO | train_inner | epoch 008:    356 / 393 loss=7.198, ppl=146.78, wps=14680.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.695, loss_scale=32, train_wall=442, gb_free=10.1, wall=13947
2022-03-04 00:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:34:52 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.342 | ppl 162.22 | wps 33813.8 | wpb 2034.1 | bsz 4 | num_updates 3137 | best_loss 7.342
2022-03-04 00:34:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3137 updates
2022-03-04 00:34:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 00:34:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 00:34:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 8 @ 3137 updates, score 7.342) (writing took 4.368945142254233 seconds)
2022-03-04 00:34:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 00:34:57 | INFO | train | epoch 008 | loss 7.227 | ppl 149.8 | wps 14590.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3137 | lr 0.000392147 | gnorm 0.703 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 14121
2022-03-04 00:34:57 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 00:34:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:39:38 | INFO | train_inner | epoch 009:     63 / 393 loss=7.09, ppl=136.23, wps=14332.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.701, loss_scale=32, train_wall=440, gb_free=10.1, wall=14403
2022-03-04 00:46:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 00:47:09 | INFO | train_inner | epoch 009:    164 / 393 loss=7.029, ppl=130.63, wps=14539.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.665, loss_scale=16, train_wall=446, gb_free=10.1, wall=14853
2022-03-04 00:54:35 | INFO | train_inner | epoch 009:    264 / 393 loss=7.017, ppl=129.55, wps=14680.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.685, loss_scale=16, train_wall=442, gb_free=10.1, wall=15300
2022-03-04 01:02:02 | INFO | train_inner | epoch 009:    364 / 393 loss=6.991, ppl=127.17, wps=14681.5, ups=0.22, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.643, loss_scale=16, train_wall=442, gb_free=10.1, wall=15746
2022-03-04 01:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:04:15 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.224 | ppl 149.54 | wps 33822.6 | wpb 2034.1 | bsz 4 | num_updates 3529 | best_loss 7.224
2022-03-04 01:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3529 updates
2022-03-04 01:04:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 01:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 01:04:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 9 @ 3529 updates, score 7.224) (writing took 4.3598063960671425 seconds)
2022-03-04 01:04:20 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 01:04:20 | INFO | train | epoch 009 | loss 7.014 | ppl 129.28 | wps 14554.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3529 | lr 0.000441137 | gnorm 0.673 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 15884
2022-03-04 01:04:20 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 01:04:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:09:37 | INFO | train_inner | epoch 010:     71 / 393 loss=6.878, ppl=117.64, wps=14330.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.685, loss_scale=16, train_wall=440, gb_free=10.1, wall=16202
2022-03-04 01:17:03 | INFO | train_inner | epoch 010:    171 / 393 loss=6.833, ppl=113.99, wps=14682, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.646, loss_scale=16, train_wall=441, gb_free=10.1, wall=16648
2022-03-04 01:24:30 | INFO | train_inner | epoch 010:    271 / 393 loss=6.84, ppl=114.53, wps=14678.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.65, loss_scale=32, train_wall=442, gb_free=10.1, wall=17094
2022-03-04 01:25:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 01:32:00 | INFO | train_inner | epoch 010:    372 / 393 loss=6.832, ppl=113.95, wps=14538.4, ups=0.22, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.643, loss_scale=16, train_wall=446, gb_free=10.1, wall=17545
2022-03-04 01:33:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:33:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.117 | ppl 138.84 | wps 33857.5 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.117
2022-03-04 01:33:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-04 01:33:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 01:33:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 01:33:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.117) (writing took 4.357638037763536 seconds)
2022-03-04 01:33:43 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 01:33:43 | INFO | train | epoch 010 | loss 6.834 | ppl 114.06 | wps 14552.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.652 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 17648
2022-03-04 01:33:43 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 01:33:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:39:36 | INFO | train_inner | epoch 011:     79 / 393 loss=6.691, ppl=103.35, wps=14329.3, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.654, loss_scale=16, train_wall=440, gb_free=10.1, wall=18000
2022-03-04 01:47:02 | INFO | train_inner | epoch 011:    179 / 393 loss=6.672, ppl=101.96, wps=14683, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.625, loss_scale=16, train_wall=441, gb_free=10.1, wall=18447
2022-03-04 01:54:28 | INFO | train_inner | epoch 011:    279 / 393 loss=6.674, ppl=102.13, wps=14683.4, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.608, loss_scale=16, train_wall=441, gb_free=10.1, wall=18893
2022-03-04 02:01:55 | INFO | train_inner | epoch 011:    379 / 393 loss=6.683, ppl=102.78, wps=14681.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.607, loss_scale=16, train_wall=441, gb_free=10.1, wall=19339
2022-03-04 02:02:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:03:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.029 | ppl 130.61 | wps 33937.3 | wpb 2034.1 | bsz 4 | num_updates 4314 | best_loss 7.029
2022-03-04 02:03:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4314 updates
2022-03-04 02:03:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 02:03:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 02:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 11 @ 4314 updates, score 7.029) (writing took 4.363265696913004 seconds)
2022-03-04 02:03:06 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 02:03:06 | INFO | train | epoch 011 | loss 6.672 | ppl 101.98 | wps 14591.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4314 | lr 0.00048146 | gnorm 0.619 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 19411
2022-03-04 02:03:06 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 02:03:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:07:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:09:35 | INFO | train_inner | epoch 012:     87 / 393 loss=6.525, ppl=92.12, wps=14191.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.6, loss_scale=16, train_wall=444, gb_free=10.1, wall=19799
2022-03-04 02:17:01 | INFO | train_inner | epoch 012:    187 / 393 loss=6.517, ppl=91.59, wps=14682.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.59, loss_scale=16, train_wall=441, gb_free=10.1, wall=20246
2022-03-04 02:24:27 | INFO | train_inner | epoch 012:    287 / 393 loss=6.529, ppl=92.35, wps=14680.9, ups=0.22, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.571, loss_scale=16, train_wall=442, gb_free=10.1, wall=20692
2022-03-04 02:31:54 | INFO | train_inner | epoch 012:    387 / 393 loss=6.523, ppl=91.94, wps=14679.9, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.57, loss_scale=16, train_wall=442, gb_free=10.1, wall=21138
2022-03-04 02:32:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:32:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.997 | ppl 127.75 | wps 33862.2 | wpb 2034.1 | bsz 4 | num_updates 4706 | best_loss 6.997
2022-03-04 02:32:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4706 updates
2022-03-04 02:32:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 02:32:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 02:32:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 12 @ 4706 updates, score 6.997) (writing took 4.384524370543659 seconds)
2022-03-04 02:32:29 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 02:32:29 | INFO | train | epoch 012 | loss 6.519 | ppl 91.7 | wps 14553.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4706 | lr 0.000460971 | gnorm 0.586 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 21174
2022-03-04 02:32:29 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 02:32:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:39:29 | INFO | train_inner | epoch 013:     94 / 393 loss=6.376, ppl=83.07, wps=14329.1, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.6, loss_scale=16, train_wall=440, gb_free=10.1, wall=21594
2022-03-04 02:45:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:47:00 | INFO | train_inner | epoch 013:    195 / 393 loss=6.385, ppl=83.57, wps=14536.9, ups=0.22, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.568, loss_scale=16, train_wall=446, gb_free=10.1, wall=22045
2022-03-04 02:54:26 | INFO | train_inner | epoch 013:    295 / 393 loss=6.398, ppl=84.33, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.553, loss_scale=16, train_wall=441, gb_free=10.1, wall=22491
2022-03-04 03:01:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:01:48 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.981 | ppl 126.29 | wps 33901.7 | wpb 2034.1 | bsz 4 | num_updates 5098 | best_loss 6.981
2022-03-04 03:01:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5098 updates
2022-03-04 03:01:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 03:01:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 03:01:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 13 @ 5098 updates, score 6.981) (writing took 4.393114651553333 seconds)
2022-03-04 03:01:52 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 03:01:52 | INFO | train | epoch 013 | loss 6.391 | ppl 83.91 | wps 14555.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5098 | lr 0.000442894 | gnorm 0.568 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 22937
2022-03-04 03:01:52 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 03:01:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:02:01 | INFO | train_inner | epoch 014:      2 / 393 loss=6.41, ppl=85.06, wps=14332.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.574, loss_scale=16, train_wall=440, gb_free=10.1, wall=22946
2022-03-04 03:09:28 | INFO | train_inner | epoch 014:    102 / 393 loss=6.237, ppl=75.44, wps=14683, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.554, loss_scale=16, train_wall=441, gb_free=10.1, wall=23392
2022-03-04 03:16:54 | INFO | train_inner | epoch 014:    202 / 393 loss=6.279, ppl=77.67, wps=14683.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.565, loss_scale=16, train_wall=441, gb_free=10.1, wall=23839
2022-03-04 03:23:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 03:24:25 | INFO | train_inner | epoch 014:    303 / 393 loss=6.304, ppl=78.99, wps=14539.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.55, loss_scale=16, train_wall=446, gb_free=10.1, wall=24289
2022-03-04 03:31:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:31:11 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.935 | ppl 122.36 | wps 34019.6 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 6.935
2022-03-04 03:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-04 03:31:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 03:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 03:31:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 14 @ 5490 updates, score 6.935) (writing took 4.135289550758898 seconds)
2022-03-04 03:31:15 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 03:31:15 | INFO | train | epoch 014 | loss 6.281 | ppl 77.76 | wps 14558 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.557 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 24700
2022-03-04 03:31:15 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 03:31:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:32:00 | INFO | train_inner | epoch 015:     10 / 393 loss=6.288, ppl=78.17, wps=14340.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.544, loss_scale=16, train_wall=440, gb_free=10.1, wall=24744
2022-03-04 03:39:26 | INFO | train_inner | epoch 015:    110 / 393 loss=6.146, ppl=70.8, wps=14684.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.574, loss_scale=16, train_wall=441, gb_free=10.1, wall=25191
2022-03-04 03:46:52 | INFO | train_inner | epoch 015:    210 / 393 loss=6.181, ppl=72.57, wps=14681.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.558, loss_scale=16, train_wall=441, gb_free=10.1, wall=25637
2022-03-04 03:54:19 | INFO | train_inner | epoch 015:    310 / 393 loss=6.207, ppl=73.88, wps=14678.4, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.541, loss_scale=16, train_wall=442, gb_free=10.1, wall=26084
2022-03-04 04:00:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:00:34 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.924 | ppl 121.4 | wps 33885.5 | wpb 2034.1 | bsz 4 | num_updates 5883 | best_loss 6.924
2022-03-04 04:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5883 updates
2022-03-04 04:00:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 04:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt
2022-03-04 04:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#1/checkpoint_best.pt (epoch 15 @ 5883 updates, score 6.924) (writing took 4.220299605280161 seconds)
2022-03-04 04:00:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 04:00:38 | INFO | train | epoch 015 | loss 6.185 | ppl 72.78 | wps 14590.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5883 | lr 0.000412288 | gnorm 0.556 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 26463
2022-03-04 04:00:38 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 04:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:01:54 | INFO | train_inner | epoch 016:     17 / 393 loss=6.19, ppl=73.02, wps=14330.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.55, loss_scale=16, train_wall=440, gb_free=10.1, wall=26539
2022-03-04 04:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 04:09:25 | INFO | train_inner | epoch 016:    118 / 393 loss=6.055, ppl=66.51, wps=14539.4, ups=0.22, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.546, loss_scale=16, train_wall=446, gb_free=10.1, wall=26990
2022-03-04 04:16:51 | INFO | train_inner | epoch 016:    218 / 393 loss=6.098, ppl=68.49, wps=14686.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.57, loss_scale=16, train_wall=441, gb_free=10.1, wall=27436
2022-03-04 04:24:17 | INFO | train_inner | epoch 016:    318 / 393 loss=6.127, ppl=69.89, wps=14685.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.561, loss_scale=16, train_wall=441, gb_free=10.1, wall=27882
2022-03-04 04:29:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:29:57 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.947 | ppl 123.38 | wps 33932.2 | wpb 2034.1 | bsz 4 | num_updates 6275 | best_loss 6.924
2022-03-04 04:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6275 updates
2022-03-04 04:29:57 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 04:29:57 | INFO | train | epoch 016 | loss 6.101 | ppl 68.62 | wps 14593.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6275 | lr 0.000399202 | gnorm 0.555 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 28221
2022-03-04 04:29:57 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 04:29:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:31:48 | INFO | train_inner | epoch 017:     25 / 393 loss=6.094, ppl=68.33, wps=14470.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.546, loss_scale=16, train_wall=440, gb_free=10.1, wall=28333
2022-03-04 04:39:15 | INFO | train_inner | epoch 017:    125 / 393 loss=5.989, ppl=63.49, wps=14681.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.554, loss_scale=16, train_wall=441, gb_free=10.1, wall=28779
2022-03-04 04:46:41 | INFO | train_inner | epoch 017:    225 / 393 loss=6.026, ppl=65.18, wps=14682.8, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.551, loss_scale=32, train_wall=441, gb_free=10.1, wall=29226
2022-03-04 04:47:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 04:54:12 | INFO | train_inner | epoch 017:    326 / 393 loss=6.047, ppl=66.12, wps=14542.4, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.563, loss_scale=16, train_wall=446, gb_free=10.1, wall=29676
2022-03-04 04:59:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:59:15 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.938 | ppl 122.6 | wps 33710.6 | wpb 2034.1 | bsz 4 | num_updates 6667 | best_loss 6.924
2022-03-04 04:59:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6667 updates
2022-03-04 04:59:15 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 04:59:15 | INFO | train | epoch 017 | loss 6.024 | ppl 65.09 | wps 14591.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6667 | lr 0.000387289 | gnorm 0.551 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 29980
2022-03-04 04:59:15 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 04:59:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:01:43 | INFO | train_inner | epoch 018:     33 / 393 loss=6.012, ppl=64.55, wps=14469.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.555, loss_scale=16, train_wall=440, gb_free=10.1, wall=30127
2022-03-04 05:09:09 | INFO | train_inner | epoch 018:    133 / 393 loss=5.909, ppl=60.07, wps=14682.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.544, loss_scale=16, train_wall=441, gb_free=10.1, wall=30574
2022-03-04 05:16:35 | INFO | train_inner | epoch 018:    233 / 393 loss=5.953, ppl=61.95, wps=14682.5, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=31020
2022-03-04 05:24:01 | INFO | train_inner | epoch 018:    333 / 393 loss=5.993, ppl=63.69, wps=14687.9, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.536, loss_scale=16, train_wall=441, gb_free=10.1, wall=31466
2022-03-04 05:26:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 05:28:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:28:34 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.957 | ppl 124.23 | wps 33770.6 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 6.924
2022-03-04 05:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-04 05:28:34 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 05:28:34 | INFO | train | epoch 018 | loss 5.956 | ppl 62.07 | wps 14591.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.564 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 31738
2022-03-04 05:28:34 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 05:28:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:31:37 | INFO | train_inner | epoch 019:     41 / 393 loss=5.934, ppl=61.14, wps=14326.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.563, loss_scale=16, train_wall=444, gb_free=10.1, wall=31922
2022-03-04 05:39:03 | INFO | train_inner | epoch 019:    141 / 393 loss=5.861, ppl=58.14, wps=14685.7, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.568, loss_scale=16, train_wall=441, gb_free=10.1, wall=32368
2022-03-04 05:46:30 | INFO | train_inner | epoch 019:    241 / 393 loss=5.893, ppl=59.41, wps=14681.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.557, loss_scale=16, train_wall=441, gb_free=10.1, wall=32814
2022-03-04 05:53:56 | INFO | train_inner | epoch 019:    341 / 393 loss=5.924, ppl=60.73, wps=14682.4, ups=0.22, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.596, loss_scale=16, train_wall=441, gb_free=10.1, wall=33261
2022-03-04 05:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:57:52 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.97 | ppl 125.38 | wps 33957.8 | wpb 2034.1 | bsz 4 | num_updates 7452 | best_loss 6.924
2022-03-04 05:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7452 updates
2022-03-04 05:57:52 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 05:57:52 | INFO | train | epoch 019 | loss 5.893 | ppl 59.42 | wps 14628.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7452 | lr 0.000366322 | gnorm 0.561 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 33497
2022-03-04 05:57:52 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 05:57:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:01:27 | INFO | train_inner | epoch 020:     48 / 393 loss=5.859, ppl=58.03, wps=14470.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.55, loss_scale=16, train_wall=440, gb_free=10.1, wall=33711
2022-03-04 06:04:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 06:08:58 | INFO | train_inner | epoch 020:    149 / 393 loss=5.801, ppl=55.77, wps=14539.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.566, loss_scale=16, train_wall=446, gb_free=10.1, wall=34162
2022-03-04 06:16:24 | INFO | train_inner | epoch 020:    249 / 393 loss=5.838, ppl=57.19, wps=14684.5, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.56, loss_scale=16, train_wall=441, gb_free=10.1, wall=34608
2022-03-04 06:23:50 | INFO | train_inner | epoch 020:    349 / 393 loss=5.876, ppl=58.74, wps=14679.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.609, loss_scale=16, train_wall=442, gb_free=10.1, wall=35055
2022-03-04 06:27:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:27:11 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.001 | ppl 128.1 | wps 33981.7 | wpb 2034.1 | bsz 4 | num_updates 7844 | best_loss 6.924
2022-03-04 06:27:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7844 updates
2022-03-04 06:27:11 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 06:27:11 | INFO | train | epoch 020 | loss 5.835 | ppl 57.09 | wps 14590.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7844 | lr 0.000357052 | gnorm 0.573 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 35256
2022-03-04 06:27:11 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 06:27:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:31:21 | INFO | train_inner | epoch 021:     56 / 393 loss=5.792, ppl=55.41, wps=14467.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.552, loss_scale=16, train_wall=440, gb_free=10.1, wall=35506
2022-03-04 06:38:48 | INFO | train_inner | epoch 021:    156 / 393 loss=5.752, ppl=53.88, wps=14683.8, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.565, loss_scale=16, train_wall=441, gb_free=10.1, wall=35952
2022-03-04 06:44:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 06:46:18 | INFO | train_inner | epoch 021:    257 / 393 loss=5.796, ppl=55.56, wps=14535.6, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.584, loss_scale=16, train_wall=446, gb_free=10.1, wall=36403
2022-03-04 06:53:45 | INFO | train_inner | epoch 021:    357 / 393 loss=5.805, ppl=55.92, wps=14685.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.56, loss_scale=16, train_wall=441, gb_free=10.1, wall=36849
2022-03-04 06:56:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:56:30 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.016 | ppl 129.38 | wps 33895.6 | wpb 2034.1 | bsz 4 | num_updates 8236 | best_loss 6.924
2022-03-04 06:56:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8236 updates
2022-03-04 06:56:30 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 06:56:30 | INFO | train | epoch 021 | loss 5.781 | ppl 55 | wps 14590.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8236 | lr 0.000348451 | gnorm 0.57 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 37015
2022-03-04 06:56:30 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 06:56:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:01:16 | INFO | train_inner | epoch 022:     64 / 393 loss=5.733, ppl=53.18, wps=14469.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.566, loss_scale=16, train_wall=440, gb_free=10.1, wall=37300
2022-03-04 07:04:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:08:46 | INFO | train_inner | epoch 022:    165 / 393 loss=5.705, ppl=52.17, wps=14545.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.579, loss_scale=8, train_wall=446, gb_free=10.1, wall=37751
2022-03-04 07:16:12 | INFO | train_inner | epoch 022:    265 / 393 loss=5.742, ppl=53.53, wps=14687.1, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.559, loss_scale=8, train_wall=441, gb_free=10.1, wall=38197
2022-03-04 07:23:39 | INFO | train_inner | epoch 022:    365 / 393 loss=5.775, ppl=54.77, wps=14683, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=38643
2022-03-04 07:25:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:25:48 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.05 | ppl 132.5 | wps 33928.1 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 6.924
2022-03-04 07:25:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-04 07:25:48 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 07:25:48 | INFO | train | epoch 022 | loss 5.731 | ppl 53.13 | wps 14594.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.567 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 38773
2022-03-04 07:25:48 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 07:25:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:31:09 | INFO | train_inner | epoch 023:     72 / 393 loss=5.665, ppl=50.72, wps=14475.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.577, loss_scale=8, train_wall=439, gb_free=10.1, wall=39094
2022-03-04 07:38:36 | INFO | train_inner | epoch 023:    172 / 393 loss=5.656, ppl=50.43, wps=14688.1, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.585, loss_scale=8, train_wall=441, gb_free=10.1, wall=39540
2022-03-04 07:46:02 | INFO | train_inner | epoch 023:    272 / 393 loss=5.703, ppl=52.1, wps=14688, ups=0.22, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.593, loss_scale=16, train_wall=441, gb_free=10.1, wall=39986
2022-03-04 07:53:28 | INFO | train_inner | epoch 023:    372 / 393 loss=5.733, ppl=53.2, wps=14678.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.588, loss_scale=16, train_wall=442, gb_free=10.1, wall=40433
2022-03-04 07:55:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:55:07 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.053 | ppl 132.75 | wps 33927.1 | wpb 2034.1 | bsz 4 | num_updates 9021 | best_loss 6.924
2022-03-04 07:55:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9021 updates
2022-03-04 07:55:07 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 07:55:07 | INFO | train | epoch 023 | loss 5.685 | ppl 51.46 | wps 14630.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9021 | lr 0.000332945 | gnorm 0.584 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 40531
2022-03-04 07:55:07 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 07:55:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:00:59 | INFO | train_inner | epoch 024:     79 / 393 loss=5.61, ppl=48.83, wps=14471.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.603, loss_scale=16, train_wall=440, gb_free=10.1, wall=40884
2022-03-04 08:08:26 | INFO | train_inner | epoch 024:    179 / 393 loss=5.614, ppl=48.99, wps=14680.7, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.586, loss_scale=16, train_wall=442, gb_free=10.1, wall=41330
2022-03-04 08:15:52 | INFO | train_inner | epoch 024:    279 / 393 loss=5.663, ppl=50.67, wps=14680.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.585, loss_scale=16, train_wall=442, gb_free=10.1, wall=41777
2022-03-04 08:22:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 08:23:23 | INFO | train_inner | epoch 024:    380 / 393 loss=5.69, ppl=51.63, wps=14540.5, ups=0.22, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.588, loss_scale=16, train_wall=446, gb_free=10.1, wall=42227
2022-03-04 08:23:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:24:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:24:25 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.082 | ppl 135.52 | wps 33928.2 | wpb 2034.1 | bsz 4 | num_updates 9412 | best_loss 6.924
2022-03-04 08:24:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9412 updates
2022-03-04 08:24:25 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 08:24:25 | INFO | train | epoch 024 | loss 5.641 | ppl 49.91 | wps 14554.7 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 9412 | lr 0.000325956 | gnorm 0.595 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 42290
2022-03-04 08:24:25 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 08:24:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:30:58 | INFO | train_inner | epoch 025:     88 / 393 loss=5.558, ppl=47.1, wps=14330.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.652, loss_scale=8, train_wall=444, gb_free=10.1, wall=42683
2022-03-04 08:38:24 | INFO | train_inner | epoch 025:    188 / 393 loss=5.579, ppl=47.79, wps=14683, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.584, loss_scale=8, train_wall=441, gb_free=10.1, wall=43129
2022-03-04 08:45:51 | INFO | train_inner | epoch 025:    288 / 393 loss=5.621, ppl=49.2, wps=14682.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.604, loss_scale=8, train_wall=442, gb_free=10.1, wall=43575
2022-03-04 08:53:17 | INFO | train_inner | epoch 025:    388 / 393 loss=5.658, ppl=50.48, wps=14686.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.607, loss_scale=8, train_wall=441, gb_free=10.1, wall=44021
2022-03-04 08:53:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:53:44 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.078 | ppl 135.14 | wps 33896.1 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 6.924
2022-03-04 08:53:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-04 08:53:44 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 08:53:44 | INFO | train | epoch 025 | loss 5.601 | ppl 48.54 | wps 14629 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.61 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 44048
2022-03-04 08:53:44 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 08:53:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:00:48 | INFO | train_inner | epoch 026:     95 / 393 loss=5.507, ppl=45.48, wps=14473.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.588, loss_scale=8, train_wall=439, gb_free=10.1, wall=44472
2022-03-04 09:08:14 | INFO | train_inner | epoch 026:    195 / 393 loss=5.55, ppl=46.86, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.588, loss_scale=16, train_wall=441, gb_free=10.1, wall=44919
2022-03-04 09:15:40 | INFO | train_inner | epoch 026:    295 / 393 loss=5.577, ppl=47.74, wps=14683.1, ups=0.22, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.615, loss_scale=16, train_wall=441, gb_free=10.1, wall=45365
2022-03-04 09:22:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:23:02 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.115 | ppl 138.67 | wps 34077.1 | wpb 2034.1 | bsz 4 | num_updates 10198 | best_loss 6.924
2022-03-04 09:23:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10198 updates
2022-03-04 09:23:02 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 09:23:02 | INFO | train | epoch 026 | loss 5.562 | ppl 47.26 | wps 14629.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10198 | lr 0.000313143 | gnorm 0.602 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 45807
2022-03-04 09:23:02 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 09:23:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:23:11 | INFO | train_inner | epoch 027:      2 / 393 loss=5.617, ppl=49.09, wps=14467, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.615, loss_scale=16, train_wall=440, gb_free=10.1, wall=45816
2022-03-04 09:30:37 | INFO | train_inner | epoch 027:    102 / 393 loss=5.462, ppl=44.09, wps=14684.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.582, loss_scale=16, train_wall=441, gb_free=10.1, wall=46262
2022-03-04 09:38:04 | INFO | train_inner | epoch 027:    202 / 393 loss=5.515, ppl=45.73, wps=14679.8, ups=0.22, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.618, loss_scale=16, train_wall=442, gb_free=10.1, wall=46709
2022-03-04 09:40:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:45:35 | INFO | train_inner | epoch 027:    303 / 393 loss=5.551, ppl=46.89, wps=14536.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.599, loss_scale=16, train_wall=446, gb_free=10.1, wall=47159
2022-03-04 09:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:52:21 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.11 | ppl 138.16 | wps 33970.6 | wpb 2034.1 | bsz 4 | num_updates 10590 | best_loss 6.924
2022-03-04 09:52:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10590 updates
2022-03-04 09:52:21 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 09:52:21 | INFO | train | epoch 027 | loss 5.525 | ppl 46.06 | wps 14590.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10590 | lr 0.000307293 | gnorm 0.603 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 47566
2022-03-04 09:52:21 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 09:52:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:53:06 | INFO | train_inner | epoch 028:     10 / 393 loss=5.563, ppl=47.29, wps=14470.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.613, loss_scale=16, train_wall=439, gb_free=10.1, wall=47610
2022-03-04 10:00:32 | INFO | train_inner | epoch 028:    110 / 393 loss=5.432, ppl=43.18, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.618, loss_scale=16, train_wall=441, gb_free=10.1, wall=48057
2022-03-04 10:07:58 | INFO | train_inner | epoch 028:    210 / 393 loss=5.477, ppl=44.54, wps=14680.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.612, loss_scale=16, train_wall=441, gb_free=10.1, wall=48503
2022-03-04 10:09:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:15:29 | INFO | train_inner | epoch 028:    311 / 393 loss=5.518, ppl=45.81, wps=14542.1, ups=0.22, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.612, loss_scale=8, train_wall=446, gb_free=10.1, wall=48954
2022-03-04 10:21:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:21:39 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.166 | ppl 143.58 | wps 33948.5 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 6.924
2022-03-04 10:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-04 10:21:39 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 10:21:39 | INFO | train | epoch 028 | loss 5.492 | ppl 45 | wps 14593 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.62 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 49324
2022-03-04 10:21:39 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 10:21:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:23:00 | INFO | train_inner | epoch 029:     18 / 393 loss=5.53, ppl=46.21, wps=14473.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.637, loss_scale=8, train_wall=439, gb_free=10.1, wall=49404
2022-03-04 10:30:26 | INFO | train_inner | epoch 029:    118 / 393 loss=5.41, ppl=42.52, wps=14686.2, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.614, loss_scale=8, train_wall=441, gb_free=10.1, wall=49851
2022-03-04 10:37:52 | INFO | train_inner | epoch 029:    218 / 393 loss=5.451, ppl=43.73, wps=14684.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.617, loss_scale=8, train_wall=441, gb_free=10.1, wall=50297
2022-03-04 10:45:19 | INFO | train_inner | epoch 029:    318 / 393 loss=5.494, ppl=45.06, wps=14680.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.613, loss_scale=8, train_wall=442, gb_free=10.1, wall=50743
2022-03-04 10:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:50:58 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.212 | ppl 148.23 | wps 33959.9 | wpb 2034.1 | bsz 4 | num_updates 11375 | best_loss 6.924
2022-03-04 10:50:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11375 updates
2022-03-04 10:50:58 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 10:50:58 | INFO | train | epoch 029 | loss 5.459 | ppl 44 | wps 14629.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11375 | lr 0.0002965 | gnorm 0.615 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 51082
2022-03-04 10:50:58 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 10:50:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:52:50 | INFO | train_inner | epoch 030:     25 / 393 loss=5.473, ppl=44.42, wps=14472.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.636, loss_scale=16, train_wall=440, gb_free=10.1, wall=51194
2022-03-04 11:00:16 | INFO | train_inner | epoch 030:    125 / 393 loss=5.386, ppl=41.81, wps=14682.4, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.6, loss_scale=16, train_wall=441, gb_free=10.1, wall=51641
2022-03-04 11:04:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:07:47 | INFO | train_inner | epoch 030:    226 / 393 loss=5.416, ppl=42.69, wps=14541.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.641, loss_scale=8, train_wall=446, gb_free=10.1, wall=52091
2022-03-04 11:15:13 | INFO | train_inner | epoch 030:    326 / 393 loss=5.462, ppl=44.08, wps=14685.3, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.621, loss_scale=8, train_wall=441, gb_free=10.1, wall=52537
2022-03-04 11:20:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:20:16 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.228 | ppl 149.86 | wps 34086.9 | wpb 2034.1 | bsz 4 | num_updates 11767 | best_loss 6.924
2022-03-04 11:20:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11767 updates
2022-03-04 11:20:16 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 11:20:16 | INFO | train | epoch 030 | loss 5.428 | ppl 43.06 | wps 14593.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11767 | lr 0.000291519 | gnorm 0.624 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 52841
2022-03-04 11:20:16 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 11:20:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:22:44 | INFO | train_inner | epoch 031:     33 / 393 loss=5.429, ppl=43.07, wps=14475, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.634, loss_scale=8, train_wall=439, gb_free=10.1, wall=52988
2022-03-04 11:30:10 | INFO | train_inner | epoch 031:    133 / 393 loss=5.354, ppl=40.91, wps=14683.2, ups=0.22, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.63, loss_scale=8, train_wall=441, gb_free=10.1, wall=53435
2022-03-04 11:37:36 | INFO | train_inner | epoch 031:    233 / 393 loss=5.403, ppl=42.31, wps=14684.5, ups=0.22, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.627, loss_scale=8, train_wall=441, gb_free=10.1, wall=53881
2022-03-04 11:45:02 | INFO | train_inner | epoch 031:    333 / 393 loss=5.436, ppl=43.31, wps=14688.7, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.653, loss_scale=16, train_wall=441, gb_free=10.1, wall=54327
2022-03-04 11:45:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:49:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:49:35 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.232 | ppl 150.34 | wps 34028.6 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 6.924
2022-03-04 11:49:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-04 11:49:35 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 11:49:35 | INFO | train | epoch 031 | loss 5.399 | ppl 42.2 | wps 14593.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.645 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 54599
2022-03-04 11:49:35 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 11:49:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:52:38 | INFO | train_inner | epoch 032:     41 / 393 loss=5.381, ppl=41.67, wps=14326.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.661, loss_scale=8, train_wall=444, gb_free=10.1, wall=54782
2022-03-04 12:00:04 | INFO | train_inner | epoch 032:    141 / 393 loss=5.326, ppl=40.11, wps=14686.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.599, loss_scale=8, train_wall=441, gb_free=10.1, wall=55229
2022-03-04 12:07:30 | INFO | train_inner | epoch 032:    241 / 393 loss=5.378, ppl=41.59, wps=14687.4, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.646, loss_scale=8, train_wall=441, gb_free=10.1, wall=55675
2022-03-04 12:14:56 | INFO | train_inner | epoch 032:    341 / 393 loss=5.408, ppl=42.46, wps=14687.3, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.618, loss_scale=8, train_wall=441, gb_free=10.1, wall=56121
2022-03-04 12:18:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:18:53 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.279 | ppl 155.3 | wps 33939.8 | wpb 2034.1 | bsz 4 | num_updates 12552 | best_loss 6.924
2022-03-04 12:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12552 updates
2022-03-04 12:18:53 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 12:18:53 | INFO | train | epoch 032 | loss 5.371 | ppl 41.38 | wps 14629.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12552 | lr 0.000282256 | gnorm 0.626 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 56358
2022-03-04 12:18:53 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 12:18:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:22:27 | INFO | train_inner | epoch 033:     48 / 393 loss=5.361, ppl=41.1, wps=14468.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.634, loss_scale=8, train_wall=440, gb_free=10.1, wall=56572
2022-03-04 12:29:54 | INFO | train_inner | epoch 033:    148 / 393 loss=5.309, ppl=39.65, wps=14682.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.643, loss_scale=16, train_wall=441, gb_free=10.1, wall=57018
2022-03-04 12:32:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:37:24 | INFO | train_inner | epoch 033:    249 / 393 loss=5.342, ppl=40.56, wps=14541.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.635, loss_scale=8, train_wall=446, gb_free=10.1, wall=57469
2022-03-04 12:44:51 | INFO | train_inner | epoch 033:    349 / 393 loss=5.383, ppl=41.74, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.654, loss_scale=8, train_wall=441, gb_free=10.1, wall=57915
2022-03-04 12:48:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:48:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.277 | ppl 155.13 | wps 33946.2 | wpb 2034.1 | bsz 4 | num_updates 12944 | best_loss 6.924
2022-03-04 12:48:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12944 updates
2022-03-04 12:48:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 12:48:12 | INFO | train | epoch 033 | loss 5.344 | ppl 40.62 | wps 14593.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12944 | lr 0.000277949 | gnorm 0.639 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 58116
2022-03-04 12:48:12 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 12:48:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:52:22 | INFO | train_inner | epoch 034:     56 / 393 loss=5.32, ppl=39.95, wps=14473.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.672, loss_scale=8, train_wall=439, gb_free=10.1, wall=58366
2022-03-04 12:59:48 | INFO | train_inner | epoch 034:    156 / 393 loss=5.277, ppl=38.78, wps=14685.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.628, loss_scale=8, train_wall=441, gb_free=10.1, wall=58812
2022-03-04 13:07:14 | INFO | train_inner | epoch 034:    256 / 393 loss=5.331, ppl=40.25, wps=14686, ups=0.22, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.64, loss_scale=8, train_wall=441, gb_free=10.1, wall=59259
2022-03-04 13:12:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:14:45 | INFO | train_inner | epoch 034:    357 / 393 loss=5.359, ppl=41.05, wps=14538.7, ups=0.22, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.663, loss_scale=8, train_wall=446, gb_free=10.1, wall=59709
2022-03-04 13:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:17:30 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.318 | ppl 159.52 | wps 33908.4 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 6.924
2022-03-04 13:17:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-04 13:17:30 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 13:17:30 | INFO | train | epoch 034 | loss 5.318 | ppl 39.89 | wps 14593.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.655 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 59875
2022-03-04 13:17:30 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 13:17:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:22:16 | INFO | train_inner | epoch 035:     64 / 393 loss=5.285, ppl=38.98, wps=14471.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.667, loss_scale=8, train_wall=439, gb_free=10.1, wall=60160
2022-03-04 13:29:42 | INFO | train_inner | epoch 035:    164 / 393 loss=5.261, ppl=38.36, wps=14680.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.631, loss_scale=8, train_wall=442, gb_free=10.1, wall=60607
2022-03-04 13:37:08 | INFO | train_inner | epoch 035:    264 / 393 loss=5.307, ppl=39.59, wps=14683.4, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.634, loss_scale=8, train_wall=441, gb_free=10.1, wall=61053
2022-03-04 13:44:35 | INFO | train_inner | epoch 035:    364 / 393 loss=5.338, ppl=40.46, wps=14686.9, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.668, loss_scale=8, train_wall=441, gb_free=10.1, wall=61499
2022-03-04 13:46:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:46:48 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.316 | ppl 159.37 | wps 33880.8 | wpb 2034.1 | bsz 4 | num_updates 13729 | best_loss 6.924
2022-03-04 13:46:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13729 updates
2022-03-04 13:46:48 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 13:46:48 | INFO | train | epoch 035 | loss 5.294 | ppl 39.23 | wps 14629.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13729 | lr 0.000269886 | gnorm 0.65 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 61633
2022-03-04 13:46:48 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 13:46:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:52:05 | INFO | train_inner | epoch 036:     71 / 393 loss=5.252, ppl=38.11, wps=14474.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.689, loss_scale=16, train_wall=439, gb_free=10.1, wall=61950
2022-03-04 13:59:32 | INFO | train_inner | epoch 036:    171 / 393 loss=5.246, ppl=37.96, wps=14687, ups=0.22, wpb=65530.9, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.644, loss_scale=16, train_wall=441, gb_free=10.1, wall=62396
2022-03-04 14:06:58 | INFO | train_inner | epoch 036:    271 / 393 loss=5.283, ppl=38.93, wps=14685.6, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.656, loss_scale=16, train_wall=441, gb_free=10.1, wall=62842
2022-03-04 14:14:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:14:28 | INFO | train_inner | epoch 036:    372 / 393 loss=5.313, ppl=39.76, wps=14544.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.641, loss_scale=8, train_wall=446, gb_free=10.1, wall=63293
2022-03-04 14:16:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:16:07 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.335 | ppl 161.5 | wps 33970.7 | wpb 2034.1 | bsz 4 | num_updates 14121 | best_loss 6.924
2022-03-04 14:16:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14121 updates
2022-03-04 14:16:07 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 14:16:07 | INFO | train | epoch 036 | loss 5.27 | ppl 38.59 | wps 14595.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14121 | lr 0.000266114 | gnorm 0.659 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 63391
2022-03-04 14:16:07 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 14:16:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:21:59 | INFO | train_inner | epoch 037:     79 / 393 loss=5.206, ppl=36.92, wps=14477.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.658, loss_scale=8, train_wall=439, gb_free=10.1, wall=63744
2022-03-04 14:29:25 | INFO | train_inner | epoch 037:    179 / 393 loss=5.224, ppl=37.39, wps=14689.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.676, loss_scale=8, train_wall=441, gb_free=10.1, wall=64190
2022-03-04 14:36:51 | INFO | train_inner | epoch 037:    279 / 393 loss=5.268, ppl=38.52, wps=14686.4, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.652, loss_scale=8, train_wall=441, gb_free=10.1, wall=64636
2022-03-04 14:44:18 | INFO | train_inner | epoch 037:    379 / 393 loss=5.305, ppl=39.54, wps=14690.7, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.65, loss_scale=8, train_wall=441, gb_free=10.1, wall=65082
2022-03-04 14:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:45:25 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.356 | ppl 163.78 | wps 33882 | wpb 2034.1 | bsz 4 | num_updates 14514 | best_loss 6.924
2022-03-04 14:45:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14514 updates
2022-03-04 14:45:25 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 14:45:25 | INFO | train | epoch 037 | loss 5.248 | ppl 38.01 | wps 14634.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14514 | lr 0.000262486 | gnorm 0.66 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 65149
2022-03-04 14:45:25 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 14:45:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:51:49 | INFO | train_inner | epoch 038:     86 / 393 loss=5.184, ppl=36.35, wps=14467.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.658, loss_scale=8, train_wall=440, gb_free=10.1, wall=65533
2022-03-04 14:56:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:59:19 | INFO | train_inner | epoch 038:    187 / 393 loss=5.203, ppl=36.84, wps=14534.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.653, loss_scale=8, train_wall=446, gb_free=10.1, wall=65984
2022-03-04 15:06:46 | INFO | train_inner | epoch 038:    287 / 393 loss=5.246, ppl=37.95, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.657, loss_scale=8, train_wall=441, gb_free=10.1, wall=66430
2022-03-04 15:14:12 | INFO | train_inner | epoch 038:    387 / 393 loss=5.28, ppl=38.87, wps=14686.7, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.687, loss_scale=8, train_wall=441, gb_free=10.1, wall=66877
2022-03-04 15:14:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:14:43 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.392 | ppl 167.94 | wps 33903.9 | wpb 2034.1 | bsz 4 | num_updates 14906 | best_loss 6.924
2022-03-04 15:14:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14906 updates
2022-03-04 15:14:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 15:14:43 | INFO | train | epoch 038 | loss 5.227 | ppl 37.44 | wps 14591.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14906 | lr 0.000259012 | gnorm 0.661 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 66908
2022-03-04 15:14:43 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 15:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:21:43 | INFO | train_inner | epoch 039:     94 / 393 loss=5.148, ppl=35.46, wps=14474.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.648, loss_scale=8, train_wall=439, gb_free=10.1, wall=67327
2022-03-04 15:29:09 | INFO | train_inner | epoch 039:    194 / 393 loss=5.192, ppl=36.54, wps=14683.4, ups=0.22, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.667, loss_scale=8, train_wall=441, gb_free=10.1, wall=67774
2022-03-04 15:36:35 | INFO | train_inner | epoch 039:    294 / 393 loss=5.232, ppl=37.59, wps=14684.4, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.659, loss_scale=16, train_wall=441, gb_free=10.1, wall=68220
2022-03-04 15:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:44:02 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.387 | ppl 167.34 | wps 34004.6 | wpb 2034.1 | bsz 4 | num_updates 15299 | best_loss 6.924
2022-03-04 15:44:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15299 updates
2022-03-04 15:44:02 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 15:44:02 | INFO | train | epoch 039 | loss 5.206 | ppl 36.92 | wps 14629.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15299 | lr 0.000255663 | gnorm 0.66 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 68666
2022-03-04 15:44:02 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 15:44:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:44:06 | INFO | train_inner | epoch 040:      1 / 393 loss=5.258, ppl=38.26, wps=14470.5, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.667, loss_scale=16, train_wall=440, gb_free=10.1, wall=68671
2022-03-04 15:44:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:51:37 | INFO | train_inner | epoch 040:    102 / 393 loss=5.124, ppl=34.87, wps=14542.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.673, loss_scale=8, train_wall=446, gb_free=10.1, wall=69121
2022-03-04 15:59:03 | INFO | train_inner | epoch 040:    202 / 393 loss=5.167, ppl=35.93, wps=14685.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.658, loss_scale=8, train_wall=441, gb_free=10.1, wall=69568
2022-03-04 16:06:29 | INFO | train_inner | epoch 040:    302 / 393 loss=5.211, ppl=37.05, wps=14682.1, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.681, loss_scale=8, train_wall=441, gb_free=10.1, wall=70014
2022-03-04 16:13:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:13:20 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.435 | ppl 173.04 | wps 33936 | wpb 2034.1 | bsz 4 | num_updates 15691 | best_loss 6.924
2022-03-04 16:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15691 updates
2022-03-04 16:13:20 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:13:20 | INFO | train | epoch 040 | loss 5.186 | ppl 36.39 | wps 14593.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15691 | lr 0.00025245 | gnorm 0.67 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 70425
2022-03-04 16:13:20 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:13:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:14:00 | INFO | train_inner | epoch 041:      9 / 393 loss=5.23, ppl=37.53, wps=14472.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.662, loss_scale=8, train_wall=439, gb_free=10.1, wall=70465
2022-03-04 16:21:26 | INFO | train_inner | epoch 041:    109 / 393 loss=5.103, ppl=34.37, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.682, loss_scale=8, train_wall=441, gb_free=10.1, wall=70911
2022-03-04 16:26:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:28:57 | INFO | train_inner | epoch 041:    210 / 393 loss=5.16, ppl=35.74, wps=14535.8, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.669, loss_scale=8, train_wall=446, gb_free=10.1, wall=71362
2022-03-04 16:36:24 | INFO | train_inner | epoch 041:    310 / 393 loss=5.194, ppl=36.61, wps=14686.1, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.701, loss_scale=8, train_wall=441, gb_free=10.1, wall=71808
2022-03-04 16:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:42:38 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.44 | ppl 173.64 | wps 33908.3 | wpb 2034.1 | bsz 4 | num_updates 16083 | best_loss 6.924
2022-03-04 16:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16083 updates
2022-03-04 16:42:38 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:42:38 | INFO | train | epoch 041 | loss 5.167 | ppl 35.92 | wps 14593.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16083 | lr 0.000249354 | gnorm 0.675 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 72183
2022-03-04 16:42:38 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:43:54 | INFO | train_inner | epoch 042:     17 / 393 loss=5.205, ppl=36.89, wps=14474.1, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.666, loss_scale=8, train_wall=439, gb_free=10.1, wall=72259
2022-03-04 16:51:21 | INFO | train_inner | epoch 042:    117 / 393 loss=5.1, ppl=34.29, wps=14686.2, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.669, loss_scale=8, train_wall=441, gb_free=10.1, wall=72705
2022-03-04 16:58:47 | INFO | train_inner | epoch 042:    217 / 393 loss=5.143, ppl=35.33, wps=14691.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.667, loss_scale=8, train_wall=441, gb_free=10.1, wall=73151
2022-03-04 17:06:13 | INFO | train_inner | epoch 042:    317 / 393 loss=5.176, ppl=36.15, wps=14688.9, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.678, loss_scale=16, train_wall=441, gb_free=10.1, wall=73597
2022-03-04 17:11:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:11:56 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.439 | ppl 173.49 | wps 33888.9 | wpb 2034.1 | bsz 4 | num_updates 16476 | best_loss 6.924
2022-03-04 17:11:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16476 updates
2022-03-04 17:11:56 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 17:11:56 | INFO | train | epoch 042 | loss 5.149 | ppl 35.47 | wps 14633.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16476 | lr 0.000246362 | gnorm 0.674 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 73941
2022-03-04 17:11:57 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 17:11:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:12:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:13:48 | INFO | train_inner | epoch 043:     25 / 393 loss=5.16, ppl=35.75, wps=14330.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.687, loss_scale=8, train_wall=444, gb_free=10.1, wall=74053
2022-03-04 17:21:14 | INFO | train_inner | epoch 043:    125 / 393 loss=5.084, ppl=33.91, wps=14681.7, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.669, loss_scale=8, train_wall=441, gb_free=10.1, wall=74499
2022-03-04 17:28:41 | INFO | train_inner | epoch 043:    225 / 393 loss=5.126, ppl=34.92, wps=14686.5, ups=0.22, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.692, loss_scale=8, train_wall=441, gb_free=10.1, wall=74945
2022-03-04 17:36:07 | INFO | train_inner | epoch 043:    325 / 393 loss=5.167, ppl=35.94, wps=14683.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.664, loss_scale=8, train_wall=441, gb_free=10.1, wall=75392
2022-03-04 17:41:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:41:15 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.504 | ppl 181.52 | wps 33973.8 | wpb 2034.1 | bsz 4 | num_updates 16868 | best_loss 6.924
2022-03-04 17:41:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16868 updates
2022-03-04 17:41:15 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 17:41:15 | INFO | train | epoch 043 | loss 5.13 | ppl 35.03 | wps 14593 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16868 | lr 0.000243483 | gnorm 0.686 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 75700
2022-03-04 17:41:15 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 17:41:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:43:38 | INFO | train_inner | epoch 044:     32 / 393 loss=5.133, ppl=35.1, wps=14471.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.706, loss_scale=8, train_wall=440, gb_free=10.1, wall=75842
2022-03-04 17:51:04 | INFO | train_inner | epoch 044:    132 / 393 loss=5.062, ppl=33.41, wps=14692.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.684, loss_scale=16, train_wall=441, gb_free=10.1, wall=76289
2022-03-04 17:58:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:58:35 | INFO | train_inner | epoch 044:    233 / 393 loss=5.113, ppl=34.6, wps=14543.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.683, loss_scale=8, train_wall=446, gb_free=10.1, wall=76739
2022-03-04 18:06:01 | INFO | train_inner | epoch 044:    333 / 393 loss=5.148, ppl=35.47, wps=14681.9, ups=0.22, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.737, loss_scale=8, train_wall=442, gb_free=10.1, wall=77185
2022-03-04 18:10:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:10:33 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.497 | ppl 180.61 | wps 33937.7 | wpb 2034.1 | bsz 4 | num_updates 17260 | best_loss 6.924
2022-03-04 18:10:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17260 updates
2022-03-04 18:10:33 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 18:10:33 | INFO | train | epoch 044 | loss 5.113 | ppl 34.61 | wps 14595 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17260 | lr 0.000240702 | gnorm 0.705 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 77458
2022-03-04 18:10:33 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 18:10:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:32 | INFO | train_inner | epoch 045:     40 / 393 loss=5.119, ppl=34.76, wps=14475.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.697, loss_scale=8, train_wall=439, gb_free=10.1, wall=77636
2022-03-04 18:20:58 | INFO | train_inner | epoch 045:    140 / 393 loss=5.059, ppl=33.33, wps=14691.2, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.669, loss_scale=8, train_wall=441, gb_free=10.1, wall=78082
2022-03-04 18:28:24 | INFO | train_inner | epoch 045:    240 / 393 loss=5.096, ppl=34.2, wps=14684.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.696, loss_scale=8, train_wall=441, gb_free=10.1, wall=78529
2022-03-04 18:35:50 | INFO | train_inner | epoch 045:    340 / 393 loss=5.135, ppl=35.14, wps=14683.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.695, loss_scale=8, train_wall=441, gb_free=10.1, wall=78975
2022-03-04 18:39:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:39:51 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.531 | ppl 184.89 | wps 33902.1 | wpb 2034.1 | bsz 4 | num_updates 17653 | best_loss 6.924
2022-03-04 18:39:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17653 updates
2022-03-04 18:39:51 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 18:39:51 | INFO | train | epoch 045 | loss 5.096 | ppl 34.21 | wps 14632.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17653 | lr 0.000238008 | gnorm 0.685 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 79216
2022-03-04 18:39:51 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 18:39:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:43:21 | INFO | train_inner | epoch 046:     47 / 393 loss=5.089, ppl=34.04, wps=14469.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.69, loss_scale=16, train_wall=440, gb_free=10.1, wall=79426
2022-03-04 18:48:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:50:52 | INFO | train_inner | epoch 046:    148 / 393 loss=5.039, ppl=32.87, wps=14542.1, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.696, loss_scale=8, train_wall=446, gb_free=10.1, wall=79877
2022-03-04 18:58:18 | INFO | train_inner | epoch 046:    248 / 393 loss=5.08, ppl=33.83, wps=14684.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.684, loss_scale=8, train_wall=441, gb_free=10.1, wall=80323
2022-03-04 19:05:44 | INFO | train_inner | epoch 046:    348 / 393 loss=5.125, ppl=34.89, wps=14686, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.696, loss_scale=8, train_wall=441, gb_free=10.1, wall=80769
2022-03-04 19:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:10 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.544 | ppl 186.68 | wps 33878.7 | wpb 2034.1 | bsz 4 | num_updates 18045 | best_loss 6.924
2022-03-04 19:09:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18045 updates
2022-03-04 19:09:10 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 19:09:10 | INFO | train | epoch 046 | loss 5.08 | ppl 33.83 | wps 14593.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18045 | lr 0.000235408 | gnorm 0.692 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 80974
2022-03-04 19:09:10 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 19:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:15 | INFO | train_inner | epoch 047:     55 / 393 loss=5.059, ppl=33.34, wps=14479.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.676, loss_scale=8, train_wall=439, gb_free=10.1, wall=81220
2022-03-04 19:20:41 | INFO | train_inner | epoch 047:    155 / 393 loss=5.026, ppl=32.57, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.685, loss_scale=8, train_wall=441, gb_free=10.1, wall=81666
2022-03-04 19:28:08 | INFO | train_inner | epoch 047:    255 / 393 loss=5.071, ppl=33.62, wps=14685.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.695, loss_scale=16, train_wall=441, gb_free=10.1, wall=82112
2022-03-04 19:30:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:35:38 | INFO | train_inner | epoch 047:    356 / 393 loss=5.113, ppl=34.6, wps=14545.9, ups=0.22, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.711, loss_scale=8, train_wall=446, gb_free=10.1, wall=82563
2022-03-04 19:38:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:38:28 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.523 | ppl 183.94 | wps 33892 | wpb 2034.1 | bsz 4 | num_updates 18437 | best_loss 6.924
2022-03-04 19:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18437 updates
2022-03-04 19:38:28 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 19:38:28 | INFO | train | epoch 047 | loss 5.065 | ppl 33.47 | wps 14595.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18437 | lr 0.000232892 | gnorm 0.693 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 82732
2022-03-04 19:38:28 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 19:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:43:09 | INFO | train_inner | epoch 048:     63 / 393 loss=5.037, ppl=32.82, wps=14476.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.719, loss_scale=8, train_wall=439, gb_free=10.1, wall=83013
2022-03-04 19:50:35 | INFO | train_inner | epoch 048:    163 / 393 loss=5.022, ppl=32.48, wps=14686.3, ups=0.22, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.692, loss_scale=8, train_wall=441, gb_free=10.1, wall=83460
2022-03-04 19:58:01 | INFO | train_inner | epoch 048:    263 / 393 loss=5.057, ppl=33.3, wps=14682.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.706, loss_scale=8, train_wall=441, gb_free=10.1, wall=83906
2022-03-04 20:05:28 | INFO | train_inner | epoch 048:    363 / 393 loss=5.096, ppl=34.19, wps=14686.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.698, loss_scale=8, train_wall=441, gb_free=10.1, wall=84352
2022-03-04 20:07:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:07:46 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.543 | ppl 186.48 | wps 33952 | wpb 2034.1 | bsz 4 | num_updates 18830 | best_loss 6.924
2022-03-04 20:07:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18830 updates
2022-03-04 20:07:46 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 20:07:46 | INFO | train | epoch 048 | loss 5.049 | ppl 33.11 | wps 14631.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18830 | lr 0.000230449 | gnorm 0.704 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 84491
2022-03-04 20:07:46 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 20:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:08:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:13:03 | INFO | train_inner | epoch 049:     71 / 393 loss=5.008, ppl=32.19, wps=14330.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.691, loss_scale=8, train_wall=444, gb_free=10.1, wall=84808
2022-03-04 20:20:29 | INFO | train_inner | epoch 049:    171 / 393 loss=5.008, ppl=32.18, wps=14684.9, ups=0.22, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.739, loss_scale=8, train_wall=441, gb_free=10.1, wall=85254
2022-03-04 20:27:56 | INFO | train_inner | epoch 049:    271 / 393 loss=5.046, ppl=33.04, wps=14683.1, ups=0.22, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.705, loss_scale=8, train_wall=441, gb_free=10.1, wall=85700
2022-03-04 20:35:22 | INFO | train_inner | epoch 049:    371 / 393 loss=5.085, ppl=33.94, wps=14685.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.721, loss_scale=8, train_wall=441, gb_free=10.1, wall=86146
2022-03-04 20:36:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:37:05 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.567 | ppl 189.58 | wps 33867.3 | wpb 2034.1 | bsz 4 | num_updates 19222 | best_loss 6.924
2022-03-04 20:37:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19222 updates
2022-03-04 20:37:05 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 20:37:05 | INFO | train | epoch 049 | loss 5.035 | ppl 32.79 | wps 14592.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19222 | lr 0.000228087 | gnorm 0.715 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 86249
2022-03-04 20:37:05 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 20:37:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:42:53 | INFO | train_inner | epoch 050:     78 / 393 loss=4.987, ppl=31.7, wps=14474.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.706, loss_scale=8, train_wall=439, gb_free=10.1, wall=86597
2022-03-04 20:50:19 | INFO | train_inner | epoch 050:    178 / 393 loss=4.986, ppl=31.69, wps=14681.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.688, loss_scale=16, train_wall=441, gb_free=10.1, wall=87044
2022-03-04 20:50:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:57:50 | INFO | train_inner | epoch 050:    279 / 393 loss=5.041, ppl=32.92, wps=14543.1, ups=0.22, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.71, loss_scale=8, train_wall=446, gb_free=10.1, wall=87494
2022-03-04 21:05:16 | INFO | train_inner | epoch 050:    379 / 393 loss=5.078, ppl=33.77, wps=14688.6, ups=0.22, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.736, loss_scale=8, train_wall=441, gb_free=10.1, wall=87940
2022-03-04 21:06:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:06:23 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.584 | ppl 191.91 | wps 33947.7 | wpb 2034.1 | bsz 4 | num_updates 19614 | best_loss 6.924
2022-03-04 21:06:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19614 updates
2022-03-04 21:06:23 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 21:06:23 | INFO | train | epoch 050 | loss 5.021 | ppl 32.46 | wps 14595 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19614 | lr 0.000225796 | gnorm 0.709 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 88007
2022-03-04 21:06:23 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 21:06:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:12:47 | INFO | train_inner | epoch 051:     86 / 393 loss=4.965, ppl=31.22, wps=14473.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.711, loss_scale=8, train_wall=439, gb_free=10.1, wall=88391
2022-03-04 21:20:13 | INFO | train_inner | epoch 051:    186 / 393 loss=4.984, ppl=31.65, wps=14684.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.699, loss_scale=8, train_wall=441, gb_free=10.1, wall=88837
2022-03-04 21:27:39 | INFO | train_inner | epoch 051:    286 / 393 loss=5.033, ppl=32.75, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.705, loss_scale=8, train_wall=441, gb_free=10.1, wall=89284
2022-03-04 21:28:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:35:10 | INFO | train_inner | epoch 051:    387 / 393 loss=5.055, ppl=33.24, wps=14544.6, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.725, loss_scale=8, train_wall=446, gb_free=10.1, wall=89734
2022-03-04 21:35:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:35:41 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.605 | ppl 194.72 | wps 33903.4 | wpb 2034.1 | bsz 4 | num_updates 20006 | best_loss 6.924
2022-03-04 21:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20006 updates
2022-03-04 21:35:41 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 21:35:41 | INFO | train | epoch 051 | loss 5.007 | ppl 32.16 | wps 14594.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20006 | lr 0.000223573 | gnorm 0.711 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 89766
2022-03-04 21:35:41 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 21:35:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:42:40 | INFO | train_inner | epoch 052:     94 / 393 loss=4.939, ppl=30.67, wps=14474.5, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.713, loss_scale=8, train_wall=439, gb_free=10.1, wall=90185
2022-03-04 21:50:07 | INFO | train_inner | epoch 052:    194 / 393 loss=4.979, ppl=31.54, wps=14684.7, ups=0.22, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.725, loss_scale=8, train_wall=441, gb_free=10.1, wall=90631
2022-03-04 21:57:33 | INFO | train_inner | epoch 052:    294 / 393 loss=5.01, ppl=32.23, wps=14686.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.715, loss_scale=8, train_wall=441, gb_free=10.1, wall=91078
2022-03-04 22:04:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:59 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.624 | ppl 197.24 | wps 33952.2 | wpb 2034.1 | bsz 4 | num_updates 20399 | best_loss 6.924
2022-03-04 22:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20399 updates
2022-03-04 22:04:59 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 22:04:59 | INFO | train | epoch 052 | loss 4.993 | ppl 31.85 | wps 14631.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20399 | lr 0.000221409 | gnorm 0.718 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 91524
2022-03-04 22:04:59 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 22:04:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:05:04 | INFO | train_inner | epoch 053:      1 / 393 loss=5.049, ppl=33.1, wps=14472.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.72, loss_scale=8, train_wall=439, gb_free=10.1, wall=91528
2022-03-04 22:12:30 | INFO | train_inner | epoch 053:    101 / 393 loss=4.918, ppl=30.23, wps=14684, ups=0.22, wpb=65530.2, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.709, loss_scale=16, train_wall=441, gb_free=10.1, wall=91975
2022-03-04 22:12:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:20:01 | INFO | train_inner | epoch 053:    202 / 393 loss=4.967, ppl=31.28, wps=14539, ups=0.22, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.729, loss_scale=8, train_wall=446, gb_free=10.1, wall=92425
2022-03-04 22:27:27 | INFO | train_inner | epoch 053:    302 / 393 loss=5.005, ppl=32.12, wps=14691, ups=0.22, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.716, loss_scale=8, train_wall=441, gb_free=10.1, wall=92872
2022-03-04 22:34:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:34:18 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.626 | ppl 197.5 | wps 33968.3 | wpb 2034.1 | bsz 4 | num_updates 20791 | best_loss 6.924
2022-03-04 22:34:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20791 updates
2022-03-04 22:34:18 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 22:34:18 | INFO | train | epoch 053 | loss 4.981 | ppl 31.57 | wps 14594.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20791 | lr 0.000219312 | gnorm 0.722 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 93282
2022-03-04 22:34:18 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 22:34:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:34:58 | INFO | train_inner | epoch 054:      9 / 393 loss=5.025, ppl=32.55, wps=14475.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.73, loss_scale=8, train_wall=439, gb_free=10.1, wall=93322
2022-03-04 22:42:24 | INFO | train_inner | epoch 054:    109 / 393 loss=4.909, ppl=30.05, wps=14689.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.716, loss_scale=8, train_wall=441, gb_free=10.1, wall=93768
2022-03-04 22:49:50 | INFO | train_inner | epoch 054:    209 / 393 loss=4.956, ppl=31.04, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.719, loss_scale=8, train_wall=441, gb_free=10.1, wall=94215
2022-03-04 22:54:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:57:21 | INFO | train_inner | epoch 054:    310 / 393 loss=4.989, ppl=31.77, wps=14543.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.717, loss_scale=8, train_wall=446, gb_free=10.1, wall=94665
2022-03-04 23:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:03:36 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.623 | ppl 197.12 | wps 33828.7 | wpb 2034.1 | bsz 4 | num_updates 21183 | best_loss 6.924
2022-03-04 23:03:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21183 updates
2022-03-04 23:03:36 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 23:03:36 | INFO | train | epoch 054 | loss 4.968 | ppl 31.3 | wps 14596 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21183 | lr 0.000217273 | gnorm 0.716 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 95040
2022-03-04 23:03:36 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 23:03:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:04:52 | INFO | train_inner | epoch 055:     17 / 393 loss=5.008, ppl=32.18, wps=14473.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.718, loss_scale=8, train_wall=439, gb_free=10.1, wall=95116
2022-03-04 23:12:18 | INFO | train_inner | epoch 055:    117 / 393 loss=4.907, ppl=30.01, wps=14687.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.716, loss_scale=8, train_wall=441, gb_free=10.1, wall=95562
2022-03-04 23:19:44 | INFO | train_inner | epoch 055:    217 / 393 loss=4.95, ppl=30.91, wps=14685.6, ups=0.22, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.737, loss_scale=8, train_wall=441, gb_free=10.1, wall=96009
2022-03-04 23:27:10 | INFO | train_inner | epoch 055:    317 / 393 loss=4.98, ppl=31.56, wps=14684.1, ups=0.22, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.746, loss_scale=8, train_wall=441, gb_free=10.1, wall=96455
2022-03-04 23:32:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:32:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:32:54 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.646 | ppl 200.33 | wps 33935.9 | wpb 2034.1 | bsz 4 | num_updates 21575 | best_loss 6.924
2022-03-04 23:32:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21575 updates
2022-03-04 23:32:54 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 23:32:54 | INFO | train | epoch 055 | loss 4.956 | ppl 31.04 | wps 14594.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21575 | lr 0.00021529 | gnorm 0.739 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 96798
2022-03-04 23:32:54 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 23:32:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:34:45 | INFO | train_inner | epoch 056:     25 / 393 loss=4.982, ppl=31.6, wps=14332.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.744, loss_scale=8, train_wall=444, gb_free=10.1, wall=96910
2022-03-04 23:42:12 | INFO | train_inner | epoch 056:    125 / 393 loss=4.896, ppl=29.77, wps=14686.1, ups=0.22, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.694, loss_scale=8, train_wall=441, gb_free=10.1, wall=97356
2022-03-04 23:49:38 | INFO | train_inner | epoch 056:    225 / 393 loss=4.939, ppl=30.67, wps=14688.6, ups=0.22, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.718, loss_scale=8, train_wall=441, gb_free=10.1, wall=97803
2022-03-04 23:57:04 | INFO | train_inner | epoch 056:    325 / 393 loss=4.972, ppl=31.39, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.727, loss_scale=8, train_wall=441, gb_free=10.1, wall=98249
2022-03-05 00:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:02:12 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.664 | ppl 202.88 | wps 33914.2 | wpb 2034.1 | bsz 4 | num_updates 21968 | best_loss 6.924
2022-03-05 00:02:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21968 updates
2022-03-05 00:02:12 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-05 00:02:12 | INFO | train | epoch 056 | loss 4.944 | ppl 30.78 | wps 14632.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 21968 | lr 0.000213356 | gnorm 0.718 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 98557
2022-03-05 00:02:12 | INFO | fairseq.trainer | begin training epoch 57
2022-03-05 00:02:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:04:35 | INFO | train_inner | epoch 057:     32 / 393 loss=4.956, ppl=31.03, wps=14474.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.744, loss_scale=8, train_wall=439, gb_free=10.1, wall=98699
2022-03-05 00:10:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:12:05 | INFO | train_inner | epoch 057:    133 / 393 loss=4.893, ppl=29.71, wps=14542.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.736, loss_scale=8, train_wall=446, gb_free=10.1, wall=99150
2022-03-05 00:19:32 | INFO | train_inner | epoch 057:    233 / 393 loss=4.929, ppl=30.46, wps=14680.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.723, loss_scale=8, train_wall=441, gb_free=10.1, wall=99597
2022-03-05 00:26:58 | INFO | train_inner | epoch 057:    333 / 393 loss=4.96, ppl=31.12, wps=14685.5, ups=0.22, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.759, loss_scale=8, train_wall=441, gb_free=10.1, wall=100043
2022-03-05 00:31:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:31:30 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.687 | ppl 206.07 | wps 33836.5 | wpb 2034.1 | bsz 4 | num_updates 22360 | best_loss 6.924
2022-03-05 00:31:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22360 updates
2022-03-05 00:31:30 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-05 00:31:30 | INFO | train | epoch 057 | loss 4.932 | ppl 30.52 | wps 14594 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 22360 | lr 0.000211477 | gnorm 0.733 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 100315
2022-03-05 00:31:30 | INFO | fairseq.trainer | begin training epoch 58
2022-03-05 00:31:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:34:29 | INFO | train_inner | epoch 058:     40 / 393 loss=4.934, ppl=30.56, wps=14475.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.707, loss_scale=8, train_wall=439, gb_free=10.1, wall=100494
2022-03-05 00:41:55 | INFO | train_inner | epoch 058:    140 / 393 loss=4.88, ppl=29.45, wps=14688.6, ups=0.22, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.749, loss_scale=8, train_wall=441, gb_free=10.1, wall=100940
2022-03-05 00:49:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:49:26 | INFO | train_inner | epoch 058:    241 / 393 loss=4.922, ppl=30.31, wps=14542.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.746, loss_scale=8, train_wall=446, gb_free=10.1, wall=101390
2022-03-05 00:56:52 | INFO | train_inner | epoch 058:    341 / 393 loss=4.964, ppl=31.22, wps=14688.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.719, loss_scale=8, train_wall=441, gb_free=10.1, wall=101836
2022-03-05 01:00:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:00:48 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.657 | ppl 201.86 | wps 33953.4 | wpb 2034.1 | bsz 4 | num_updates 22752 | best_loss 6.924
2022-03-05 01:00:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22752 updates
2022-03-05 01:00:48 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-05 01:00:48 | INFO | train | epoch 058 | loss 4.921 | ppl 30.3 | wps 14596.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 22752 | lr 0.000209648 | gnorm 0.736 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 102073
2022-03-05 01:00:48 | INFO | fairseq.trainer | begin training epoch 59
2022-03-05 01:00:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:04:23 | INFO | train_inner | epoch 059:     48 / 393 loss=4.91, ppl=30.06, wps=14475.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.746, loss_scale=8, train_wall=439, gb_free=10.1, wall=102287
2022-03-05 01:11:49 | INFO | train_inner | epoch 059:    148 / 393 loss=4.88, ppl=29.45, wps=14684, ups=0.22, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.726, loss_scale=8, train_wall=441, gb_free=10.1, wall=102734
2022-03-05 01:19:15 | INFO | train_inner | epoch 059:    248 / 393 loss=4.907, ppl=30, wps=14679.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.733, loss_scale=8, train_wall=442, gb_free=10.1, wall=103180
2022-03-05 01:26:42 | INFO | train_inner | epoch 059:    348 / 393 loss=4.95, ppl=30.92, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.736, loss_scale=8, train_wall=441, gb_free=10.1, wall=103626
2022-03-05 01:27:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:30:07 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.701 | ppl 208.06 | wps 33974.5 | wpb 2034.1 | bsz 4 | num_updates 23144 | best_loss 6.924
2022-03-05 01:30:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23144 updates
2022-03-05 01:30:07 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-05 01:30:07 | INFO | train | epoch 059 | loss 4.91 | ppl 30.07 | wps 14592.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23144 | lr 0.000207865 | gnorm 0.738 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 103831
2022-03-05 01:30:07 | INFO | fairseq.trainer | begin training epoch 60
2022-03-05 01:30:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:17 | INFO | train_inner | epoch 060:     56 / 393 loss=4.897, ppl=29.79, wps=14334.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.757, loss_scale=8, train_wall=444, gb_free=10.1, wall=104081
2022-03-05 01:41:43 | INFO | train_inner | epoch 060:    156 / 393 loss=4.865, ppl=29.15, wps=14688.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.745, loss_scale=8, train_wall=441, gb_free=10.1, wall=104528
2022-03-05 01:49:09 | INFO | train_inner | epoch 060:    256 / 393 loss=4.907, ppl=30, wps=14689.8, ups=0.22, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.738, loss_scale=8, train_wall=441, gb_free=10.1, wall=104974
2022-03-05 01:56:35 | INFO | train_inner | epoch 060:    356 / 393 loss=4.941, ppl=30.72, wps=14685.3, ups=0.22, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.745, loss_scale=8, train_wall=441, gb_free=10.1, wall=105420
2022-03-05 01:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:59:25 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.736 | ppl 213.15 | wps 33943.7 | wpb 2034.1 | bsz 4 | num_updates 23537 | best_loss 6.924
2022-03-05 01:59:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23537 updates
2022-03-05 01:59:25 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-05 01:59:25 | INFO | train | epoch 060 | loss 4.9 | ppl 29.85 | wps 14633.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 23537 | lr 0.000206122 | gnorm 0.746 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 105589
2022-03-05 01:59:25 | INFO | fairseq.trainer | begin training epoch 61
2022-03-05 01:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:06 | INFO | train_inner | epoch 061:     63 / 393 loss=4.875, ppl=29.34, wps=14477.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.729, loss_scale=8, train_wall=439, gb_free=10.1, wall=105871
2022-03-05 02:06:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:11:37 | INFO | train_inner | epoch 061:    164 / 393 loss=4.865, ppl=29.14, wps=14545.8, ups=0.22, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.737, loss_scale=8, train_wall=446, gb_free=10.1, wall=106321
2022-03-05 02:19:03 | INFO | train_inner | epoch 061:    264 / 393 loss=4.891, ppl=29.67, wps=14688.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.755, loss_scale=8, train_wall=441, gb_free=10.1, wall=106767
2022-03-05 02:26:29 | INFO | train_inner | epoch 061:    364 / 393 loss=4.928, ppl=30.45, wps=14692.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.751, loss_scale=8, train_wall=441, gb_free=10.1, wall=107213
2022-03-05 02:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:28:43 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.716 | ppl 210.23 | wps 33887.5 | wpb 2034.1 | bsz 4 | num_updates 23929 | best_loss 6.924
2022-03-05 02:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23929 updates
2022-03-05 02:28:43 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-05 02:28:43 | INFO | train | epoch 061 | loss 4.889 | ppl 29.63 | wps 14597.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23929 | lr 0.000204427 | gnorm 0.743 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 107347
2022-03-05 02:28:43 | INFO | fairseq.trainer | begin training epoch 62
2022-03-05 02:28:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:34:00 | INFO | train_inner | epoch 062:     71 / 393 loss=4.862, ppl=29.07, wps=14471.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.745, loss_scale=8, train_wall=440, gb_free=10.1, wall=107664
2022-03-05 02:41:26 | INFO | train_inner | epoch 062:    171 / 393 loss=4.852, ppl=28.89, wps=14690.4, ups=0.22, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.756, loss_scale=8, train_wall=441, gb_free=10.1, wall=108110
2022-03-05 02:48:52 | INFO | train_inner | epoch 062:    271 / 393 loss=4.888, ppl=29.6, wps=14679.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.73, loss_scale=16, train_wall=442, gb_free=10.1, wall=108557
2022-03-05 02:49:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:56:23 | INFO | train_inner | epoch 062:    372 / 393 loss=4.926, ppl=30.4, wps=14545.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.762, loss_scale=8, train_wall=446, gb_free=10.1, wall=109007
2022-03-05 02:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:58:01 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.721 | ppl 211.03 | wps 33901.6 | wpb 2034.1 | bsz 4 | num_updates 24321 | best_loss 6.924
2022-03-05 02:58:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24321 updates
2022-03-05 02:58:01 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-05 02:58:01 | INFO | train | epoch 062 | loss 4.879 | ppl 29.42 | wps 14595.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 24321 | lr 0.000202773 | gnorm 0.747 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 109106
2022-03-05 02:58:01 | INFO | fairseq.trainer | begin training epoch 63
2022-03-05 02:58:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:03:53 | INFO | train_inner | epoch 063:     79 / 393 loss=4.835, ppl=28.55, wps=14476.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.743, loss_scale=8, train_wall=439, gb_free=10.1, wall=109458
2022-03-05 03:11:20 | INFO | train_inner | epoch 063:    179 / 393 loss=4.842, ppl=28.68, wps=14691.2, ups=0.22, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.741, loss_scale=8, train_wall=441, gb_free=10.1, wall=109904
2022-03-05 03:18:46 | INFO | train_inner | epoch 063:    279 / 393 loss=4.887, ppl=29.59, wps=14683.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.772, loss_scale=8, train_wall=441, gb_free=10.1, wall=110350
2022-03-05 03:26:12 | INFO | train_inner | epoch 063:    379 / 393 loss=4.918, ppl=30.24, wps=14689.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.748, loss_scale=8, train_wall=441, gb_free=10.1, wall=110797
2022-03-05 03:27:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:27:19 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.766 | ppl 217.69 | wps 33983.9 | wpb 2034.1 | bsz 4 | num_updates 24714 | best_loss 6.924
2022-03-05 03:27:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24714 updates
2022-03-05 03:27:19 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-05 03:27:19 | INFO | train | epoch 063 | loss 4.869 | ppl 29.23 | wps 14634.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 24714 | lr 0.000201154 | gnorm 0.752 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 110864
2022-03-05 03:27:19 | INFO | fairseq.trainer | begin training epoch 64
2022-03-05 03:27:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:27:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:33:47 | INFO | train_inner | epoch 064:     87 / 393 loss=4.817, ppl=28.2, wps=14331.7, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.752, loss_scale=8, train_wall=444, gb_free=10.1, wall=111252
2022-03-05 03:41:13 | INFO | train_inner | epoch 064:    187 / 393 loss=4.838, ppl=28.61, wps=14685.1, ups=0.22, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.764, loss_scale=8, train_wall=441, gb_free=10.1, wall=111698
2022-03-05 03:48:40 | INFO | train_inner | epoch 064:    287 / 393 loss=4.879, ppl=29.42, wps=14681.5, ups=0.22, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.745, loss_scale=8, train_wall=441, gb_free=10.1, wall=112144
2022-03-05 03:56:06 | INFO | train_inner | epoch 064:    387 / 393 loss=4.91, ppl=30.06, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.75, loss_scale=8, train_wall=441, gb_free=10.1, wall=112591
2022-03-05 03:56:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:56:37 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.771 | ppl 218.5 | wps 33929.6 | wpb 2034.1 | bsz 4 | num_updates 25106 | best_loss 6.924
2022-03-05 03:56:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25106 updates
2022-03-05 03:56:37 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-05 03:56:37 | INFO | train | epoch 064 | loss 4.859 | ppl 29.03 | wps 14592.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 25106 | lr 0.000199577 | gnorm 0.754 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 112622
2022-03-05 03:56:37 | INFO | fairseq.trainer | begin training epoch 65
2022-03-05 03:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:03:37 | INFO | train_inner | epoch 065:     94 / 393 loss=4.796, ppl=27.78, wps=14476.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.739, loss_scale=8, train_wall=439, gb_free=10.1, wall=113041
2022-03-05 04:10:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:11:08 | INFO | train_inner | epoch 065:    195 / 393 loss=4.832, ppl=28.48, wps=14536.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.755, loss_scale=8, train_wall=446, gb_free=10.1, wall=113492
2022-03-05 04:18:34 | INFO | train_inner | epoch 065:    295 / 393 loss=4.873, ppl=29.31, wps=14684.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.746, loss_scale=8, train_wall=441, gb_free=10.1, wall=113939
2022-03-05 04:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:25:56 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.795 | ppl 222.07 | wps 33962.6 | wpb 2034.1 | bsz 4 | num_updates 25498 | best_loss 6.924
2022-03-05 04:25:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25498 updates
2022-03-05 04:25:56 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-05 04:25:56 | INFO | train | epoch 065 | loss 4.85 | ppl 28.84 | wps 14593.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 25498 | lr 0.000198037 | gnorm 0.751 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 114380
2022-03-05 04:25:56 | INFO | fairseq.trainer | begin training epoch 66
2022-03-05 04:25:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:26:05 | INFO | train_inner | epoch 066:      2 / 393 loss=4.902, ppl=29.9, wps=14475, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.767, loss_scale=8, train_wall=439, gb_free=10.1, wall=114389
2022-03-05 04:33:31 | INFO | train_inner | epoch 066:    102 / 393 loss=4.784, ppl=27.55, wps=14685.1, ups=0.22, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.774, loss_scale=8, train_wall=441, gb_free=10.1, wall=114836
2022-03-05 04:40:57 | INFO | train_inner | epoch 066:    202 / 393 loss=4.825, ppl=28.35, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.726, loss_scale=8, train_wall=441, gb_free=10.1, wall=115282
2022-03-05 04:48:23 | INFO | train_inner | epoch 066:    302 / 393 loss=4.869, ppl=29.23, wps=14689.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.768, loss_scale=8, train_wall=441, gb_free=10.1, wall=115728
2022-03-05 04:49:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:55:14 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.793 | ppl 221.81 | wps 33918.8 | wpb 2034.1 | bsz 4 | num_updates 25890 | best_loss 6.924
2022-03-05 04:55:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25890 updates
2022-03-05 04:55:14 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-05 04:55:14 | INFO | train | epoch 066 | loss 4.841 | ppl 28.67 | wps 14596.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 25890 | lr 0.000196532 | gnorm 0.761 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 116138
2022-03-05 04:55:14 | INFO | fairseq.trainer | begin training epoch 67
2022-03-05 04:55:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:55:58 | INFO | train_inner | epoch 067:     10 / 393 loss=4.881, ppl=29.47, wps=14337.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.778, loss_scale=8, train_wall=444, gb_free=10.1, wall=116183
2022-03-05 05:03:25 | INFO | train_inner | epoch 067:    110 / 393 loss=4.781, ppl=27.5, wps=14688.4, ups=0.22, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.769, loss_scale=8, train_wall=441, gb_free=10.1, wall=116629
2022-03-05 05:10:51 | INFO | train_inner | epoch 067:    210 / 393 loss=4.817, ppl=28.19, wps=14685.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.776, loss_scale=8, train_wall=441, gb_free=10.1, wall=117075
2022-03-05 05:18:17 | INFO | train_inner | epoch 067:    310 / 393 loss=4.862, ppl=29.07, wps=14685.3, ups=0.22, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.752, loss_scale=8, train_wall=441, gb_free=10.1, wall=117522
2022-03-05 05:24:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:32 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.798 | ppl 222.54 | wps 33947.6 | wpb 2034.1 | bsz 4 | num_updates 26283 | best_loss 6.924
2022-03-05 05:24:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26283 updates
2022-03-05 05:24:32 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-05 05:24:32 | INFO | train | epoch 067 | loss 4.833 | ppl 28.5 | wps 14632.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 26283 | lr 0.000195057 | gnorm 0.766 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 117896
2022-03-05 05:24:32 | INFO | fairseq.trainer | begin training epoch 68
2022-03-05 05:24:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:25:48 | INFO | train_inner | epoch 068:     17 / 393 loss=4.862, ppl=29.09, wps=14476.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.763, loss_scale=8, train_wall=439, gb_free=10.1, wall=117972
2022-03-05 05:31:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:33:19 | INFO | train_inner | epoch 068:    118 / 393 loss=4.777, ppl=27.43, wps=14538.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.76, loss_scale=8, train_wall=446, gb_free=10.1, wall=118423
2022-03-05 05:40:45 | INFO | train_inner | epoch 068:    218 / 393 loss=4.817, ppl=28.18, wps=14685, ups=0.22, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.774, loss_scale=8, train_wall=441, gb_free=10.1, wall=118869
2022-03-05 05:48:11 | INFO | train_inner | epoch 068:    318 / 393 loss=4.849, ppl=28.82, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.744, loss_scale=8, train_wall=441, gb_free=10.1, wall=119316
2022-03-05 05:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:53:50 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.825 | ppl 226.75 | wps 33960.3 | wpb 2034.1 | bsz 4 | num_updates 26675 | best_loss 6.924
2022-03-05 05:53:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26675 updates
2022-03-05 05:53:50 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-05 05:53:50 | INFO | train | epoch 068 | loss 4.824 | ppl 28.32 | wps 14592.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 26675 | lr 0.000193619 | gnorm 0.762 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 119655
2022-03-05 05:53:50 | INFO | fairseq.trainer | begin training epoch 69
2022-03-05 05:53:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:42 | INFO | train_inner | epoch 069:     25 / 393 loss=4.847, ppl=28.77, wps=14468.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.766, loss_scale=8, train_wall=440, gb_free=10.1, wall=119767
2022-03-05 06:03:08 | INFO | train_inner | epoch 069:    125 / 393 loss=4.775, ppl=27.37, wps=14687.2, ups=0.22, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.763, loss_scale=8, train_wall=441, gb_free=10.1, wall=120213
2022-03-05 06:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:10:39 | INFO | train_inner | epoch 069:    226 / 393 loss=4.813, ppl=28.1, wps=14543.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.775, loss_scale=8, train_wall=446, gb_free=10.1, wall=120663
2022-03-05 06:18:05 | INFO | train_inner | epoch 069:    326 / 393 loss=4.837, ppl=28.57, wps=14690.2, ups=0.22, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.794, loss_scale=8, train_wall=441, gb_free=10.1, wall=121110
2022-03-05 06:23:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:23:08 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.813 | ppl 224.91 | wps 33883.1 | wpb 2034.1 | bsz 4 | num_updates 27067 | best_loss 6.924
2022-03-05 06:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27067 updates
2022-03-05 06:23:08 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-05 06:23:08 | INFO | train | epoch 069 | loss 4.816 | ppl 28.16 | wps 14596.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 27067 | lr 0.000192212 | gnorm 0.774 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 121413
2022-03-05 06:23:08 | INFO | fairseq.trainer | begin training epoch 70
2022-03-05 06:23:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:25:36 | INFO | train_inner | epoch 070:     33 / 393 loss=4.832, ppl=28.48, wps=14476, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.764, loss_scale=8, train_wall=439, gb_free=10.1, wall=121560
2022-03-05 06:33:02 | INFO | train_inner | epoch 070:    133 / 393 loss=4.764, ppl=27.17, wps=14684.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.763, loss_scale=8, train_wall=441, gb_free=10.1, wall=122007
2022-03-05 06:40:28 | INFO | train_inner | epoch 070:    233 / 393 loss=4.806, ppl=27.97, wps=14685.6, ups=0.22, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.764, loss_scale=8, train_wall=441, gb_free=10.1, wall=122453
2022-03-05 06:47:54 | INFO | train_inner | epoch 070:    333 / 393 loss=4.838, ppl=28.59, wps=14688.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.754, loss_scale=16, train_wall=441, gb_free=10.1, wall=122899
2022-03-05 06:47:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:52:27 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.809 | ppl 224.32 | wps 33835.5 | wpb 2034.1 | bsz 4 | num_updates 27459 | best_loss 6.924
2022-03-05 06:52:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27459 updates
2022-03-05 06:52:27 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-05 06:52:27 | INFO | train | epoch 070 | loss 4.807 | ppl 28 | wps 14595 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 27459 | lr 0.000190835 | gnorm 0.764 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 123171
2022-03-05 06:52:27 | INFO | fairseq.trainer | begin training epoch 71
2022-03-05 06:52:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:55:30 | INFO | train_inner | epoch 071:     41 / 393 loss=4.812, ppl=28.1, wps=14334.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.794, loss_scale=8, train_wall=444, gb_free=10.1, wall=123354
2022-03-05 07:02:56 | INFO | train_inner | epoch 071:    141 / 393 loss=4.757, ppl=27.04, wps=14691.5, ups=0.22, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.766, loss_scale=8, train_wall=441, gb_free=10.1, wall=123800
2022-03-05 07:10:22 | INFO | train_inner | epoch 071:    241 / 393 loss=4.809, ppl=28.04, wps=14684.7, ups=0.22, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.782, loss_scale=8, train_wall=441, gb_free=10.1, wall=124247
2022-03-05 07:17:48 | INFO | train_inner | epoch 071:    341 / 393 loss=4.826, ppl=28.36, wps=14683.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.763, loss_scale=8, train_wall=441, gb_free=10.1, wall=124693
2022-03-05 07:21:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:21:45 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.849 | ppl 230.5 | wps 33923.6 | wpb 2034.1 | bsz 4 | num_updates 27852 | best_loss 6.924
2022-03-05 07:21:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27852 updates
2022-03-05 07:21:45 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-05 07:21:45 | INFO | train | epoch 071 | loss 4.8 | ppl 27.85 | wps 14631.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 27852 | lr 0.000189484 | gnorm 0.779 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 124930
2022-03-05 07:21:45 | INFO | fairseq.trainer | begin training epoch 72
2022-03-05 07:21:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:25:19 | INFO | train_inner | epoch 072:     48 / 393 loss=4.795, ppl=27.77, wps=14471.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.784, loss_scale=8, train_wall=440, gb_free=10.1, wall=125144
2022-03-05 07:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:32:50 | INFO | train_inner | epoch 072:    149 / 393 loss=4.76, ppl=27.1, wps=14543, ups=0.22, wpb=65530.2, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.796, loss_scale=8, train_wall=446, gb_free=10.1, wall=125594
2022-03-05 07:40:16 | INFO | train_inner | epoch 072:    249 / 393 loss=4.799, ppl=27.83, wps=14687.8, ups=0.22, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.769, loss_scale=8, train_wall=441, gb_free=10.1, wall=126041
2022-03-05 07:47:42 | INFO | train_inner | epoch 072:    349 / 393 loss=4.825, ppl=28.34, wps=14687.4, ups=0.22, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.793, loss_scale=8, train_wall=441, gb_free=10.1, wall=126487
2022-03-05 07:50:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:51:03 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.836 | ppl 228.44 | wps 33908.8 | wpb 2034.1 | bsz 4 | num_updates 28244 | best_loss 6.924
2022-03-05 07:51:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28244 updates
2022-03-05 07:51:03 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-05 07:51:03 | INFO | train | epoch 072 | loss 4.792 | ppl 27.7 | wps 14596 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 28244 | lr 0.000188164 | gnorm 0.784 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 126688
2022-03-05 07:51:03 | INFO | fairseq.trainer | begin training epoch 73
2022-03-05 07:51:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:55:13 | INFO | train_inner | epoch 073:     56 / 393 loss=4.773, ppl=27.35, wps=14474.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.767, loss_scale=8, train_wall=439, gb_free=10.1, wall=126938
2022-03-05 08:02:39 | INFO | train_inner | epoch 073:    156 / 393 loss=4.759, ppl=27.08, wps=14685.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.777, loss_scale=8, train_wall=441, gb_free=10.1, wall=127384
2022-03-05 08:05:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:10:10 | INFO | train_inner | epoch 073:    257 / 393 loss=4.783, ppl=27.53, wps=14545.8, ups=0.22, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.787, loss_scale=8, train_wall=446, gb_free=10.1, wall=127834
2022-03-05 08:17:36 | INFO | train_inner | epoch 073:    357 / 393 loss=4.83, ppl=28.45, wps=14689.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.786, loss_scale=8, train_wall=441, gb_free=10.1, wall=128280
2022-03-05 08:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:20:21 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.854 | ppl 231.34 | wps 33896.6 | wpb 2034.1 | bsz 4 | num_updates 28636 | best_loss 6.924
2022-03-05 08:20:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28636 updates
2022-03-05 08:20:21 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-05 08:20:21 | INFO | train | epoch 073 | loss 4.784 | ppl 27.55 | wps 14597.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 28636 | lr 0.000186872 | gnorm 0.779 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 128446
2022-03-05 08:20:21 | INFO | fairseq.trainer | begin training epoch 74
2022-03-05 08:20:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:25:07 | INFO | train_inner | epoch 074:     64 / 393 loss=4.756, ppl=27.02, wps=14471.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.785, loss_scale=8, train_wall=440, gb_free=10.1, wall=128731
2022-03-05 08:32:33 | INFO | train_inner | epoch 074:    164 / 393 loss=4.743, ppl=26.78, wps=14686.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.777, loss_scale=8, train_wall=441, gb_free=10.1, wall=129178
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
