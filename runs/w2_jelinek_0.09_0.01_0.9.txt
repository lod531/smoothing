Sender: LSF System <lsfadmin@eu-g3-009>
Subject: Job 202286019: <w2_jelinek_0.09_0.01_0.9> in cluster <euler> Exited

Job <w2_jelinek_0.09_0.01_0.9> was submitted from host <eu-login-22> by user <andriusb> in cluster <euler> at Fri Jan 28 06:49:17 2022
Job was executed on host(s) <eu-g3-009>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Jan 28 06:49:49 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Jan 28 06:49:49 2022
Terminated at Sat Jan 29 02:49:55 2022
Results reported at Sat Jan 29 02:49:55 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-2-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.09, 0.01, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 16 --save-interval 40 --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   71915.00 sec.
    Max Memory :                                 6039 MB
    Average Memory :                             3610.42 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13961.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   72005 sec.
    Turnaround time :                            72038 sec.

The output (if any) follows:

2022-01-28 06:49:59 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 40, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-2-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.09, 0.01, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-28 06:49:59 | INFO | fairseq.tasks.language_modeling | dictionary: 76624 types
2022-01-28 06:50:00 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
Calculating frequency stats:
  0%|          | 0/36718 [00:00<?, ?it/s]  3%|▎         | 1009/36718 [00:00<00:03, 10077.16it/s]  5%|▌         | 2017/36718 [00:00<00:03, 9587.12it/s]   9%|▊         | 3188/36718 [00:00<00:03, 10518.64it/s] 12%|█▏        | 4247/36718 [00:00<00:03, 10539.90it/s] 15%|█▍        | 5472/36718 [00:00<00:02, 11145.30it/s] 18%|█▊        | 6589/36718 [00:00<00:02, 10620.33it/s] 21%|██        | 7657/36718 [00:00<00:02, 10594.84it/s] 24%|██▍       | 8721/36718 [00:00<00:02, 10408.53it/s] 27%|██▋       | 9845/36718 [00:00<00:02, 10654.95it/s] 30%|██▉       | 10914/36718 [00:01<00:02, 10436.09it/s] 33%|███▎      | 12014/36718 [00:01<00:02, 10597.03it/s] 36%|███▌      | 13076/36718 [00:01<00:02, 10408.41it/s] 39%|███▊      | 14184/36718 [00:01<00:02, 10598.34it/s] 42%|████▏     | 15308/36718 [00:01<00:01, 10785.07it/s] 45%|████▍     | 16389/36718 [00:01<00:01, 10314.13it/s] 48%|████▊     | 17480/36718 [00:01<00:01, 10485.24it/s] 50%|█████     | 18533/36718 [00:01<00:01, 10302.72it/s] 54%|█████▍    | 19767/36718 [00:01<00:01, 10888.69it/s] 57%|█████▋    | 20861/36718 [00:01<00:01, 10643.67it/s] 60%|█████▉    | 21930/36718 [00:02<00:01, 10378.51it/s] 63%|██████▎   | 23050/36718 [00:02<00:01, 10610.64it/s] 66%|██████▌   | 24284/36718 [00:02<00:01, 11113.64it/s] 69%|██████▉   | 25517/36718 [00:02<00:00, 11463.02it/s] 73%|███████▎  | 26667/36718 [00:02<00:00, 10832.98it/s] 76%|███████▌  | 27760/36718 [00:02<00:00, 10420.72it/s] 79%|███████▊  | 28908/36718 [00:02<00:00, 10717.75it/s] 82%|████████▏ | 29988/36718 [00:02<00:00, 10717.68it/s] 85%|████████▍ | 31066/36718 [00:02<00:00, 10244.89it/s] 87%|████████▋ | 32098/36718 [00:03<00:00, 10216.24it/s] 90%|█████████ | 33125/36718 [00:03<00:00, 10118.41it/s] 93%|█████████▎| 34160/36718 [00:03<00:00, 10176.41it/s] 96%|█████████▌| 35258/36718 [00:03<00:00, 10410.21it/s] 99%|█████████▉| 36315/36718 [00:03<00:00, 10451.35it/s]100%|██████████| 36718/36718 [00:03<00:00, 10538.30it/s]

gathering stats for n=1
  0%|          | 0/36718 [00:00<?, ?it/s]  5%|▌         | 1940/36718 [00:00<00:01, 19393.41it/s] 11%|█         | 4094/36718 [00:00<00:01, 20653.13it/s] 17%|█▋        | 6331/36718 [00:00<00:01, 21424.86it/s] 23%|██▎       | 8474/36718 [00:00<00:01, 20462.94it/s] 29%|██▊       | 10527/36718 [00:00<00:01, 20473.35it/s] 34%|███▍      | 12579/36718 [00:00<00:01, 20296.35it/s] 40%|███▉      | 14671/36718 [00:00<00:01, 20495.53it/s] 46%|████▌     | 16723/36718 [00:00<00:00, 20093.55it/s] 51%|█████▏    | 18884/36718 [00:00<00:00, 20554.04it/s] 57%|█████▋    | 21008/36718 [00:01<00:00, 20754.88it/s] 63%|██████▎   | 23086/36718 [00:01<00:00, 20667.41it/s] 69%|██████▉   | 25493/36718 [00:01<00:00, 21683.81it/s] 75%|███████▌  | 27664/36718 [00:01<00:00, 20688.02it/s] 81%|████████  | 29820/36718 [00:01<00:00, 20935.42it/s] 87%|████████▋ | 31922/36718 [00:01<00:00, 20363.97it/s] 93%|█████████▎| 33967/36718 [00:01<00:00, 20003.26it/s] 98%|█████████▊| 35974/36718 [00:01<00:00, 19725.16it/s]100%|██████████| 36718/36718 [00:01<00:00, 20424.81it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 71.16it/s]2022-01-28 06:50:15 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(76624, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=76624, bias=False)
  )
)
2022-01-28 06:50:15 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-01-28 06:50:15 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-01-28 06:50:15 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-01-28 06:50:15 | INFO | fairseq_cli.train | num. shared model params: 58,145,792 (num. trained: 58,145,792)
2022-01-28 06:50:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-28 06:50:15 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-2-raw-full/valid
2022-01-28 06:50:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-28 06:50:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-28 06:50:15 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-01-28 06:50:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-28 06:50:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-28 06:50:15 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-01-28 06:50:15 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint_last.pt
2022-01-28 06:50:15 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint_last.pt
2022-01-28 06:50:15 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-28 06:50:15 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
2022-01-28 06:50:15 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-01-28 06:50:15 | INFO | fairseq.trainer | begin training epoch 1
2022-01-28 06:50:15 | INFO | fairseq_cli.train | Start iterating over samples

2022-01-28 06:55:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/home/andriusb/fq/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-01-28 06:56:19 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.679 | ppl 26227.6 | wps 7764.6 | wpb 2034.1 | bsz 4 | num_updates 64
2022-01-28 06:56:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-28 06:56:19 | INFO | train | epoch 001 | loss 16.131 | ppl 71746.6 | wps 5790.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 64 | lr 8.0984e-06 | gnorm 3.273 | train_wall 331 | gb_free 6.1 | wall 364
KL Stats: Epoch 1 Divergences: Uniform: 0.5172926276935453 Unigram: 3.6853185550826844
2022-01-28 06:56:19 | INFO | fairseq.trainer | begin training epoch 2
2022-01-28 06:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 06:59:26 | INFO | train_inner | epoch 002:     36 / 64 loss=15.584, ppl=49132, wps=5965.4, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=100, lr=1.25975e-05, gnorm=2.682, train_wall=517, gb_free=6.1, wall=551
2022-01-28 07:01:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:02:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.667 | ppl 13005.3 | wps 7784.9 | wpb 2034.1 | bsz 4 | num_updates 128
2022-01-28 07:02:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-28 07:02:18 | INFO | train | epoch 002 | loss 14.402 | ppl 21650.5 | wps 5805.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 128 | lr 1.60968e-05 | gnorm 1.512 | train_wall 330 | gb_free 6.1 | wall 723
KL Stats: Epoch 2 Divergences: Uniform: 0.535506547396838 Unigram: 2.4150737645199603
2022-01-28 07:02:18 | INFO | fairseq.trainer | begin training epoch 3
2022-01-28 07:02:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:08:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.836 | ppl 7313.05 | wps 7753.9 | wpb 2034.1 | bsz 4 | num_updates 192
2022-01-28 07:08:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-28 07:08:19 | INFO | train | epoch 003 | loss 13.489 | ppl 11496.4 | wps 5798.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 192 | lr 2.40952e-05 | gnorm 1.219 | train_wall 330 | gb_free 6.1 | wall 1084
KL Stats: Epoch 3 Divergences: Uniform: 0.521831843993741 Unigram: 1.731302136005532
2022-01-28 07:08:19 | INFO | fairseq.trainer | begin training epoch 4
2022-01-28 07:08:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:09:00 | INFO | train_inner | epoch 004:      8 / 64 loss=13.623, ppl=12620.1, wps=5675.6, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=200, lr=2.5095e-05, gnorm=1.248, train_wall=515, gb_free=6.1, wall=1125
2022-01-28 07:13:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:14:19 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.98 | ppl 4038.4 | wps 7771.9 | wpb 2034.1 | bsz 4 | num_updates 256
2022-01-28 07:14:19 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-28 07:14:19 | INFO | train | epoch 004 | loss 12.532 | ppl 5920.85 | wps 5797.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 256 | lr 3.20936e-05 | gnorm 0.972 | train_wall 330 | gb_free 6.1 | wall 1444
KL Stats: Epoch 4 Divergences: Uniform: 0.6080038100944007 Unigram: 1.1139892474894941
2022-01-28 07:14:19 | INFO | fairseq.trainer | begin training epoch 5
2022-01-28 07:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:18:08 | INFO | train_inner | epoch 005:     44 / 64 loss=12.178, ppl=4634.48, wps=5965.4, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=300, lr=3.75925e-05, gnorm=0.853, train_wall=517, gb_free=6.1, wall=1673
2022-01-28 07:19:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:20:19 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.455 | ppl 2807.03 | wps 7744.5 | wpb 2034.1 | bsz 4 | num_updates 320
2022-01-28 07:20:19 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-28 07:20:19 | INFO | train | epoch 005 | loss 11.725 | ppl 3385.97 | wps 5795.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 320 | lr 4.0092e-05 | gnorm 0.694 | train_wall 330 | gb_free 6.1 | wall 1804
KL Stats: Epoch 5 Divergences: Uniform: 0.8526854515075684 Unigram: 0.657000121094627
2022-01-28 07:20:19 | INFO | fairseq.trainer | begin training epoch 6
2022-01-28 07:20:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:25:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:26:20 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.208 | ppl 2365.98 | wps 7739.5 | wpb 2034.1 | bsz 4 | num_updates 384
2022-01-28 07:26:20 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-28 07:26:20 | INFO | train | epoch 006 | loss 11.288 | ppl 2500.33 | wps 5795.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 384 | lr 4.80904e-05 | gnorm 0.587 | train_wall 330 | gb_free 6.1 | wall 2165
KL Stats: Epoch 6 Divergences: Uniform: 1.158626014213799 Unigram: 0.45164549387776476
2022-01-28 07:26:20 | INFO | fairseq.trainer | begin training epoch 7
2022-01-28 07:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:27:43 | INFO | train_inner | epoch 007:     16 / 64 loss=11.311, ppl=2540.18, wps=5668.8, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=400, lr=5.009e-05, gnorm=0.586, train_wall=516, gb_free=6.1, wall=2248
2022-01-28 07:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:32:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.065 | ppl 2142.87 | wps 7753.8 | wpb 2034.1 | bsz 4 | num_updates 448
2022-01-28 07:32:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-28 07:32:20 | INFO | train | epoch 007 | loss 11.085 | ppl 2172.54 | wps 5797 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 448 | lr 5.60888e-05 | gnorm 0.525 | train_wall 330 | gb_free 6.1 | wall 2525
KL Stats: Epoch 7 Divergences: Uniform: 1.3926509841515973 Unigram: 0.4565270706594546
2022-01-28 07:32:20 | INFO | fairseq.trainer | begin training epoch 8
2022-01-28 07:32:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:36:50 | INFO | train_inner | epoch 008:     52 / 64 loss=11.024, ppl=2082.16, wps=5972.4, ups=0.18, wpb=32686.1, bsz=63.8, num_updates=500, lr=6.25875e-05, gnorm=0.517, train_wall=516, gb_free=6.1, wall=2795
2022-01-28 07:37:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:38:20 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.962 | ppl 1994.64 | wps 7764.1 | wpb 2034.1 | bsz 4 | num_updates 512
2022-01-28 07:38:20 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-28 07:38:20 | INFO | train | epoch 008 | loss 10.972 | ppl 2009.12 | wps 5803.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 512 | lr 6.40872e-05 | gnorm 0.512 | train_wall 330 | gb_free 6.1 | wall 2885
KL Stats: Epoch 8 Divergences: Uniform: 1.515704787893926 Unigram: 0.5257864319320472
2022-01-28 07:38:20 | INFO | fairseq.trainer | begin training epoch 9
2022-01-28 07:38:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:43:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:44:20 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.846 | ppl 1840.96 | wps 7769.9 | wpb 2034.1 | bsz 4 | num_updates 576
2022-01-28 07:44:20 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-28 07:44:20 | INFO | train | epoch 009 | loss 10.868 | ppl 1868.46 | wps 5801.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 576 | lr 7.20856e-05 | gnorm 0.485 | train_wall 330 | gb_free 6.1 | wall 3245
KL Stats: Epoch 9 Divergences: Uniform: 1.5656768388870923 Unigram: 0.6215113272573807
2022-01-28 07:44:20 | INFO | fairseq.trainer | begin training epoch 10
2022-01-28 07:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:46:25 | INFO | train_inner | epoch 010:     24 / 64 loss=10.859, ppl=1856.76, wps=5675.1, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=600, lr=7.5085e-05, gnorm=0.488, train_wall=515, gb_free=6.1, wall=3370
2022-01-28 07:49:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:50:20 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.743 | ppl 1713.55 | wps 7756.3 | wpb 2034.1 | bsz 4 | num_updates 640
2022-01-28 07:50:20 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-28 07:50:20 | INFO | train | epoch 010 | loss 10.759 | ppl 1733.45 | wps 5801 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 640 | lr 8.0084e-05 | gnorm 0.484 | train_wall 330 | gb_free 6.1 | wall 3605
KL Stats: Epoch 10 Divergences: Uniform: 1.5929517903647525 Unigram: 0.7264871824808878
2022-01-28 07:50:20 | INFO | fairseq.trainer | begin training epoch 11
2022-01-28 07:50:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:55:32 | INFO | train_inner | epoch 011:     60 / 64 loss=10.684, ppl=1645.04, wps=5971.8, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=700, lr=8.75825e-05, gnorm=0.494, train_wall=516, gb_free=6.1, wall=3917
2022-01-28 07:55:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:56:20 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.636 | ppl 1590.94 | wps 7778.8 | wpb 2034.1 | bsz 4 | num_updates 704
2022-01-28 07:56:20 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-28 07:56:20 | INFO | train | epoch 011 | loss 10.644 | ppl 1600.54 | wps 5804.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 704 | lr 8.80824e-05 | gnorm 0.498 | train_wall 330 | gb_free 6.1 | wall 3965
KL Stats: Epoch 11 Divergences: Uniform: 1.612439618447173 Unigram: 0.8305030339990676
2022-01-28 07:56:20 | INFO | fairseq.trainer | begin training epoch 12
2022-01-28 07:56:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:01:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:02:20 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.528 | ppl 1476.98 | wps 7757.8 | wpb 2034.1 | bsz 4 | num_updates 768
2022-01-28 08:02:20 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-28 08:02:20 | INFO | train | epoch 012 | loss 10.528 | ppl 1476.51 | wps 5797 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 768 | lr 9.60808e-05 | gnorm 0.479 | train_wall 330 | gb_free 6.1 | wall 4325
KL Stats: Epoch 12 Divergences: Uniform: 1.6238949221078947 Unigram: 0.9320540869442346
2022-01-28 08:02:20 | INFO | fairseq.trainer | begin training epoch 13
2022-01-28 08:02:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:05:07 | INFO | train_inner | epoch 013:     32 / 64 loss=10.504, ppl=1452.2, wps=5669.4, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=800, lr=0.00010008, gnorm=0.492, train_wall=516, gb_free=6.1, wall=4492
2022-01-28 08:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:08:21 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.438 | ppl 1387.24 | wps 7757 | wpb 2034.1 | bsz 4 | num_updates 832
2022-01-28 08:08:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-28 08:08:21 | INFO | train | epoch 013 | loss 10.414 | ppl 1364.39 | wps 5787 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 832 | lr 0.000104079 | gnorm 0.519 | train_wall 331 | gb_free 6.1 | wall 4686
KL Stats: Epoch 13 Divergences: Uniform: 1.6510038007062326 Unigram: 1.0207123463808883
2022-01-28 08:08:21 | INFO | fairseq.trainer | begin training epoch 14
2022-01-28 08:08:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:14:22 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.344 | ppl 1299.63 | wps 7769.9 | wpb 2034.1 | bsz 4 | num_updates 896
2022-01-28 08:14:22 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-28 08:14:22 | INFO | train | epoch 014 | loss 10.303 | ppl 1263.68 | wps 5788.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 896 | lr 0.000112078 | gnorm 0.555 | train_wall 331 | gb_free 6.1 | wall 5047
KL Stats: Epoch 14 Divergences: Uniform: 1.6774501410051288 Unigram: 1.103438947447342
2022-01-28 08:14:22 | INFO | fairseq.trainer | begin training epoch 15
2022-01-28 08:14:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:14:43 | INFO | train_inner | epoch 015:      4 / 64 loss=10.326, ppl=1283.61, wps=5663.6, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=900, lr=0.000112578, gnorm=0.537, train_wall=516, gb_free=6.1, wall=5068
2022-01-28 08:19:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:20:22 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.275 | ppl 1239.46 | wps 7766.7 | wpb 2034.1 | bsz 4 | num_updates 960
2022-01-28 08:20:22 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-28 08:20:22 | INFO | train | epoch 015 | loss 10.192 | ppl 1169.56 | wps 5804.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 960 | lr 0.000120076 | gnorm 0.532 | train_wall 330 | gb_free 6.1 | wall 5407
KL Stats: Epoch 15 Divergences: Uniform: 1.7024737484868875 Unigram: 1.1781917866526104
2022-01-28 08:20:22 | INFO | fairseq.trainer | begin training epoch 16
2022-01-28 08:20:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:23:50 | INFO | train_inner | epoch 016:     40 / 64 loss=10.151, ppl=1136.76, wps=5975.5, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=1000, lr=0.000125075, gnorm=0.553, train_wall=516, gb_free=6.1, wall=5615
2022-01-28 08:25:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:26:21 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.194 | ppl 1171.45 | wps 7767.7 | wpb 2034.1 | bsz 4 | num_updates 1024
2022-01-28 08:26:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-28 08:26:21 | INFO | train | epoch 016 | loss 10.086 | ppl 1086.56 | wps 5805.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1024 | lr 0.000128074 | gnorm 0.552 | train_wall 330 | gb_free 6.1 | wall 5766
KL Stats: Epoch 16 Divergences: Uniform: 1.7319772164132718 Unigram: 1.2509441373086854
2022-01-28 08:26:21 | INFO | fairseq.trainer | begin training epoch 17
2022-01-28 08:26:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:31:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:32:21 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.103 | ppl 1099.79 | wps 7771.2 | wpb 2034.1 | bsz 4 | num_updates 1088
2022-01-28 08:32:21 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-28 08:32:21 | INFO | train | epoch 017 | loss 9.979 | ppl 1009.02 | wps 5807.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1088 | lr 0.000136073 | gnorm 0.543 | train_wall 330 | gb_free 6.1 | wall 6126
KL Stats: Epoch 17 Divergences: Uniform: 1.7658928679827315 Unigram: 1.3139891623392457
2022-01-28 08:32:21 | INFO | fairseq.trainer | begin training epoch 18
2022-01-28 08:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:33:24 | INFO | train_inner | epoch 018:     12 / 64 loss=9.993, ppl=1018.95, wps=5678.6, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=1100, lr=0.000137573, gnorm=0.547, train_wall=515, gb_free=6.1, wall=6189
2022-01-28 08:37:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:38:21 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.038 | ppl 1051.17 | wps 7744.4 | wpb 2034.1 | bsz 4 | num_updates 1152
2022-01-28 08:38:21 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-28 08:38:21 | INFO | train | epoch 018 | loss 9.879 | ppl 941.39 | wps 5795.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1152 | lr 0.000144071 | gnorm 0.571 | train_wall 330 | gb_free 6.1 | wall 6486
KL Stats: Epoch 18 Divergences: Uniform: 1.8026881310022307 Unigram: 1.37569467607626
2022-01-28 08:38:21 | INFO | fairseq.trainer | begin training epoch 19
2022-01-28 08:38:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:42:31 | INFO | train_inner | epoch 019:     48 / 64 loss=9.829, ppl=909.5, wps=5966.1, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=1200, lr=0.00015007, gnorm=0.538, train_wall=517, gb_free=6.1, wall=6736
2022-01-28 08:43:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:44:22 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.965 | ppl 999.64 | wps 7765.5 | wpb 2034.1 | bsz 4 | num_updates 1216
2022-01-28 08:44:22 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-28 08:44:22 | INFO | train | epoch 019 | loss 9.775 | ppl 876.18 | wps 5796.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1216 | lr 0.00015207 | gnorm 0.521 | train_wall 330 | gb_free 6.1 | wall 6847
KL Stats: Epoch 19 Divergences: Uniform: 1.8338479249148278 Unigram: 1.4376394846044642
2022-01-28 08:44:22 | INFO | fairseq.trainer | begin training epoch 20
2022-01-28 08:44:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:49:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:50:22 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.88 | ppl 942.47 | wps 7771.4 | wpb 2034.1 | bsz 4 | num_updates 1280
2022-01-28 08:50:22 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-28 08:50:22 | INFO | train | epoch 020 | loss 9.679 | ppl 819.62 | wps 5800.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1280 | lr 0.000160068 | gnorm 0.549 | train_wall 330 | gb_free 6.1 | wall 7207
KL Stats: Epoch 20 Divergences: Uniform: 1.865427219386213 Unigram: 1.493379079602424
2022-01-28 08:50:22 | INFO | fairseq.trainer | begin training epoch 21
2022-01-28 08:50:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:52:06 | INFO | train_inner | epoch 021:     20 / 64 loss=9.674, ppl=816.88, wps=5672.8, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=1300, lr=0.000162568, gnorm=0.547, train_wall=516, gb_free=6.1, wall=7311
2022-01-28 08:55:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:56:22 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.834 | ppl 912.87 | wps 7775.2 | wpb 2034.1 | bsz 4 | num_updates 1344
2022-01-28 08:56:22 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-28 08:56:22 | INFO | train | epoch 021 | loss 9.585 | ppl 768.07 | wps 5792.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1344 | lr 0.000168066 | gnorm 0.535 | train_wall 331 | gb_free 6.1 | wall 7567
KL Stats: Epoch 21 Divergences: Uniform: 1.8964809686725954 Unigram: 1.5469733581932397
2022-01-28 08:56:22 | INFO | fairseq.trainer | begin training epoch 22
2022-01-28 08:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:01:14 | INFO | train_inner | epoch 022:     56 / 64 loss=9.532, ppl=740.51, wps=5963.3, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=1400, lr=0.000175065, gnorm=0.537, train_wall=517, gb_free=6.1, wall=7859
2022-01-28 09:01:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:02:23 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.771 | ppl 873.98 | wps 7745.4 | wpb 2034.1 | bsz 4 | num_updates 1408
2022-01-28 09:02:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-28 09:02:23 | INFO | train | epoch 022 | loss 9.496 | ppl 721.98 | wps 5795.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1408 | lr 0.000176065 | gnorm 0.542 | train_wall 330 | gb_free 6.1 | wall 7928
KL Stats: Epoch 22 Divergences: Uniform: 1.9225437859138594 Unigram: 1.5984492769677323
2022-01-28 09:02:23 | INFO | fairseq.trainer | begin training epoch 23
2022-01-28 09:02:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:07:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:08:23 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.716 | ppl 840.93 | wps 7771.2 | wpb 2034.1 | bsz 4 | num_updates 1472
2022-01-28 09:08:23 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-28 09:08:23 | INFO | train | epoch 023 | loss 9.41 | ppl 680.22 | wps 5794.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1472 | lr 0.000184063 | gnorm 0.521 | train_wall 330 | gb_free 6.1 | wall 8288
KL Stats: Epoch 23 Divergences: Uniform: 1.951205435192855 Unigram: 1.6460889248227841
2022-01-28 09:08:23 | INFO | fairseq.trainer | begin training epoch 24
2022-01-28 09:08:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:10:49 | INFO | train_inner | epoch 024:     28 / 64 loss=9.395, ppl=673.07, wps=5671.1, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=1500, lr=0.000187563, gnorm=0.527, train_wall=516, gb_free=6.1, wall=8434
2022-01-28 09:13:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:14:24 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.656 | ppl 806.61 | wps 7766.4 | wpb 2034.1 | bsz 4 | num_updates 1536
2022-01-28 09:14:24 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-28 09:14:24 | INFO | train | epoch 024 | loss 9.326 | ppl 641.77 | wps 5795.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1536 | lr 0.000192062 | gnorm 0.539 | train_wall 330 | gb_free 6.1 | wall 8648
KL Stats: Epoch 24 Divergences: Uniform: 1.973030490236075 Unigram: 1.6884933985766442
2022-01-28 09:14:24 | INFO | fairseq.trainer | begin training epoch 25
2022-01-28 09:14:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:19:56 | INFO | train_inner | epoch 025:     64 / 64 loss=9.272, ppl=618.29, wps=5960.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1600, lr=0.00020006, gnorm=0.527, train_wall=516, gb_free=6.1, wall=8981
2022-01-28 09:19:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:20:24 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.626 | ppl 790.03 | wps 7761.4 | wpb 2034.1 | bsz 4 | num_updates 1600
2022-01-28 09:20:24 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-28 09:20:24 | INFO | train | epoch 025 | loss 9.245 | ppl 606.6 | wps 5792.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1600 | lr 0.00020006 | gnorm 0.521 | train_wall 331 | gb_free 6.1 | wall 9009
KL Stats: Epoch 25 Divergences: Uniform: 2.0033843760695715 Unigram: 1.732597001541788
2022-01-28 09:20:24 | INFO | fairseq.trainer | begin training epoch 26
2022-01-28 09:20:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:25:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:26:24 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.579 | ppl 764.92 | wps 7782 | wpb 2034.1 | bsz 4 | num_updates 1664
2022-01-28 09:26:24 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-28 09:26:24 | INFO | train | epoch 026 | loss 9.164 | ppl 573.5 | wps 5801.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1664 | lr 0.000208058 | gnorm 0.534 | train_wall 330 | gb_free 6.1 | wall 9369
KL Stats: Epoch 26 Divergences: Uniform: 2.0160242459322153 Unigram: 1.7725593984470969
2022-01-28 09:26:24 | INFO | fairseq.trainer | begin training epoch 27
2022-01-28 09:26:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:29:32 | INFO | train_inner | epoch 027:     36 / 64 loss=9.136, ppl=562.48, wps=5677.5, ups=0.17, wpb=32682.8, bsz=63.8, num_updates=1700, lr=0.000212558, gnorm=0.525, train_wall=517, gb_free=6.1, wall=9557
2022-01-28 09:31:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:32:24 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.544 | ppl 746.42 | wps 7741.7 | wpb 2034.1 | bsz 4 | num_updates 1728
2022-01-28 09:32:24 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-28 09:32:24 | INFO | train | epoch 027 | loss 9.083 | ppl 542.38 | wps 5797 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1728 | lr 0.000216057 | gnorm 0.515 | train_wall 330 | gb_free 6.1 | wall 9729
KL Stats: Epoch 27 Divergences: Uniform: 2.042995260389339 Unigram: 1.8096160666444172
2022-01-28 09:32:24 | INFO | fairseq.trainer | begin training epoch 28
2022-01-28 09:32:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:38:25 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.517 | ppl 732.42 | wps 7763.7 | wpb 2034.1 | bsz 4 | num_updates 1792
2022-01-28 09:38:25 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-28 09:38:25 | INFO | train | epoch 028 | loss 9.005 | ppl 513.82 | wps 5798.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1792 | lr 0.000224055 | gnorm 0.52 | train_wall 330 | gb_free 6.1 | wall 10090
KL Stats: Epoch 28 Divergences: Uniform: 2.0736893576830693 Unigram: 1.846546074468932
2022-01-28 09:38:25 | INFO | fairseq.trainer | begin training epoch 29
2022-01-28 09:38:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:39:06 | INFO | train_inner | epoch 029:      8 / 64 loss=9.021, ppl=519.42, wps=5670.5, ups=0.17, wpb=32594.2, bsz=63.7, num_updates=1800, lr=0.000225055, gnorm=0.524, train_wall=516, gb_free=6.1, wall=10131
2022-01-28 09:43:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:44:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.485 | ppl 716.83 | wps 7755.7 | wpb 2034.1 | bsz 4 | num_updates 1856
2022-01-28 09:44:25 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-28 09:44:25 | INFO | train | epoch 029 | loss 8.926 | ppl 486.56 | wps 5789.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1856 | lr 0.000232054 | gnorm 0.526 | train_wall 331 | gb_free 6.1 | wall 10450
KL Stats: Epoch 29 Divergences: Uniform: 2.096342231451645 Unigram: 1.8813112317536917
2022-01-28 09:44:25 | INFO | fairseq.trainer | begin training epoch 30
2022-01-28 09:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:48:15 | INFO | train_inner | epoch 030:     44 / 64 loss=8.893, ppl=475.49, wps=5962.7, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=1900, lr=0.000237553, gnorm=0.515, train_wall=517, gb_free=6.1, wall=10679
2022-01-28 09:49:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:50:26 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.46 | ppl 704.36 | wps 7734.9 | wpb 2034.1 | bsz 4 | num_updates 1920
2022-01-28 09:50:26 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-28 09:50:26 | INFO | train | epoch 030 | loss 8.849 | ppl 460.96 | wps 5797 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1920 | lr 0.000240052 | gnorm 0.518 | train_wall 330 | gb_free 6.1 | wall 10811
KL Stats: Epoch 30 Divergences: Uniform: 2.1150986915474936 Unigram: 1.918886666714854
2022-01-28 09:50:26 | INFO | fairseq.trainer | begin training epoch 31
2022-01-28 09:50:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:55:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:56:26 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.409 | ppl 679.9 | wps 7722 | wpb 2034.1 | bsz 4 | num_updates 1984
2022-01-28 09:56:26 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-28 09:56:26 | INFO | train | epoch 031 | loss 8.77 | ppl 436.49 | wps 5788.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1984 | lr 0.00024805 | gnorm 0.495 | train_wall 331 | gb_free 6.1 | wall 11171
KL Stats: Epoch 31 Divergences: Uniform: 2.1327092250717303 Unigram: 1.9509168927319251
2022-01-28 09:56:26 | INFO | fairseq.trainer | begin training epoch 32
2022-01-28 09:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:57:50 | INFO | train_inner | epoch 032:     16 / 64 loss=8.771, ppl=436.82, wps=5663.8, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=2000, lr=0.00025005, gnorm=0.501, train_wall=516, gb_free=6.1, wall=11255
2022-01-28 10:02:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:02:28 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.378 | ppl 665.14 | wps 7727.1 | wpb 2034.1 | bsz 4 | num_updates 2048
2022-01-28 10:02:28 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-28 10:02:28 | INFO | train | epoch 032 | loss 8.695 | ppl 414.5 | wps 5776.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2048 | lr 0.000256049 | gnorm 0.501 | train_wall 331 | gb_free 6.1 | wall 11533
KL Stats: Epoch 32 Divergences: Uniform: 2.158816162131386 Unigram: 1.9858279901242923
2022-01-28 10:02:28 | INFO | fairseq.trainer | begin training epoch 33
2022-01-28 10:02:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:07:00 | INFO | train_inner | epoch 033:     52 / 64 loss=8.658, ppl=404.05, wps=5948.3, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=2100, lr=0.000262548, gnorm=0.505, train_wall=518, gb_free=6.1, wall=11805
2022-01-28 10:08:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:08:29 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.362 | ppl 657.93 | wps 7721.1 | wpb 2034.1 | bsz 4 | num_updates 2112
2022-01-28 10:08:29 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-28 10:08:29 | INFO | train | epoch 033 | loss 8.621 | ppl 393.7 | wps 5783.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2112 | lr 0.000264047 | gnorm 0.503 | train_wall 331 | gb_free 6.1 | wall 11894
KL Stats: Epoch 33 Divergences: Uniform: 2.1858374485163172 Unigram: 2.0199363615795396
2022-01-28 10:08:29 | INFO | fairseq.trainer | begin training epoch 34
2022-01-28 10:08:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:14:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:14:31 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.347 | ppl 651.11 | wps 7739 | wpb 2034.1 | bsz 4 | num_updates 2176
2022-01-28 10:14:31 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-28 10:14:31 | INFO | train | epoch 034 | loss 8.545 | ppl 373.55 | wps 5779.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2176 | lr 0.000272046 | gnorm 0.509 | train_wall 331 | gb_free 6.1 | wall 12256
KL Stats: Epoch 34 Divergences: Uniform: 2.2058259606943413 Unigram: 2.0526119002330794
2022-01-28 10:14:31 | INFO | fairseq.trainer | begin training epoch 35
2022-01-28 10:14:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:16:36 | INFO | train_inner | epoch 035:     24 / 64 loss=8.532, ppl=370.28, wps=5655.7, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=2200, lr=0.000275045, gnorm=0.507, train_wall=517, gb_free=6.1, wall=12381
2022-01-28 10:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:20:32 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.314 | ppl 636.43 | wps 7763.5 | wpb 2034.1 | bsz 4 | num_updates 2240
2022-01-28 10:20:32 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-28 10:20:32 | INFO | train | epoch 035 | loss 8.473 | ppl 355.36 | wps 5782.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2240 | lr 0.000280044 | gnorm 0.499 | train_wall 331 | gb_free 6.1 | wall 12617
KL Stats: Epoch 35 Divergences: Uniform: 2.22939619941813 Unigram: 2.0811919181864367
2022-01-28 10:20:32 | INFO | fairseq.trainer | begin training epoch 36
2022-01-28 10:20:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:25:44 | INFO | train_inner | epoch 036:     60 / 64 loss=8.429, ppl=344.72, wps=5958.1, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=2300, lr=0.000287543, gnorm=0.497, train_wall=518, gb_free=6.1, wall=12929
2022-01-28 10:26:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:26:32 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.29 | ppl 625.91 | wps 7758 | wpb 2034.1 | bsz 4 | num_updates 2304
2022-01-28 10:26:32 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-28 10:26:32 | INFO | train | epoch 036 | loss 8.4 | ppl 337.73 | wps 5794.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2304 | lr 0.000288042 | gnorm 0.498 | train_wall 331 | gb_free 6.1 | wall 12977
KL Stats: Epoch 36 Divergences: Uniform: 2.2500439998154316 Unigram: 2.1159563270929493
2022-01-28 10:26:32 | INFO | fairseq.trainer | begin training epoch 37
2022-01-28 10:26:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:32:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:32:33 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.299 | ppl 629.87 | wps 7761.5 | wpb 2034.1 | bsz 4 | num_updates 2368
2022-01-28 10:32:33 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-28 10:32:33 | INFO | train | epoch 037 | loss 8.33 | ppl 321.82 | wps 5792.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2368 | lr 0.000296041 | gnorm 0.505 | train_wall 331 | gb_free 6.1 | wall 13338
KL Stats: Epoch 37 Divergences: Uniform: 2.2722111293278004 Unigram: 2.1474760047499464
2022-01-28 10:32:33 | INFO | fairseq.trainer | begin training epoch 38
2022-01-28 10:32:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:35:20 | INFO | train_inner | epoch 038:     32 / 64 loss=8.309, ppl=317.07, wps=5668.3, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=2400, lr=0.00030004, gnorm=0.502, train_wall=516, gb_free=6.1, wall=13505
2022-01-28 10:38:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:38:33 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.279 | ppl 621.34 | wps 7752.9 | wpb 2034.1 | bsz 4 | num_updates 2432
2022-01-28 10:38:33 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-28 10:38:33 | INFO | train | epoch 038 | loss 8.262 | ppl 306.93 | wps 5795.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2432 | lr 0.000304039 | gnorm 0.503 | train_wall 330 | gb_free 6.1 | wall 13698
KL Stats: Epoch 38 Divergences: Uniform: 2.3015673921418305 Unigram: 2.1702069517669815
2022-01-28 10:38:33 | INFO | fairseq.trainer | begin training epoch 39
2022-01-28 10:38:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:44:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:44:34 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.266 | ppl 615.79 | wps 7752.4 | wpb 2034.1 | bsz 4 | num_updates 2496
2022-01-28 10:44:34 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-28 10:44:34 | INFO | train | epoch 039 | loss 8.193 | ppl 292.57 | wps 5789.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2496 | lr 0.000312038 | gnorm 0.494 | train_wall 331 | gb_free 6.1 | wall 14059
KL Stats: Epoch 39 Divergences: Uniform: 2.3087453681312557 Unigram: 2.206716824360969
2022-01-28 10:44:34 | INFO | fairseq.trainer | begin training epoch 40
2022-01-28 10:44:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:44:55 | INFO | train_inner | epoch 040:      4 / 64 loss=8.215, ppl=297.06, wps=5665.8, ups=0.17, wpb=32594.2, bsz=63.7, num_updates=2500, lr=0.000312538, gnorm=0.499, train_wall=516, gb_free=6.1, wall=14080
2022-01-28 10:50:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:50:34 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.247 | ppl 607.43 | wps 7764.4 | wpb 2034.1 | bsz 4 | num_updates 2560
2022-01-28 10:50:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2560 updates
2022-01-28 10:50:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint40.pt
2022-01-28 10:50:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint40.pt
2022-01-28 10:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint40.pt (epoch 40 @ 2560 updates, score 9.247) (writing took 5.0042713028378785 seconds)
2022-01-28 10:50:39 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-28 10:50:39 | INFO | train | epoch 040 | loss 8.124 | ppl 279.02 | wps 5723.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2560 | lr 0.000320036 | gnorm 0.497 | train_wall 330 | gb_free 6.1 | wall 14424
KL Stats: Epoch 40 Divergences: Uniform: 2.338443692609886 Unigram: 2.233636136167165
2022-01-28 10:50:39 | INFO | fairseq.trainer | begin training epoch 41
2022-01-28 10:50:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:54:07 | INFO | train_inner | epoch 041:     40 / 64 loss=8.101, ppl=274.5, wps=5915.9, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=2600, lr=0.000325035, gnorm=0.495, train_wall=516, gb_free=6.1, wall=14632
2022-01-28 10:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:56:39 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.237 | ppl 603.28 | wps 7754.1 | wpb 2034.1 | bsz 4 | num_updates 2624 | best_loss 9.237
2022-01-28 10:56:39 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-28 10:56:39 | INFO | train | epoch 041 | loss 8.06 | ppl 266.9 | wps 5795.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2624 | lr 0.000328034 | gnorm 0.5 | train_wall 330 | gb_free 6.1 | wall 14784
KL Stats: Epoch 41 Divergences: Uniform: 2.353695103077692 Unigram: 2.2585212201263656
2022-01-28 10:56:39 | INFO | fairseq.trainer | begin training epoch 42
2022-01-28 10:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:02:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:02:40 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.215 | ppl 594.23 | wps 7757.7 | wpb 2034.1 | bsz 4 | num_updates 2688 | best_loss 9.215
2022-01-28 11:02:40 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-28 11:02:40 | INFO | train | epoch 042 | loss 7.996 | ppl 255.32 | wps 5795.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2688 | lr 0.000336033 | gnorm 0.507 | train_wall 330 | gb_free 6.1 | wall 15145
KL Stats: Epoch 42 Divergences: Uniform: 2.372820148772787 Unigram: 2.291900601960583
2022-01-28 11:02:40 | INFO | fairseq.trainer | begin training epoch 43
2022-01-28 11:02:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:03:42 | INFO | train_inner | epoch 043:     12 / 64 loss=8.003, ppl=256.46, wps=5669.3, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=2700, lr=0.000337533, gnorm=0.507, train_wall=516, gb_free=6.1, wall=15207
2022-01-28 11:08:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:08:40 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.236 | ppl 603.08 | wps 7744.3 | wpb 2034.1 | bsz 4 | num_updates 2752 | best_loss 9.236
2022-01-28 11:08:40 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-28 11:08:40 | INFO | train | epoch 043 | loss 7.931 | ppl 244.07 | wps 5796 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2752 | lr 0.000344031 | gnorm 0.497 | train_wall 330 | gb_free 6.1 | wall 15505
KL Stats: Epoch 43 Divergences: Uniform: 2.3939443199675825 Unigram: 2.314864079686625
2022-01-28 11:08:40 | INFO | fairseq.trainer | begin training epoch 44
2022-01-28 11:08:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:12:50 | INFO | train_inner | epoch 044:     48 / 64 loss=7.897, ppl=238.35, wps=5962.9, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=2800, lr=0.00035003, gnorm=0.499, train_wall=517, gb_free=6.1, wall=15755
2022-01-28 11:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:14:41 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.243 | ppl 605.96 | wps 7747.4 | wpb 2034.1 | bsz 4 | num_updates 2816 | best_loss 9.243
2022-01-28 11:14:41 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-28 11:14:41 | INFO | train | epoch 044 | loss 7.872 | ppl 234.19 | wps 5787.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2816 | lr 0.00035203 | gnorm 0.5 | train_wall 331 | gb_free 6.1 | wall 15866
KL Stats: Epoch 44 Divergences: Uniform: 2.4102449591721737 Unigram: 2.3398781770925443
2022-01-28 11:14:41 | INFO | fairseq.trainer | begin training epoch 45
2022-01-28 11:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:20:42 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.22 | ppl 596.18 | wps 7740.5 | wpb 2034.1 | bsz 4 | num_updates 2880 | best_loss 9.22
2022-01-28 11:20:42 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-28 11:20:42 | INFO | train | epoch 045 | loss 7.809 | ppl 224.19 | wps 5787.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2880 | lr 0.000360028 | gnorm 0.503 | train_wall 331 | gb_free 6.1 | wall 16227
KL Stats: Epoch 45 Divergences: Uniform: 2.431461939957742 Unigram: 2.3716529058023816
2022-01-28 11:20:42 | INFO | fairseq.trainer | begin training epoch 46
2022-01-28 11:20:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:22:26 | INFO | train_inner | epoch 046:     20 / 64 loss=7.808, ppl=224.14, wps=5663.9, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=2900, lr=0.000362528, gnorm=0.503, train_wall=516, gb_free=6.1, wall=16331
2022-01-28 11:26:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:26:42 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.232 | ppl 601.21 | wps 7760.1 | wpb 2034.1 | bsz 4 | num_updates 2944 | best_loss 9.232
2022-01-28 11:26:42 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-28 11:26:42 | INFO | train | epoch 046 | loss 7.749 | ppl 215.16 | wps 5792.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2944 | lr 0.000368026 | gnorm 0.506 | train_wall 331 | gb_free 6.1 | wall 16587
KL Stats: Epoch 46 Divergences: Uniform: 2.4456329117126057 Unigram: 2.3897701842066343
2022-01-28 11:26:42 | INFO | fairseq.trainer | begin training epoch 47
2022-01-28 11:26:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:31:34 | INFO | train_inner | epoch 047:     56 / 64 loss=7.718, ppl=210.61, wps=5964.7, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=3000, lr=0.000375025, gnorm=0.497, train_wall=517, gb_free=6.1, wall=16879
2022-01-28 11:32:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:32:42 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.212 | ppl 593.2 | wps 7738.5 | wpb 2034.1 | bsz 4 | num_updates 3008 | best_loss 9.212
2022-01-28 11:32:42 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-28 11:32:42 | INFO | train | epoch 047 | loss 7.69 | ppl 206.56 | wps 5799.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3008 | lr 0.000376025 | gnorm 0.49 | train_wall 330 | gb_free 6.1 | wall 16947
KL Stats: Epoch 47 Divergences: Uniform: 2.4689768139542445 Unigram: 2.4125851327665
2022-01-28 11:32:42 | INFO | fairseq.trainer | begin training epoch 48
2022-01-28 11:32:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:38:43 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.203 | ppl 589.33 | wps 7766.2 | wpb 2034.1 | bsz 4 | num_updates 3072 | best_loss 9.203
2022-01-28 11:38:43 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-28 11:38:43 | INFO | train | epoch 048 | loss 7.634 | ppl 198.64 | wps 5798.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3072 | lr 0.000384023 | gnorm 0.508 | train_wall 330 | gb_free 6.1 | wall 17308
KL Stats: Epoch 48 Divergences: Uniform: 2.4868599425619355 Unigram: 2.4431616088521784
2022-01-28 11:38:43 | INFO | fairseq.trainer | begin training epoch 49
2022-01-28 11:38:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:41:09 | INFO | train_inner | epoch 049:     28 / 64 loss=7.616, ppl=196.19, wps=5672.6, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=3100, lr=0.000387523, gnorm=0.502, train_wall=515, gb_free=6.1, wall=17454
2022-01-28 11:44:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:44:43 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.233 | ppl 601.57 | wps 7750.3 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 9.233
2022-01-28 11:44:43 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-28 11:44:43 | INFO | train | epoch 049 | loss 7.577 | ppl 190.91 | wps 5797.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3136 | lr 0.000392022 | gnorm 0.505 | train_wall 330 | gb_free 6.1 | wall 17668
KL Stats: Epoch 49 Divergences: Uniform: 2.4924834308971366 Unigram: 2.462762750099883
2022-01-28 11:44:43 | INFO | fairseq.trainer | begin training epoch 50
2022-01-28 11:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:50:15 | INFO | train_inner | epoch 050:     64 / 64 loss=7.553, ppl=187.73, wps=5967.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=3200, lr=0.00040002, gnorm=0.522, train_wall=515, gb_free=6.1, wall=18000
2022-01-28 11:50:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:50:43 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.256 | ppl 611.33 | wps 7770.2 | wpb 2034.1 | bsz 4 | num_updates 3200 | best_loss 9.247
2022-01-28 11:50:43 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-28 11:50:43 | INFO | train | epoch 050 | loss 7.526 | ppl 184.3 | wps 5801 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3200 | lr 0.00040002 | gnorm 0.527 | train_wall 330 | gb_free 6.1 | wall 18028
KL Stats: Epoch 50 Divergences: Uniform: 2.5113861392563415 Unigram: 2.479310064066054
2022-01-28 11:50:43 | INFO | fairseq.trainer | begin training epoch 51
2022-01-28 11:50:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:56:43 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.251 | ppl 609.24 | wps 7744.8 | wpb 2034.1 | bsz 4 | num_updates 3264 | best_loss 9.247
2022-01-28 11:56:43 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-28 11:56:43 | INFO | train | epoch 051 | loss 7.469 | ppl 177.14 | wps 5793.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3264 | lr 0.000408018 | gnorm 0.5 | train_wall 330 | gb_free 6.1 | wall 18388
KL Stats: Epoch 51 Divergences: Uniform: 2.539319454591855 Unigram: 2.501596230588592
2022-01-28 11:56:43 | INFO | fairseq.trainer | begin training epoch 52
2022-01-28 11:56:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:59:51 | INFO | train_inner | epoch 052:     36 / 64 loss=7.445, ppl=174.25, wps=5670.7, ups=0.17, wpb=32682.8, bsz=63.8, num_updates=3300, lr=0.000412518, gnorm=0.502, train_wall=517, gb_free=6.1, wall=18576
2022-01-28 12:02:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:02:44 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.246 | ppl 607.02 | wps 7784.2 | wpb 2034.1 | bsz 4 | num_updates 3328 | best_loss 9.246
2022-01-28 12:02:44 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-28 12:02:44 | INFO | train | epoch 052 | loss 7.417 | ppl 170.95 | wps 5798.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3328 | lr 0.000416017 | gnorm 0.515 | train_wall 330 | gb_free 6.1 | wall 18749
KL Stats: Epoch 52 Divergences: Uniform: 2.5465110020271364 Unigram: 2.5286453626338927
2022-01-28 12:02:44 | INFO | fairseq.trainer | begin training epoch 53
2022-01-28 12:02:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:08:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:08:44 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.216 | ppl 594.61 | wps 7821.7 | wpb 2034.1 | bsz 4 | num_updates 3392 | best_loss 9.216
2022-01-28 12:08:44 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-28 12:08:44 | INFO | train | epoch 053 | loss 7.366 | ppl 164.91 | wps 5793.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3392 | lr 0.000424015 | gnorm 0.498 | train_wall 331 | gb_free 6.1 | wall 19109
KL Stats: Epoch 53 Divergences: Uniform: 2.571222722450998 Unigram: 2.546206268267371
2022-01-28 12:08:44 | INFO | fairseq.trainer | begin training epoch 54
2022-01-28 12:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:09:26 | INFO | train_inner | epoch 054:      8 / 64 loss=7.379, ppl=166.43, wps=5674.1, ups=0.17, wpb=32597.5, bsz=63.7, num_updates=3400, lr=0.000425015, gnorm=0.511, train_wall=516, gb_free=6.1, wall=19151
2022-01-28 12:14:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:14:43 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.296 | ppl 628.5 | wps 7787.8 | wpb 2034.1 | bsz 4 | num_updates 3456 | best_loss 9.247
2022-01-28 12:14:43 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-28 12:14:43 | INFO | train | epoch 054 | loss 7.315 | ppl 159.29 | wps 5822 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3456 | lr 0.000432014 | gnorm 0.519 | train_wall 329 | gb_free 6.1 | wall 19468
KL Stats: Epoch 54 Divergences: Uniform: 2.5740644630198473 Unigram: 2.5646475020695196
2022-01-28 12:14:43 | INFO | fairseq.trainer | begin training epoch 55
2022-01-28 12:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:18:31 | INFO | train_inner | epoch 055:     44 / 64 loss=7.288, ppl=156.28, wps=5989.9, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=3500, lr=0.000437513, gnorm=0.515, train_wall=515, gb_free=6.1, wall=19696
2022-01-28 12:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:20:42 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.278 | ppl 620.84 | wps 7787.2 | wpb 2034.1 | bsz 4 | num_updates 3520 | best_loss 9.247
2022-01-28 12:20:42 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-28 12:20:42 | INFO | train | epoch 055 | loss 7.269 | ppl 154.21 | wps 5820.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3520 | lr 0.000440012 | gnorm 0.525 | train_wall 329 | gb_free 6.1 | wall 19827
KL Stats: Epoch 55 Divergences: Uniform: 2.5912032571835777 Unigram: 2.5899584917719505
2022-01-28 12:20:42 | INFO | fairseq.trainer | begin training epoch 56
2022-01-28 12:20:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:26:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:26:40 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.374 | ppl 663.64 | wps 7816.2 | wpb 2034.1 | bsz 4 | num_updates 3584 | best_loss 9.247
2022-01-28 12:26:40 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-28 12:26:40 | INFO | train | epoch 056 | loss 7.22 | ppl 149.05 | wps 5823.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3584 | lr 0.00044801 | gnorm 0.522 | train_wall 329 | gb_free 6.1 | wall 20185
KL Stats: Epoch 56 Divergences: Uniform: 2.590784819772706 Unigram: 2.6031009681056565
2022-01-28 12:26:40 | INFO | fairseq.trainer | begin training epoch 57
2022-01-28 12:26:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:28:03 | INFO | train_inner | epoch 057:     16 / 64 loss=7.224, ppl=149.46, wps=5698.3, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=3600, lr=0.00045001, gnorm=0.527, train_wall=513, gb_free=6.1, wall=20268
2022-01-28 12:32:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:32:39 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.367 | ppl 660.41 | wps 7783.3 | wpb 2034.1 | bsz 4 | num_updates 3648 | best_loss 9.247
2022-01-28 12:32:39 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-28 12:32:39 | INFO | train | epoch 057 | loss 7.172 | ppl 144.24 | wps 5817.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3648 | lr 0.000456009 | gnorm 0.528 | train_wall 329 | gb_free 6.1 | wall 20544
KL Stats: Epoch 57 Divergences: Uniform: 2.6208872901985925 Unigram: 2.6300605920172773
2022-01-28 12:32:39 | INFO | fairseq.trainer | begin training epoch 58
2022-01-28 12:32:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:37:09 | INFO | train_inner | epoch 058:     52 / 64 loss=7.148, ppl=141.82, wps=5984.9, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=3700, lr=0.000462508, gnorm=0.521, train_wall=515, gb_free=6.1, wall=20814
2022-01-28 12:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:38:39 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.371 | ppl 661.97 | wps 7781.5 | wpb 2034.1 | bsz 4 | num_updates 3712 | best_loss 9.247
2022-01-28 12:38:39 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-28 12:38:39 | INFO | train | epoch 058 | loss 7.128 | ppl 139.83 | wps 5815.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3712 | lr 0.000464007 | gnorm 0.519 | train_wall 329 | gb_free 6.1 | wall 20903
KL Stats: Epoch 58 Divergences: Uniform: 2.6323871434766666 Unigram: 2.643866601648513
2022-01-28 12:38:39 | INFO | fairseq.trainer | begin training epoch 59
2022-01-28 12:38:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:44:38 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.429 | ppl 689.46 | wps 7768.4 | wpb 2034.1 | bsz 4 | num_updates 3776 | best_loss 9.247
2022-01-28 12:44:38 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-28 12:44:38 | INFO | train | epoch 059 | loss 7.083 | ppl 135.58 | wps 5809.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3776 | lr 0.000472006 | gnorm 0.527 | train_wall 330 | gb_free 6.1 | wall 21263
KL Stats: Epoch 59 Divergences: Uniform: 2.648977071721449 Unigram: 2.6605543543167554
2022-01-28 12:44:38 | INFO | fairseq.trainer | begin training epoch 60
2022-01-28 12:44:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:46:43 | INFO | train_inner | epoch 060:     24 / 64 loss=7.077, ppl=135.06, wps=5684.5, ups=0.17, wpb=32600.8, bsz=63.7, num_updates=3800, lr=0.000475005, gnorm=0.533, train_wall=514, gb_free=6.1, wall=21388
2022-01-28 12:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:50:37 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.387 | ppl 669.75 | wps 7805.1 | wpb 2034.1 | bsz 4 | num_updates 3840 | best_loss 9.247
2022-01-28 12:50:37 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-28 12:50:37 | INFO | train | epoch 060 | loss 7.039 | ppl 131.48 | wps 5811.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3840 | lr 0.000480004 | gnorm 0.541 | train_wall 330 | gb_free 6.1 | wall 21622
KL Stats: Epoch 60 Divergences: Uniform: 2.661808255615099 Unigram: 2.686170412496202
2022-01-28 12:50:37 | INFO | fairseq.trainer | begin training epoch 61
2022-01-28 12:50:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:55:49 | INFO | train_inner | epoch 061:     60 / 64 loss=7.018, ppl=129.64, wps=5980.5, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=3900, lr=0.000487503, gnorm=0.537, train_wall=516, gb_free=6.1, wall=21934
2022-01-28 12:56:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:56:37 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.416 | ppl 683.04 | wps 7757.3 | wpb 2034.1 | bsz 4 | num_updates 3904 | best_loss 9.247
2022-01-28 12:56:37 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-28 12:56:37 | INFO | train | epoch 061 | loss 6.995 | ppl 127.58 | wps 5809.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3904 | lr 0.000488002 | gnorm 0.539 | train_wall 330 | gb_free 6.1 | wall 21982
KL Stats: Epoch 61 Divergences: Uniform: 2.6821968288777196 Unigram: 2.6959172476147524
2022-01-28 12:56:37 | INFO | fairseq.trainer | begin training epoch 62
2022-01-28 12:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:02:33 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.412 | ppl 681.38 | wps 7963 | wpb 2034.1 | bsz 4 | num_updates 3968 | best_loss 9.247
2022-01-28 13:02:33 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-28 13:02:33 | INFO | train | epoch 062 | loss 6.954 | ppl 124.01 | wps 5867.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3968 | lr 0.000496001 | gnorm 0.54 | train_wall 327 | gb_free 6.1 | wall 22338
KL Stats: Epoch 62 Divergences: Uniform: 2.686842432987198 Unigram: 2.7209146196345886
2022-01-28 13:02:33 | INFO | fairseq.trainer | begin training epoch 63
2022-01-28 13:02:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:05:16 | INFO | train_inner | epoch 063:     32 / 64 loss=6.929, ppl=121.83, wps=5751.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4000, lr=0.0005, gnorm=0.542, train_wall=508, gb_free=6.1, wall=22501
2022-01-28 13:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:08:25 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.436 | ppl 692.69 | wps 7934.9 | wpb 2034.1 | bsz 4 | num_updates 4032 | best_loss 9.247
2022-01-28 13:08:25 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-28 13:08:25 | INFO | train | epoch 063 | loss 6.912 | ppl 120.39 | wps 5924.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4032 | lr 0.000498012 | gnorm 0.541 | train_wall 323 | gb_free 6.1 | wall 22690
KL Stats: Epoch 63 Divergences: Uniform: 2.6988715676799964 Unigram: 2.7330695664253013
2022-01-28 13:08:26 | INFO | fairseq.trainer | begin training epoch 64
2022-01-28 13:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:14:18 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.495 | ppl 721.74 | wps 7955.7 | wpb 2034.1 | bsz 4 | num_updates 4096 | best_loss 9.247
2022-01-28 13:14:18 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-28 13:14:18 | INFO | train | epoch 064 | loss 6.868 | ppl 116.83 | wps 5929.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4096 | lr 0.000494106 | gnorm 0.547 | train_wall 323 | gb_free 6.1 | wall 23043
KL Stats: Epoch 64 Divergences: Uniform: 2.707592549982253 Unigram: 2.7494868150669576
2022-01-28 13:14:18 | INFO | fairseq.trainer | begin training epoch 65
2022-01-28 13:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:14:38 | INFO | train_inner | epoch 065:      4 / 64 loss=6.895, ppl=119.04, wps=5800.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4100, lr=0.000493865, gnorm=0.545, train_wall=504, gb_free=6.1, wall=23063
2022-01-28 13:19:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:20:11 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.483 | ppl 715.63 | wps 7945 | wpb 2034.1 | bsz 4 | num_updates 4160 | best_loss 9.247
2022-01-28 13:20:11 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-28 13:20:11 | INFO | train | epoch 065 | loss 6.825 | ppl 113.35 | wps 5920.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4160 | lr 0.00049029 | gnorm 0.545 | train_wall 324 | gb_free 6.1 | wall 23395
KL Stats: Epoch 65 Divergences: Uniform: 2.7188952768108234 Unigram: 2.770402343866828
2022-01-28 13:20:11 | INFO | fairseq.trainer | begin training epoch 66
2022-01-28 13:20:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:23:35 | INFO | train_inner | epoch 066:     40 / 64 loss=6.8, ppl=111.46, wps=6091.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4200, lr=0.00048795, gnorm=0.547, train_wall=506, gb_free=6.1, wall=23600
2022-01-28 13:25:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:26:03 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.529 | ppl 738.57 | wps 7987.3 | wpb 2034.1 | bsz 4 | num_updates 4224 | best_loss 9.247
2022-01-28 13:26:03 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-28 13:26:03 | INFO | train | epoch 066 | loss 6.786 | ppl 110.33 | wps 5920.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4224 | lr 0.000486562 | gnorm 0.547 | train_wall 324 | gb_free 6.1 | wall 23748
KL Stats: Epoch 66 Divergences: Uniform: 2.736579817663126 Unigram: 2.7819889463745535
2022-01-28 13:26:03 | INFO | fairseq.trainer | begin training epoch 67
2022-01-28 13:26:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:31:56 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.466 | ppl 707.14 | wps 7961.1 | wpb 2034.1 | bsz 4 | num_updates 4288 | best_loss 9.247
2022-01-28 13:31:56 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-28 13:31:56 | INFO | train | epoch 067 | loss 6.745 | ppl 107.27 | wps 5922.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4288 | lr 0.000482917 | gnorm 0.552 | train_wall 323 | gb_free 6.1 | wall 24101
KL Stats: Epoch 67 Divergences: Uniform: 2.7536592450715465 Unigram: 2.80615889227131
2022-01-28 13:31:56 | INFO | fairseq.trainer | begin training epoch 68
2022-01-28 13:31:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:32:57 | INFO | train_inner | epoch 068:     12 / 64 loss=6.754, ppl=107.93, wps=5795.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4300, lr=0.000482243, gnorm=0.549, train_wall=505, gb_free=6.1, wall=24162
2022-01-28 13:37:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:37:48 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.541 | ppl 745.04 | wps 7969.4 | wpb 2034.1 | bsz 4 | num_updates 4352 | best_loss 9.247
2022-01-28 13:37:48 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-28 13:37:48 | INFO | train | epoch 068 | loss 6.707 | ppl 104.48 | wps 5925.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4352 | lr 0.000479353 | gnorm 0.567 | train_wall 323 | gb_free 6.1 | wall 24453
KL Stats: Epoch 68 Divergences: Uniform: 2.769608273275471 Unigram: 2.823921502139163
2022-01-28 13:37:48 | INFO | fairseq.trainer | begin training epoch 69
2022-01-28 13:37:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:41:53 | INFO | train_inner | epoch 069:     48 / 64 loss=6.689, ppl=103.19, wps=6094.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4400, lr=0.000476731, gnorm=0.56, train_wall=506, gb_free=6.1, wall=24698
2022-01-28 13:43:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:43:41 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.551 | ppl 750.08 | wps 7971.9 | wpb 2034.1 | bsz 4 | num_updates 4416 | best_loss 9.247
2022-01-28 13:43:41 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-28 13:43:41 | INFO | train | epoch 069 | loss 6.669 | ppl 101.78 | wps 5921.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4416 | lr 0.000475867 | gnorm 0.556 | train_wall 324 | gb_free 6.1 | wall 24806
KL Stats: Epoch 69 Divergences: Uniform: 2.7802756702534843 Unigram: 2.8366515489350514
2022-01-28 13:43:41 | INFO | fairseq.trainer | begin training epoch 70
2022-01-28 13:43:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 13:49:34 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.639 | ppl 797.48 | wps 8005.6 | wpb 2034.1 | bsz 4 | num_updates 4480 | best_loss 9.247
2022-01-28 13:49:34 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-28 13:49:34 | INFO | train | epoch 070 | loss 6.635 | ppl 99.36 | wps 5925.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4480 | lr 0.000472456 | gnorm 0.554 | train_wall 323 | gb_free 6.1 | wall 25159
KL Stats: Epoch 70 Divergences: Uniform: 2.783487492119848 Unigram: 2.8471096115434005
2022-01-28 13:49:34 | INFO | fairseq.trainer | begin training epoch 71
2022-01-28 13:49:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:51:16 | INFO | train_inner | epoch 071:     20 / 64 loss=6.629, ppl=98.99, wps=5799.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4500, lr=0.000471405, gnorm=0.563, train_wall=505, gb_free=6.1, wall=25260
2022-01-28 13:54:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:55:26 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.571 | ppl 760.82 | wps 7973.7 | wpb 2034.1 | bsz 4 | num_updates 4544 | best_loss 9.247
2022-01-28 13:55:26 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-28 13:55:26 | INFO | train | epoch 071 | loss 6.601 | ppl 97.07 | wps 5930.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4544 | lr 0.000469117 | gnorm 0.581 | train_wall 323 | gb_free 6.1 | wall 25511
KL Stats: Epoch 71 Divergences: Uniform: 2.7978977773965226 Unigram: 2.867257242249321
2022-01-28 13:55:26 | INFO | fairseq.trainer | begin training epoch 72
2022-01-28 13:55:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:00:12 | INFO | train_inner | epoch 072:     56 / 64 loss=6.586, ppl=96.07, wps=6094.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4600, lr=0.000466252, gnorm=0.579, train_wall=506, gb_free=6.1, wall=25797
2022-01-28 14:00:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:01:19 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.562 | ppl 756.09 | wps 7981.2 | wpb 2034.1 | bsz 4 | num_updates 4608 | best_loss 9.247
2022-01-28 14:01:19 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-01-28 14:01:19 | INFO | train | epoch 072 | loss 6.566 | ppl 94.76 | wps 5920.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4608 | lr 0.000465847 | gnorm 0.573 | train_wall 324 | gb_free 6.1 | wall 25864
KL Stats: Epoch 72 Divergences: Uniform: 2.815378693226333 Unigram: 2.8849706046590793
2022-01-28 14:01:19 | INFO | fairseq.trainer | begin training epoch 73
2022-01-28 14:01:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:06:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:07:11 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.657 | ppl 807.24 | wps 7978.1 | wpb 2034.1 | bsz 4 | num_updates 4672 | best_loss 9.247
2022-01-28 14:07:11 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-01-28 14:07:11 | INFO | train | epoch 073 | loss 6.533 | ppl 92.58 | wps 5928.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4672 | lr 0.000462646 | gnorm 0.562 | train_wall 323 | gb_free 6.1 | wall 26216
KL Stats: Epoch 73 Divergences: Uniform: 2.8123347743373417 Unigram: 2.8955931025402206
2022-01-28 14:07:11 | INFO | fairseq.trainer | begin training epoch 74
2022-01-28 14:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:09:34 | INFO | train_inner | epoch 074:     28 / 64 loss=6.522, ppl=91.88, wps=5802, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4700, lr=0.000461266, gnorm=0.564, train_wall=504, gb_free=6.1, wall=26358
2022-01-28 14:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:13:03 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.569 | ppl 759.39 | wps 7980.2 | wpb 2034.1 | bsz 4 | num_updates 4736 | best_loss 9.247
2022-01-28 14:13:03 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-01-28 14:13:03 | INFO | train | epoch 074 | loss 6.501 | ppl 90.6 | wps 5935.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4736 | lr 0.000459509 | gnorm 0.579 | train_wall 323 | gb_free 6.1 | wall 26568
KL Stats: Epoch 74 Divergences: Uniform: 2.8278323329928923 Unigram: 2.9194955334788264
2022-01-28 14:13:03 | INFO | fairseq.trainer | begin training epoch 75
2022-01-28 14:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:18:28 | INFO | train_inner | epoch 075:     64 / 64 loss=6.492, ppl=90.02, wps=6103.7, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=4800, lr=0.000456435, gnorm=0.581, train_wall=504, gb_free=6.1, wall=26893
2022-01-28 14:18:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:18:55 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.745 | ppl 858.23 | wps 7956.7 | wpb 2034.1 | bsz 4 | num_updates 4800 | best_loss 9.247
2022-01-28 14:18:55 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-01-28 14:18:55 | INFO | train | epoch 075 | loss 6.472 | ppl 88.74 | wps 5929.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4800 | lr 0.000456435 | gnorm 0.579 | train_wall 323 | gb_free 6.1 | wall 26920
KL Stats: Epoch 75 Divergences: Uniform: 2.829393900080993 Unigram: 2.931276286172168
2022-01-28 14:18:55 | INFO | fairseq.trainer | begin training epoch 76
2022-01-28 14:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:24:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:24:47 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.673 | ppl 816.12 | wps 7956.4 | wpb 2034.1 | bsz 4 | num_updates 4864 | best_loss 9.247
2022-01-28 14:24:47 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-01-28 14:24:47 | INFO | train | epoch 076 | loss 6.442 | ppl 86.94 | wps 5932.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.589 | train_wall 323 | gb_free 6.1 | wall 27272
KL Stats: Epoch 76 Divergences: Uniform: 2.8426823078448433 Unigram: 2.949531626500292
2022-01-28 14:24:47 | INFO | fairseq.trainer | begin training epoch 77
2022-01-28 14:24:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:27:51 | INFO | train_inner | epoch 077:     36 / 64 loss=6.417, ppl=85.47, wps=5803.3, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=4900, lr=0.000451754, gnorm=0.588, train_wall=505, gb_free=6.1, wall=27456
2022-01-28 14:30:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:30:40 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.704 | ppl 833.79 | wps 7956.7 | wpb 2034.1 | bsz 4 | num_updates 4928 | best_loss 9.247
2022-01-28 14:30:40 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-01-28 14:30:40 | INFO | train | epoch 077 | loss 6.413 | ppl 85.21 | wps 5926.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4928 | lr 0.000450469 | gnorm 0.589 | train_wall 323 | gb_free 6.1 | wall 27625
KL Stats: Epoch 77 Divergences: Uniform: 2.8442787828839093 Unigram: 2.966262071332665
2022-01-28 14:30:40 | INFO | fairseq.trainer | begin training epoch 78
2022-01-28 14:30:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:36:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:36:32 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.651 | ppl 804.11 | wps 7964.4 | wpb 2034.1 | bsz 4 | num_updates 4992 | best_loss 9.247
2022-01-28 14:36:32 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-01-28 14:36:32 | INFO | train | epoch 078 | loss 6.384 | ppl 83.52 | wps 5932.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4992 | lr 0.000447572 | gnorm 0.588 | train_wall 323 | gb_free 6.1 | wall 27977
KL Stats: Epoch 78 Divergences: Uniform: 2.860074531963228 Unigram: 2.9778245575841775
2022-01-28 14:36:32 | INFO | fairseq.trainer | begin training epoch 79
2022-01-28 14:36:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:37:12 | INFO | train_inner | epoch 079:      8 / 64 loss=6.399, ppl=84.39, wps=5803.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5000, lr=0.000447214, gnorm=0.59, train_wall=504, gb_free=6.1, wall=28017
2022-01-28 14:41:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:42:24 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.745 | ppl 858.06 | wps 7970.2 | wpb 2034.1 | bsz 4 | num_updates 5056 | best_loss 9.247
2022-01-28 14:42:24 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-01-28 14:42:24 | INFO | train | epoch 079 | loss 6.354 | ppl 81.78 | wps 5923.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5056 | lr 0.00044473 | gnorm 0.583 | train_wall 323 | gb_free 6.1 | wall 28329
KL Stats: Epoch 79 Divergences: Uniform: 2.859760517657094 Unigram: 2.9870573222451986
2022-01-28 14:42:24 | INFO | fairseq.trainer | begin training epoch 80
2022-01-28 14:42:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:46:08 | INFO | train_inner | epoch 080:     44 / 64 loss=6.338, ppl=80.88, wps=6096.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5100, lr=0.000442807, gnorm=0.583, train_wall=506, gb_free=6.1, wall=28553
2022-01-28 14:47:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 14:48:16 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.682 | ppl 821.57 | wps 8004.3 | wpb 2034.1 | bsz 4 | num_updates 5120 | best_loss 9.247
2022-01-28 14:48:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5120 updates
2022-01-28 14:48:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint80.pt
2022-01-28 14:48:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint80.pt
2022-01-28 14:48:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint80.pt (epoch 80 @ 5120 updates, score 9.682) (writing took 3.294665423221886 seconds)
2022-01-28 14:48:20 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-01-28 14:48:20 | INFO | train | epoch 080 | loss 6.329 | ppl 80.38 | wps 5873.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5120 | lr 0.000441942 | gnorm 0.59 | train_wall 323 | gb_free 6.1 | wall 28685
KL Stats: Epoch 80 Divergences: Uniform: 2.870236872991773 Unigram: 3.0051293097968848
2022-01-28 14:48:20 | INFO | fairseq.trainer | begin training epoch 81
2022-01-28 14:48:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:54:12 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.736 | ppl 852.56 | wps 7965.2 | wpb 2034.1 | bsz 4 | num_updates 5184 | best_loss 9.247
2022-01-28 14:54:12 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-01-28 14:54:12 | INFO | train | epoch 081 | loss 6.303 | ppl 78.97 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5184 | lr 0.000439205 | gnorm 0.609 | train_wall 323 | gb_free 6.1 | wall 29037
KL Stats: Epoch 81 Divergences: Uniform: 2.8862520826164477 Unigram: 3.02248710893918
2022-01-28 14:54:12 | INFO | fairseq.trainer | begin training epoch 82
2022-01-28 14:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:55:33 | INFO | train_inner | epoch 082:     16 / 64 loss=6.31, ppl=79.32, wps=5774.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5200, lr=0.000438529, gnorm=0.611, train_wall=504, gb_free=6.1, wall=29118
2022-01-28 14:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:00:04 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.703 | ppl 833.68 | wps 7999.5 | wpb 2034.1 | bsz 4 | num_updates 5248 | best_loss 9.247
2022-01-28 15:00:04 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-01-28 15:00:04 | INFO | train | epoch 082 | loss 6.277 | ppl 77.56 | wps 5934 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5248 | lr 0.000436519 | gnorm 0.6 | train_wall 323 | gb_free 6.1 | wall 29389
KL Stats: Epoch 82 Divergences: Uniform: 2.895997605004207 Unigram: 3.043431758245679
2022-01-28 15:00:04 | INFO | fairseq.trainer | begin training epoch 83
2022-01-28 15:00:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:04:29 | INFO | train_inner | epoch 083:     52 / 64 loss=6.263, ppl=76.82, wps=6097.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5300, lr=0.000434372, gnorm=0.608, train_wall=506, gb_free=6.1, wall=29654
2022-01-28 15:05:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:05:57 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.623 | ppl 788.26 | wps 7961.4 | wpb 2034.1 | bsz 4 | num_updates 5312 | best_loss 9.247
2022-01-28 15:05:57 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-01-28 15:05:57 | INFO | train | epoch 083 | loss 6.254 | ppl 76.32 | wps 5918.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5312 | lr 0.000433881 | gnorm 0.625 | train_wall 324 | gb_free 6.1 | wall 29741
KL Stats: Epoch 83 Divergences: Uniform: 2.9003080671964834 Unigram: 3.051369467894871
2022-01-28 15:05:57 | INFO | fairseq.trainer | begin training epoch 84
2022-01-28 15:05:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:11:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:11:49 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.711 | ppl 837.96 | wps 7967.2 | wpb 2034.1 | bsz 4 | num_updates 5376 | best_loss 9.247
2022-01-28 15:11:49 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-01-28 15:11:49 | INFO | train | epoch 084 | loss 6.227 | ppl 74.93 | wps 5929.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5376 | lr 0.000431291 | gnorm 0.602 | train_wall 323 | gb_free 6.1 | wall 30094
KL Stats: Epoch 84 Divergences: Uniform: 2.9059415356720284 Unigram: 3.066888159346803
2022-01-28 15:11:49 | INFO | fairseq.trainer | begin training epoch 85
2022-01-28 15:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:13:51 | INFO | train_inner | epoch 085:     24 / 64 loss=6.217, ppl=74.4, wps=5801.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5400, lr=0.000430331, gnorm=0.606, train_wall=504, gb_free=6.1, wall=30216
2022-01-28 15:17:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:17:41 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.75 | ppl 860.85 | wps 7972.6 | wpb 2034.1 | bsz 4 | num_updates 5440 | best_loss 9.247
2022-01-28 15:17:41 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-01-28 15:17:41 | INFO | train | epoch 085 | loss 6.204 | ppl 73.72 | wps 5927 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5440 | lr 0.000428746 | gnorm 0.611 | train_wall 323 | gb_free 6.1 | wall 30446
KL Stats: Epoch 85 Divergences: Uniform: 2.9215983692614493 Unigram: 3.071539362642006
2022-01-28 15:17:41 | INFO | fairseq.trainer | begin training epoch 86
2022-01-28 15:17:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:22:47 | INFO | train_inner | epoch 086:     60 / 64 loss=6.202, ppl=73.61, wps=6098.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5500, lr=0.000426401, gnorm=0.62, train_wall=506, gb_free=6.1, wall=30752
2022-01-28 15:23:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 15:23:34 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.745 | ppl 857.91 | wps 7959.7 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 9.247
2022-01-28 15:23:34 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-01-28 15:23:34 | INFO | train | epoch 086 | loss 6.181 | ppl 72.56 | wps 5926.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5504 | lr 0.000426246 | gnorm 0.627 | train_wall 323 | gb_free 6.1 | wall 30799
KL Stats: Epoch 86 Divergences: Uniform: 2.917900749910052 Unigram: 3.092534819445079
2022-01-28 15:23:34 | INFO | fairseq.trainer | begin training epoch 87
2022-01-28 15:23:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:28:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:29:26 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.726 | ppl 846.91 | wps 7956.3 | wpb 2034.1 | bsz 4 | num_updates 5568 | best_loss 9.247
2022-01-28 15:29:26 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-01-28 15:29:26 | INFO | train | epoch 087 | loss 6.16 | ppl 71.49 | wps 5923.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5568 | lr 0.00042379 | gnorm 0.647 | train_wall 323 | gb_free 6.1 | wall 31151
KL Stats: Epoch 87 Divergences: Uniform: 2.9271323726381455 Unigram: 3.1082236159392798
2022-01-28 15:29:26 | INFO | fairseq.trainer | begin training epoch 88
2022-01-28 15:29:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:32:09 | INFO | train_inner | epoch 088:     32 / 64 loss=6.146, ppl=70.82, wps=5795.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5600, lr=0.000422577, gnorm=0.642, train_wall=505, gb_free=6.1, wall=31314
2022-01-28 15:34:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:35:19 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.742 | ppl 856.46 | wps 8003.5 | wpb 2034.1 | bsz 4 | num_updates 5632 | best_loss 9.247
2022-01-28 15:35:19 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-01-28 15:35:19 | INFO | train | epoch 088 | loss 6.136 | ppl 70.33 | wps 5924.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5632 | lr 0.000421375 | gnorm 0.636 | train_wall 323 | gb_free 6.1 | wall 31504
KL Stats: Epoch 88 Divergences: Uniform: 2.929668886948309 Unigram: 3.1184501530486
2022-01-28 15:35:19 | INFO | fairseq.trainer | begin training epoch 89
2022-01-28 15:35:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:40:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:41:11 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.758 | ppl 866.08 | wps 7971.2 | wpb 2034.1 | bsz 4 | num_updates 5696 | best_loss 9.247
2022-01-28 15:41:11 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-01-28 15:41:11 | INFO | train | epoch 089 | loss 6.118 | ppl 69.44 | wps 5933.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5696 | lr 0.000419001 | gnorm 0.65 | train_wall 323 | gb_free 6.1 | wall 31856
KL Stats: Epoch 89 Divergences: Uniform: 2.939248973932787 Unigram: 3.127234343470757
2022-01-28 15:41:11 | INFO | fairseq.trainer | begin training epoch 90
2022-01-28 15:41:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:41:31 | INFO | train_inner | epoch 090:      4 / 64 loss=6.128, ppl=69.96, wps=5804.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5700, lr=0.000418854, gnorm=0.649, train_wall=504, gb_free=6.1, wall=31876
2022-01-28 15:46:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:47:03 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.8 | ppl 891.24 | wps 7966.7 | wpb 2034.1 | bsz 4 | num_updates 5760 | best_loss 9.247
2022-01-28 15:47:03 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-01-28 15:47:03 | INFO | train | epoch 090 | loss 6.095 | ppl 68.36 | wps 5932.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5760 | lr 0.000416667 | gnorm 0.628 | train_wall 323 | gb_free 6.1 | wall 32208
KL Stats: Epoch 90 Divergences: Uniform: 2.9419329482822274 Unigram: 3.1372350177604043
2022-01-28 15:47:03 | INFO | fairseq.trainer | begin training epoch 91
2022-01-28 15:47:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:50:26 | INFO | train_inner | epoch 091:     40 / 64 loss=6.077, ppl=67.5, wps=6104, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5800, lr=0.000415227, gnorm=0.625, train_wall=505, gb_free=6.1, wall=32411
2022-01-28 15:52:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:52:55 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.816 | ppl 901.09 | wps 7956.5 | wpb 2034.1 | bsz 4 | num_updates 5824 | best_loss 9.247
2022-01-28 15:52:55 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-01-28 15:52:55 | INFO | train | epoch 091 | loss 6.073 | ppl 67.32 | wps 5930.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5824 | lr 0.000414371 | gnorm 0.632 | train_wall 323 | gb_free 6.1 | wall 32560
KL Stats: Epoch 91 Divergences: Uniform: 2.9508154938287645 Unigram: 3.1594974319510496
2022-01-28 15:52:55 | INFO | fairseq.trainer | begin training epoch 92
2022-01-28 15:52:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:58:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:58:47 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.807 | ppl 895.79 | wps 8001.4 | wpb 2034.1 | bsz 4 | num_updates 5888 | best_loss 9.247
2022-01-28 15:58:47 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-01-28 15:58:47 | INFO | train | epoch 092 | loss 6.054 | ppl 66.45 | wps 5930.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5888 | lr 0.000412113 | gnorm 0.655 | train_wall 323 | gb_free 6.1 | wall 32912
KL Stats: Epoch 92 Divergences: Uniform: 2.9615668571106446 Unigram: 3.164969831162752
2022-01-28 15:58:47 | INFO | fairseq.trainer | begin training epoch 93
2022-01-28 15:58:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:59:48 | INFO | train_inner | epoch 093:     12 / 64 loss=6.064, ppl=66.88, wps=5803.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5900, lr=0.000411693, gnorm=0.649, train_wall=504, gb_free=6.1, wall=32973
2022-01-28 16:04:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 16:04:40 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.788 | ppl 884.35 | wps 7963 | wpb 2034.1 | bsz 4 | num_updates 5952 | best_loss 9.247
2022-01-28 16:04:40 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-01-28 16:04:40 | INFO | train | epoch 093 | loss 6.037 | ppl 65.65 | wps 5925.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5952 | lr 0.000409891 | gnorm 0.65 | train_wall 323 | gb_free 6.1 | wall 33265
KL Stats: Epoch 93 Divergences: Uniform: 2.964761796232546 Unigram: 3.177550069713296
2022-01-28 16:04:40 | INFO | fairseq.trainer | begin training epoch 94
2022-01-28 16:04:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:08:44 | INFO | train_inner | epoch 094:     48 / 64 loss=6.022, ppl=65, wps=6095.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6000, lr=0.000408248, gnorm=0.652, train_wall=506, gb_free=6.1, wall=33509
2022-01-28 16:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:10:32 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.813 | ppl 899.51 | wps 7976 | wpb 2034.1 | bsz 4 | num_updates 6016 | best_loss 9.247
2022-01-28 16:10:32 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-01-28 16:10:32 | INFO | train | epoch 094 | loss 6.015 | ppl 64.68 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6016 | lr 0.000407705 | gnorm 0.656 | train_wall 323 | gb_free 6.1 | wall 33617
KL Stats: Epoch 94 Divergences: Uniform: 2.9683277381561726 Unigram: 3.1957400667365796
2022-01-28 16:10:32 | INFO | fairseq.trainer | begin training epoch 95
2022-01-28 16:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:16:25 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.887 | ppl 946.64 | wps 7970.8 | wpb 2034.1 | bsz 4 | num_updates 6080 | best_loss 9.247
2022-01-28 16:16:25 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-01-28 16:16:25 | INFO | train | epoch 095 | loss 5.997 | ppl 63.87 | wps 5918.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6080 | lr 0.000405554 | gnorm 0.651 | train_wall 324 | gb_free 6.1 | wall 33970
KL Stats: Epoch 95 Divergences: Uniform: 2.9712666777362307 Unigram: 3.205259640270797
2022-01-28 16:16:25 | INFO | fairseq.trainer | begin training epoch 96
2022-01-28 16:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:18:07 | INFO | train_inner | epoch 096:     20 / 64 loss=5.996, ppl=63.82, wps=5794.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6100, lr=0.000404888, gnorm=0.657, train_wall=505, gb_free=6.1, wall=34072
2022-01-28 16:21:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:22:17 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 9.797 | ppl 889.83 | wps 7990.9 | wpb 2034.1 | bsz 4 | num_updates 6144 | best_loss 9.247
2022-01-28 16:22:17 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-01-28 16:22:17 | INFO | train | epoch 096 | loss 5.98 | ppl 63.14 | wps 5928.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6144 | lr 0.000403436 | gnorm 0.67 | train_wall 323 | gb_free 6.1 | wall 34322
KL Stats: Epoch 96 Divergences: Uniform: 2.9787591783089264 Unigram: 3.2127459601619432
2022-01-28 16:22:17 | INFO | fairseq.trainer | begin training epoch 97
2022-01-28 16:22:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:27:03 | INFO | train_inner | epoch 097:     56 / 64 loss=5.975, ppl=62.9, wps=6092.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=6200, lr=0.00040161, gnorm=0.678, train_wall=506, gb_free=6.1, wall=34608
2022-01-28 16:27:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 16:28:10 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 9.899 | ppl 954.55 | wps 7967.6 | wpb 2034.1 | bsz 4 | num_updates 6208 | best_loss 9.247
2022-01-28 16:28:10 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-01-28 16:28:10 | INFO | train | epoch 097 | loss 5.963 | ppl 62.4 | wps 5915.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6208 | lr 0.000401351 | gnorm 0.682 | train_wall 324 | gb_free 6.1 | wall 34675
KL Stats: Epoch 97 Divergences: Uniform: 2.985536746115246 Unigram: 3.2277458739502927
2022-01-28 16:28:10 | INFO | fairseq.trainer | begin training epoch 98
2022-01-28 16:28:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:34:03 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 9.875 | ppl 938.74 | wps 7969.6 | wpb 2034.1 | bsz 4 | num_updates 6272 | best_loss 9.247
2022-01-28 16:34:03 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-01-28 16:34:03 | INFO | train | epoch 098 | loss 5.944 | ppl 61.56 | wps 5916.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6272 | lr 0.000399298 | gnorm 0.686 | train_wall 324 | gb_free 6.1 | wall 35028
KL Stats: Epoch 98 Divergences: Uniform: 2.988820434727777 Unigram: 3.232075247931787
2022-01-28 16:34:03 | INFO | fairseq.trainer | begin training epoch 99
2022-01-28 16:34:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:36:26 | INFO | train_inner | epoch 099:     28 / 64 loss=5.934, ppl=61.15, wps=5791.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=6300, lr=0.00039841, gnorm=0.681, train_wall=505, gb_free=6.1, wall=35171
2022-01-28 16:39:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 16:39:56 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 9.822 | ppl 905.26 | wps 7926.8 | wpb 2034.1 | bsz 4 | num_updates 6336 | best_loss 9.247
2022-01-28 16:39:56 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-01-28 16:39:56 | INFO | train | epoch 099 | loss 5.924 | ppl 60.71 | wps 5917.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6336 | lr 0.000397276 | gnorm 0.677 | train_wall 324 | gb_free 6.1 | wall 35381
KL Stats: Epoch 99 Divergences: Uniform: 2.996671880132825 Unigram: 3.2518185935335797
2022-01-28 16:39:56 | INFO | fairseq.trainer | begin training epoch 100
2022-01-28 16:39:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:45:22 | INFO | train_inner | epoch 100:     64 / 64 loss=5.929, ppl=60.93, wps=6090.2, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=6400, lr=0.000395285, gnorm=0.701, train_wall=505, gb_free=6.1, wall=35706
2022-01-28 16:45:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:45:49 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 9.829 | ppl 909.44 | wps 7970.7 | wpb 2034.1 | bsz 4 | num_updates 6400 | best_loss 9.247
2022-01-28 16:45:49 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-01-28 16:45:49 | INFO | train | epoch 100 | loss 5.913 | ppl 60.26 | wps 5923 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6400 | lr 0.000395285 | gnorm 0.709 | train_wall 323 | gb_free 6.1 | wall 35734
KL Stats: Epoch 100 Divergences: Uniform: 2.997190563085076 Unigram: 3.259376627680359
2022-01-28 16:45:49 | INFO | fairseq.trainer | begin training epoch 101
2022-01-28 16:45:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:51:42 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 9.858 | ppl 927.94 | wps 7969.6 | wpb 2034.1 | bsz 4 | num_updates 6464 | best_loss 9.247
2022-01-28 16:51:42 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-01-28 16:51:42 | INFO | train | epoch 101 | loss 5.892 | ppl 59.38 | wps 5922.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6464 | lr 0.000393323 | gnorm 0.699 | train_wall 324 | gb_free 6.1 | wall 36087
KL Stats: Epoch 101 Divergences: Uniform: 3.010668493724642 Unigram: 3.2763632129046107
2022-01-28 16:51:42 | INFO | fairseq.trainer | begin training epoch 102
2022-01-28 16:51:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:54:45 | INFO | train_inner | epoch 102:     36 / 64 loss=5.877, ppl=58.79, wps=5797.8, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=6500, lr=0.000392232, gnorm=0.703, train_wall=506, gb_free=6.1, wall=36270
2022-01-28 16:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:57:34 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 9.883 | ppl 944.39 | wps 7954.5 | wpb 2034.1 | bsz 4 | num_updates 6528 | best_loss 9.247
2022-01-28 16:57:34 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-01-28 16:57:34 | INFO | train | epoch 102 | loss 5.877 | ppl 58.79 | wps 5922.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6528 | lr 0.00039139 | gnorm 0.713 | train_wall 323 | gb_free 6.1 | wall 36439
KL Stats: Epoch 102 Divergences: Uniform: 3.0089978245804163 Unigram: 3.2818860406617403
2022-01-28 16:57:34 | INFO | fairseq.trainer | begin training epoch 103
2022-01-28 16:57:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:03:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:03:27 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 9.876 | ppl 939.86 | wps 7996.9 | wpb 2034.1 | bsz 4 | num_updates 6592 | best_loss 9.247
2022-01-28 17:03:27 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-01-28 17:03:27 | INFO | train | epoch 103 | loss 5.86 | ppl 58.06 | wps 5923.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6592 | lr 0.000389486 | gnorm 0.693 | train_wall 324 | gb_free 6.1 | wall 36792
KL Stats: Epoch 103 Divergences: Uniform: 3.018506600811277 Unigram: 3.2948820655524758
2022-01-28 17:03:27 | INFO | fairseq.trainer | begin training epoch 104
2022-01-28 17:03:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:04:08 | INFO | train_inner | epoch 104:      8 / 64 loss=5.868, ppl=58.39, wps=5795.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6600, lr=0.000389249, gnorm=0.702, train_wall=505, gb_free=6.1, wall=36833
2022-01-28 17:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:09:20 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 9.884 | ppl 944.65 | wps 7982.5 | wpb 2034.1 | bsz 4 | num_updates 6656 | best_loss 9.247
2022-01-28 17:09:20 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-01-28 17:09:20 | INFO | train | epoch 104 | loss 5.848 | ppl 57.58 | wps 5921.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6656 | lr 0.000387609 | gnorm 0.715 | train_wall 324 | gb_free 6.1 | wall 37145
KL Stats: Epoch 104 Divergences: Uniform: 3.016431790481818 Unigram: 3.3058545248697877
2022-01-28 17:09:20 | INFO | fairseq.trainer | begin training epoch 105
2022-01-28 17:09:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:13:04 | INFO | train_inner | epoch 105:     44 / 64 loss=5.835, ppl=57.07, wps=6098.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6700, lr=0.000386334, gnorm=0.712, train_wall=506, gb_free=6.1, wall=37369
2022-01-28 17:14:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:15:12 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 9.85 | ppl 923.01 | wps 7970.7 | wpb 2034.1 | bsz 4 | num_updates 6720 | best_loss 9.247
2022-01-28 17:15:12 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-01-28 17:15:12 | INFO | train | epoch 105 | loss 5.831 | ppl 56.91 | wps 5933.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6720 | lr 0.000385758 | gnorm 0.712 | train_wall 323 | gb_free 6.1 | wall 37497
KL Stats: Epoch 105 Divergences: Uniform: 3.019785041021761 Unigram: 3.3119131406571567
2022-01-28 17:15:12 | INFO | fairseq.trainer | begin training epoch 106
2022-01-28 17:15:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:20:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:21:04 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 9.875 | ppl 939.14 | wps 8002.2 | wpb 2034.1 | bsz 4 | num_updates 6784 | best_loss 9.247
2022-01-28 17:21:04 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-01-28 17:21:04 | INFO | train | epoch 106 | loss 5.816 | ppl 56.32 | wps 5931.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6784 | lr 0.000383934 | gnorm 0.728 | train_wall 323 | gb_free 6.1 | wall 37849
KL Stats: Epoch 106 Divergences: Uniform: 3.0217152702189316 Unigram: 3.320495279852253
2022-01-28 17:21:04 | INFO | fairseq.trainer | begin training epoch 107
2022-01-28 17:21:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:22:25 | INFO | train_inner | epoch 107:     16 / 64 loss=5.819, ppl=56.44, wps=5805, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6800, lr=0.000383482, gnorm=0.725, train_wall=504, gb_free=6.1, wall=37930
2022-01-28 17:26:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:26:56 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 9.907 | ppl 959.94 | wps 7971.2 | wpb 2034.1 | bsz 4 | num_updates 6848 | best_loss 9.247
2022-01-28 17:26:56 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-01-28 17:26:56 | INFO | train | epoch 107 | loss 5.799 | ppl 55.69 | wps 5931.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6848 | lr 0.000382136 | gnorm 0.709 | train_wall 323 | gb_free 6.1 | wall 38201
KL Stats: Epoch 107 Divergences: Uniform: 3.0310486252975632 Unigram: 3.3362078452925124
2022-01-28 17:26:56 | INFO | fairseq.trainer | begin training epoch 108
2022-01-28 17:26:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:31:21 | INFO | train_inner | epoch 108:     52 / 64 loss=5.794, ppl=55.5, wps=6104.1, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=6900, lr=0.000380693, gnorm=0.711, train_wall=505, gb_free=6.1, wall=38466
2022-01-28 17:32:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:32:48 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 9.897 | ppl 953.24 | wps 7988.8 | wpb 2034.1 | bsz 4 | num_updates 6912 | best_loss 9.247
2022-01-28 17:32:48 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-01-28 17:32:48 | INFO | train | epoch 108 | loss 5.788 | ppl 55.25 | wps 5932.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6912 | lr 0.000380363 | gnorm 0.725 | train_wall 323 | gb_free 6.1 | wall 38553
KL Stats: Epoch 108 Divergences: Uniform: 3.0333264112155067 Unigram: 3.3435715234284924
2022-01-28 17:32:48 | INFO | fairseq.trainer | begin training epoch 109
2022-01-28 17:32:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:38:40 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 9.906 | ppl 959.69 | wps 7954.6 | wpb 2034.1 | bsz 4 | num_updates 6976 | best_loss 9.247
2022-01-28 17:38:40 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-01-28 17:38:40 | INFO | train | epoch 109 | loss 5.772 | ppl 54.66 | wps 5927.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6976 | lr 0.000378614 | gnorm 0.73 | train_wall 323 | gb_free 6.1 | wall 38905
KL Stats: Epoch 109 Divergences: Uniform: 3.037703618440319 Unigram: 3.353742749543916
2022-01-28 17:38:40 | INFO | fairseq.trainer | begin training epoch 110
2022-01-28 17:38:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:40:42 | INFO | train_inner | epoch 110:     24 / 64 loss=5.767, ppl=54.46, wps=5801.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7000, lr=0.000377964, gnorm=0.738, train_wall=504, gb_free=6.1, wall=39027
2022-01-28 17:44:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:44:32 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 9.932 | ppl 976.57 | wps 7981.3 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 9.247
2022-01-28 17:44:32 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-01-28 17:44:32 | INFO | train | epoch 110 | loss 5.759 | ppl 54.14 | wps 5933.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.731 | train_wall 323 | gb_free 6.1 | wall 39257
KL Stats: Epoch 110 Divergences: Uniform: 3.043035331959731 Unigram: 3.3664374989612442
2022-01-28 17:44:32 | INFO | fairseq.trainer | begin training epoch 111
2022-01-28 17:44:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:49:38 | INFO | train_inner | epoch 111:     60 / 64 loss=5.759, ppl=54.14, wps=6104.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7100, lr=0.000375293, gnorm=0.738, train_wall=505, gb_free=6.1, wall=39563
2022-01-28 17:49:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:50:24 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 9.945 | ppl 985.44 | wps 7964.1 | wpb 2034.1 | bsz 4 | num_updates 7104 | best_loss 9.247
2022-01-28 17:50:24 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-01-28 17:50:24 | INFO | train | epoch 111 | loss 5.747 | ppl 53.7 | wps 5931 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7104 | lr 0.000375188 | gnorm 0.749 | train_wall 323 | gb_free 6.1 | wall 39609
KL Stats: Epoch 111 Divergences: Uniform: 3.0440846737151777 Unigram: 3.3786198257923763
2022-01-28 17:50:24 | INFO | fairseq.trainer | begin training epoch 112
2022-01-28 17:50:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:55:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:56:16 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 9.951 | ppl 989.96 | wps 8002.1 | wpb 2034.1 | bsz 4 | num_updates 7168 | best_loss 9.247
2022-01-28 17:56:16 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-01-28 17:56:16 | INFO | train | epoch 112 | loss 5.732 | ppl 53.16 | wps 5934 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7168 | lr 0.000373509 | gnorm 0.746 | train_wall 323 | gb_free 6.1 | wall 39961
KL Stats: Epoch 112 Divergences: Uniform: 3.0466581053380377 Unigram: 3.38623514547911
2022-01-28 17:56:16 | INFO | fairseq.trainer | begin training epoch 113
2022-01-28 17:56:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:58:59 | INFO | train_inner | epoch 113:     32 / 64 loss=5.72, ppl=52.73, wps=5808.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7200, lr=0.000372678, gnorm=0.745, train_wall=504, gb_free=6.1, wall=40124
2022-01-28 18:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:02:08 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 9.949 | ppl 988.11 | wps 7968.5 | wpb 2034.1 | bsz 4 | num_updates 7232 | best_loss 9.247
2022-01-28 18:02:08 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-01-28 18:02:08 | INFO | train | epoch 113 | loss 5.717 | ppl 52.6 | wps 5933.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7232 | lr 0.000371853 | gnorm 0.744 | train_wall 323 | gb_free 6.1 | wall 40313
KL Stats: Epoch 113 Divergences: Uniform: 3.0588745457090063 Unigram: 3.3940507882021294
2022-01-28 18:02:08 | INFO | fairseq.trainer | begin training epoch 114
2022-01-28 18:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:08:01 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 9.969 | ppl 1002.51 | wps 7991.8 | wpb 2034.1 | bsz 4 | num_updates 7296 | best_loss 9.247
2022-01-28 18:08:01 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-01-28 18:08:01 | INFO | train | epoch 114 | loss 5.704 | ppl 52.15 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7296 | lr 0.000370218 | gnorm 0.746 | train_wall 323 | gb_free 6.1 | wall 40666
KL Stats: Epoch 114 Divergences: Uniform: 3.0566848532866753 Unigram: 3.4021889186677963
2022-01-28 18:08:01 | INFO | fairseq.trainer | begin training epoch 115
2022-01-28 18:08:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:08:21 | INFO | train_inner | epoch 115:      4 / 64 loss=5.716, ppl=52.58, wps=5798.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7300, lr=0.000370117, gnorm=0.746, train_wall=505, gb_free=6.1, wall=40686
2022-01-28 18:13:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:13:53 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 9.972 | ppl 1004.42 | wps 7958.6 | wpb 2034.1 | bsz 4 | num_updates 7360 | best_loss 9.247
2022-01-28 18:13:53 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-01-28 18:13:53 | INFO | train | epoch 115 | loss 5.693 | ppl 51.72 | wps 5933.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7360 | lr 0.000368605 | gnorm 0.751 | train_wall 323 | gb_free 6.1 | wall 41018
KL Stats: Epoch 115 Divergences: Uniform: 3.0578477284244787 Unigram: 3.409242051154626
2022-01-28 18:13:53 | INFO | fairseq.trainer | begin training epoch 116
2022-01-28 18:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:17:17 | INFO | train_inner | epoch 116:     40 / 64 loss=5.679, ppl=51.25, wps=6102.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7400, lr=0.000367607, gnorm=0.75, train_wall=505, gb_free=6.1, wall=41222
2022-01-28 18:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:19:45 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 9.972 | ppl 1004.52 | wps 7994.6 | wpb 2034.1 | bsz 4 | num_updates 7424 | best_loss 9.247
2022-01-28 18:19:45 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-01-28 18:19:45 | INFO | train | epoch 116 | loss 5.68 | ppl 51.28 | wps 5926.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7424 | lr 0.000367013 | gnorm 0.761 | train_wall 323 | gb_free 6.1 | wall 41370
KL Stats: Epoch 116 Divergences: Uniform: 3.0618368263938827 Unigram: 3.411325968163732
2022-01-28 18:19:45 | INFO | fairseq.trainer | begin training epoch 117
2022-01-28 18:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:25:37 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 9.999 | ppl 1023.36 | wps 7978.2 | wpb 2034.1 | bsz 4 | num_updates 7488 | best_loss 9.247
2022-01-28 18:25:37 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-01-28 18:25:37 | INFO | train | epoch 117 | loss 5.668 | ppl 50.85 | wps 5932.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7488 | lr 0.000365441 | gnorm 0.773 | train_wall 323 | gb_free 6.1 | wall 41722
KL Stats: Epoch 117 Divergences: Uniform: 3.0612969957242475 Unigram: 3.42901738738675
2022-01-28 18:25:37 | INFO | fairseq.trainer | begin training epoch 118
2022-01-28 18:25:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:26:38 | INFO | train_inner | epoch 118:     12 / 64 loss=5.674, ppl=51.04, wps=5804.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7500, lr=0.000365148, gnorm=0.779, train_wall=504, gb_free=6.1, wall=41783
2022-01-28 18:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:31:29 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.016 | ppl 1035.75 | wps 7991.3 | wpb 2034.1 | bsz 4 | num_updates 7552 | best_loss 9.247
2022-01-28 18:31:29 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-01-28 18:31:29 | INFO | train | epoch 118 | loss 5.656 | ppl 50.43 | wps 5931.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7552 | lr 0.000363889 | gnorm 0.782 | train_wall 323 | gb_free 6.1 | wall 42074
KL Stats: Epoch 118 Divergences: Uniform: 3.072941074464462 Unigram: 3.4393594274508175
2022-01-28 18:31:29 | INFO | fairseq.trainer | begin training epoch 119
2022-01-28 18:31:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:35:34 | INFO | train_inner | epoch 119:     48 / 64 loss=5.646, ppl=50.07, wps=6106.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7600, lr=0.000362738, gnorm=0.773, train_wall=505, gb_free=6.1, wall=42319
2022-01-28 18:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 18:37:21 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 9.963 | ppl 997.85 | wps 7991 | wpb 2034.1 | bsz 4 | num_updates 7616 | best_loss 9.247
2022-01-28 18:37:21 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-01-28 18:37:21 | INFO | train | epoch 119 | loss 5.643 | ppl 49.97 | wps 5939.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7616 | lr 0.000362357 | gnorm 0.769 | train_wall 323 | gb_free 6.1 | wall 42426
KL Stats: Epoch 119 Divergences: Uniform: 3.072187988376067 Unigram: 3.4484198295535693
2022-01-28 18:37:21 | INFO | fairseq.trainer | begin training epoch 120
2022-01-28 18:37:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:42:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:43:13 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 9.99 | ppl 1016.84 | wps 7977.9 | wpb 2034.1 | bsz 4 | num_updates 7680 | best_loss 9.247
2022-01-28 18:43:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7680 updates
2022-01-28 18:43:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint120.pt
2022-01-28 18:43:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint120.pt
2022-01-28 18:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint120.pt (epoch 120 @ 7680 updates, score 9.99) (writing took 3.083498790860176 seconds)
2022-01-28 18:43:16 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-01-28 18:43:16 | INFO | train | epoch 120 | loss 5.634 | ppl 49.68 | wps 5882.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7680 | lr 0.000360844 | gnorm 0.794 | train_wall 323 | gb_free 6.1 | wall 42781
KL Stats: Epoch 120 Divergences: Uniform: 3.083634870929608 Unigram: 3.4561153826478637
2022-01-28 18:43:16 | INFO | fairseq.trainer | begin training epoch 121
2022-01-28 18:43:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:44:58 | INFO | train_inner | epoch 121:     20 / 64 loss=5.635, ppl=49.69, wps=5776.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7700, lr=0.000360375, gnorm=0.787, train_wall=504, gb_free=6.1, wall=42883
2022-01-28 18:48:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:49:08 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 9.99 | ppl 1016.62 | wps 7955.5 | wpb 2034.1 | bsz 4 | num_updates 7744 | best_loss 9.247
2022-01-28 18:49:08 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-01-28 18:49:08 | INFO | train | epoch 121 | loss 5.621 | ppl 49.22 | wps 5929.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7744 | lr 0.00035935 | gnorm 0.773 | train_wall 323 | gb_free 6.1 | wall 43133
KL Stats: Epoch 121 Divergences: Uniform: 3.083176597615533 Unigram: 3.463400884146591
2022-01-28 18:49:08 | INFO | fairseq.trainer | begin training epoch 122
2022-01-28 18:49:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:53:54 | INFO | train_inner | epoch 122:     56 / 64 loss=5.619, ppl=49.15, wps=6098.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7800, lr=0.000358057, gnorm=0.783, train_wall=506, gb_free=6.1, wall=43419
2022-01-28 18:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:55:01 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 9.986 | ppl 1014.22 | wps 7964.1 | wpb 2034.1 | bsz 4 | num_updates 7808 | best_loss 9.247
2022-01-28 18:55:01 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-01-28 18:55:01 | INFO | train | epoch 122 | loss 5.611 | ppl 48.88 | wps 5927.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7808 | lr 0.000357874 | gnorm 0.796 | train_wall 323 | gb_free 6.1 | wall 43486
KL Stats: Epoch 122 Divergences: Uniform: 3.089297112178337 Unigram: 3.475395069088451
2022-01-28 18:55:01 | INFO | fairseq.trainer | begin training epoch 123
2022-01-28 18:55:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:00:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:00:54 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.039 | ppl 1052.32 | wps 7975.1 | wpb 2034.1 | bsz 4 | num_updates 7872 | best_loss 9.247
2022-01-28 19:00:54 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-01-28 19:00:54 | INFO | train | epoch 123 | loss 5.6 | ppl 48.51 | wps 5919.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7872 | lr 0.000356416 | gnorm 0.8 | train_wall 324 | gb_free 6.1 | wall 43838
KL Stats: Epoch 123 Divergences: Uniform: 3.0885568039712514 Unigram: 3.4819249167105806
2022-01-28 19:00:54 | INFO | fairseq.trainer | begin training epoch 124
2022-01-28 19:00:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:03:16 | INFO | train_inner | epoch 124:     28 / 64 loss=5.593, ppl=48.27, wps=5795.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7900, lr=0.000355784, gnorm=0.801, train_wall=505, gb_free=6.1, wall=43981
2022-01-28 19:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 19:06:46 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 9.964 | ppl 998.47 | wps 8014.7 | wpb 2034.1 | bsz 4 | num_updates 7936 | best_loss 9.247
2022-01-28 19:06:46 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-01-28 19:06:46 | INFO | train | epoch 124 | loss 5.588 | ppl 48.1 | wps 5931.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7936 | lr 0.000354976 | gnorm 0.804 | train_wall 323 | gb_free 6.1 | wall 44191
KL Stats: Epoch 124 Divergences: Uniform: 3.078961124750333 Unigram: 3.4839757323741276
2022-01-28 19:06:46 | INFO | fairseq.trainer | begin training epoch 125
2022-01-28 19:06:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:12:11 | INFO | train_inner | epoch 125:     64 / 64 loss=5.592, ppl=48.22, wps=6098.7, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=8000, lr=0.000353553, gnorm=0.824, train_wall=505, gb_free=6.1, wall=44516
2022-01-28 19:12:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:12:38 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.037 | ppl 1050.35 | wps 7974.7 | wpb 2034.1 | bsz 4 | num_updates 8000 | best_loss 9.247
2022-01-28 19:12:38 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-01-28 19:12:38 | INFO | train | epoch 125 | loss 5.578 | ppl 47.76 | wps 5924.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8000 | lr 0.000353553 | gnorm 0.834 | train_wall 323 | gb_free 6.1 | wall 44543
KL Stats: Epoch 125 Divergences: Uniform: 3.0912106792714082 Unigram: 3.4957857319264214
2022-01-28 19:12:38 | INFO | fairseq.trainer | begin training epoch 126
2022-01-28 19:12:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:18:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:18:30 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10 | ppl 1023.91 | wps 7987.9 | wpb 2034.1 | bsz 4 | num_updates 8064 | best_loss 9.247
2022-01-28 19:18:30 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-01-28 19:18:30 | INFO | train | epoch 126 | loss 5.566 | ppl 47.39 | wps 5939 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8064 | lr 0.000352148 | gnorm 0.806 | train_wall 323 | gb_free 6.1 | wall 44895
KL Stats: Epoch 126 Divergences: Uniform: 3.0968285705373386 Unigram: 3.5022603152224714
2022-01-28 19:18:30 | INFO | fairseq.trainer | begin training epoch 127
2022-01-28 19:18:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:21:33 | INFO | train_inner | epoch 127:     36 / 64 loss=5.555, ppl=47.03, wps=5812.1, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=8100, lr=0.000351364, gnorm=0.828, train_wall=505, gb_free=6.1, wall=45078
2022-01-28 19:23:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:24:22 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.047 | ppl 1057.61 | wps 7984.1 | wpb 2034.1 | bsz 4 | num_updates 8128 | best_loss 9.247
2022-01-28 19:24:22 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-01-28 19:24:22 | INFO | train | epoch 127 | loss 5.558 | ppl 47.11 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8128 | lr 0.000350758 | gnorm 0.837 | train_wall 323 | gb_free 6.1 | wall 45247
KL Stats: Epoch 127 Divergences: Uniform: 3.091563556229227 Unigram: 3.509414005216047
2022-01-28 19:24:22 | INFO | fairseq.trainer | begin training epoch 128
2022-01-28 19:24:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:30:14 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.08 | ppl 1082.45 | wps 7965 | wpb 2034.1 | bsz 4 | num_updates 8192 | best_loss 9.247
2022-01-28 19:30:14 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-01-28 19:30:14 | INFO | train | epoch 128 | loss 5.545 | ppl 46.7 | wps 5920.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8192 | lr 0.000349386 | gnorm 0.838 | train_wall 324 | gb_free 6.1 | wall 45599
KL Stats: Epoch 128 Divergences: Uniform: 3.0943092855410317 Unigram: 3.5161095484595166
2022-01-28 19:30:14 | INFO | fairseq.trainer | begin training epoch 129
2022-01-28 19:30:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:30:55 | INFO | train_inner | epoch 129:      8 / 64 loss=5.553, ppl=46.95, wps=5798.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8200, lr=0.000349215, gnorm=0.831, train_wall=505, gb_free=6.1, wall=45640
2022-01-28 19:35:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:36:06 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.034 | ppl 1048.71 | wps 7970.2 | wpb 2034.1 | bsz 4 | num_updates 8256 | best_loss 9.247
2022-01-28 19:36:06 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-01-28 19:36:06 | INFO | train | epoch 129 | loss 5.538 | ppl 46.46 | wps 5933.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8256 | lr 0.000348029 | gnorm 0.828 | train_wall 323 | gb_free 6.1 | wall 45951
KL Stats: Epoch 129 Divergences: Uniform: 3.0958655177885515 Unigram: 3.526698186986985
2022-01-28 19:36:06 | INFO | fairseq.trainer | begin training epoch 130
2022-01-28 19:36:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:39:50 | INFO | train_inner | epoch 130:     44 / 64 loss=5.527, ppl=46.09, wps=6110.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=8300, lr=0.000347105, gnorm=0.832, train_wall=505, gb_free=6.1, wall=46175
2022-01-28 19:41:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:41:58 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.066 | ppl 1071.74 | wps 7999.1 | wpb 2034.1 | bsz 4 | num_updates 8320 | best_loss 9.247
2022-01-28 19:41:58 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-01-28 19:41:58 | INFO | train | epoch 130 | loss 5.528 | ppl 46.13 | wps 5944 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8320 | lr 0.000346688 | gnorm 0.846 | train_wall 322 | gb_free 6.1 | wall 46303
KL Stats: Epoch 130 Divergences: Uniform: 3.0992035059414595 Unigram: 3.53661623934455
2022-01-28 19:41:58 | INFO | fairseq.trainer | begin training epoch 131
2022-01-28 19:41:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:47:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:47:49 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.007 | ppl 1029.17 | wps 7958.1 | wpb 2034.1 | bsz 4 | num_updates 8384 | best_loss 9.247
2022-01-28 19:47:49 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-01-28 19:47:49 | INFO | train | epoch 131 | loss 5.518 | ppl 45.81 | wps 5943.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8384 | lr 0.000345362 | gnorm 0.856 | train_wall 322 | gb_free 6.1 | wall 46654
KL Stats: Epoch 131 Divergences: Uniform: 3.0994898943810054 Unigram: 3.535177005009285
2022-01-28 19:47:49 | INFO | fairseq.trainer | begin training epoch 132
2022-01-28 19:47:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:49:11 | INFO | train_inner | epoch 132:     16 / 64 loss=5.521, ppl=45.91, wps=5815.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8400, lr=0.000345033, gnorm=0.856, train_wall=503, gb_free=6.1, wall=46736
2022-01-28 19:53:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:53:41 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.049 | ppl 1059.41 | wps 7986.6 | wpb 2034.1 | bsz 4 | num_updates 8448 | best_loss 9.247
2022-01-28 19:53:41 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-01-28 19:53:41 | INFO | train | epoch 132 | loss 5.509 | ppl 45.52 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8448 | lr 0.000344051 | gnorm 0.865 | train_wall 323 | gb_free 6.1 | wall 47006
KL Stats: Epoch 132 Divergences: Uniform: 3.104738720353317 Unigram: 3.5489789873454183
2022-01-28 19:53:41 | INFO | fairseq.trainer | begin training epoch 133
2022-01-28 19:53:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:58:06 | INFO | train_inner | epoch 133:     52 / 64 loss=5.503, ppl=45.36, wps=6110.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8500, lr=0.000342997, gnorm=0.863, train_wall=505, gb_free=6.1, wall=47271
2022-01-28 19:59:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:59:33 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.05 | ppl 1059.91 | wps 7971.9 | wpb 2034.1 | bsz 4 | num_updates 8512 | best_loss 9.247
2022-01-28 19:59:33 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-01-28 19:59:33 | INFO | train | epoch 133 | loss 5.499 | ppl 45.22 | wps 5939.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8512 | lr 0.000342755 | gnorm 0.861 | train_wall 323 | gb_free 6.1 | wall 47358
KL Stats: Epoch 133 Divergences: Uniform: 3.114755847006441 Unigram: 3.55498826477298
2022-01-28 19:59:33 | INFO | fairseq.trainer | begin training epoch 134
2022-01-28 19:59:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:04:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:05:25 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.139 | ppl 1127.36 | wps 7948.1 | wpb 2034.1 | bsz 4 | num_updates 8576 | best_loss 9.247
2022-01-28 20:05:25 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-01-28 20:05:25 | INFO | train | epoch 134 | loss 5.491 | ppl 44.97 | wps 5926.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8576 | lr 0.000341474 | gnorm 0.89 | train_wall 323 | gb_free 6.1 | wall 47710
KL Stats: Epoch 134 Divergences: Uniform: 3.1082637609506363 Unigram: 3.5610575038911607
2022-01-28 20:05:25 | INFO | fairseq.trainer | begin training epoch 135
2022-01-28 20:05:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:07:27 | INFO | train_inner | epoch 135:     24 / 64 loss=5.49, ppl=44.93, wps=5801.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8600, lr=0.000340997, gnorm=0.882, train_wall=504, gb_free=6.1, wall=47832
2022-01-28 20:10:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 20:11:17 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.024 | ppl 1041.25 | wps 7958.4 | wpb 2034.1 | bsz 4 | num_updates 8640 | best_loss 9.247
2022-01-28 20:11:17 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-01-28 20:11:17 | INFO | train | epoch 135 | loss 5.48 | ppl 44.63 | wps 5927.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8640 | lr 0.000340207 | gnorm 0.86 | train_wall 323 | gb_free 6.1 | wall 48062
KL Stats: Epoch 135 Divergences: Uniform: 3.113005234091429 Unigram: 3.569006650012914
2022-01-28 20:11:17 | INFO | fairseq.trainer | begin training epoch 136
2022-01-28 20:11:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:16:23 | INFO | train_inner | epoch 136:     60 / 64 loss=5.481, ppl=44.67, wps=6100.7, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=8700, lr=0.000339032, gnorm=0.865, train_wall=506, gb_free=6.1, wall=48368
2022-01-28 20:16:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:17:10 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.09 | ppl 1089.63 | wps 7977 | wpb 2034.1 | bsz 4 | num_updates 8704 | best_loss 9.247
2022-01-28 20:17:10 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-01-28 20:17:10 | INFO | train | epoch 136 | loss 5.472 | ppl 44.4 | wps 5930.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8704 | lr 0.000338954 | gnorm 0.875 | train_wall 323 | gb_free 6.1 | wall 48415
KL Stats: Epoch 136 Divergences: Uniform: 3.111475834886709 Unigram: 3.5721915050765647
2022-01-28 20:17:10 | INFO | fairseq.trainer | begin training epoch 137
2022-01-28 20:17:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:23:02 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.112 | ppl 1106.63 | wps 7967.6 | wpb 2034.1 | bsz 4 | num_updates 8768 | best_loss 9.247
2022-01-28 20:23:02 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-01-28 20:23:02 | INFO | train | epoch 137 | loss 5.463 | ppl 44.12 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8768 | lr 0.000337715 | gnorm 0.871 | train_wall 323 | gb_free 6.1 | wall 48767
KL Stats: Epoch 137 Divergences: Uniform: 3.1129259965814486 Unigram: 3.583767860664879
2022-01-28 20:23:02 | INFO | fairseq.trainer | begin training epoch 138
2022-01-28 20:23:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:25:45 | INFO | train_inner | epoch 138:     32 / 64 loss=5.455, ppl=43.86, wps=5805.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8800, lr=0.0003371, gnorm=0.875, train_wall=504, gb_free=6.1, wall=48930
2022-01-28 20:28:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:28:54 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.16 | ppl 1144.49 | wps 7985.1 | wpb 2034.1 | bsz 4 | num_updates 8832 | best_loss 9.247
2022-01-28 20:28:54 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-01-28 20:28:54 | INFO | train | epoch 138 | loss 5.454 | ppl 43.82 | wps 5933.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8832 | lr 0.000336489 | gnorm 0.897 | train_wall 323 | gb_free 6.1 | wall 49119
KL Stats: Epoch 138 Divergences: Uniform: 3.1180761642472987 Unigram: 3.585721987594354
2022-01-28 20:28:54 | INFO | fairseq.trainer | begin training epoch 139
2022-01-28 20:28:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:34:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:34:46 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.124 | ppl 1115.64 | wps 7988.6 | wpb 2034.1 | bsz 4 | num_updates 8896 | best_loss 9.247
2022-01-28 20:34:46 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-01-28 20:34:46 | INFO | train | epoch 139 | loss 5.444 | ppl 43.54 | wps 5928 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8896 | lr 0.000335276 | gnorm 0.886 | train_wall 323 | gb_free 6.1 | wall 49471
KL Stats: Epoch 139 Divergences: Uniform: 3.118582500334163 Unigram: 3.599172301150617
2022-01-28 20:34:46 | INFO | fairseq.trainer | begin training epoch 140
2022-01-28 20:34:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:35:06 | INFO | train_inner | epoch 140:      4 / 64 loss=5.452, ppl=43.77, wps=5802.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8900, lr=0.000335201, gnorm=0.892, train_wall=504, gb_free=6.1, wall=49491
2022-01-28 20:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:40:38 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.13 | ppl 1120.4 | wps 7960.1 | wpb 2034.1 | bsz 4 | num_updates 8960 | best_loss 9.247
2022-01-28 20:40:38 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-01-28 20:40:38 | INFO | train | epoch 140 | loss 5.438 | ppl 43.35 | wps 5934.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8960 | lr 0.000334077 | gnorm 0.917 | train_wall 323 | gb_free 6.1 | wall 49823
KL Stats: Epoch 140 Divergences: Uniform: 3.1190012029367606 Unigram: 3.6021437893641877
2022-01-28 20:40:38 | INFO | fairseq.trainer | begin training epoch 141
2022-01-28 20:40:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:44:02 | INFO | train_inner | epoch 141:     40 / 64 loss=5.43, ppl=43.1, wps=6103.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9000, lr=0.000333333, gnorm=0.912, train_wall=505, gb_free=6.1, wall=50027
2022-01-28 20:46:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 20:46:30 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.133 | ppl 1122.76 | wps 7966.3 | wpb 2034.1 | bsz 4 | num_updates 9024 | best_loss 9.247
2022-01-28 20:46:30 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-01-28 20:46:30 | INFO | train | epoch 141 | loss 5.428 | ppl 43.06 | wps 5927.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9024 | lr 0.00033289 | gnorm 0.892 | train_wall 323 | gb_free 6.1 | wall 50175
KL Stats: Epoch 141 Divergences: Uniform: 3.1243955176827964 Unigram: 3.6074536374765556
2022-01-28 20:46:30 | INFO | fairseq.trainer | begin training epoch 142
2022-01-28 20:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:51:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:52:23 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.095 | ppl 1094.05 | wps 7990.7 | wpb 2034.1 | bsz 4 | num_updates 9088 | best_loss 9.247
2022-01-28 20:52:23 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-01-28 20:52:23 | INFO | train | epoch 142 | loss 5.42 | ppl 42.82 | wps 5930.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9088 | lr 0.000331716 | gnorm 0.917 | train_wall 323 | gb_free 6.1 | wall 50528
KL Stats: Epoch 142 Divergences: Uniform: 3.1248777255554807 Unigram: 3.6114102182417938
2022-01-28 20:52:23 | INFO | fairseq.trainer | begin training epoch 143
2022-01-28 20:52:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:53:24 | INFO | train_inner | epoch 143:     12 / 64 loss=5.422, ppl=42.86, wps=5802, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9100, lr=0.000331497, gnorm=0.912, train_wall=504, gb_free=6.1, wall=50589
2022-01-28 20:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:58:15 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.101 | ppl 1098.54 | wps 7973.6 | wpb 2034.1 | bsz 4 | num_updates 9152 | best_loss 9.247
2022-01-28 20:58:15 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-01-28 20:58:15 | INFO | train | epoch 143 | loss 5.412 | ppl 42.58 | wps 5927.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9152 | lr 0.000330554 | gnorm 0.921 | train_wall 323 | gb_free 6.1 | wall 50880
KL Stats: Epoch 143 Divergences: Uniform: 3.13066679607007 Unigram: 3.616520317511933
2022-01-28 20:58:15 | INFO | fairseq.trainer | begin training epoch 144
2022-01-28 20:58:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:02:19 | INFO | train_inner | epoch 144:     48 / 64 loss=5.409, ppl=42.48, wps=6102.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9200, lr=0.00032969, gnorm=0.925, train_wall=505, gb_free=6.1, wall=51124
2022-01-28 21:03:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:04:07 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.15 | ppl 1136.36 | wps 8006.5 | wpb 2034.1 | bsz 4 | num_updates 9216 | best_loss 9.247
2022-01-28 21:04:07 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-01-28 21:04:07 | INFO | train | epoch 144 | loss 5.406 | ppl 42.39 | wps 5935.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9216 | lr 0.000329404 | gnorm 0.93 | train_wall 323 | gb_free 6.1 | wall 51232
KL Stats: Epoch 144 Divergences: Uniform: 3.1218450764682735 Unigram: 3.617595898644743
2022-01-28 21:04:07 | INFO | fairseq.trainer | begin training epoch 145
2022-01-28 21:04:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:09:59 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.123 | ppl 1115.2 | wps 7981.4 | wpb 2034.1 | bsz 4 | num_updates 9280 | best_loss 9.247
2022-01-28 21:09:59 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-01-28 21:09:59 | INFO | train | epoch 145 | loss 5.397 | ppl 42.14 | wps 5923.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9280 | lr 0.000328266 | gnorm 0.919 | train_wall 323 | gb_free 6.1 | wall 51584
KL Stats: Epoch 145 Divergences: Uniform: 3.132068140052765 Unigram: 3.632036743511161
2022-01-28 21:09:59 | INFO | fairseq.trainer | begin training epoch 146
2022-01-28 21:09:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:11:41 | INFO | train_inner | epoch 146:     20 / 64 loss=5.394, ppl=42.04, wps=5802.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9300, lr=0.000327913, gnorm=0.918, train_wall=504, gb_free=6.1, wall=51686
2022-01-28 21:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:15:51 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.115 | ppl 1108.66 | wps 7988 | wpb 2034.1 | bsz 4 | num_updates 9344 | best_loss 9.247
2022-01-28 21:15:51 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-01-28 21:15:51 | INFO | train | epoch 146 | loss 5.388 | ppl 41.87 | wps 5939.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.935 | train_wall 323 | gb_free 6.1 | wall 51936
KL Stats: Epoch 146 Divergences: Uniform: 3.1295831448969396 Unigram: 3.638488731223427
2022-01-28 21:15:51 | INFO | fairseq.trainer | begin training epoch 147
2022-01-28 21:15:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:20:36 | INFO | train_inner | epoch 147:     56 / 64 loss=5.389, ppl=41.92, wps=6111.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9400, lr=0.000326164, gnorm=0.942, train_wall=505, gb_free=6.1, wall=52221
2022-01-28 21:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:21:43 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.123 | ppl 1115.13 | wps 7976.5 | wpb 2034.1 | bsz 4 | num_updates 9408 | best_loss 9.247
2022-01-28 21:21:43 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-01-28 21:21:43 | INFO | train | epoch 147 | loss 5.38 | ppl 41.65 | wps 5939 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9408 | lr 0.000326025 | gnorm 0.943 | train_wall 323 | gb_free 6.1 | wall 52288
KL Stats: Epoch 147 Divergences: Uniform: 3.133983672379495 Unigram: 3.6424154464901153
2022-01-28 21:21:43 | INFO | fairseq.trainer | begin training epoch 148
2022-01-28 21:21:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:27:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 21:27:35 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.122 | ppl 1114.73 | wps 7974.7 | wpb 2034.1 | bsz 4 | num_updates 9472 | best_loss 9.247
2022-01-28 21:27:35 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-01-28 21:27:35 | INFO | train | epoch 148 | loss 5.373 | ppl 41.44 | wps 5931.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9472 | lr 0.000324922 | gnorm 0.95 | train_wall 323 | gb_free 6.1 | wall 52640
KL Stats: Epoch 148 Divergences: Uniform: 3.1329127821560387 Unigram: 3.6469369351572847
2022-01-28 21:27:35 | INFO | fairseq.trainer | begin training epoch 149
2022-01-28 21:27:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:29:57 | INFO | train_inner | epoch 149:     28 / 64 loss=5.368, ppl=41.31, wps=5804.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9500, lr=0.000324443, gnorm=0.949, train_wall=504, gb_free=6.1, wall=52782
2022-01-28 21:32:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:33:27 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.153 | ppl 1138.75 | wps 7968 | wpb 2034.1 | bsz 4 | num_updates 9536 | best_loss 9.247
2022-01-28 21:33:27 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-01-28 21:33:27 | INFO | train | epoch 149 | loss 5.368 | ppl 41.29 | wps 5932.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9536 | lr 0.00032383 | gnorm 0.974 | train_wall 323 | gb_free 6.1 | wall 52992
KL Stats: Epoch 149 Divergences: Uniform: 3.141819714335828 Unigram: 3.6526781174007437
2022-01-28 21:33:27 | INFO | fairseq.trainer | begin training epoch 150
2022-01-28 21:33:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:38:51 | INFO | train_inner | epoch 150:     64 / 64 loss=5.369, ppl=41.34, wps=6105.3, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=9600, lr=0.000322749, gnorm=0.973, train_wall=504, gb_free=6.1, wall=53316
2022-01-28 21:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:39:19 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.166 | ppl 1149.21 | wps 8010.3 | wpb 2034.1 | bsz 4 | num_updates 9600 | best_loss 9.247
2022-01-28 21:39:19 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-01-28 21:39:19 | INFO | train | epoch 150 | loss 5.358 | ppl 41.02 | wps 5936 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9600 | lr 0.000322749 | gnorm 0.959 | train_wall 323 | gb_free 6.1 | wall 53344
KL Stats: Epoch 150 Divergences: Uniform: 3.141863404725638 Unigram: 3.653259830267441
2022-01-28 21:39:19 | INFO | fairseq.trainer | begin training epoch 151
2022-01-28 21:39:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:45:11 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.195 | ppl 1172.46 | wps 7965.5 | wpb 2034.1 | bsz 4 | num_updates 9664 | best_loss 9.247
2022-01-28 21:45:11 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-01-28 21:45:11 | INFO | train | epoch 151 | loss 5.351 | ppl 40.81 | wps 5936.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9664 | lr 0.000321678 | gnorm 0.977 | train_wall 323 | gb_free 6.1 | wall 53696
KL Stats: Epoch 151 Divergences: Uniform: 3.1358385949572933 Unigram: 3.6642055318877267
2022-01-28 21:45:11 | INFO | fairseq.trainer | begin training epoch 152
2022-01-28 21:45:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:48:14 | INFO | train_inner | epoch 152:     36 / 64 loss=5.338, ppl=40.46, wps=5808, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=9700, lr=0.000321081, gnorm=0.98, train_wall=505, gb_free=6.1, wall=53879
2022-01-28 21:50:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:51:03 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.15 | ppl 1136.53 | wps 7994.4 | wpb 2034.1 | bsz 4 | num_updates 9728 | best_loss 9.247
2022-01-28 21:51:03 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-01-28 21:51:03 | INFO | train | epoch 152 | loss 5.344 | ppl 40.61 | wps 5930.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9728 | lr 0.000320618 | gnorm 0.975 | train_wall 323 | gb_free 6.1 | wall 54048
KL Stats: Epoch 152 Divergences: Uniform: 3.1424803358155042 Unigram: 3.6716095564156914
2022-01-28 21:51:03 | INFO | fairseq.trainer | begin training epoch 153
2022-01-28 21:51:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:56:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 21:56:56 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.189 | ppl 1167.23 | wps 7981.4 | wpb 2034.1 | bsz 4 | num_updates 9792 | best_loss 9.247
2022-01-28 21:56:56 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-01-28 21:56:56 | INFO | train | epoch 153 | loss 5.336 | ppl 40.4 | wps 5920.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9792 | lr 0.000319569 | gnorm 0.953 | train_wall 324 | gb_free 6.1 | wall 54401
KL Stats: Epoch 153 Divergences: Uniform: 3.1432067337323972 Unigram: 3.675214992646794
2022-01-28 21:56:56 | INFO | fairseq.trainer | begin training epoch 154
2022-01-28 21:56:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:57:36 | INFO | train_inner | epoch 154:      8 / 64 loss=5.344, ppl=40.61, wps=5796.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9800, lr=0.000319438, gnorm=0.956, train_wall=505, gb_free=6.1, wall=54441
2022-01-28 22:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:02:48 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.215 | ppl 1188.53 | wps 7973.8 | wpb 2034.1 | bsz 4 | num_updates 9856 | best_loss 9.247
2022-01-28 22:02:48 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-01-28 22:02:48 | INFO | train | epoch 154 | loss 5.33 | ppl 40.23 | wps 5927.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9856 | lr 0.000318529 | gnorm 0.983 | train_wall 323 | gb_free 6.1 | wall 54753
KL Stats: Epoch 154 Divergences: Uniform: 3.151888629462102 Unigram: 3.680671537816657
2022-01-28 22:02:48 | INFO | fairseq.trainer | begin training epoch 155
2022-01-28 22:02:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:06:32 | INFO | train_inner | epoch 155:     44 / 64 loss=5.321, ppl=39.99, wps=6101, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=9900, lr=0.000317821, gnorm=0.981, train_wall=506, gb_free=6.1, wall=54977
2022-01-28 22:08:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 22:08:40 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.193 | ppl 1170.19 | wps 7958.3 | wpb 2034.1 | bsz 4 | num_updates 9920 | best_loss 9.247
2022-01-28 22:08:40 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-01-28 22:08:40 | INFO | train | epoch 155 | loss 5.323 | ppl 40.04 | wps 5930.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9920 | lr 0.0003175 | gnorm 0.999 | train_wall 323 | gb_free 6.1 | wall 55105
KL Stats: Epoch 155 Divergences: Uniform: 3.1451597850797866 Unigram: 3.68986761966812
2022-01-28 22:08:40 | INFO | fairseq.trainer | begin training epoch 156
2022-01-28 22:08:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:14:33 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.192 | ppl 1169.59 | wps 7991.7 | wpb 2034.1 | bsz 4 | num_updates 9984 | best_loss 9.247
2022-01-28 22:14:33 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-01-28 22:14:33 | INFO | train | epoch 156 | loss 5.318 | ppl 39.88 | wps 5926.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9984 | lr 0.000316481 | gnorm 1.026 | train_wall 323 | gb_free 6.1 | wall 55458
KL Stats: Epoch 156 Divergences: Uniform: 3.1480392181127272 Unigram: 3.6867878192591763
2022-01-28 22:14:33 | INFO | fairseq.trainer | begin training epoch 157
2022-01-28 22:14:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:15:54 | INFO | train_inner | epoch 157:     16 / 64 loss=5.323, ppl=40.02, wps=5799.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10000, lr=0.000316228, gnorm=1.015, train_wall=505, gb_free=6.1, wall=55539
2022-01-28 22:19:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:20:25 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.223 | ppl 1195.41 | wps 7970.7 | wpb 2034.1 | bsz 4 | num_updates 10048 | best_loss 9.247
2022-01-28 22:20:25 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-01-28 22:20:25 | INFO | train | epoch 157 | loss 5.309 | ppl 39.64 | wps 5918.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10048 | lr 0.000315472 | gnorm 0.991 | train_wall 324 | gb_free 6.1 | wall 55810
KL Stats: Epoch 157 Divergences: Uniform: 3.1490334348047884 Unigram: 3.702428226623332
2022-01-28 22:20:25 | INFO | fairseq.trainer | begin training epoch 158
2022-01-28 22:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:24:51 | INFO | train_inner | epoch 158:     52 / 64 loss=5.302, ppl=39.46, wps=6093.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10100, lr=0.000314658, gnorm=0.997, train_wall=506, gb_free=6.1, wall=56076
2022-01-28 22:25:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:26:18 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.227 | ppl 1198.16 | wps 7963.7 | wpb 2034.1 | bsz 4 | num_updates 10112 | best_loss 9.247
2022-01-28 22:26:18 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-01-28 22:26:18 | INFO | train | epoch 158 | loss 5.303 | ppl 39.48 | wps 5924.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10112 | lr 0.000314472 | gnorm 1.001 | train_wall 323 | gb_free 6.1 | wall 56163
KL Stats: Epoch 158 Divergences: Uniform: 3.1464256935525987 Unigram: 3.7018871334886954
2022-01-28 22:26:18 | INFO | fairseq.trainer | begin training epoch 159
2022-01-28 22:26:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:31:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:32:11 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.199 | ppl 1175.1 | wps 7969.5 | wpb 2034.1 | bsz 4 | num_updates 10176 | best_loss 9.247
2022-01-28 22:32:11 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-01-28 22:32:11 | INFO | train | epoch 159 | loss 5.295 | ppl 39.27 | wps 5914.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10176 | lr 0.000313481 | gnorm 1.002 | train_wall 324 | gb_free 6.1 | wall 56516
KL Stats: Epoch 159 Divergences: Uniform: 3.143821295980766 Unigram: 3.709025279500989
2022-01-28 22:32:11 | INFO | fairseq.trainer | begin training epoch 160
2022-01-28 22:32:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:34:14 | INFO | train_inner | epoch 160:     24 / 64 loss=5.295, ppl=39.27, wps=5790.8, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=10200, lr=0.000313112, gnorm=1.022, train_wall=505, gb_free=6.1, wall=56638
2022-01-28 22:37:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:38:03 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.185 | ppl 1163.74 | wps 7964.3 | wpb 2034.1 | bsz 4 | num_updates 10240 | best_loss 9.247
2022-01-28 22:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 10240 updates
2022-01-28 22:38:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint160.pt
2022-01-28 22:38:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint160.pt
2022-01-28 22:38:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint160.pt (epoch 160 @ 10240 updates, score 10.185) (writing took 3.0477141649462283 seconds)
2022-01-28 22:38:06 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-01-28 22:38:06 | INFO | train | epoch 160 | loss 5.29 | ppl 39.12 | wps 5878.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10240 | lr 0.0003125 | gnorm 1.03 | train_wall 323 | gb_free 6.1 | wall 56871
KL Stats: Epoch 160 Divergences: Uniform: 3.152270287870567 Unigram: 3.7136292908385338
2022-01-28 22:38:06 | INFO | fairseq.trainer | begin training epoch 161
2022-01-28 22:38:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:43:12 | INFO | train_inner | epoch 161:     60 / 64 loss=5.291, ppl=39.15, wps=6066.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10300, lr=0.000311588, gnorm=1.028, train_wall=505, gb_free=6.1, wall=57177
2022-01-28 22:43:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 22:43:59 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.139 | ppl 1127.67 | wps 7975.7 | wpb 2034.1 | bsz 4 | num_updates 10304 | best_loss 9.247
2022-01-28 22:43:59 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-01-28 22:43:59 | INFO | train | epoch 161 | loss 5.284 | ppl 38.97 | wps 5928.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10304 | lr 0.000311528 | gnorm 1.039 | train_wall 323 | gb_free 6.1 | wall 57224
KL Stats: Epoch 161 Divergences: Uniform: 3.1538474142842436 Unigram: 3.7151709280170655
2022-01-28 22:43:59 | INFO | fairseq.trainer | begin training epoch 162
2022-01-28 22:43:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:49:51 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.197 | ppl 1174.05 | wps 7983.8 | wpb 2034.1 | bsz 4 | num_updates 10368 | best_loss 9.247
2022-01-28 22:49:51 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-01-28 22:49:51 | INFO | train | epoch 162 | loss 5.277 | ppl 38.78 | wps 5929 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10368 | lr 0.000310565 | gnorm 1.06 | train_wall 323 | gb_free 6.1 | wall 57576
KL Stats: Epoch 162 Divergences: Uniform: 3.1597861720888125 Unigram: 3.724012951823377
2022-01-28 22:49:51 | INFO | fairseq.trainer | begin training epoch 163
2022-01-28 22:49:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:52:34 | INFO | train_inner | epoch 163:     32 / 64 loss=5.264, ppl=38.43, wps=5802.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10400, lr=0.000310087, gnorm=1.048, train_wall=504, gb_free=6.1, wall=57739
2022-01-28 22:55:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:55:43 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.258 | ppl 1224.8 | wps 7954.2 | wpb 2034.1 | bsz 4 | num_updates 10432 | best_loss 9.247
2022-01-28 22:55:43 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-01-28 22:55:43 | INFO | train | epoch 163 | loss 5.272 | ppl 38.65 | wps 5932.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10432 | lr 0.000309611 | gnorm 1.028 | train_wall 323 | gb_free 6.1 | wall 57928
KL Stats: Epoch 163 Divergences: Uniform: 3.1567763152039956 Unigram: 3.7293750425999925
2022-01-28 22:55:43 | INFO | fairseq.trainer | begin training epoch 164
2022-01-28 22:55:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:01:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:01:35 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.195 | ppl 1172.57 | wps 8005.5 | wpb 2034.1 | bsz 4 | num_updates 10496 | best_loss 9.247
2022-01-28 23:01:35 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-01-28 23:01:35 | INFO | train | epoch 164 | loss 5.265 | ppl 38.45 | wps 5937.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10496 | lr 0.000308665 | gnorm 1.056 | train_wall 323 | gb_free 6.1 | wall 58280
KL Stats: Epoch 164 Divergences: Uniform: 3.148810767831138 Unigram: 3.734931341129782
2022-01-28 23:01:35 | INFO | fairseq.trainer | begin training epoch 165
2022-01-28 23:01:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:01:55 | INFO | train_inner | epoch 165:      4 / 64 loss=5.28, ppl=38.85, wps=5808.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10500, lr=0.000308607, gnorm=1.049, train_wall=504, gb_free=6.1, wall=58300
2022-01-28 23:06:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:07:27 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.238 | ppl 1207.49 | wps 7953.5 | wpb 2034.1 | bsz 4 | num_updates 10560 | best_loss 9.247
2022-01-28 23:07:27 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-01-28 23:07:27 | INFO | train | epoch 165 | loss 5.258 | ppl 38.26 | wps 5933.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10560 | lr 0.000307729 | gnorm 1.047 | train_wall 323 | gb_free 6.1 | wall 58632
KL Stats: Epoch 165 Divergences: Uniform: 3.1546727071271916 Unigram: 3.7424384685182805
2022-01-28 23:07:27 | INFO | fairseq.trainer | begin training epoch 166
2022-01-28 23:07:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:10:51 | INFO | train_inner | epoch 166:     40 / 64 loss=5.251, ppl=38.08, wps=6102.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10600, lr=0.000307148, gnorm=1.059, train_wall=505, gb_free=6.1, wall=58836
2022-01-28 23:12:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:13:19 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.126 | ppl 1117.7 | wps 7945.3 | wpb 2034.1 | bsz 4 | num_updates 10624 | best_loss 9.247
2022-01-28 23:13:19 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-01-28 23:13:19 | INFO | train | epoch 166 | loss 5.253 | ppl 38.13 | wps 5927.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10624 | lr 0.0003068 | gnorm 1.061 | train_wall 323 | gb_free 6.1 | wall 58984
KL Stats: Epoch 166 Divergences: Uniform: 3.154940357115128 Unigram: 3.74314284208241
2022-01-28 23:13:19 | INFO | fairseq.trainer | begin training epoch 167
2022-01-28 23:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:18:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:19:12 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.118 | ppl 1111.27 | wps 7975.8 | wpb 2034.1 | bsz 4 | num_updates 10688 | best_loss 9.247
2022-01-28 23:19:12 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-01-28 23:19:12 | INFO | train | epoch 167 | loss 5.246 | ppl 37.96 | wps 5922.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10688 | lr 0.00030588 | gnorm 1.079 | train_wall 324 | gb_free 6.1 | wall 59337
KL Stats: Epoch 167 Divergences: Uniform: 3.155855891760645 Unigram: 3.745071999930404
2022-01-28 23:19:12 | INFO | fairseq.trainer | begin training epoch 168
2022-01-28 23:19:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:20:13 | INFO | train_inner | epoch 168:     12 / 64 loss=5.248, ppl=38, wps=5797.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10700, lr=0.000305709, gnorm=1.069, train_wall=505, gb_free=6.1, wall=59398
2022-01-28 23:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:25:04 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.231 | ppl 1201.77 | wps 7989.4 | wpb 2034.1 | bsz 4 | num_updates 10752 | best_loss 9.247
2022-01-28 23:25:04 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-01-28 23:25:04 | INFO | train | epoch 168 | loss 5.24 | ppl 37.79 | wps 5938.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10752 | lr 0.000304969 | gnorm 1.071 | train_wall 323 | gb_free 6.1 | wall 59689
KL Stats: Epoch 168 Divergences: Uniform: 3.161451520255233 Unigram: 3.7502548917896528
2022-01-28 23:25:04 | INFO | fairseq.trainer | begin training epoch 169
2022-01-28 23:25:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:29:08 | INFO | train_inner | epoch 169:     48 / 64 loss=5.24, ppl=37.79, wps=6109.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10800, lr=0.00030429, gnorm=1.076, train_wall=505, gb_free=6.1, wall=59933
2022-01-28 23:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:30:56 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.202 | ppl 1178.27 | wps 7984.6 | wpb 2034.1 | bsz 4 | num_updates 10816 | best_loss 9.247
2022-01-28 23:30:56 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-01-28 23:30:56 | INFO | train | epoch 169 | loss 5.236 | ppl 37.68 | wps 5935.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10816 | lr 0.000304065 | gnorm 1.081 | train_wall 323 | gb_free 6.1 | wall 60041
KL Stats: Epoch 169 Divergences: Uniform: 3.1643542576725094 Unigram: 3.7591434587545516
2022-01-28 23:30:56 | INFO | fairseq.trainer | begin training epoch 170
2022-01-28 23:30:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:36:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 23:36:47 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.224 | ppl 1196.34 | wps 7982.3 | wpb 2034.1 | bsz 4 | num_updates 10880 | best_loss 9.247
2022-01-28 23:36:47 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-01-28 23:36:47 | INFO | train | epoch 170 | loss 5.23 | ppl 37.54 | wps 5937.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10880 | lr 0.00030317 | gnorm 1.117 | train_wall 323 | gb_free 6.1 | wall 60392
KL Stats: Epoch 170 Divergences: Uniform: 3.163783756828393 Unigram: 3.7595453861285675
2022-01-28 23:36:47 | INFO | fairseq.trainer | begin training epoch 171
2022-01-28 23:36:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:38:29 | INFO | train_inner | epoch 171:     20 / 64 loss=5.224, ppl=37.38, wps=5809.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10900, lr=0.000302891, gnorm=1.11, train_wall=504, gb_free=6.1, wall=60494
2022-01-28 23:42:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:42:39 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.209 | ppl 1183.33 | wps 7990.8 | wpb 2034.1 | bsz 4 | num_updates 10944 | best_loss 9.247
2022-01-28 23:42:39 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-01-28 23:42:39 | INFO | train | epoch 171 | loss 5.224 | ppl 37.37 | wps 5937.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10944 | lr 0.000302282 | gnorm 1.113 | train_wall 323 | gb_free 6.1 | wall 60744
KL Stats: Epoch 171 Divergences: Uniform: 3.1619901212649086 Unigram: 3.764159386707209
2022-01-28 23:42:39 | INFO | fairseq.trainer | begin training epoch 172
2022-01-28 23:42:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:47:24 | INFO | train_inner | epoch 172:     56 / 64 loss=5.227, ppl=37.45, wps=6107.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11000, lr=0.000301511, gnorm=1.097, train_wall=505, gb_free=6.1, wall=61029
2022-01-28 23:48:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:48:31 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.208 | ppl 1182.72 | wps 7987 | wpb 2034.1 | bsz 4 | num_updates 11008 | best_loss 9.247
2022-01-28 23:48:31 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-01-28 23:48:31 | INFO | train | epoch 172 | loss 5.217 | ppl 37.2 | wps 5933.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11008 | lr 0.000301402 | gnorm 1.093 | train_wall 323 | gb_free 6.1 | wall 61096
KL Stats: Epoch 172 Divergences: Uniform: 3.1670383126588595 Unigram: 3.7696949251482237
2022-01-28 23:48:31 | INFO | fairseq.trainer | begin training epoch 173
2022-01-28 23:48:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:53:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 23:54:24 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.191 | ppl 1169 | wps 7953.4 | wpb 2034.1 | bsz 4 | num_updates 11072 | best_loss 9.247
2022-01-28 23:54:24 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-01-28 23:54:24 | INFO | train | epoch 173 | loss 5.212 | ppl 37.06 | wps 5922.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11072 | lr 0.000300529 | gnorm 1.123 | train_wall 323 | gb_free 6.1 | wall 61449
KL Stats: Epoch 173 Divergences: Uniform: 3.164421660075757 Unigram: 3.7717282102206418
2022-01-28 23:54:24 | INFO | fairseq.trainer | begin training epoch 174
2022-01-28 23:54:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:56:47 | INFO | train_inner | epoch 174:     28 / 64 loss=5.206, ppl=36.92, wps=5797.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11100, lr=0.00030015, gnorm=1.113, train_wall=505, gb_free=6.1, wall=61591
2022-01-28 23:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 00:00:16 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 10.307 | ppl 1266.83 | wps 7980.5 | wpb 2034.1 | bsz 4 | num_updates 11136 | best_loss 9.247
2022-01-29 00:00:16 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-01-29 00:00:16 | INFO | train | epoch 174 | loss 5.208 | ppl 36.97 | wps 5928.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11136 | lr 0.000299665 | gnorm 1.115 | train_wall 323 | gb_free 6.1 | wall 61801
KL Stats: Epoch 174 Divergences: Uniform: 3.165127265204655 Unigram: 3.7823429550004968
2022-01-29 00:00:16 | INFO | fairseq.trainer | begin training epoch 175
2022-01-29 00:00:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:05:42 | INFO | train_inner | epoch 175:     64 / 64 loss=5.211, ppl=37.05, wps=6093.5, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=11200, lr=0.000298807, gnorm=1.136, train_wall=505, gb_free=6.1, wall=62126
2022-01-29 00:05:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:06:09 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.299 | ppl 1259.85 | wps 7989.7 | wpb 2034.1 | bsz 4 | num_updates 11200 | best_loss 9.247
2022-01-29 00:06:09 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-01-29 00:06:09 | INFO | train | epoch 175 | loss 5.201 | ppl 36.77 | wps 5920 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11200 | lr 0.000298807 | gnorm 1.132 | train_wall 324 | gb_free 6.1 | wall 62154
KL Stats: Epoch 175 Divergences: Uniform: 3.1726337700381153 Unigram: 3.784205629303886
2022-01-29 00:06:09 | INFO | fairseq.trainer | begin training epoch 176
2022-01-29 00:06:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:12:01 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.265 | ppl 1230.35 | wps 7972.3 | wpb 2034.1 | bsz 4 | num_updates 11264 | best_loss 9.247
2022-01-29 00:12:01 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-01-29 00:12:01 | INFO | train | epoch 176 | loss 5.196 | ppl 36.66 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11264 | lr 0.000297957 | gnorm 1.138 | train_wall 323 | gb_free 6.1 | wall 62506
KL Stats: Epoch 176 Divergences: Uniform: 3.1697208008340767 Unigram: 3.790165657156899
2022-01-29 00:12:01 | INFO | fairseq.trainer | begin training epoch 177
2022-01-29 00:12:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:15:05 | INFO | train_inner | epoch 177:     36 / 64 loss=5.184, ppl=36.35, wps=5803.9, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=11300, lr=0.000297482, gnorm=1.134, train_wall=506, gb_free=6.1, wall=62690
2022-01-29 00:17:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:17:53 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 10.227 | ppl 1198.55 | wps 7988.6 | wpb 2034.1 | bsz 4 | num_updates 11328 | best_loss 9.247
2022-01-29 00:17:53 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-01-29 00:17:53 | INFO | train | epoch 177 | loss 5.191 | ppl 36.53 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11328 | lr 0.000297114 | gnorm 1.149 | train_wall 323 | gb_free 6.1 | wall 62858
KL Stats: Epoch 177 Divergences: Uniform: 3.169566041672965 Unigram: 3.791016910745999
2022-01-29 00:17:53 | INFO | fairseq.trainer | begin training epoch 178
2022-01-29 00:17:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:23:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:23:45 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 10.234 | ppl 1204.36 | wps 7988.9 | wpb 2034.1 | bsz 4 | num_updates 11392 | best_loss 9.247
2022-01-29 00:23:45 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-01-29 00:23:45 | INFO | train | epoch 178 | loss 5.189 | ppl 36.47 | wps 5941.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11392 | lr 0.000296278 | gnorm 1.144 | train_wall 322 | gb_free 6.1 | wall 63210
KL Stats: Epoch 178 Divergences: Uniform: 3.1780036456628316 Unigram: 3.796197594931747
2022-01-29 00:23:45 | INFO | fairseq.trainer | begin training epoch 179
2022-01-29 00:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:24:26 | INFO | train_inner | epoch 179:      8 / 64 loss=5.196, ppl=36.66, wps=5810.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11400, lr=0.000296174, gnorm=1.156, train_wall=504, gb_free=6.1, wall=63251
2022-01-29 00:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:29:39 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.238 | ppl 1207.45 | wps 7685.2 | wpb 2034.1 | bsz 4 | num_updates 11456 | best_loss 9.247
2022-01-29 00:29:39 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-01-29 00:29:39 | INFO | train | epoch 179 | loss 5.181 | ppl 36.27 | wps 5895.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11456 | lr 0.00029545 | gnorm 1.172 | train_wall 324 | gb_free 6.1 | wall 63564
KL Stats: Epoch 179 Divergences: Uniform: 3.1756704354148626 Unigram: 3.7977737971956445
2022-01-29 00:29:39 | INFO | fairseq.trainer | begin training epoch 180
2022-01-29 00:29:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:33:24 | INFO | train_inner | epoch 180:     44 / 64 loss=5.176, ppl=36.14, wps=6074.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11500, lr=0.000294884, gnorm=1.169, train_wall=507, gb_free=6.1, wall=63789
2022-01-29 00:35:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:35:32 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.24 | ppl 1209.64 | wps 7962.4 | wpb 2034.1 | bsz 4 | num_updates 11520 | best_loss 9.247
2022-01-29 00:35:32 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-01-29 00:35:32 | INFO | train | epoch 180 | loss 5.177 | ppl 36.17 | wps 5923 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11520 | lr 0.000294628 | gnorm 1.193 | train_wall 323 | gb_free 6.1 | wall 63917
KL Stats: Epoch 180 Divergences: Uniform: 3.174358183047637 Unigram: 3.808568492869086
2022-01-29 00:35:32 | INFO | fairseq.trainer | begin training epoch 181
2022-01-29 00:35:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:40:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:41:24 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.205 | ppl 1180.42 | wps 7951.3 | wpb 2034.1 | bsz 4 | num_updates 11584 | best_loss 9.247
2022-01-29 00:41:24 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-01-29 00:41:24 | INFO | train | epoch 181 | loss 5.17 | ppl 35.99 | wps 5923.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11584 | lr 0.000293813 | gnorm 1.165 | train_wall 323 | gb_free 6.1 | wall 64269
KL Stats: Epoch 181 Divergences: Uniform: 3.179286162905094 Unigram: 3.8085030838483394
2022-01-29 00:41:24 | INFO | fairseq.trainer | begin training epoch 182
2022-01-29 00:41:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:42:46 | INFO | train_inner | epoch 182:     16 / 64 loss=5.171, ppl=36.03, wps=5796.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11600, lr=0.00029361, gnorm=1.186, train_wall=505, gb_free=6.1, wall=64351
2022-01-29 00:46:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:47:17 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 10.223 | ppl 1195.22 | wps 7968.9 | wpb 2034.1 | bsz 4 | num_updates 11648 | best_loss 9.247
2022-01-29 00:47:17 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-01-29 00:47:17 | INFO | train | epoch 182 | loss 5.167 | ppl 35.94 | wps 5924.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11648 | lr 0.000293005 | gnorm 1.187 | train_wall 323 | gb_free 6.1 | wall 64622
KL Stats: Epoch 182 Divergences: Uniform: 3.1774451674433006 Unigram: 3.8109889544754942
2022-01-29 00:47:17 | INFO | fairseq.trainer | begin training epoch 183
2022-01-29 00:47:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:51:42 | INFO | train_inner | epoch 183:     52 / 64 loss=5.165, ppl=35.87, wps=6096.9, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=11700, lr=0.000292353, gnorm=1.186, train_wall=506, gb_free=6.1, wall=64887
2022-01-29 00:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:53:09 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.242 | ppl 1211.38 | wps 7985.9 | wpb 2034.1 | bsz 4 | num_updates 11712 | best_loss 9.247
2022-01-29 00:53:09 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-01-29 00:53:09 | INFO | train | epoch 183 | loss 5.161 | ppl 35.77 | wps 5925.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11712 | lr 0.000292203 | gnorm 1.19 | train_wall 323 | gb_free 6.1 | wall 64974
KL Stats: Epoch 183 Divergences: Uniform: 3.1821134180372894 Unigram: 3.817170271596219
2022-01-29 00:53:09 | INFO | fairseq.trainer | begin training epoch 184
2022-01-29 00:53:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:58:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:59:02 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.172 | ppl 1153.27 | wps 7946.4 | wpb 2034.1 | bsz 4 | num_updates 11776 | best_loss 9.247
2022-01-29 00:59:02 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-01-29 00:59:02 | INFO | train | epoch 184 | loss 5.158 | ppl 35.71 | wps 5914.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11776 | lr 0.000291408 | gnorm 1.209 | train_wall 324 | gb_free 6.1 | wall 65327
KL Stats: Epoch 184 Divergences: Uniform: 3.180675028484766 Unigram: 3.819567600145268
2022-01-29 00:59:02 | INFO | fairseq.trainer | begin training epoch 185
2022-01-29 00:59:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:01:05 | INFO | train_inner | epoch 185:     24 / 64 loss=5.158, ppl=35.71, wps=5791.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11800, lr=0.000291111, gnorm=1.215, train_wall=505, gb_free=6.1, wall=65450
2022-01-29 01:04:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:04:55 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 10.254 | ppl 1221.48 | wps 7956.7 | wpb 2034.1 | bsz 4 | num_updates 11840 | best_loss 9.247
2022-01-29 01:04:55 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-01-29 01:04:55 | INFO | train | epoch 185 | loss 5.152 | ppl 35.56 | wps 5924.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11840 | lr 0.000290619 | gnorm 1.203 | train_wall 323 | gb_free 6.1 | wall 65680
KL Stats: Epoch 185 Divergences: Uniform: 3.179069210358106 Unigram: 3.822111154523258
2022-01-29 01:04:55 | INFO | fairseq.trainer | begin training epoch 186
2022-01-29 01:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:10:01 | INFO | train_inner | epoch 186:     60 / 64 loss=5.152, ppl=35.56, wps=6096, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11900, lr=0.000289886, gnorm=1.198, train_wall=506, gb_free=6.1, wall=65986
2022-01-29 01:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:10:48 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.171 | ppl 1152.72 | wps 7968 | wpb 2034.1 | bsz 4 | num_updates 11904 | best_loss 9.247
2022-01-29 01:10:48 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-01-29 01:10:48 | INFO | train | epoch 186 | loss 5.148 | ppl 35.47 | wps 5925 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11904 | lr 0.000289837 | gnorm 1.216 | train_wall 323 | gb_free 6.1 | wall 66032
KL Stats: Epoch 186 Divergences: Uniform: 3.178192004594494 Unigram: 3.8248598549136363
2022-01-29 01:10:48 | INFO | fairseq.trainer | begin training epoch 187
2022-01-29 01:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:16:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 01:16:40 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 10.262 | ppl 1228.09 | wps 7949.6 | wpb 2034.1 | bsz 4 | num_updates 11968 | best_loss 9.247
2022-01-29 01:16:40 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-01-29 01:16:40 | INFO | train | epoch 187 | loss 5.143 | ppl 35.34 | wps 5924.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11968 | lr 0.000289061 | gnorm 1.213 | train_wall 323 | gb_free 6.1 | wall 66385
KL Stats: Epoch 187 Divergences: Uniform: 3.1787241585668435 Unigram: 3.8312455607793936
2022-01-29 01:16:40 | INFO | fairseq.trainer | begin training epoch 188
2022-01-29 01:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:19:23 | INFO | train_inner | epoch 188:     32 / 64 loss=5.136, ppl=35.17, wps=5798.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12000, lr=0.000288675, gnorm=1.213, train_wall=505, gb_free=6.1, wall=66548
2022-01-29 01:22:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:22:32 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.245 | ppl 1213.76 | wps 7979 | wpb 2034.1 | bsz 4 | num_updates 12032 | best_loss 9.247
2022-01-29 01:22:32 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-01-29 01:22:32 | INFO | train | epoch 188 | loss 5.136 | ppl 35.17 | wps 5929.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12032 | lr 0.000288291 | gnorm 1.228 | train_wall 323 | gb_free 6.1 | wall 66737
KL Stats: Epoch 188 Divergences: Uniform: 3.180325158902713 Unigram: 3.839727385225055
2022-01-29 01:22:32 | INFO | fairseq.trainer | begin training epoch 189
2022-01-29 01:22:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:28:25 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 10.222 | ppl 1194.24 | wps 7971.7 | wpb 2034.1 | bsz 4 | num_updates 12096 | best_loss 9.247
2022-01-29 01:28:25 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-01-29 01:28:25 | INFO | train | epoch 189 | loss 5.134 | ppl 35.11 | wps 5925.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12096 | lr 0.000287527 | gnorm 1.288 | train_wall 323 | gb_free 6.1 | wall 67090
KL Stats: Epoch 189 Divergences: Uniform: 3.184218246990768 Unigram: 3.840302909050208
2022-01-29 01:28:25 | INFO | fairseq.trainer | begin training epoch 190
2022-01-29 01:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:28:45 | INFO | train_inner | epoch 190:      4 / 64 loss=5.139, ppl=35.24, wps=5798.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12100, lr=0.00028748, gnorm=1.274, train_wall=505, gb_free=6.1, wall=67110
2022-01-29 01:33:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 01:34:18 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 10.256 | ppl 1223.1 | wps 7944 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 9.247
2022-01-29 01:34:18 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-01-29 01:34:18 | INFO | train | epoch 190 | loss 5.127 | ppl 34.94 | wps 5916.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12160 | lr 0.00028677 | gnorm 1.207 | train_wall 324 | gb_free 6.1 | wall 67443
KL Stats: Epoch 190 Divergences: Uniform: 3.1839567348261806 Unigram: 3.8433191028109035
2022-01-29 01:34:18 | INFO | fairseq.trainer | begin training epoch 191
2022-01-29 01:34:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:37:42 | INFO | train_inner | epoch 191:     40 / 64 loss=5.117, ppl=34.69, wps=6088.5, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12200, lr=0.000286299, gnorm=1.232, train_wall=507, gb_free=6.1, wall=67647
2022-01-29 01:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 01:40:11 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 10.225 | ppl 1196.52 | wps 7958.9 | wpb 2034.1 | bsz 4 | num_updates 12224 | best_loss 9.247
2022-01-29 01:40:11 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-01-29 01:40:11 | INFO | train | epoch 191 | loss 5.124 | ppl 34.87 | wps 5913.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12224 | lr 0.000286018 | gnorm 1.257 | train_wall 324 | gb_free 6.1 | wall 67796
KL Stats: Epoch 191 Divergences: Uniform: 3.181850240963912 Unigram: 3.846259483696353
2022-01-29 01:40:11 | INFO | fairseq.trainer | begin training epoch 192
2022-01-29 01:40:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:45:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:46:03 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 10.3 | ppl 1260.62 | wps 7962.7 | wpb 2034.1 | bsz 4 | num_updates 12288 | best_loss 9.247
2022-01-29 01:46:03 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-01-29 01:46:03 | INFO | train | epoch 192 | loss 5.119 | ppl 34.76 | wps 5927.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12288 | lr 0.000285272 | gnorm 1.29 | train_wall 323 | gb_free 6.1 | wall 68148
KL Stats: Epoch 192 Divergences: Uniform: 3.1811958447171484 Unigram: 3.8425164722200504
2022-01-29 01:46:03 | INFO | fairseq.trainer | begin training epoch 193
2022-01-29 01:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:47:05 | INFO | train_inner | epoch 193:     12 / 64 loss=5.126, ppl=34.92, wps=5794.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12300, lr=0.000285133, gnorm=1.278, train_wall=505, gb_free=6.1, wall=68210
2022-01-29 01:51:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:51:56 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 10.225 | ppl 1196.47 | wps 7965.3 | wpb 2034.1 | bsz 4 | num_updates 12352 | best_loss 9.247
2022-01-29 01:51:56 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-01-29 01:51:56 | INFO | train | epoch 193 | loss 5.116 | ppl 34.67 | wps 5919.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12352 | lr 0.000284532 | gnorm 1.28 | train_wall 324 | gb_free 6.1 | wall 68501
KL Stats: Epoch 193 Divergences: Uniform: 3.1876401714997136 Unigram: 3.8520504733058964
2022-01-29 01:51:56 | INFO | fairseq.trainer | begin training epoch 194
2022-01-29 01:51:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:56:01 | INFO | train_inner | epoch 194:     48 / 64 loss=5.112, ppl=34.58, wps=6097.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12400, lr=0.000283981, gnorm=1.287, train_wall=506, gb_free=6.1, wall=68746
2022-01-29 01:57:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:57:48 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 10.23 | ppl 1200.99 | wps 8000.7 | wpb 2034.1 | bsz 4 | num_updates 12416 | best_loss 9.247
2022-01-29 01:57:48 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-01-29 01:57:48 | INFO | train | epoch 194 | loss 5.111 | ppl 34.55 | wps 5934.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12416 | lr 0.000283798 | gnorm 1.307 | train_wall 323 | gb_free 6.1 | wall 68853
KL Stats: Epoch 194 Divergences: Uniform: 3.1854011816709664 Unigram: 3.85813656246932
2022-01-29 01:57:48 | INFO | fairseq.trainer | begin training epoch 195
2022-01-29 01:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:03:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:03:40 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 10.256 | ppl 1222.52 | wps 7961.1 | wpb 2034.1 | bsz 4 | num_updates 12480 | best_loss 9.247
2022-01-29 02:03:40 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-01-29 02:03:40 | INFO | train | epoch 195 | loss 5.108 | ppl 34.48 | wps 5929.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12480 | lr 0.000283069 | gnorm 1.341 | train_wall 323 | gb_free 6.1 | wall 69205
KL Stats: Epoch 195 Divergences: Uniform: 3.1869818812944706 Unigram: 3.8572184974968406
2022-01-29 02:03:40 | INFO | fairseq.trainer | begin training epoch 196
2022-01-29 02:03:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:05:22 | INFO | train_inner | epoch 196:     20 / 64 loss=5.106, ppl=34.43, wps=5803.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12500, lr=0.000282843, gnorm=1.334, train_wall=504, gb_free=6.1, wall=69307
2022-01-29 02:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:09:33 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 10.215 | ppl 1188.65 | wps 7946.6 | wpb 2034.1 | bsz 4 | num_updates 12544 | best_loss 9.247
2022-01-29 02:09:33 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-01-29 02:09:33 | INFO | train | epoch 196 | loss 5.104 | ppl 34.38 | wps 5925.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12544 | lr 0.000282346 | gnorm 1.317 | train_wall 323 | gb_free 6.1 | wall 69558
KL Stats: Epoch 196 Divergences: Uniform: 3.1856849311036215 Unigram: 3.8620593495967914
2022-01-29 02:09:33 | INFO | fairseq.trainer | begin training epoch 197
2022-01-29 02:09:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:14:18 | INFO | train_inner | epoch 197:     56 / 64 loss=5.103, ppl=34.37, wps=6097.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12600, lr=0.000281718, gnorm=1.323, train_wall=506, gb_free=6.1, wall=69843
2022-01-29 02:14:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:15:25 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 10.203 | ppl 1178.96 | wps 7953 | wpb 2034.1 | bsz 4 | num_updates 12608 | best_loss 9.247
2022-01-29 02:15:25 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-01-29 02:15:25 | INFO | train | epoch 197 | loss 5.098 | ppl 34.26 | wps 5927.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12608 | lr 0.000281629 | gnorm 1.333 | train_wall 323 | gb_free 6.1 | wall 69910
KL Stats: Epoch 197 Divergences: Uniform: 3.1864009005093226 Unigram: 3.8639310890973224
2022-01-29 02:15:25 | INFO | fairseq.trainer | begin training epoch 198
2022-01-29 02:15:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:20:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:21:18 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 10.258 | ppl 1224.31 | wps 7991.2 | wpb 2034.1 | bsz 4 | num_updates 12672 | best_loss 9.247
2022-01-29 02:21:18 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-01-29 02:21:18 | INFO | train | epoch 198 | loss 5.093 | ppl 34.14 | wps 5925 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12672 | lr 0.000280917 | gnorm 1.353 | train_wall 323 | gb_free 6.1 | wall 70263
KL Stats: Epoch 198 Divergences: Uniform: 3.191508264156431 Unigram: 3.869806668094884
2022-01-29 02:21:18 | INFO | fairseq.trainer | begin training epoch 199
2022-01-29 02:21:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:23:40 | INFO | train_inner | epoch 199:     28 / 64 loss=5.089, ppl=34.03, wps=5800.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12700, lr=0.000280607, gnorm=1.342, train_wall=504, gb_free=6.1, wall=70405
2022-01-29 02:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:27:10 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 10.236 | ppl 1206.21 | wps 7997.9 | wpb 2034.1 | bsz 4 | num_updates 12736 | best_loss 9.247
2022-01-29 02:27:10 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-01-29 02:27:10 | INFO | train | epoch 199 | loss 5.089 | ppl 34.04 | wps 5933.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12736 | lr 0.00028021 | gnorm 1.3 | train_wall 323 | gb_free 6.1 | wall 70615
KL Stats: Epoch 199 Divergences: Uniform: 3.191794121365943 Unigram: 3.879257312204062
2022-01-29 02:27:10 | INFO | fairseq.trainer | begin training epoch 200
2022-01-29 02:27:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:32:34 | INFO | train_inner | epoch 200:     64 / 64 loss=5.097, ppl=34.24, wps=6107.3, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=12800, lr=0.000279508, gnorm=1.326, train_wall=504, gb_free=6.1, wall=70939
2022-01-29 02:32:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:33:01 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 10.314 | ppl 1272.81 | wps 7979.8 | wpb 2034.1 | bsz 4 | num_updates 12800 | best_loss 9.247
2022-01-29 02:33:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 12800 updates
2022-01-29 02:33:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint200.pt
2022-01-29 02:33:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint200.pt
2022-01-29 02:33:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9/checkpoint200.pt (epoch 200 @ 12800 updates, score 10.314) (writing took 3.362935333047062 seconds)
2022-01-29 02:33:05 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-01-29 02:33:05 | INFO | train | epoch 200 | loss 5.086 | ppl 33.97 | wps 5881.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12800 | lr 0.000279508 | gnorm 1.343 | train_wall 323 | gb_free 6.1 | wall 70970
KL Stats: Epoch 200 Divergences: Uniform: 3.186383800405002 Unigram: 3.8750800809170265
2022-01-29 02:33:05 | INFO | fairseq.trainer | begin training epoch 201
2022-01-29 02:33:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:38:57 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 10.253 | ppl 1220.34 | wps 7992.9 | wpb 2034.1 | bsz 4 | num_updates 12864 | best_loss 9.247
2022-01-29 02:38:57 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-01-29 02:38:57 | INFO | train | epoch 201 | loss 5.084 | ppl 33.91 | wps 5937.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12864 | lr 0.000278812 | gnorm 1.351 | train_wall 323 | gb_free 6.1 | wall 71322
KL Stats: Epoch 201 Divergences: Uniform: 3.188740128366036 Unigram: 3.877036737855995
2022-01-29 02:38:57 | INFO | fairseq.trainer | begin training epoch 202
2022-01-29 02:38:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:42:00 | INFO | train_inner | epoch 202:     36 / 64 loss=5.07, ppl=33.59, wps=5773.5, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=12900, lr=0.000278423, gnorm=1.369, train_wall=505, gb_free=6.1, wall=71505
2022-01-29 02:44:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:44:49 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 10.258 | ppl 1224.78 | wps 7954 | wpb 2034.1 | bsz 4 | num_updates 12928 | best_loss 9.247
2022-01-29 02:44:49 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-01-29 02:44:49 | INFO | train | epoch 202 | loss 5.078 | ppl 33.78 | wps 5929.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12928 | lr 0.000278121 | gnorm 1.375 | train_wall 323 | gb_free 6.1 | wall 71674
KL Stats: Epoch 202 Divergences: Uniform: 3.188276330259605 Unigram: 3.880647571680549
2022-01-29 02:44:49 | INFO | fairseq.trainer | begin training epoch 203
2022-01-29 02:44:49 | INFO | fairseq_cli.train | Start iterating over samples
User defined signal 2
