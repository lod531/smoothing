Sender: LSF System <lsfadmin@eu-g3-010>
Subject: Job 208727102: <ru_dropout_0.3_jelinek_0.0_0.025_0.975_#1> in cluster <euler> Done

Job <ru_dropout_0.3_jelinek_0.0_0.025_0.975_#1> was submitted from host <eu-login-24> by user <andriusb> in cluster <euler> at Tue Mar 15 16:39:37 2022
Job was executed on host(s) <eu-g3-010>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 16:40:04 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 16:40:04 2022
Terminated at Wed Mar 16 14:28:13 2022
Results reported at Wed Mar 16 14:28:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/ru --save-dir /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.0,0.025,0.975\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   78356.06 sec.
    Max Memory :                                 4154 MB
    Average Memory :                             3262.12 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15846.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   78489 sec.
    Turnaround time :                            78516 sec.

The output (if any) follows:

2022-03-15 16:40:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/ru', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.0,0.025,0.975)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 16:40:20 | INFO | fairseq.tasks.language_modeling | dictionary: 35920 types
2022-03-15 16:40:21 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
Calculating frequency stats:
  0%|          | 0/53136 [00:00<?, ?it/s]  0%|          | 77/53136 [00:00<01:09, 767.07it/s]  0%|          | 154/53136 [00:00<01:10, 747.15it/s]  0%|          | 242/53136 [00:00<01:05, 805.63it/s]  1%|          | 323/53136 [00:00<01:09, 765.13it/s]  1%|          | 400/53136 [00:00<01:11, 733.37it/s]  1%|          | 498/53136 [00:00<01:04, 810.55it/s]  1%|          | 589/53136 [00:00<01:02, 836.71it/s]  1%|▏         | 674/53136 [00:00<01:02, 840.23it/s]  1%|▏         | 759/53136 [00:00<01:02, 841.62it/s]  2%|▏         | 844/53136 [00:01<01:11, 729.72it/s]  2%|▏         | 940/53136 [00:01<01:06, 782.52it/s]  2%|▏         | 1021/53136 [00:01<01:09, 753.03it/s]  2%|▏         | 1098/53136 [00:01<01:10, 738.52it/s]  2%|▏         | 1173/53136 [00:01<01:11, 730.68it/s]  2%|▏         | 1261/53136 [00:01<01:07, 770.87it/s]  3%|▎         | 1339/53136 [00:01<01:08, 761.40it/s]  3%|▎         | 1430/53136 [00:01<01:04, 802.35it/s]  3%|▎         | 1511/53136 [00:01<01:04, 802.78it/s]  3%|▎         | 1618/53136 [00:02<00:58, 878.08it/s]  3%|▎         | 1733/53136 [00:02<00:53, 955.09it/s]  3%|▎         | 1829/53136 [00:02<00:54, 936.09it/s]  4%|▎         | 1923/53136 [00:02<00:58, 873.22it/s]  4%|▍         | 2012/53136 [00:02<00:59, 853.43it/s]  4%|▍         | 2108/53136 [00:02<00:57, 881.43it/s]  4%|▍         | 2197/53136 [00:02<00:59, 862.46it/s]  4%|▍         | 2304/53136 [00:02<00:55, 915.37it/s]  5%|▍         | 2397/53136 [00:02<00:56, 904.48it/s]  5%|▍         | 2488/53136 [00:03<01:07, 748.12it/s]  5%|▍         | 2578/53136 [00:03<01:04, 785.13it/s]  5%|▌         | 2675/53136 [00:03<01:00, 831.42it/s]  5%|▌         | 2772/53136 [00:03<00:57, 869.41it/s]  5%|▌         | 2862/53136 [00:03<00:58, 862.37it/s]  6%|▌         | 2951/53136 [00:03<01:05, 771.11it/s]  6%|▌         | 3034/53136 [00:03<01:05, 767.97it/s]  6%|▌         | 3113/53136 [00:03<01:15, 661.96it/s]  6%|▌         | 3188/53136 [00:04<01:19, 630.99it/s]  6%|▌         | 3275/53136 [00:04<01:12, 689.15it/s]  6%|▋         | 3356/53136 [00:04<01:09, 717.38it/s]  6%|▋         | 3431/53136 [00:04<01:11, 692.77it/s]  7%|▋         | 3527/53136 [00:04<01:05, 762.78it/s]  7%|▋         | 3606/53136 [00:04<01:14, 669.25it/s]  7%|▋         | 3680/53136 [00:04<01:12, 679.38it/s]  7%|▋         | 3751/53136 [00:04<01:13, 672.86it/s]  7%|▋         | 3838/53136 [00:04<01:07, 725.65it/s]  7%|▋         | 3913/53136 [00:05<01:07, 724.80it/s]  8%|▊         | 4018/53136 [00:05<01:02, 792.18it/s]  8%|▊         | 4099/53136 [00:05<01:01, 796.18it/s]  8%|▊         | 4192/53136 [00:05<00:59, 819.59it/s]  8%|▊         | 4275/53136 [00:05<00:59, 821.71it/s]  8%|▊         | 4358/53136 [00:05<01:05, 748.88it/s]  8%|▊         | 4435/53136 [00:05<01:07, 726.81it/s]  8%|▊         | 4512/53136 [00:05<01:06, 730.02it/s]  9%|▊         | 4586/53136 [00:05<01:07, 723.20it/s]  9%|▉         | 4676/53136 [00:05<01:02, 770.72it/s]  9%|▉         | 4754/53136 [00:06<01:03, 760.13it/s]  9%|▉         | 4831/53136 [00:06<01:14, 646.82it/s]  9%|▉         | 4900/53136 [00:06<01:13, 655.13it/s]  9%|▉         | 4981/53136 [00:06<01:09, 694.29it/s] 10%|▉         | 5069/53136 [00:06<01:04, 744.05it/s] 10%|▉         | 5146/53136 [00:06<01:12, 663.88it/s] 10%|▉         | 5219/53136 [00:06<01:10, 680.02it/s] 10%|▉         | 5291/53136 [00:06<01:09, 688.77it/s] 10%|█         | 5362/53136 [00:07<01:16, 626.34it/s] 10%|█         | 5452/53136 [00:07<01:08, 697.33it/s] 10%|█         | 5543/53136 [00:07<01:03, 753.62it/s] 11%|█         | 5628/53136 [00:07<01:00, 779.84it/s] 11%|█         | 5708/53136 [00:07<01:01, 771.42it/s] 11%|█         | 5793/53136 [00:07<00:59, 792.19it/s] 11%|█         | 5874/53136 [00:07<00:59, 788.51it/s] 11%|█         | 5954/53136 [00:07<01:03, 746.23it/s] 11%|█▏        | 6059/53136 [00:07<00:56, 827.40it/s] 12%|█▏        | 6143/53136 [00:08<00:58, 809.07it/s] 12%|█▏        | 6225/53136 [00:08<01:08, 689.39it/s] 12%|█▏        | 6305/53136 [00:08<01:05, 717.59it/s] 12%|█▏        | 6403/53136 [00:08<00:59, 786.32it/s] 12%|█▏        | 6489/53136 [00:08<00:57, 804.42it/s] 12%|█▏        | 6596/53136 [00:08<00:53, 877.13it/s] 13%|█▎        | 6693/53136 [00:08<00:51, 902.46it/s] 13%|█▎        | 6788/53136 [00:08<00:50, 916.13it/s] 13%|█▎        | 6881/53136 [00:08<00:57, 811.47it/s] 13%|█▎        | 6965/53136 [00:09<01:11, 645.46it/s] 13%|█▎        | 7054/53136 [00:09<01:05, 701.18it/s] 13%|█▎        | 7136/53136 [00:09<01:02, 730.74it/s] 14%|█▎        | 7228/53136 [00:09<00:58, 779.91it/s] 14%|█▍        | 7314/53136 [00:09<00:57, 790.74it/s] 14%|█▍        | 7397/53136 [00:09<01:17, 591.74it/s] 14%|█▍        | 7473/53136 [00:09<01:17, 587.91it/s] 14%|█▍        | 7558/53136 [00:09<01:10, 648.16it/s] 14%|█▍        | 7647/53136 [00:10<01:04, 707.68it/s] 15%|█▍        | 7732/53136 [00:10<01:01, 743.75it/s] 15%|█▍        | 7825/53136 [00:10<00:57, 792.55it/s] 15%|█▍        | 7922/53136 [00:10<00:53, 841.18it/s] 15%|█▌        | 8009/53136 [00:10<00:54, 833.95it/s] 15%|█▌        | 8095/53136 [00:10<00:56, 802.91it/s] 15%|█▌        | 8177/53136 [00:10<00:57, 776.87it/s] 16%|█▌        | 8256/53136 [00:10<01:07, 668.08it/s] 16%|█▌        | 8332/53136 [00:10<01:04, 691.25it/s] 16%|█▌        | 8412/53136 [00:11<01:02, 720.08it/s] 16%|█▌        | 8487/53136 [00:11<01:01, 727.73it/s] 16%|█▌        | 8581/53136 [00:11<00:56, 786.31it/s] 16%|█▋        | 8662/53136 [00:11<01:01, 719.40it/s] 16%|█▋        | 8736/53136 [00:11<01:04, 689.08it/s] 17%|█▋        | 8827/53136 [00:11<00:59, 743.63it/s] 17%|█▋        | 8905/53136 [00:11<00:58, 752.67it/s] 17%|█▋        | 8982/53136 [00:11<01:00, 731.04it/s] 17%|█▋        | 9056/53136 [00:11<01:02, 708.80it/s] 17%|█▋        | 9148/53136 [00:12<00:57, 766.82it/s] 17%|█▋        | 9239/53136 [00:12<00:54, 807.35it/s] 18%|█▊        | 9321/53136 [00:12<01:04, 675.76it/s] 18%|█▊        | 9420/53136 [00:12<00:58, 753.27it/s] 18%|█▊        | 9514/53136 [00:12<00:54, 798.62it/s] 18%|█▊        | 9606/53136 [00:12<00:52, 831.43it/s] 18%|█▊        | 9692/53136 [00:12<00:53, 810.67it/s] 18%|█▊        | 9775/53136 [00:12<00:58, 737.49it/s] 19%|█▊        | 9852/53136 [00:13<01:17, 560.96it/s] 19%|█▊        | 9920/53136 [00:13<01:13, 586.49it/s] 19%|█▉        | 9985/53136 [00:13<01:15, 571.28it/s] 19%|█▉        | 10091/53136 [00:13<01:02, 689.14it/s] 19%|█▉        | 10177/53136 [00:13<00:58, 732.19it/s] 19%|█▉        | 10255/53136 [00:13<01:02, 681.17it/s] 19%|█▉        | 10331/53136 [00:13<01:01, 698.36it/s] 20%|█▉        | 10404/53136 [00:13<01:00, 701.15it/s] 20%|█▉        | 10477/53136 [00:14<01:05, 649.28it/s] 20%|█▉        | 10544/53136 [00:14<01:10, 604.96it/s] 20%|█▉        | 10607/53136 [00:14<01:09, 610.07it/s] 20%|██        | 10695/53136 [00:14<01:02, 681.83it/s] 20%|██        | 10769/53136 [00:14<01:00, 694.70it/s] 20%|██        | 10840/53136 [00:14<01:02, 676.99it/s] 21%|██        | 10940/53136 [00:14<00:55, 765.48it/s] 21%|██        | 11040/53136 [00:14<00:50, 830.78it/s] 21%|██        | 11125/53136 [00:14<01:07, 618.51it/s] 21%|██        | 11204/53136 [00:15<01:03, 658.47it/s] 21%|██        | 11277/53136 [00:15<01:02, 673.98it/s] 21%|██▏       | 11353/53136 [00:15<01:00, 696.36it/s] 22%|██▏       | 11427/53136 [00:15<01:01, 675.85it/s] 22%|██▏       | 11502/53136 [00:15<01:03, 659.58it/s] 22%|██▏       | 11570/53136 [00:15<01:02, 662.05it/s] 22%|██▏       | 11647/53136 [00:15<00:59, 691.52it/s] 22%|██▏       | 11718/53136 [00:15<00:59, 695.02it/s] 22%|██▏       | 11800/53136 [00:15<00:57, 724.30it/s] 22%|██▏       | 11900/53136 [00:16<00:51, 797.76it/s] 23%|██▎       | 11981/53136 [00:16<00:54, 758.95it/s] 23%|██▎       | 12058/53136 [00:16<00:58, 704.87it/s] 23%|██▎       | 12130/53136 [00:16<01:00, 678.12it/s] 23%|██▎       | 12199/53136 [00:16<01:03, 639.69it/s] 23%|██▎       | 12302/53136 [00:16<00:55, 741.90it/s] 23%|██▎       | 12406/53136 [00:16<00:49, 822.43it/s] 24%|██▎       | 12498/53136 [00:16<00:47, 847.99it/s] 24%|██▎       | 12585/53136 [00:17<01:00, 670.55it/s] 24%|██▍       | 12659/53136 [00:17<01:05, 620.83it/s] 24%|██▍       | 12732/53136 [00:17<01:02, 646.22it/s] 24%|██▍       | 12801/53136 [00:17<01:07, 599.40it/s] 24%|██▍       | 12887/53136 [00:17<01:00, 663.76it/s] 24%|██▍       | 12982/53136 [00:17<00:54, 737.58it/s] 25%|██▍       | 13061/53136 [00:17<00:53, 750.33it/s] 25%|██▍       | 13139/53136 [00:17<00:52, 757.27it/s] 25%|██▍       | 13217/53136 [00:17<00:52, 763.24it/s] 25%|██▌       | 13297/53136 [00:18<00:51, 773.27it/s] 25%|██▌       | 13376/53136 [00:18<00:51, 773.28it/s] 25%|██▌       | 13459/53136 [00:18<00:50, 788.46it/s] 25%|██▌       | 13539/53136 [00:18<00:57, 690.25it/s] 26%|██▌       | 13624/53136 [00:18<00:53, 732.75it/s] 26%|██▌       | 13700/53136 [00:18<00:53, 733.88it/s] 26%|██▌       | 13798/53136 [00:18<00:48, 802.84it/s] 26%|██▌       | 13880/53136 [00:18<00:52, 746.24it/s] 26%|██▋       | 13957/53136 [00:18<01:00, 642.45it/s] 26%|██▋       | 14031/53136 [00:19<00:58, 666.60it/s] 27%|██▋       | 14113/53136 [00:19<00:55, 705.86it/s] 27%|██▋       | 14187/53136 [00:19<00:58, 666.77it/s] 27%|██▋       | 14267/53136 [00:19<00:56, 693.37it/s] 27%|██▋       | 14338/53136 [00:19<00:56, 682.46it/s] 27%|██▋       | 14408/53136 [00:19<00:58, 663.07it/s] 27%|██▋       | 14484/53136 [00:19<00:56, 684.29it/s] 27%|██▋       | 14565/53136 [00:19<00:53, 716.95it/s] 28%|██▊       | 14685/53136 [00:19<00:45, 839.73it/s] 28%|██▊       | 14777/53136 [00:20<00:44, 860.80it/s] 28%|██▊       | 14864/53136 [00:20<00:45, 832.95it/s] 28%|██▊       | 14948/53136 [00:20<00:47, 807.33it/s] 28%|██▊       | 15030/53136 [00:20<00:56, 676.83it/s] 28%|██▊       | 15102/53136 [00:20<00:57, 657.83it/s] 29%|██▊       | 15186/53136 [00:20<00:53, 704.27it/s] 29%|██▊       | 15259/53136 [00:20<01:00, 630.89it/s] 29%|██▉       | 15341/53136 [00:20<00:56, 666.46it/s] 29%|██▉       | 15411/53136 [00:21<01:00, 624.30it/s] 29%|██▉       | 15478/53136 [00:21<00:59, 635.85it/s] 29%|██▉       | 15558/53136 [00:21<00:55, 672.93it/s] 29%|██▉       | 15628/53136 [00:21<00:55, 673.43it/s] 30%|██▉       | 15701/53136 [00:21<00:54, 688.35it/s] 30%|██▉       | 15789/53136 [00:21<00:50, 740.63it/s] 30%|██▉       | 15864/53136 [00:21<01:00, 619.07it/s] 30%|███       | 15942/53136 [00:21<00:56, 656.88it/s] 30%|███       | 16021/53136 [00:21<00:53, 688.15it/s] 30%|███       | 16099/53136 [00:21<00:51, 712.95it/s] 30%|███       | 16173/53136 [00:22<00:51, 713.06it/s] 31%|███       | 16246/53136 [00:22<00:56, 655.79it/s] 31%|███       | 16339/53136 [00:22<00:51, 709.23it/s] 31%|███       | 16422/53136 [00:22<00:49, 740.42it/s] 31%|███       | 16512/53136 [00:22<00:46, 780.92it/s] 31%|███       | 16601/53136 [00:22<00:45, 810.75it/s] 31%|███▏      | 16685/53136 [00:22<00:47, 768.82it/s] 32%|███▏      | 16777/53136 [00:22<00:44, 808.48it/s] 32%|███▏      | 16869/53136 [00:22<00:43, 834.35it/s] 32%|███▏      | 16954/53136 [00:23<00:44, 812.95it/s] 32%|███▏      | 17036/53136 [00:23<00:46, 783.82it/s] 32%|███▏      | 17130/53136 [00:23<00:43, 826.91it/s] 32%|███▏      | 17214/53136 [00:23<00:43, 829.03it/s] 33%|███▎      | 17298/53136 [00:23<00:45, 780.40it/s] 33%|███▎      | 17379/53136 [00:23<00:45, 788.30it/s] 33%|███▎      | 17459/53136 [00:23<00:46, 759.47it/s] 33%|███▎      | 17536/53136 [00:23<00:50, 699.07it/s] 33%|███▎      | 17623/53136 [00:23<00:47, 743.62it/s] 33%|███▎      | 17707/53136 [00:24<00:46, 767.26it/s] 33%|███▎      | 17789/53136 [00:24<00:46, 754.12it/s] 34%|███▎      | 17866/53136 [00:24<00:48, 725.94it/s] 34%|███▍      | 17940/53136 [00:24<00:54, 644.02it/s] 34%|███▍      | 18032/53136 [00:24<00:49, 715.35it/s] 34%|███▍      | 18112/53136 [00:24<00:47, 737.92it/s] 34%|███▍      | 18188/53136 [00:24<00:48, 724.95it/s] 34%|███▍      | 18262/53136 [00:24<00:53, 657.12it/s] 34%|███▍      | 18330/53136 [00:25<00:55, 629.79it/s] 35%|███▍      | 18401/53136 [00:25<00:55, 626.34it/s] 35%|███▍      | 18469/53136 [00:25<00:54, 639.26it/s] 35%|███▍      | 18534/53136 [00:25<00:55, 621.00it/s] 35%|███▌      | 18603/53136 [00:25<00:54, 631.96it/s] 35%|███▌      | 18667/53136 [00:25<00:55, 624.74it/s] 35%|███▌      | 18754/53136 [00:25<00:49, 693.19it/s] 35%|███▌      | 18824/53136 [00:25<00:51, 666.13it/s] 36%|███▌      | 18922/53136 [00:25<00:45, 753.87it/s] 36%|███▌      | 19009/53136 [00:25<00:43, 783.78it/s] 36%|███▌      | 19094/53136 [00:26<00:42, 799.91it/s] 36%|███▌      | 19175/53136 [00:26<00:42, 791.54it/s] 36%|███▋      | 19265/53136 [00:26<00:47, 707.67it/s] 36%|███▋      | 19354/53136 [00:26<00:45, 748.11it/s] 37%|███▋      | 19431/53136 [00:26<00:45, 747.34it/s] 37%|███▋      | 19508/53136 [00:26<00:49, 682.16it/s] 37%|███▋      | 19586/53136 [00:26<00:53, 631.68it/s] 37%|███▋      | 19686/53136 [00:26<00:46, 724.03it/s] 37%|███▋      | 19762/53136 [00:27<00:49, 677.83it/s] 37%|███▋      | 19833/53136 [00:27<00:49, 675.83it/s] 37%|███▋      | 19912/53136 [00:27<00:47, 705.55it/s] 38%|███▊      | 19999/53136 [00:27<00:44, 749.82it/s] 38%|███▊      | 20091/53136 [00:27<00:41, 797.02it/s] 38%|███▊      | 20172/53136 [00:27<00:42, 780.90it/s] 38%|███▊      | 20252/53136 [00:27<00:53, 613.76it/s] 38%|███▊      | 20323/53136 [00:27<00:52, 626.75it/s] 38%|███▊      | 20393/53136 [00:27<00:50, 644.32it/s] 39%|███▊      | 20469/53136 [00:28<00:48, 675.01it/s] 39%|███▊      | 20541/53136 [00:28<00:55, 585.23it/s] 39%|███▉      | 20608/53136 [00:28<00:53, 603.50it/s] 39%|███▉      | 20690/53136 [00:28<00:49, 658.11it/s] 39%|███▉      | 20788/53136 [00:28<00:43, 743.21it/s] 39%|███▉      | 20866/53136 [00:28<00:45, 713.33it/s] 39%|███▉      | 20950/53136 [00:28<00:44, 719.16it/s] 40%|███▉      | 21030/53136 [00:28<00:43, 738.59it/s] 40%|███▉      | 21106/53136 [00:28<00:43, 740.20it/s] 40%|███▉      | 21181/53136 [00:29<00:52, 610.02it/s] 40%|████      | 21269/53136 [00:29<00:47, 677.23it/s] 40%|████      | 21353/53136 [00:29<00:44, 717.07it/s] 40%|████      | 21429/53136 [00:29<00:44, 711.34it/s] 41%|████      | 21528/53136 [00:29<00:40, 787.79it/s] 41%|████      | 21610/53136 [00:29<00:44, 712.66it/s] 41%|████      | 21685/53136 [00:29<00:50, 616.93it/s] 41%|████      | 21751/53136 [00:30<00:56, 555.93it/s] 41%|████      | 21825/53136 [00:30<00:52, 598.18it/s] 41%|████      | 21917/53136 [00:30<00:45, 678.97it/s] 41%|████▏     | 21995/53136 [00:30<00:44, 704.45it/s] 42%|████▏     | 22069/53136 [00:30<00:47, 655.63it/s] 42%|████▏     | 22143/53136 [00:30<00:45, 673.84it/s] 42%|████▏     | 22224/53136 [00:30<00:43, 710.38it/s] 42%|████▏     | 22300/53136 [00:30<00:43, 712.57it/s] 42%|████▏     | 22373/53136 [00:30<00:43, 699.95it/s] 42%|████▏     | 22461/53136 [00:30<00:41, 745.03it/s] 42%|████▏     | 22539/53136 [00:31<00:40, 753.12it/s] 43%|████▎     | 22621/53136 [00:31<00:39, 772.24it/s] 43%|████▎     | 22710/53136 [00:31<00:37, 803.34it/s] 43%|████▎     | 22791/53136 [00:31<00:38, 786.64it/s] 43%|████▎     | 22870/53136 [00:31<00:40, 742.33it/s] 43%|████▎     | 22962/53136 [00:31<00:38, 790.45it/s] 43%|████▎     | 23042/53136 [00:31<00:46, 650.17it/s] 44%|████▎     | 23139/53136 [00:31<00:41, 729.41it/s] 44%|████▎     | 23217/53136 [00:32<00:41, 728.51it/s] 44%|████▍     | 23301/53136 [00:32<00:43, 691.38it/s] 44%|████▍     | 23387/53136 [00:32<00:41, 722.84it/s] 44%|████▍     | 23462/53136 [00:32<00:41, 712.07it/s] 44%|████▍     | 23535/53136 [00:32<00:44, 658.21it/s] 44%|████▍     | 23603/53136 [00:32<00:51, 577.07it/s] 45%|████▍     | 23669/53136 [00:32<00:49, 595.96it/s] 45%|████▍     | 23778/53136 [00:32<00:40, 722.31it/s] 45%|████▍     | 23854/53136 [00:33<00:45, 642.65it/s] 45%|████▌     | 23935/53136 [00:33<00:42, 681.08it/s] 45%|████▌     | 24007/53136 [00:33<00:42, 683.91it/s] 45%|████▌     | 24091/53136 [00:33<00:40, 722.72it/s] 46%|████▌     | 24179/53136 [00:33<00:38, 761.68it/s] 46%|████▌     | 24261/53136 [00:33<00:37, 773.67it/s] 46%|████▌     | 24341/53136 [00:33<00:37, 777.10it/s] 46%|████▌     | 24420/53136 [00:33<00:37, 761.60it/s] 46%|████▌     | 24513/53136 [00:33<00:35, 807.20it/s] 46%|████▋     | 24609/53136 [00:33<00:33, 849.51it/s] 46%|████▋     | 24698/53136 [00:34<00:33, 858.46it/s] 47%|████▋     | 24785/53136 [00:34<00:39, 724.42it/s] 47%|████▋     | 24875/53136 [00:34<00:36, 770.04it/s] 47%|████▋     | 24956/53136 [00:34<00:38, 732.56it/s] 47%|████▋     | 25055/53136 [00:34<00:35, 800.28it/s] 47%|████▋     | 25144/53136 [00:34<00:34, 811.40it/s] 47%|████▋     | 25227/53136 [00:34<00:38, 729.39it/s] 48%|████▊     | 25330/53136 [00:34<00:34, 805.40it/s] 48%|████▊     | 25414/53136 [00:34<00:36, 754.51it/s] 48%|████▊     | 25492/53136 [00:35<00:39, 703.28it/s] 48%|████▊     | 25567/53136 [00:35<00:39, 698.04it/s] 48%|████▊     | 25639/53136 [00:35<00:40, 675.14it/s] 48%|████▊     | 25729/53136 [00:35<00:37, 733.64it/s] 49%|████▊     | 25804/53136 [00:35<00:37, 725.59it/s] 49%|████▊     | 25878/53136 [00:35<00:37, 723.55it/s] 49%|████▉     | 25951/53136 [00:35<00:41, 652.96it/s] 49%|████▉     | 26031/53136 [00:35<00:40, 676.77it/s] 49%|████▉     | 26100/53136 [00:36<00:42, 638.48it/s] 49%|████▉     | 26165/53136 [00:36<00:44, 601.82it/s] 49%|████▉     | 26227/53136 [00:36<00:49, 542.65it/s] 49%|████▉     | 26289/53136 [00:36<00:47, 561.03it/s] 50%|████▉     | 26360/53136 [00:36<00:45, 593.23it/s] 50%|████▉     | 26445/53136 [00:36<00:40, 660.27it/s] 50%|████▉     | 26525/53136 [00:36<00:39, 670.45it/s] 50%|█████     | 26597/53136 [00:36<00:38, 681.28it/s] 50%|█████     | 26671/53136 [00:36<00:38, 692.76it/s] 50%|█████     | 26757/53136 [00:37<00:35, 735.34it/s] 50%|█████     | 26832/53136 [00:37<00:35, 735.49it/s] 51%|█████     | 26906/53136 [00:37<00:38, 679.21it/s] 51%|█████     | 26975/53136 [00:37<00:39, 668.63it/s] 51%|█████     | 27061/53136 [00:37<00:36, 719.53it/s] 51%|█████     | 27134/53136 [00:37<00:36, 706.03it/s] 51%|█████     | 27206/53136 [00:37<00:37, 685.48it/s] 51%|█████▏    | 27279/53136 [00:37<00:37, 694.09it/s] 51%|█████▏    | 27349/53136 [00:37<00:39, 650.39it/s] 52%|█████▏    | 27431/53136 [00:38<00:37, 680.34it/s] 52%|█████▏    | 27500/53136 [00:38<00:39, 651.38it/s] 52%|█████▏    | 27566/53136 [00:38<00:39, 650.60it/s] 52%|█████▏    | 27633/53136 [00:38<00:39, 651.06it/s] 52%|█████▏    | 27708/53136 [00:38<00:37, 678.91it/s] 52%|█████▏    | 27800/53136 [00:38<00:33, 746.81it/s] 52%|█████▏    | 27876/53136 [00:38<00:37, 676.43it/s] 53%|█████▎    | 27951/53136 [00:38<00:36, 693.83it/s] 53%|█████▎    | 28022/53136 [00:38<00:36, 685.98it/s] 53%|█████▎    | 28092/53136 [00:39<00:39, 638.46it/s] 53%|█████▎    | 28159/53136 [00:39<00:38, 644.73it/s] 53%|█████▎    | 28225/53136 [00:39<00:39, 638.01it/s] 53%|█████▎    | 28296/53136 [00:39<00:37, 654.15it/s] 53%|█████▎    | 28375/53136 [00:39<00:35, 692.87it/s] 54%|█████▎    | 28457/53136 [00:39<00:33, 728.01it/s] 54%|█████▍    | 28571/53136 [00:39<00:28, 847.24it/s] 54%|█████▍    | 28657/53136 [00:39<00:30, 808.12it/s] 54%|█████▍    | 28739/53136 [00:39<00:30, 801.22it/s] 54%|█████▍    | 28820/53136 [00:39<00:33, 728.09it/s] 54%|█████▍    | 28911/53136 [00:40<00:31, 777.01it/s] 55%|█████▍    | 28991/53136 [00:40<00:32, 751.75it/s] 55%|█████▍    | 29074/53136 [00:40<00:31, 758.69it/s] 55%|█████▍    | 29151/53136 [00:40<00:32, 730.39it/s] 55%|█████▌    | 29225/53136 [00:40<00:34, 687.19it/s] 55%|█████▌    | 29301/53136 [00:40<00:33, 701.50it/s] 55%|█████▌    | 29372/53136 [00:40<00:35, 677.37it/s] 55%|█████▌    | 29450/53136 [00:40<00:33, 705.13it/s] 56%|█████▌    | 29548/53136 [00:40<00:30, 781.82it/s] 56%|█████▌    | 29628/53136 [00:41<00:31, 746.42it/s] 56%|█████▌    | 29707/53136 [00:41<00:31, 744.09it/s] 56%|█████▌    | 29783/53136 [00:41<00:40, 569.70it/s] 56%|█████▌    | 29852/53136 [00:41<00:39, 596.64it/s] 56%|█████▋    | 29956/53136 [00:41<00:32, 707.36it/s] 57%|█████▋    | 30033/53136 [00:41<00:34, 664.69it/s] 57%|█████▋    | 30104/53136 [00:41<00:43, 531.29it/s] 57%|█████▋    | 30185/53136 [00:42<00:38, 591.91it/s] 57%|█████▋    | 30268/53136 [00:42<00:35, 649.41it/s] 57%|█████▋    | 30361/53136 [00:42<00:31, 721.26it/s] 57%|█████▋    | 30439/53136 [00:42<00:39, 578.14it/s] 57%|█████▋    | 30522/53136 [00:42<00:35, 633.49it/s] 58%|█████▊    | 30593/53136 [00:42<00:35, 632.51it/s] 58%|█████▊    | 30662/53136 [00:42<00:35, 639.06it/s] 58%|█████▊    | 30758/53136 [00:42<00:30, 723.35it/s] 58%|█████▊    | 30834/53136 [00:43<00:32, 679.25it/s] 58%|█████▊    | 30920/53136 [00:43<00:30, 718.60it/s] 58%|█████▊    | 30995/53136 [00:43<00:34, 649.97it/s] 58%|█████▊    | 31076/53136 [00:43<00:31, 690.74it/s] 59%|█████▊    | 31148/53136 [00:43<00:32, 686.14it/s] 59%|█████▉    | 31225/53136 [00:43<00:31, 705.74it/s] 59%|█████▉    | 31320/53136 [00:43<00:28, 759.84it/s] 59%|█████▉    | 31418/53136 [00:43<00:26, 821.41it/s] 59%|█████▉    | 31502/53136 [00:43<00:27, 779.52it/s] 59%|█████▉    | 31586/53136 [00:43<00:27, 793.87it/s] 60%|█████▉    | 31667/53136 [00:44<00:27, 783.61it/s] 60%|█████▉    | 31746/53136 [00:44<00:28, 744.39it/s] 60%|█████▉    | 31845/53136 [00:44<00:26, 811.11it/s] 60%|██████    | 31928/53136 [00:44<00:27, 765.10it/s] 60%|██████    | 32006/53136 [00:44<00:30, 692.73it/s] 60%|██████    | 32092/53136 [00:44<00:28, 736.09it/s] 61%|██████    | 32199/53136 [00:44<00:25, 826.69it/s] 61%|██████    | 32284/53136 [00:44<00:27, 761.51it/s] 61%|██████    | 32363/53136 [00:45<00:30, 684.67it/s] 61%|██████    | 32450/53136 [00:45<00:28, 717.96it/s] 61%|██████    | 32528/53136 [00:45<00:28, 730.90it/s] 61%|██████▏   | 32621/53136 [00:45<00:26, 782.22it/s] 62%|██████▏   | 32701/53136 [00:45<00:27, 749.62it/s] 62%|██████▏   | 32778/53136 [00:45<00:27, 753.44it/s] 62%|██████▏   | 32878/53136 [00:45<00:24, 821.49it/s] 62%|██████▏   | 32962/53136 [00:45<00:25, 800.05it/s] 62%|██████▏   | 33046/53136 [00:45<00:24, 809.21it/s] 62%|██████▏   | 33128/53136 [00:46<00:25, 774.86it/s] 62%|██████▏   | 33207/53136 [00:46<00:28, 697.16it/s] 63%|██████▎   | 33288/53136 [00:46<00:27, 726.25it/s] 63%|██████▎   | 33363/53136 [00:46<00:31, 637.25it/s] 63%|██████▎   | 33458/53136 [00:46<00:27, 715.88it/s] 63%|██████▎   | 33541/53136 [00:46<00:26, 743.69it/s] 63%|██████▎   | 33619/53136 [00:46<00:30, 641.54it/s] 63%|██████▎   | 33696/53136 [00:46<00:29, 669.64it/s] 64%|██████▎   | 33767/53136 [00:47<00:31, 609.44it/s] 64%|██████▎   | 33844/53136 [00:47<00:29, 649.15it/s] 64%|██████▍   | 33912/53136 [00:47<00:30, 627.65it/s] 64%|██████▍   | 34001/53136 [00:47<00:27, 696.85it/s] 64%|██████▍   | 34086/53136 [00:47<00:25, 738.37it/s] 64%|██████▍   | 34180/53136 [00:47<00:23, 794.44it/s] 64%|██████▍   | 34262/53136 [00:47<00:24, 782.33it/s] 65%|██████▍   | 34342/53136 [00:47<00:24, 774.82it/s] 65%|██████▍   | 34421/53136 [00:47<00:28, 650.30it/s] 65%|██████▍   | 34490/53136 [00:48<00:29, 641.03it/s] 65%|██████▌   | 34557/53136 [00:48<00:29, 624.01it/s] 65%|██████▌   | 34653/53136 [00:48<00:26, 709.38it/s] 65%|██████▌   | 34729/53136 [00:48<00:25, 721.85it/s] 66%|██████▌   | 34807/53136 [00:48<00:24, 737.71it/s] 66%|██████▌   | 34883/53136 [00:48<00:31, 574.32it/s] 66%|██████▌   | 34952/53136 [00:48<00:30, 598.08it/s] 66%|██████▌   | 35017/53136 [00:48<00:32, 557.96it/s] 66%|██████▌   | 35085/53136 [00:49<00:33, 542.98it/s] 66%|██████▌   | 35154/53136 [00:49<00:31, 575.13it/s] 66%|██████▋   | 35216/53136 [00:49<00:30, 585.56it/s] 66%|██████▋   | 35299/53136 [00:49<00:27, 642.88it/s] 67%|██████▋   | 35373/53136 [00:49<00:26, 667.62it/s] 67%|██████▋   | 35448/53136 [00:49<00:27, 650.46it/s] 67%|██████▋   | 35515/53136 [00:49<00:26, 653.17it/s] 67%|██████▋   | 35604/53136 [00:49<00:24, 718.31it/s] 67%|██████▋   | 35677/53136 [00:49<00:25, 672.91it/s] 67%|██████▋   | 35757/53136 [00:50<00:24, 706.36it/s] 67%|██████▋   | 35845/53136 [00:50<00:22, 755.17it/s] 68%|██████▊   | 35922/53136 [00:50<00:25, 685.38it/s] 68%|██████▊   | 35993/53136 [00:50<00:28, 597.87it/s] 68%|██████▊   | 36076/53136 [00:50<00:26, 655.84it/s] 68%|██████▊   | 36145/53136 [00:50<00:26, 643.02it/s] 68%|██████▊   | 36212/53136 [00:50<00:29, 574.99it/s] 68%|██████▊   | 36273/53136 [00:50<00:31, 535.44it/s] 68%|██████▊   | 36358/53136 [00:50<00:27, 612.61it/s] 69%|██████▊   | 36436/53136 [00:51<00:25, 647.46it/s] 69%|██████▊   | 36511/53136 [00:51<00:24, 675.01it/s] 69%|██████▉   | 36582/53136 [00:51<00:24, 679.36it/s] 69%|██████▉   | 36652/53136 [00:51<00:25, 650.15it/s] 69%|██████▉   | 36719/53136 [00:51<00:25, 644.44it/s] 69%|██████▉   | 36785/53136 [00:51<00:28, 575.56it/s] 69%|██████▉   | 36854/53136 [00:51<00:27, 600.00it/s] 69%|██████▉   | 36926/53136 [00:51<00:25, 628.24it/s] 70%|██████▉   | 37000/53136 [00:51<00:24, 658.89it/s] 70%|██████▉   | 37068/53136 [00:52<00:24, 663.32it/s] 70%|██████▉   | 37136/53136 [00:52<00:25, 627.02it/s] 70%|███████   | 37212/53136 [00:52<00:24, 661.18it/s] 70%|███████   | 37279/53136 [00:52<00:24, 656.96it/s] 70%|███████   | 37346/53136 [00:52<00:24, 649.28it/s] 70%|███████   | 37421/53136 [00:52<00:23, 671.04it/s] 71%|███████   | 37500/53136 [00:52<00:22, 704.19it/s] 71%|███████   | 37584/53136 [00:52<00:20, 741.87it/s] 71%|███████   | 37663/53136 [00:52<00:20, 755.33it/s] 71%|███████   | 37739/53136 [00:53<00:20, 741.93it/s] 71%|███████   | 37830/53136 [00:53<00:19, 790.99it/s] 71%|███████▏  | 37910/53136 [00:53<00:19, 769.40it/s] 71%|███████▏  | 37988/53136 [00:53<00:19, 765.15it/s] 72%|███████▏  | 38069/53136 [00:53<00:19, 777.94it/s] 72%|███████▏  | 38153/53136 [00:53<00:18, 795.45it/s] 72%|███████▏  | 38233/53136 [00:53<00:25, 584.38it/s] 72%|███████▏  | 38305/53136 [00:53<00:24, 616.39it/s] 72%|███████▏  | 38374/53136 [00:53<00:23, 630.53it/s] 72%|███████▏  | 38442/53136 [00:54<00:23, 635.78it/s] 72%|███████▏  | 38523/53136 [00:54<00:21, 682.31it/s] 73%|███████▎  | 38595/53136 [00:54<00:21, 670.68it/s] 73%|███████▎  | 38665/53136 [00:54<00:22, 641.90it/s] 73%|███████▎  | 38739/53136 [00:54<00:22, 628.26it/s] 73%|███████▎  | 38821/53136 [00:54<00:21, 678.76it/s] 73%|███████▎  | 38891/53136 [00:54<00:20, 679.16it/s] 73%|███████▎  | 38988/53136 [00:54<00:18, 760.44it/s] 74%|███████▎  | 39066/53136 [00:54<00:18, 747.97it/s] 74%|███████▎  | 39143/53136 [00:55<00:18, 753.94it/s] 74%|███████▍  | 39235/53136 [00:55<00:17, 801.78it/s] 74%|███████▍  | 39316/53136 [00:55<00:17, 803.18it/s] 74%|███████▍  | 39397/53136 [00:55<00:18, 733.82it/s] 74%|███████▍  | 39472/53136 [00:55<00:18, 719.89it/s] 74%|███████▍  | 39545/53136 [00:55<00:21, 641.16it/s] 75%|███████▍  | 39612/53136 [00:55<00:21, 640.34it/s] 75%|███████▍  | 39679/53136 [00:55<00:22, 600.87it/s] 75%|███████▍  | 39763/53136 [00:55<00:20, 663.01it/s] 75%|███████▌  | 39852/53136 [00:56<00:18, 719.42it/s] 75%|███████▌  | 39942/53136 [00:56<00:17, 764.42it/s] 75%|███████▌  | 40031/53136 [00:56<00:16, 789.54it/s] 75%|███████▌  | 40112/53136 [00:56<00:17, 755.80it/s] 76%|███████▌  | 40208/53136 [00:56<00:15, 812.21it/s] 76%|███████▌  | 40291/53136 [00:56<00:16, 768.64it/s] 76%|███████▌  | 40369/53136 [00:56<00:16, 757.63it/s] 76%|███████▌  | 40446/53136 [00:56<00:16, 752.81it/s] 76%|███████▋  | 40529/53136 [00:56<00:16, 772.21it/s] 76%|███████▋  | 40614/53136 [00:57<00:15, 794.03it/s] 77%|███████▋  | 40694/53136 [00:57<00:16, 741.60it/s] 77%|███████▋  | 40770/53136 [00:57<00:17, 706.63it/s] 77%|███████▋  | 40843/53136 [00:57<00:17, 712.56it/s] 77%|███████▋  | 40928/53136 [00:57<00:16, 749.92it/s] 77%|███████▋  | 41012/53136 [00:57<00:15, 775.02it/s] 77%|███████▋  | 41103/53136 [00:57<00:14, 813.19it/s] 78%|███████▊  | 41185/53136 [00:57<00:19, 628.48it/s] 78%|███████▊  | 41255/53136 [00:57<00:18, 635.72it/s] 78%|███████▊  | 41344/53136 [00:58<00:16, 698.96it/s] 78%|███████▊  | 41420/53136 [00:58<00:16, 709.58it/s] 78%|███████▊  | 41511/53136 [00:58<00:15, 764.47it/s] 78%|███████▊  | 41609/53136 [00:58<00:13, 824.70it/s] 78%|███████▊  | 41694/53136 [00:58<00:15, 754.72it/s] 79%|███████▊  | 41773/53136 [00:58<00:15, 729.93it/s] 79%|███████▉  | 41865/53136 [00:58<00:14, 778.82it/s] 79%|███████▉  | 41945/53136 [00:58<00:15, 716.40it/s] 79%|███████▉  | 42019/53136 [00:59<00:16, 669.49it/s] 79%|███████▉  | 42094/53136 [00:59<00:16, 689.36it/s] 79%|███████▉  | 42165/53136 [00:59<00:15, 688.69it/s] 80%|███████▉  | 42246/53136 [00:59<00:15, 718.32it/s] 80%|███████▉  | 42320/53136 [00:59<00:14, 723.56it/s] 80%|███████▉  | 42412/53136 [00:59<00:13, 779.94it/s] 80%|███████▉  | 42506/53136 [00:59<00:12, 826.35it/s] 80%|████████  | 42591/53136 [00:59<00:12, 832.89it/s] 80%|████████  | 42681/53136 [00:59<00:12, 845.83it/s] 80%|████████  | 42766/53136 [00:59<00:13, 777.31it/s] 81%|████████  | 42857/53136 [01:00<00:12, 813.11it/s] 81%|████████  | 42946/53136 [01:00<00:13, 773.64it/s] 81%|████████  | 43025/53136 [01:00<00:13, 777.55it/s] 81%|████████  | 43104/53136 [01:00<00:16, 615.30it/s] 81%|████████  | 43172/53136 [01:00<00:16, 619.24it/s] 81%|████████▏ | 43257/53136 [01:00<00:14, 674.01it/s] 82%|████████▏ | 43339/53136 [01:00<00:13, 711.80it/s] 82%|████████▏ | 43414/53136 [01:00<00:14, 690.89it/s] 82%|████████▏ | 43486/53136 [01:01<00:14, 682.41it/s] 82%|████████▏ | 43556/53136 [01:01<00:15, 603.74it/s] 82%|████████▏ | 43634/53136 [01:01<00:14, 647.68it/s] 82%|████████▏ | 43702/53136 [01:01<00:15, 611.83it/s] 82%|████████▏ | 43767/53136 [01:01<00:15, 620.75it/s] 83%|████████▎ | 43848/53136 [01:01<00:13, 669.65it/s] 83%|████████▎ | 43917/53136 [01:01<00:14, 620.94it/s] 83%|████████▎ | 43993/53136 [01:01<00:14, 652.10it/s] 83%|████████▎ | 44072/53136 [01:01<00:13, 689.62it/s] 83%|████████▎ | 44143/53136 [01:02<00:14, 602.89it/s] 83%|████████▎ | 44219/53136 [01:02<00:14, 598.13it/s] 83%|████████▎ | 44308/53136 [01:02<00:13, 672.28it/s] 84%|████████▎ | 44379/53136 [01:02<00:12, 681.19it/s] 84%|████████▎ | 44450/53136 [01:02<00:13, 637.23it/s] 84%|████████▍ | 44547/53136 [01:02<00:11, 725.14it/s] 84%|████████▍ | 44626/53136 [01:02<00:11, 741.24it/s] 84%|████████▍ | 44702/53136 [01:02<00:11, 730.06it/s] 84%|████████▍ | 44777/53136 [01:02<00:11, 715.71it/s] 84%|████████▍ | 44878/53136 [01:03<00:10, 796.65it/s] 85%|████████▍ | 44967/53136 [01:03<00:09, 819.85it/s] 85%|████████▍ | 45050/53136 [01:03<00:10, 798.69it/s] 85%|████████▍ | 45149/53136 [01:03<00:09, 844.43it/s] 85%|████████▌ | 45236/53136 [01:03<00:09, 842.16it/s] 85%|████████▌ | 45321/53136 [01:03<00:09, 810.03it/s] 85%|████████▌ | 45403/53136 [01:03<00:10, 740.62it/s] 86%|████████▌ | 45480/53136 [01:03<00:10, 748.07it/s] 86%|████████▌ | 45556/53136 [01:03<00:10, 695.63it/s] 86%|████████▌ | 45627/53136 [01:04<00:11, 656.48it/s] 86%|████████▌ | 45694/53136 [01:04<00:11, 644.70it/s] 86%|████████▌ | 45770/53136 [01:04<00:10, 674.73it/s] 86%|████████▋ | 45852/53136 [01:04<00:10, 707.26it/s] 86%|████████▋ | 45932/53136 [01:04<00:09, 733.08it/s] 87%|████████▋ | 46020/53136 [01:04<00:09, 763.78it/s] 87%|████████▋ | 46097/53136 [01:04<00:09, 764.31it/s] 87%|████████▋ | 46181/53136 [01:04<00:08, 785.99it/s] 87%|████████▋ | 46260/53136 [01:04<00:10, 684.79it/s] 87%|████████▋ | 46336/53136 [01:05<00:09, 703.04it/s] 87%|████████▋ | 46434/53136 [01:05<00:08, 774.66it/s] 88%|████████▊ | 46523/53136 [01:05<00:08, 805.36it/s] 88%|████████▊ | 46606/53136 [01:05<00:09, 708.28it/s] 88%|████████▊ | 46693/53136 [01:05<00:08, 743.33it/s] 88%|████████▊ | 46770/53136 [01:05<00:09, 668.29it/s] 88%|████████▊ | 46855/53136 [01:05<00:08, 708.76it/s] 88%|████████▊ | 46936/53136 [01:05<00:08, 729.45it/s] 88%|████████▊ | 47011/53136 [01:05<00:08, 701.75it/s] 89%|████████▊ | 47083/53136 [01:06<00:08, 700.16it/s] 89%|████████▉ | 47175/53136 [01:06<00:07, 758.86it/s] 89%|████████▉ | 47253/53136 [01:06<00:08, 688.36it/s] 89%|████████▉ | 47343/53136 [01:06<00:07, 744.81it/s] 89%|████████▉ | 47420/53136 [01:06<00:07, 714.66it/s] 89%|████████▉ | 47499/53136 [01:06<00:07, 731.80it/s] 90%|████████▉ | 47574/53136 [01:06<00:08, 672.25it/s] 90%|████████▉ | 47657/53136 [01:06<00:07, 689.45it/s] 90%|████████▉ | 47742/53136 [01:06<00:07, 725.78it/s] 90%|████████▉ | 47816/53136 [01:07<00:07, 705.29it/s] 90%|█████████ | 47888/53136 [01:07<00:09, 579.29it/s] 90%|█████████ | 47958/53136 [01:07<00:08, 608.15it/s] 90%|█████████ | 48036/53136 [01:07<00:07, 651.84it/s] 91%|█████████ | 48105/53136 [01:07<00:07, 634.39it/s] 91%|█████████ | 48171/53136 [01:07<00:07, 637.20it/s] 91%|█████████ | 48252/53136 [01:07<00:07, 684.91it/s] 91%|█████████ | 48322/53136 [01:07<00:07, 681.10it/s] 91%|█████████ | 48401/53136 [01:08<00:06, 710.92it/s] 91%|█████████ | 48473/53136 [01:08<00:07, 648.46it/s] 91%|█████████▏| 48549/53136 [01:08<00:06, 678.40it/s] 92%|█████████▏| 48623/53136 [01:08<00:06, 693.49it/s] 92%|█████████▏| 48695/53136 [01:08<00:06, 698.76it/s] 92%|█████████▏| 48766/53136 [01:08<00:07, 579.98it/s] 92%|█████████▏| 48828/53136 [01:08<00:07, 587.30it/s] 92%|█████████▏| 48890/53136 [01:08<00:07, 561.39it/s] 92%|█████████▏| 48982/53136 [01:08<00:06, 651.21it/s] 92%|█████████▏| 49060/53136 [01:09<00:06, 671.30it/s] 92%|█████████▏| 49143/53136 [01:09<00:05, 712.34it/s] 93%|█████████▎| 49216/53136 [01:09<00:06, 630.72it/s] 93%|█████████▎| 49294/53136 [01:09<00:06, 634.17it/s] 93%|█████████▎| 49360/53136 [01:09<00:06, 597.88it/s] 93%|█████████▎| 49437/53136 [01:09<00:05, 641.44it/s] 93%|█████████▎| 49503/53136 [01:09<00:05, 645.95it/s] 93%|█████████▎| 49582/53136 [01:09<00:05, 678.53it/s] 93%|█████████▎| 49655/53136 [01:09<00:05, 688.78it/s] 94%|█████████▎| 49735/53136 [01:10<00:04, 711.42it/s] 94%|█████████▎| 49807/53136 [01:10<00:04, 702.31it/s] 94%|█████████▍| 49904/53136 [01:10<00:04, 779.23it/s] 94%|█████████▍| 49983/53136 [01:10<00:04, 707.97it/s] 94%|█████████▍| 50087/53136 [01:10<00:03, 797.00it/s] 94%|█████████▍| 50169/53136 [01:10<00:03, 796.33it/s] 95%|█████████▍| 50250/53136 [01:10<00:03, 742.30it/s] 95%|█████████▍| 50337/53136 [01:10<00:03, 759.27it/s] 95%|█████████▍| 50429/53136 [01:10<00:03, 796.93it/s] 95%|█████████▌| 50510/53136 [01:11<00:03, 728.66it/s] 95%|█████████▌| 50585/53136 [01:11<00:04, 596.98it/s] 95%|█████████▌| 50669/53136 [01:11<00:03, 649.16it/s] 96%|█████████▌| 50756/53136 [01:11<00:03, 703.84it/s] 96%|█████████▌| 50831/53136 [01:11<00:03, 658.84it/s] 96%|█████████▌| 50921/53136 [01:11<00:03, 720.86it/s] 96%|█████████▌| 50997/53136 [01:11<00:03, 706.41it/s] 96%|█████████▌| 51070/53136 [01:11<00:03, 644.94it/s] 96%|█████████▋| 51159/53136 [01:12<00:02, 705.03it/s] 96%|█████████▋| 51245/53136 [01:12<00:02, 746.57it/s] 97%|█████████▋| 51322/53136 [01:12<00:02, 703.10it/s] 97%|█████████▋| 51395/53136 [01:12<00:02, 611.61it/s] 97%|█████████▋| 51460/53136 [01:12<00:02, 616.67it/s] 97%|█████████▋| 51526/53136 [01:12<00:02, 575.26it/s] 97%|█████████▋| 51610/53136 [01:12<00:02, 642.13it/s] 97%|█████████▋| 51677/53136 [01:12<00:02, 517.32it/s] 97%|█████████▋| 51761/53136 [01:13<00:02, 585.13it/s] 98%|█████████▊| 51858/53136 [01:13<00:01, 680.15it/s] 98%|█████████▊| 51932/53136 [01:13<00:01, 685.27it/s] 98%|█████████▊| 52014/53136 [01:13<00:01, 697.92it/s] 98%|█████████▊| 52087/53136 [01:13<00:01, 700.61it/s] 98%|█████████▊| 52160/53136 [01:13<00:01, 630.54it/s] 98%|█████████▊| 52226/53136 [01:13<00:01, 475.16it/s] 98%|█████████▊| 52293/53136 [01:13<00:01, 513.04it/s] 99%|█████████▊| 52387/53136 [01:14<00:01, 612.93it/s] 99%|█████████▊| 52457/53136 [01:14<00:01, 634.68it/s] 99%|█████████▉| 52526/53136 [01:14<00:00, 613.72it/s] 99%|█████████▉| 52592/53136 [01:14<00:00, 602.86it/s] 99%|█████████▉| 52655/53136 [01:14<00:00, 600.46it/s] 99%|█████████▉| 52741/53136 [01:14<00:00, 668.45it/s] 99%|█████████▉| 52831/53136 [01:14<00:00, 732.67it/s]100%|█████████▉| 52913/53136 [01:14<00:00, 745.75it/s]100%|█████████▉| 52989/53136 [01:15<00:00, 630.63it/s]100%|█████████▉| 53056/53136 [01:15<00:00, 600.04it/s]100%|█████████▉| 53123/53136 [01:15<00:00, 615.51it/s]100%|██████████| 53136/53136 [01:15<00:00, 706.08it/s]

gathering stats for n=1
  0%|          | 0/53136 [00:00<?, ?it/s]  0%|          | 247/53136 [00:00<00:21, 2457.32it/s]  1%|          | 560/53136 [00:00<00:18, 2851.61it/s]  2%|▏         | 846/53136 [00:00<00:19, 2684.09it/s]  2%|▏         | 1119/53136 [00:00<00:19, 2698.85it/s]  3%|▎         | 1407/53136 [00:00<00:18, 2761.24it/s]  3%|▎         | 1748/53136 [00:00<00:17, 2975.74it/s]  4%|▍         | 2047/53136 [00:00<00:17, 2920.88it/s]  4%|▍         | 2367/53136 [00:00<00:16, 3005.03it/s]  5%|▌         | 2669/53136 [00:00<00:17, 2920.37it/s]  6%|▌         | 2962/53136 [00:01<00:17, 2853.35it/s]  6%|▌         | 3249/53136 [00:01<00:18, 2661.02it/s]  7%|▋         | 3518/53136 [00:01<00:18, 2648.21it/s]  7%|▋         | 3785/53136 [00:01<00:19, 2570.42it/s]  8%|▊         | 4068/53136 [00:01<00:18, 2642.16it/s]  8%|▊         | 4344/53136 [00:01<00:18, 2675.62it/s]  9%|▊         | 4613/53136 [00:01<00:18, 2647.95it/s]  9%|▉         | 4879/53136 [00:01<00:18, 2577.67it/s] 10%|▉         | 5138/53136 [00:01<00:19, 2490.60it/s] 10%|█         | 5388/53136 [00:02<00:19, 2483.46it/s] 11%|█         | 5678/53136 [00:02<00:18, 2602.82it/s] 11%|█         | 5940/53136 [00:02<00:18, 2589.23it/s] 12%|█▏        | 6218/53136 [00:02<00:17, 2642.24it/s] 12%|█▏        | 6530/53136 [00:02<00:16, 2781.79it/s] 13%|█▎        | 6840/53136 [00:02<00:16, 2873.96it/s] 13%|█▎        | 7128/53136 [00:02<00:16, 2714.16it/s] 14%|█▍        | 7402/53136 [00:02<00:18, 2536.21it/s] 14%|█▍        | 7660/53136 [00:02<00:17, 2546.38it/s] 15%|█▌        | 7975/53136 [00:02<00:16, 2712.22it/s] 16%|█▌        | 8249/53136 [00:03<00:17, 2597.17it/s] 16%|█▌        | 8520/53136 [00:03<00:17, 2612.62it/s] 17%|█▋        | 8799/53136 [00:03<00:16, 2662.72it/s] 17%|█▋        | 9067/53136 [00:03<00:17, 2588.99it/s] 18%|█▊        | 9346/53136 [00:03<00:16, 2645.21it/s] 18%|█▊        | 9667/53136 [00:03<00:15, 2808.84it/s] 19%|█▊        | 9950/53136 [00:03<00:17, 2475.90it/s] 19%|█▉        | 10222/53136 [00:03<00:16, 2541.07it/s] 20%|█▉        | 10483/53136 [00:03<00:17, 2498.00it/s] 20%|██        | 10738/53136 [00:04<00:17, 2463.44it/s] 21%|██        | 11041/53136 [00:04<00:16, 2619.97it/s] 21%|██▏       | 11307/53136 [00:04<00:16, 2467.40it/s] 22%|██▏       | 11560/53136 [00:04<00:16, 2483.88it/s] 22%|██▏       | 11821/53136 [00:04<00:16, 2516.32it/s] 23%|██▎       | 12077/53136 [00:04<00:16, 2527.76it/s] 23%|██▎       | 12350/53136 [00:04<00:15, 2586.52it/s] 24%|██▎       | 12610/53136 [00:04<00:16, 2493.40it/s] 24%|██▍       | 12872/53136 [00:04<00:15, 2526.22it/s] 25%|██▍       | 13156/53136 [00:04<00:15, 2604.99it/s] 25%|██▌       | 13427/53136 [00:05<00:15, 2633.64it/s] 26%|██▌       | 13698/53136 [00:05<00:14, 2654.08it/s] 26%|██▋       | 13964/53136 [00:05<00:14, 2614.03it/s] 27%|██▋       | 14226/53136 [00:05<00:15, 2590.57it/s] 27%|██▋       | 14486/53136 [00:05<00:15, 2558.91it/s] 28%|██▊       | 14814/53136 [00:05<00:13, 2769.19it/s] 28%|██▊       | 15092/53136 [00:05<00:14, 2611.62it/s] 29%|██▉       | 15356/53136 [00:05<00:14, 2537.57it/s] 29%|██▉       | 15612/53136 [00:05<00:15, 2479.42it/s] 30%|██▉       | 15862/53136 [00:06<00:15, 2473.94it/s] 30%|███       | 16128/53136 [00:06<00:14, 2491.00it/s] 31%|███       | 16378/53136 [00:06<00:14, 2488.63it/s] 31%|███▏      | 16688/53136 [00:06<00:13, 2659.78it/s] 32%|███▏      | 16991/53136 [00:06<00:13, 2766.76it/s] 32%|███▏      | 17269/53136 [00:06<00:13, 2734.62it/s] 33%|███▎      | 17544/53136 [00:06<00:13, 2682.06it/s] 34%|███▎      | 17813/53136 [00:06<00:13, 2675.93it/s] 34%|███▍      | 18081/53136 [00:06<00:13, 2635.43it/s] 35%|███▍      | 18345/53136 [00:06<00:13, 2508.65it/s] 35%|███▌      | 18598/53136 [00:07<00:14, 2422.93it/s] 35%|███▌      | 18863/53136 [00:07<00:13, 2486.49it/s] 36%|███▌      | 19155/53136 [00:07<00:13, 2598.03it/s] 37%|███▋      | 19425/53136 [00:07<00:12, 2608.43it/s] 37%|███▋      | 19687/53136 [00:07<00:12, 2585.88it/s] 38%|███▊      | 19947/53136 [00:07<00:13, 2534.69it/s] 38%|███▊      | 20202/53136 [00:07<00:13, 2455.25it/s] 38%|███▊      | 20449/53136 [00:07<00:13, 2451.98it/s] 39%|███▉      | 20695/53136 [00:07<00:13, 2442.41it/s] 39%|███▉      | 20962/53136 [00:08<00:12, 2507.73it/s] 40%|███▉      | 21214/53136 [00:08<00:12, 2484.23it/s] 40%|████      | 21487/53136 [00:08<00:12, 2555.55it/s] 41%|████      | 21743/53136 [00:08<00:13, 2319.48it/s] 41%|████▏     | 22016/53136 [00:08<00:12, 2421.15it/s] 42%|████▏     | 22262/53136 [00:08<00:12, 2424.99it/s] 42%|████▏     | 22541/53136 [00:08<00:12, 2524.30it/s] 43%|████▎     | 22818/53136 [00:08<00:11, 2595.54it/s] 43%|████▎     | 23080/53136 [00:08<00:11, 2537.08it/s] 44%|████▍     | 23351/53136 [00:08<00:11, 2582.27it/s] 44%|████▍     | 23611/53136 [00:09<00:12, 2435.32it/s] 45%|████▍     | 23874/53136 [00:09<00:11, 2489.32it/s] 45%|████▌     | 24150/53136 [00:09<00:11, 2566.56it/s] 46%|████▌     | 24413/53136 [00:09<00:11, 2584.45it/s] 47%|████▋     | 24724/53136 [00:09<00:10, 2738.07it/s] 47%|████▋     | 25000/53136 [00:09<00:10, 2684.80it/s] 48%|████▊     | 25277/53136 [00:09<00:10, 2708.25it/s] 48%|████▊     | 25549/53136 [00:09<00:10, 2626.21it/s] 49%|████▊     | 25827/53136 [00:09<00:10, 2663.21it/s] 49%|████▉     | 26095/53136 [00:10<00:10, 2602.71it/s] 50%|████▉     | 26357/53136 [00:10<00:10, 2472.79it/s] 50%|█████     | 26608/53136 [00:10<00:10, 2481.76it/s] 51%|█████     | 26886/53136 [00:10<00:10, 2566.21it/s] 51%|█████     | 27144/53136 [00:10<00:10, 2527.36it/s] 52%|█████▏    | 27398/53136 [00:10<00:10, 2520.66it/s] 52%|█████▏    | 27651/53136 [00:10<00:10, 2439.89it/s] 53%|█████▎    | 27950/53136 [00:10<00:09, 2581.35it/s] 53%|█████▎    | 28210/53136 [00:10<00:09, 2500.41it/s] 54%|█████▎    | 28497/53136 [00:10<00:09, 2605.69it/s] 54%|█████▍    | 28782/53136 [00:11<00:09, 2671.24it/s] 55%|█████▍    | 29067/53136 [00:11<00:08, 2719.60it/s] 55%|█████▌    | 29340/53136 [00:11<00:09, 2586.86it/s] 56%|█████▌    | 29623/53136 [00:11<00:08, 2652.77it/s] 56%|█████▋    | 29890/53136 [00:11<00:09, 2481.30it/s] 57%|█████▋    | 30142/53136 [00:11<00:09, 2395.09it/s] 57%|█████▋    | 30401/53136 [00:11<00:09, 2407.53it/s] 58%|█████▊    | 30644/53136 [00:11<00:09, 2408.17it/s] 58%|█████▊    | 30920/53136 [00:11<00:08, 2500.69it/s] 59%|█████▊    | 31172/53136 [00:12<00:08, 2460.39it/s] 59%|█████▉    | 31474/53136 [00:12<00:08, 2620.86it/s] 60%|█████▉    | 31760/53136 [00:12<00:07, 2689.40it/s] 60%|██████    | 32031/53136 [00:12<00:08, 2612.77it/s] 61%|██████    | 32316/53136 [00:12<00:07, 2679.43it/s] 61%|██████▏   | 32588/53136 [00:12<00:07, 2687.71it/s] 62%|██████▏   | 32865/53136 [00:12<00:07, 2710.79it/s] 62%|██████▏   | 33137/53136 [00:12<00:07, 2669.84it/s] 63%|██████▎   | 33405/53136 [00:12<00:07, 2571.88it/s] 63%|██████▎   | 33664/53136 [00:12<00:07, 2543.77it/s] 64%|██████▍   | 33920/53136 [00:13<00:07, 2477.17it/s] 64%|██████▍   | 34217/53136 [00:13<00:07, 2604.91it/s] 65%|██████▍   | 34479/53136 [00:13<00:07, 2529.62it/s] 65%|██████▌   | 34733/53136 [00:13<00:07, 2527.89it/s] 66%|██████▌   | 34987/53136 [00:13<00:07, 2398.58it/s] 66%|██████▋   | 35229/53136 [00:13<00:07, 2264.86it/s] 67%|██████▋   | 35463/53136 [00:13<00:07, 2281.98it/s] 67%|██████▋   | 35743/53136 [00:13<00:07, 2426.85it/s] 68%|██████▊   | 35988/53136 [00:13<00:07, 2385.14it/s] 68%|██████▊   | 36228/53136 [00:14<00:07, 2331.62it/s] 69%|██████▊   | 36477/53136 [00:14<00:07, 2375.78it/s] 69%|██████▉   | 36716/53136 [00:14<00:06, 2352.14it/s] 70%|██████▉   | 36952/53136 [00:14<00:06, 2353.89it/s] 70%|██████▉   | 37188/53136 [00:14<00:06, 2330.63it/s] 70%|███████   | 37432/53136 [00:14<00:06, 2350.65it/s] 71%|███████   | 37725/53136 [00:14<00:06, 2517.91it/s] 71%|███████▏  | 37985/53136 [00:14<00:05, 2539.60it/s] 72%|███████▏  | 38240/53136 [00:14<00:06, 2410.29it/s] 72%|███████▏  | 38483/53136 [00:15<00:06, 2393.41it/s] 73%|███████▎  | 38739/53136 [00:15<00:06, 2393.45it/s] 73%|███████▎  | 39016/53136 [00:15<00:05, 2470.39it/s] 74%|███████▍  | 39301/53136 [00:15<00:05, 2577.48it/s] 74%|███████▍  | 39560/53136 [00:15<00:05, 2491.36it/s] 75%|███████▍  | 39811/53136 [00:15<00:05, 2480.59it/s] 75%|███████▌  | 40098/53136 [00:15<00:05, 2591.25it/s] 76%|███████▌  | 40358/53136 [00:15<00:05, 2551.13it/s] 76%|███████▋  | 40644/53136 [00:15<00:04, 2640.13it/s] 77%|███████▋  | 40909/53136 [00:15<00:04, 2602.99it/s] 77%|███████▋  | 41170/53136 [00:16<00:04, 2571.53it/s] 78%|███████▊  | 41433/53136 [00:16<00:04, 2587.49it/s] 78%|███████▊  | 41708/53136 [00:16<00:04, 2635.13it/s] 79%|███████▉  | 41972/53136 [00:16<00:04, 2557.24it/s] 79%|███████▉  | 42236/53136 [00:16<00:04, 2579.67it/s] 80%|████████  | 42528/53136 [00:16<00:03, 2666.40it/s] 81%|████████  | 42824/53136 [00:16<00:03, 2743.33it/s] 81%|████████  | 43099/53136 [00:16<00:03, 2610.36it/s] 82%|████████▏ | 43372/53136 [00:16<00:03, 2623.06it/s] 82%|████████▏ | 43636/53136 [00:16<00:03, 2519.24it/s] 83%|████████▎ | 43890/53136 [00:17<00:03, 2468.98it/s] 83%|████████▎ | 44141/53136 [00:17<00:03, 2370.41it/s] 84%|████████▎ | 44402/53136 [00:17<00:03, 2406.92it/s] 84%|████████▍ | 44695/53136 [00:17<00:03, 2550.89it/s] 85%|████████▍ | 44980/53136 [00:17<00:03, 2633.35it/s] 85%|████████▌ | 45273/53136 [00:17<00:02, 2711.85it/s] 86%|████████▌ | 45546/53136 [00:17<00:02, 2616.44it/s] 86%|████████▌ | 45810/53136 [00:17<00:02, 2502.72it/s] 87%|████████▋ | 46090/53136 [00:17<00:02, 2582.21it/s] 87%|████████▋ | 46350/53136 [00:18<00:02, 2582.38it/s] 88%|████████▊ | 46632/53136 [00:18<00:02, 2647.98it/s] 88%|████████▊ | 46898/53136 [00:18<00:02, 2577.87it/s] 89%|████████▉ | 47186/53136 [00:18<00:02, 2656.96it/s] 89%|████████▉ | 47453/53136 [00:18<00:02, 2590.83it/s] 90%|████████▉ | 47713/53136 [00:18<00:02, 2569.87it/s] 90%|█████████ | 47971/53136 [00:18<00:02, 2447.05it/s] 91%|█████████ | 48218/53136 [00:18<00:02, 2419.52it/s] 91%|█████████ | 48471/53136 [00:18<00:01, 2447.42it/s] 92%|█████████▏| 48721/53136 [00:19<00:01, 2459.71it/s] 92%|█████████▏| 48968/53136 [00:19<00:01, 2317.47it/s] 93%|█████████▎| 49205/53136 [00:19<00:01, 2325.06it/s] 93%|█████████▎| 49445/53136 [00:19<00:01, 2346.49it/s] 94%|█████████▎| 49689/53136 [00:19<00:01, 2370.53it/s] 94%|█████████▍| 49976/53136 [00:19<00:01, 2514.55it/s] 95%|█████████▍| 50245/53136 [00:19<00:01, 2563.00it/s] 95%|█████████▌| 50515/53136 [00:19<00:01, 2535.13it/s] 96%|█████████▌| 50770/53136 [00:19<00:00, 2535.15it/s] 96%|█████████▌| 51024/53136 [00:19<00:00, 2489.62it/s] 97%|█████████▋| 51278/53136 [00:20<00:00, 2503.34it/s] 97%|█████████▋| 51529/53136 [00:20<00:00, 2345.90it/s] 97%|█████████▋| 51766/53136 [00:20<00:00, 2313.63it/s] 98%|█████████▊| 52059/53136 [00:20<00:00, 2485.00it/s] 98%|█████████▊| 52310/53136 [00:20<00:00, 2181.21it/s] 99%|█████████▉| 52573/53136 [00:20<00:00, 2297.39it/s] 99%|█████████▉| 52838/53136 [00:20<00:00, 2390.74it/s]100%|█████████▉| 53083/53136 [00:20<00:00, 2294.37it/s]100%|██████████| 53136/53136 [00:20<00:00, 2546.01it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 57.13it/s]2022-03-15 16:42:00 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=35920, bias=False)
  )
)
2022-03-15 16:42:00 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 16:42:00 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 16:42:00 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 16:42:00 | INFO | fairseq_cli.train | num. shared model params: 37,305,344 (num. trained: 37,305,344)
2022-03-15 16:42:00 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 16:42:00 | INFO | fairseq.data.data_utils | loaded 2,558 examples from: data-bin/ru/valid
2022-03-15 16:42:01 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 16:42:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:42:01 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 16:42:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:42:01 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 16:42:01 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 16:42:01 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-15 16:42:01 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-15 16:42:01 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 16:42:01 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
2022-03-15 16:42:01 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 16:42:01 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 16:42:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 16:42:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:42:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 16:42:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 16:46:52 | INFO | train_inner | epoch 001:    104 / 407 loss=14.812, ppl=28763, wps=24299.2, ups=0.37, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.243, loss_scale=8, train_wall=268, gb_free=9.6, wall=291
2022-03-15 16:51:20 | INFO | train_inner | epoch 001:    204 / 407 loss=13.315, ppl=10193.1, wps=24436.9, ups=0.37, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.663, loss_scale=16, train_wall=246, gb_free=9.6, wall=560
2022-03-15 16:55:48 | INFO | train_inner | epoch 001:    304 / 407 loss=12.42, ppl=5481.84, wps=24440.8, ups=0.37, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.411, loss_scale=32, train_wall=247, gb_free=9.6, wall=828
2022-03-15 17:00:16 | INFO | train_inner | epoch 001:    404 / 407 loss=11.965, ppl=3997.26, wps=24447.8, ups=0.37, wpb=65534.2, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.395, loss_scale=64, train_wall=246, gb_free=9.6, wall=1096
2022-03-15 17:00:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:01:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.78 | ppl 3517.66 | wps 36980.6 | wpb 511.9 | bsz 1 | num_updates 403
2022-03-15 17:01:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 403 updates
2022-03-15 17:01:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:01:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 1 @ 403 updates, score 11.78) (writing took 2.086134335026145 seconds)
2022-03-15 17:01:02 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 17:01:02 | INFO | train | epoch 001 | loss 13.12 | ppl 8900.31 | wps 23569.3 | ups 0.36 | wpb 65492.3 | bsz 127.9 | num_updates 403 | lr 5.04649e-05 | gnorm 0.925 | loss_scale 64 | train_wall 1014 | gb_free 9.6 | wall 1141
KL Stats: Epoch 1 Divergences: Uniform: 0.715565736974499 Unigram: 0.8173474278698213
2022-03-15 17:01:02 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 17:01:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:03:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:05:24 | INFO | train_inner | epoch 002:     98 / 407 loss=11.775, ppl=3504.19, wps=21218.7, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=500, lr=6.25875e-05, gnorm=0.415, loss_scale=32, train_wall=248, gb_free=9.6, wall=1404
2022-03-15 17:09:52 | INFO | train_inner | epoch 002:    198 / 407 loss=11.548, ppl=2994.58, wps=24479.6, ups=0.37, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.45, loss_scale=64, train_wall=246, gb_free=9.6, wall=1672
2022-03-15 17:13:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:14:23 | INFO | train_inner | epoch 002:    299 / 407 loss=11.256, ppl=2445.65, wps=24238.4, ups=0.37, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.451, loss_scale=32, train_wall=248, gb_free=9.6, wall=1942
2022-03-15 17:18:50 | INFO | train_inner | epoch 002:    399 / 407 loss=10.918, ppl=1934.47, wps=24472.6, ups=0.37, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.517, loss_scale=64, train_wall=246, gb_free=9.6, wall=2210
2022-03-15 17:18:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:19:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:19:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.517 | ppl 1465.75 | wps 36967.3 | wpb 511.9 | bsz 1 | num_updates 807 | best_loss 10.517
2022-03-15 17:19:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 807 updates
2022-03-15 17:19:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:19:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 2 @ 807 updates, score 10.517) (writing took 2.0581781901419163 seconds)
2022-03-15 17:19:49 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 17:19:49 | INFO | train | epoch 002 | loss 11.36 | ppl 2627.81 | wps 23471.8 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 807 | lr 0.000100955 | gnorm 0.46 | loss_scale 32 | train_wall 1000 | gb_free 9.6 | wall 2269
KL Stats: Epoch 2 Divergences: Uniform: 1.561519169087262 Unigram: 0.5305810447445974
2022-03-15 17:19:49 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 17:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:23:58 | INFO | train_inner | epoch 003:     93 / 407 loss=10.565, ppl=1514.64, wps=21272.1, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=900, lr=0.000112578, gnorm=0.531, loss_scale=32, train_wall=247, gb_free=9.6, wall=2517
2022-03-15 17:25:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:28:27 | INFO | train_inner | epoch 003:    194 / 407 loss=10.302, ppl=1262.03, wps=24283.7, ups=0.37, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.541, loss_scale=32, train_wall=248, gb_free=9.6, wall=2787
2022-03-15 17:31:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:32:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:33:03 | INFO | train_inner | epoch 003:    296 / 407 loss=10.074, ppl=1078.14, wps=23805.4, ups=0.36, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.597, loss_scale=16, train_wall=253, gb_free=9.6, wall=3062
2022-03-15 17:37:32 | INFO | train_inner | epoch 003:    396 / 407 loss=9.864, ppl=931.84, wps=24343.4, ups=0.37, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.596, loss_scale=16, train_wall=247, gb_free=9.6, wall=3331
2022-03-15 17:38:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:38:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.539 | ppl 743.88 | wps 36922.6 | wpb 511.9 | bsz 1 | num_updates 1211 | best_loss 9.539
2022-03-15 17:38:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1211 updates
2022-03-15 17:38:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:38:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:38:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 3 @ 1211 updates, score 9.539) (writing took 2.1286683520302176 seconds)
2022-03-15 17:38:39 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 17:38:39 | INFO | train | epoch 003 | loss 10.18 | ppl 1159.67 | wps 23418.6 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 1211 | lr 0.000151445 | gnorm 0.567 | loss_scale 32 | train_wall 1003 | gb_free 9.6 | wall 3399
KL Stats: Epoch 3 Divergences: Uniform: 2.347808061050798 Unigram: 1.4263612144323927
2022-03-15 17:38:39 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 17:38:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:42:37 | INFO | train_inner | epoch 004:     89 / 407 loss=9.649, ppl=803.09, wps=21412.8, ups=0.33, wpb=65360.1, bsz=127.7, num_updates=1300, lr=0.000162568, gnorm=0.665, loss_scale=32, train_wall=245, gb_free=9.6, wall=3637
2022-03-15 17:44:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:44:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:47:10 | INFO | train_inner | epoch 004:    191 / 407 loss=9.46, ppl=704.33, wps=24034.6, ups=0.37, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.704, loss_scale=16, train_wall=251, gb_free=9.6, wall=3909
2022-03-15 17:51:37 | INFO | train_inner | epoch 004:    291 / 407 loss=9.269, ppl=616.87, wps=24519.7, ups=0.37, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.669, loss_scale=32, train_wall=246, gb_free=9.6, wall=4177
2022-03-15 17:56:04 | INFO | train_inner | epoch 004:    391 / 407 loss=9.089, ppl=544.56, wps=24523.7, ups=0.37, wpb=65536, bsz=128, num_updates=1600, lr=0.00020006, gnorm=0.711, loss_scale=64, train_wall=246, gb_free=9.6, wall=4444
2022-03-15 17:56:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:56:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:57:23 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.749 | ppl 430.26 | wps 36980.2 | wpb 511.9 | bsz 1 | num_updates 1615 | best_loss 8.749
2022-03-15 17:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1615 updates
2022-03-15 17:57:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 17:57:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 4 @ 1615 updates, score 8.749) (writing took 2.7668912541121244 seconds)
2022-03-15 17:57:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 17:57:26 | INFO | train | epoch 004 | loss 9.344 | ppl 649.66 | wps 23486.6 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 1615 | lr 0.000201935 | gnorm 0.692 | loss_scale 32 | train_wall 999 | gb_free 9.6 | wall 4525
KL Stats: Epoch 4 Divergences: Uniform: 3.041550553611142 Unigram: 1.9486147224461017
2022-03-15 17:57:26 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 17:57:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:01:13 | INFO | train_inner | epoch 005:     85 / 407 loss=8.91, ppl=481.18, wps=21173.3, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=1700, lr=0.000212558, gnorm=0.7, loss_scale=32, train_wall=248, gb_free=9.6, wall=4753
2022-03-15 18:02:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:05:43 | INFO | train_inner | epoch 005:    186 / 407 loss=8.754, ppl=431.64, wps=24272.8, ups=0.37, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.744, loss_scale=32, train_wall=248, gb_free=9.6, wall=5023
2022-03-15 18:08:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:10:16 | INFO | train_inner | epoch 005:    287 / 407 loss=8.602, ppl=388.51, wps=24058.7, ups=0.37, wpb=65534.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.733, loss_scale=32, train_wall=250, gb_free=9.6, wall=5295
2022-03-15 18:14:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:14:50 | INFO | train_inner | epoch 005:    388 / 407 loss=8.476, ppl=356.14, wps=23867.4, ups=0.36, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.723, loss_scale=32, train_wall=252, gb_free=9.6, wall=5570
2022-03-15 18:15:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:16:18 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.158 | ppl 285.54 | wps 36219.4 | wpb 511.9 | bsz 1 | num_updates 2019 | best_loss 8.158
2022-03-15 18:16:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2019 updates
2022-03-15 18:16:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 18:16:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 18:16:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 5 @ 2019 updates, score 8.158) (writing took 2.0486953500658274 seconds)
2022-03-15 18:16:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 18:16:20 | INFO | train | epoch 005 | loss 8.66 | ppl 404.6 | wps 23325.9 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 2019 | lr 0.000252425 | gnorm 0.727 | loss_scale 32 | train_wall 1007 | gb_free 9.6 | wall 5659
KL Stats: Epoch 5 Divergences: Uniform: 3.626321239196141 Unigram: 2.317985508013544
2022-03-15 18:16:20 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 18:16:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:20:03 | INFO | train_inner | epoch 006:     82 / 407 loss=8.327, ppl=321.02, wps=20887, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=2100, lr=0.000262548, gnorm=0.737, loss_scale=16, train_wall=252, gb_free=9.6, wall=5882
2022-03-15 18:24:35 | INFO | train_inner | epoch 006:    182 / 407 loss=8.196, ppl=293.33, wps=24087.9, ups=0.37, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.719, loss_scale=16, train_wall=250, gb_free=9.6, wall=6155
2022-03-15 18:29:07 | INFO | train_inner | epoch 006:    282 / 407 loss=8.096, ppl=273.7, wps=24112.3, ups=0.37, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.718, loss_scale=32, train_wall=250, gb_free=9.6, wall=6426
2022-03-15 18:30:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:33:41 | INFO | train_inner | epoch 006:    383 / 407 loss=7.998, ppl=255.57, wps=23873.7, ups=0.36, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.71, loss_scale=32, train_wall=252, gb_free=9.6, wall=6701
2022-03-15 18:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:35:23 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.727 | ppl 211.93 | wps 36163.9 | wpb 511.9 | bsz 1 | num_updates 2424 | best_loss 7.727
2022-03-15 18:35:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2424 updates
2022-03-15 18:35:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 18:35:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 18:35:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 6 @ 2424 updates, score 7.727) (writing took 2.1098902095109224 seconds)
2022-03-15 18:35:25 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 18:35:25 | INFO | train | epoch 006 | loss 8.131 | ppl 280.42 | wps 23164.3 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 2424 | lr 0.000303039 | gnorm 0.714 | loss_scale 32 | train_wall 1017 | gb_free 9.6 | wall 6805
KL Stats: Epoch 6 Divergences: Uniform: 4.095770450394421 Unigram: 2.6122062060848985
2022-03-15 18:35:25 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 18:35:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:37:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:38:58 | INFO | train_inner | epoch 007:     77 / 407 loss=7.871, ppl=234.12, wps=20614, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=2500, lr=0.000312538, gnorm=0.685, loss_scale=32, train_wall=255, gb_free=9.6, wall=7018
2022-03-15 18:43:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:43:33 | INFO | train_inner | epoch 007:    178 / 407 loss=7.796, ppl=222.31, wps=23901.4, ups=0.36, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.691, loss_scale=32, train_wall=252, gb_free=9.6, wall=7292
2022-03-15 18:48:04 | INFO | train_inner | epoch 007:    278 / 407 loss=7.723, ppl=211.22, wps=24145.7, ups=0.37, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.664, loss_scale=32, train_wall=249, gb_free=9.6, wall=7564
2022-03-15 18:49:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:51:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:52:41 | INFO | train_inner | epoch 007:    380 / 407 loss=7.642, ppl=199.77, wps=23663.9, ups=0.36, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.659, loss_scale=16, train_wall=254, gb_free=9.6, wall=7840
2022-03-15 18:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:54:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.405 | ppl 169.47 | wps 36351.9 | wpb 511.9 | bsz 1 | num_updates 2827 | best_loss 7.405
2022-03-15 18:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2827 updates
2022-03-15 18:54:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 18:54:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 18:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 7 @ 2827 updates, score 7.405) (writing took 1.9826042344793677 seconds)
2022-03-15 18:54:33 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 18:54:33 | INFO | train | epoch 007 | loss 7.737 | ppl 213.32 | wps 23001.4 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 2827 | lr 0.000353404 | gnorm 0.675 | loss_scale 16 | train_wall 1018 | gb_free 9.6 | wall 7952
KL Stats: Epoch 7 Divergences: Uniform: 4.458006485071543 Unigram: 2.830536939210933
2022-03-15 18:54:33 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 18:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:57:51 | INFO | train_inner | epoch 008:     73 / 407 loss=7.54, ppl=186.1, wps=21076.2, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=2900, lr=0.000362528, gnorm=0.647, loss_scale=32, train_wall=249, gb_free=9.6, wall=8151
2022-03-15 19:02:24 | INFO | train_inner | epoch 008:    173 / 407 loss=7.476, ppl=178.06, wps=24048.2, ups=0.37, wpb=65534.2, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.634, loss_scale=32, train_wall=250, gb_free=9.6, wall=8423
2022-03-15 19:03:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:07:00 | INFO | train_inner | epoch 008:    274 / 407 loss=7.425, ppl=171.83, wps=23750.4, ups=0.36, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.622, loss_scale=32, train_wall=253, gb_free=9.6, wall=8699
2022-03-15 19:09:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:11:35 | INFO | train_inner | epoch 008:    375 / 407 loss=7.37, ppl=165.44, wps=23789.6, ups=0.36, wpb=65536, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.617, loss_scale=32, train_wall=253, gb_free=9.6, wall=8975
2022-03-15 19:12:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:13:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:13:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.149 | ppl 141.88 | wps 36178.2 | wpb 511.9 | bsz 1 | num_updates 3231 | best_loss 7.149
2022-03-15 19:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3231 updates
2022-03-15 19:13:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 19:13:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 19:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 8 @ 3231 updates, score 7.149) (writing took 1.8511772733181715 seconds)
2022-03-15 19:13:40 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 19:13:40 | INFO | train | epoch 008 | loss 7.432 | ppl 172.67 | wps 23056.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 3231 | lr 0.000403894 | gnorm 0.629 | loss_scale 16 | train_wall 1018 | gb_free 9.6 | wall 9100
KL Stats: Epoch 8 Divergences: Uniform: 4.681159463253174 Unigram: 2.9893105848429156
2022-03-15 19:13:40 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 19:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:16:50 | INFO | train_inner | epoch 009:     69 / 407 loss=7.27, ppl=154.36, wps=20744.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=3300, lr=0.000412518, gnorm=0.614, loss_scale=16, train_wall=253, gb_free=9.6, wall=9290
2022-03-15 19:21:27 | INFO | train_inner | epoch 009:    169 / 407 loss=7.211, ppl=148.16, wps=23706.2, ups=0.36, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.598, loss_scale=32, train_wall=253, gb_free=9.6, wall=9566
2022-03-15 19:22:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:26:06 | INFO | train_inner | epoch 009:    270 / 407 loss=7.167, ppl=143.7, wps=23468.7, ups=0.36, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.588, loss_scale=16, train_wall=256, gb_free=9.6, wall=9845
2022-03-15 19:30:39 | INFO | train_inner | epoch 009:    370 / 407 loss=7.116, ppl=138.73, wps=23978.9, ups=0.37, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.585, loss_scale=32, train_wall=251, gb_free=9.6, wall=10119
2022-03-15 19:32:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:32:56 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.892 | ppl 118.78 | wps 35814.3 | wpb 511.9 | bsz 1 | num_updates 3637 | best_loss 6.892
2022-03-15 19:32:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3637 updates
2022-03-15 19:32:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 19:32:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 19:32:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 9 @ 3637 updates, score 6.892) (writing took 1.893276784569025 seconds)
2022-03-15 19:32:58 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 19:32:58 | INFO | train | epoch 009 | loss 7.171 | ppl 144.11 | wps 22963.1 | ups 0.35 | wpb 65492.7 | bsz 127.9 | num_updates 3637 | lr 0.000454634 | gnorm 0.592 | loss_scale 32 | train_wall 1027 | gb_free 9.6 | wall 10258
KL Stats: Epoch 9 Divergences: Uniform: 4.834711736801125 Unigram: 3.110211910043751
2022-03-15 19:32:58 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 19:32:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:35:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:35:55 | INFO | train_inner | epoch 010:     64 / 407 loss=7.024, ppl=130.1, wps=20685.8, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.581, loss_scale=32, train_wall=254, gb_free=9.6, wall=10435
2022-03-15 19:40:27 | INFO | train_inner | epoch 010:    164 / 407 loss=6.972, ppl=125.52, wps=24063.6, ups=0.37, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.568, loss_scale=32, train_wall=250, gb_free=9.6, wall=10707
2022-03-15 19:41:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:42:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:45:04 | INFO | train_inner | epoch 010:    266 / 407 loss=6.937, ppl=122.55, wps=23673.1, ups=0.36, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.552, loss_scale=16, train_wall=254, gb_free=9.6, wall=10984
2022-03-15 19:49:35 | INFO | train_inner | epoch 010:    366 / 407 loss=6.894, ppl=118.9, wps=24182.5, ups=0.37, wpb=65534.2, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.555, loss_scale=32, train_wall=249, gb_free=9.6, wall=11255
2022-03-15 19:51:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:51:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:52:03 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.682 | ppl 102.67 | wps 36300.1 | wpb 511.9 | bsz 1 | num_updates 4040 | best_loss 6.682
2022-03-15 19:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4040 updates
2022-03-15 19:52:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 19:52:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 19:52:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 10 @ 4040 updates, score 6.682) (writing took 2.17496944591403 seconds)
2022-03-15 19:52:05 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 19:52:05 | INFO | train | epoch 010 | loss 6.937 | ppl 122.51 | wps 23018.2 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 4040 | lr 0.000497519 | gnorm 0.562 | loss_scale 16 | train_wall 1017 | gb_free 9.6 | wall 11404
KL Stats: Epoch 10 Divergences: Uniform: 4.976117483688658 Unigram: 3.2110791692685603
2022-03-15 19:52:05 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 19:52:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:54:47 | INFO | train_inner | epoch 011:     60 / 407 loss=6.807, ppl=111.97, wps=20950.7, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.557, loss_scale=16, train_wall=251, gb_free=9.6, wall=11567
2022-03-15 19:59:20 | INFO | train_inner | epoch 011:    160 / 407 loss=6.758, ppl=108.24, wps=24049.3, ups=0.37, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.536, loss_scale=32, train_wall=250, gb_free=9.6, wall=11839
2022-03-15 20:03:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 20:03:55 | INFO | train_inner | epoch 011:    261 / 407 loss=6.715, ppl=105.08, wps=23778.1, ups=0.36, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.532, loss_scale=32, train_wall=253, gb_free=9.6, wall=12115
2022-03-15 20:07:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:08:34 | INFO | train_inner | epoch 011:    362 / 407 loss=6.694, ppl=103.51, wps=23524.9, ups=0.36, wpb=65534.2, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.533, loss_scale=16, train_wall=256, gb_free=9.6, wall=12393
2022-03-15 20:10:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:11:15 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.506 | ppl 90.89 | wps 35653.9 | wpb 511.9 | bsz 1 | num_updates 4445 | best_loss 6.506
2022-03-15 20:11:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4445 updates
2022-03-15 20:11:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 20:11:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 20:11:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 11 @ 4445 updates, score 6.506) (writing took 1.990979753434658 seconds)
2022-03-15 20:11:17 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 20:11:17 | INFO | train | epoch 011 | loss 6.724 | ppl 105.68 | wps 23009.9 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 4445 | lr 0.000474312 | gnorm 0.536 | loss_scale 16 | train_wall 1022 | gb_free 9.6 | wall 12557
KL Stats: Epoch 11 Divergences: Uniform: 5.0977474259859425 Unigram: 3.301882569361764
2022-03-15 20:11:18 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 20:11:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:13:49 | INFO | train_inner | epoch 012:     55 / 407 loss=6.616, ppl=98.08, wps=20727.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.514, loss_scale=16, train_wall=253, gb_free=9.6, wall=12709
2022-03-15 20:18:26 | INFO | train_inner | epoch 012:    155 / 407 loss=6.563, ppl=94.57, wps=23660.1, ups=0.36, wpb=65534.2, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.506, loss_scale=32, train_wall=254, gb_free=9.6, wall=12986
2022-03-15 20:20:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 20:23:06 | INFO | train_inner | epoch 012:    256 / 407 loss=6.558, ppl=94.24, wps=23458.8, ups=0.36, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.51, loss_scale=32, train_wall=256, gb_free=9.6, wall=13265
2022-03-15 20:26:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 20:27:42 | INFO | train_inner | epoch 012:    357 / 407 loss=6.536, ppl=92.78, wps=23753.2, ups=0.36, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.506, loss_scale=32, train_wall=253, gb_free=9.6, wall=13541
2022-03-15 20:29:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:30:34 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.399 | ppl 84.39 | wps 36258.8 | wpb 511.9 | bsz 1 | num_updates 4850 | best_loss 6.399
2022-03-15 20:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4850 updates
2022-03-15 20:30:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 20:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 20:30:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 12 @ 4850 updates, score 6.399) (writing took 1.9474839540198445 seconds)
2022-03-15 20:30:36 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 20:30:36 | INFO | train | epoch 012 | loss 6.552 | ppl 93.85 | wps 22904.5 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 4850 | lr 0.000454077 | gnorm 0.507 | loss_scale 32 | train_wall 1027 | gb_free 9.6 | wall 13715
KL Stats: Epoch 12 Divergences: Uniform: 5.191642613931853 Unigram: 3.380090714841003
2022-03-15 20:30:36 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 20:30:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:32:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 20:32:55 | INFO | train_inner | epoch 013:     51 / 407 loss=6.478, ppl=89.15, wps=20885.1, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.505, loss_scale=32, train_wall=252, gb_free=9.6, wall=13854
2022-03-15 20:35:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:37:34 | INFO | train_inner | epoch 013:    152 / 407 loss=6.431, ppl=86.27, wps=23444.9, ups=0.36, wpb=65534.2, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.488, loss_scale=16, train_wall=256, gb_free=9.6, wall=14134
2022-03-15 20:42:07 | INFO | train_inner | epoch 013:    252 / 407 loss=6.431, ppl=86.29, wps=24022.4, ups=0.37, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.492, loss_scale=32, train_wall=250, gb_free=9.6, wall=14406
2022-03-15 20:43:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:46:41 | INFO | train_inner | epoch 013:    353 / 407 loss=6.411, ppl=85.11, wps=23913.5, ups=0.36, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.49, loss_scale=16, train_wall=252, gb_free=9.6, wall=14680
2022-03-15 20:49:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:49:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.302 | ppl 78.88 | wps 36280.8 | wpb 511.9 | bsz 1 | num_updates 5254 | best_loss 6.302
2022-03-15 20:49:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5254 updates
2022-03-15 20:49:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 20:49:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 20:49:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 13 @ 5254 updates, score 6.302) (writing took 2.037105822004378 seconds)
2022-03-15 20:49:46 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 20:49:46 | INFO | train | epoch 013 | loss 6.422 | ppl 85.73 | wps 22996.9 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 5254 | lr 0.00043627 | gnorm 0.492 | loss_scale 16 | train_wall 1020 | gb_free 9.6 | wall 14865
KL Stats: Epoch 13 Divergences: Uniform: 5.283945285408382 Unigram: 3.437429426041518
2022-03-15 20:49:46 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 20:49:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:51:51 | INFO | train_inner | epoch 014:     46 / 407 loss=6.358, ppl=82.04, wps=21053.5, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.494, loss_scale=32, train_wall=249, gb_free=9.6, wall=14991
2022-03-15 20:56:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 20:56:26 | INFO | train_inner | epoch 014:    147 / 407 loss=6.322, ppl=80.02, wps=23877.3, ups=0.36, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.49, loss_scale=32, train_wall=252, gb_free=9.6, wall=15265
2022-03-15 21:00:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:01:00 | INFO | train_inner | epoch 014:    248 / 407 loss=6.322, ppl=79.98, wps=23869.9, ups=0.36, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.477, loss_scale=16, train_wall=252, gb_free=9.6, wall=15540
2022-03-15 21:05:38 | INFO | train_inner | epoch 014:    348 / 407 loss=6.32, ppl=79.88, wps=23632.7, ups=0.36, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.485, loss_scale=16, train_wall=255, gb_free=9.6, wall=15817
2022-03-15 21:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:08:59 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.231 | ppl 75.1 | wps 35233.5 | wpb 511.9 | bsz 1 | num_updates 5659 | best_loss 6.231
2022-03-15 21:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5659 updates
2022-03-15 21:08:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 21:09:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 21:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 14 @ 5659 updates, score 6.231) (writing took 1.9769561756402254 seconds)
2022-03-15 21:09:01 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 21:09:01 | INFO | train | epoch 014 | loss 6.318 | ppl 79.78 | wps 22973.8 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 5659 | lr 0.000420368 | gnorm 0.486 | loss_scale 32 | train_wall 1023 | gb_free 9.6 | wall 16020
KL Stats: Epoch 14 Divergences: Uniform: 5.345844901279851 Unigram: 3.4847590731073925
2022-03-15 21:09:01 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 21:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:10:52 | INFO | train_inner | epoch 015:     41 / 407 loss=6.275, ppl=77.45, wps=20785.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.482, loss_scale=32, train_wall=252, gb_free=9.6, wall=16132
2022-03-15 21:13:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:15:26 | INFO | train_inner | epoch 015:    142 / 407 loss=6.244, ppl=75.79, wps=23894.2, ups=0.36, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.484, loss_scale=32, train_wall=252, gb_free=9.6, wall=16406
2022-03-15 21:19:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:20:01 | INFO | train_inner | epoch 015:    243 / 407 loss=6.233, ppl=75.23, wps=23873.9, ups=0.36, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.479, loss_scale=32, train_wall=252, gb_free=9.6, wall=16680
2022-03-15 21:24:32 | INFO | train_inner | epoch 015:    343 / 407 loss=6.229, ppl=75.02, wps=24143.3, ups=0.37, wpb=65534.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.479, loss_scale=32, train_wall=249, gb_free=9.6, wall=16952
2022-03-15 21:25:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:27:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:28:04 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.17 | ppl 72.01 | wps 35552 | wpb 511.9 | bsz 1 | num_updates 6063 | best_loss 6.17
2022-03-15 21:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 6063 updates
2022-03-15 21:28:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 21:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 21:28:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 15 @ 6063 updates, score 6.17) (writing took 1.9489851612597704 seconds)
2022-03-15 21:28:06 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 21:28:06 | INFO | train | epoch 015 | loss 6.232 | ppl 75.14 | wps 23102.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 6063 | lr 0.000406122 | gnorm 0.481 | loss_scale 32 | train_wall 1015 | gb_free 9.6 | wall 17165
KL Stats: Epoch 15 Divergences: Uniform: 5.412390474987383 Unigram: 3.524994116746648
2022-03-15 21:28:06 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 21:28:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:29:48 | INFO | train_inner | epoch 016:     37 / 407 loss=6.188, ppl=72.93, wps=20698.8, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.483, loss_scale=32, train_wall=253, gb_free=9.6, wall=17268
2022-03-15 21:31:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:34:28 | INFO | train_inner | epoch 016:    138 / 407 loss=6.162, ppl=71.6, wps=23424, ups=0.36, wpb=65534.2, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.485, loss_scale=32, train_wall=257, gb_free=9.6, wall=17547
2022-03-15 21:37:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:39:08 | INFO | train_inner | epoch 016:    239 / 407 loss=6.166, ppl=71.79, wps=23407, ups=0.36, wpb=65536, bsz=128, num_updates=6300, lr=0.00039841, gnorm=0.476, loss_scale=32, train_wall=257, gb_free=9.6, wall=17827
2022-03-15 21:39:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:43:48 | INFO | train_inner | epoch 016:    340 / 407 loss=6.156, ppl=71.29, wps=23381.1, ups=0.36, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.475, loss_scale=16, train_wall=257, gb_free=9.6, wall=18108
2022-03-15 21:46:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:47:30 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.118 | ppl 69.46 | wps 35694.1 | wpb 511.9 | bsz 1 | num_updates 6467 | best_loss 6.118
2022-03-15 21:47:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6467 updates
2022-03-15 21:47:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 21:47:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 21:47:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 16 @ 6467 updates, score 6.118) (writing took 1.9947284385561943 seconds)
2022-03-15 21:47:32 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 21:47:32 | INFO | train | epoch 016 | loss 6.159 | ppl 71.45 | wps 22693.6 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 6467 | lr 0.000393232 | gnorm 0.478 | loss_scale 32 | train_wall 1034 | gb_free 9.6 | wall 18331
KL Stats: Epoch 16 Divergences: Uniform: 5.478955543805866 Unigram: 3.5614168064865663
2022-03-15 21:47:32 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 21:47:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:49:03 | INFO | train_inner | epoch 017:     33 / 407 loss=6.137, ppl=70.36, wps=20776.1, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.473, loss_scale=32, train_wall=253, gb_free=9.6, wall=18422
2022-03-15 21:51:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:53:42 | INFO | train_inner | epoch 017:    134 / 407 loss=6.09, ppl=68.11, wps=23498.4, ups=0.36, wpb=65534.2, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.475, loss_scale=16, train_wall=256, gb_free=9.6, wall=18701
2022-03-15 21:58:13 | INFO | train_inner | epoch 017:    234 / 407 loss=6.096, ppl=68.41, wps=24133.8, ups=0.37, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.477, loss_scale=32, train_wall=249, gb_free=9.6, wall=18973
2022-03-15 22:02:47 | INFO | train_inner | epoch 017:    334 / 407 loss=6.097, ppl=68.44, wps=23953.9, ups=0.37, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.484, loss_scale=32, train_wall=251, gb_free=9.6, wall=19246
2022-03-15 22:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:06:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:06:45 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.074 | ppl 67.37 | wps 35723 | wpb 511.9 | bsz 1 | num_updates 6872 | best_loss 6.074
2022-03-15 22:06:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6872 updates
2022-03-15 22:06:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 22:06:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 22:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 17 @ 6872 updates, score 6.074) (writing took 1.8731237584725022 seconds)
2022-03-15 22:06:47 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 22:06:47 | INFO | train | epoch 017 | loss 6.096 | ppl 68.41 | wps 22956.4 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 6872 | lr 0.000381468 | gnorm 0.48 | loss_scale 32 | train_wall 1024 | gb_free 9.6 | wall 19487
KL Stats: Epoch 17 Divergences: Uniform: 5.531244265228385 Unigram: 3.5925071040655494
2022-03-15 22:06:47 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 22:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:08:05 | INFO | train_inner | epoch 018:     28 / 407 loss=6.079, ppl=67.6, wps=20558.9, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.484, loss_scale=32, train_wall=255, gb_free=9.6, wall=19564
2022-03-15 22:09:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:09:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:12:48 | INFO | train_inner | epoch 018:    130 / 407 loss=6.032, ppl=65.44, wps=23147.8, ups=0.35, wpb=65534.2, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.476, loss_scale=16, train_wall=260, gb_free=9.6, wall=19847
2022-03-15 22:17:24 | INFO | train_inner | epoch 018:    230 / 407 loss=6.044, ppl=65.99, wps=23725.4, ups=0.36, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.477, loss_scale=32, train_wall=254, gb_free=9.6, wall=20124
2022-03-15 22:21:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:22:05 | INFO | train_inner | epoch 018:    331 / 407 loss=6.046, ppl=66.09, wps=23337.3, ups=0.36, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.477, loss_scale=32, train_wall=258, gb_free=9.6, wall=20404
2022-03-15 22:25:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:26:12 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.028 | ppl 65.26 | wps 35626.7 | wpb 511.9 | bsz 1 | num_updates 7276 | best_loss 6.028
2022-03-15 22:26:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7276 updates
2022-03-15 22:26:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 22:26:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 22:26:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 18 @ 7276 updates, score 6.028) (writing took 1.8961344501003623 seconds)
2022-03-15 22:26:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 22:26:14 | INFO | train | epoch 018 | loss 6.042 | ppl 65.87 | wps 22669.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 7276 | lr 0.000370727 | gnorm 0.479 | loss_scale 32 | train_wall 1035 | gb_free 9.6 | wall 20654
KL Stats: Epoch 18 Divergences: Uniform: 5.576149846411178 Unigram: 3.6182578156907095
2022-03-15 22:26:14 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 22:26:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:27:21 | INFO | train_inner | epoch 019:     24 / 407 loss=6.039, ppl=65.75, wps=20706.2, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.488, loss_scale=32, train_wall=253, gb_free=9.6, wall=20720
2022-03-15 22:28:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:31:57 | INFO | train_inner | epoch 019:    125 / 407 loss=5.983, ppl=63.23, wps=23726.4, ups=0.36, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.479, loss_scale=32, train_wall=254, gb_free=9.6, wall=20996
2022-03-15 22:34:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:34:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:36:39 | INFO | train_inner | epoch 019:    227 / 407 loss=5.99, ppl=63.55, wps=23197.7, ups=0.35, wpb=65534.2, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.478, loss_scale=16, train_wall=259, gb_free=9.6, wall=21279
2022-03-15 22:41:15 | INFO | train_inner | epoch 019:    327 / 407 loss=6.002, ppl=64.1, wps=23747.4, ups=0.36, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.481, loss_scale=32, train_wall=253, gb_free=9.6, wall=21555
2022-03-15 22:44:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:45:30 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.001 | ppl 64.03 | wps 36214.1 | wpb 511.9 | bsz 1 | num_updates 7680 | best_loss 6.001
2022-03-15 22:45:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7680 updates
2022-03-15 22:45:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 22:45:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 22:45:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 19 @ 7680 updates, score 6.001) (writing took 1.9491175645962358 seconds)
2022-03-15 22:45:32 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-15 22:45:32 | INFO | train | epoch 019 | loss 5.994 | ppl 63.72 | wps 22863.6 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 7680 | lr 0.000360844 | gnorm 0.477 | loss_scale 32 | train_wall 1026 | gb_free 9.6 | wall 21811
KL Stats: Epoch 19 Divergences: Uniform: 5.626303992242502 Unigram: 3.643344347002402
2022-03-15 22:45:32 | INFO | fairseq.trainer | begin training epoch 20
2022-03-15 22:45:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:46:26 | INFO | train_inner | epoch 020:     20 / 407 loss=5.991, ppl=63.58, wps=21033.9, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=7700, lr=0.000360375, gnorm=0.469, loss_scale=32, train_wall=250, gb_free=9.6, wall=21866
2022-03-15 22:47:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:51:02 | INFO | train_inner | epoch 020:    121 / 407 loss=5.933, ppl=61.1, wps=23716.6, ups=0.36, wpb=65534.2, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.476, loss_scale=32, train_wall=254, gb_free=9.6, wall=22142
2022-03-15 22:52:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:55:42 | INFO | train_inner | epoch 020:    222 / 407 loss=5.957, ppl=62.13, wps=23444.4, ups=0.36, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.478, loss_scale=32, train_wall=257, gb_free=9.6, wall=22421
2022-03-15 22:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:00:22 | INFO | train_inner | epoch 020:    323 / 407 loss=5.957, ppl=62.12, wps=23409.1, ups=0.36, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.478, loss_scale=32, train_wall=257, gb_free=9.6, wall=22701
2022-03-15 23:04:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:04:52 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.976 | ppl 62.95 | wps 35529.8 | wpb 511.9 | bsz 1 | num_updates 8084 | best_loss 5.976
2022-03-15 23:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 8084 updates
2022-03-15 23:04:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 23:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 23:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 20 @ 8084 updates, score 5.976) (writing took 1.8697781506925821 seconds)
2022-03-15 23:04:54 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-15 23:04:54 | INFO | train | epoch 020 | loss 5.951 | ppl 61.86 | wps 22770.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 8084 | lr 0.000351712 | gnorm 0.478 | loss_scale 32 | train_wall 1031 | gb_free 9.6 | wall 22973
KL Stats: Epoch 20 Divergences: Uniform: 5.669586767459574 Unigram: 3.6661440935388563
2022-03-15 23:04:54 | INFO | fairseq.trainer | begin training epoch 21
2022-03-15 23:04:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:05:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:05:41 | INFO | train_inner | epoch 021:     17 / 407 loss=5.948, ppl=61.72, wps=20501.6, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=8100, lr=0.000351364, gnorm=0.482, loss_scale=32, train_wall=256, gb_free=9.6, wall=23020
2022-03-15 23:10:16 | INFO | train_inner | epoch 021:    117 / 407 loss=5.893, ppl=59.42, wps=23824, ups=0.36, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.475, loss_scale=32, train_wall=253, gb_free=9.6, wall=23295
2022-03-15 23:11:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:14:55 | INFO | train_inner | epoch 021:    218 / 407 loss=5.916, ppl=60.37, wps=23502.3, ups=0.36, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.48, loss_scale=32, train_wall=256, gb_free=9.6, wall=23574
2022-03-15 23:17:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:19:29 | INFO | train_inner | epoch 021:    319 / 407 loss=5.916, ppl=60.39, wps=23903.2, ups=0.36, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.481, loss_scale=32, train_wall=252, gb_free=9.6, wall=23848
2022-03-15 23:23:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:23:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:24:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.949 | ppl 61.76 | wps 36140.5 | wpb 511.9 | bsz 1 | num_updates 8487 | best_loss 5.949
2022-03-15 23:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8487 updates
2022-03-15 23:24:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 23:24:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 23:24:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 21 @ 8487 updates, score 5.949) (writing took 1.9426720682531595 seconds)
2022-03-15 23:24:07 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-15 23:24:07 | INFO | train | epoch 021 | loss 5.911 | ppl 60.18 | wps 22891.6 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 8487 | lr 0.00034326 | gnorm 0.479 | loss_scale 32 | train_wall 1023 | gb_free 9.6 | wall 24126
KL Stats: Epoch 21 Divergences: Uniform: 5.7005391117667905 Unigram: 3.686209325066818
2022-03-15 23:24:07 | INFO | fairseq.trainer | begin training epoch 22
2022-03-15 23:24:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:24:42 | INFO | train_inner | epoch 022:     13 / 407 loss=5.92, ppl=60.53, wps=20868.1, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=8500, lr=0.000342997, gnorm=0.482, loss_scale=32, train_wall=252, gb_free=9.6, wall=24161
2022-03-15 23:29:14 | INFO | train_inner | epoch 022:    113 / 407 loss=5.86, ppl=58.07, wps=24135, ups=0.37, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.48, loss_scale=32, train_wall=249, gb_free=9.6, wall=24433
2022-03-15 23:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:33:53 | INFO | train_inner | epoch 022:    214 / 407 loss=5.875, ppl=58.67, wps=23453.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.482, loss_scale=32, train_wall=257, gb_free=9.6, wall=24712
2022-03-15 23:35:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:38:29 | INFO | train_inner | epoch 022:    315 / 407 loss=5.884, ppl=59.06, wps=23717.9, ups=0.36, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.477, loss_scale=32, train_wall=254, gb_free=9.6, wall=24989
2022-03-15 23:41:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:42:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:43:21 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.929 | ppl 60.91 | wps 35448.1 | wpb 511.9 | bsz 1 | num_updates 8891 | best_loss 5.929
2022-03-15 23:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8891 updates
2022-03-15 23:43:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 23:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-15 23:43:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 22 @ 8891 updates, score 5.929) (writing took 1.8722950657829642 seconds)
2022-03-15 23:43:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-15 23:43:23 | INFO | train | epoch 022 | loss 5.876 | ppl 58.72 | wps 22877.3 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 8891 | lr 0.00033537 | gnorm 0.48 | loss_scale 32 | train_wall 1025 | gb_free 9.6 | wall 25283
KL Stats: Epoch 22 Divergences: Uniform: 5.732847496327634 Unigram: 3.705929743327591
2022-03-15 23:43:23 | INFO | fairseq.trainer | begin training epoch 23
2022-03-15 23:43:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:43:48 | INFO | train_inner | epoch 023:      9 / 407 loss=5.882, ppl=58.97, wps=20500.4, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=8900, lr=0.000335201, gnorm=0.478, loss_scale=32, train_wall=256, gb_free=9.6, wall=25308
2022-03-15 23:48:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:48:28 | INFO | train_inner | epoch 023:    110 / 407 loss=5.824, ppl=56.65, wps=23414.8, ups=0.36, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.477, loss_scale=32, train_wall=257, gb_free=9.6, wall=25587
2022-03-15 23:53:05 | INFO | train_inner | epoch 023:    210 / 407 loss=5.837, ppl=57.16, wps=23629.6, ups=0.36, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.484, loss_scale=32, train_wall=254, gb_free=9.6, wall=25865
2022-03-15 23:54:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:57:45 | INFO | train_inner | epoch 023:    311 / 407 loss=5.864, ppl=58.26, wps=23446.1, ups=0.36, wpb=65534.2, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.479, loss_scale=32, train_wall=257, gb_free=9.6, wall=26144
2022-03-16 00:00:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:02:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:02:48 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.906 | ppl 59.96 | wps 35194 | wpb 511.9 | bsz 1 | num_updates 9295 | best_loss 5.906
2022-03-16 00:02:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9295 updates
2022-03-16 00:02:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 00:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 00:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 23 @ 9295 updates, score 5.906) (writing took 1.9200280858203769 seconds)
2022-03-16 00:02:50 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-16 00:02:50 | INFO | train | epoch 023 | loss 5.843 | ppl 57.41 | wps 22683.2 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 9295 | lr 0.000328001 | gnorm 0.481 | loss_scale 32 | train_wall 1034 | gb_free 9.6 | wall 26449
KL Stats: Epoch 23 Divergences: Uniform: 5.767841613388545 Unigram: 3.722070411221214
2022-03-16 00:02:50 | INFO | fairseq.trainer | begin training epoch 24
2022-03-16 00:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:03:04 | INFO | train_inner | epoch 024:      5 / 407 loss=5.847, ppl=57.55, wps=20502.2, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=9300, lr=0.000327913, gnorm=0.483, loss_scale=32, train_wall=256, gb_free=9.6, wall=26463
2022-03-16 00:06:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:07:40 | INFO | train_inner | epoch 024:    106 / 407 loss=5.792, ppl=55.39, wps=23760.7, ups=0.36, wpb=65534.2, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.483, loss_scale=32, train_wall=253, gb_free=9.6, wall=26739
2022-03-16 00:12:11 | INFO | train_inner | epoch 024:    206 / 407 loss=5.811, ppl=56.15, wps=24165.1, ups=0.37, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.482, loss_scale=32, train_wall=249, gb_free=9.6, wall=27010
2022-03-16 00:12:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:16:45 | INFO | train_inner | epoch 024:    307 / 407 loss=5.824, ppl=56.64, wps=23872.8, ups=0.36, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.476, loss_scale=32, train_wall=252, gb_free=9.6, wall=27285
2022-03-16 00:18:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:21:53 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.888 | ppl 59.23 | wps 36347.1 | wpb 511.9 | bsz 1 | num_updates 9699 | best_loss 5.888
2022-03-16 00:21:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9699 updates
2022-03-16 00:21:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 00:21:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 00:21:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 24 @ 9699 updates, score 5.888) (writing took 1.8917462415993214 seconds)
2022-03-16 00:21:55 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-16 00:21:55 | INFO | train | epoch 024 | loss 5.814 | ppl 56.26 | wps 23106.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 9699 | lr 0.000321097 | gnorm 0.48 | loss_scale 32 | train_wall 1015 | gb_free 9.6 | wall 27594
KL Stats: Epoch 24 Divergences: Uniform: 5.804563859832404 Unigram: 3.7403714524102427
2022-03-16 00:21:55 | INFO | fairseq.trainer | begin training epoch 25
2022-03-16 00:21:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:21:58 | INFO | train_inner | epoch 025:      1 / 407 loss=5.833, ppl=56.99, wps=20926.3, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=9700, lr=0.000321081, gnorm=0.478, loss_scale=32, train_wall=251, gb_free=9.6, wall=27597
2022-03-16 00:25:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:26:33 | INFO | train_inner | epoch 025:    102 / 407 loss=5.761, ppl=54.25, wps=23786.2, ups=0.36, wpb=65534.2, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.487, loss_scale=32, train_wall=253, gb_free=9.6, wall=27873
2022-03-16 00:30:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:31:14 | INFO | train_inner | epoch 025:    203 / 407 loss=5.779, ppl=54.93, wps=23361.7, ups=0.36, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.48, loss_scale=16, train_wall=257, gb_free=9.6, wall=28153
2022-03-16 00:35:50 | INFO | train_inner | epoch 025:    303 / 407 loss=5.802, ppl=55.78, wps=23687, ups=0.36, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.478, loss_scale=16, train_wall=254, gb_free=9.6, wall=28430
2022-03-16 00:40:28 | INFO | train_inner | epoch 025:    403 / 407 loss=5.803, ppl=55.83, wps=23560.4, ups=0.36, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.481, loss_scale=32, train_wall=255, gb_free=9.6, wall=28708
2022-03-16 00:40:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:41:17 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.868 | ppl 58.39 | wps 35435.1 | wpb 511.9 | bsz 1 | num_updates 10104 | best_loss 5.868
2022-03-16 00:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 10104 updates
2022-03-16 00:41:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 00:41:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 00:41:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 25 @ 10104 updates, score 5.868) (writing took 1.9077638303861022 seconds)
2022-03-16 00:41:19 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-16 00:41:19 | INFO | train | epoch 025 | loss 5.787 | ppl 55.2 | wps 22791 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 10104 | lr 0.000314596 | gnorm 0.482 | loss_scale 32 | train_wall 1032 | gb_free 9.6 | wall 28758
KL Stats: Epoch 25 Divergences: Uniform: 5.833764097445902 Unigram: 3.7545089806969814
2022-03-16 00:41:19 | INFO | fairseq.trainer | begin training epoch 26
2022-03-16 00:41:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:42:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:45:47 | INFO | train_inner | epoch 026:     97 / 407 loss=5.74, ppl=53.46, wps=20525.2, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.483, loss_scale=32, train_wall=256, gb_free=9.6, wall=29026
2022-03-16 00:48:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:50:28 | INFO | train_inner | epoch 026:    198 / 407 loss=5.762, ppl=54.28, wps=23335.3, ups=0.36, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.483, loss_scale=32, train_wall=258, gb_free=9.6, wall=29307
2022-03-16 00:54:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:55:03 | INFO | train_inner | epoch 026:    299 / 407 loss=5.761, ppl=54.21, wps=23769.2, ups=0.36, wpb=65534.2, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.479, loss_scale=32, train_wall=253, gb_free=9.6, wall=29583
2022-03-16 00:59:36 | INFO | train_inner | epoch 026:    399 / 407 loss=5.782, ppl=55.03, wps=24046.4, ups=0.37, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.48, loss_scale=32, train_wall=250, gb_free=9.6, wall=29855
2022-03-16 00:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:00:35 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.86 | ppl 58.1 | wps 35568.7 | wpb 511.9 | bsz 1 | num_updates 10508 | best_loss 5.86
2022-03-16 01:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10508 updates
2022-03-16 01:00:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:00:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 26 @ 10508 updates, score 5.86) (writing took 1.9067963361740112 seconds)
2022-03-16 01:00:37 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 01:00:37 | INFO | train | epoch 026 | loss 5.762 | ppl 54.26 | wps 22840.9 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 10508 | lr 0.000308489 | gnorm 0.482 | loss_scale 32 | train_wall 1027 | gb_free 9.6 | wall 29916
KL Stats: Epoch 26 Divergences: Uniform: 5.8652622216689725 Unigram: 3.7695926448842725
2022-03-16 01:00:37 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 01:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:01:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:04:55 | INFO | train_inner | epoch 027:     93 / 407 loss=5.724, ppl=52.87, wps=20501, ups=0.31, wpb=65360.1, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.491, loss_scale=32, train_wall=256, gb_free=9.6, wall=30174
2022-03-16 01:07:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:09:35 | INFO | train_inner | epoch 027:    194 / 407 loss=5.737, ppl=53.34, wps=23365.7, ups=0.36, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.491, loss_scale=32, train_wall=257, gb_free=9.6, wall=30455
2022-03-16 01:13:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:14:14 | INFO | train_inner | epoch 027:    295 / 407 loss=5.736, ppl=53.28, wps=23519, ups=0.36, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.488, loss_scale=32, train_wall=256, gb_free=9.6, wall=30733
2022-03-16 01:18:50 | INFO | train_inner | epoch 027:    395 / 407 loss=5.763, ppl=54.31, wps=23713, ups=0.36, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.476, loss_scale=32, train_wall=253, gb_free=9.6, wall=31010
2022-03-16 01:19:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:19:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:20:01 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.84 | ppl 57.28 | wps 35242.5 | wpb 511.9 | bsz 1 | num_updates 10911 | best_loss 5.84
2022-03-16 01:20:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10911 updates
2022-03-16 01:20:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:20:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:20:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 27 @ 10911 updates, score 5.84) (writing took 1.9074209341779351 seconds)
2022-03-16 01:20:03 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 01:20:03 | INFO | train | epoch 027 | loss 5.739 | ppl 53.4 | wps 22634.1 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 10911 | lr 0.000302739 | gnorm 0.486 | loss_scale 32 | train_wall 1033 | gb_free 9.6 | wall 31082
KL Stats: Epoch 27 Divergences: Uniform: 5.886434207031446 Unigram: 3.7832857088285965
2022-03-16 01:20:03 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 01:20:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:24:11 | INFO | train_inner | epoch 028:     89 / 407 loss=5.69, ppl=51.63, wps=20412, ups=0.31, wpb=65360.1, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.491, loss_scale=32, train_wall=257, gb_free=9.6, wall=31330
2022-03-16 01:25:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:28:50 | INFO | train_inner | epoch 028:    190 / 407 loss=5.708, ppl=52.28, wps=23443.6, ups=0.36, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.484, loss_scale=32, train_wall=257, gb_free=9.6, wall=31610
2022-03-16 01:31:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:33:31 | INFO | train_inner | epoch 028:    291 / 407 loss=5.733, ppl=53.19, wps=23339.6, ups=0.36, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.487, loss_scale=32, train_wall=257, gb_free=9.6, wall=31890
2022-03-16 01:37:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:38:10 | INFO | train_inner | epoch 028:    392 / 407 loss=5.734, ppl=53.22, wps=23515.6, ups=0.36, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.492, loss_scale=32, train_wall=256, gb_free=9.6, wall=32169
2022-03-16 01:38:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:39:28 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.83 | ppl 56.88 | wps 35663.1 | wpb 511.9 | bsz 1 | num_updates 11315 | best_loss 5.83
2022-03-16 01:39:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11315 updates
2022-03-16 01:39:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:39:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:39:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 28 @ 11315 updates, score 5.83) (writing took 1.927933574654162 seconds)
2022-03-16 01:39:30 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 01:39:30 | INFO | train | epoch 028 | loss 5.717 | ppl 52.6 | wps 22680.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 11315 | lr 0.000297285 | gnorm 0.489 | loss_scale 32 | train_wall 1034 | gb_free 9.6 | wall 32249
KL Stats: Epoch 28 Divergences: Uniform: 5.9079700586557 Unigram: 3.79500212062916
2022-03-16 01:39:30 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 01:39:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:43:26 | INFO | train_inner | epoch 029:     85 / 407 loss=5.682, ppl=51.33, wps=20668.1, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=11400, lr=0.000296174, gnorm=0.484, loss_scale=32, train_wall=254, gb_free=9.6, wall=32485
2022-03-16 01:44:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:48:03 | INFO | train_inner | epoch 029:    186 / 407 loss=5.702, ppl=52.04, wps=23627.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.484, loss_scale=32, train_wall=254, gb_free=9.6, wall=32763
2022-03-16 01:50:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:52:43 | INFO | train_inner | epoch 029:    287 / 407 loss=5.688, ppl=51.55, wps=23458.3, ups=0.36, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.485, loss_scale=32, train_wall=256, gb_free=9.6, wall=33042
2022-03-16 01:56:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:57:21 | INFO | train_inner | epoch 029:    388 / 407 loss=5.715, ppl=52.53, wps=23533.1, ups=0.36, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.494, loss_scale=32, train_wall=256, gb_free=9.6, wall=33320
2022-03-16 01:58:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:58:49 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.816 | ppl 56.34 | wps 36194.3 | wpb 511.9 | bsz 1 | num_updates 11719 | best_loss 5.816
2022-03-16 01:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11719 updates
2022-03-16 01:58:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:58:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 01:58:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 29 @ 11719 updates, score 5.816) (writing took 2.0536904176697135 seconds)
2022-03-16 01:58:51 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 01:58:51 | INFO | train | epoch 029 | loss 5.696 | ppl 51.85 | wps 22772.7 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 11719 | lr 0.000292116 | gnorm 0.487 | loss_scale 32 | train_wall 1030 | gb_free 9.6 | wall 33411
KL Stats: Epoch 29 Divergences: Uniform: 5.939929584362145 Unigram: 3.8070679460812444
2022-03-16 01:58:52 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 01:58:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:02:33 | INFO | train_inner | epoch 030:     81 / 407 loss=5.67, ppl=50.9, wps=20960.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=11800, lr=0.000291111, gnorm=0.486, loss_scale=32, train_wall=250, gb_free=9.6, wall=33632
2022-03-16 02:02:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:07:07 | INFO | train_inner | epoch 030:    182 / 407 loss=5.662, ppl=50.65, wps=23863.5, ups=0.36, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.487, loss_scale=32, train_wall=252, gb_free=9.6, wall=33907
2022-03-16 02:08:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:11:42 | INFO | train_inner | epoch 030:    283 / 407 loss=5.686, ppl=51.48, wps=23867.1, ups=0.36, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.48, loss_scale=32, train_wall=252, gb_free=9.6, wall=34182
2022-03-16 02:12:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:16:16 | INFO | train_inner | epoch 030:    384 / 407 loss=5.693, ppl=51.74, wps=23894.7, ups=0.36, wpb=65534.2, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.493, loss_scale=16, train_wall=252, gb_free=9.6, wall=34456
2022-03-16 02:17:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:17:56 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.801 | ppl 55.76 | wps 36017.6 | wpb 511.9 | bsz 1 | num_updates 12123 | best_loss 5.801
2022-03-16 02:17:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 12123 updates
2022-03-16 02:17:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 02:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 02:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 30 @ 12123 updates, score 5.801) (writing took 1.9079336235299706 seconds)
2022-03-16 02:17:58 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 02:17:58 | INFO | train | epoch 030 | loss 5.677 | ppl 51.18 | wps 23082.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 12123 | lr 0.000287207 | gnorm 0.486 | loss_scale 16 | train_wall 1017 | gb_free 9.6 | wall 34557
KL Stats: Epoch 30 Divergences: Uniform: 5.961712437335576 Unigram: 3.8181678722467534
2022-03-16 02:17:58 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 02:17:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:21:28 | INFO | train_inner | epoch 031:     77 / 407 loss=5.638, ppl=49.81, wps=20969.2, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=12200, lr=0.000286299, gnorm=0.49, loss_scale=32, train_wall=250, gb_free=9.6, wall=34767
2022-03-16 02:22:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:26:09 | INFO | train_inner | epoch 031:    178 / 407 loss=5.651, ppl=50.23, wps=23328.4, ups=0.36, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.494, loss_scale=16, train_wall=258, gb_free=9.6, wall=35048
2022-03-16 02:30:45 | INFO | train_inner | epoch 031:    278 / 407 loss=5.668, ppl=50.83, wps=23727.6, ups=0.36, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.485, loss_scale=32, train_wall=254, gb_free=9.6, wall=35325
2022-03-16 02:33:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:35:24 | INFO | train_inner | epoch 031:    379 / 407 loss=5.683, ppl=51.37, wps=23505.4, ups=0.36, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.487, loss_scale=32, train_wall=256, gb_free=9.6, wall=35603
2022-03-16 02:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:37:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.792 | ppl 55.42 | wps 36262.9 | wpb 511.9 | bsz 1 | num_updates 12528 | best_loss 5.792
2022-03-16 02:37:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12528 updates
2022-03-16 02:37:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 02:37:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 02:37:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 31 @ 12528 updates, score 5.792) (writing took 1.8860525274649262 seconds)
2022-03-16 02:37:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 02:37:19 | INFO | train | epoch 031 | loss 5.659 | ppl 50.51 | wps 22845.5 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 12528 | lr 0.000282526 | gnorm 0.489 | loss_scale 32 | train_wall 1030 | gb_free 9.6 | wall 35718
KL Stats: Epoch 31 Divergences: Uniform: 5.98949264362894 Unigram: 3.8305407162766665
2022-03-16 02:37:19 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 02:37:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:40:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:40:37 | INFO | train_inner | epoch 032:     73 / 407 loss=5.629, ppl=49.49, wps=20900, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=12600, lr=0.000281718, gnorm=0.487, loss_scale=32, train_wall=251, gb_free=9.6, wall=35916
2022-03-16 02:45:09 | INFO | train_inner | epoch 032:    173 / 407 loss=5.631, ppl=49.55, wps=24031.8, ups=0.37, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.487, loss_scale=32, train_wall=250, gb_free=9.6, wall=36189
2022-03-16 02:46:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:49:44 | INFO | train_inner | epoch 032:    274 / 407 loss=5.647, ppl=50.12, wps=23901.4, ups=0.36, wpb=65534.2, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.485, loss_scale=32, train_wall=252, gb_free=9.6, wall=36463
2022-03-16 02:52:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:54:17 | INFO | train_inner | epoch 032:    375 / 407 loss=5.665, ppl=50.72, wps=23929.2, ups=0.37, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.484, loss_scale=32, train_wall=252, gb_free=9.6, wall=36737
2022-03-16 02:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:56:20 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.782 | ppl 55.04 | wps 36362.7 | wpb 511.9 | bsz 1 | num_updates 12932 | best_loss 5.782
2022-03-16 02:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12932 updates
2022-03-16 02:56:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 02:56:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 02:56:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 32 @ 12932 updates, score 5.782) (writing took 1.8314502025023103 seconds)
2022-03-16 02:56:22 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 02:56:22 | INFO | train | epoch 032 | loss 5.642 | ppl 49.92 | wps 23141.9 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 12932 | lr 0.000278078 | gnorm 0.487 | loss_scale 32 | train_wall 1015 | gb_free 9.6 | wall 36862
KL Stats: Epoch 32 Divergences: Uniform: 6.010301659387215 Unigram: 3.839129900588197
2022-03-16 02:56:22 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 02:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:58:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:59:30 | INFO | train_inner | epoch 033:     69 / 407 loss=5.614, ppl=48.98, wps=20943.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=13000, lr=0.00027735, gnorm=0.492, loss_scale=32, train_wall=251, gb_free=9.6, wall=37049
2022-03-16 03:04:03 | INFO | train_inner | epoch 033:    169 / 407 loss=5.615, ppl=49.02, wps=23975, ups=0.37, wpb=65534.2, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.492, loss_scale=32, train_wall=251, gb_free=9.6, wall=37322
2022-03-16 03:04:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:08:43 | INFO | train_inner | epoch 033:    270 / 407 loss=5.635, ppl=49.69, wps=23422.3, ups=0.36, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.491, loss_scale=32, train_wall=257, gb_free=9.6, wall=37602
2022-03-16 03:10:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:13:23 | INFO | train_inner | epoch 033:    371 / 407 loss=5.64, ppl=49.88, wps=23358.5, ups=0.36, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.49, loss_scale=32, train_wall=258, gb_free=9.6, wall=37883
2022-03-16 03:15:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:15:40 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.774 | ppl 54.73 | wps 35448.7 | wpb 511.9 | bsz 1 | num_updates 13336 | best_loss 5.774
2022-03-16 03:15:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13336 updates
2022-03-16 03:15:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 03:15:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 03:15:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 33 @ 13336 updates, score 5.774) (writing took 1.8875648705288768 seconds)
2022-03-16 03:15:42 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 03:15:42 | INFO | train | epoch 033 | loss 5.626 | ppl 49.37 | wps 22818.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.491 | loss_scale 32 | train_wall 1028 | gb_free 9.6 | wall 38021
KL Stats: Epoch 33 Divergences: Uniform: 6.036191499253944 Unigram: 3.8505919976906076
2022-03-16 03:15:42 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 03:15:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:16:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:18:41 | INFO | train_inner | epoch 034:     65 / 407 loss=5.599, ppl=48.47, wps=20557, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=13400, lr=0.000273179, gnorm=0.498, loss_scale=32, train_wall=256, gb_free=9.6, wall=38201
2022-03-16 03:22:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:23:16 | INFO | train_inner | epoch 034:    166 / 407 loss=5.595, ppl=48.34, wps=23860.6, ups=0.36, wpb=65534.2, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.49, loss_scale=32, train_wall=252, gb_free=9.6, wall=38475
2022-03-16 03:26:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:27:53 | INFO | train_inner | epoch 034:    267 / 407 loss=5.623, ppl=49.27, wps=23693.1, ups=0.36, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.493, loss_scale=16, train_wall=254, gb_free=9.6, wall=38752
2022-03-16 03:32:31 | INFO | train_inner | epoch 034:    367 / 407 loss=5.63, ppl=49.52, wps=23506.2, ups=0.36, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.491, loss_scale=32, train_wall=256, gb_free=9.6, wall=39031
2022-03-16 03:34:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:35:00 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.769 | ppl 54.54 | wps 35371.2 | wpb 511.9 | bsz 1 | num_updates 13740 | best_loss 5.769
2022-03-16 03:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13740 updates
2022-03-16 03:35:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 03:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 03:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 34 @ 13740 updates, score 5.769) (writing took 1.9825436118990183 seconds)
2022-03-16 03:35:02 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 03:35:02 | INFO | train | epoch 034 | loss 5.61 | ppl 48.83 | wps 22810.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 13740 | lr 0.000269778 | gnorm 0.493 | loss_scale 32 | train_wall 1028 | gb_free 9.6 | wall 39181
KL Stats: Epoch 34 Divergences: Uniform: 6.055039832182511 Unigram: 3.860611054664995
2022-03-16 03:35:02 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 03:35:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:37:48 | INFO | train_inner | epoch 035:     60 / 407 loss=5.585, ppl=48.01, wps=20656.3, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=13800, lr=0.000269191, gnorm=0.488, loss_scale=32, train_wall=253, gb_free=9.6, wall=39347
2022-03-16 03:38:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:42:27 | INFO | train_inner | epoch 035:    161 / 407 loss=5.585, ppl=48, wps=23437.5, ups=0.36, wpb=65534.2, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.492, loss_scale=32, train_wall=257, gb_free=9.6, wall=39627
2022-03-16 03:44:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:47:07 | INFO | train_inner | epoch 035:    262 / 407 loss=5.602, ppl=48.58, wps=23417.4, ups=0.36, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.491, loss_scale=32, train_wall=257, gb_free=9.6, wall=39907
2022-03-16 03:50:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:51:44 | INFO | train_inner | epoch 035:    363 / 407 loss=5.615, ppl=49, wps=23680.7, ups=0.36, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.496, loss_scale=32, train_wall=254, gb_free=9.6, wall=40183
2022-03-16 03:53:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:54:23 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.759 | ppl 54.17 | wps 35572.2 | wpb 511.9 | bsz 1 | num_updates 14144 | best_loss 5.759
2022-03-16 03:54:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 14144 updates
2022-03-16 03:54:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 03:54:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 03:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 35 @ 14144 updates, score 5.759) (writing took 2.124013615772128 seconds)
2022-03-16 03:54:25 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 03:54:25 | INFO | train | epoch 035 | loss 5.595 | ppl 48.34 | wps 22749.3 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 14144 | lr 0.000265897 | gnorm 0.492 | loss_scale 32 | train_wall 1031 | gb_free 9.6 | wall 40344
KL Stats: Epoch 35 Divergences: Uniform: 6.074777499952547 Unigram: 3.8696548710422847
2022-03-16 03:54:25 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 03:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:55:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:57:03 | INFO | train_inner | epoch 036:     57 / 407 loss=5.582, ppl=47.89, wps=20515, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=14200, lr=0.000265372, gnorm=0.495, loss_scale=16, train_wall=256, gb_free=9.6, wall=40502
2022-03-16 04:01:38 | INFO | train_inner | epoch 036:    157 / 407 loss=5.573, ppl=47.59, wps=23762.2, ups=0.36, wpb=65534.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.495, loss_scale=32, train_wall=253, gb_free=9.6, wall=40778
2022-03-16 04:06:10 | INFO | train_inner | epoch 036:    257 / 407 loss=5.579, ppl=47.81, wps=24160.9, ups=0.37, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.498, loss_scale=32, train_wall=249, gb_free=9.6, wall=41049
2022-03-16 04:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:10:44 | INFO | train_inner | epoch 036:    358 / 407 loss=5.599, ppl=48.46, wps=23863.6, ups=0.36, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.492, loss_scale=32, train_wall=252, gb_free=9.6, wall=41324
2022-03-16 04:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:13:34 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.747 | ppl 53.69 | wps 36189.3 | wpb 511.9 | bsz 1 | num_updates 14549 | best_loss 5.747
2022-03-16 04:13:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14549 updates
2022-03-16 04:13:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 04:13:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 04:13:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 36 @ 14549 updates, score 5.747) (writing took 1.93336043599993 seconds)
2022-03-16 04:13:36 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 04:13:36 | INFO | train | epoch 036 | loss 5.582 | ppl 47.89 | wps 23030.6 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 14549 | lr 0.00026217 | gnorm 0.494 | loss_scale 32 | train_wall 1021 | gb_free 9.6 | wall 41496
KL Stats: Epoch 36 Divergences: Uniform: 6.093716548551691 Unigram: 3.8760638813498143
2022-03-16 04:13:36 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 04:13:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:13:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:15:58 | INFO | train_inner | epoch 037:     52 / 407 loss=5.558, ppl=47.1, wps=20835.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=0.492, loss_scale=32, train_wall=252, gb_free=9.6, wall=41637
2022-03-16 04:19:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:20:32 | INFO | train_inner | epoch 037:    153 / 407 loss=5.556, ppl=47.06, wps=23890, ups=0.36, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.492, loss_scale=32, train_wall=252, gb_free=9.6, wall=41912
2022-03-16 04:23:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:25:11 | INFO | train_inner | epoch 037:    254 / 407 loss=5.572, ppl=47.57, wps=23527.5, ups=0.36, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.491, loss_scale=16, train_wall=256, gb_free=9.6, wall=42190
2022-03-16 04:29:48 | INFO | train_inner | epoch 037:    354 / 407 loss=5.589, ppl=48.15, wps=23672.7, ups=0.36, wpb=65534.2, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.492, loss_scale=32, train_wall=254, gb_free=9.6, wall=42467
2022-03-16 04:32:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:32:51 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.738 | ppl 53.39 | wps 35701.3 | wpb 511.9 | bsz 1 | num_updates 14953 | best_loss 5.738
2022-03-16 04:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14953 updates
2022-03-16 04:32:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 04:32:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 04:32:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 37 @ 14953 updates, score 5.738) (writing took 1.9731173189356923 seconds)
2022-03-16 04:32:53 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 04:32:53 | INFO | train | epoch 037 | loss 5.568 | ppl 47.45 | wps 22875.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 14953 | lr 0.000258604 | gnorm 0.493 | loss_scale 32 | train_wall 1026 | gb_free 9.6 | wall 42652
KL Stats: Epoch 37 Divergences: Uniform: 6.110830294361119 Unigram: 3.8850010256099954
2022-03-16 04:32:53 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 04:32:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:35:03 | INFO | train_inner | epoch 038:     47 / 407 loss=5.555, ppl=47.01, wps=20730.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=0.495, loss_scale=32, train_wall=253, gb_free=9.6, wall=42782
2022-03-16 04:35:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:39:44 | INFO | train_inner | epoch 038:    148 / 407 loss=5.532, ppl=46.28, wps=23340.8, ups=0.36, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.497, loss_scale=32, train_wall=258, gb_free=9.6, wall=43063
2022-03-16 04:41:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:44:24 | INFO | train_inner | epoch 038:    249 / 407 loss=5.569, ppl=47.47, wps=23353.9, ups=0.36, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.491, loss_scale=32, train_wall=257, gb_free=9.6, wall=43344
2022-03-16 04:47:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:49:02 | INFO | train_inner | epoch 038:    350 / 407 loss=5.569, ppl=47.48, wps=23589.5, ups=0.36, wpb=65534.2, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.493, loss_scale=32, train_wall=255, gb_free=9.6, wall=43622
2022-03-16 04:51:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:52:14 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.732 | ppl 53.15 | wps 36071.3 | wpb 511.9 | bsz 1 | num_updates 15357 | best_loss 5.732
2022-03-16 04:52:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 15357 updates
2022-03-16 04:52:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 04:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 04:52:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 38 @ 15357 updates, score 5.732) (writing took 1.8821094008162618 seconds)
2022-03-16 04:52:16 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 04:52:16 | INFO | train | epoch 038 | loss 5.556 | ppl 47.03 | wps 22759.2 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 15357 | lr 0.00025518 | gnorm 0.493 | loss_scale 32 | train_wall 1031 | gb_free 9.6 | wall 43815
KL Stats: Epoch 38 Divergences: Uniform: 6.128913740851433 Unigram: 3.893447069898067
2022-03-16 04:52:16 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 04:52:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:53:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:54:15 | INFO | train_inner | epoch 039:     44 / 407 loss=5.554, ppl=46.97, wps=20880.7, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=0.497, loss_scale=16, train_wall=251, gb_free=9.6, wall=43935
2022-03-16 04:58:47 | INFO | train_inner | epoch 039:    144 / 407 loss=5.531, ppl=46.22, wps=24148.5, ups=0.37, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.504, loss_scale=16, train_wall=249, gb_free=9.6, wall=44206
2022-03-16 05:03:18 | INFO | train_inner | epoch 039:    244 / 407 loss=5.542, ppl=46.6, wps=24170.7, ups=0.37, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.501, loss_scale=32, train_wall=249, gb_free=9.6, wall=44477
2022-03-16 05:05:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:07:56 | INFO | train_inner | epoch 039:    345 / 407 loss=5.557, ppl=47.07, wps=23531.9, ups=0.36, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.504, loss_scale=32, train_wall=256, gb_free=9.6, wall=44756
2022-03-16 05:10:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:11:22 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.737 | ppl 53.32 | wps 36276.8 | wpb 511.9 | bsz 1 | num_updates 15762 | best_loss 5.732
2022-03-16 05:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15762 updates
2022-03-16 05:11:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 05:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 05:11:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt (epoch 39 @ 15762 updates, score 5.737) (writing took 0.9493467826396227 seconds)
2022-03-16 05:11:23 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 05:11:23 | INFO | train | epoch 039 | loss 5.543 | ppl 46.63 | wps 23117.2 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 15762 | lr 0.00025188 | gnorm 0.501 | loss_scale 32 | train_wall 1019 | gb_free 9.6 | wall 44962
KL Stats: Epoch 39 Divergences: Uniform: 6.148583716473241 Unigram: 3.9034101475183496
2022-03-16 05:11:23 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 05:11:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:12:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:13:09 | INFO | train_inner | epoch 040:     39 / 407 loss=5.54, ppl=46.53, wps=20903.3, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=0.493, loss_scale=32, train_wall=252, gb_free=9.6, wall=45068
2022-03-16 05:17:40 | INFO | train_inner | epoch 040:    139 / 407 loss=5.513, ppl=45.65, wps=24150.9, ups=0.37, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.5, loss_scale=32, train_wall=249, gb_free=9.6, wall=45340
2022-03-16 05:17:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:22:15 | INFO | train_inner | epoch 040:    240 / 407 loss=5.525, ppl=46.05, wps=23894.3, ups=0.36, wpb=65534.2, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.495, loss_scale=32, train_wall=252, gb_free=9.6, wall=45614
2022-03-16 05:23:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:26:51 | INFO | train_inner | epoch 040:    341 / 407 loss=5.552, ppl=46.92, wps=23717.4, ups=0.36, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.493, loss_scale=32, train_wall=254, gb_free=9.6, wall=45890
2022-03-16 05:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:30:31 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.729 | ppl 53.03 | wps 35246.9 | wpb 511.9 | bsz 1 | num_updates 16165 | best_loss 5.729
2022-03-16 05:30:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 16165 updates
2022-03-16 05:30:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 05:30:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 05:30:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 40 @ 16165 updates, score 5.729) (writing took 1.829840511083603 seconds)
2022-03-16 05:30:33 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 05:30:33 | INFO | train | epoch 040 | loss 5.531 | ppl 46.25 | wps 22949.2 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 16165 | lr 0.000248721 | gnorm 0.497 | loss_scale 32 | train_wall 1020 | gb_free 9.6 | wall 46113
KL Stats: Epoch 40 Divergences: Uniform: 6.167202718389383 Unigram: 3.910059611872105
2022-03-16 05:30:33 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 05:30:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:32:10 | INFO | train_inner | epoch 041:     35 / 407 loss=5.527, ppl=46.1, wps=20472, ups=0.31, wpb=65360.1, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=0.502, loss_scale=32, train_wall=256, gb_free=9.6, wall=46210
2022-03-16 05:36:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:36:49 | INFO | train_inner | epoch 041:    136 / 407 loss=5.51, ppl=45.56, wps=23499.4, ups=0.36, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.503, loss_scale=32, train_wall=256, gb_free=9.6, wall=46488
2022-03-16 05:37:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:41:29 | INFO | train_inner | epoch 041:    237 / 407 loss=5.521, ppl=45.93, wps=23440.8, ups=0.36, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.498, loss_scale=16, train_wall=257, gb_free=9.6, wall=46768
2022-03-16 05:46:03 | INFO | train_inner | epoch 041:    337 / 407 loss=5.534, ppl=46.32, wps=23886.1, ups=0.36, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.502, loss_scale=32, train_wall=252, gb_free=9.6, wall=47042
2022-03-16 05:48:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:49:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:49:49 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.725 | ppl 52.88 | wps 36248.4 | wpb 511.9 | bsz 1 | num_updates 16569 | best_loss 5.725
2022-03-16 05:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16569 updates
2022-03-16 05:49:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 05:49:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 05:49:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 41 @ 16569 updates, score 5.725) (writing took 1.8558550355955958 seconds)
2022-03-16 05:49:51 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 05:49:51 | INFO | train | epoch 041 | loss 5.52 | ppl 45.89 | wps 22849.6 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 16569 | lr 0.00024567 | gnorm 0.501 | loss_scale 32 | train_wall 1027 | gb_free 9.6 | wall 47270
KL Stats: Epoch 41 Divergences: Uniform: 6.18054856008372 Unigram: 3.917775732821653
2022-03-16 05:49:51 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 05:49:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:51:15 | INFO | train_inner | epoch 042:     31 / 407 loss=5.519, ppl=45.85, wps=20939.3, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=0.501, loss_scale=32, train_wall=251, gb_free=9.6, wall=47355
2022-03-16 05:55:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:55:49 | INFO | train_inner | epoch 042:    132 / 407 loss=5.491, ppl=44.98, wps=23937.1, ups=0.37, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.495, loss_scale=32, train_wall=251, gb_free=9.6, wall=47628
2022-03-16 05:56:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:00:23 | INFO | train_inner | epoch 042:    233 / 407 loss=5.504, ppl=45.37, wps=23868.2, ups=0.36, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.505, loss_scale=16, train_wall=252, gb_free=9.6, wall=47903
2022-03-16 06:04:57 | INFO | train_inner | epoch 042:    333 / 407 loss=5.53, ppl=46.2, wps=23940.1, ups=0.37, wpb=65534.2, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.497, loss_scale=32, train_wall=251, gb_free=9.6, wall=48177
2022-03-16 06:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:08:58 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.713 | ppl 52.47 | wps 35610.9 | wpb 511.9 | bsz 1 | num_updates 16974 | best_loss 5.713
2022-03-16 06:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16974 updates
2022-03-16 06:08:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 06:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 06:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 42 @ 16974 updates, score 5.713) (writing took 1.8335129674524069 seconds)
2022-03-16 06:09:00 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 06:09:00 | INFO | train | epoch 042 | loss 5.51 | ppl 45.56 | wps 23084.2 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 16974 | lr 0.000242721 | gnorm 0.498 | loss_scale 32 | train_wall 1019 | gb_free 9.6 | wall 48420
KL Stats: Epoch 42 Divergences: Uniform: 6.19624880780259 Unigram: 3.923537619209665
2022-03-16 06:09:00 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 06:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:09:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:10:15 | INFO | train_inner | epoch 043:     27 / 407 loss=5.513, ppl=45.67, wps=20584.7, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=0.501, loss_scale=32, train_wall=255, gb_free=9.6, wall=48494
2022-03-16 06:14:52 | INFO | train_inner | epoch 043:    127 / 407 loss=5.482, ppl=44.68, wps=23655.7, ups=0.36, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.5, loss_scale=32, train_wall=254, gb_free=9.6, wall=48771
2022-03-16 06:15:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:19:33 | INFO | train_inner | epoch 043:    228 / 407 loss=5.505, ppl=45.4, wps=23321.4, ups=0.36, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.499, loss_scale=32, train_wall=258, gb_free=9.6, wall=49052
2022-03-16 06:21:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:24:13 | INFO | train_inner | epoch 043:    329 / 407 loss=5.506, ppl=45.45, wps=23410.5, ups=0.36, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.506, loss_scale=32, train_wall=257, gb_free=9.6, wall=49332
2022-03-16 06:27:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:27:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:28:21 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.716 | ppl 52.58 | wps 36367.1 | wpb 511.9 | bsz 1 | num_updates 17377 | best_loss 5.713
2022-03-16 06:28:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 17377 updates
2022-03-16 06:28:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 06:28:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 06:28:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt (epoch 43 @ 17377 updates, score 5.716) (writing took 0.9419146124273539 seconds)
2022-03-16 06:28:22 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 06:28:22 | INFO | train | epoch 043 | loss 5.5 | ppl 45.24 | wps 22715.5 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 17377 | lr 0.00023989 | gnorm 0.502 | loss_scale 32 | train_wall 1032 | gb_free 9.6 | wall 49581
KL Stats: Epoch 43 Divergences: Uniform: 6.219386820780826 Unigram: 3.9335121434009124
2022-03-16 06:28:22 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 06:28:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:29:24 | INFO | train_inner | epoch 044:     23 / 407 loss=5.504, ppl=45.38, wps=20970, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=0.502, loss_scale=32, train_wall=251, gb_free=9.6, wall=49644
2022-03-16 06:33:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:33:59 | INFO | train_inner | epoch 044:    124 / 407 loss=5.471, ppl=44.35, wps=23885.1, ups=0.36, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.503, loss_scale=32, train_wall=252, gb_free=9.6, wall=49918
2022-03-16 06:38:31 | INFO | train_inner | epoch 044:    224 / 407 loss=5.49, ppl=44.95, wps=24116.4, ups=0.37, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.507, loss_scale=32, train_wall=249, gb_free=9.6, wall=50190
2022-03-16 06:39:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:43:05 | INFO | train_inner | epoch 044:    325 / 407 loss=5.504, ppl=45.38, wps=23846.3, ups=0.36, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.506, loss_scale=32, train_wall=252, gb_free=9.6, wall=50465
2022-03-16 06:43:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:46:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:47:26 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.703 | ppl 52.09 | wps 35492.6 | wpb 511.9 | bsz 1 | num_updates 17781 | best_loss 5.703
2022-03-16 06:47:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17781 updates
2022-03-16 06:47:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 06:47:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 06:47:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 44 @ 17781 updates, score 5.703) (writing took 1.8733413498848677 seconds)
2022-03-16 06:47:28 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 06:47:28 | INFO | train | epoch 044 | loss 5.49 | ppl 44.96 | wps 23083.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 17781 | lr 0.000237149 | gnorm 0.505 | loss_scale 16 | train_wall 1016 | gb_free 9.6 | wall 50728
KL Stats: Epoch 44 Divergences: Uniform: 6.233516151041035 Unigram: 3.93740706936251
2022-03-16 06:47:28 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 06:47:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:48:21 | INFO | train_inner | epoch 045:     19 / 407 loss=5.492, ppl=45, wps=20707.6, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=0.503, loss_scale=16, train_wall=253, gb_free=9.6, wall=50780
2022-03-16 06:52:57 | INFO | train_inner | epoch 045:    119 / 407 loss=5.451, ppl=43.75, wps=23704.3, ups=0.36, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.5, loss_scale=32, train_wall=253, gb_free=9.6, wall=51057
2022-03-16 06:56:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:57:36 | INFO | train_inner | epoch 045:    220 / 407 loss=5.475, ppl=44.48, wps=23536.7, ups=0.36, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.503, loss_scale=32, train_wall=256, gb_free=9.6, wall=51335
2022-03-16 07:00:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:02:11 | INFO | train_inner | epoch 045:    321 / 407 loss=5.506, ppl=45.46, wps=23828.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.505, loss_scale=16, train_wall=253, gb_free=9.6, wall=51610
2022-03-16 07:06:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:06:41 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.703 | ppl 52.1 | wps 36347.3 | wpb 511.9 | bsz 1 | num_updates 18186 | best_loss 5.703
2022-03-16 07:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 18186 updates
2022-03-16 07:06:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 07:06:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 07:06:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 45 @ 18186 updates, score 5.703) (writing took 1.8770188484340906 seconds)
2022-03-16 07:06:43 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 07:06:43 | INFO | train | epoch 045 | loss 5.481 | ppl 44.66 | wps 22976.2 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 18186 | lr 0.000234494 | gnorm 0.504 | loss_scale 16 | train_wall 1024 | gb_free 9.6 | wall 51882
KL Stats: Epoch 45 Divergences: Uniform: 6.251285537191897 Unigram: 3.945938754659296
2022-03-16 07:06:43 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 07:06:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:07:21 | INFO | train_inner | epoch 046:     14 / 407 loss=5.495, ppl=45.1, wps=21092.6, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=0.507, loss_scale=32, train_wall=249, gb_free=9.6, wall=51920
2022-03-16 07:11:52 | INFO | train_inner | epoch 046:    114 / 407 loss=5.448, ppl=43.64, wps=24139.1, ups=0.37, wpb=65534.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.503, loss_scale=32, train_wall=249, gb_free=9.6, wall=52192
2022-03-16 07:12:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:16:26 | INFO | train_inner | epoch 046:    215 / 407 loss=5.475, ppl=44.47, wps=23902.2, ups=0.36, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.501, loss_scale=32, train_wall=252, gb_free=9.6, wall=52466
2022-03-16 07:18:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:21:01 | INFO | train_inner | epoch 046:    316 / 407 loss=5.482, ppl=44.69, wps=23893.1, ups=0.36, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.505, loss_scale=32, train_wall=252, gb_free=9.6, wall=52740
2022-03-16 07:24:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:25:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:25:45 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.694 | ppl 51.78 | wps 35961.4 | wpb 511.9 | bsz 1 | num_updates 18590 | best_loss 5.694
2022-03-16 07:25:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18590 updates
2022-03-16 07:25:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 07:25:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 07:25:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 46 @ 18590 updates, score 5.694) (writing took 1.8251463267952204 seconds)
2022-03-16 07:25:47 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 07:25:47 | INFO | train | epoch 046 | loss 5.471 | ppl 44.36 | wps 23130.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 18590 | lr 0.000231932 | gnorm 0.502 | loss_scale 32 | train_wall 1015 | gb_free 9.6 | wall 53026
KL Stats: Epoch 46 Divergences: Uniform: 6.264130446413597 Unigram: 3.949685963569148
2022-03-16 07:25:47 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 07:25:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:26:14 | INFO | train_inner | epoch 047:     10 / 407 loss=5.482, ppl=44.7, wps=20871.2, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=18600, lr=0.000231869, gnorm=0.502, loss_scale=32, train_wall=252, gb_free=9.6, wall=53053
2022-03-16 07:30:45 | INFO | train_inner | epoch 047:    110 / 407 loss=5.437, ppl=43.31, wps=24140.3, ups=0.37, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.506, loss_scale=32, train_wall=249, gb_free=9.6, wall=53325
2022-03-16 07:30:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:35:21 | INFO | train_inner | epoch 047:    211 / 407 loss=5.448, ppl=43.66, wps=23772.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.503, loss_scale=32, train_wall=253, gb_free=9.6, wall=53601
2022-03-16 07:36:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:39:55 | INFO | train_inner | epoch 047:    312 / 407 loss=5.48, ppl=44.62, wps=23928.1, ups=0.37, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.507, loss_scale=32, train_wall=252, gb_free=9.6, wall=53874
2022-03-16 07:42:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:44:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:44:51 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.691 | ppl 51.67 | wps 35852.3 | wpb 511.9 | bsz 1 | num_updates 18994 | best_loss 5.691
2022-03-16 07:44:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18994 updates
2022-03-16 07:44:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 07:44:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 07:44:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 47 @ 18994 updates, score 5.691) (writing took 1.905391937121749 seconds)
2022-03-16 07:44:53 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 07:44:53 | INFO | train | epoch 047 | loss 5.463 | ppl 44.1 | wps 23081.9 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 18994 | lr 0.000229452 | gnorm 0.505 | loss_scale 32 | train_wall 1017 | gb_free 9.6 | wall 54172
KL Stats: Epoch 47 Divergences: Uniform: 6.281085896856023 Unigram: 3.958085664089782
2022-03-16 07:44:53 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 07:44:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:45:09 | INFO | train_inner | epoch 048:      6 / 407 loss=5.489, ppl=44.91, wps=20786.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=19000, lr=0.000229416, gnorm=0.504, loss_scale=32, train_wall=253, gb_free=9.6, wall=54189
2022-03-16 07:49:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:49:45 | INFO | train_inner | epoch 048:    107 / 407 loss=5.43, ppl=43.13, wps=23798.5, ups=0.36, wpb=65534.2, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.501, loss_scale=32, train_wall=253, gb_free=9.6, wall=54464
2022-03-16 07:54:17 | INFO | train_inner | epoch 048:    207 / 407 loss=5.449, ppl=43.69, wps=24114.3, ups=0.37, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.503, loss_scale=32, train_wall=249, gb_free=9.6, wall=54736
2022-03-16 07:55:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:56:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:58:53 | INFO | train_inner | epoch 048:    309 / 407 loss=5.462, ppl=44.08, wps=23710.4, ups=0.36, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.507, loss_scale=16, train_wall=254, gb_free=9.6, wall=55012
2022-03-16 08:03:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:03:55 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.684 | ppl 51.41 | wps 36404.1 | wpb 511.9 | bsz 1 | num_updates 19398 | best_loss 5.684
2022-03-16 08:03:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 19398 updates
2022-03-16 08:03:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 08:03:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 08:03:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 48 @ 19398 updates, score 5.684) (writing took 1.9868063190951943 seconds)
2022-03-16 08:03:57 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 08:03:57 | INFO | train | epoch 048 | loss 5.454 | ppl 43.83 | wps 23127.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 19398 | lr 0.00022705 | gnorm 0.503 | loss_scale 32 | train_wall 1015 | gb_free 9.6 | wall 55316
KL Stats: Epoch 48 Divergences: Uniform: 6.293667318029348 Unigram: 3.9627601561339136
2022-03-16 08:03:57 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 08:03:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:04:02 | INFO | train_inner | epoch 049:      2 / 407 loss=5.472, ppl=44.39, wps=21119.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=19400, lr=0.000227038, gnorm=0.502, loss_scale=32, train_wall=249, gb_free=9.6, wall=55322
2022-03-16 08:05:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:08:40 | INFO | train_inner | epoch 049:    103 / 407 loss=5.419, ppl=42.77, wps=23573, ups=0.36, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.505, loss_scale=16, train_wall=255, gb_free=9.6, wall=55600
2022-03-16 08:13:18 | INFO | train_inner | epoch 049:    203 / 407 loss=5.452, ppl=43.78, wps=23600.5, ups=0.36, wpb=65534.2, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.503, loss_scale=32, train_wall=255, gb_free=9.6, wall=55878
2022-03-16 08:13:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:17:59 | INFO | train_inner | epoch 049:    304 / 407 loss=5.453, ppl=43.8, wps=23374, ups=0.36, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.503, loss_scale=16, train_wall=257, gb_free=9.6, wall=56158
2022-03-16 08:22:31 | INFO | train_inner | epoch 049:    404 / 407 loss=5.459, ppl=43.99, wps=24038.1, ups=0.37, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.506, loss_scale=32, train_wall=250, gb_free=9.6, wall=56431
2022-03-16 08:22:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:23:15 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.683 | ppl 51.39 | wps 36357.1 | wpb 511.9 | bsz 1 | num_updates 19803 | best_loss 5.683
2022-03-16 08:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19803 updates
2022-03-16 08:23:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 08:23:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 08:23:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 49 @ 19803 updates, score 5.683) (writing took 1.8450055178254843 seconds)
2022-03-16 08:23:17 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 08:23:17 | INFO | train | epoch 049 | loss 5.446 | ppl 43.6 | wps 22859.4 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 19803 | lr 0.000224716 | gnorm 0.505 | loss_scale 32 | train_wall 1030 | gb_free 9.6 | wall 56477
KL Stats: Epoch 49 Divergences: Uniform: 6.305580258513725 Unigram: 3.9681905327937805
2022-03-16 08:23:17 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 08:23:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:26:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:27:44 | INFO | train_inner | epoch 050:     98 / 407 loss=5.413, ppl=42.6, wps=20920.9, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=0.505, loss_scale=32, train_wall=251, gb_free=9.6, wall=56743
2022-03-16 08:32:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:32:22 | INFO | train_inner | epoch 050:    199 / 407 loss=5.435, ppl=43.26, wps=23528.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.506, loss_scale=32, train_wall=256, gb_free=9.6, wall=57022
2022-03-16 08:36:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:37:06 | INFO | train_inner | epoch 050:    300 / 407 loss=5.444, ppl=43.52, wps=23109.5, ups=0.35, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.512, loss_scale=16, train_wall=260, gb_free=9.6, wall=57305
2022-03-16 08:41:42 | INFO | train_inner | epoch 050:    400 / 407 loss=5.462, ppl=44.08, wps=23732.1, ups=0.36, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.507, loss_scale=16, train_wall=253, gb_free=9.6, wall=57581
2022-03-16 08:42:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:42:38 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.677 | ppl 51.15 | wps 35613.1 | wpb 511.9 | bsz 1 | num_updates 20207 | best_loss 5.677
2022-03-16 08:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 20207 updates
2022-03-16 08:42:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 08:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 08:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 50 @ 20207 updates, score 5.677) (writing took 1.8829052755609155 seconds)
2022-03-16 08:42:40 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 08:42:40 | INFO | train | epoch 050 | loss 5.438 | ppl 43.36 | wps 22755.7 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 20207 | lr 0.000222459 | gnorm 0.507 | loss_scale 16 | train_wall 1031 | gb_free 9.6 | wall 57639
KL Stats: Epoch 50 Divergences: Uniform: 6.319215245174567 Unigram: 3.973114551463851
2022-03-16 08:42:40 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 08:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:46:56 | INFO | train_inner | epoch 051:     93 / 407 loss=5.404, ppl=42.33, wps=20784.9, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=0.513, loss_scale=32, train_wall=252, gb_free=9.6, wall=57896
2022-03-16 08:49:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:50:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:51:33 | INFO | train_inner | epoch 051:    195 / 407 loss=5.427, ppl=43.02, wps=23668.6, ups=0.36, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.505, loss_scale=16, train_wall=254, gb_free=9.6, wall=58173
2022-03-16 08:56:04 | INFO | train_inner | epoch 051:    295 / 407 loss=5.436, ppl=43.3, wps=24169.6, ups=0.37, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.513, loss_scale=32, train_wall=249, gb_free=9.6, wall=58444
2022-03-16 09:00:36 | INFO | train_inner | epoch 051:    395 / 407 loss=5.454, ppl=43.83, wps=24170.7, ups=0.37, wpb=65534.2, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.516, loss_scale=32, train_wall=249, gb_free=9.6, wall=58715
2022-03-16 09:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:01:45 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.672 | ppl 51 | wps 35621.6 | wpb 511.9 | bsz 1 | num_updates 20612 | best_loss 5.672
2022-03-16 09:01:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20612 updates
2022-03-16 09:01:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 09:01:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 09:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 51 @ 20612 updates, score 5.672) (writing took 1.975434553809464 seconds)
2022-03-16 09:01:47 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 09:01:47 | INFO | train | epoch 051 | loss 5.43 | ppl 43.13 | wps 23128.9 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 20612 | lr 0.000220262 | gnorm 0.512 | loss_scale 32 | train_wall 1017 | gb_free 9.6 | wall 58786
KL Stats: Epoch 51 Divergences: Uniform: 6.330894315884992 Unigram: 3.979064285790645
2022-03-16 09:01:47 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 09:01:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:02:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:05:50 | INFO | train_inner | epoch 052:     89 / 407 loss=5.408, ppl=42.45, wps=20774.1, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=0.51, loss_scale=32, train_wall=252, gb_free=9.6, wall=59030
2022-03-16 09:07:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:10:25 | INFO | train_inner | epoch 052:    190 / 407 loss=5.421, ppl=42.83, wps=23886.7, ups=0.36, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.506, loss_scale=16, train_wall=252, gb_free=9.6, wall=59304
2022-03-16 09:14:57 | INFO | train_inner | epoch 052:    290 / 407 loss=5.43, ppl=43.11, wps=24014, ups=0.37, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.511, loss_scale=32, train_wall=251, gb_free=9.6, wall=59577
2022-03-16 09:19:29 | INFO | train_inner | epoch 052:    390 / 407 loss=5.436, ppl=43.28, wps=24086.4, ups=0.37, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.511, loss_scale=32, train_wall=250, gb_free=9.6, wall=59849
2022-03-16 09:19:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:20:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:20:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.671 | ppl 50.94 | wps 35841.3 | wpb 511.9 | bsz 1 | num_updates 21016 | best_loss 5.671
2022-03-16 09:20:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 21016 updates
2022-03-16 09:20:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 09:20:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 09:20:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 52 @ 21016 updates, score 5.671) (writing took 3.081686455756426 seconds)
2022-03-16 09:20:56 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 09:20:56 | INFO | train | epoch 052 | loss 5.423 | ppl 42.9 | wps 23026.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 21016 | lr 0.000218135 | gnorm 0.51 | loss_scale 32 | train_wall 1018 | gb_free 9.6 | wall 59935
KL Stats: Epoch 52 Divergences: Uniform: 6.344120859608292 Unigram: 3.984061200000538
2022-03-16 09:20:56 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 09:20:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:24:46 | INFO | train_inner | epoch 053:     84 / 407 loss=5.393, ppl=42.02, wps=20663.2, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=0.515, loss_scale=32, train_wall=253, gb_free=9.6, wall=60165
2022-03-16 09:26:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:29:22 | INFO | train_inner | epoch 053:    185 / 407 loss=5.401, ppl=42.27, wps=23772.4, ups=0.36, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.503, loss_scale=32, train_wall=253, gb_free=9.6, wall=60441
2022-03-16 09:32:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:33:56 | INFO | train_inner | epoch 053:    286 / 407 loss=5.44, ppl=43.42, wps=23884.6, ups=0.36, wpb=65534.2, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.511, loss_scale=32, train_wall=252, gb_free=9.6, wall=60715
2022-03-16 09:34:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:38:31 | INFO | train_inner | epoch 053:    387 / 407 loss=5.436, ppl=43.29, wps=23779.3, ups=0.36, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.512, loss_scale=16, train_wall=253, gb_free=9.6, wall=60991
2022-03-16 09:39:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:40:04 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.672 | ppl 50.99 | wps 35471.9 | wpb 511.9 | bsz 1 | num_updates 21420 | best_loss 5.671
2022-03-16 09:40:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 21420 updates
2022-03-16 09:40:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 09:40:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 09:40:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt (epoch 53 @ 21420 updates, score 5.672) (writing took 0.8590538930147886 seconds)
2022-03-16 09:40:05 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 09:40:05 | INFO | train | epoch 053 | loss 5.416 | ppl 42.71 | wps 23030.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 21420 | lr 0.000216068 | gnorm 0.51 | loss_scale 16 | train_wall 1020 | gb_free 9.6 | wall 61084
KL Stats: Epoch 53 Divergences: Uniform: 6.361743804421555 Unigram: 3.991190576039155
2022-03-16 09:40:05 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 09:40:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:43:46 | INFO | train_inner | epoch 054:     80 / 407 loss=5.375, ppl=41.49, wps=20793.2, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=0.514, loss_scale=32, train_wall=253, gb_free=9.6, wall=61305
2022-03-16 09:47:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:48:21 | INFO | train_inner | epoch 054:    181 / 407 loss=5.403, ppl=42.33, wps=23811.4, ups=0.36, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.511, loss_scale=32, train_wall=253, gb_free=9.6, wall=61580
2022-03-16 09:52:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:53:00 | INFO | train_inner | epoch 054:    282 / 407 loss=5.421, ppl=42.84, wps=23482, ups=0.36, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.505, loss_scale=16, train_wall=256, gb_free=9.6, wall=61860
2022-03-16 09:57:33 | INFO | train_inner | epoch 054:    382 / 407 loss=5.433, ppl=43.2, wps=23994, ups=0.37, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.51, loss_scale=16, train_wall=251, gb_free=9.6, wall=62133
2022-03-16 09:58:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:59:18 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.668 | ppl 50.85 | wps 36080.3 | wpb 511.9 | bsz 1 | num_updates 21825 | best_loss 5.668
2022-03-16 09:59:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21825 updates
2022-03-16 09:59:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 09:59:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 09:59:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 54 @ 21825 updates, score 5.668) (writing took 1.951943140476942 seconds)
2022-03-16 09:59:20 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 09:59:20 | INFO | train | epoch 054 | loss 5.41 | ppl 42.5 | wps 22962.1 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 21825 | lr 0.000214054 | gnorm 0.51 | loss_scale 32 | train_wall 1025 | gb_free 9.6 | wall 62239
KL Stats: Epoch 54 Divergences: Uniform: 6.373748587240227 Unigram: 3.9953187845939935
2022-03-16 09:59:20 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 09:59:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:02:48 | INFO | train_inner | epoch 055:     75 / 407 loss=5.395, ppl=42.08, wps=20797.1, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=0.51, loss_scale=32, train_wall=253, gb_free=9.6, wall=62447
2022-03-16 10:05:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:05:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:07:25 | INFO | train_inner | epoch 055:    177 / 407 loss=5.397, ppl=42.14, wps=23608.6, ups=0.36, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.507, loss_scale=16, train_wall=255, gb_free=9.6, wall=62725
2022-03-16 10:11:57 | INFO | train_inner | epoch 055:    277 / 407 loss=5.4, ppl=42.22, wps=24143.2, ups=0.37, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.511, loss_scale=32, train_wall=249, gb_free=9.6, wall=62996
2022-03-16 10:16:29 | INFO | train_inner | epoch 055:    377 / 407 loss=5.417, ppl=42.73, wps=24052.4, ups=0.37, wpb=65534.2, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.509, loss_scale=32, train_wall=250, gb_free=9.6, wall=63268
2022-03-16 10:17:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:17:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:18:27 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.661 | ppl 50.6 | wps 35776.8 | wpb 511.9 | bsz 1 | num_updates 22229 | best_loss 5.661
2022-03-16 10:18:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 22229 updates
2022-03-16 10:18:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 10:18:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 10:18:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 55 @ 22229 updates, score 5.661) (writing took 1.887497017160058 seconds)
2022-03-16 10:18:29 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 10:18:29 | INFO | train | epoch 055 | loss 5.403 | ppl 42.31 | wps 23018.6 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 22229 | lr 0.0002121 | gnorm 0.51 | loss_scale 32 | train_wall 1020 | gb_free 9.6 | wall 63389
KL Stats: Epoch 55 Divergences: Uniform: 6.385209641072497 Unigram: 4.00013466718234
2022-03-16 10:18:29 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 10:18:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:21:45 | INFO | train_inner | epoch 056:     71 / 407 loss=5.383, ppl=41.74, wps=20700.6, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=22300, lr=0.000211762, gnorm=0.521, loss_scale=32, train_wall=254, gb_free=9.6, wall=63584
2022-03-16 10:24:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:25:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:26:25 | INFO | train_inner | epoch 056:    173 / 407 loss=5.386, ppl=41.82, wps=23373.7, ups=0.36, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.515, loss_scale=16, train_wall=257, gb_free=9.6, wall=63865
2022-03-16 10:31:02 | INFO | train_inner | epoch 056:    273 / 407 loss=5.403, ppl=42.32, wps=23657.1, ups=0.36, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.513, loss_scale=16, train_wall=254, gb_free=9.6, wall=64142
2022-03-16 10:35:38 | INFO | train_inner | epoch 056:    373 / 407 loss=5.416, ppl=42.71, wps=23757.1, ups=0.36, wpb=65534.2, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.517, loss_scale=32, train_wall=253, gb_free=9.6, wall=64418
2022-03-16 10:37:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:37:46 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.661 | ppl 50.61 | wps 36433.4 | wpb 511.9 | bsz 1 | num_updates 22634 | best_loss 5.661
2022-03-16 10:37:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 22634 updates
2022-03-16 10:37:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 10:37:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 10:37:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 56 @ 22634 updates, score 5.661) (writing took 1.8591475719586015 seconds)
2022-03-16 10:37:48 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 10:37:48 | INFO | train | epoch 056 | loss 5.396 | ppl 42.12 | wps 22892.2 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 22634 | lr 0.000210194 | gnorm 0.516 | loss_scale 32 | train_wall 1028 | gb_free 9.6 | wall 64547
KL Stats: Epoch 56 Divergences: Uniform: 6.396939736515339 Unigram: 4.002136017366367
2022-03-16 10:37:48 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 10:37:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:38:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:40:50 | INFO | train_inner | epoch 057:     67 / 407 loss=5.378, ppl=41.57, wps=20982.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=22700, lr=0.000209888, gnorm=0.517, loss_scale=32, train_wall=251, gb_free=9.6, wall=64729
2022-03-16 10:43:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:45:24 | INFO | train_inner | epoch 057:    168 / 407 loss=5.377, ppl=41.57, wps=23901.8, ups=0.36, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.513, loss_scale=32, train_wall=252, gb_free=9.6, wall=65003
2022-03-16 10:49:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:50:00 | INFO | train_inner | epoch 057:    269 / 407 loss=5.39, ppl=41.93, wps=23756.1, ups=0.36, wpb=65534.2, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.51, loss_scale=32, train_wall=253, gb_free=9.6, wall=65279
2022-03-16 10:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:54:35 | INFO | train_inner | epoch 057:    370 / 407 loss=5.412, ppl=42.59, wps=23788.8, ups=0.36, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.517, loss_scale=16, train_wall=253, gb_free=9.6, wall=65555
2022-03-16 10:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:56:55 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.657 | ppl 50.46 | wps 35212.7 | wpb 511.9 | bsz 1 | num_updates 23037 | best_loss 5.657
2022-03-16 10:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 23037 updates
2022-03-16 10:56:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 10:56:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 10:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 57 @ 23037 updates, score 5.657) (writing took 1.9222951401025057 seconds)
2022-03-16 10:56:57 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 10:56:57 | INFO | train | epoch 057 | loss 5.389 | ppl 41.91 | wps 22971.9 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 23037 | lr 0.000208347 | gnorm 0.514 | loss_scale 32 | train_wall 1018 | gb_free 9.6 | wall 65696
KL Stats: Epoch 57 Divergences: Uniform: 6.415332356713476 Unigram: 4.008996760271176
2022-03-16 10:56:57 | INFO | fairseq.trainer | begin training epoch 58
2022-03-16 10:56:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:59:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:59:54 | INFO | train_inner | epoch 058:     64 / 407 loss=5.379, ppl=41.6, wps=20467.6, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=23100, lr=0.000208063, gnorm=0.513, loss_scale=16, train_wall=256, gb_free=9.6, wall=65874
2022-03-16 11:04:31 | INFO | train_inner | epoch 058:    164 / 407 loss=5.369, ppl=41.32, wps=23686.7, ups=0.36, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.515, loss_scale=16, train_wall=254, gb_free=9.6, wall=66151
2022-03-16 11:09:06 | INFO | train_inner | epoch 058:    264 / 407 loss=5.386, ppl=41.82, wps=23808, ups=0.36, wpb=65534.2, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.515, loss_scale=32, train_wall=253, gb_free=9.6, wall=66426
2022-03-16 11:11:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:13:46 | INFO | train_inner | epoch 058:    365 / 407 loss=5.4, ppl=42.23, wps=23432.9, ups=0.36, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.512, loss_scale=32, train_wall=257, gb_free=9.6, wall=66706
2022-03-16 11:15:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:16:20 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 5.661 | ppl 50.59 | wps 35287.4 | wpb 511.9 | bsz 1 | num_updates 23442 | best_loss 5.657
2022-03-16 11:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 23442 updates
2022-03-16 11:16:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 11:16:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 11:16:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt (epoch 58 @ 23442 updates, score 5.661) (writing took 0.9531750362366438 seconds)
2022-03-16 11:16:21 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-16 11:16:21 | INFO | train | epoch 058 | loss 5.384 | ppl 41.76 | wps 22784.8 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 23442 | lr 0.000206539 | gnorm 0.513 | loss_scale 32 | train_wall 1033 | gb_free 9.6 | wall 66861
KL Stats: Epoch 58 Divergences: Uniform: 6.427327886015585 Unigram: 4.013488524302583
2022-03-16 11:16:21 | INFO | fairseq.trainer | begin training epoch 59
2022-03-16 11:16:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:17:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:19:01 | INFO | train_inner | epoch 059:     59 / 407 loss=5.377, ppl=41.56, wps=20746, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=23500, lr=0.000206284, gnorm=0.512, loss_scale=32, train_wall=254, gb_free=9.6, wall=67021
2022-03-16 11:23:32 | INFO | train_inner | epoch 059:    159 / 407 loss=5.358, ppl=41.02, wps=24172.6, ups=0.37, wpb=65534.2, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.516, loss_scale=32, train_wall=249, gb_free=9.6, wall=67292
2022-03-16 11:23:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:28:06 | INFO | train_inner | epoch 059:    260 / 407 loss=5.385, ppl=41.77, wps=23907.7, ups=0.36, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.518, loss_scale=32, train_wall=252, gb_free=9.6, wall=67566
2022-03-16 11:29:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:32:44 | INFO | train_inner | epoch 059:    361 / 407 loss=5.39, ppl=41.94, wps=23619.9, ups=0.36, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.51, loss_scale=32, train_wall=255, gb_free=9.6, wall=67843
2022-03-16 11:34:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:35:28 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.653 | ppl 50.31 | wps 35787.6 | wpb 511.9 | bsz 1 | num_updates 23846 | best_loss 5.653
2022-03-16 11:35:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23846 updates
2022-03-16 11:35:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 11:35:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 11:35:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 59 @ 23846 updates, score 5.653) (writing took 1.9216029001399875 seconds)
2022-03-16 11:35:30 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-16 11:35:30 | INFO | train | epoch 059 | loss 5.377 | ppl 41.56 | wps 23037.3 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 23846 | lr 0.000204782 | gnorm 0.514 | loss_scale 32 | train_wall 1019 | gb_free 9.6 | wall 68009
KL Stats: Epoch 59 Divergences: Uniform: 6.438895289446125 Unigram: 4.018701194708146
2022-03-16 11:35:30 | INFO | fairseq.trainer | begin training epoch 60
2022-03-16 11:35:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:36:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:37:59 | INFO | train_inner | epoch 060:     55 / 407 loss=5.369, ppl=41.32, wps=20762, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=23900, lr=0.000204551, gnorm=0.512, loss_scale=32, train_wall=253, gb_free=9.6, wall=68158
2022-03-16 11:41:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:42:37 | INFO | train_inner | epoch 060:    156 / 407 loss=5.357, ppl=40.97, wps=23522.6, ups=0.36, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.519, loss_scale=16, train_wall=256, gb_free=9.6, wall=68437
2022-03-16 11:47:15 | INFO | train_inner | epoch 060:    256 / 407 loss=5.367, ppl=41.28, wps=23575.7, ups=0.36, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.517, loss_scale=32, train_wall=255, gb_free=9.6, wall=68715
2022-03-16 11:51:50 | INFO | train_inner | epoch 060:    356 / 407 loss=5.396, ppl=42.11, wps=23852.3, ups=0.36, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.514, loss_scale=32, train_wall=252, gb_free=9.6, wall=68989
2022-03-16 11:52:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:54:47 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.65 | ppl 50.22 | wps 36136.4 | wpb 511.9 | bsz 1 | num_updates 24250 | best_loss 5.65
2022-03-16 11:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 24250 updates
2022-03-16 11:54:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 11:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 11:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 60 @ 24250 updates, score 5.65) (writing took 1.8139441851526499 seconds)
2022-03-16 11:54:49 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-16 11:54:49 | INFO | train | epoch 060 | loss 5.372 | ppl 41.4 | wps 22820.3 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 24250 | lr 0.000203069 | gnorm 0.515 | loss_scale 32 | train_wall 1029 | gb_free 9.6 | wall 69168
KL Stats: Epoch 60 Divergences: Uniform: 6.448809391957283 Unigram: 4.022883758051585
2022-03-16 11:54:49 | INFO | fairseq.trainer | begin training epoch 61
2022-03-16 11:54:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:57:05 | INFO | train_inner | epoch 061:     50 / 407 loss=5.366, ppl=41.25, wps=20764.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=24300, lr=0.00020286, gnorm=0.514, loss_scale=32, train_wall=253, gb_free=9.6, wall=69304
2022-03-16 11:59:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:01:38 | INFO | train_inner | epoch 061:    151 / 407 loss=5.355, ppl=40.91, wps=23942.7, ups=0.37, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.516, loss_scale=32, train_wall=251, gb_free=9.6, wall=69578
2022-03-16 12:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:06:14 | INFO | train_inner | epoch 061:    252 / 407 loss=5.362, ppl=41.12, wps=23791.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.519, loss_scale=32, train_wall=253, gb_free=9.6, wall=69853
2022-03-16 12:10:49 | INFO | train_inner | epoch 061:    352 / 407 loss=5.388, ppl=41.87, wps=23786.8, ups=0.36, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.519, loss_scale=32, train_wall=253, gb_free=9.6, wall=70129
2022-03-16 12:11:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:13:58 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 5.65 | ppl 50.22 | wps 35499.2 | wpb 511.9 | bsz 1 | num_updates 24654 | best_loss 5.65
2022-03-16 12:13:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 24654 updates
2022-03-16 12:13:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 12:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 12:14:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 61 @ 24654 updates, score 5.65) (writing took 1.9357719151303172 seconds)
2022-03-16 12:14:00 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-16 12:14:00 | INFO | train | epoch 061 | loss 5.366 | ppl 41.25 | wps 22983.2 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 24654 | lr 0.000201399 | gnorm 0.517 | loss_scale 32 | train_wall 1021 | gb_free 9.6 | wall 70320
KL Stats: Epoch 61 Divergences: Uniform: 6.460752833182544 Unigram: 4.027856013453961
2022-03-16 12:14:00 | INFO | fairseq.trainer | begin training epoch 62
2022-03-16 12:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:16:05 | INFO | train_inner | epoch 062:     46 / 407 loss=5.365, ppl=41.21, wps=20694.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=24700, lr=0.000201211, gnorm=0.511, loss_scale=32, train_wall=253, gb_free=9.6, wall=70445
2022-03-16 12:17:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:20:39 | INFO | train_inner | epoch 062:    147 / 407 loss=5.343, ppl=40.58, wps=23942, ups=0.37, wpb=65534.2, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.517, loss_scale=32, train_wall=251, gb_free=9.6, wall=70718
2022-03-16 12:23:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:25:15 | INFO | train_inner | epoch 062:    248 / 407 loss=5.364, ppl=41.17, wps=23761.5, ups=0.36, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.521, loss_scale=32, train_wall=253, gb_free=9.6, wall=70994
2022-03-16 12:26:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:29:51 | INFO | train_inner | epoch 062:    349 / 407 loss=5.378, ppl=41.57, wps=23748.2, ups=0.36, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.518, loss_scale=16, train_wall=253, gb_free=9.6, wall=71270
2022-03-16 12:32:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:33:06 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 5.644 | ppl 49.99 | wps 35709.9 | wpb 511.9 | bsz 1 | num_updates 25058 | best_loss 5.644
2022-03-16 12:33:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 25058 updates
2022-03-16 12:33:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 12:33:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 12:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 62 @ 25058 updates, score 5.644) (writing took 1.8492826949805021 seconds)
2022-03-16 12:33:08 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-16 12:33:08 | INFO | train | epoch 062 | loss 5.361 | ppl 41.09 | wps 23052.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 25058 | lr 0.000199768 | gnorm 0.518 | loss_scale 32 | train_wall 1018 | gb_free 9.6 | wall 71468
KL Stats: Epoch 62 Divergences: Uniform: 6.472428074050988 Unigram: 4.031589433775241
2022-03-16 12:33:08 | INFO | fairseq.trainer | begin training epoch 63
2022-03-16 12:33:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:35:04 | INFO | train_inner | epoch 063:     42 / 407 loss=5.353, ppl=40.86, wps=20866.8, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=25100, lr=0.000199601, gnorm=0.521, loss_scale=32, train_wall=251, gb_free=9.6, wall=71583
2022-03-16 12:38:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:39:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:39:41 | INFO | train_inner | epoch 063:    144 / 407 loss=5.344, ppl=40.62, wps=23624.2, ups=0.36, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.525, loss_scale=16, train_wall=255, gb_free=9.6, wall=71861
2022-03-16 12:44:14 | INFO | train_inner | epoch 063:    244 / 407 loss=5.358, ppl=41.03, wps=24009.2, ups=0.37, wpb=65534.2, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.521, loss_scale=16, train_wall=251, gb_free=9.6, wall=72134
2022-03-16 12:48:47 | INFO | train_inner | epoch 063:    344 / 407 loss=5.369, ppl=41.31, wps=24013, ups=0.37, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.524, loss_scale=32, train_wall=251, gb_free=9.6, wall=72407
2022-03-16 12:50:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:51:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:51:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:52:15 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 5.643 | ppl 49.98 | wps 36419.1 | wpb 511.9 | bsz 1 | num_updates 25461 | best_loss 5.643
2022-03-16 12:52:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 25461 updates
2022-03-16 12:52:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 12:52:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 12:52:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 63 @ 25461 updates, score 5.643) (writing took 2.048210274428129 seconds)
2022-03-16 12:52:17 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-16 12:52:17 | INFO | train | epoch 063 | loss 5.356 | ppl 40.96 | wps 22970.2 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 25461 | lr 0.000198181 | gnorm 0.522 | loss_scale 16 | train_wall 1019 | gb_free 9.6 | wall 72617
KL Stats: Epoch 63 Divergences: Uniform: 6.478491742552586 Unigram: 4.033872127810283
2022-03-16 12:52:17 | INFO | fairseq.trainer | begin training epoch 64
2022-03-16 12:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:54:03 | INFO | train_inner | epoch 064:     39 / 407 loss=5.354, ppl=40.89, wps=20706.7, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=25500, lr=0.00019803, gnorm=0.521, loss_scale=16, train_wall=254, gb_free=9.6, wall=72722
2022-03-16 12:58:38 | INFO | train_inner | epoch 064:    139 / 407 loss=5.336, ppl=40.38, wps=23826.9, ups=0.36, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.521, loss_scale=32, train_wall=253, gb_free=9.6, wall=72997
2022-03-16 13:03:14 | INFO | train_inner | epoch 064:    239 / 407 loss=5.353, ppl=40.86, wps=23710.9, ups=0.36, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.522, loss_scale=32, train_wall=254, gb_free=9.6, wall=73274
2022-03-16 13:03:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:07:55 | INFO | train_inner | epoch 064:    340 / 407 loss=5.359, ppl=41.04, wps=23389.2, ups=0.36, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.521, loss_scale=32, train_wall=257, gb_free=9.6, wall=73554
2022-03-16 13:09:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:10:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:11:36 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 5.64 | ppl 49.86 | wps 36279.3 | wpb 511.9 | bsz 1 | num_updates 25866 | best_loss 5.64
2022-03-16 13:11:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25866 updates
2022-03-16 13:11:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 13:11:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 13:11:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 64 @ 25866 updates, score 5.64) (writing took 1.8324918877333403 seconds)
2022-03-16 13:11:37 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-16 13:11:37 | INFO | train | epoch 064 | loss 5.351 | ppl 40.81 | wps 22859.2 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 25866 | lr 0.000196623 | gnorm 0.522 | loss_scale 32 | train_wall 1030 | gb_free 9.6 | wall 73777
KL Stats: Epoch 64 Divergences: Uniform: 6.490985741591392 Unigram: 4.039803571599296
2022-03-16 13:11:37 | INFO | fairseq.trainer | begin training epoch 65
2022-03-16 13:11:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:13:10 | INFO | train_inner | epoch 065:     34 / 407 loss=5.36, ppl=41.07, wps=20725, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=0.521, loss_scale=32, train_wall=254, gb_free=9.6, wall=73869
2022-03-16 13:16:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:17:45 | INFO | train_inner | epoch 065:    135 / 407 loss=5.336, ppl=40.38, wps=23853.4, ups=0.36, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.525, loss_scale=32, train_wall=252, gb_free=9.6, wall=74144
2022-03-16 13:18:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:22:21 | INFO | train_inner | epoch 065:    236 / 407 loss=5.345, ppl=40.64, wps=23735.5, ups=0.36, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.523, loss_scale=16, train_wall=254, gb_free=9.6, wall=74420
2022-03-16 13:25:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:26:56 | INFO | train_inner | epoch 065:    337 / 407 loss=5.357, ppl=40.99, wps=23837, ups=0.36, wpb=65534.2, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.52, loss_scale=16, train_wall=253, gb_free=9.6, wall=74695
2022-03-16 13:30:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:30:42 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 5.637 | ppl 49.75 | wps 36205.9 | wpb 511.9 | bsz 1 | num_updates 26270 | best_loss 5.637
2022-03-16 13:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 26270 updates
2022-03-16 13:30:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 13:30:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt
2022-03-16 13:30:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_best.pt (epoch 65 @ 26270 updates, score 5.637) (writing took 1.8641238836571574 seconds)
2022-03-16 13:30:44 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-16 13:30:44 | INFO | train | epoch 065 | loss 5.346 | ppl 40.66 | wps 23079.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 26270 | lr 0.000195106 | gnorm 0.522 | loss_scale 16 | train_wall 1017 | gb_free 9.6 | wall 74923
KL Stats: Epoch 65 Divergences: Uniform: 6.502524978655773 Unigram: 4.043234606836797
2022-03-16 13:30:44 | INFO | fairseq.trainer | begin training epoch 66
2022-03-16 13:30:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:32:06 | INFO | train_inner | epoch 066:     30 / 407 loss=5.341, ppl=40.54, wps=21054.3, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=0.524, loss_scale=16, train_wall=249, gb_free=9.6, wall=75006
2022-03-16 13:33:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:36:44 | INFO | train_inner | epoch 066:    131 / 407 loss=5.323, ppl=40.04, wps=23625.3, ups=0.36, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.519, loss_scale=16, train_wall=255, gb_free=9.6, wall=75283
2022-03-16 13:41:20 | INFO | train_inner | epoch 066:    231 / 407 loss=5.343, ppl=40.59, wps=23727.5, ups=0.36, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.519, loss_scale=32, train_wall=254, gb_free=9.6, wall=75559
2022-03-16 13:42:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:45:59 | INFO | train_inner | epoch 066:    332 / 407 loss=5.349, ppl=40.76, wps=23458.1, ups=0.36, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.527, loss_scale=16, train_wall=257, gb_free=9.6, wall=75839
2022-03-16 13:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:50:01 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 5.64 | ppl 49.86 | wps 36400.6 | wpb 511.9 | bsz 1 | num_updates 26675 | best_loss 5.637
2022-03-16 13:50:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 26675 updates
2022-03-16 13:50:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 13:50:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 13:50:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt (epoch 66 @ 26675 updates, score 5.64) (writing took 0.95006512850523 seconds)
2022-03-16 13:50:02 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-16 13:50:02 | INFO | train | epoch 066 | loss 5.341 | ppl 40.52 | wps 22899.4 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 26675 | lr 0.000193619 | gnorm 0.522 | loss_scale 32 | train_wall 1029 | gb_free 9.6 | wall 76082
KL Stats: Epoch 66 Divergences: Uniform: 6.514298232106662 Unigram: 4.048205064996805
2022-03-16 13:50:02 | INFO | fairseq.trainer | begin training epoch 67
2022-03-16 13:50:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:51:10 | INFO | train_inner | epoch 067:     25 / 407 loss=5.346, ppl=40.67, wps=21022.3, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=0.524, loss_scale=32, train_wall=251, gb_free=9.6, wall=76150
2022-03-16 13:55:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:55:44 | INFO | train_inner | epoch 067:    126 / 407 loss=5.312, ppl=39.72, wps=23943, ups=0.37, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.524, loss_scale=32, train_wall=251, gb_free=9.6, wall=76423
2022-03-16 14:00:17 | INFO | train_inner | epoch 067:    226 / 407 loss=5.337, ppl=40.42, wps=23972.3, ups=0.37, wpb=65534.2, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.525, loss_scale=32, train_wall=251, gb_free=9.6, wall=76697
2022-03-16 14:00:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:04:51 | INFO | train_inner | epoch 067:    327 / 407 loss=5.348, ppl=40.74, wps=23901.2, ups=0.36, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.529, loss_scale=16, train_wall=252, gb_free=9.6, wall=76971
2022-03-16 14:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:09:07 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 5.642 | ppl 49.92 | wps 36636.5 | wpb 511.9 | bsz 1 | num_updates 27080 | best_loss 5.637
2022-03-16 14:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 27080 updates
2022-03-16 14:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 14:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 14:09:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt (epoch 67 @ 27080 updates, score 5.642) (writing took 0.8869886649772525 seconds)
2022-03-16 14:09:07 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-16 14:09:07 | INFO | train | epoch 067 | loss 5.336 | ppl 40.39 | wps 23159.4 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 27080 | lr 0.000192166 | gnorm 0.525 | loss_scale 32 | train_wall 1017 | gb_free 9.6 | wall 77227
KL Stats: Epoch 67 Divergences: Uniform: 6.525518720762325 Unigram: 4.052550827427974
2022-03-16 14:09:08 | INFO | fairseq.trainer | begin training epoch 68
2022-03-16 14:09:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:10:02 | INFO | train_inner | epoch 068:     20 / 407 loss=5.348, ppl=40.74, wps=21022.2, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=0.518, loss_scale=32, train_wall=251, gb_free=9.6, wall=77282
2022-03-16 14:13:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:13:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:14:40 | INFO | train_inner | epoch 068:    122 / 407 loss=5.307, ppl=39.58, wps=23604.3, ups=0.36, wpb=65534.2, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.525, loss_scale=16, train_wall=255, gb_free=9.6, wall=77559
2022-03-16 14:19:14 | INFO | train_inner | epoch 068:    222 / 407 loss=5.327, ppl=40.15, wps=23956.7, ups=0.37, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.523, loss_scale=16, train_wall=251, gb_free=9.6, wall=77833
2022-03-16 14:23:45 | INFO | train_inner | epoch 068:    322 / 407 loss=5.343, ppl=40.59, wps=24180, ups=0.37, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.516, loss_scale=32, train_wall=249, gb_free=9.6, wall=78104
2022-03-16 14:25:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:27:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:28:11 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 5.638 | ppl 49.78 | wps 36442.2 | wpb 511.9 | bsz 1 | num_updates 27484 | best_loss 5.637
2022-03-16 14:28:11 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 14:28:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 27484 updates
2022-03-16 14:28:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 14:28:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt
2022-03-16 14:28:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0_0.025_0.975/checkpoint_last.pt (epoch 68 @ 27484 updates, score 5.638) (writing took 0.9199876366183162 seconds)
2022-03-16 14:28:12 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-16 14:28:12 | INFO | train | epoch 068 | loss 5.331 | ppl 40.24 | wps 23115.7 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 27484 | lr 0.000190748 | gnorm 0.522 | loss_scale 32 | train_wall 1017 | gb_free 9.6 | wall 78372
2022-03-16 14:28:12 | INFO | fairseq_cli.train | done training in 78371.4 seconds
