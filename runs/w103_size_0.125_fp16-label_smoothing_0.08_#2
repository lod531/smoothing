Sender: LSF System <lsfadmin@eu-g3-067>
Subject: Job 207345376: <w103_size_0.125_fp16_label_smoothing_0.08_#2> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.08_#2> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 12:41:53 2022
Job was executed on host(s) <eu-g3-067>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Sun Mar  6 12:42:15 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 12:42:15 2022
Terminated at Tue Mar  8 06:20:45 2022
Results reported at Tue Mar  8 06:20:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.08 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   149732.62 sec.
    Max Memory :                                 6792 MB
    Average Memory :                             3748.57 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13208.00 MB
    Max Swap :                                   52 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   149910 sec.
    Turnaround time :                            149932 sec.

The output (if any) follows:

2022-03-06 12:42:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.08, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 12:42:22 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-06 12:42:25 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-06 12:42:25 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 12:42:25 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 12:42:25 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-06 12:42:25 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-06 12:42:25 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 12:42:25 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-06 12:42:28 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 12:42:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:42:28 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-06 12:42:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:42:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 12:42:28 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 12:42:28 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 12:42:28 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 12:42:28 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 12:42:28 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-06 12:42:28 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 12:42:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:42:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 12:42:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 12:42:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 12:42:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 12:43:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 12:48:21 | INFO | train_inner | epoch 001:    105 / 196 loss=16.566, nll_loss=16.409, ppl=86988.1, wps=20767.2, ups=0.32, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.327, loss_scale=4, train_wall=328, gb_free=19.9, wall=353
2022-03-06 12:53:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 12:53:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.445 | nll_loss 13.011 | ppl 8257.22 | wps 39123.9 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-06 12:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-06 12:53:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 12:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 12:53:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.445) (writing took 7.676591293886304 seconds)
2022-03-06 12:53:21 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 12:53:21 | INFO | train | epoch 001 | loss 15.542 | nll_loss 15.297 | ppl 40253.7 | wps 20309.8 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.406 | loss_scale 8 | train_wall 594 | gb_free 19.9 | wall 653
2022-03-06 12:53:21 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 12:53:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:53:49 | INFO | train_inner | epoch 002:      9 / 196 loss=14.331, nll_loss=13.981, ppl=16167.9, wps=19905.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.372, loss_scale=8, train_wall=292, gb_free=19.9, wall=681
2022-03-06 12:59:05 | INFO | train_inner | epoch 002:    109 / 196 loss=12.55, nll_loss=12.022, ppl=4159.33, wps=20769.8, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.891, loss_scale=16, train_wall=293, gb_free=19.9, wall=997
2022-03-06 13:03:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:03:44 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.059 | nll_loss 10.306 | ppl 1265.78 | wps 38863.7 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 11.059
2022-03-06 13:03:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-06 13:03:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:03:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:03:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 2 @ 387 updates, score 11.059) (writing took 7.78951648902148 seconds)
2022-03-06 13:03:52 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 13:03:52 | INFO | train | epoch 002 | loss 12.087 | nll_loss 11.494 | ppl 2884.77 | wps 20314.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.747 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 1284
2022-03-06 13:03:52 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 13:03:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:04:33 | INFO | train_inner | epoch 003:     13 / 196 loss=11.365, nll_loss=10.672, ppl=1631.39, wps=19892.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.526, loss_scale=32, train_wall=292, gb_free=19.9, wall=1326
2022-03-06 13:09:49 | INFO | train_inner | epoch 003:    113 / 196 loss=10.879, nll_loss=10.084, ppl=1085.56, wps=20753.8, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.484, loss_scale=32, train_wall=293, gb_free=19.9, wall=1641
2022-03-06 13:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:14:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:14:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.45 | nll_loss 9.586 | ppl 768.77 | wps 39936.4 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.45
2022-03-06 13:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-06 13:14:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:14:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:14:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 3 @ 582 updates, score 10.45) (writing took 7.7930298410356045 seconds)
2022-03-06 13:14:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 13:14:24 | INFO | train | epoch 003 | loss 10.782 | nll_loss 9.973 | ppl 1005.04 | wps 20201.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.497 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 1916
2022-03-06 13:14:24 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 13:14:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:15:21 | INFO | train_inner | epoch 004:     18 / 196 loss=10.588, nll_loss=9.747, ppl=859.21, wps=19703.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.541, loss_scale=32, train_wall=295, gb_free=19.9, wall=1973
2022-03-06 13:15:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:20:40 | INFO | train_inner | epoch 004:    119 / 196 loss=10.318, nll_loss=9.444, ppl=696.53, wps=20537.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.647, loss_scale=16, train_wall=296, gb_free=19.9, wall=2292
2022-03-06 13:24:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:24:48 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.011 | nll_loss 9.095 | ppl 547.03 | wps 39809.9 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 10.011
2022-03-06 13:24:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-06 13:24:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:24:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 4 @ 777 updates, score 10.011) (writing took 7.818506405688822 seconds)
2022-03-06 13:24:56 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 13:24:56 | INFO | train | epoch 004 | loss 10.256 | nll_loss 9.375 | ppl 663.87 | wps 20200.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.67 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 2548
2022-03-06 13:24:56 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 13:24:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:26:08 | INFO | train_inner | epoch 005:     23 / 196 loss=10.095, nll_loss=9.195, ppl=586.07, wps=19902.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.686, loss_scale=32, train_wall=292, gb_free=19.9, wall=2621
2022-03-06 13:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:31:28 | INFO | train_inner | epoch 005:    124 / 196 loss=9.889, nll_loss=8.963, ppl=499.13, wps=20531.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.776, loss_scale=32, train_wall=296, gb_free=19.9, wall=2940
2022-03-06 13:35:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:35:20 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.648 | nll_loss 8.696 | ppl 414.81 | wps 39455.7 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 9.648
2022-03-06 13:35:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-06 13:35:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:35:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:35:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 5 @ 972 updates, score 9.648) (writing took 7.822329401038587 seconds)
2022-03-06 13:35:28 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 13:35:28 | INFO | train | epoch 005 | loss 9.841 | nll_loss 8.91 | ppl 481.14 | wps 20195.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.766 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 3180
2022-03-06 13:35:28 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 13:35:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:36:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:36:59 | INFO | train_inner | epoch 006:     29 / 196 loss=9.699, nll_loss=8.752, ppl=431.02, wps=19703.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.809, loss_scale=32, train_wall=295, gb_free=19.9, wall=3272
2022-03-06 13:42:15 | INFO | train_inner | epoch 006:    129 / 196 loss=9.522, nll_loss=8.553, ppl=375.7, wps=20739, ups=0.32, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.832, loss_scale=32, train_wall=293, gb_free=19.9, wall=3588
2022-03-06 13:43:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:45:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:45:52 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.348 | nll_loss 8.343 | ppl 324.77 | wps 39436.5 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 9.348
2022-03-06 13:45:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-06 13:45:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:45:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:46:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 6 @ 1166 updates, score 9.348) (writing took 7.795981607399881 seconds)
2022-03-06 13:46:00 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 13:46:00 | INFO | train | epoch 006 | loss 9.497 | nll_loss 8.525 | ppl 368.48 | wps 20087.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.825 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 3812
2022-03-06 13:46:00 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 13:46:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:47:47 | INFO | train_inner | epoch 007:     34 / 196 loss=9.37, nll_loss=8.383, ppl=333.92, wps=19695.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.814, loss_scale=32, train_wall=295, gb_free=19.9, wall=3919
2022-03-06 13:50:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:53:06 | INFO | train_inner | epoch 007:    135 / 196 loss=9.225, nll_loss=8.22, ppl=298.24, wps=20534.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.858, loss_scale=32, train_wall=296, gb_free=19.9, wall=4239
2022-03-06 13:56:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:56:24 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.112 | nll_loss 8.081 | ppl 270.8 | wps 39362.4 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 9.112
2022-03-06 13:56:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-06 13:56:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 13:56:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 7 @ 1361 updates, score 9.112) (writing took 7.776309537701309 seconds)
2022-03-06 13:56:32 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 13:56:32 | INFO | train | epoch 007 | loss 9.21 | nll_loss 8.203 | ppl 294.74 | wps 20195.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.853 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 4444
2022-03-06 13:56:32 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 13:56:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:58:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:58:38 | INFO | train_inner | epoch 008:     40 / 196 loss=9.088, nll_loss=8.068, ppl=268.33, wps=19698.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.849, loss_scale=32, train_wall=295, gb_free=19.9, wall=4570
2022-03-06 14:03:54 | INFO | train_inner | epoch 008:    140 / 196 loss=8.958, nll_loss=7.921, ppl=242.44, wps=20730.7, ups=0.32, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.86, loss_scale=32, train_wall=293, gb_free=19.9, wall=4887
2022-03-06 14:05:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:06:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.891 | nll_loss 7.832 | ppl 227.87 | wps 39207.7 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 8.891
2022-03-06 14:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-06 14:06:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:07:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:07:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 8 @ 1555 updates, score 8.891) (writing took 7.848570695146918 seconds)
2022-03-06 14:07:04 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 14:07:04 | INFO | train | epoch 008 | loss 8.951 | nll_loss 7.913 | ppl 241.05 | wps 20084.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 0.854 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 5076
2022-03-06 14:07:04 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 14:07:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:09:26 | INFO | train_inner | epoch 009:     45 / 196 loss=8.834, nll_loss=7.782, ppl=220.16, wps=19687.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.879, loss_scale=32, train_wall=295, gb_free=19.9, wall=5219
2022-03-06 14:12:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:14:46 | INFO | train_inner | epoch 009:    146 / 196 loss=8.711, nll_loss=7.644, ppl=200.06, wps=20528.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.854, loss_scale=32, train_wall=296, gb_free=19.9, wall=5538
2022-03-06 14:15:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 14:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:17:28 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.699 | nll_loss 7.619 | ppl 196.52 | wps 39562.1 | wpb 510.9 | bsz 1 | num_updates 1749 | best_loss 8.699
2022-03-06 14:17:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1749 updates
2022-03-06 14:17:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:17:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:17:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 9 @ 1749 updates, score 8.699) (writing took 7.857458207756281 seconds)
2022-03-06 14:17:36 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 14:17:36 | INFO | train | epoch 009 | loss 8.709 | nll_loss 7.642 | ppl 199.75 | wps 20074.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1749 | lr 0.000218681 | gnorm 0.873 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 5709
2022-03-06 14:17:36 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 14:17:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:20:18 | INFO | train_inner | epoch 010:     51 / 196 loss=8.591, nll_loss=7.51, ppl=182.24, wps=19683.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.866, loss_scale=16, train_wall=295, gb_free=19.9, wall=5870
2022-03-06 14:25:34 | INFO | train_inner | epoch 010:    151 / 196 loss=8.481, nll_loss=7.386, ppl=167.23, wps=20735.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.858, loss_scale=32, train_wall=293, gb_free=19.9, wall=6186
2022-03-06 14:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:28:03 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.536 | nll_loss 7.43 | ppl 172.47 | wps 37869.9 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 8.536
2022-03-06 14:28:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-06 14:28:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:28:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:28:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 10 @ 1945 updates, score 8.536) (writing took 7.179525821469724 seconds)
2022-03-06 14:28:10 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 14:28:10 | INFO | train | epoch 010 | loss 8.483 | nll_loss 7.388 | ppl 167.52 | wps 20232.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.845 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 6343
2022-03-06 14:28:10 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 14:28:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:30:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:31:11 | INFO | train_inner | epoch 011:     56 / 196 loss=8.365, nll_loss=7.256, ppl=152.85, wps=19407.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.843, loss_scale=32, train_wall=299, gb_free=19.9, wall=6523
2022-03-06 14:36:33 | INFO | train_inner | epoch 011:    156 / 196 loss=8.269, nll_loss=7.148, ppl=141.8, wps=20329, ups=0.31, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.833, loss_scale=32, train_wall=298, gb_free=19.9, wall=6845
2022-03-06 14:37:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:38:47 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.406 | nll_loss 7.275 | ppl 154.83 | wps 37680.2 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 8.406
2022-03-06 14:38:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2139 updates
2022-03-06 14:38:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:38:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:38:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 11 @ 2139 updates, score 8.406) (writing took 7.009725139476359 seconds)
2022-03-06 14:38:54 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 14:38:54 | INFO | train | epoch 011 | loss 8.271 | nll_loss 7.15 | ppl 142.05 | wps 19729.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2139 | lr 0.000267422 | gnorm 0.837 | loss_scale 32 | train_wall 583 | gb_free 19.9 | wall 6986
2022-03-06 14:38:54 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 14:38:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:42:11 | INFO | train_inner | epoch 012:     61 / 196 loss=8.151, nll_loss=7.016, ppl=129.43, wps=19346.4, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.824, loss_scale=32, train_wall=300, gb_free=19.9, wall=7183
2022-03-06 14:44:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:47:36 | INFO | train_inner | epoch 012:    162 / 196 loss=8.068, nll_loss=6.922, ppl=121.24, wps=20126.3, ups=0.31, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.809, loss_scale=32, train_wall=301, gb_free=19.9, wall=7509
2022-03-06 14:49:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:49:31 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.293 | nll_loss 7.164 | ppl 143.42 | wps 37684.5 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 8.293
2022-03-06 14:49:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-06 14:49:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:49:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 14:49:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 12 @ 2334 updates, score 8.293) (writing took 7.110642337240279 seconds)
2022-03-06 14:49:38 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 14:49:38 | INFO | train | epoch 012 | loss 8.076 | nll_loss 6.931 | ppl 122.05 | wps 19806.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.817 | loss_scale 32 | train_wall 584 | gb_free 19.9 | wall 7630
2022-03-06 14:49:38 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 14:49:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:52:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:53:14 | INFO | train_inner | epoch 013:     67 / 196 loss=7.956, nll_loss=6.797, ppl=111.19, wps=19334.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.812, loss_scale=32, train_wall=300, gb_free=19.9, wall=7847
2022-03-06 14:58:37 | INFO | train_inner | epoch 013:    167 / 196 loss=7.893, nll_loss=6.725, ppl=105.78, wps=20321.6, ups=0.31, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.806, loss_scale=32, train_wall=298, gb_free=19.9, wall=8169
2022-03-06 14:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:00:15 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.19 | nll_loss 7.038 | ppl 131.44 | wps 37565 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 8.19
2022-03-06 15:00:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2528 updates
2022-03-06 15:00:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:00:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 13 @ 2528 updates, score 8.19) (writing took 7.088267443701625 seconds)
2022-03-06 15:00:22 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 15:00:22 | INFO | train | epoch 013 | loss 7.896 | nll_loss 6.728 | ppl 106.04 | wps 19706.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2528 | lr 0.000316037 | gnorm 0.804 | loss_scale 32 | train_wall 584 | gb_free 19.9 | wall 8275
2022-03-06 15:00:23 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 15:00:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:04:15 | INFO | train_inner | epoch 014:     72 / 196 loss=7.776, nll_loss=6.593, ppl=96.55, wps=19343.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.787, loss_scale=32, train_wall=300, gb_free=19.9, wall=8507
2022-03-06 15:06:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:09:40 | INFO | train_inner | epoch 014:    173 / 196 loss=7.724, nll_loss=6.534, ppl=92.69, wps=20143.1, ups=0.31, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.783, loss_scale=32, train_wall=301, gb_free=19.9, wall=8832
2022-03-06 15:10:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:10:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:10:59 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.116 | nll_loss 6.937 | ppl 122.57 | wps 37897.6 | wpb 510.9 | bsz 1 | num_updates 2722 | best_loss 8.116
2022-03-06 15:10:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2722 updates
2022-03-06 15:10:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:11:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 14 @ 2722 updates, score 8.116) (writing took 7.072689249180257 seconds)
2022-03-06 15:11:06 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 15:11:06 | INFO | train | epoch 014 | loss 7.732 | nll_loss 6.543 | ppl 93.27 | wps 19722.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2722 | lr 0.000340282 | gnorm 0.78 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 8918
2022-03-06 15:11:06 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 15:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:15:18 | INFO | train_inner | epoch 015:     78 / 196 loss=7.618, nll_loss=6.415, ppl=85.35, wps=19355, ups=0.3, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.786, loss_scale=16, train_wall=300, gb_free=19.9, wall=9170
2022-03-06 15:20:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:20:43 | INFO | train_inner | epoch 015:    179 / 196 loss=7.576, nll_loss=6.367, ppl=82.56, wps=20145.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.782, loss_scale=16, train_wall=301, gb_free=19.9, wall=9495
2022-03-06 15:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:21:43 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.048 | nll_loss 6.857 | ppl 115.9 | wps 37726.2 | wpb 510.9 | bsz 1 | num_updates 2917 | best_loss 8.048
2022-03-06 15:21:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2917 updates
2022-03-06 15:21:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:21:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 15 @ 2917 updates, score 8.048) (writing took 7.109848410822451 seconds)
2022-03-06 15:21:50 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 15:21:50 | INFO | train | epoch 015 | loss 7.58 | nll_loss 6.372 | ppl 82.82 | wps 19820.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2917 | lr 0.000364652 | gnorm 0.786 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 9562
2022-03-06 15:21:50 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 15:21:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:26:18 | INFO | train_inner | epoch 016:     83 / 196 loss=7.452, nll_loss=6.228, ppl=74.98, wps=19518.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.762, loss_scale=16, train_wall=297, gb_free=19.9, wall=9830
2022-03-06 15:31:41 | INFO | train_inner | epoch 016:    183 / 196 loss=7.443, nll_loss=6.217, ppl=74.41, wps=20310.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.758, loss_scale=32, train_wall=298, gb_free=19.9, wall=10153
2022-03-06 15:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:32:28 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.013 | nll_loss 6.815 | ppl 112.61 | wps 37228.2 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 8.013
2022-03-06 15:32:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3113 updates
2022-03-06 15:32:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:32:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:32:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 16 @ 3113 updates, score 8.013) (writing took 7.107459373772144 seconds)
2022-03-06 15:32:35 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 15:32:35 | INFO | train | epoch 016 | loss 7.435 | nll_loss 6.209 | ppl 73.96 | wps 19898.3 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 3113 | lr 0.000389147 | gnorm 0.753 | loss_scale 32 | train_wall 584 | gb_free 19.9 | wall 10207
2022-03-06 15:32:35 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 15:32:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:34:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:37:19 | INFO | train_inner | epoch 017:     88 / 196 loss=7.31, nll_loss=6.069, ppl=67.12, wps=19329.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.769, loss_scale=16, train_wall=300, gb_free=19.9, wall=10491
2022-03-06 15:41:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:42:44 | INFO | train_inner | epoch 017:    189 / 196 loss=7.312, nll_loss=6.069, ppl=67.13, wps=20133.9, ups=0.31, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.765, loss_scale=16, train_wall=301, gb_free=19.9, wall=10817
2022-03-06 15:43:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:43:12 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.989 | nll_loss 6.789 | ppl 110.55 | wps 37475.3 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 7.989
2022-03-06 15:43:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3307 updates
2022-03-06 15:43:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:43:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:43:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 17 @ 3307 updates, score 7.989) (writing took 7.089895058423281 seconds)
2022-03-06 15:43:19 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 15:43:19 | INFO | train | epoch 017 | loss 7.304 | nll_loss 6.061 | ppl 66.79 | wps 19706 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 3307 | lr 0.000413392 | gnorm 0.773 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 10851
2022-03-06 15:43:19 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 15:43:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:48:19 | INFO | train_inner | epoch 018:     93 / 196 loss=7.175, nll_loss=5.917, ppl=60.41, wps=19534.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.757, loss_scale=16, train_wall=297, gb_free=19.9, wall=11151
2022-03-06 15:52:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:53:44 | INFO | train_inner | epoch 018:    194 / 196 loss=7.196, nll_loss=5.938, ppl=61.31, wps=20135.1, ups=0.31, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.751, loss_scale=16, train_wall=301, gb_free=19.9, wall=11477
2022-03-06 15:53:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:53:56 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.984 | nll_loss 6.796 | ppl 111.13 | wps 37649.6 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 7.984
2022-03-06 15:53:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3502 updates
2022-03-06 15:53:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:53:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 15:54:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 18 @ 3502 updates, score 7.984) (writing took 7.473009434528649 seconds)
2022-03-06 15:54:03 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 15:54:03 | INFO | train | epoch 018 | loss 7.18 | nll_loss 5.921 | ppl 60.59 | wps 19807.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3502 | lr 0.000437762 | gnorm 0.755 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 11496
2022-03-06 15:54:03 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 15:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:59:20 | INFO | train_inner | epoch 019:     98 / 196 loss=7.044, nll_loss=5.769, ppl=54.54, wps=19495.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.749, loss_scale=32, train_wall=297, gb_free=19.9, wall=11812
2022-03-06 16:00:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:04:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:04:41 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.96 | nll_loss 6.769 | ppl 109.04 | wps 37586.5 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 7.96
2022-03-06 16:04:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3697 updates
2022-03-06 16:04:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 16:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt
2022-03-06 16:04:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_best.pt (epoch 19 @ 3697 updates, score 7.96) (writing took 7.150380262173712 seconds)
2022-03-06 16:04:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 16:04:48 | INFO | train | epoch 019 | loss 7.062 | nll_loss 5.788 | ppl 55.24 | wps 19795 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3697 | lr 0.000462133 | gnorm 0.744 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 12140
2022-03-06 16:04:48 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 16:04:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:04:58 | INFO | train_inner | epoch 020:      3 / 196 loss=7.075, nll_loss=5.802, ppl=55.8, wps=19324.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.736, loss_scale=16, train_wall=301, gb_free=19.9, wall=12150
2022-03-06 16:07:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:10:24 | INFO | train_inner | epoch 020:    104 / 196 loss=6.925, nll_loss=5.634, ppl=49.67, wps=20130.5, ups=0.31, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.774, loss_scale=16, train_wall=301, gb_free=19.9, wall=12476
2022-03-06 16:15:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:15:25 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.001 | nll_loss 6.81 | ppl 112.22 | wps 38225.2 | wpb 510.9 | bsz 1 | num_updates 3892 | best_loss 7.96
2022-03-06 16:15:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3892 updates
2022-03-06 16:15:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:15:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 20 @ 3892 updates, score 8.001) (writing took 3.456943499855697 seconds)
2022-03-06 16:15:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 16:15:29 | INFO | train | epoch 020 | loss 6.95 | nll_loss 5.662 | ppl 50.64 | wps 19930.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3892 | lr 0.000486503 | gnorm 0.754 | loss_scale 32 | train_wall 583 | gb_free 19.9 | wall 12781
2022-03-06 16:15:29 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 16:15:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:15:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:15:58 | INFO | train_inner | epoch 021:      9 / 196 loss=6.965, nll_loss=5.678, ppl=51.2, wps=19566.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.737, loss_scale=16, train_wall=300, gb_free=19.9, wall=12810
2022-03-06 16:21:20 | INFO | train_inner | epoch 021:    109 / 196 loss=6.815, nll_loss=5.51, ppl=45.56, wps=20328.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.747, loss_scale=16, train_wall=298, gb_free=19.9, wall=13132
2022-03-06 16:24:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:26:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.984 | nll_loss 6.776 | ppl 109.6 | wps 38013.5 | wpb 510.9 | bsz 1 | num_updates 4086 | best_loss 7.96
2022-03-06 16:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4086 updates
2022-03-06 16:26:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 21 @ 4086 updates, score 7.984) (writing took 3.4780645268037915 seconds)
2022-03-06 16:26:09 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 16:26:09 | INFO | train | epoch 021 | loss 6.842 | nll_loss 5.54 | ppl 46.52 | wps 19833.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 4086 | lr 0.00049471 | gnorm 0.75 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 13421
2022-03-06 16:26:09 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 16:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:26:54 | INFO | train_inner | epoch 022:     14 / 196 loss=6.847, nll_loss=5.544, ppl=46.67, wps=19573.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.749, loss_scale=16, train_wall=300, gb_free=19.9, wall=13466
2022-03-06 16:32:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:32:19 | INFO | train_inner | epoch 022:    115 / 196 loss=6.713, nll_loss=5.394, ppl=42.05, wps=20135.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.713, loss_scale=16, train_wall=301, gb_free=19.9, wall=13792
2022-03-06 16:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:36:46 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.99 | nll_loss 6.764 | ppl 108.7 | wps 37583 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 7.96
2022-03-06 16:36:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-06 16:36:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:36:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:36:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 22 @ 4281 updates, score 7.99) (writing took 3.4143454087898135 seconds)
2022-03-06 16:36:49 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 16:36:49 | INFO | train | epoch 022 | loss 6.73 | nll_loss 5.413 | ppl 42.61 | wps 19931 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.722 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 14061
2022-03-06 16:36:49 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 16:36:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:37:50 | INFO | train_inner | epoch 023:     19 / 196 loss=6.728, nll_loss=5.41, ppl=42.53, wps=19745.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.725, loss_scale=16, train_wall=297, gb_free=19.9, wall=14123
2022-03-06 16:39:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:43:16 | INFO | train_inner | epoch 023:    120 / 196 loss=6.602, nll_loss=5.269, ppl=38.55, wps=20132.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.706, loss_scale=16, train_wall=301, gb_free=19.9, wall=14448
2022-03-06 16:47:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:47:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:47:26 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.994 | nll_loss 6.771 | ppl 109.21 | wps 37595.1 | wpb 510.9 | bsz 1 | num_updates 4475 | best_loss 7.96
2022-03-06 16:47:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4475 updates
2022-03-06 16:47:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:47:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 23 @ 4475 updates, score 7.994) (writing took 3.4254971873015165 seconds)
2022-03-06 16:47:29 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 16:47:29 | INFO | train | epoch 023 | loss 6.62 | nll_loss 5.289 | ppl 39.09 | wps 19833.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 4475 | lr 0.000472719 | gnorm 0.709 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 14701
2022-03-06 16:47:29 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 16:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:48:50 | INFO | train_inner | epoch 024:     25 / 196 loss=6.602, nll_loss=5.268, ppl=38.53, wps=19577.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.704, loss_scale=16, train_wall=300, gb_free=19.9, wall=14782
2022-03-06 16:54:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:54:15 | INFO | train_inner | epoch 024:    126 / 196 loss=6.507, nll_loss=5.162, ppl=35.8, wps=20133.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.709, loss_scale=16, train_wall=301, gb_free=19.9, wall=15108
2022-03-06 16:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:58:06 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.008 | nll_loss 6.774 | ppl 109.47 | wps 37786.3 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 7.96
2022-03-06 16:58:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-06 16:58:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 16:58:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 24 @ 4670 updates, score 8.008) (writing took 3.4426291240379214 seconds)
2022-03-06 16:58:10 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 16:58:10 | INFO | train | epoch 024 | loss 6.521 | nll_loss 5.177 | ppl 36.18 | wps 19927.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.698 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 15342
2022-03-06 16:58:10 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 16:58:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:59:47 | INFO | train_inner | epoch 025:     30 / 196 loss=6.508, nll_loss=5.162, ppl=35.8, wps=19730.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.685, loss_scale=16, train_wall=297, gb_free=19.9, wall=15439
2022-03-06 16:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:05:12 | INFO | train_inner | epoch 025:    131 / 196 loss=6.411, nll_loss=5.053, ppl=33.2, wps=20120.4, ups=0.31, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.698, loss_scale=8, train_wall=301, gb_free=19.9, wall=15765
2022-03-06 17:07:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:08:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:08:47 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.045 | nll_loss 6.822 | ppl 113.18 | wps 37450.4 | wpb 510.9 | bsz 1 | num_updates 4864 | best_loss 7.96
2022-03-06 17:08:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4864 updates
2022-03-06 17:08:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:08:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:08:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 25 @ 4864 updates, score 8.045) (writing took 3.4063861472532153 seconds)
2022-03-06 17:08:50 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:08:50 | INFO | train | epoch 025 | loss 6.426 | nll_loss 5.07 | ppl 33.58 | wps 19814.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.702 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 15983
2022-03-06 17:08:50 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:08:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:10:47 | INFO | train_inner | epoch 026:     36 / 196 loss=6.405, nll_loss=5.046, ppl=33.03, wps=19556.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.712, loss_scale=8, train_wall=300, gb_free=19.9, wall=16099
2022-03-06 17:16:09 | INFO | train_inner | epoch 026:    136 / 196 loss=6.331, nll_loss=4.962, ppl=31.16, wps=20330.6, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.689, loss_scale=16, train_wall=298, gb_free=19.9, wall=16421
2022-03-06 17:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:19:27 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.113 | nll_loss 6.876 | ppl 117.43 | wps 37653 | wpb 510.9 | bsz 1 | num_updates 5059 | best_loss 7.96
2022-03-06 17:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5059 updates
2022-03-06 17:19:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:19:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:19:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 26 @ 5059 updates, score 8.113) (writing took 3.466143660247326 seconds)
2022-03-06 17:19:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:19:31 | INFO | train | epoch 026 | loss 6.336 | nll_loss 4.967 | ppl 31.28 | wps 19928.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5059 | lr 0.000444598 | gnorm 0.687 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 16623
2022-03-06 17:19:31 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:19:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:21:43 | INFO | train_inner | epoch 027:     41 / 196 loss=6.299, nll_loss=4.926, ppl=30.4, wps=19557.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.684, loss_scale=8, train_wall=300, gb_free=19.9, wall=16755
2022-03-06 17:27:05 | INFO | train_inner | epoch 027:    141 / 196 loss=6.251, nll_loss=4.871, ppl=29.27, wps=20331.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.698, loss_scale=16, train_wall=298, gb_free=19.9, wall=17078
2022-03-06 17:28:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:30:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:30:08 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.149 | nll_loss 6.941 | ppl 122.83 | wps 37753.6 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 7.96
2022-03-06 17:30:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5254 updates
2022-03-06 17:30:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:30:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:30:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 27 @ 5254 updates, score 8.149) (writing took 3.406704126857221 seconds)
2022-03-06 17:30:11 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:30:11 | INFO | train | epoch 027 | loss 6.252 | nll_loss 4.872 | ppl 29.28 | wps 19928.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5254 | lr 0.00043627 | gnorm 0.693 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 17263
2022-03-06 17:30:11 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:30:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:32:40 | INFO | train_inner | epoch 028:     46 / 196 loss=6.214, nll_loss=4.83, ppl=28.44, wps=19553.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.707, loss_scale=8, train_wall=300, gb_free=19.9, wall=17412
2022-03-06 17:37:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:38:05 | INFO | train_inner | epoch 028:    147 / 196 loss=6.167, nll_loss=4.776, ppl=27.39, wps=20123.2, ups=0.31, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.694, loss_scale=8, train_wall=301, gb_free=19.9, wall=17738
2022-03-06 17:40:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:40:48 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.163 | nll_loss 6.936 | ppl 122.47 | wps 37654.1 | wpb 510.9 | bsz 1 | num_updates 5449 | best_loss 7.96
2022-03-06 17:40:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5449 updates
2022-03-06 17:40:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:40:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 28 @ 5449 updates, score 8.163) (writing took 3.385281807743013 seconds)
2022-03-06 17:40:52 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:40:52 | INFO | train | epoch 028 | loss 6.171 | nll_loss 4.78 | ppl 27.48 | wps 19920.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5449 | lr 0.000428392 | gnorm 0.703 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 17904
2022-03-06 17:40:52 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:43:36 | INFO | train_inner | epoch 029:     51 / 196 loss=6.135, nll_loss=4.739, ppl=26.7, wps=19744.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.698, loss_scale=8, train_wall=297, gb_free=19.9, wall=18069
2022-03-06 17:47:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:49:02 | INFO | train_inner | epoch 029:    152 / 196 loss=6.098, nll_loss=4.696, ppl=25.93, wps=20134.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.702, loss_scale=8, train_wall=301, gb_free=19.9, wall=18394
2022-03-06 17:51:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:51:29 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.22 | nll_loss 6.974 | ppl 125.72 | wps 37363.6 | wpb 510.9 | bsz 1 | num_updates 5644 | best_loss 7.96
2022-03-06 17:51:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5644 updates
2022-03-06 17:51:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 17:51:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 29 @ 5644 updates, score 8.22) (writing took 3.355113717727363 seconds)
2022-03-06 17:51:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:51:32 | INFO | train | epoch 029 | loss 6.096 | nll_loss 4.695 | ppl 25.9 | wps 19933.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5644 | lr 0.000420927 | gnorm 0.704 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 18544
2022-03-06 17:51:32 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:51:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:54:33 | INFO | train_inner | epoch 030:     56 / 196 loss=6.052, nll_loss=4.645, ppl=25.02, wps=19759.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.714, loss_scale=8, train_wall=297, gb_free=19.9, wall=18725
2022-03-06 17:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:59:58 | INFO | train_inner | epoch 030:    157 / 196 loss=6.03, nll_loss=4.619, ppl=24.58, wps=20126.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.699, loss_scale=8, train_wall=301, gb_free=19.9, wall=19051
2022-03-06 18:02:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:02:09 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.281 | nll_loss 7.063 | ppl 133.76 | wps 37935.9 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 7.96
2022-03-06 18:02:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-06 18:02:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:02:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 30 @ 5839 updates, score 8.281) (writing took 3.388833054341376 seconds)
2022-03-06 18:02:12 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 18:02:12 | INFO | train | epoch 030 | loss 6.023 | nll_loss 4.612 | ppl 24.45 | wps 19931.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.705 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 19185
2022-03-06 18:02:12 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 18:02:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:05:29 | INFO | train_inner | epoch 031:     61 / 196 loss=5.965, nll_loss=4.546, ppl=23.36, wps=19770, ups=0.3, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.707, loss_scale=8, train_wall=297, gb_free=19.9, wall=19381
2022-03-06 18:10:51 | INFO | train_inner | epoch 031:    161 / 196 loss=5.973, nll_loss=4.553, ppl=23.48, wps=20333.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.726, loss_scale=16, train_wall=298, gb_free=19.9, wall=19704
2022-03-06 18:12:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:12:49 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.301 | nll_loss 7.069 | ppl 134.29 | wps 37556.8 | wpb 510.9 | bsz 1 | num_updates 6035 | best_loss 7.96
2022-03-06 18:12:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6035 updates
2022-03-06 18:12:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:12:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:12:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 31 @ 6035 updates, score 8.301) (writing took 3.4283163426443934 seconds)
2022-03-06 18:12:52 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 18:12:52 | INFO | train | epoch 031 | loss 5.954 | nll_loss 4.533 | ppl 23.16 | wps 20042.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6035 | lr 0.000407063 | gnorm 0.715 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 19825
2022-03-06 18:12:53 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 18:12:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:13:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:15:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:16:28 | INFO | train_inner | epoch 032:     67 / 196 loss=5.892, nll_loss=4.463, ppl=22.05, wps=19388.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.694, loss_scale=8, train_wall=303, gb_free=19.9, wall=20041
2022-03-06 18:21:51 | INFO | train_inner | epoch 032:    167 / 196 loss=5.909, nll_loss=4.48, ppl=22.32, wps=20335.4, ups=0.31, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.715, loss_scale=8, train_wall=298, gb_free=19.9, wall=20363
2022-03-06 18:23:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:23:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:23:29 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.381 | nll_loss 7.166 | ppl 143.62 | wps 37880.9 | wpb 510.9 | bsz 1 | num_updates 6228 | best_loss 7.96
2022-03-06 18:23:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6228 updates
2022-03-06 18:23:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:23:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:23:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 32 @ 6228 updates, score 8.381) (writing took 3.3981970753520727 seconds)
2022-03-06 18:23:32 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 18:23:32 | INFO | train | epoch 032 | loss 5.887 | nll_loss 4.456 | ppl 21.95 | wps 19737.6 | ups 0.3 | wpb 65446.6 | bsz 127.8 | num_updates 6228 | lr 0.000400706 | gnorm 0.706 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 20465
2022-03-06 18:23:32 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 18:23:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:27:25 | INFO | train_inner | epoch 033:     72 / 196 loss=5.823, nll_loss=4.385, ppl=20.89, wps=19569.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.716, loss_scale=8, train_wall=300, gb_free=19.9, wall=20697
2022-03-06 18:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:32:48 | INFO | train_inner | epoch 033:    173 / 196 loss=5.849, nll_loss=4.412, ppl=21.3, wps=20270.6, ups=0.31, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.728, loss_scale=8, train_wall=299, gb_free=19.9, wall=21020
2022-03-06 18:34:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:34:06 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.416 | nll_loss 7.186 | ppl 145.58 | wps 38809.4 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 7.96
2022-03-06 18:34:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6423 updates
2022-03-06 18:34:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:34:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:34:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 33 @ 6423 updates, score 8.416) (writing took 3.335333318449557 seconds)
2022-03-06 18:34:10 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 18:34:10 | INFO | train | epoch 033 | loss 5.826 | nll_loss 4.387 | ppl 20.92 | wps 20029.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6423 | lr 0.000394576 | gnorm 0.724 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 21102
2022-03-06 18:34:10 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 18:34:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:38:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:38:19 | INFO | train_inner | epoch 034:     78 / 196 loss=5.753, nll_loss=4.305, ppl=19.77, wps=19775.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.737, loss_scale=8, train_wall=298, gb_free=19.9, wall=21351
2022-03-06 18:43:38 | INFO | train_inner | epoch 034:    178 / 196 loss=5.792, nll_loss=4.348, ppl=20.36, wps=20543.7, ups=0.31, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.716, loss_scale=8, train_wall=296, gb_free=19.9, wall=21670
2022-03-06 18:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:44:40 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.44 | nll_loss 7.212 | ppl 148.25 | wps 38722.3 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 7.96
2022-03-06 18:44:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-06 18:44:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 34 @ 6618 updates, score 8.44) (writing took 3.3637148030102253 seconds)
2022-03-06 18:44:43 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 18:44:43 | INFO | train | epoch 034 | loss 5.766 | nll_loss 4.319 | ppl 19.95 | wps 20140.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.731 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 21735
2022-03-06 18:44:43 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 18:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:48:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:49:08 | INFO | train_inner | epoch 035:     83 / 196 loss=5.69, nll_loss=4.232, ppl=18.79, wps=19758.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.733, loss_scale=8, train_wall=298, gb_free=19.9, wall=22001
2022-03-06 18:54:27 | INFO | train_inner | epoch 035:    183 / 196 loss=5.747, nll_loss=4.296, ppl=19.64, wps=20540.3, ups=0.31, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.739, loss_scale=8, train_wall=296, gb_free=19.9, wall=22320
2022-03-06 18:55:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:55:14 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.519 | nll_loss 7.304 | ppl 158.04 | wps 38595.4 | wpb 510.9 | bsz 1 | num_updates 6813 | best_loss 7.96
2022-03-06 18:55:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6813 updates
2022-03-06 18:55:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:55:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 18:55:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 35 @ 6813 updates, score 8.519) (writing took 3.416274373419583 seconds)
2022-03-06 18:55:17 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 18:55:17 | INFO | train | epoch 035 | loss 5.71 | nll_loss 4.255 | ppl 19.09 | wps 20130.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6813 | lr 0.000383116 | gnorm 0.737 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 22369
2022-03-06 18:55:17 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 18:55:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:56:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:59:58 | INFO | train_inner | epoch 036:     88 / 196 loss=5.626, nll_loss=4.159, ppl=17.87, wps=19765.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.738, loss_scale=8, train_wall=298, gb_free=19.9, wall=22650
2022-03-06 19:03:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:05:20 | INFO | train_inner | epoch 036:    189 / 196 loss=5.695, nll_loss=4.236, ppl=18.84, wps=20334.8, ups=0.31, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.737, loss_scale=8, train_wall=299, gb_free=19.9, wall=22973
2022-03-06 19:05:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:05:48 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.556 | nll_loss 7.333 | ppl 161.26 | wps 38806.7 | wpb 510.9 | bsz 1 | num_updates 7007 | best_loss 7.96
2022-03-06 19:05:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7007 updates
2022-03-06 19:05:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 36 @ 7007 updates, score 8.556) (writing took 3.374802304431796 seconds)
2022-03-06 19:05:51 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 19:05:51 | INFO | train | epoch 036 | loss 5.654 | nll_loss 4.19 | ppl 18.25 | wps 20032.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7007 | lr 0.000377776 | gnorm 0.736 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 23003
2022-03-06 19:05:51 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 19:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:10:48 | INFO | train_inner | epoch 037:     93 / 196 loss=5.57, nll_loss=4.095, ppl=17.09, wps=19955.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.762, loss_scale=16, train_wall=295, gb_free=19.9, wall=23300
2022-03-06 19:12:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:16:10 | INFO | train_inner | epoch 037:    194 / 196 loss=5.643, nll_loss=4.177, ppl=18.08, wps=20340.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.747, loss_scale=8, train_wall=299, gb_free=19.9, wall=23622
2022-03-06 19:16:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:16:22 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.598 | nll_loss 7.394 | ppl 168.18 | wps 38013.5 | wpb 510.9 | bsz 1 | num_updates 7202 | best_loss 7.96
2022-03-06 19:16:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7202 updates
2022-03-06 19:16:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:16:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:16:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 37 @ 7202 updates, score 8.598) (writing took 3.355083060450852 seconds)
2022-03-06 19:16:25 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 19:16:25 | INFO | train | epoch 037 | loss 5.603 | nll_loss 4.132 | ppl 17.54 | wps 20133.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7202 | lr 0.000372626 | gnorm 0.756 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 23637
2022-03-06 19:16:25 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 19:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:20:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:21:41 | INFO | train_inner | epoch 038:     99 / 196 loss=5.511, nll_loss=4.028, ppl=16.31, wps=19752.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.75, loss_scale=8, train_wall=298, gb_free=19.9, wall=23953
2022-03-06 19:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:26:56 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.65 | nll_loss 7.45 | ppl 174.91 | wps 38735.7 | wpb 510.9 | bsz 1 | num_updates 7397 | best_loss 7.96
2022-03-06 19:26:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7397 updates
2022-03-06 19:26:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:26:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:26:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 38 @ 7397 updates, score 8.65) (writing took 3.409881180152297 seconds)
2022-03-06 19:26:59 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 19:26:59 | INFO | train | epoch 038 | loss 5.552 | nll_loss 4.074 | ppl 16.84 | wps 20128.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7397 | lr 0.000367682 | gnorm 0.749 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 24271
2022-03-06 19:26:59 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 19:26:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:27:09 | INFO | train_inner | epoch 039:      3 / 196 loss=5.592, nll_loss=4.118, ppl=17.37, wps=19956.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.752, loss_scale=8, train_wall=295, gb_free=19.9, wall=24281
2022-03-06 19:29:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:32:31 | INFO | train_inner | epoch 039:    104 / 196 loss=5.457, nll_loss=3.966, ppl=15.63, wps=20351.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.751, loss_scale=8, train_wall=298, gb_free=19.9, wall=24603
2022-03-06 19:36:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:37:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:37:30 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.732 | nll_loss 7.527 | ppl 184.5 | wps 37393 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 7.96
2022-03-06 19:37:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-06 19:37:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:37:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:37:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 39 @ 7591 updates, score 8.732) (writing took 3.4124104995280504 seconds)
2022-03-06 19:37:34 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 19:37:34 | INFO | train | epoch 039 | loss 5.504 | nll_loss 4.019 | ppl 16.21 | wps 20006.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.763 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 24906
2022-03-06 19:37:34 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 19:37:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:38:03 | INFO | train_inner | epoch 040:      9 / 196 loss=5.54, nll_loss=4.059, ppl=16.66, wps=19683.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.772, loss_scale=8, train_wall=299, gb_free=19.9, wall=24935
2022-03-06 19:43:25 | INFO | train_inner | epoch 040:    109 / 196 loss=5.426, nll_loss=3.93, ppl=15.24, wps=20312.5, ups=0.31, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.754, loss_scale=8, train_wall=298, gb_free=19.9, wall=25258
2022-03-06 19:44:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:48:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:48:11 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.768 | nll_loss 7.554 | ppl 187.98 | wps 37685.7 | wpb 510.9 | bsz 1 | num_updates 7786 | best_loss 7.96
2022-03-06 19:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7786 updates
2022-03-06 19:48:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:48:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 40 @ 7786 updates, score 8.768) (writing took 3.3711760230362415 seconds)
2022-03-06 19:48:14 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 19:48:14 | INFO | train | epoch 040 | loss 5.459 | nll_loss 3.967 | ppl 15.64 | wps 19922.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7786 | lr 0.000358379 | gnorm 0.762 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 25546
2022-03-06 19:48:14 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 19:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:48:59 | INFO | train_inner | epoch 041:     14 / 196 loss=5.486, nll_loss=3.997, ppl=15.97, wps=19563.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.772, loss_scale=8, train_wall=300, gb_free=19.9, wall=25592
2022-03-06 19:54:22 | INFO | train_inner | epoch 041:    114 / 196 loss=5.381, nll_loss=3.879, ppl=14.71, wps=20313.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.765, loss_scale=16, train_wall=298, gb_free=19.9, wall=25914
2022-03-06 19:56:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:58:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:58:52 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.814 | nll_loss 7.602 | ppl 194.32 | wps 37298.3 | wpb 510.9 | bsz 1 | num_updates 7981 | best_loss 7.96
2022-03-06 19:58:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7981 updates
2022-03-06 19:58:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 19:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 41 @ 7981 updates, score 8.814) (writing took 3.407422953285277 seconds)
2022-03-06 19:58:55 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 19:58:55 | INFO | train | epoch 041 | loss 5.414 | nll_loss 3.916 | ppl 15.09 | wps 19916.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7981 | lr 0.000353974 | gnorm 0.767 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 26187
2022-03-06 19:58:55 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 19:58:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:59:56 | INFO | train_inner | epoch 042:     19 / 196 loss=5.435, nll_loss=3.939, ppl=15.34, wps=19553.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.777, loss_scale=8, train_wall=300, gb_free=19.9, wall=26249
2022-03-06 20:04:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:05:22 | INFO | train_inner | epoch 042:    120 / 196 loss=5.345, nll_loss=3.837, ppl=14.29, wps=20135.9, ups=0.31, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.773, loss_scale=8, train_wall=301, gb_free=19.9, wall=26574
2022-03-06 20:09:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:09:32 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.813 | nll_loss 7.602 | ppl 194.34 | wps 37542.9 | wpb 510.9 | bsz 1 | num_updates 8176 | best_loss 7.96
2022-03-06 20:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8176 updates
2022-03-06 20:09:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:09:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 42 @ 8176 updates, score 8.813) (writing took 3.375583198852837 seconds)
2022-03-06 20:09:35 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 20:09:35 | INFO | train | epoch 042 | loss 5.372 | nll_loss 3.867 | ppl 14.59 | wps 19938.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8176 | lr 0.000349727 | gnorm 0.786 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 26827
2022-03-06 20:09:35 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 20:09:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:10:53 | INFO | train_inner | epoch 043:     24 / 196 loss=5.384, nll_loss=3.881, ppl=14.73, wps=19764.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.797, loss_scale=8, train_wall=297, gb_free=19.9, wall=26905
2022-03-06 20:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:16:18 | INFO | train_inner | epoch 043:    125 / 196 loss=5.313, nll_loss=3.8, ppl=13.92, wps=20113.9, ups=0.31, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.79, loss_scale=8, train_wall=301, gb_free=19.9, wall=27231
2022-03-06 20:20:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:20:12 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.902 | nll_loss 7.698 | ppl 207.72 | wps 37864.9 | wpb 510.9 | bsz 1 | num_updates 8371 | best_loss 7.96
2022-03-06 20:20:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8371 updates
2022-03-06 20:20:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:20:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:20:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 43 @ 8371 updates, score 8.902) (writing took 3.510652953758836 seconds)
2022-03-06 20:20:16 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 20:20:16 | INFO | train | epoch 043 | loss 5.331 | nll_loss 3.82 | ppl 14.12 | wps 19918.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8371 | lr 0.00034563 | gnorm 0.792 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 27468
2022-03-06 20:20:16 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 20:20:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:21:49 | INFO | train_inner | epoch 044:     29 / 196 loss=5.33, nll_loss=3.819, ppl=14.12, wps=19740.9, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.791, loss_scale=16, train_wall=297, gb_free=19.9, wall=27562
2022-03-06 20:22:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:27:15 | INFO | train_inner | epoch 044:    130 / 196 loss=5.278, nll_loss=3.759, ppl=13.54, wps=20110.7, ups=0.31, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.774, loss_scale=8, train_wall=301, gb_free=19.9, wall=27888
2022-03-06 20:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:30:53 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.943 | nll_loss 7.736 | ppl 213.12 | wps 37905.3 | wpb 510.9 | bsz 1 | num_updates 8566 | best_loss 7.96
2022-03-06 20:30:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8566 updates
2022-03-06 20:30:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:30:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:30:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 44 @ 8566 updates, score 8.943) (writing took 3.4390042684972286 seconds)
2022-03-06 20:30:56 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 20:30:56 | INFO | train | epoch 044 | loss 5.292 | nll_loss 3.775 | ppl 13.69 | wps 19919.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8566 | lr 0.000341673 | gnorm 0.781 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 28109
2022-03-06 20:30:57 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 20:30:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:31:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:32:50 | INFO | train_inner | epoch 045:     35 / 196 loss=5.293, nll_loss=3.776, ppl=13.7, wps=19560.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.797, loss_scale=8, train_wall=300, gb_free=19.9, wall=28222
2022-03-06 20:38:12 | INFO | train_inner | epoch 045:    135 / 196 loss=5.243, nll_loss=3.719, ppl=13.17, wps=20317.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.802, loss_scale=8, train_wall=298, gb_free=19.9, wall=28544
2022-03-06 20:39:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:41:34 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.014 | nll_loss 7.825 | ppl 226.75 | wps 37829.1 | wpb 510.9 | bsz 1 | num_updates 8760 | best_loss 7.96
2022-03-06 20:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8760 updates
2022-03-06 20:41:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:41:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 45 @ 8760 updates, score 9.014) (writing took 3.433318866416812 seconds)
2022-03-06 20:41:37 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 20:41:37 | INFO | train | epoch 045 | loss 5.253 | nll_loss 3.731 | ppl 13.28 | wps 19818.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 8760 | lr 0.000337869 | gnorm 0.805 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 28749
2022-03-06 20:41:37 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 20:41:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:43:46 | INFO | train_inner | epoch 046:     40 / 196 loss=5.249, nll_loss=3.726, ppl=13.23, wps=19562.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.804, loss_scale=8, train_wall=300, gb_free=19.9, wall=28878
2022-03-06 20:47:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:49:12 | INFO | train_inner | epoch 046:    141 / 196 loss=5.209, nll_loss=3.68, ppl=12.81, wps=20129.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.797, loss_scale=8, train_wall=301, gb_free=19.9, wall=29204
2022-03-06 20:52:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:52:14 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.028 | nll_loss 7.832 | ppl 227.82 | wps 37679.2 | wpb 510.9 | bsz 1 | num_updates 8955 | best_loss 7.96
2022-03-06 20:52:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8955 updates
2022-03-06 20:52:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:52:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 20:52:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 46 @ 8955 updates, score 9.028) (writing took 3.4906176049262285 seconds)
2022-03-06 20:52:18 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 20:52:18 | INFO | train | epoch 046 | loss 5.217 | nll_loss 3.689 | ppl 12.9 | wps 19928 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8955 | lr 0.00033417 | gnorm 0.8 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 29390
2022-03-06 20:52:18 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 20:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:54:43 | INFO | train_inner | epoch 047:     45 / 196 loss=5.201, nll_loss=3.671, ppl=12.74, wps=19753.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.809, loss_scale=16, train_wall=297, gb_free=19.9, wall=29535
2022-03-06 20:55:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:00:08 | INFO | train_inner | epoch 047:    146 / 196 loss=5.183, nll_loss=3.65, ppl=12.55, wps=20132.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.816, loss_scale=8, train_wall=301, gb_free=19.9, wall=29860
2022-03-06 21:02:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:02:54 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.057 | nll_loss 7.859 | ppl 232.17 | wps 37528.5 | wpb 510.9 | bsz 1 | num_updates 9150 | best_loss 7.96
2022-03-06 21:02:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9150 updates
2022-03-06 21:02:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:02:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:02:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 47 @ 9150 updates, score 9.057) (writing took 3.4746046997606754 seconds)
2022-03-06 21:02:58 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 21:02:58 | INFO | train | epoch 047 | loss 5.183 | nll_loss 3.65 | ppl 12.55 | wps 19931.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9150 | lr 0.00033059 | gnorm 0.819 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 30030
2022-03-06 21:02:58 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 21:02:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:03:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:05:42 | INFO | train_inner | epoch 048:     51 / 196 loss=5.159, nll_loss=3.623, ppl=12.32, wps=19559.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.819, loss_scale=8, train_wall=300, gb_free=19.9, wall=30195
2022-03-06 21:10:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:11:08 | INFO | train_inner | epoch 048:    152 / 196 loss=5.16, nll_loss=3.623, ppl=12.32, wps=20126.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.815, loss_scale=8, train_wall=301, gb_free=19.9, wall=30520
2022-03-06 21:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:13:35 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.141 | nll_loss 7.942 | ppl 245.86 | wps 37902.3 | wpb 510.9 | bsz 1 | num_updates 9344 | best_loss 7.96
2022-03-06 21:13:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9344 updates
2022-03-06 21:13:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:13:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 48 @ 9344 updates, score 9.141) (writing took 3.437391604296863 seconds)
2022-03-06 21:13:38 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 21:13:38 | INFO | train | epoch 048 | loss 5.148 | nll_loss 3.61 | ppl 12.21 | wps 19824.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.812 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 30671
2022-03-06 21:13:38 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 21:13:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:16:39 | INFO | train_inner | epoch 049:     56 / 196 loss=5.12, nll_loss=3.578, ppl=11.94, wps=19752.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.816, loss_scale=8, train_wall=297, gb_free=19.9, wall=30851
2022-03-06 21:20:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:22:04 | INFO | train_inner | epoch 049:    157 / 196 loss=5.131, nll_loss=3.589, ppl=12.04, wps=20175.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.812, loss_scale=8, train_wall=300, gb_free=19.9, wall=31176
2022-03-06 21:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:24:14 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.195 | nll_loss 8.007 | ppl 257.31 | wps 37795.2 | wpb 510.9 | bsz 1 | num_updates 9539 | best_loss 7.96
2022-03-06 21:24:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9539 updates
2022-03-06 21:24:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:24:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 49 @ 9539 updates, score 9.195) (writing took 3.420545701868832 seconds)
2022-03-06 21:24:18 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 21:24:18 | INFO | train | epoch 049 | loss 5.117 | nll_loss 3.574 | ppl 11.91 | wps 19959.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9539 | lr 0.000323779 | gnorm 0.817 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 31310
2022-03-06 21:24:18 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 21:24:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:27:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:27:38 | INFO | train_inner | epoch 050:     62 / 196 loss=5.086, nll_loss=3.538, ppl=11.62, wps=19564.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.82, loss_scale=8, train_wall=300, gb_free=19.9, wall=31510
2022-03-06 21:33:00 | INFO | train_inner | epoch 050:    162 / 196 loss=5.102, nll_loss=3.556, ppl=11.77, wps=20327.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.843, loss_scale=8, train_wall=298, gb_free=19.9, wall=31832
2022-03-06 21:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:34:55 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.216 | nll_loss 8.019 | ppl 259.46 | wps 37748.4 | wpb 510.9 | bsz 1 | num_updates 9734 | best_loss 7.96
2022-03-06 21:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9734 updates
2022-03-06 21:34:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:34:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 50 @ 9734 updates, score 9.216) (writing took 3.4398694494739175 seconds)
2022-03-06 21:34:58 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 21:34:58 | INFO | train | epoch 050 | loss 5.086 | nll_loss 3.539 | ppl 11.62 | wps 19927.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9734 | lr 0.000320519 | gnorm 0.829 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 31950
2022-03-06 21:34:58 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 21:34:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:36:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:38:34 | INFO | train_inner | epoch 051:     67 / 196 loss=5.041, nll_loss=3.487, ppl=11.21, wps=19564.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.833, loss_scale=8, train_wall=300, gb_free=19.9, wall=32167
2022-03-06 21:43:57 | INFO | train_inner | epoch 051:    167 / 196 loss=5.079, nll_loss=3.529, ppl=11.55, wps=20332.4, ups=0.31, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.846, loss_scale=16, train_wall=298, gb_free=19.9, wall=32489
2022-03-06 21:45:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:45:35 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.25 | nll_loss 8.066 | ppl 268.03 | wps 37634.4 | wpb 510.9 | bsz 1 | num_updates 9929 | best_loss 7.96
2022-03-06 21:45:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9929 updates
2022-03-06 21:45:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 51 @ 9929 updates, score 9.25) (writing took 3.5028063766658306 seconds)
2022-03-06 21:45:39 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 21:45:39 | INFO | train | epoch 051 | loss 5.055 | nll_loss 3.503 | ppl 11.34 | wps 19926.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9929 | lr 0.000317356 | gnorm 0.843 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 32591
2022-03-06 21:45:39 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 21:45:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:46:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:49:31 | INFO | train_inner | epoch 052:     72 / 196 loss=5.016, nll_loss=3.458, ppl=10.99, wps=19548.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.834, loss_scale=8, train_wall=300, gb_free=19.9, wall=32823
2022-03-06 21:53:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:54:57 | INFO | train_inner | epoch 052:    173 / 196 loss=5.054, nll_loss=3.502, ppl=11.33, wps=20130.4, ups=0.31, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.843, loss_scale=8, train_wall=301, gb_free=19.9, wall=33149
2022-03-06 21:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:56:16 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.329 | nll_loss 8.147 | ppl 283.42 | wps 37685.1 | wpb 510.9 | bsz 1 | num_updates 10123 | best_loss 7.96
2022-03-06 21:56:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10123 updates
2022-03-06 21:56:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 21:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 52 @ 10123 updates, score 9.329) (writing took 3.4874548008665442 seconds)
2022-03-06 21:56:19 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 21:56:19 | INFO | train | epoch 052 | loss 5.026 | nll_loss 3.469 | ppl 11.07 | wps 19820.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 10123 | lr 0.000314301 | gnorm 0.845 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 33231
2022-03-06 21:56:19 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 21:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:00:28 | INFO | train_inner | epoch 053:     77 / 196 loss=4.978, nll_loss=3.415, ppl=10.67, wps=19744.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.841, loss_scale=16, train_wall=297, gb_free=19.9, wall=33480
2022-03-06 22:01:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:05:53 | INFO | train_inner | epoch 053:    178 / 196 loss=5.032, nll_loss=3.476, ppl=11.13, wps=20132.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.854, loss_scale=8, train_wall=301, gb_free=19.9, wall=33805
2022-03-06 22:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:06:56 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.364 | nll_loss 8.178 | ppl 289.63 | wps 37905.4 | wpb 510.9 | bsz 1 | num_updates 10318 | best_loss 7.96
2022-03-06 22:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10318 updates
2022-03-06 22:06:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:07:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 53 @ 10318 updates, score 9.364) (writing took 3.48175117559731 seconds)
2022-03-06 22:07:00 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 22:07:00 | INFO | train | epoch 053 | loss 4.998 | nll_loss 3.438 | ppl 10.84 | wps 19931.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 10318 | lr 0.000311317 | gnorm 0.848 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 33872
2022-03-06 22:07:00 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 22:07:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:11:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:11:27 | INFO | train_inner | epoch 054:     83 / 196 loss=4.934, nll_loss=3.364, ppl=10.3, wps=19564.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.846, loss_scale=8, train_wall=300, gb_free=19.9, wall=34139
2022-03-06 22:16:50 | INFO | train_inner | epoch 054:    183 / 196 loss=5.011, nll_loss=3.452, ppl=10.94, wps=20328.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.858, loss_scale=8, train_wall=298, gb_free=19.9, wall=34462
2022-03-06 22:17:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:17:37 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.357 | nll_loss 8.176 | ppl 289.3 | wps 37592.9 | wpb 510.9 | bsz 1 | num_updates 10513 | best_loss 7.96
2022-03-06 22:17:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10513 updates
2022-03-06 22:17:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 54 @ 10513 updates, score 9.357) (writing took 3.48615634907037 seconds)
2022-03-06 22:17:40 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 22:17:40 | INFO | train | epoch 054 | loss 4.971 | nll_loss 3.406 | ppl 10.6 | wps 19921.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 10513 | lr 0.000308416 | gnorm 0.848 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 34512
2022-03-06 22:17:40 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 22:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:20:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:22:24 | INFO | train_inner | epoch 055:     88 / 196 loss=4.91, nll_loss=3.336, ppl=10.1, wps=19547.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.853, loss_scale=8, train_wall=300, gb_free=19.9, wall=34796
2022-03-06 22:27:46 | INFO | train_inner | epoch 055:    188 / 196 loss=4.989, nll_loss=3.426, ppl=10.74, wps=20327.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.868, loss_scale=16, train_wall=298, gb_free=19.9, wall=35119
2022-03-06 22:28:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:28:17 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.406 | nll_loss 8.23 | ppl 300.24 | wps 37754.6 | wpb 510.9 | bsz 1 | num_updates 10708 | best_loss 7.96
2022-03-06 22:28:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10708 updates
2022-03-06 22:28:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:28:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 55 @ 10708 updates, score 9.406) (writing took 3.4728597290813923 seconds)
2022-03-06 22:28:21 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 22:28:21 | INFO | train | epoch 055 | loss 4.945 | nll_loss 3.376 | ppl 10.38 | wps 19924.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 10708 | lr 0.000305595 | gnorm 0.862 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 35153
2022-03-06 22:28:21 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 22:28:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:29:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:33:20 | INFO | train_inner | epoch 056:     93 / 196 loss=4.88, nll_loss=3.302, ppl=9.87, wps=19570.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.865, loss_scale=8, train_wall=300, gb_free=19.9, wall=35453
2022-03-06 22:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:38:45 | INFO | train_inner | epoch 056:    194 / 196 loss=4.961, nll_loss=3.394, ppl=10.51, wps=20165.2, ups=0.31, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.87, loss_scale=8, train_wall=301, gb_free=19.9, wall=35778
2022-03-06 22:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:38:57 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.436 | nll_loss 8.254 | ppl 305.28 | wps 37881 | wpb 510.9 | bsz 1 | num_updates 10902 | best_loss 7.96
2022-03-06 22:38:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10902 updates
2022-03-06 22:38:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:39:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:39:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 56 @ 10902 updates, score 9.436) (writing took 3.4885133672505617 seconds)
2022-03-06 22:39:00 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 22:39:00 | INFO | train | epoch 056 | loss 4.919 | nll_loss 3.346 | ppl 10.17 | wps 19850.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 10902 | lr 0.000302863 | gnorm 0.866 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 35793
2022-03-06 22:39:00 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 22:39:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:44:15 | INFO | train_inner | epoch 057:     98 / 196 loss=4.855, nll_loss=3.273, ppl=9.67, wps=19805.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.868, loss_scale=16, train_wall=297, gb_free=19.9, wall=36108
2022-03-06 22:44:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:49:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:49:35 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.497 | nll_loss 8.335 | ppl 322.81 | wps 37868.3 | wpb 510.9 | bsz 1 | num_updates 11097 | best_loss 7.96
2022-03-06 22:49:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11097 updates
2022-03-06 22:49:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:49:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 22:49:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 57 @ 11097 updates, score 9.497) (writing took 3.570568848401308 seconds)
2022-03-06 22:49:39 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 22:49:39 | INFO | train | epoch 057 | loss 4.896 | nll_loss 3.319 | ppl 9.98 | wps 19993.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11097 | lr 0.000300191 | gnorm 0.877 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 36431
2022-03-06 22:49:39 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 22:49:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:49:48 | INFO | train_inner | epoch 058:      3 / 196 loss=4.936, nll_loss=3.365, ppl=10.3, wps=19627.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.888, loss_scale=8, train_wall=299, gb_free=19.9, wall=36441
2022-03-06 22:52:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:55:13 | INFO | train_inner | epoch 058:    104 / 196 loss=4.829, nll_loss=3.243, ppl=9.47, wps=20175.8, ups=0.31, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.863, loss_scale=8, train_wall=300, gb_free=19.9, wall=36766
2022-03-06 22:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:00:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:00:14 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.521 | nll_loss 8.342 | ppl 324.46 | wps 37994.5 | wpb 510.9 | bsz 1 | num_updates 11291 | best_loss 7.96
2022-03-06 23:00:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11291 updates
2022-03-06 23:00:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:00:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:00:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 58 @ 11291 updates, score 9.521) (writing took 3.4952761800959706 seconds)
2022-03-06 23:00:18 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 23:00:18 | INFO | train | epoch 058 | loss 4.87 | nll_loss 3.29 | ppl 9.78 | wps 19862.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 11291 | lr 0.000297601 | gnorm 0.875 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 37070
2022-03-06 23:00:18 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 23:00:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:00:47 | INFO | train_inner | epoch 059:      9 / 196 loss=4.902, nll_loss=3.327, ppl=10.03, wps=19587.6, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.887, loss_scale=8, train_wall=300, gb_free=19.9, wall=37099
2022-03-06 23:06:09 | INFO | train_inner | epoch 059:    109 / 196 loss=4.811, nll_loss=3.222, ppl=9.33, wps=20335.4, ups=0.31, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.878, loss_scale=8, train_wall=298, gb_free=19.9, wall=37421
2022-03-06 23:08:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:10:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:10:54 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.538 | nll_loss 8.356 | ppl 327.73 | wps 37622.8 | wpb 510.9 | bsz 1 | num_updates 11486 | best_loss 7.96
2022-03-06 23:10:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11486 updates
2022-03-06 23:10:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:10:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:10:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 59 @ 11486 updates, score 9.538) (writing took 3.43955984339118 seconds)
2022-03-06 23:10:57 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 23:10:57 | INFO | train | epoch 059 | loss 4.848 | nll_loss 3.265 | ppl 9.61 | wps 19952.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 11486 | lr 0.000295064 | gnorm 0.89 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 37710
2022-03-06 23:10:58 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 23:10:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:11:42 | INFO | train_inner | epoch 060:     14 / 196 loss=4.88, nll_loss=3.301, ppl=9.86, wps=19619.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.899, loss_scale=8, train_wall=300, gb_free=19.9, wall=37755
2022-03-06 23:16:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:17:07 | INFO | train_inner | epoch 060:    115 / 196 loss=4.795, nll_loss=3.204, ppl=9.21, wps=20181.8, ups=0.31, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.886, loss_scale=8, train_wall=300, gb_free=19.9, wall=38079
2022-03-06 23:21:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:21:33 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.567 | nll_loss 8.394 | ppl 336.48 | wps 37605.6 | wpb 510.9 | bsz 1 | num_updates 11681 | best_loss 7.96
2022-03-06 23:21:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11681 updates
2022-03-06 23:21:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:21:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:21:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 60 @ 11681 updates, score 9.567) (writing took 3.525314537808299 seconds)
2022-03-06 23:21:37 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 23:21:37 | INFO | train | epoch 060 | loss 4.825 | nll_loss 3.238 | ppl 9.43 | wps 19969.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11681 | lr 0.00029259 | gnorm 0.89 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 38349
2022-03-06 23:21:37 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 23:21:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:22:38 | INFO | train_inner | epoch 061:     19 / 196 loss=4.846, nll_loss=3.262, ppl=9.59, wps=19765.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.893, loss_scale=8, train_wall=297, gb_free=19.9, wall=38410
2022-03-06 23:28:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:28:03 | INFO | train_inner | epoch 061:    120 / 196 loss=4.778, nll_loss=3.184, ppl=9.09, wps=20143.7, ups=0.31, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.876, loss_scale=8, train_wall=301, gb_free=19.9, wall=38735
2022-03-06 23:32:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:32:13 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.616 | nll_loss 8.449 | ppl 349.48 | wps 37887.2 | wpb 510.9 | bsz 1 | num_updates 11876 | best_loss 7.96
2022-03-06 23:32:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11876 updates
2022-03-06 23:32:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:32:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:32:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 61 @ 11876 updates, score 9.616) (writing took 3.4791942508891225 seconds)
2022-03-06 23:32:16 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 23:32:16 | INFO | train | epoch 061 | loss 4.804 | nll_loss 3.214 | ppl 9.28 | wps 19944.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 11876 | lr 0.000290178 | gnorm 0.88 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 38989
2022-03-06 23:32:16 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 23:32:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:33:34 | INFO | train_inner | epoch 062:     24 / 196 loss=4.823, nll_loss=3.235, ppl=9.41, wps=19752.1, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.889, loss_scale=8, train_wall=297, gb_free=19.9, wall=39066
2022-03-06 23:38:57 | INFO | train_inner | epoch 062:    124 / 196 loss=4.76, nll_loss=3.163, ppl=8.96, wps=20325.7, ups=0.31, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.897, loss_scale=16, train_wall=298, gb_free=19.9, wall=39389
2022-03-06 23:39:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:42:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:42:54 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.677 | nll_loss 8.509 | ppl 364.41 | wps 37932 | wpb 510.9 | bsz 1 | num_updates 12071 | best_loss 7.96
2022-03-06 23:42:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12071 updates
2022-03-06 23:42:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:42:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:42:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 62 @ 12071 updates, score 9.677) (writing took 3.480643393471837 seconds)
2022-03-06 23:42:57 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 23:42:57 | INFO | train | epoch 062 | loss 4.784 | nll_loss 3.19 | ppl 9.13 | wps 19922.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 12071 | lr 0.000287825 | gnorm 0.9 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 39629
2022-03-06 23:42:57 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 23:42:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:44:31 | INFO | train_inner | epoch 063:     29 / 196 loss=4.798, nll_loss=3.207, ppl=9.23, wps=19559.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.901, loss_scale=8, train_wall=300, gb_free=19.9, wall=39723
2022-03-06 23:47:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:49:56 | INFO | train_inner | epoch 063:    130 / 196 loss=4.742, nll_loss=3.142, ppl=8.83, wps=20117.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.888, loss_scale=8, train_wall=301, gb_free=19.9, wall=40049
2022-03-06 23:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:53:34 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.696 | nll_loss 8.534 | ppl 370.75 | wps 37725 | wpb 510.9 | bsz 1 | num_updates 12266 | best_loss 7.96
2022-03-06 23:53:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12266 updates
2022-03-06 23:53:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:53:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-06 23:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 63 @ 12266 updates, score 9.696) (writing took 3.523434509523213 seconds)
2022-03-06 23:53:38 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 23:53:38 | INFO | train | epoch 063 | loss 4.762 | nll_loss 3.165 | ppl 8.97 | wps 19919.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 12266 | lr 0.000285528 | gnorm 0.893 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 40270
2022-03-06 23:53:38 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 23:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:55:28 | INFO | train_inner | epoch 064:     34 / 196 loss=4.771, nll_loss=3.176, ppl=9.04, wps=19742, ups=0.3, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.905, loss_scale=16, train_wall=297, gb_free=19.9, wall=40380
2022-03-06 23:59:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:00:53 | INFO | train_inner | epoch 064:    135 / 196 loss=4.731, nll_loss=3.13, ppl=8.75, wps=20133.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.904, loss_scale=8, train_wall=301, gb_free=19.9, wall=40705
2022-03-07 00:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:04:15 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.713 | nll_loss 8.555 | ppl 375.99 | wps 37858.1 | wpb 510.9 | bsz 1 | num_updates 12461 | best_loss 7.96
2022-03-07 00:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12461 updates
2022-03-07 00:04:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:04:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 64 @ 12461 updates, score 9.713) (writing took 3.4992592949420214 seconds)
2022-03-07 00:04:18 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-07 00:04:18 | INFO | train | epoch 064 | loss 4.743 | nll_loss 3.144 | ppl 8.84 | wps 19931.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 12461 | lr 0.000283285 | gnorm 0.907 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 40910
2022-03-07 00:04:18 | INFO | fairseq.trainer | begin training epoch 65
2022-03-07 00:04:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:06:24 | INFO | train_inner | epoch 065:     39 / 196 loss=4.744, nll_loss=3.144, ppl=8.84, wps=19755.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.902, loss_scale=16, train_wall=297, gb_free=19.9, wall=41036
2022-03-07 00:07:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:11:50 | INFO | train_inner | epoch 065:    140 / 196 loss=4.711, nll_loss=3.107, ppl=8.62, wps=20120.9, ups=0.31, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.902, loss_scale=8, train_wall=301, gb_free=19.9, wall=41362
2022-03-07 00:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:14:55 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.715 | nll_loss 8.539 | ppl 371.99 | wps 37683.8 | wpb 510.9 | bsz 1 | num_updates 12656 | best_loss 7.96
2022-03-07 00:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12656 updates
2022-03-07 00:14:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:14:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 65 @ 12656 updates, score 9.715) (writing took 3.450618589296937 seconds)
2022-03-07 00:14:59 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-07 00:14:59 | INFO | train | epoch 065 | loss 4.722 | nll_loss 3.12 | ppl 8.69 | wps 19924 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 12656 | lr 0.000281094 | gnorm 0.902 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 41551
2022-03-07 00:14:59 | INFO | fairseq.trainer | begin training epoch 66
2022-03-07 00:14:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:17:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:17:24 | INFO | train_inner | epoch 066:     45 / 196 loss=4.722, nll_loss=3.12, ppl=8.69, wps=19566.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.914, loss_scale=8, train_wall=300, gb_free=19.9, wall=41696
2022-03-07 00:22:46 | INFO | train_inner | epoch 066:    145 / 196 loss=4.706, nll_loss=3.101, ppl=8.58, wps=20334.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.921, loss_scale=8, train_wall=298, gb_free=19.9, wall=42018
2022-03-07 00:25:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:25:35 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.74 | nll_loss 8.567 | ppl 379.34 | wps 37776.7 | wpb 510.9 | bsz 1 | num_updates 12851 | best_loss 7.96
2022-03-07 00:25:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12851 updates
2022-03-07 00:25:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:25:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:25:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 66 @ 12851 updates, score 9.74) (writing took 3.513157428242266 seconds)
2022-03-07 00:25:39 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-07 00:25:39 | INFO | train | epoch 066 | loss 4.705 | nll_loss 3.1 | ppl 8.57 | wps 19928.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 12851 | lr 0.000278953 | gnorm 0.919 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 42191
2022-03-07 00:25:39 | INFO | fairseq.trainer | begin training epoch 67
2022-03-07 00:25:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:27:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:28:20 | INFO | train_inner | epoch 067:     50 / 196 loss=4.693, nll_loss=3.086, ppl=8.49, wps=19558.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.909, loss_scale=8, train_wall=300, gb_free=19.9, wall=42352
2022-03-07 00:33:42 | INFO | train_inner | epoch 067:    150 / 196 loss=4.69, nll_loss=3.082, ppl=8.47, wps=20347.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.917, loss_scale=8, train_wall=298, gb_free=19.9, wall=42674
2022-03-07 00:36:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:36:16 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.797 | nll_loss 8.634 | ppl 397.34 | wps 37877.2 | wpb 510.9 | bsz 1 | num_updates 13046 | best_loss 7.96
2022-03-07 00:36:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13046 updates
2022-03-07 00:36:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 67 @ 13046 updates, score 9.797) (writing took 3.527314865961671 seconds)
2022-03-07 00:36:19 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-07 00:36:19 | INFO | train | epoch 067 | loss 4.687 | nll_loss 3.079 | ppl 8.45 | wps 19938.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 13046 | lr 0.000276861 | gnorm 0.912 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 42831
2022-03-07 00:36:19 | INFO | fairseq.trainer | begin training epoch 68
2022-03-07 00:36:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:39:13 | INFO | train_inner | epoch 068:     54 / 196 loss=4.672, nll_loss=3.062, ppl=8.35, wps=19740.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.919, loss_scale=16, train_wall=297, gb_free=19.9, wall=43006
2022-03-07 00:40:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:44:38 | INFO | train_inner | epoch 068:    155 / 196 loss=4.679, nll_loss=3.069, ppl=8.39, wps=20163.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.933, loss_scale=8, train_wall=301, gb_free=19.9, wall=43331
2022-03-07 00:46:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:46:55 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.848 | nll_loss 8.698 | ppl 415.29 | wps 37498.1 | wpb 510.9 | bsz 1 | num_updates 13241 | best_loss 7.96
2022-03-07 00:46:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13241 updates
2022-03-07 00:46:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:46:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:46:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 68 @ 13241 updates, score 9.848) (writing took 3.4656110936775804 seconds)
2022-03-07 00:46:59 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-07 00:46:59 | INFO | train | epoch 068 | loss 4.669 | nll_loss 3.058 | ppl 8.33 | wps 19945.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 13241 | lr 0.000274814 | gnorm 0.926 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 43471
2022-03-07 00:46:59 | INFO | fairseq.trainer | begin training epoch 69
2022-03-07 00:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:47:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:50:12 | INFO | train_inner | epoch 069:     60 / 196 loss=4.649, nll_loss=3.036, ppl=8.2, wps=19573.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=0.921, loss_scale=8, train_wall=300, gb_free=19.9, wall=43665
2022-03-07 00:55:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:55:38 | INFO | train_inner | epoch 069:    161 / 196 loss=4.666, nll_loss=3.054, ppl=8.3, wps=20133.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.933, loss_scale=8, train_wall=301, gb_free=19.9, wall=43990
2022-03-07 00:57:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:57:36 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.845 | nll_loss 8.682 | ppl 410.67 | wps 37709.9 | wpb 510.9 | bsz 1 | num_updates 13435 | best_loss 7.96
2022-03-07 00:57:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13435 updates
2022-03-07 00:57:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:57:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 00:57:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 69 @ 13435 updates, score 9.845) (writing took 3.4975022254511714 seconds)
2022-03-07 00:57:39 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-07 00:57:39 | INFO | train | epoch 069 | loss 4.652 | nll_loss 3.039 | ppl 8.22 | wps 19833.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 13435 | lr 0.000272823 | gnorm 0.936 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 44111
2022-03-07 00:57:39 | INFO | fairseq.trainer | begin training epoch 70
2022-03-07 00:57:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:01:09 | INFO | train_inner | epoch 070:     65 / 196 loss=4.625, nll_loss=3.008, ppl=8.04, wps=19750.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.936, loss_scale=8, train_wall=297, gb_free=19.9, wall=44321
2022-03-07 01:02:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:06:34 | INFO | train_inner | epoch 070:    166 / 196 loss=4.655, nll_loss=3.041, ppl=8.23, wps=20129.1, ups=0.31, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.925, loss_scale=8, train_wall=301, gb_free=19.9, wall=44647
2022-03-07 01:08:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:08:16 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.892 | nll_loss 8.731 | ppl 425.04 | wps 37553.1 | wpb 510.9 | bsz 1 | num_updates 13630 | best_loss 7.96
2022-03-07 01:08:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13630 updates
2022-03-07 01:08:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:08:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:08:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 70 @ 13630 updates, score 9.892) (writing took 3.5308594638481736 seconds)
2022-03-07 01:08:20 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-07 01:08:20 | INFO | train | epoch 070 | loss 4.635 | nll_loss 3.019 | ppl 8.11 | wps 19922.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 13630 | lr 0.000270864 | gnorm 0.929 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 44752
2022-03-07 01:08:20 | INFO | fairseq.trainer | begin training epoch 71
2022-03-07 01:08:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:12:05 | INFO | train_inner | epoch 071:     70 / 196 loss=4.608, nll_loss=2.988, ppl=7.93, wps=19741, ups=0.3, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=0.938, loss_scale=16, train_wall=297, gb_free=19.9, wall=44978
2022-03-07 01:16:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:17:31 | INFO | train_inner | epoch 071:    171 / 196 loss=4.636, nll_loss=3.02, ppl=8.11, wps=20120.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.94, loss_scale=16, train_wall=301, gb_free=19.9, wall=45303
2022-03-07 01:18:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:18:57 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.884 | nll_loss 8.726 | ppl 423.43 | wps 37641.1 | wpb 510.9 | bsz 1 | num_updates 13825 | best_loss 7.96
2022-03-07 01:18:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13825 updates
2022-03-07 01:18:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:19:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:19:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 71 @ 13825 updates, score 9.884) (writing took 3.5228938907384872 seconds)
2022-03-07 01:19:00 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-07 01:19:00 | INFO | train | epoch 071 | loss 4.619 | nll_loss 3 | ppl 8 | wps 19917.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 13825 | lr 0.000268947 | gnorm 0.935 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 45393
2022-03-07 01:19:00 | INFO | fairseq.trainer | begin training epoch 72
2022-03-07 01:19:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:19:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:23:06 | INFO | train_inner | epoch 072:     76 / 196 loss=4.584, nll_loss=2.961, ppl=7.78, wps=19541.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.929, loss_scale=8, train_wall=300, gb_free=19.9, wall=45638
2022-03-07 01:28:28 | INFO | train_inner | epoch 072:    176 / 196 loss=4.629, nll_loss=3.012, ppl=8.07, wps=20337.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.942, loss_scale=16, train_wall=298, gb_free=19.9, wall=45960
2022-03-07 01:29:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:29:37 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.937 | nll_loss 8.784 | ppl 440.87 | wps 37924.2 | wpb 510.9 | bsz 1 | num_updates 14020 | best_loss 7.96
2022-03-07 01:29:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14020 updates
2022-03-07 01:29:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:29:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:29:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 72 @ 14020 updates, score 9.937) (writing took 3.513443671166897 seconds)
2022-03-07 01:29:41 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-07 01:29:41 | INFO | train | epoch 072 | loss 4.604 | nll_loss 2.983 | ppl 7.91 | wps 19926.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 14020 | lr 0.000267071 | gnorm 0.938 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 46033
2022-03-07 01:29:41 | INFO | fairseq.trainer | begin training epoch 73
2022-03-07 01:29:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:30:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:34:02 | INFO | train_inner | epoch 073:     81 / 196 loss=4.568, nll_loss=2.941, ppl=7.68, wps=19556.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=0.931, loss_scale=8, train_wall=300, gb_free=19.9, wall=46294
2022-03-07 01:37:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:39:28 | INFO | train_inner | epoch 073:    182 / 196 loss=4.619, nll_loss=3, ppl=8, wps=20129.2, ups=0.31, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.949, loss_scale=8, train_wall=301, gb_free=19.9, wall=46620
2022-03-07 01:40:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:40:18 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.946 | nll_loss 8.784 | ppl 440.77 | wps 37681.1 | wpb 510.9 | bsz 1 | num_updates 14214 | best_loss 7.96
2022-03-07 01:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14214 updates
2022-03-07 01:40:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:40:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:40:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 73 @ 14214 updates, score 9.946) (writing took 3.5615404304116964 seconds)
2022-03-07 01:40:21 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-07 01:40:21 | INFO | train | epoch 073 | loss 4.589 | nll_loss 2.966 | ppl 7.81 | wps 19825.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 14214 | lr 0.000265242 | gnorm 0.94 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 46674
2022-03-07 01:40:21 | INFO | fairseq.trainer | begin training epoch 74
2022-03-07 01:40:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:44:59 | INFO | train_inner | epoch 074:     86 / 196 loss=4.544, nll_loss=2.914, ppl=7.54, wps=19749.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.944, loss_scale=16, train_wall=297, gb_free=19.9, wall=46951
2022-03-07 01:46:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:50:23 | INFO | train_inner | epoch 074:    187 / 196 loss=4.613, nll_loss=2.993, ppl=7.96, wps=20237.3, ups=0.31, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.951, loss_scale=8, train_wall=300, gb_free=19.9, wall=47275
2022-03-07 01:50:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:50:56 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.006 | nll_loss 8.864 | ppl 465.83 | wps 38553.9 | wpb 510.9 | bsz 1 | num_updates 14409 | best_loss 7.96
2022-03-07 01:50:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14409 updates
2022-03-07 01:50:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:51:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 01:51:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 74 @ 14409 updates, score 10.006) (writing took 3.4649267261847854 seconds)
2022-03-07 01:51:00 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-07 01:51:00 | INFO | train | epoch 074 | loss 4.574 | nll_loss 2.949 | ppl 7.72 | wps 19995.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14409 | lr 0.000263441 | gnorm 0.949 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 47312
2022-03-07 01:51:00 | INFO | fairseq.trainer | begin training epoch 75
2022-03-07 01:51:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:55:50 | INFO | train_inner | epoch 075:     91 / 196 loss=4.522, nll_loss=2.889, ppl=7.41, wps=19951.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=0.954, loss_scale=16, train_wall=295, gb_free=19.9, wall=47602
2022-03-07 01:58:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:01:12 | INFO | train_inner | epoch 075:    192 / 196 loss=4.601, nll_loss=2.979, ppl=7.89, wps=20347.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.959, loss_scale=8, train_wall=298, gb_free=19.9, wall=47924
2022-03-07 02:01:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:01:30 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.995 | nll_loss 8.847 | ppl 460.56 | wps 38301.5 | wpb 510.9 | bsz 1 | num_updates 14604 | best_loss 7.96
2022-03-07 02:01:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14604 updates
2022-03-07 02:01:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:01:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:01:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 75 @ 14604 updates, score 9.995) (writing took 3.506092901341617 seconds)
2022-03-07 02:01:33 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-07 02:01:33 | INFO | train | epoch 075 | loss 4.559 | nll_loss 2.931 | ppl 7.63 | wps 20134.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14604 | lr 0.000261676 | gnorm 0.956 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 47946
2022-03-07 02:01:33 | INFO | fairseq.trainer | begin training epoch 76
2022-03-07 02:01:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:06:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:06:43 | INFO | train_inner | epoch 076:     97 / 196 loss=4.509, nll_loss=2.874, ppl=7.33, wps=19748.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=0.953, loss_scale=8, train_wall=298, gb_free=19.9, wall=48255
2022-03-07 02:11:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:12:04 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.015 | nll_loss 8.863 | ppl 465.49 | wps 38702.5 | wpb 510.9 | bsz 1 | num_updates 14799 | best_loss 7.96
2022-03-07 02:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14799 updates
2022-03-07 02:12:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:12:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 76 @ 14799 updates, score 10.015) (writing took 3.5256128879263997 seconds)
2022-03-07 02:12:07 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-07 02:12:07 | INFO | train | epoch 076 | loss 4.544 | nll_loss 2.915 | ppl 7.54 | wps 20134 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14799 | lr 0.000259946 | gnorm 0.951 | loss_scale 8 | train_wall 578 | gb_free 19.9 | wall 48580
2022-03-07 02:12:07 | INFO | fairseq.trainer | begin training epoch 77
2022-03-07 02:12:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:12:11 | INFO | train_inner | epoch 077:      1 / 196 loss=4.581, nll_loss=2.957, ppl=7.76, wps=19965.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=0.951, loss_scale=8, train_wall=295, gb_free=19.9, wall=48583
2022-03-07 02:17:30 | INFO | train_inner | epoch 077:    101 / 196 loss=4.49, nll_loss=2.852, ppl=7.22, wps=20531.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.952, loss_scale=16, train_wall=296, gb_free=19.9, wall=48902
2022-03-07 02:18:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:22:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:22:38 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.051 | nll_loss 8.898 | ppl 477.01 | wps 39702.4 | wpb 510.9 | bsz 1 | num_updates 14994 | best_loss 7.96
2022-03-07 02:22:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14994 updates
2022-03-07 02:22:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:22:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:22:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 77 @ 14994 updates, score 10.051) (writing took 3.4725735737010837 seconds)
2022-03-07 02:22:41 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-07 02:22:41 | INFO | train | epoch 077 | loss 4.531 | nll_loss 2.899 | ppl 7.46 | wps 20138 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14994 | lr 0.000258251 | gnorm 0.963 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 49213
2022-03-07 02:22:41 | INFO | fairseq.trainer | begin training epoch 78
2022-03-07 02:22:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:23:00 | INFO | train_inner | epoch 078:      6 / 196 loss=4.567, nll_loss=2.94, ppl=7.67, wps=19777.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=0.974, loss_scale=8, train_wall=298, gb_free=19.9, wall=49233
2022-03-07 02:28:19 | INFO | train_inner | epoch 078:    106 / 196 loss=4.476, nll_loss=2.836, ppl=7.14, wps=20540.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.965, loss_scale=16, train_wall=296, gb_free=19.9, wall=49552
2022-03-07 02:32:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:33:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:33:11 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.062 | nll_loss 8.913 | ppl 482.08 | wps 38842.3 | wpb 510.9 | bsz 1 | num_updates 15189 | best_loss 7.96
2022-03-07 02:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15189 updates
2022-03-07 02:33:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:33:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:33:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 78 @ 15189 updates, score 10.062) (writing took 3.480338708497584 seconds)
2022-03-07 02:33:15 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-07 02:33:15 | INFO | train | epoch 078 | loss 4.518 | nll_loss 2.884 | ppl 7.38 | wps 20135.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15189 | lr 0.000256587 | gnorm 0.964 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 49847
2022-03-07 02:33:15 | INFO | fairseq.trainer | begin training epoch 79
2022-03-07 02:33:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:33:50 | INFO | train_inner | epoch 079:     11 / 196 loss=4.555, nll_loss=2.927, ppl=7.6, wps=19765, ups=0.3, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=0.959, loss_scale=16, train_wall=298, gb_free=19.9, wall=49882
2022-03-07 02:35:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:39:12 | INFO | train_inner | epoch 079:    112 / 196 loss=4.469, nll_loss=2.829, ppl=7.1, wps=20335.6, ups=0.31, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.958, loss_scale=8, train_wall=299, gb_free=19.9, wall=50205
2022-03-07 02:43:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:43:45 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.071 | nll_loss 8.925 | ppl 485.99 | wps 38735.4 | wpb 510.9 | bsz 1 | num_updates 15384 | best_loss 7.96
2022-03-07 02:43:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15384 updates
2022-03-07 02:43:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:43:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:43:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 79 @ 15384 updates, score 10.071) (writing took 3.5189054869115353 seconds)
2022-03-07 02:43:49 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-07 02:43:49 | INFO | train | epoch 079 | loss 4.504 | nll_loss 2.868 | ppl 7.3 | wps 20139.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15384 | lr 0.000254956 | gnorm 0.955 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 50481
2022-03-07 02:43:49 | INFO | fairseq.trainer | begin training epoch 80
2022-03-07 02:43:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:44:40 | INFO | train_inner | epoch 080:     16 / 196 loss=4.533, nll_loss=2.901, ppl=7.47, wps=19963.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=0.947, loss_scale=16, train_wall=295, gb_free=19.9, wall=50532
2022-03-07 02:49:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:50:02 | INFO | train_inner | epoch 080:    117 / 196 loss=4.463, nll_loss=2.822, ppl=7.07, wps=20330.6, ups=0.31, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.968, loss_scale=16, train_wall=299, gb_free=19.9, wall=50854
2022-03-07 02:50:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:54:21 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.133 | nll_loss 8.993 | ppl 509.41 | wps 37487.7 | wpb 510.9 | bsz 1 | num_updates 15578 | best_loss 7.96
2022-03-07 02:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15578 updates
2022-03-07 02:54:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:54:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 02:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 80 @ 15578 updates, score 10.133) (writing took 3.497571907006204 seconds)
2022-03-07 02:54:25 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-07 02:54:25 | INFO | train | epoch 080 | loss 4.492 | nll_loss 2.854 | ppl 7.23 | wps 19956.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 15578 | lr 0.000253364 | gnorm 0.967 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 51117
2022-03-07 02:54:25 | INFO | fairseq.trainer | begin training epoch 81
2022-03-07 02:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:55:36 | INFO | train_inner | epoch 081:     22 / 196 loss=4.521, nll_loss=2.888, ppl=7.4, wps=19580.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=0.977, loss_scale=8, train_wall=300, gb_free=19.9, wall=51188
2022-03-07 03:00:58 | INFO | train_inner | epoch 081:    122 / 196 loss=4.454, nll_loss=2.811, ppl=7.02, wps=20335.2, ups=0.31, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.956, loss_scale=16, train_wall=298, gb_free=19.9, wall=51510
2022-03-07 03:04:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:04:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:05:02 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.174 | nll_loss 9.042 | ppl 527.12 | wps 37184.6 | wpb 510.9 | bsz 1 | num_updates 15773 | best_loss 7.96
2022-03-07 03:05:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15773 updates
2022-03-07 03:05:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:05:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:05:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 81 @ 15773 updates, score 10.174) (writing took 3.4676885353401303 seconds)
2022-03-07 03:05:05 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-07 03:05:05 | INFO | train | epoch 081 | loss 4.478 | nll_loss 2.838 | ppl 7.15 | wps 19926.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 15773 | lr 0.000251793 | gnorm 0.962 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 51757
2022-03-07 03:05:05 | INFO | fairseq.trainer | begin training epoch 82
2022-03-07 03:05:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:06:33 | INFO | train_inner | epoch 082:     27 / 196 loss=4.492, nll_loss=2.855, ppl=7.23, wps=19549.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=0.971, loss_scale=16, train_wall=300, gb_free=19.9, wall=51845
2022-03-07 03:10:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:11:58 | INFO | train_inner | epoch 082:    128 / 196 loss=4.447, nll_loss=2.803, ppl=6.98, wps=20127.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.965, loss_scale=8, train_wall=301, gb_free=19.9, wall=52170
2022-03-07 03:15:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:15:42 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.11 | nll_loss 8.964 | ppl 499.38 | wps 37685.3 | wpb 510.9 | bsz 1 | num_updates 15968 | best_loss 7.96
2022-03-07 03:15:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15968 updates
2022-03-07 03:15:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:15:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:15:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 82 @ 15968 updates, score 10.11) (writing took 3.4773173900321126 seconds)
2022-03-07 03:15:46 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-07 03:15:46 | INFO | train | epoch 082 | loss 4.468 | nll_loss 2.827 | ppl 7.1 | wps 19924.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 15968 | lr 0.00025025 | gnorm 0.972 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 52398
2022-03-07 03:15:46 | INFO | fairseq.trainer | begin training epoch 83
2022-03-07 03:15:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:17:29 | INFO | train_inner | epoch 083:     32 / 196 loss=4.477, nll_loss=2.837, ppl=7.15, wps=19753.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=0.971, loss_scale=8, train_wall=297, gb_free=19.9, wall=52501
2022-03-07 03:22:52 | INFO | train_inner | epoch 083:    132 / 196 loss=4.444, nll_loss=2.799, ppl=6.96, wps=20319.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.966, loss_scale=16, train_wall=298, gb_free=19.9, wall=52824
2022-03-07 03:24:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:26:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:26:23 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.185 | nll_loss 9.046 | ppl 528.69 | wps 37205.9 | wpb 510.9 | bsz 1 | num_updates 16163 | best_loss 7.96
2022-03-07 03:26:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16163 updates
2022-03-07 03:26:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:26:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:26:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 83 @ 16163 updates, score 10.185) (writing took 3.4984185118228197 seconds)
2022-03-07 03:26:26 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-07 03:26:26 | INFO | train | epoch 083 | loss 4.456 | nll_loss 2.813 | ppl 7.03 | wps 19922.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 16163 | lr 0.000248736 | gnorm 0.97 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 53039
2022-03-07 03:26:26 | INFO | fairseq.trainer | begin training epoch 84
2022-03-07 03:26:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:27:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:28:29 | INFO | train_inner | epoch 084:     38 / 196 loss=4.457, nll_loss=2.815, ppl=7.04, wps=19374.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=0.981, loss_scale=8, train_wall=303, gb_free=19.9, wall=53161
2022-03-07 03:33:51 | INFO | train_inner | epoch 084:    138 / 196 loss=4.442, nll_loss=2.796, ppl=6.95, wps=20325.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.98, loss_scale=8, train_wall=298, gb_free=19.9, wall=53484
2022-03-07 03:36:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:37:03 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.179 | nll_loss 9.031 | ppl 523.29 | wps 37585.1 | wpb 510.9 | bsz 1 | num_updates 16358 | best_loss 7.96
2022-03-07 03:37:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16358 updates
2022-03-07 03:37:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:37:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:37:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 84 @ 16358 updates, score 10.179) (writing took 3.4605948999524117 seconds)
2022-03-07 03:37:07 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-07 03:37:07 | INFO | train | epoch 084 | loss 4.444 | nll_loss 2.799 | ppl 6.96 | wps 19929.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 16358 | lr 0.000247249 | gnorm 0.983 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 53679
2022-03-07 03:37:07 | INFO | fairseq.trainer | begin training epoch 85
2022-03-07 03:37:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:39:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:39:25 | INFO | train_inner | epoch 085:     43 / 196 loss=4.446, nll_loss=2.801, ppl=6.97, wps=19570.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=0.979, loss_scale=8, train_wall=300, gb_free=19.9, wall=53818
2022-03-07 03:44:48 | INFO | train_inner | epoch 085:    143 / 196 loss=4.429, nll_loss=2.782, ppl=6.88, wps=20343, ups=0.31, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.001, loss_scale=8, train_wall=298, gb_free=19.9, wall=54140
2022-03-07 03:47:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:47:43 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.241 | nll_loss 9.097 | ppl 547.54 | wps 37277.8 | wpb 510.9 | bsz 1 | num_updates 16553 | best_loss 7.96
2022-03-07 03:47:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16553 updates
2022-03-07 03:47:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:47:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:47:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 85 @ 16553 updates, score 10.241) (writing took 3.5234926054254174 seconds)
2022-03-07 03:47:47 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-07 03:47:47 | INFO | train | epoch 085 | loss 4.432 | nll_loss 2.786 | ppl 6.9 | wps 19931.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 16553 | lr 0.000245789 | gnorm 0.988 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 54319
2022-03-07 03:47:47 | INFO | fairseq.trainer | begin training epoch 86
2022-03-07 03:47:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:50:19 | INFO | train_inner | epoch 086:     47 / 196 loss=4.423, nll_loss=2.775, ppl=6.85, wps=19729.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=0.966, loss_scale=16, train_wall=298, gb_free=19.9, wall=54471
2022-03-07 03:52:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:55:44 | INFO | train_inner | epoch 086:    148 / 196 loss=4.423, nll_loss=2.775, ppl=6.85, wps=20129.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.001, loss_scale=8, train_wall=301, gb_free=19.9, wall=54797
2022-03-07 03:58:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:58:24 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.224 | nll_loss 9.08 | ppl 541.22 | wps 37462.6 | wpb 510.9 | bsz 1 | num_updates 16748 | best_loss 7.96
2022-03-07 03:58:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16748 updates
2022-03-07 03:58:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:58:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 03:58:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 86 @ 16748 updates, score 10.224) (writing took 3.4473664201796055 seconds)
2022-03-07 03:58:28 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-07 03:58:28 | INFO | train | epoch 086 | loss 4.42 | nll_loss 2.772 | ppl 6.83 | wps 19922 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 16748 | lr 0.000244353 | gnorm 0.983 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 54960
2022-03-07 03:58:28 | INFO | fairseq.trainer | begin training epoch 87
2022-03-07 03:58:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:01:16 | INFO | train_inner | epoch 087:     52 / 196 loss=4.412, nll_loss=2.763, ppl=6.79, wps=19741.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=0.972, loss_scale=16, train_wall=297, gb_free=19.9, wall=55128
2022-03-07 04:06:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:06:41 | INFO | train_inner | epoch 087:    153 / 196 loss=4.41, nll_loss=2.761, ppl=6.78, wps=20128.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.998, loss_scale=16, train_wall=301, gb_free=19.9, wall=55453
2022-03-07 04:08:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:09:05 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.222 | nll_loss 9.082 | ppl 542.01 | wps 37538.8 | wpb 510.9 | bsz 1 | num_updates 16943 | best_loss 7.96
2022-03-07 04:09:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16943 updates
2022-03-07 04:09:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:09:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 87 @ 16943 updates, score 10.222) (writing took 3.4939304208382964 seconds)
2022-03-07 04:09:08 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-07 04:09:08 | INFO | train | epoch 087 | loss 4.409 | nll_loss 2.76 | ppl 6.77 | wps 19921.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 16943 | lr 0.000242943 | gnorm 0.989 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 55600
2022-03-07 04:09:08 | INFO | fairseq.trainer | begin training epoch 88
2022-03-07 04:09:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:12:12 | INFO | train_inner | epoch 088:     57 / 196 loss=4.402, nll_loss=2.751, ppl=6.73, wps=19734, ups=0.3, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=0.997, loss_scale=16, train_wall=297, gb_free=19.9, wall=55785
2022-03-07 04:13:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:17:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:17:41 | INFO | train_inner | epoch 088:    159 / 196 loss=4.404, nll_loss=2.754, ppl=6.75, wps=19930.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.996, loss_scale=8, train_wall=304, gb_free=19.9, wall=56113
2022-03-07 04:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:19:45 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.285 | nll_loss 9.153 | ppl 569.19 | wps 37488.5 | wpb 510.9 | bsz 1 | num_updates 17137 | best_loss 7.96
2022-03-07 04:19:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17137 updates
2022-03-07 04:19:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 88 @ 17137 updates, score 10.285) (writing took 3.520077240653336 seconds)
2022-03-07 04:19:49 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-07 04:19:49 | INFO | train | epoch 088 | loss 4.398 | nll_loss 2.746 | ppl 6.71 | wps 19814.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 17137 | lr 0.000241564 | gnorm 0.994 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 56241
2022-03-07 04:19:49 | INFO | fairseq.trainer | begin training epoch 89
2022-03-07 04:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:23:12 | INFO | train_inner | epoch 089:     63 / 196 loss=4.381, nll_loss=2.727, ppl=6.62, wps=19738.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=0.981, loss_scale=8, train_wall=297, gb_free=19.9, wall=56444
2022-03-07 04:28:35 | INFO | train_inner | epoch 089:    163 / 196 loss=4.399, nll_loss=2.747, ppl=6.72, wps=20338, ups=0.31, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.986, loss_scale=16, train_wall=298, gb_free=19.9, wall=56767
2022-03-07 04:30:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:30:26 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.267 | nll_loss 9.131 | ppl 560.7 | wps 37945 | wpb 510.9 | bsz 1 | num_updates 17333 | best_loss 7.96
2022-03-07 04:30:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17333 updates
2022-03-07 04:30:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:30:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:30:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 89 @ 17333 updates, score 10.267) (writing took 3.5163145614787936 seconds)
2022-03-07 04:30:29 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-07 04:30:29 | INFO | train | epoch 089 | loss 4.389 | nll_loss 2.736 | ppl 6.66 | wps 20038.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 17333 | lr 0.000240195 | gnorm 0.988 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 56881
2022-03-07 04:30:29 | INFO | fairseq.trainer | begin training epoch 90
2022-03-07 04:30:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:31:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:34:09 | INFO | train_inner | epoch 090:     68 / 196 loss=4.37, nll_loss=2.715, ppl=6.57, wps=19562.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=0.989, loss_scale=16, train_wall=300, gb_free=19.9, wall=57101
2022-03-07 04:38:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:39:34 | INFO | train_inner | epoch 090:    169 / 196 loss=4.399, nll_loss=2.748, ppl=6.72, wps=20138.9, ups=0.31, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.002, loss_scale=16, train_wall=301, gb_free=19.9, wall=57426
2022-03-07 04:41:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:41:06 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.296 | nll_loss 9.16 | ppl 571.88 | wps 37891.4 | wpb 510.9 | bsz 1 | num_updates 17527 | best_loss 7.96
2022-03-07 04:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17527 updates
2022-03-07 04:41:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:41:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 90 @ 17527 updates, score 10.296) (writing took 3.512823635712266 seconds)
2022-03-07 04:41:10 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-07 04:41:10 | INFO | train | epoch 090 | loss 4.378 | nll_loss 2.723 | ppl 6.6 | wps 19826 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 17527 | lr 0.000238862 | gnorm 0.995 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 57522
2022-03-07 04:41:10 | INFO | fairseq.trainer | begin training epoch 91
2022-03-07 04:41:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:45:05 | INFO | train_inner | epoch 091:     73 / 196 loss=4.348, nll_loss=2.689, ppl=6.45, wps=19746.9, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=0.989, loss_scale=16, train_wall=297, gb_free=19.9, wall=57757
2022-03-07 04:45:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:46:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:50:34 | INFO | train_inner | epoch 091:    175 / 196 loss=4.391, nll_loss=2.739, ppl=6.68, wps=19938.1, ups=0.3, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.012, loss_scale=8, train_wall=304, gb_free=19.9, wall=58086
2022-03-07 04:51:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:51:47 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.32 | nll_loss 9.188 | ppl 583.11 | wps 37397.8 | wpb 510.9 | bsz 1 | num_updates 17721 | best_loss 7.96
2022-03-07 04:51:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17721 updates
2022-03-07 04:51:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:51:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 04:51:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 91 @ 17721 updates, score 10.32) (writing took 3.5022295294329524 seconds)
2022-03-07 04:51:50 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-07 04:51:50 | INFO | train | epoch 091 | loss 4.368 | nll_loss 2.712 | ppl 6.55 | wps 19824.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 17721 | lr 0.00023755 | gnorm 1.002 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 58162
2022-03-07 04:51:50 | INFO | fairseq.trainer | begin training epoch 92
2022-03-07 04:51:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:56:05 | INFO | train_inner | epoch 092:     79 / 196 loss=4.334, nll_loss=2.673, ppl=6.38, wps=19742.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=0.984, loss_scale=16, train_wall=297, gb_free=19.9, wall=58417
2022-03-07 05:00:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:01:30 | INFO | train_inner | epoch 092:    180 / 196 loss=4.387, nll_loss=2.734, ppl=6.65, wps=20125.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.001, loss_scale=16, train_wall=301, gb_free=19.9, wall=58743
2022-03-07 05:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:02:27 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.308 | nll_loss 9.174 | ppl 577.54 | wps 37578.4 | wpb 510.9 | bsz 1 | num_updates 17916 | best_loss 7.96
2022-03-07 05:02:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17916 updates
2022-03-07 05:02:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:02:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 92 @ 17916 updates, score 10.308) (writing took 3.4917463129386306 seconds)
2022-03-07 05:02:31 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-07 05:02:31 | INFO | train | epoch 092 | loss 4.359 | nll_loss 2.701 | ppl 6.5 | wps 19923 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 17916 | lr 0.000236254 | gnorm 0.994 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 58803
2022-03-07 05:02:31 | INFO | fairseq.trainer | begin training epoch 93
2022-03-07 05:02:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:07:02 | INFO | train_inner | epoch 093:     84 / 196 loss=4.322, nll_loss=2.659, ppl=6.32, wps=19738.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=0.996, loss_scale=16, train_wall=297, gb_free=19.9, wall=59074
2022-03-07 05:07:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:12:27 | INFO | train_inner | epoch 093:    185 / 196 loss=4.382, nll_loss=2.729, ppl=6.63, wps=20126.7, ups=0.31, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.013, loss_scale=16, train_wall=301, gb_free=19.9, wall=59399
2022-03-07 05:13:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:13:08 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.317 | nll_loss 9.184 | ppl 581.76 | wps 37876.4 | wpb 510.9 | bsz 1 | num_updates 18111 | best_loss 7.96
2022-03-07 05:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18111 updates
2022-03-07 05:13:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:13:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:13:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 93 @ 18111 updates, score 10.317) (writing took 3.4875357672572136 seconds)
2022-03-07 05:13:11 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-07 05:13:11 | INFO | train | epoch 093 | loss 4.349 | nll_loss 2.691 | ppl 6.46 | wps 19924.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 18111 | lr 0.000234979 | gnorm 1.003 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 59443
2022-03-07 05:13:11 | INFO | fairseq.trainer | begin training epoch 94
2022-03-07 05:13:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:14:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:18:01 | INFO | train_inner | epoch 094:     90 / 196 loss=4.307, nll_loss=2.643, ppl=6.25, wps=19560.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.011, loss_scale=16, train_wall=300, gb_free=19.9, wall=59734
2022-03-07 05:21:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:23:27 | INFO | train_inner | epoch 094:    191 / 196 loss=4.377, nll_loss=2.723, ppl=6.6, wps=20129.8, ups=0.31, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.007, loss_scale=16, train_wall=301, gb_free=19.9, wall=60059
2022-03-07 05:23:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:23:48 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.313 | nll_loss 9.181 | ppl 580.42 | wps 37939 | wpb 510.9 | bsz 1 | num_updates 18305 | best_loss 7.96
2022-03-07 05:23:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18305 updates
2022-03-07 05:23:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:23:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 94 @ 18305 updates, score 10.313) (writing took 3.527139271609485 seconds)
2022-03-07 05:23:52 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-07 05:23:52 | INFO | train | epoch 094 | loss 4.339 | nll_loss 2.679 | ppl 6.4 | wps 19824.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 18305 | lr 0.00023373 | gnorm 1.007 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 60084
2022-03-07 05:23:52 | INFO | fairseq.trainer | begin training epoch 95
2022-03-07 05:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:25:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 05:29:01 | INFO | train_inner | epoch 095:     96 / 196 loss=4.3, nll_loss=2.635, ppl=6.21, wps=19558, ups=0.3, wpb=65367, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=1.007, loss_scale=8, train_wall=300, gb_free=19.9, wall=60393
2022-03-07 05:34:23 | INFO | train_inner | epoch 095:    196 / 196 loss=4.365, nll_loss=2.709, ppl=6.54, wps=20336.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.008, loss_scale=16, train_wall=297, gb_free=19.9, wall=60715
2022-03-07 05:34:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:34:28 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.356 | nll_loss 9.232 | ppl 601.43 | wps 37651.8 | wpb 510.9 | bsz 1 | num_updates 18500 | best_loss 7.96
2022-03-07 05:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18500 updates
2022-03-07 05:34:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:34:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:34:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 95 @ 18500 updates, score 10.356) (writing took 3.5230010403320193 seconds)
2022-03-07 05:34:32 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-07 05:34:32 | INFO | train | epoch 095 | loss 4.331 | nll_loss 2.67 | ppl 6.36 | wps 19929.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 18500 | lr 0.000232495 | gnorm 1.007 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 60724
2022-03-07 05:34:32 | INFO | fairseq.trainer | begin training epoch 96
2022-03-07 05:34:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:39:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:39:58 | INFO | train_inner | epoch 096:    101 / 196 loss=4.284, nll_loss=2.617, ppl=6.13, wps=19559.6, ups=0.3, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.004, loss_scale=16, train_wall=301, gb_free=19.9, wall=61050
2022-03-07 05:45:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:45:09 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.416 | nll_loss 9.301 | ppl 630.9 | wps 37622.7 | wpb 510.9 | bsz 1 | num_updates 18695 | best_loss 7.96
2022-03-07 05:45:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18695 updates
2022-03-07 05:45:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:45:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:45:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 96 @ 18695 updates, score 10.416) (writing took 3.536997625604272 seconds)
2022-03-07 05:45:13 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-07 05:45:13 | INFO | train | epoch 096 | loss 4.322 | nll_loss 2.659 | ppl 6.32 | wps 19925.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 18695 | lr 0.00023128 | gnorm 1.012 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 61365
2022-03-07 05:45:13 | INFO | fairseq.trainer | begin training epoch 97
2022-03-07 05:45:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:45:29 | INFO | train_inner | epoch 097:      5 / 196 loss=4.355, nll_loss=2.697, ppl=6.49, wps=19743, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.022, loss_scale=16, train_wall=297, gb_free=19.9, wall=61381
2022-03-07 05:46:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:50:54 | INFO | train_inner | epoch 097:    106 / 196 loss=4.278, nll_loss=2.61, ppl=6.1, wps=20123.8, ups=0.31, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.015, loss_scale=16, train_wall=301, gb_free=19.9, wall=61707
2022-03-07 05:53:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:53:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 05:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:55:50 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.43 | nll_loss 9.305 | ppl 632.49 | wps 37310.5 | wpb 510.9 | bsz 1 | num_updates 18888 | best_loss 7.96
2022-03-07 05:55:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18888 updates
2022-03-07 05:55:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:55:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 05:55:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 97 @ 18888 updates, score 10.43) (writing took 3.5105511909350753 seconds)
2022-03-07 05:55:53 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-07 05:55:53 | INFO | train | epoch 097 | loss 4.311 | nll_loss 2.647 | ppl 6.27 | wps 19720.6 | ups 0.3 | wpb 65446.6 | bsz 127.8 | num_updates 18888 | lr 0.000230095 | gnorm 1.024 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 62005
2022-03-07 05:55:53 | INFO | fairseq.trainer | begin training epoch 98
2022-03-07 05:55:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:56:32 | INFO | train_inner | epoch 098:     12 / 196 loss=4.338, nll_loss=2.678, ppl=6.4, wps=19373.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.029, loss_scale=8, train_wall=303, gb_free=19.9, wall=62044
2022-03-07 06:01:54 | INFO | train_inner | epoch 098:    112 / 196 loss=4.271, nll_loss=2.602, ppl=6.07, wps=20324.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.008, loss_scale=16, train_wall=298, gb_free=19.9, wall=62366
2022-03-07 06:06:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:06:30 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.42 | nll_loss 9.299 | ppl 629.75 | wps 37908 | wpb 510.9 | bsz 1 | num_updates 19084 | best_loss 7.96
2022-03-07 06:06:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19084 updates
2022-03-07 06:06:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:06:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:06:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 98 @ 19084 updates, score 10.42) (writing took 3.5161749301478267 seconds)
2022-03-07 06:06:34 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-07 06:06:34 | INFO | train | epoch 098 | loss 4.304 | nll_loss 2.639 | ppl 6.23 | wps 20025.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 19084 | lr 0.00022891 | gnorm 1.009 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 62646
2022-03-07 06:06:34 | INFO | fairseq.trainer | begin training epoch 99
2022-03-07 06:06:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:07:25 | INFO | train_inner | epoch 099:     16 / 196 loss=4.336, nll_loss=2.675, ppl=6.39, wps=19743.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.011, loss_scale=16, train_wall=297, gb_free=19.9, wall=62698
2022-03-07 06:08:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:09:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 06:12:54 | INFO | train_inner | epoch 099:    118 / 196 loss=4.267, nll_loss=2.597, ppl=6.05, wps=19939.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.029, loss_scale=8, train_wall=304, gb_free=19.9, wall=63026
2022-03-07 06:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:17:10 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.461 | nll_loss 9.35 | ppl 652.55 | wps 37669.9 | wpb 510.9 | bsz 1 | num_updates 19278 | best_loss 7.96
2022-03-07 06:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19278 updates
2022-03-07 06:17:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:17:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:17:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 99 @ 19278 updates, score 10.461) (writing took 3.5217568827793 seconds)
2022-03-07 06:17:14 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-07 06:17:14 | INFO | train | epoch 099 | loss 4.296 | nll_loss 2.63 | ppl 6.19 | wps 19833 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 19278 | lr 0.000227756 | gnorm 1.024 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 63286
2022-03-07 06:17:14 | INFO | fairseq.trainer | begin training epoch 100
2022-03-07 06:17:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:18:25 | INFO | train_inner | epoch 100:     22 / 196 loss=4.318, nll_loss=2.655, ppl=6.3, wps=19751.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.019, loss_scale=16, train_wall=297, gb_free=19.9, wall=63357
2022-03-07 06:23:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:23:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 06:23:54 | INFO | train_inner | epoch 100:    124 / 196 loss=4.265, nll_loss=2.595, ppl=6.04, wps=19925.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.029, loss_scale=8, train_wall=304, gb_free=19.9, wall=63686
2022-03-07 06:27:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:27:51 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.442 | nll_loss 9.316 | ppl 637.48 | wps 38192.9 | wpb 510.9 | bsz 1 | num_updates 19472 | best_loss 7.96
2022-03-07 06:27:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19472 updates
2022-03-07 06:27:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:27:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:27:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 100 @ 19472 updates, score 10.442) (writing took 3.5471885362640023 seconds)
2022-03-07 06:27:54 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-07 06:27:54 | INFO | train | epoch 100 | loss 4.287 | nll_loss 2.619 | ppl 6.14 | wps 19817.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 19472 | lr 0.000226618 | gnorm 1.031 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 63927
2022-03-07 06:27:54 | INFO | fairseq.trainer | begin training epoch 101
2022-03-07 06:27:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:29:25 | INFO | train_inner | epoch 101:     28 / 196 loss=4.301, nll_loss=2.636, ppl=6.22, wps=19746.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.027, loss_scale=8, train_wall=297, gb_free=19.9, wall=64017
2022-03-07 06:34:47 | INFO | train_inner | epoch 101:    128 / 196 loss=4.264, nll_loss=2.594, ppl=6.04, wps=20311.7, ups=0.31, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.02, loss_scale=16, train_wall=298, gb_free=19.9, wall=64340
2022-03-07 06:37:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:38:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:38:32 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.431 | nll_loss 9.304 | ppl 632.28 | wps 37514.7 | wpb 510.9 | bsz 1 | num_updates 19667 | best_loss 7.96
2022-03-07 06:38:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19667 updates
2022-03-07 06:38:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:38:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:38:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 101 @ 19667 updates, score 10.431) (writing took 3.555515273474157 seconds)
2022-03-07 06:38:35 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-07 06:38:35 | INFO | train | epoch 101 | loss 4.279 | nll_loss 2.61 | ppl 6.11 | wps 19917.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 19667 | lr 0.000225492 | gnorm 1.022 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 64567
2022-03-07 06:38:35 | INFO | fairseq.trainer | begin training epoch 102
2022-03-07 06:38:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:40:22 | INFO | train_inner | epoch 102:     33 / 196 loss=4.284, nll_loss=2.617, ppl=6.13, wps=19561, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.032, loss_scale=16, train_wall=300, gb_free=19.9, wall=64674
2022-03-07 06:44:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:45:47 | INFO | train_inner | epoch 102:    134 / 196 loss=4.26, nll_loss=2.589, ppl=6.02, wps=20124.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.03, loss_scale=16, train_wall=301, gb_free=19.9, wall=64999
2022-03-07 06:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:49:12 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.508 | nll_loss 9.39 | ppl 671.12 | wps 37551.5 | wpb 510.9 | bsz 1 | num_updates 19862 | best_loss 7.96
2022-03-07 06:49:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19862 updates
2022-03-07 06:49:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:49:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:49:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 102 @ 19862 updates, score 10.508) (writing took 3.460906540043652 seconds)
2022-03-07 06:49:16 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-07 06:49:16 | INFO | train | epoch 102 | loss 4.271 | nll_loss 2.601 | ppl 6.07 | wps 19930.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 19862 | lr 0.000224382 | gnorm 1.028 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 65208
2022-03-07 06:49:16 | INFO | fairseq.trainer | begin training epoch 103
2022-03-07 06:49:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:50:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 06:51:21 | INFO | train_inner | epoch 103:     39 / 196 loss=4.281, nll_loss=2.613, ppl=6.12, wps=19566, ups=0.3, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.023, loss_scale=8, train_wall=300, gb_free=19.9, wall=65334
2022-03-07 06:56:44 | INFO | train_inner | epoch 103:    139 / 196 loss=4.257, nll_loss=2.585, ppl=6, wps=20329.6, ups=0.31, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.026, loss_scale=8, train_wall=298, gb_free=19.9, wall=65656
2022-03-07 06:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:59:53 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.499 | nll_loss 9.386 | ppl 668.9 | wps 37356.5 | wpb 510.9 | bsz 1 | num_updates 20057 | best_loss 7.96
2022-03-07 06:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20057 updates
2022-03-07 06:59:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 06:59:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 103 @ 20057 updates, score 10.499) (writing took 3.5047837775200605 seconds)
2022-03-07 06:59:56 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-07 06:59:56 | INFO | train | epoch 103 | loss 4.263 | nll_loss 2.592 | ppl 6.03 | wps 19925.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 20057 | lr 0.000223289 | gnorm 1.032 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 65848
2022-03-07 06:59:56 | INFO | fairseq.trainer | begin training epoch 104
2022-03-07 06:59:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:02:15 | INFO | train_inner | epoch 104:     43 / 196 loss=4.256, nll_loss=2.584, ppl=6, wps=19743, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.038, loss_scale=16, train_wall=297, gb_free=19.9, wall=65987
2022-03-07 07:04:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:07:40 | INFO | train_inner | epoch 104:    144 / 196 loss=4.248, nll_loss=2.575, ppl=5.96, wps=20123.6, ups=0.31, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.013, loss_scale=16, train_wall=301, gb_free=19.9, wall=66313
2022-03-07 07:10:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:10:33 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.486 | nll_loss 9.369 | ppl 661.17 | wps 37443.7 | wpb 510.9 | bsz 1 | num_updates 20252 | best_loss 7.96
2022-03-07 07:10:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20252 updates
2022-03-07 07:10:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:10:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 104 @ 20252 updates, score 10.486) (writing took 3.5877678422257304 seconds)
2022-03-07 07:10:37 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-07 07:10:37 | INFO | train | epoch 104 | loss 4.253 | nll_loss 2.581 | ppl 5.98 | wps 19923.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 20252 | lr 0.000222211 | gnorm 1.021 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 66489
2022-03-07 07:10:37 | INFO | fairseq.trainer | begin training epoch 105
2022-03-07 07:10:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:13:15 | INFO | train_inner | epoch 105:     49 / 196 loss=4.255, nll_loss=2.583, ppl=5.99, wps=19561.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.026, loss_scale=16, train_wall=300, gb_free=19.9, wall=66647
2022-03-07 07:15:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:18:40 | INFO | train_inner | epoch 105:    150 / 196 loss=4.256, nll_loss=2.584, ppl=6, wps=20130.2, ups=0.31, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.038, loss_scale=8, train_wall=301, gb_free=19.9, wall=66972
2022-03-07 07:21:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:21:13 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.495 | nll_loss 9.38 | ppl 666.07 | wps 38031.5 | wpb 510.9 | bsz 1 | num_updates 20446 | best_loss 7.96
2022-03-07 07:21:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20446 updates
2022-03-07 07:21:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:21:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:21:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 105 @ 20446 updates, score 10.495) (writing took 3.5171615136787295 seconds)
2022-03-07 07:21:17 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-07 07:21:17 | INFO | train | epoch 105 | loss 4.247 | nll_loss 2.574 | ppl 5.95 | wps 19833.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 20446 | lr 0.000221155 | gnorm 1.033 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 67129
2022-03-07 07:21:17 | INFO | fairseq.trainer | begin training epoch 106
2022-03-07 07:21:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:24:11 | INFO | train_inner | epoch 106:     54 / 196 loss=4.232, nll_loss=2.557, ppl=5.88, wps=19758.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.038, loss_scale=16, train_wall=297, gb_free=19.9, wall=67303
2022-03-07 07:29:33 | INFO | train_inner | epoch 106:    154 / 196 loss=4.248, nll_loss=2.574, ppl=5.96, wps=20333, ups=0.31, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.025, loss_scale=16, train_wall=298, gb_free=19.9, wall=67625
2022-03-07 07:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:31:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:31:54 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.506 | nll_loss 9.388 | ppl 669.97 | wps 37908.3 | wpb 510.9 | bsz 1 | num_updates 20641 | best_loss 7.96
2022-03-07 07:31:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20641 updates
2022-03-07 07:31:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:31:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 106 @ 20641 updates, score 10.506) (writing took 3.549987227655947 seconds)
2022-03-07 07:31:57 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-07 07:31:57 | INFO | train | epoch 106 | loss 4.24 | nll_loss 2.566 | ppl 5.92 | wps 19928 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 20641 | lr 0.000220107 | gnorm 1.036 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 67769
2022-03-07 07:31:57 | INFO | fairseq.trainer | begin training epoch 107
2022-03-07 07:31:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:35:08 | INFO | train_inner | epoch 107:     59 / 196 loss=4.226, nll_loss=2.55, ppl=5.86, wps=19556.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.04, loss_scale=16, train_wall=300, gb_free=19.9, wall=67960
2022-03-07 07:36:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:40:33 | INFO | train_inner | epoch 107:    160 / 196 loss=4.242, nll_loss=2.568, ppl=5.93, wps=20132.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.029, loss_scale=16, train_wall=301, gb_free=19.9, wall=68285
2022-03-07 07:42:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:42:34 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.571 | nll_loss 9.461 | ppl 704.62 | wps 38429.8 | wpb 510.9 | bsz 1 | num_updates 20836 | best_loss 7.96
2022-03-07 07:42:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20836 updates
2022-03-07 07:42:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:42:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:42:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 107 @ 20836 updates, score 10.571) (writing took 3.539525025524199 seconds)
2022-03-07 07:42:38 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-07 07:42:38 | INFO | train | epoch 107 | loss 4.231 | nll_loss 2.556 | ppl 5.88 | wps 19926.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 20836 | lr 0.000219075 | gnorm 1.03 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 68410
2022-03-07 07:42:38 | INFO | fairseq.trainer | begin training epoch 108
2022-03-07 07:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:43:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:46:07 | INFO | train_inner | epoch 108:     65 / 196 loss=4.214, nll_loss=2.536, ppl=5.8, wps=19551.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.027, loss_scale=16, train_wall=300, gb_free=19.9, wall=68620
2022-03-07 07:47:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:51:33 | INFO | train_inner | epoch 108:    166 / 196 loss=4.243, nll_loss=2.57, ppl=5.94, wps=20131.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.043, loss_scale=8, train_wall=301, gb_free=19.9, wall=68945
2022-03-07 07:53:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:53:15 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.522 | nll_loss 9.405 | ppl 678.15 | wps 37479.6 | wpb 510.9 | bsz 1 | num_updates 21030 | best_loss 7.96
2022-03-07 07:53:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21030 updates
2022-03-07 07:53:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:53:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 07:53:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 108 @ 21030 updates, score 10.522) (writing took 3.6623865785077214 seconds)
2022-03-07 07:53:18 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-07 07:53:18 | INFO | train | epoch 108 | loss 4.225 | nll_loss 2.548 | ppl 5.85 | wps 19816.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 21030 | lr 0.000218062 | gnorm 1.04 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 69051
2022-03-07 07:53:18 | INFO | fairseq.trainer | begin training epoch 109
2022-03-07 07:53:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:57:04 | INFO | train_inner | epoch 109:     70 / 196 loss=4.195, nll_loss=2.515, ppl=5.71, wps=19731.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.026, loss_scale=16, train_wall=297, gb_free=19.9, wall=69276
2022-03-07 08:01:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:02:29 | INFO | train_inner | epoch 109:    171 / 196 loss=4.242, nll_loss=2.568, ppl=5.93, wps=20147.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.054, loss_scale=16, train_wall=301, gb_free=19.9, wall=69602
2022-03-07 08:03:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:03:55 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.524 | nll_loss 9.408 | ppl 679.33 | wps 37454.2 | wpb 510.9 | bsz 1 | num_updates 21225 | best_loss 7.96
2022-03-07 08:03:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21225 updates
2022-03-07 08:03:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:03:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 109 @ 21225 updates, score 10.524) (writing took 3.5146182272583246 seconds)
2022-03-07 08:03:59 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-07 08:03:59 | INFO | train | epoch 109 | loss 4.217 | nll_loss 2.54 | ppl 5.82 | wps 19930.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 21225 | lr 0.000217058 | gnorm 1.036 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 69691
2022-03-07 08:03:59 | INFO | fairseq.trainer | begin training epoch 110
2022-03-07 08:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:08:00 | INFO | train_inner | epoch 110:     75 / 196 loss=4.191, nll_loss=2.509, ppl=5.69, wps=19752.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.035, loss_scale=16, train_wall=297, gb_free=19.9, wall=69933
2022-03-07 08:08:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:12:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:13:29 | INFO | train_inner | epoch 110:    177 / 196 loss=4.237, nll_loss=2.562, ppl=5.91, wps=19950.7, ups=0.3, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.045, loss_scale=8, train_wall=304, gb_free=19.9, wall=70261
2022-03-07 08:14:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:14:35 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.502 | nll_loss 9.382 | ppl 667.38 | wps 37866.9 | wpb 510.9 | bsz 1 | num_updates 21419 | best_loss 7.96
2022-03-07 08:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21419 updates
2022-03-07 08:14:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:14:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:14:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 110 @ 21419 updates, score 10.502) (writing took 3.524554904550314 seconds)
2022-03-07 08:14:39 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-07 08:14:39 | INFO | train | epoch 110 | loss 4.21 | nll_loss 2.532 | ppl 5.78 | wps 19842.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 21419 | lr 0.000216073 | gnorm 1.039 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 70331
2022-03-07 08:14:39 | INFO | fairseq.trainer | begin training epoch 111
2022-03-07 08:14:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:19:00 | INFO | train_inner | epoch 111:     81 / 196 loss=4.178, nll_loss=2.494, ppl=5.63, wps=19750.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.043, loss_scale=8, train_wall=297, gb_free=19.9, wall=70592
2022-03-07 08:23:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:24:25 | INFO | train_inner | epoch 111:    182 / 196 loss=4.235, nll_loss=2.56, ppl=5.9, wps=20131.9, ups=0.31, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.05, loss_scale=8, train_wall=301, gb_free=19.9, wall=70918
2022-03-07 08:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:25:15 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.566 | nll_loss 9.453 | ppl 701.06 | wps 37590.9 | wpb 510.9 | bsz 1 | num_updates 21614 | best_loss 7.96
2022-03-07 08:25:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21614 updates
2022-03-07 08:25:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 111 @ 21614 updates, score 10.566) (writing took 3.591198709793389 seconds)
2022-03-07 08:25:19 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-07 08:25:19 | INFO | train | epoch 111 | loss 4.204 | nll_loss 2.525 | ppl 5.75 | wps 19924.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 21614 | lr 0.000215096 | gnorm 1.044 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 70971
2022-03-07 08:25:19 | INFO | fairseq.trainer | begin training epoch 112
2022-03-07 08:25:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:29:56 | INFO | train_inner | epoch 112:     86 / 196 loss=4.171, nll_loss=2.486, ppl=5.6, wps=19741.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.033, loss_scale=8, train_wall=297, gb_free=19.9, wall=71249
2022-03-07 08:35:19 | INFO | train_inner | epoch 112:    186 / 196 loss=4.227, nll_loss=2.551, ppl=5.86, wps=20336.2, ups=0.31, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.05, loss_scale=16, train_wall=298, gb_free=19.9, wall=71571
2022-03-07 08:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:35:56 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.57 | nll_loss 9.455 | ppl 701.93 | wps 37741.5 | wpb 510.9 | bsz 1 | num_updates 21810 | best_loss 7.96
2022-03-07 08:35:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21810 updates
2022-03-07 08:35:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:35:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 112 @ 21810 updates, score 10.57) (writing took 3.563346919603646 seconds)
2022-03-07 08:35:59 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-07 08:35:59 | INFO | train | epoch 112 | loss 4.198 | nll_loss 2.518 | ppl 5.73 | wps 20032.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 21810 | lr 0.000214127 | gnorm 1.047 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 71612
2022-03-07 08:35:59 | INFO | fairseq.trainer | begin training epoch 113
2022-03-07 08:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:37:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:40:53 | INFO | train_inner | epoch 113:     91 / 196 loss=4.158, nll_loss=2.472, ppl=5.55, wps=19556.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.035, loss_scale=16, train_wall=300, gb_free=19.9, wall=71905
2022-03-07 08:45:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:46:18 | INFO | train_inner | epoch 113:    192 / 196 loss=4.227, nll_loss=2.552, ppl=5.86, wps=20144.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.051, loss_scale=16, train_wall=301, gb_free=19.9, wall=72230
2022-03-07 08:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:46:36 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.556 | nll_loss 9.442 | ppl 695.61 | wps 37737.2 | wpb 510.9 | bsz 1 | num_updates 22004 | best_loss 7.96
2022-03-07 08:46:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22004 updates
2022-03-07 08:46:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:46:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:46:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 113 @ 22004 updates, score 10.556) (writing took 3.6444285847246647 seconds)
2022-03-07 08:46:40 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-07 08:46:40 | INFO | train | epoch 113 | loss 4.19 | nll_loss 2.508 | ppl 5.69 | wps 19827.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 22004 | lr 0.000213181 | gnorm 1.04 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 72252
2022-03-07 08:46:40 | INFO | fairseq.trainer | begin training epoch 114
2022-03-07 08:46:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:47:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:51:53 | INFO | train_inner | epoch 114:     97 / 196 loss=4.142, nll_loss=2.454, ppl=5.48, wps=19550.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.051, loss_scale=8, train_wall=300, gb_free=19.9, wall=72565
2022-03-07 08:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:57:15 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.602 | nll_loss 9.497 | ppl 722.42 | wps 38436.8 | wpb 510.9 | bsz 1 | num_updates 22199 | best_loss 7.96
2022-03-07 08:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22199 updates
2022-03-07 08:57:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:57:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 08:57:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 114 @ 22199 updates, score 10.602) (writing took 3.449693680740893 seconds)
2022-03-07 08:57:19 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-07 08:57:19 | INFO | train | epoch 114 | loss 4.184 | nll_loss 2.502 | ppl 5.66 | wps 19979.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22199 | lr 0.000212243 | gnorm 1.056 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 72891
2022-03-07 08:57:19 | INFO | fairseq.trainer | begin training epoch 115
2022-03-07 08:57:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:57:22 | INFO | train_inner | epoch 115:      1 / 196 loss=4.229, nll_loss=2.554, ppl=5.87, wps=19850.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.06, loss_scale=16, train_wall=296, gb_free=19.9, wall=72894
2022-03-07 09:01:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:02:44 | INFO | train_inner | epoch 115:    102 / 196 loss=4.145, nll_loss=2.457, ppl=5.49, wps=20328.6, ups=0.31, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.043, loss_scale=16, train_wall=299, gb_free=19.9, wall=73216
2022-03-07 09:07:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:07:49 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.655 | nll_loss 9.556 | ppl 752.57 | wps 38659.9 | wpb 510.9 | bsz 1 | num_updates 22394 | best_loss 7.96
2022-03-07 09:07:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22394 updates
2022-03-07 09:07:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:07:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:07:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 115 @ 22394 updates, score 10.655) (writing took 3.53271418903023 seconds)
2022-03-07 09:07:52 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-07 09:07:52 | INFO | train | epoch 115 | loss 4.177 | nll_loss 2.494 | ppl 5.63 | wps 20133.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22394 | lr 0.000211317 | gnorm 1.045 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 73525
2022-03-07 09:07:52 | INFO | fairseq.trainer | begin training epoch 116
2022-03-07 09:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:08:12 | INFO | train_inner | epoch 116:      6 / 196 loss=4.203, nll_loss=2.524, ppl=5.75, wps=19959.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.047, loss_scale=16, train_wall=295, gb_free=19.9, wall=73544
2022-03-07 09:08:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:13:34 | INFO | train_inner | epoch 116:    107 / 196 loss=4.138, nll_loss=2.449, ppl=5.46, wps=20328.5, ups=0.31, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.041, loss_scale=16, train_wall=299, gb_free=19.9, wall=73866
2022-03-07 09:15:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:18:23 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.612 | nll_loss 9.509 | ppl 728.49 | wps 38564.8 | wpb 510.9 | bsz 1 | num_updates 22588 | best_loss 7.96
2022-03-07 09:18:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22588 updates
2022-03-07 09:18:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:18:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:18:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 116 @ 22588 updates, score 10.612) (writing took 3.4920494835823774 seconds)
2022-03-07 09:18:26 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-07 09:18:26 | INFO | train | epoch 116 | loss 4.171 | nll_loss 2.487 | ppl 5.6 | wps 20024.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22588 | lr 0.000210407 | gnorm 1.055 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 74159
2022-03-07 09:18:27 | INFO | fairseq.trainer | begin training epoch 117
2022-03-07 09:18:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:19:05 | INFO | train_inner | epoch 117:     12 / 196 loss=4.199, nll_loss=2.519, ppl=5.73, wps=19762.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.069, loss_scale=16, train_wall=298, gb_free=19.9, wall=74197
2022-03-07 09:22:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:24:27 | INFO | train_inner | epoch 117:    113 / 196 loss=4.141, nll_loss=2.453, ppl=5.47, wps=20340.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.055, loss_scale=16, train_wall=299, gb_free=19.9, wall=74519
2022-03-07 09:28:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:28:57 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.615 | nll_loss 9.512 | ppl 730.26 | wps 38716.3 | wpb 510.9 | bsz 1 | num_updates 22783 | best_loss 7.96
2022-03-07 09:28:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22783 updates
2022-03-07 09:28:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:29:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 117 @ 22783 updates, score 10.615) (writing took 3.373102662153542 seconds)
2022-03-07 09:29:00 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-07 09:29:00 | INFO | train | epoch 117 | loss 4.165 | nll_loss 2.48 | ppl 5.58 | wps 20141.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22783 | lr 0.000209505 | gnorm 1.056 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 74792
2022-03-07 09:29:00 | INFO | fairseq.trainer | begin training epoch 118
2022-03-07 09:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:29:55 | INFO | train_inner | epoch 118:     17 / 196 loss=4.184, nll_loss=2.502, ppl=5.67, wps=19953.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.048, loss_scale=32, train_wall=295, gb_free=19.9, wall=74847
2022-03-07 09:30:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:35:17 | INFO | train_inner | epoch 118:    118 / 196 loss=4.136, nll_loss=2.447, ppl=5.45, wps=20349.5, ups=0.31, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.073, loss_scale=16, train_wall=298, gb_free=19.9, wall=75169
2022-03-07 09:37:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:39:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:39:30 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.618 | nll_loss 9.515 | ppl 731.52 | wps 38683.3 | wpb 510.9 | bsz 1 | num_updates 22977 | best_loss 7.96
2022-03-07 09:39:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22977 updates
2022-03-07 09:39:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:39:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:39:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 118 @ 22977 updates, score 10.618) (writing took 3.4939405685290694 seconds)
2022-03-07 09:39:34 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-07 09:39:34 | INFO | train | epoch 118 | loss 4.158 | nll_loss 2.473 | ppl 5.55 | wps 20030.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22977 | lr 0.000208619 | gnorm 1.059 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 75426
2022-03-07 09:39:34 | INFO | fairseq.trainer | begin training epoch 119
2022-03-07 09:39:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:40:47 | INFO | train_inner | epoch 119:     23 / 196 loss=4.178, nll_loss=2.495, ppl=5.64, wps=19760.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.054, loss_scale=16, train_wall=298, gb_free=19.9, wall=75500
2022-03-07 09:44:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:46:10 | INFO | train_inner | epoch 119:    124 / 196 loss=4.136, nll_loss=2.446, ppl=5.45, wps=20338.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.054, loss_scale=16, train_wall=299, gb_free=19.9, wall=75822
2022-03-07 09:49:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 09:49:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:50:04 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.588 | nll_loss 9.474 | ppl 710.96 | wps 38776.3 | wpb 510.9 | bsz 1 | num_updates 23171 | best_loss 7.96
2022-03-07 09:50:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23171 updates
2022-03-07 09:50:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:50:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 09:50:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 119 @ 23171 updates, score 10.588) (writing took 3.4424639781937003 seconds)
2022-03-07 09:50:08 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-07 09:50:08 | INFO | train | epoch 119 | loss 4.152 | nll_loss 2.466 | ppl 5.52 | wps 20036.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23171 | lr 0.000207744 | gnorm 1.065 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 76060
2022-03-07 09:50:08 | INFO | fairseq.trainer | begin training epoch 120
2022-03-07 09:50:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:51:40 | INFO | train_inner | epoch 120:     29 / 196 loss=4.164, nll_loss=2.48, ppl=5.58, wps=19767, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.077, loss_scale=8, train_wall=298, gb_free=19.9, wall=76153
2022-03-07 09:56:59 | INFO | train_inner | epoch 120:    129 / 196 loss=4.134, nll_loss=2.445, ppl=5.45, wps=20549.7, ups=0.31, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.062, loss_scale=16, train_wall=296, gb_free=19.9, wall=76471
2022-03-07 10:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:00:38 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.658 | nll_loss 9.551 | ppl 750.13 | wps 38604.2 | wpb 510.9 | bsz 1 | num_updates 23367 | best_loss 7.96
2022-03-07 10:00:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23367 updates
2022-03-07 10:00:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:00:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 120 @ 23367 updates, score 10.658) (writing took 4.014591470360756 seconds)
2022-03-07 10:00:42 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-07 10:00:42 | INFO | train | epoch 120 | loss 4.148 | nll_loss 2.461 | ppl 5.5 | wps 20223.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 23367 | lr 0.00020687 | gnorm 1.064 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 76694
2022-03-07 10:00:42 | INFO | fairseq.trainer | begin training epoch 121
2022-03-07 10:00:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:02:28 | INFO | train_inner | epoch 121:     33 / 196 loss=4.156, nll_loss=2.471, ppl=5.54, wps=19860.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.056, loss_scale=16, train_wall=296, gb_free=19.9, wall=76801
2022-03-07 10:03:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:07:54 | INFO | train_inner | epoch 121:    134 / 196 loss=4.13, nll_loss=2.44, ppl=5.43, wps=20137.6, ups=0.31, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.059, loss_scale=16, train_wall=301, gb_free=19.9, wall=77126
2022-03-07 10:10:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:11:18 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.637 | nll_loss 9.538 | ppl 743.22 | wps 37643 | wpb 510.9 | bsz 1 | num_updates 23561 | best_loss 7.96
2022-03-07 10:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23561 updates
2022-03-07 10:11:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:11:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:11:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 121 @ 23561 updates, score 10.637) (writing took 4.197000831365585 seconds)
2022-03-07 10:11:23 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-07 10:11:23 | INFO | train | epoch 121 | loss 4.141 | nll_loss 2.453 | ppl 5.47 | wps 19820.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 23561 | lr 0.000206017 | gnorm 1.057 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 77335
2022-03-07 10:11:23 | INFO | fairseq.trainer | begin training epoch 122
2022-03-07 10:11:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:13:28 | INFO | train_inner | epoch 122:     39 / 196 loss=4.145, nll_loss=2.458, ppl=5.49, wps=19538.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.057, loss_scale=16, train_wall=300, gb_free=19.9, wall=77461
2022-03-07 10:17:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:18:54 | INFO | train_inner | epoch 122:    140 / 196 loss=4.13, nll_loss=2.441, ppl=5.43, wps=20142.8, ups=0.31, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.064, loss_scale=16, train_wall=301, gb_free=19.9, wall=77786
2022-03-07 10:21:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:21:59 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.682 | nll_loss 9.582 | ppl 766.57 | wps 37523.6 | wpb 510.9 | bsz 1 | num_updates 23756 | best_loss 7.96
2022-03-07 10:21:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23756 updates
2022-03-07 10:21:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 122 @ 23756 updates, score 10.682) (writing took 4.110099640674889 seconds)
2022-03-07 10:22:03 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-07 10:22:03 | INFO | train | epoch 122 | loss 4.135 | nll_loss 2.446 | ppl 5.45 | wps 19916.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 23756 | lr 0.00020517 | gnorm 1.058 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 77976
2022-03-07 10:22:03 | INFO | fairseq.trainer | begin training epoch 123
2022-03-07 10:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:24:25 | INFO | train_inner | epoch 123:     44 / 196 loss=4.14, nll_loss=2.452, ppl=5.47, wps=19716.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.052, loss_scale=32, train_wall=297, gb_free=19.9, wall=78117
2022-03-07 10:24:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:26:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 10:29:54 | INFO | train_inner | epoch 123:    146 / 196 loss=4.127, nll_loss=2.437, ppl=5.41, wps=19922.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.06, loss_scale=8, train_wall=304, gb_free=19.9, wall=78446
2022-03-07 10:32:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:32:40 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.689 | nll_loss 9.588 | ppl 769.87 | wps 37667 | wpb 510.9 | bsz 1 | num_updates 23950 | best_loss 7.96
2022-03-07 10:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23950 updates
2022-03-07 10:32:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:32:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:32:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 123 @ 23950 updates, score 10.689) (writing took 3.8691270733252168 seconds)
2022-03-07 10:32:44 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-07 10:32:44 | INFO | train | epoch 123 | loss 4.13 | nll_loss 2.44 | ppl 5.43 | wps 19813.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 23950 | lr 0.000204337 | gnorm 1.063 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 78616
2022-03-07 10:32:44 | INFO | fairseq.trainer | begin training epoch 124
2022-03-07 10:32:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:35:26 | INFO | train_inner | epoch 124:     50 / 196 loss=4.125, nll_loss=2.435, ppl=5.41, wps=19727.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.073, loss_scale=16, train_wall=297, gb_free=19.9, wall=78778
2022-03-07 10:40:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:40:51 | INFO | train_inner | epoch 124:    151 / 196 loss=4.127, nll_loss=2.437, ppl=5.42, wps=20134.9, ups=0.31, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.08, loss_scale=16, train_wall=301, gb_free=19.9, wall=79103
2022-03-07 10:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:43:21 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.676 | nll_loss 9.576 | ppl 763.1 | wps 37720.5 | wpb 510.9 | bsz 1 | num_updates 24145 | best_loss 7.96
2022-03-07 10:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24145 updates
2022-03-07 10:43:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:43:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:43:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 124 @ 24145 updates, score 10.676) (writing took 4.307418980635703 seconds)
2022-03-07 10:43:25 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-07 10:43:25 | INFO | train | epoch 124 | loss 4.124 | nll_loss 2.434 | ppl 5.4 | wps 19903.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 24145 | lr 0.00020351 | gnorm 1.079 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 79258
2022-03-07 10:43:25 | INFO | fairseq.trainer | begin training epoch 125
2022-03-07 10:43:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:46:23 | INFO | train_inner | epoch 125:     55 / 196 loss=4.119, nll_loss=2.428, ppl=5.38, wps=19697, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.069, loss_scale=16, train_wall=297, gb_free=19.9, wall=79435
2022-03-07 10:47:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:51:48 | INFO | train_inner | epoch 125:    156 / 196 loss=4.122, nll_loss=2.431, ppl=5.39, wps=20125, ups=0.31, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.077, loss_scale=16, train_wall=301, gb_free=19.9, wall=79761
2022-03-07 10:53:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:54:02 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.674 | nll_loss 9.572 | ppl 761.2 | wps 37530.8 | wpb 510.9 | bsz 1 | num_updates 24340 | best_loss 7.96
2022-03-07 10:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24340 updates
2022-03-07 10:54:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 10:54:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 125 @ 24340 updates, score 10.674) (writing took 3.836445838212967 seconds)
2022-03-07 10:54:06 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-07 10:54:06 | INFO | train | epoch 125 | loss 4.119 | nll_loss 2.428 | ppl 5.38 | wps 19914.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 24340 | lr 0.000202693 | gnorm 1.074 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 79898
2022-03-07 10:54:06 | INFO | fairseq.trainer | begin training epoch 126
2022-03-07 10:54:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:54:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:57:23 | INFO | train_inner | epoch 126:     61 / 196 loss=4.108, nll_loss=2.416, ppl=5.34, wps=19546.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.067, loss_scale=16, train_wall=300, gb_free=19.9, wall=80095
2022-03-07 11:01:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:02:48 | INFO | train_inner | epoch 126:    162 / 196 loss=4.127, nll_loss=2.437, ppl=5.41, wps=20136, ups=0.31, wpb=65532.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.066, loss_scale=16, train_wall=301, gb_free=19.9, wall=80421
2022-03-07 11:04:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:04:43 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.704 | nll_loss 9.609 | ppl 780.71 | wps 38035.3 | wpb 510.9 | bsz 1 | num_updates 24534 | best_loss 7.96
2022-03-07 11:04:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24534 updates
2022-03-07 11:04:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:04:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:04:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 126 @ 24534 updates, score 10.704) (writing took 4.142257426865399 seconds)
2022-03-07 11:04:47 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-07 11:04:47 | INFO | train | epoch 126 | loss 4.113 | nll_loss 2.421 | ppl 5.36 | wps 19814.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 24534 | lr 0.00020189 | gnorm 1.061 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 80539
2022-03-07 11:04:47 | INFO | fairseq.trainer | begin training epoch 127
2022-03-07 11:04:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:08:20 | INFO | train_inner | epoch 127:     66 / 196 loss=4.095, nll_loss=2.4, ppl=5.28, wps=19708.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.063, loss_scale=16, train_wall=297, gb_free=19.9, wall=80752
2022-03-07 11:09:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:13:46 | INFO | train_inner | epoch 127:    167 / 196 loss=4.123, nll_loss=2.433, ppl=5.4, wps=20127.4, ups=0.31, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.072, loss_scale=16, train_wall=301, gb_free=19.9, wall=81078
2022-03-07 11:15:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:15:24 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.631 | nll_loss 9.527 | ppl 737.62 | wps 37854.3 | wpb 510.9 | bsz 1 | num_updates 24729 | best_loss 7.96
2022-03-07 11:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24729 updates
2022-03-07 11:15:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 127 @ 24729 updates, score 10.631) (writing took 4.240248247049749 seconds)
2022-03-07 11:15:28 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-07 11:15:28 | INFO | train | epoch 127 | loss 4.109 | nll_loss 2.417 | ppl 5.34 | wps 19898.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 24729 | lr 0.000201093 | gnorm 1.07 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 81181
2022-03-07 11:15:28 | INFO | fairseq.trainer | begin training epoch 128
2022-03-07 11:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:16:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:19:21 | INFO | train_inner | epoch 128:     72 / 196 loss=4.087, nll_loss=2.392, ppl=5.25, wps=19505.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.075, loss_scale=16, train_wall=300, gb_free=19.9, wall=81413
2022-03-07 11:23:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:24:46 | INFO | train_inner | epoch 128:    173 / 196 loss=4.124, nll_loss=2.433, ppl=5.4, wps=20131.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.061, loss_scale=16, train_wall=301, gb_free=19.9, wall=81738
2022-03-07 11:26:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:26:05 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.672 | nll_loss 9.572 | ppl 761.34 | wps 37663.2 | wpb 510.9 | bsz 1 | num_updates 24923 | best_loss 7.96
2022-03-07 11:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24923 updates
2022-03-07 11:26:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:26:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 128 @ 24923 updates, score 10.672) (writing took 4.189824872650206 seconds)
2022-03-07 11:26:10 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-07 11:26:10 | INFO | train | epoch 128 | loss 4.102 | nll_loss 2.409 | ppl 5.31 | wps 19801.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 24923 | lr 0.000200309 | gnorm 1.066 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 81822
2022-03-07 11:26:10 | INFO | fairseq.trainer | begin training epoch 129
2022-03-07 11:26:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:30:18 | INFO | train_inner | epoch 129:     77 / 196 loss=4.079, nll_loss=2.382, ppl=5.21, wps=19711.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.077, loss_scale=16, train_wall=297, gb_free=19.9, wall=82070
2022-03-07 11:30:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:35:43 | INFO | train_inner | epoch 129:    178 / 196 loss=4.121, nll_loss=2.43, ppl=5.39, wps=20131.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.067, loss_scale=16, train_wall=301, gb_free=19.9, wall=82396
2022-03-07 11:36:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:36:47 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.729 | nll_loss 9.637 | ppl 796.2 | wps 37682.2 | wpb 510.9 | bsz 1 | num_updates 25118 | best_loss 7.96
2022-03-07 11:36:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25118 updates
2022-03-07 11:36:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 129 @ 25118 updates, score 10.729) (writing took 4.235208968631923 seconds)
2022-03-07 11:36:51 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-07 11:36:51 | INFO | train | epoch 129 | loss 4.098 | nll_loss 2.404 | ppl 5.29 | wps 19904.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 25118 | lr 0.00019953 | gnorm 1.074 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 82463
2022-03-07 11:36:51 | INFO | fairseq.trainer | begin training epoch 130
2022-03-07 11:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:37:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:41:18 | INFO | train_inner | epoch 130:     83 / 196 loss=4.071, nll_loss=2.373, ppl=5.18, wps=19511.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.07, loss_scale=16, train_wall=300, gb_free=19.9, wall=82731
2022-03-07 11:44:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:46:44 | INFO | train_inner | epoch 130:    184 / 196 loss=4.122, nll_loss=2.431, ppl=5.39, wps=20116.4, ups=0.31, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.098, loss_scale=16, train_wall=301, gb_free=19.9, wall=83056
2022-03-07 11:47:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:47:28 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.688 | nll_loss 9.595 | ppl 773.23 | wps 37800.5 | wpb 510.9 | bsz 1 | num_updates 25312 | best_loss 7.96
2022-03-07 11:47:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25312 updates
2022-03-07 11:47:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:47:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:47:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 130 @ 25312 updates, score 10.688) (writing took 4.364093744195998 seconds)
2022-03-07 11:47:32 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-07 11:47:32 | INFO | train | epoch 130 | loss 4.093 | nll_loss 2.399 | ppl 5.27 | wps 19791.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 25312 | lr 0.000198764 | gnorm 1.084 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 83105
2022-03-07 11:47:32 | INFO | fairseq.trainer | begin training epoch 131
2022-03-07 11:47:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:51:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:52:19 | INFO | train_inner | epoch 131:     89 / 196 loss=4.061, nll_loss=2.362, ppl=5.14, wps=19503.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.074, loss_scale=16, train_wall=300, gb_free=19.9, wall=83392
2022-03-07 11:57:41 | INFO | train_inner | epoch 131:    189 / 196 loss=4.119, nll_loss=2.429, ppl=5.38, wps=20344.5, ups=0.31, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.095, loss_scale=16, train_wall=298, gb_free=19.9, wall=83714
2022-03-07 11:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:58:09 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.731 | nll_loss 9.637 | ppl 796.43 | wps 37636.3 | wpb 510.9 | bsz 1 | num_updates 25507 | best_loss 7.96
2022-03-07 11:58:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25507 updates
2022-03-07 11:58:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:58:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 11:58:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 131 @ 25507 updates, score 10.731) (writing took 4.231981532648206 seconds)
2022-03-07 11:58:13 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-07 11:58:13 | INFO | train | epoch 131 | loss 4.088 | nll_loss 2.393 | ppl 5.25 | wps 19908.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 25507 | lr 0.000198002 | gnorm 1.083 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 83746
2022-03-07 11:58:13 | INFO | fairseq.trainer | begin training epoch 132
2022-03-07 11:58:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:58:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:03:16 | INFO | train_inner | epoch 132:     94 / 196 loss=4.05, nll_loss=2.349, ppl=5.1, wps=19519.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.068, loss_scale=16, train_wall=300, gb_free=19.9, wall=84049
2022-03-07 12:05:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:08:42 | INFO | train_inner | epoch 132:    195 / 196 loss=4.119, nll_loss=2.429, ppl=5.38, wps=20142.6, ups=0.31, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=1.095, loss_scale=16, train_wall=301, gb_free=19.9, wall=84374
2022-03-07 12:08:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:08:50 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.739 | nll_loss 9.641 | ppl 798.19 | wps 37342.9 | wpb 510.9 | bsz 1 | num_updates 25701 | best_loss 7.96
2022-03-07 12:08:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25701 updates
2022-03-07 12:08:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 132 @ 25701 updates, score 10.739) (writing took 4.213864238932729 seconds)
2022-03-07 12:08:54 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-07 12:08:54 | INFO | train | epoch 132 | loss 4.082 | nll_loss 2.386 | ppl 5.23 | wps 19810.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 25701 | lr 0.000197254 | gnorm 1.083 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 84386
2022-03-07 12:08:54 | INFO | fairseq.trainer | begin training epoch 133
2022-03-07 12:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:12:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:14:16 | INFO | train_inner | epoch 133:    100 / 196 loss=4.042, nll_loss=2.339, ppl=5.06, wps=19526, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25800, lr=0.000196875, gnorm=1.078, loss_scale=16, train_wall=300, gb_free=19.9, wall=84709
2022-03-07 12:19:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:19:31 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.711 | nll_loss 9.609 | ppl 780.9 | wps 37885.8 | wpb 510.9 | bsz 1 | num_updates 25896 | best_loss 7.96
2022-03-07 12:19:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25896 updates
2022-03-07 12:19:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:19:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 133 @ 25896 updates, score 10.711) (writing took 4.242414938285947 seconds)
2022-03-07 12:19:35 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-07 12:19:35 | INFO | train | epoch 133 | loss 4.077 | nll_loss 2.38 | ppl 5.21 | wps 19915.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 25896 | lr 0.00019651 | gnorm 1.084 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 85027
2022-03-07 12:19:35 | INFO | fairseq.trainer | begin training epoch 134
2022-03-07 12:19:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:19:48 | INFO | train_inner | epoch 134:      4 / 196 loss=4.11, nll_loss=2.418, ppl=5.34, wps=19710.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.09, loss_scale=32, train_wall=297, gb_free=19.9, wall=85040
2022-03-07 12:19:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:25:13 | INFO | train_inner | epoch 134:    105 / 196 loss=4.04, nll_loss=2.337, ppl=5.05, wps=20139.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.089, loss_scale=16, train_wall=301, gb_free=19.9, wall=85366
2022-03-07 12:26:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:30:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:30:12 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.749 | nll_loss 9.655 | ppl 806.37 | wps 37865.1 | wpb 510.9 | bsz 1 | num_updates 26090 | best_loss 7.96
2022-03-07 12:30:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26090 updates
2022-03-07 12:30:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:30:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:30:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 134 @ 26090 updates, score 10.749) (writing took 4.264316408894956 seconds)
2022-03-07 12:30:16 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-07 12:30:16 | INFO | train | epoch 134 | loss 4.072 | nll_loss 2.374 | ppl 5.19 | wps 19816.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 26090 | lr 0.000195778 | gnorm 1.095 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 85668
2022-03-07 12:30:16 | INFO | fairseq.trainer | begin training epoch 135
2022-03-07 12:30:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:30:48 | INFO | train_inner | epoch 135:     10 / 196 loss=4.1, nll_loss=2.407, ppl=5.3, wps=19533.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.1, loss_scale=16, train_wall=300, gb_free=19.9, wall=85700
2022-03-07 12:33:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:36:13 | INFO | train_inner | epoch 135:    111 / 196 loss=4.04, nll_loss=2.338, ppl=5.05, wps=20143.2, ups=0.31, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.086, loss_scale=16, train_wall=301, gb_free=19.9, wall=86026
2022-03-07 12:40:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:40:52 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.718 | nll_loss 9.628 | ppl 791.23 | wps 37491.1 | wpb 510.9 | bsz 1 | num_updates 26285 | best_loss 7.96
2022-03-07 12:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26285 updates
2022-03-07 12:40:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:40:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:40:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 135 @ 26285 updates, score 10.718) (writing took 4.235491191968322 seconds)
2022-03-07 12:40:57 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-07 12:40:57 | INFO | train | epoch 135 | loss 4.068 | nll_loss 2.369 | ppl 5.17 | wps 19912.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 26285 | lr 0.00019505 | gnorm 1.078 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 86309
2022-03-07 12:40:57 | INFO | fairseq.trainer | begin training epoch 136
2022-03-07 12:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:41:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:41:48 | INFO | train_inner | epoch 136:     16 / 196 loss=4.089, nll_loss=2.394, ppl=5.26, wps=19519.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.067, loss_scale=16, train_wall=300, gb_free=19.9, wall=86361
2022-03-07 12:47:10 | INFO | train_inner | epoch 136:    116 / 196 loss=4.04, nll_loss=2.338, ppl=5.06, wps=20342.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.081, loss_scale=16, train_wall=298, gb_free=19.9, wall=86683
2022-03-07 12:47:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:51:33 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.787 | nll_loss 9.706 | ppl 835.42 | wps 37875.8 | wpb 510.9 | bsz 1 | num_updates 26479 | best_loss 7.96
2022-03-07 12:51:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26479 updates
2022-03-07 12:51:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:51:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 12:51:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 136 @ 26479 updates, score 10.787) (writing took 4.216285781934857 seconds)
2022-03-07 12:51:37 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-07 12:51:37 | INFO | train | epoch 136 | loss 4.064 | nll_loss 2.365 | ppl 5.15 | wps 19821.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 26479 | lr 0.000194334 | gnorm 1.087 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 86950
2022-03-07 12:51:37 | INFO | fairseq.trainer | begin training epoch 137
2022-03-07 12:51:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:52:45 | INFO | train_inner | epoch 137:     21 / 196 loss=4.085, nll_loss=2.39, ppl=5.24, wps=19533, ups=0.3, wpb=65367, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.1, loss_scale=16, train_wall=300, gb_free=19.9, wall=87017
2022-03-07 12:55:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:58:11 | INFO | train_inner | epoch 137:    122 / 196 loss=4.037, nll_loss=2.335, ppl=5.05, wps=20132.3, ups=0.31, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.081, loss_scale=16, train_wall=301, gb_free=19.9, wall=87343
2022-03-07 13:01:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:02:11 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.755 | nll_loss 9.662 | ppl 810.34 | wps 39708.8 | wpb 510.9 | bsz 1 | num_updates 26673 | best_loss 7.96
2022-03-07 13:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26673 updates
2022-03-07 13:02:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:02:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:02:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 137 @ 26673 updates, score 10.755) (writing took 4.135191625915468 seconds)
2022-03-07 13:02:15 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-07 13:02:15 | INFO | train | epoch 137 | loss 4.059 | nll_loss 2.36 | ppl 5.13 | wps 19910.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 26673 | lr 0.000193626 | gnorm 1.089 | loss_scale 16 | train_wall 581 | gb_free 19.9 | wall 87587
2022-03-07 13:02:15 | INFO | fairseq.trainer | begin training epoch 138
2022-03-07 13:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:03:40 | INFO | train_inner | epoch 138:     27 / 196 loss=4.075, nll_loss=2.379, ppl=5.2, wps=19829.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.086, loss_scale=16, train_wall=296, gb_free=19.9, wall=87672
2022-03-07 13:08:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:08:59 | INFO | train_inner | epoch 138:    128 / 196 loss=4.04, nll_loss=2.338, ppl=5.05, wps=20549.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.083, loss_scale=16, train_wall=296, gb_free=19.9, wall=87991
2022-03-07 13:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:12:39 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.837 | nll_loss 9.748 | ppl 860.08 | wps 39163.4 | wpb 510.9 | bsz 1 | num_updates 26868 | best_loss 7.96
2022-03-07 13:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26868 updates
2022-03-07 13:12:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:12:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:12:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 138 @ 26868 updates, score 10.837) (writing took 3.4526385217905045 seconds)
2022-03-07 13:12:42 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-07 13:12:42 | INFO | train | epoch 138 | loss 4.056 | nll_loss 2.356 | ppl 5.12 | wps 20346.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26868 | lr 0.000192922 | gnorm 1.086 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 88214
2022-03-07 13:12:42 | INFO | fairseq.trainer | begin training epoch 139
2022-03-07 13:12:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:14:23 | INFO | train_inner | epoch 139:     32 / 196 loss=4.067, nll_loss=2.369, ppl=5.17, wps=20168.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.087, loss_scale=16, train_wall=292, gb_free=19.9, wall=88315
2022-03-07 13:15:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:19:42 | INFO | train_inner | epoch 139:    133 / 196 loss=4.04, nll_loss=2.338, ppl=5.06, wps=20565.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.115, loss_scale=16, train_wall=296, gb_free=19.9, wall=88634
2022-03-07 13:22:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:23:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:23:05 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.741 | nll_loss 9.644 | ppl 799.89 | wps 39303.8 | wpb 510.9 | bsz 1 | num_updates 27062 | best_loss 7.96
2022-03-07 13:23:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27062 updates
2022-03-07 13:23:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:23:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:23:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 139 @ 27062 updates, score 10.741) (writing took 3.4212872348725796 seconds)
2022-03-07 13:23:09 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-07 13:23:09 | INFO | train | epoch 139 | loss 4.051 | nll_loss 2.351 | ppl 5.1 | wps 20260.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27062 | lr 0.00019223 | gnorm 1.101 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 88841
2022-03-07 13:23:09 | INFO | fairseq.trainer | begin training epoch 140
2022-03-07 13:23:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:25:09 | INFO | train_inner | epoch 140:     38 / 196 loss=4.056, nll_loss=2.356, ppl=5.12, wps=19983.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.094, loss_scale=16, train_wall=295, gb_free=19.9, wall=88961
2022-03-07 13:29:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:30:28 | INFO | train_inner | epoch 140:    139 / 196 loss=4.042, nll_loss=2.341, ppl=5.07, wps=20559.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.096, loss_scale=16, train_wall=296, gb_free=19.9, wall=89280
2022-03-07 13:33:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:33:32 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.779 | nll_loss 9.691 | ppl 826.63 | wps 39541.5 | wpb 510.9 | bsz 1 | num_updates 27257 | best_loss 7.96
2022-03-07 13:33:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27257 updates
2022-03-07 13:33:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:33:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:33:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 140 @ 27257 updates, score 10.779) (writing took 3.4194965194910765 seconds)
2022-03-07 13:33:36 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-07 13:33:36 | INFO | train | epoch 140 | loss 4.046 | nll_loss 2.345 | ppl 5.08 | wps 20356.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27257 | lr 0.000191541 | gnorm 1.101 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 89468
2022-03-07 13:33:36 | INFO | fairseq.trainer | begin training epoch 141
2022-03-07 13:33:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:35:52 | INFO | train_inner | epoch 141:     43 / 196 loss=4.047, nll_loss=2.346, ppl=5.08, wps=20171.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.108, loss_scale=16, train_wall=292, gb_free=19.9, wall=89604
2022-03-07 13:36:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:41:11 | INFO | train_inner | epoch 141:    144 / 196 loss=4.041, nll_loss=2.339, ppl=5.06, wps=20554.6, ups=0.31, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.098, loss_scale=16, train_wall=296, gb_free=19.9, wall=89923
2022-03-07 13:43:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:43:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:44:00 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.782 | nll_loss 9.69 | ppl 826.21 | wps 39757.4 | wpb 510.9 | bsz 1 | num_updates 27451 | best_loss 7.96
2022-03-07 13:44:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27451 updates
2022-03-07 13:44:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:44:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:44:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 141 @ 27451 updates, score 10.782) (writing took 3.362599334679544 seconds)
2022-03-07 13:44:03 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-07 13:44:03 | INFO | train | epoch 141 | loss 4.042 | nll_loss 2.34 | ppl 5.06 | wps 20245.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27451 | lr 0.000190863 | gnorm 1.096 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 90095
2022-03-07 13:44:03 | INFO | fairseq.trainer | begin training epoch 142
2022-03-07 13:44:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:46:38 | INFO | train_inner | epoch 142:     49 / 196 loss=4.038, nll_loss=2.337, ppl=5.05, wps=19968.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.089, loss_scale=16, train_wall=295, gb_free=19.9, wall=90250
2022-03-07 13:50:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:51:53 | INFO | train_inner | epoch 142:    150 / 196 loss=4.041, nll_loss=2.339, ppl=5.06, wps=20777.5, ups=0.32, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.098, loss_scale=16, train_wall=293, gb_free=19.9, wall=90566
2022-03-07 13:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:54:21 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.782 | nll_loss 9.696 | ppl 829.52 | wps 41712.9 | wpb 510.9 | bsz 1 | num_updates 27646 | best_loss 7.96
2022-03-07 13:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27646 updates
2022-03-07 13:54:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:54:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 13:54:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 142 @ 27646 updates, score 10.782) (writing took 3.3780168695375323 seconds)
2022-03-07 13:54:24 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-07 13:54:24 | INFO | train | epoch 142 | loss 4.037 | nll_loss 2.335 | ppl 5.04 | wps 20547.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27646 | lr 0.000190188 | gnorm 1.091 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 90716
2022-03-07 13:54:24 | INFO | fairseq.trainer | begin training epoch 143
2022-03-07 13:54:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:57:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:57:15 | INFO | train_inner | epoch 143:     55 / 196 loss=4.029, nll_loss=2.326, ppl=5.01, wps=20322.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.096, loss_scale=16, train_wall=291, gb_free=19.9, wall=90887
2022-03-07 14:02:26 | INFO | train_inner | epoch 143:    155 / 196 loss=4.036, nll_loss=2.333, ppl=5.04, wps=21108.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.089, loss_scale=16, train_wall=288, gb_free=19.9, wall=91198
2022-03-07 14:03:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:04:37 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.842 | nll_loss 9.757 | ppl 865.11 | wps 41318.5 | wpb 510.9 | bsz 1 | num_updates 27840 | best_loss 7.96
2022-03-07 14:04:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27840 updates
2022-03-07 14:04:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:04:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:04:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 143 @ 27840 updates, score 10.842) (writing took 3.684527433477342 seconds)
2022-03-07 14:04:41 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-07 14:04:41 | INFO | train | epoch 143 | loss 4.033 | nll_loss 2.33 | ppl 5.03 | wps 20578.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27840 | lr 0.000189525 | gnorm 1.103 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 91333
2022-03-07 14:04:41 | INFO | fairseq.trainer | begin training epoch 144
2022-03-07 14:04:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:07:47 | INFO | train_inner | epoch 144:     60 / 196 loss=4.03, nll_loss=2.326, ppl=5.02, wps=20303.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.119, loss_scale=16, train_wall=291, gb_free=19.9, wall=91520
2022-03-07 14:10:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:13:01 | INFO | train_inner | epoch 144:    161 / 196 loss=4.036, nll_loss=2.334, ppl=5.04, wps=20904.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.096, loss_scale=16, train_wall=291, gb_free=19.9, wall=91833
2022-03-07 14:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:14:54 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.834 | nll_loss 9.747 | ppl 859.51 | wps 41703.3 | wpb 510.9 | bsz 1 | num_updates 28035 | best_loss 7.96
2022-03-07 14:14:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28035 updates
2022-03-07 14:14:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:14:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 144 @ 28035 updates, score 10.834) (writing took 3.6755461413413286 seconds)
2022-03-07 14:14:58 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-07 14:14:58 | INFO | train | epoch 144 | loss 4.029 | nll_loss 2.326 | ppl 5.01 | wps 20691.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28035 | lr 0.000188864 | gnorm 1.103 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 91950
2022-03-07 14:14:58 | INFO | fairseq.trainer | begin training epoch 145
2022-03-07 14:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:17:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:18:23 | INFO | train_inner | epoch 145:     66 / 196 loss=4.017, nll_loss=2.312, ppl=4.96, wps=20308.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.094, loss_scale=16, train_wall=291, gb_free=19.9, wall=92155
2022-03-07 14:23:33 | INFO | train_inner | epoch 145:    166 / 196 loss=4.037, nll_loss=2.336, ppl=5.05, wps=21113.4, ups=0.32, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.098, loss_scale=16, train_wall=288, gb_free=19.9, wall=92465
2022-03-07 14:24:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:25:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:25:11 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.748 | nll_loss 9.65 | ppl 803.65 | wps 41737.3 | wpb 510.9 | bsz 1 | num_updates 28229 | best_loss 7.96
2022-03-07 14:25:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28229 updates
2022-03-07 14:25:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:25:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:25:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 145 @ 28229 updates, score 10.748) (writing took 3.6628025211393833 seconds)
2022-03-07 14:25:14 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-07 14:25:14 | INFO | train | epoch 145 | loss 4.024 | nll_loss 2.32 | ppl 4.99 | wps 20591 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28229 | lr 0.000188214 | gnorm 1.097 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 92567
2022-03-07 14:25:15 | INFO | fairseq.trainer | begin training epoch 146
2022-03-07 14:25:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:28:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 14:28:58 | INFO | train_inner | epoch 146:     72 / 196 loss=4.008, nll_loss=2.302, ppl=4.93, wps=20113.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.106, loss_scale=8, train_wall=294, gb_free=19.9, wall=92790
2022-03-07 14:34:08 | INFO | train_inner | epoch 146:    172 / 196 loss=4.038, nll_loss=2.336, ppl=5.05, wps=21120.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.114, loss_scale=8, train_wall=288, gb_free=19.9, wall=93101
2022-03-07 14:35:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:35:27 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.83 | nll_loss 9.748 | ppl 859.91 | wps 41537.3 | wpb 510.9 | bsz 1 | num_updates 28424 | best_loss 7.96
2022-03-07 14:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28424 updates
2022-03-07 14:35:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 146 @ 28424 updates, score 10.83) (writing took 3.661746459081769 seconds)
2022-03-07 14:35:31 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-07 14:35:31 | INFO | train | epoch 146 | loss 4.022 | nll_loss 2.317 | ppl 4.98 | wps 20695.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28424 | lr 0.000187567 | gnorm 1.111 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 93183
2022-03-07 14:35:31 | INFO | fairseq.trainer | begin training epoch 147
2022-03-07 14:35:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:39:27 | INFO | train_inner | epoch 147:     76 / 196 loss=3.998, nll_loss=2.291, ppl=4.89, wps=20506.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.101, loss_scale=16, train_wall=288, gb_free=19.9, wall=93419
2022-03-07 14:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:44:41 | INFO | train_inner | epoch 147:    177 / 196 loss=4.039, nll_loss=2.337, ppl=5.05, wps=20902.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.113, loss_scale=16, train_wall=291, gb_free=19.9, wall=93733
2022-03-07 14:45:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:45:44 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.845 | nll_loss 9.767 | ppl 871.29 | wps 41772.2 | wpb 510.9 | bsz 1 | num_updates 28619 | best_loss 7.96
2022-03-07 14:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28619 updates
2022-03-07 14:45:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:45:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 147 @ 28619 updates, score 10.845) (writing took 3.6679339837282896 seconds)
2022-03-07 14:45:48 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-07 14:45:48 | INFO | train | epoch 147 | loss 4.017 | nll_loss 2.312 | ppl 4.97 | wps 20691 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28619 | lr 0.000186927 | gnorm 1.102 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 93800
2022-03-07 14:45:48 | INFO | fairseq.trainer | begin training epoch 148
2022-03-07 14:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:49:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:50:03 | INFO | train_inner | epoch 148:     82 / 196 loss=3.995, nll_loss=2.287, ppl=4.88, wps=20305, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28700, lr=0.000186663, gnorm=1.091, loss_scale=16, train_wall=291, gb_free=19.9, wall=94055
2022-03-07 14:55:13 | INFO | train_inner | epoch 148:    182 / 196 loss=4.038, nll_loss=2.337, ppl=5.05, wps=21117.4, ups=0.32, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=1.099, loss_scale=16, train_wall=288, gb_free=19.9, wall=94365
2022-03-07 14:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:55:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:56:01 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.805 | nll_loss 9.718 | ppl 842.37 | wps 41939.7 | wpb 510.9 | bsz 1 | num_updates 28813 | best_loss 7.96
2022-03-07 14:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 28813 updates
2022-03-07 14:56:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:56:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 14:56:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 148 @ 28813 updates, score 10.805) (writing took 3.658492723479867 seconds)
2022-03-07 14:56:05 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-07 14:56:05 | INFO | train | epoch 148 | loss 4.013 | nll_loss 2.307 | ppl 4.95 | wps 20590.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28813 | lr 0.000186297 | gnorm 1.097 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 94417
2022-03-07 14:56:05 | INFO | fairseq.trainer | begin training epoch 149
2022-03-07 14:56:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:00:35 | INFO | train_inner | epoch 149:     87 / 196 loss=3.975, nll_loss=2.264, ppl=4.8, wps=20320.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28900, lr=0.000186016, gnorm=1.104, loss_scale=16, train_wall=291, gb_free=19.9, wall=94687
2022-03-07 15:02:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:05:48 | INFO | train_inner | epoch 149:    188 / 196 loss=4.045, nll_loss=2.344, ppl=5.08, wps=20907, ups=0.32, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=1.102, loss_scale=16, train_wall=291, gb_free=19.9, wall=95000
2022-03-07 15:06:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:06:17 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.795 | nll_loss 9.709 | ppl 837.11 | wps 41772.1 | wpb 510.9 | bsz 1 | num_updates 29008 | best_loss 7.96
2022-03-07 15:06:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 29008 updates
2022-03-07 15:06:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:06:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:06:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 149 @ 29008 updates, score 10.795) (writing took 3.681695295497775 seconds)
2022-03-07 15:06:21 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-07 15:06:21 | INFO | train | epoch 149 | loss 4.009 | nll_loss 2.303 | ppl 4.93 | wps 20697.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29008 | lr 0.00018567 | gnorm 1.101 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 95033
2022-03-07 15:06:21 | INFO | fairseq.trainer | begin training epoch 150
2022-03-07 15:06:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:09:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:11:10 | INFO | train_inner | epoch 150:     93 / 196 loss=3.978, nll_loss=2.267, ppl=4.81, wps=20312.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=29100, lr=0.000185376, gnorm=1.085, loss_scale=16, train_wall=291, gb_free=19.9, wall=95322
2022-03-07 15:16:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:16:23 | INFO | train_inner | epoch 150:    194 / 196 loss=4.036, nll_loss=2.335, ppl=5.04, wps=20910.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=29200, lr=0.000185058, gnorm=1.113, loss_scale=16, train_wall=291, gb_free=19.9, wall=95636
2022-03-07 15:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:16:34 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.843 | nll_loss 9.76 | ppl 867.14 | wps 41601.4 | wpb 510.9 | bsz 1 | num_updates 29202 | best_loss 7.96
2022-03-07 15:16:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 29202 updates
2022-03-07 15:16:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:16:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:16:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 150 @ 29202 updates, score 10.843) (writing took 3.654438844881952 seconds)
2022-03-07 15:16:38 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-07 15:16:38 | INFO | train | epoch 150 | loss 4.005 | nll_loss 2.299 | ppl 4.92 | wps 20593.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29202 | lr 0.000185052 | gnorm 1.1 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 95650
2022-03-07 15:16:38 | INFO | fairseq.trainer | begin training epoch 151
2022-03-07 15:16:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:21:42 | INFO | train_inner | epoch 151:     98 / 196 loss=3.963, nll_loss=2.25, ppl=4.76, wps=20512.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29300, lr=0.000184742, gnorm=1.076, loss_scale=16, train_wall=288, gb_free=19.9, wall=95954
2022-03-07 15:22:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:26:51 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.801 | nll_loss 9.72 | ppl 843.63 | wps 41319.9 | wpb 510.9 | bsz 1 | num_updates 29397 | best_loss 7.96
2022-03-07 15:26:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 29397 updates
2022-03-07 15:26:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:26:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 151 @ 29397 updates, score 10.801) (writing took 3.6184184215962887 seconds)
2022-03-07 15:26:54 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-07 15:26:54 | INFO | train | epoch 151 | loss 4.002 | nll_loss 2.295 | ppl 4.91 | wps 20702.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29397 | lr 0.000184437 | gnorm 1.098 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 96266
2022-03-07 15:26:54 | INFO | fairseq.trainer | begin training epoch 152
2022-03-07 15:26:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:27:04 | INFO | train_inner | epoch 152:      3 / 196 loss=4.04, nll_loss=2.339, ppl=5.06, wps=20325.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=29400, lr=0.000184428, gnorm=1.122, loss_scale=16, train_wall=290, gb_free=19.9, wall=96276
2022-03-07 15:29:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:32:17 | INFO | train_inner | epoch 152:    104 / 196 loss=3.959, nll_loss=2.246, ppl=4.74, wps=20920, ups=0.32, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=1.103, loss_scale=16, train_wall=291, gb_free=19.9, wall=96589
2022-03-07 15:36:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:37:07 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.88 | nll_loss 9.801 | ppl 892.2 | wps 41594.2 | wpb 510.9 | bsz 1 | num_updates 29591 | best_loss 7.96
2022-03-07 15:37:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 29591 updates
2022-03-07 15:37:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:37:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:37:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 152 @ 29591 updates, score 10.88) (writing took 3.6375805912539363 seconds)
2022-03-07 15:37:10 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-07 15:37:10 | INFO | train | epoch 152 | loss 3.996 | nll_loss 2.289 | ppl 4.89 | wps 20606.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29591 | lr 0.000183832 | gnorm 1.106 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 96883
2022-03-07 15:37:10 | INFO | fairseq.trainer | begin training epoch 153
2022-03-07 15:37:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:37:38 | INFO | train_inner | epoch 153:      9 / 196 loss=4.028, nll_loss=2.325, ppl=5.01, wps=20329.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29600, lr=0.000183804, gnorm=1.109, loss_scale=16, train_wall=290, gb_free=19.9, wall=96911
2022-03-07 15:42:49 | INFO | train_inner | epoch 153:    109 / 196 loss=3.968, nll_loss=2.256, ppl=4.78, wps=21113.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=29700, lr=0.000183494, gnorm=1.1, loss_scale=16, train_wall=288, gb_free=19.9, wall=97221
2022-03-07 15:43:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:47:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:47:23 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.871 | nll_loss 9.793 | ppl 887.39 | wps 41746.5 | wpb 510.9 | bsz 1 | num_updates 29786 | best_loss 7.96
2022-03-07 15:47:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 29786 updates
2022-03-07 15:47:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:47:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:47:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 153 @ 29786 updates, score 10.871) (writing took 3.6352337021380663 seconds)
2022-03-07 15:47:27 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-07 15:47:27 | INFO | train | epoch 153 | loss 3.994 | nll_loss 2.286 | ppl 4.88 | wps 20703.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29786 | lr 0.000183229 | gnorm 1.104 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 97499
2022-03-07 15:47:27 | INFO | fairseq.trainer | begin training epoch 154
2022-03-07 15:47:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:48:10 | INFO | train_inner | epoch 154:     14 / 196 loss=4.016, nll_loss=2.311, ppl=4.96, wps=20329.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=29800, lr=0.000183186, gnorm=1.108, loss_scale=16, train_wall=290, gb_free=19.9, wall=97542
2022-03-07 15:50:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:53:24 | INFO | train_inner | epoch 154:    115 / 196 loss=3.963, nll_loss=2.251, ppl=4.76, wps=20915.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=29900, lr=0.000182879, gnorm=1.1, loss_scale=16, train_wall=291, gb_free=19.9, wall=97856
2022-03-07 15:57:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:57:39 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.842 | nll_loss 9.762 | ppl 868.08 | wps 41317.6 | wpb 510.9 | bsz 1 | num_updates 29980 | best_loss 7.96
2022-03-07 15:57:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 29980 updates
2022-03-07 15:57:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:57:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 15:57:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 154 @ 29980 updates, score 10.842) (writing took 3.669399708509445 seconds)
2022-03-07 15:57:43 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-07 15:57:43 | INFO | train | epoch 154 | loss 3.989 | nll_loss 2.281 | ppl 4.86 | wps 20600.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29980 | lr 0.000182635 | gnorm 1.102 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 98115
2022-03-07 15:57:43 | INFO | fairseq.trainer | begin training epoch 155
2022-03-07 15:57:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:58:45 | INFO | train_inner | epoch 155:     20 / 196 loss=4.012, nll_loss=2.307, ppl=4.95, wps=20320.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=30000, lr=0.000182574, gnorm=1.11, loss_scale=16, train_wall=290, gb_free=19.9, wall=98178
2022-03-07 16:03:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:03:59 | INFO | train_inner | epoch 155:    121 / 196 loss=3.969, nll_loss=2.257, ppl=4.78, wps=20920.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=30100, lr=0.000182271, gnorm=1.101, loss_scale=16, train_wall=291, gb_free=19.9, wall=98491
2022-03-07 16:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:07:56 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.849 | nll_loss 9.771 | ppl 873.66 | wps 41512.9 | wpb 510.9 | bsz 1 | num_updates 30175 | best_loss 7.96
2022-03-07 16:07:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 30175 updates
2022-03-07 16:07:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 155 @ 30175 updates, score 10.849) (writing took 3.618148065172136 seconds)
2022-03-07 16:07:59 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-07 16:07:59 | INFO | train | epoch 155 | loss 3.986 | nll_loss 2.277 | ppl 4.85 | wps 20708.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30175 | lr 0.000182044 | gnorm 1.111 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 98732
2022-03-07 16:07:59 | INFO | fairseq.trainer | begin training epoch 156
2022-03-07 16:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:09:17 | INFO | train_inner | epoch 156:     25 / 196 loss=4.001, nll_loss=2.295, ppl=4.91, wps=20525.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=30200, lr=0.000181969, gnorm=1.11, loss_scale=16, train_wall=287, gb_free=19.9, wall=98809
2022-03-07 16:10:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:14:30 | INFO | train_inner | epoch 156:    126 / 196 loss=3.968, nll_loss=2.256, ppl=4.78, wps=20914.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=30300, lr=0.000181668, gnorm=1.102, loss_scale=16, train_wall=291, gb_free=19.9, wall=99123
2022-03-07 16:17:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:18:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:18:12 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.856 | nll_loss 9.776 | ppl 876.46 | wps 41321 | wpb 510.9 | bsz 1 | num_updates 30369 | best_loss 7.96
2022-03-07 16:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 30369 updates
2022-03-07 16:18:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:18:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:18:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 156 @ 30369 updates, score 10.856) (writing took 3.6319097774103284 seconds)
2022-03-07 16:18:16 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-07 16:18:16 | INFO | train | epoch 156 | loss 3.983 | nll_loss 2.273 | ppl 4.83 | wps 20598.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30369 | lr 0.000181462 | gnorm 1.114 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 99348
2022-03-07 16:18:16 | INFO | fairseq.trainer | begin training epoch 157
2022-03-07 16:18:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:19:52 | INFO | train_inner | epoch 157:     31 / 196 loss=3.991, nll_loss=2.283, ppl=4.87, wps=20316.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=30400, lr=0.000181369, gnorm=1.127, loss_scale=16, train_wall=291, gb_free=19.9, wall=99444
2022-03-07 16:24:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:25:06 | INFO | train_inner | epoch 157:    132 / 196 loss=3.965, nll_loss=2.253, ppl=4.77, wps=20907.2, ups=0.32, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=1.095, loss_scale=16, train_wall=291, gb_free=19.9, wall=99758
2022-03-07 16:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:28:29 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.855 | nll_loss 9.776 | ppl 876.88 | wps 41933.9 | wpb 510.9 | bsz 1 | num_updates 30564 | best_loss 7.96
2022-03-07 16:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 30564 updates
2022-03-07 16:28:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:28:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 157 @ 30564 updates, score 10.855) (writing took 3.602139580063522 seconds)
2022-03-07 16:28:32 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-07 16:28:32 | INFO | train | epoch 157 | loss 3.979 | nll_loss 2.269 | ppl 4.82 | wps 20699.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30564 | lr 0.000180882 | gnorm 1.106 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 99965
2022-03-07 16:28:32 | INFO | fairseq.trainer | begin training epoch 158
2022-03-07 16:28:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:30:24 | INFO | train_inner | epoch 158:     36 / 196 loss=3.989, nll_loss=2.281, ppl=4.86, wps=20512.2, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=30600, lr=0.000180775, gnorm=1.125, loss_scale=16, train_wall=288, gb_free=19.9, wall=100076
2022-03-07 16:31:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:35:38 | INFO | train_inner | epoch 158:    137 / 196 loss=3.969, nll_loss=2.258, ppl=4.78, wps=20901.7, ups=0.32, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=1.117, loss_scale=16, train_wall=291, gb_free=19.9, wall=100390
2022-03-07 16:37:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:38:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:38:45 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.915 | nll_loss 9.846 | ppl 920.41 | wps 41831.8 | wpb 510.9 | bsz 1 | num_updates 30758 | best_loss 7.96
2022-03-07 16:38:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 30758 updates
2022-03-07 16:38:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:38:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:38:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 158 @ 30758 updates, score 10.915) (writing took 3.6006409022957087 seconds)
2022-03-07 16:38:49 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-07 16:38:49 | INFO | train | epoch 158 | loss 3.975 | nll_loss 2.265 | ppl 4.81 | wps 20590.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30758 | lr 0.00018031 | gnorm 1.124 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 100581
2022-03-07 16:38:49 | INFO | fairseq.trainer | begin training epoch 159
2022-03-07 16:38:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:40:59 | INFO | train_inner | epoch 159:     42 / 196 loss=3.973, nll_loss=2.263, ppl=4.8, wps=20317, ups=0.31, wpb=65367, bsz=127.7, num_updates=30800, lr=0.000180187, gnorm=1.115, loss_scale=16, train_wall=291, gb_free=19.9, wall=100712
2022-03-07 16:44:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:46:13 | INFO | train_inner | epoch 159:    143 / 196 loss=3.972, nll_loss=2.261, ppl=4.79, wps=20906.3, ups=0.32, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=1.102, loss_scale=16, train_wall=291, gb_free=19.9, wall=101025
2022-03-07 16:48:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:49:02 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.905 | nll_loss 9.832 | ppl 911.36 | wps 41820.4 | wpb 510.9 | bsz 1 | num_updates 30953 | best_loss 7.96
2022-03-07 16:49:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 30953 updates
2022-03-07 16:49:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:49:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:49:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 159 @ 30953 updates, score 10.905) (writing took 3.6326583260670304 seconds)
2022-03-07 16:49:06 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-07 16:49:06 | INFO | train | epoch 159 | loss 3.973 | nll_loss 2.262 | ppl 4.8 | wps 20697.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30953 | lr 0.000179742 | gnorm 1.11 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 101198
2022-03-07 16:49:06 | INFO | fairseq.trainer | begin training epoch 160
2022-03-07 16:49:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:51:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:51:35 | INFO | train_inner | epoch 160:     48 / 196 loss=3.969, nll_loss=2.258, ppl=4.78, wps=20317.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31000, lr=0.000179605, gnorm=1.126, loss_scale=16, train_wall=291, gb_free=19.9, wall=101347
2022-03-07 16:56:45 | INFO | train_inner | epoch 160:    148 / 196 loss=3.97, nll_loss=2.258, ppl=4.78, wps=21115.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=31100, lr=0.000179316, gnorm=1.105, loss_scale=16, train_wall=288, gb_free=19.9, wall=101657
2022-03-07 16:58:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:59:19 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.88 | nll_loss 9.794 | ppl 887.83 | wps 41635.4 | wpb 510.9 | bsz 1 | num_updates 31147 | best_loss 7.96
2022-03-07 16:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 31147 updates
2022-03-07 16:59:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 16:59:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 160 @ 31147 updates, score 10.88) (writing took 3.6501299366354942 seconds)
2022-03-07 16:59:22 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-07 16:59:22 | INFO | train | epoch 160 | loss 3.968 | nll_loss 2.257 | ppl 4.78 | wps 20589.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31147 | lr 0.000179181 | gnorm 1.11 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 101814
2022-03-07 16:59:22 | INFO | fairseq.trainer | begin training epoch 161
2022-03-07 16:59:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:02:07 | INFO | train_inner | epoch 161:     53 / 196 loss=3.965, nll_loss=2.253, ppl=4.77, wps=20309, ups=0.31, wpb=65367, bsz=127.7, num_updates=31200, lr=0.000179029, gnorm=1.125, loss_scale=16, train_wall=291, gb_free=19.9, wall=101979
2022-03-07 17:05:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:07:20 | INFO | train_inner | epoch 161:    154 / 196 loss=3.969, nll_loss=2.258, ppl=4.78, wps=20906.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=31300, lr=0.000178743, gnorm=1.11, loss_scale=16, train_wall=291, gb_free=19.9, wall=102293
2022-03-07 17:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:09:35 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.893 | nll_loss 9.817 | ppl 901.96 | wps 41844.4 | wpb 510.9 | bsz 1 | num_updates 31342 | best_loss 7.96
2022-03-07 17:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 31342 updates
2022-03-07 17:09:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:09:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 161 @ 31342 updates, score 10.893) (writing took 3.66184099111706 seconds)
2022-03-07 17:09:39 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-07 17:09:39 | INFO | train | epoch 161 | loss 3.965 | nll_loss 2.253 | ppl 4.77 | wps 20698 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 31342 | lr 0.000178623 | gnorm 1.118 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 102431
2022-03-07 17:09:39 | INFO | fairseq.trainer | begin training epoch 162
2022-03-07 17:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:11:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:12:42 | INFO | train_inner | epoch 162:     59 / 196 loss=3.963, nll_loss=2.251, ppl=4.76, wps=20324.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=31400, lr=0.000178458, gnorm=1.106, loss_scale=16, train_wall=290, gb_free=19.9, wall=102614
2022-03-07 17:17:52 | INFO | train_inner | epoch 162:    159 / 196 loss=3.973, nll_loss=2.262, ppl=4.8, wps=21121.4, ups=0.32, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=1.128, loss_scale=16, train_wall=288, gb_free=19.9, wall=102924
2022-03-07 17:18:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:19:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:19:52 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.895 | nll_loss 9.822 | ppl 905.09 | wps 41717.2 | wpb 510.9 | bsz 1 | num_updates 31536 | best_loss 7.96
2022-03-07 17:19:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 31536 updates
2022-03-07 17:19:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:19:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:19:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 162 @ 31536 updates, score 10.895) (writing took 3.655112663283944 seconds)
2022-03-07 17:19:55 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-07 17:19:55 | INFO | train | epoch 162 | loss 3.964 | nll_loss 2.252 | ppl 4.76 | wps 20598.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31536 | lr 0.000178072 | gnorm 1.113 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 103047
2022-03-07 17:19:55 | INFO | fairseq.trainer | begin training epoch 163
2022-03-07 17:19:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:23:14 | INFO | train_inner | epoch 163:     64 / 196 loss=3.942, nll_loss=2.227, ppl=4.68, wps=20312, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31600, lr=0.000177892, gnorm=1.101, loss_scale=16, train_wall=291, gb_free=19.9, wall=103246
2022-03-07 17:25:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:28:28 | INFO | train_inner | epoch 163:    165 / 196 loss=3.976, nll_loss=2.265, ppl=4.81, wps=20906.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=31700, lr=0.000177611, gnorm=1.119, loss_scale=16, train_wall=291, gb_free=19.9, wall=103560
2022-03-07 17:30:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:30:08 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.825 | nll_loss 9.741 | ppl 855.9 | wps 41729.8 | wpb 510.9 | bsz 1 | num_updates 31731 | best_loss 7.96
2022-03-07 17:30:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 31731 updates
2022-03-07 17:30:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:30:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:30:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 163 @ 31731 updates, score 10.825) (writing took 3.6155965495854616 seconds)
2022-03-07 17:30:12 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-07 17:30:12 | INFO | train | epoch 163 | loss 3.958 | nll_loss 2.246 | ppl 4.74 | wps 20696.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 31731 | lr 0.000177524 | gnorm 1.116 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 103664
2022-03-07 17:30:12 | INFO | fairseq.trainer | begin training epoch 164
2022-03-07 17:30:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:32:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:33:49 | INFO | train_inner | epoch 164:     70 / 196 loss=3.943, nll_loss=2.227, ppl=4.68, wps=20314.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=31800, lr=0.000177332, gnorm=1.111, loss_scale=16, train_wall=291, gb_free=19.9, wall=103881
2022-03-07 17:39:00 | INFO | train_inner | epoch 164:    170 / 196 loss=3.971, nll_loss=2.26, ppl=4.79, wps=21118.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=31900, lr=0.000177054, gnorm=1.131, loss_scale=16, train_wall=288, gb_free=19.9, wall=104192
2022-03-07 17:39:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:40:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:40:25 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.892 | nll_loss 9.817 | ppl 901.8 | wps 41720.1 | wpb 510.9 | bsz 1 | num_updates 31925 | best_loss 7.96
2022-03-07 17:40:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 31925 updates
2022-03-07 17:40:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:40:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:40:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 164 @ 31925 updates, score 10.892) (writing took 3.6590755339711905 seconds)
2022-03-07 17:40:28 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-07 17:40:28 | INFO | train | epoch 164 | loss 3.955 | nll_loss 2.242 | ppl 4.73 | wps 20592.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31925 | lr 0.000176984 | gnorm 1.118 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 104281
2022-03-07 17:40:28 | INFO | fairseq.trainer | begin training epoch 165
2022-03-07 17:40:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:44:21 | INFO | train_inner | epoch 165:     75 / 196 loss=3.936, nll_loss=2.221, ppl=4.66, wps=20314, ups=0.31, wpb=65367, bsz=127.7, num_updates=32000, lr=0.000176777, gnorm=1.114, loss_scale=16, train_wall=291, gb_free=19.9, wall=104514
2022-03-07 17:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:49:35 | INFO | train_inner | epoch 165:    176 / 196 loss=3.975, nll_loss=2.265, ppl=4.81, wps=20900.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=32100, lr=0.000176501, gnorm=1.132, loss_scale=16, train_wall=291, gb_free=19.9, wall=104827
2022-03-07 17:50:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:50:41 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.91 | nll_loss 9.84 | ppl 916.39 | wps 41618.7 | wpb 510.9 | bsz 1 | num_updates 32120 | best_loss 7.96
2022-03-07 17:50:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 32120 updates
2022-03-07 17:50:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:50:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 17:50:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 165 @ 32120 updates, score 10.91) (writing took 3.2884556809440255 seconds)
2022-03-07 17:50:45 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-07 17:50:45 | INFO | train | epoch 165 | loss 3.953 | nll_loss 2.24 | ppl 4.72 | wps 20704.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32120 | lr 0.000176446 | gnorm 1.123 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 104897
2022-03-07 17:50:45 | INFO | fairseq.trainer | begin training epoch 166
2022-03-07 17:50:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:52:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:54:56 | INFO | train_inner | epoch 166:     81 / 196 loss=3.924, nll_loss=2.206, ppl=4.61, wps=20339.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32200, lr=0.000176227, gnorm=1.123, loss_scale=16, train_wall=290, gb_free=19.9, wall=105148
2022-03-07 17:59:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:00:10 | INFO | train_inner | epoch 166:    182 / 196 loss=3.974, nll_loss=2.264, ppl=4.8, wps=20913.9, ups=0.32, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=1.139, loss_scale=16, train_wall=291, gb_free=19.9, wall=105462
2022-03-07 18:00:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:00:58 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.885 | nll_loss 9.812 | ppl 898.65 | wps 41665.8 | wpb 510.9 | bsz 1 | num_updates 32314 | best_loss 7.96
2022-03-07 18:00:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 32314 updates
2022-03-07 18:00:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:01:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:01:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 166 @ 32314 updates, score 10.885) (writing took 3.2431907979771495 seconds)
2022-03-07 18:01:01 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-07 18:01:01 | INFO | train | epoch 166 | loss 3.948 | nll_loss 2.234 | ppl 4.7 | wps 20609.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32314 | lr 0.000175916 | gnorm 1.13 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 105513
2022-03-07 18:01:01 | INFO | fairseq.trainer | begin training epoch 167
2022-03-07 18:01:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:05:28 | INFO | train_inner | epoch 167:     86 / 196 loss=3.919, nll_loss=2.201, ppl=4.6, wps=20527.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=32400, lr=0.000175682, gnorm=1.109, loss_scale=16, train_wall=288, gb_free=19.9, wall=105780
2022-03-07 18:06:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:10:42 | INFO | train_inner | epoch 167:    187 / 196 loss=3.976, nll_loss=2.266, ppl=4.81, wps=20901, ups=0.32, wpb=65532.4, bsz=128, num_updates=32500, lr=0.000175412, gnorm=1.148, loss_scale=16, train_wall=291, gb_free=19.9, wall=106094
2022-03-07 18:11:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:11:14 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.903 | nll_loss 9.829 | ppl 909.62 | wps 41449.5 | wpb 510.9 | bsz 1 | num_updates 32509 | best_loss 7.96
2022-03-07 18:11:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 32509 updates
2022-03-07 18:11:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:11:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:11:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 167 @ 32509 updates, score 10.903) (writing took 3.2440726859495044 seconds)
2022-03-07 18:11:17 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-07 18:11:17 | INFO | train | epoch 167 | loss 3.945 | nll_loss 2.231 | ppl 4.69 | wps 20699 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32509 | lr 0.000175387 | gnorm 1.127 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 106130
2022-03-07 18:11:17 | INFO | fairseq.trainer | begin training epoch 168
2022-03-07 18:11:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:13:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:16:03 | INFO | train_inner | epoch 168:     92 / 196 loss=3.917, nll_loss=2.199, ppl=4.59, wps=20334, ups=0.31, wpb=65367, bsz=127.7, num_updates=32600, lr=0.000175142, gnorm=1.123, loss_scale=16, train_wall=291, gb_free=19.9, wall=106415
2022-03-07 18:20:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:21:17 | INFO | train_inner | epoch 168:    193 / 196 loss=3.968, nll_loss=2.257, ppl=4.78, wps=20905.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=32700, lr=0.000174874, gnorm=1.126, loss_scale=16, train_wall=291, gb_free=19.9, wall=106729
2022-03-07 18:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:21:30 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.912 | nll_loss 9.839 | ppl 915.83 | wps 41379.8 | wpb 510.9 | bsz 1 | num_updates 32703 | best_loss 7.96
2022-03-07 18:21:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 32703 updates
2022-03-07 18:21:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 168 @ 32703 updates, score 10.912) (writing took 3.2733937269076705 seconds)
2022-03-07 18:21:34 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-07 18:21:34 | INFO | train | epoch 168 | loss 3.941 | nll_loss 2.226 | ppl 4.68 | wps 20601.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32703 | lr 0.000174866 | gnorm 1.126 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 106746
2022-03-07 18:21:34 | INFO | fairseq.trainer | begin training epoch 169
2022-03-07 18:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:26:35 | INFO | train_inner | epoch 169:     97 / 196 loss=3.91, nll_loss=2.19, ppl=4.56, wps=20529.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32800, lr=0.000174608, gnorm=1.129, loss_scale=16, train_wall=288, gb_free=19.9, wall=107047
2022-03-07 18:26:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:31:47 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.949 | nll_loss 9.873 | ppl 937.66 | wps 41904.5 | wpb 510.9 | bsz 1 | num_updates 32898 | best_loss 7.96
2022-03-07 18:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 32898 updates
2022-03-07 18:31:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:31:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:31:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 169 @ 32898 updates, score 10.949) (writing took 3.2541837468743324 seconds)
2022-03-07 18:31:50 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-07 18:31:50 | INFO | train | epoch 169 | loss 3.94 | nll_loss 2.225 | ppl 4.68 | wps 20709.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32898 | lr 0.000174347 | gnorm 1.13 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 107362
2022-03-07 18:31:50 | INFO | fairseq.trainer | begin training epoch 170
2022-03-07 18:31:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:31:56 | INFO | train_inner | epoch 170:      2 / 196 loss=3.972, nll_loss=2.262, ppl=4.8, wps=20341.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=32900, lr=0.000174342, gnorm=1.132, loss_scale=16, train_wall=291, gb_free=19.9, wall=107369
2022-03-07 18:33:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:37:10 | INFO | train_inner | epoch 170:    103 / 196 loss=3.903, nll_loss=2.182, ppl=4.54, wps=20910, ups=0.32, wpb=65532.4, bsz=128, num_updates=33000, lr=0.000174078, gnorm=1.107, loss_scale=16, train_wall=291, gb_free=19.9, wall=107682
2022-03-07 18:40:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:41:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:42:03 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.952 | nll_loss 9.886 | ppl 946.36 | wps 41214.5 | wpb 510.9 | bsz 1 | num_updates 33092 | best_loss 7.96
2022-03-07 18:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 33092 updates
2022-03-07 18:42:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:42:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:42:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 170 @ 33092 updates, score 10.952) (writing took 3.23940338101238 seconds)
2022-03-07 18:42:06 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-07 18:42:06 | INFO | train | epoch 170 | loss 3.936 | nll_loss 2.221 | ppl 4.66 | wps 20608.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33092 | lr 0.000173836 | gnorm 1.116 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 107978
2022-03-07 18:42:06 | INFO | fairseq.trainer | begin training epoch 171
2022-03-07 18:42:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:42:31 | INFO | train_inner | epoch 171:      8 / 196 loss=3.966, nll_loss=2.255, ppl=4.77, wps=20345.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=33100, lr=0.000173814, gnorm=1.127, loss_scale=16, train_wall=290, gb_free=19.9, wall=108003
2022-03-07 18:47:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:47:44 | INFO | train_inner | epoch 171:    109 / 196 loss=3.907, nll_loss=2.187, ppl=4.55, wps=20911.8, ups=0.32, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=1.1, loss_scale=16, train_wall=291, gb_free=19.9, wall=108317
2022-03-07 18:52:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:52:19 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.926 | nll_loss 9.852 | ppl 924.39 | wps 41608.3 | wpb 510.9 | bsz 1 | num_updates 33287 | best_loss 7.96
2022-03-07 18:52:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 33287 updates
2022-03-07 18:52:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:52:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 18:52:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 171 @ 33287 updates, score 10.926) (writing took 3.210348947905004 seconds)
2022-03-07 18:52:22 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-07 18:52:22 | INFO | train | epoch 171 | loss 3.934 | nll_loss 2.218 | ppl 4.65 | wps 20716.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 33287 | lr 0.000173326 | gnorm 1.127 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 108594
2022-03-07 18:52:22 | INFO | fairseq.trainer | begin training epoch 172
2022-03-07 18:52:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:53:03 | INFO | train_inner | epoch 172:     13 / 196 loss=3.957, nll_loss=2.244, ppl=4.74, wps=20540.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33300, lr=0.000173292, gnorm=1.152, loss_scale=16, train_wall=288, gb_free=19.9, wall=108635
2022-03-07 18:54:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:58:16 | INFO | train_inner | epoch 172:    114 / 196 loss=3.903, nll_loss=2.183, ppl=4.54, wps=20907.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=33400, lr=0.000173032, gnorm=1.127, loss_scale=16, train_wall=291, gb_free=19.9, wall=108948
2022-03-07 19:00:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:02:35 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.891 | nll_loss 9.812 | ppl 899.09 | wps 41662.5 | wpb 510.9 | bsz 1 | num_updates 33481 | best_loss 7.96
2022-03-07 19:02:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 33481 updates
2022-03-07 19:02:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:02:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 172 @ 33481 updates, score 10.891) (writing took 3.234266061335802 seconds)
2022-03-07 19:02:38 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-07 19:02:38 | INFO | train | epoch 172 | loss 3.929 | nll_loss 2.212 | ppl 4.63 | wps 20606.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33481 | lr 0.000172823 | gnorm 1.13 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 109210
2022-03-07 19:02:38 | INFO | fairseq.trainer | begin training epoch 173
2022-03-07 19:02:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:03:37 | INFO | train_inner | epoch 173:     19 / 196 loss=3.949, nll_loss=2.235, ppl=4.71, wps=20342.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=33500, lr=0.000172774, gnorm=1.129, loss_scale=16, train_wall=290, gb_free=19.9, wall=109270
2022-03-07 19:07:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:08:51 | INFO | train_inner | epoch 173:    120 / 196 loss=3.912, nll_loss=2.193, ppl=4.57, wps=20908.4, ups=0.32, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=1.121, loss_scale=16, train_wall=291, gb_free=19.9, wall=109583
2022-03-07 19:12:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:12:51 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.915 | nll_loss 9.841 | ppl 917.29 | wps 41696.8 | wpb 510.9 | bsz 1 | num_updates 33676 | best_loss 7.96
2022-03-07 19:12:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 33676 updates
2022-03-07 19:12:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:12:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:12:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 173 @ 33676 updates, score 10.915) (writing took 3.290328837931156 seconds)
2022-03-07 19:12:54 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-07 19:12:54 | INFO | train | epoch 173 | loss 3.928 | nll_loss 2.211 | ppl 4.63 | wps 20710.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 33676 | lr 0.000172322 | gnorm 1.128 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 109827
2022-03-07 19:12:55 | INFO | fairseq.trainer | begin training epoch 174
2022-03-07 19:12:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:14:09 | INFO | train_inner | epoch 174:     24 / 196 loss=3.94, nll_loss=2.225, ppl=4.67, wps=20534, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33700, lr=0.00017226, gnorm=1.125, loss_scale=16, train_wall=288, gb_free=19.9, wall=109901
2022-03-07 19:14:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:19:23 | INFO | train_inner | epoch 174:    125 / 196 loss=3.912, nll_loss=2.193, ppl=4.57, wps=20910.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=33800, lr=0.000172005, gnorm=1.128, loss_scale=16, train_wall=291, gb_free=19.9, wall=110215
2022-03-07 19:21:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:23:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:23:07 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 10.943 | nll_loss 9.87 | ppl 935.56 | wps 41608.2 | wpb 510.9 | bsz 1 | num_updates 33870 | best_loss 7.96
2022-03-07 19:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 33870 updates
2022-03-07 19:23:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:23:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 174 @ 33870 updates, score 10.943) (writing took 3.229920976795256 seconds)
2022-03-07 19:23:11 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-07 19:23:11 | INFO | train | epoch 174 | loss 3.924 | nll_loss 2.206 | ppl 4.61 | wps 20608.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33870 | lr 0.000171827 | gnorm 1.127 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 110443
2022-03-07 19:23:11 | INFO | fairseq.trainer | begin training epoch 175
2022-03-07 19:23:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:24:44 | INFO | train_inner | epoch 175:     30 / 196 loss=3.931, nll_loss=2.215, ppl=4.64, wps=20346.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=33900, lr=0.000171751, gnorm=1.137, loss_scale=16, train_wall=290, gb_free=19.9, wall=110536
2022-03-07 19:28:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:29:57 | INFO | train_inner | epoch 175:    131 / 196 loss=3.912, nll_loss=2.193, ppl=4.57, wps=20910.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=34000, lr=0.000171499, gnorm=1.142, loss_scale=16, train_wall=291, gb_free=19.9, wall=110849
2022-03-07 19:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:33:23 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.969 | nll_loss 9.898 | ppl 954.21 | wps 41646.4 | wpb 510.9 | bsz 1 | num_updates 34065 | best_loss 7.96
2022-03-07 19:33:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 34065 updates
2022-03-07 19:33:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:33:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 175 @ 34065 updates, score 10.969) (writing took 3.256177667528391 seconds)
2022-03-07 19:33:27 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-07 19:33:27 | INFO | train | epoch 175 | loss 3.922 | nll_loss 2.204 | ppl 4.61 | wps 20713.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 34065 | lr 0.000171335 | gnorm 1.138 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 111059
2022-03-07 19:33:27 | INFO | fairseq.trainer | begin training epoch 176
2022-03-07 19:33:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:34:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:35:19 | INFO | train_inner | epoch 176:     36 / 196 loss=3.926, nll_loss=2.209, ppl=4.62, wps=20338.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=34100, lr=0.000171247, gnorm=1.13, loss_scale=16, train_wall=291, gb_free=19.9, wall=111171
2022-03-07 19:40:29 | INFO | train_inner | epoch 176:    136 / 196 loss=3.912, nll_loss=2.194, ppl=4.57, wps=21111.5, ups=0.32, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=1.131, loss_scale=16, train_wall=288, gb_free=19.9, wall=111481
2022-03-07 19:41:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:43:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:43:40 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.925 | nll_loss 9.846 | ppl 920.24 | wps 41584.8 | wpb 510.9 | bsz 1 | num_updates 34259 | best_loss 7.96
2022-03-07 19:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 34259 updates
2022-03-07 19:43:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:43:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:43:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 176 @ 34259 updates, score 10.925) (writing took 3.283915111795068 seconds)
2022-03-07 19:43:43 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-07 19:43:43 | INFO | train | epoch 176 | loss 3.918 | nll_loss 2.2 | ppl 4.59 | wps 20601.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34259 | lr 0.000170849 | gnorm 1.131 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 111675
2022-03-07 19:43:43 | INFO | fairseq.trainer | begin training epoch 177
2022-03-07 19:43:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:45:50 | INFO | train_inner | epoch 177:     41 / 196 loss=3.925, nll_loss=2.208, ppl=4.62, wps=20339.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=34300, lr=0.000170747, gnorm=1.141, loss_scale=16, train_wall=290, gb_free=19.9, wall=111803
2022-03-07 19:48:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:51:04 | INFO | train_inner | epoch 177:    142 / 196 loss=3.909, nll_loss=2.19, ppl=4.56, wps=20914.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=34400, lr=0.000170499, gnorm=1.135, loss_scale=16, train_wall=291, gb_free=19.9, wall=112116
2022-03-07 19:53:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:53:56 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 10.966 | nll_loss 9.902 | ppl 956.84 | wps 41639.8 | wpb 510.9 | bsz 1 | num_updates 34454 | best_loss 7.96
2022-03-07 19:53:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 34454 updates
2022-03-07 19:53:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:53:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 19:53:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 177 @ 34454 updates, score 10.966) (writing took 3.280620774254203 seconds)
2022-03-07 19:53:59 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-07 19:53:59 | INFO | train | epoch 177 | loss 3.916 | nll_loss 2.198 | ppl 4.59 | wps 20716.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 34454 | lr 0.000170365 | gnorm 1.136 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 112291
2022-03-07 19:53:59 | INFO | fairseq.trainer | begin training epoch 178
2022-03-07 19:53:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:55:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:56:25 | INFO | train_inner | epoch 178:     47 / 196 loss=3.92, nll_loss=2.202, ppl=4.6, wps=20341.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=34500, lr=0.000170251, gnorm=1.125, loss_scale=16, train_wall=291, gb_free=19.9, wall=112437
2022-03-07 20:01:35 | INFO | train_inner | epoch 178:    147 / 196 loss=3.908, nll_loss=2.188, ppl=4.56, wps=21111.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=34600, lr=0.000170005, gnorm=1.13, loss_scale=16, train_wall=288, gb_free=19.9, wall=112748
2022-03-07 20:01:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:04:12 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 10.929 | nll_loss 9.856 | ppl 926.87 | wps 41813.9 | wpb 510.9 | bsz 1 | num_updates 34648 | best_loss 7.96
2022-03-07 20:04:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 34648 updates
2022-03-07 20:04:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:04:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:04:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 178 @ 34648 updates, score 10.929) (writing took 3.317961278371513 seconds)
2022-03-07 20:04:15 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-07 20:04:15 | INFO | train | epoch 178 | loss 3.913 | nll_loss 2.194 | ppl 4.58 | wps 20601.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34648 | lr 0.000169887 | gnorm 1.124 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 112908
2022-03-07 20:04:15 | INFO | fairseq.trainer | begin training epoch 179
2022-03-07 20:04:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:06:57 | INFO | train_inner | epoch 179:     52 / 196 loss=3.913, nll_loss=2.194, ppl=4.58, wps=20336.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=34700, lr=0.00016976, gnorm=1.126, loss_scale=16, train_wall=291, gb_free=19.9, wall=113069
2022-03-07 20:08:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:12:10 | INFO | train_inner | epoch 179:    153 / 196 loss=3.916, nll_loss=2.198, ppl=4.59, wps=20912.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=34800, lr=0.000169516, gnorm=1.127, loss_scale=16, train_wall=291, gb_free=19.9, wall=113382
2022-03-07 20:14:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:14:28 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.947 | nll_loss 9.871 | ppl 936.48 | wps 41772.7 | wpb 510.9 | bsz 1 | num_updates 34843 | best_loss 7.96
2022-03-07 20:14:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 34843 updates
2022-03-07 20:14:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:14:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 179 @ 34843 updates, score 10.947) (writing took 3.308467405848205 seconds)
2022-03-07 20:14:31 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-07 20:14:31 | INFO | train | epoch 179 | loss 3.91 | nll_loss 2.192 | ppl 4.57 | wps 20713.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 34843 | lr 0.000169411 | gnorm 1.136 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 113524
2022-03-07 20:14:32 | INFO | fairseq.trainer | begin training epoch 180
2022-03-07 20:14:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:15:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:17:32 | INFO | train_inner | epoch 180:     58 / 196 loss=3.9, nll_loss=2.18, ppl=4.53, wps=20346, ups=0.31, wpb=65367, bsz=127.7, num_updates=34900, lr=0.000169273, gnorm=1.146, loss_scale=16, train_wall=290, gb_free=19.9, wall=113704
2022-03-07 20:22:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:22:45 | INFO | train_inner | epoch 180:    159 / 196 loss=3.913, nll_loss=2.195, ppl=4.58, wps=20916.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=35000, lr=0.000169031, gnorm=1.157, loss_scale=16, train_wall=291, gb_free=19.9, wall=114017
2022-03-07 20:24:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:24:44 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.969 | nll_loss 9.903 | ppl 957.1 | wps 41746.8 | wpb 510.9 | bsz 1 | num_updates 35037 | best_loss 7.96
2022-03-07 20:24:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 35037 updates
2022-03-07 20:24:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:24:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:24:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 180 @ 35037 updates, score 10.969) (writing took 3.3048755526542664 seconds)
2022-03-07 20:24:47 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-07 20:24:47 | INFO | train | epoch 180 | loss 3.906 | nll_loss 2.187 | ppl 4.55 | wps 20611.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35037 | lr 0.000168942 | gnorm 1.151 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 114140
2022-03-07 20:24:48 | INFO | fairseq.trainer | begin training epoch 181
2022-03-07 20:24:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:28:03 | INFO | train_inner | epoch 181:     63 / 196 loss=3.9, nll_loss=2.179, ppl=4.53, wps=20535.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=35100, lr=0.00016879, gnorm=1.136, loss_scale=16, train_wall=288, gb_free=19.9, wall=114335
2022-03-07 20:29:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:33:17 | INFO | train_inner | epoch 181:    164 / 196 loss=3.912, nll_loss=2.193, ppl=4.57, wps=20907.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=35200, lr=0.00016855, gnorm=1.14, loss_scale=16, train_wall=291, gb_free=19.9, wall=114649
2022-03-07 20:34:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:35:00 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.955 | nll_loss 9.887 | ppl 947.01 | wps 41517.6 | wpb 510.9 | bsz 1 | num_updates 35232 | best_loss 7.96
2022-03-07 20:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 35232 updates
2022-03-07 20:35:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:35:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:35:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 181 @ 35232 updates, score 10.955) (writing took 3.300206294283271 seconds)
2022-03-07 20:35:04 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-07 20:35:04 | INFO | train | epoch 181 | loss 3.905 | nll_loss 2.185 | ppl 4.55 | wps 20710 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 35232 | lr 0.000168473 | gnorm 1.136 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 114756
2022-03-07 20:35:04 | INFO | fairseq.trainer | begin training epoch 182
2022-03-07 20:35:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:36:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:38:38 | INFO | train_inner | epoch 182:     69 / 196 loss=3.889, nll_loss=2.168, ppl=4.49, wps=20334.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35300, lr=0.000168311, gnorm=1.135, loss_scale=16, train_wall=290, gb_free=19.9, wall=114970
2022-03-07 20:42:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:43:51 | INFO | train_inner | epoch 182:    170 / 196 loss=3.915, nll_loss=2.197, ppl=4.59, wps=20910.4, ups=0.32, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=1.144, loss_scale=16, train_wall=291, gb_free=19.9, wall=115284
2022-03-07 20:45:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:45:17 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 10.944 | nll_loss 9.874 | ppl 938.25 | wps 41686 | wpb 510.9 | bsz 1 | num_updates 35426 | best_loss 7.96
2022-03-07 20:45:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 35426 updates
2022-03-07 20:45:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:45:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 182 @ 35426 updates, score 10.944) (writing took 3.3490638388320804 seconds)
2022-03-07 20:45:20 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-07 20:45:20 | INFO | train | epoch 182 | loss 3.901 | nll_loss 2.181 | ppl 4.53 | wps 20600.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35426 | lr 0.000168011 | gnorm 1.143 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 115372
2022-03-07 20:45:20 | INFO | fairseq.trainer | begin training epoch 183
2022-03-07 20:45:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:49:10 | INFO | train_inner | epoch 183:     74 / 196 loss=3.886, nll_loss=2.164, ppl=4.48, wps=20533.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=35500, lr=0.000167836, gnorm=1.136, loss_scale=16, train_wall=288, gb_free=19.9, wall=115602
2022-03-07 20:49:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:54:23 | INFO | train_inner | epoch 183:    175 / 196 loss=3.919, nll_loss=2.201, ppl=4.6, wps=20914.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=35600, lr=0.0001676, gnorm=1.137, loss_scale=16, train_wall=291, gb_free=19.9, wall=115915
2022-03-07 20:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:55:33 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.991 | nll_loss 9.929 | ppl 974.49 | wps 41686.8 | wpb 510.9 | bsz 1 | num_updates 35621 | best_loss 7.96
2022-03-07 20:55:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 35621 updates
2022-03-07 20:55:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:55:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 20:55:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 183 @ 35621 updates, score 10.991) (writing took 3.326358891092241 seconds)
2022-03-07 20:55:36 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-07 20:55:36 | INFO | train | epoch 183 | loss 3.899 | nll_loss 2.178 | ppl 4.53 | wps 20711.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 35621 | lr 0.000167551 | gnorm 1.131 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 115988
2022-03-07 20:55:36 | INFO | fairseq.trainer | begin training epoch 184
2022-03-07 20:55:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:56:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:59:44 | INFO | train_inner | epoch 184:     80 / 196 loss=3.875, nll_loss=2.151, ppl=4.44, wps=20343.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=35700, lr=0.000167365, gnorm=1.119, loss_scale=16, train_wall=290, gb_free=19.9, wall=116237
2022-03-07 21:03:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:04:58 | INFO | train_inner | epoch 184:    181 / 196 loss=3.92, nll_loss=2.202, ppl=4.6, wps=20919.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=35800, lr=0.000167132, gnorm=1.157, loss_scale=16, train_wall=291, gb_free=19.9, wall=116550
2022-03-07 21:05:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:05:49 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.914 | nll_loss 9.841 | ppl 917.1 | wps 40912.9 | wpb 510.9 | bsz 1 | num_updates 35815 | best_loss 7.96
2022-03-07 21:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 35815 updates
2022-03-07 21:05:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:05:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:05:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 184 @ 35815 updates, score 10.914) (writing took 3.3166895173490047 seconds)
2022-03-07 21:05:52 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-07 21:05:52 | INFO | train | epoch 184 | loss 3.896 | nll_loss 2.176 | ppl 4.52 | wps 20613.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35815 | lr 0.000167097 | gnorm 1.141 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 116604
2022-03-07 21:05:52 | INFO | fairseq.trainer | begin training epoch 185
2022-03-07 21:05:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:10:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:10:19 | INFO | train_inner | epoch 185:     86 / 196 loss=3.869, nll_loss=2.144, ppl=4.42, wps=20335.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35900, lr=0.000166899, gnorm=1.139, loss_scale=16, train_wall=290, gb_free=19.9, wall=116871
2022-03-07 21:15:30 | INFO | train_inner | epoch 185:    186 / 196 loss=3.92, nll_loss=2.202, ppl=4.6, wps=21106.8, ups=0.32, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=1.147, loss_scale=16, train_wall=289, gb_free=19.9, wall=117182
2022-03-07 21:16:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:16:05 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 10.946 | nll_loss 9.871 | ppl 936.38 | wps 40774.5 | wpb 510.9 | bsz 1 | num_updates 36010 | best_loss 7.96
2022-03-07 21:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 36010 updates
2022-03-07 21:16:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:16:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 185 @ 36010 updates, score 10.946) (writing took 3.294048444367945 seconds)
2022-03-07 21:16:09 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-07 21:16:09 | INFO | train | epoch 185 | loss 3.893 | nll_loss 2.172 | ppl 4.51 | wps 20700.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36010 | lr 0.000166644 | gnorm 1.14 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 117221
2022-03-07 21:16:09 | INFO | fairseq.trainer | begin training epoch 186
2022-03-07 21:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:17:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:20:51 | INFO | train_inner | epoch 186:     91 / 196 loss=3.864, nll_loss=2.139, ppl=4.4, wps=20328.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=36100, lr=0.000166436, gnorm=1.132, loss_scale=16, train_wall=291, gb_free=19.9, wall=117503
2022-03-07 21:23:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:26:05 | INFO | train_inner | epoch 186:    192 / 196 loss=3.921, nll_loss=2.204, ppl=4.61, wps=20913.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=36200, lr=0.000166206, gnorm=1.144, loss_scale=16, train_wall=291, gb_free=19.9, wall=117817
2022-03-07 21:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:26:21 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.952 | nll_loss 9.885 | ppl 945.41 | wps 41687.5 | wpb 510.9 | bsz 1 | num_updates 36204 | best_loss 7.96
2022-03-07 21:26:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 36204 updates
2022-03-07 21:26:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:26:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 186 @ 36204 updates, score 10.952) (writing took 3.3189000925049186 seconds)
2022-03-07 21:26:25 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-07 21:26:25 | INFO | train | epoch 186 | loss 3.89 | nll_loss 2.168 | ppl 4.5 | wps 20608.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36204 | lr 0.000166196 | gnorm 1.14 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 117837
2022-03-07 21:26:25 | INFO | fairseq.trainer | begin training epoch 187
2022-03-07 21:26:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:30:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:31:26 | INFO | train_inner | epoch 187:     97 / 196 loss=3.86, nll_loss=2.134, ppl=4.39, wps=20339.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=36300, lr=0.000165977, gnorm=1.133, loss_scale=16, train_wall=290, gb_free=19.9, wall=118138
2022-03-07 21:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:36:38 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 10.95 | nll_loss 9.879 | ppl 941.44 | wps 41717.9 | wpb 510.9 | bsz 1 | num_updates 36399 | best_loss 7.96
2022-03-07 21:36:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 36399 updates
2022-03-07 21:36:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 187 @ 36399 updates, score 10.95) (writing took 3.2794991536065936 seconds)
2022-03-07 21:36:41 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-07 21:36:41 | INFO | train | epoch 187 | loss 3.888 | nll_loss 2.167 | ppl 4.49 | wps 20714.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36399 | lr 0.000165751 | gnorm 1.138 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 118453
2022-03-07 21:36:41 | INFO | fairseq.trainer | begin training epoch 188
2022-03-07 21:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:36:44 | INFO | train_inner | epoch 188:      1 / 196 loss=3.917, nll_loss=2.2, ppl=4.59, wps=20542.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36400, lr=0.000165748, gnorm=1.147, loss_scale=16, train_wall=288, gb_free=19.9, wall=118456
2022-03-07 21:37:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:41:57 | INFO | train_inner | epoch 188:    102 / 196 loss=3.853, nll_loss=2.125, ppl=4.36, wps=20914.3, ups=0.32, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=1.131, loss_scale=16, train_wall=291, gb_free=19.9, wall=118770
2022-03-07 21:44:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:46:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:46:54 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.954 | nll_loss 9.882 | ppl 943.43 | wps 41851.1 | wpb 510.9 | bsz 1 | num_updates 36593 | best_loss 7.96
2022-03-07 21:46:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 36593 updates
2022-03-07 21:46:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:46:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:46:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 188 @ 36593 updates, score 10.954) (writing took 3.287101937457919 seconds)
2022-03-07 21:46:57 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-07 21:46:57 | INFO | train | epoch 188 | loss 3.886 | nll_loss 2.164 | ppl 4.48 | wps 20611.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36593 | lr 0.000165311 | gnorm 1.153 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 119069
2022-03-07 21:46:57 | INFO | fairseq.trainer | begin training epoch 189
2022-03-07 21:46:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:47:19 | INFO | train_inner | epoch 189:      7 / 196 loss=3.916, nll_loss=2.199, ppl=4.59, wps=20347.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36600, lr=0.000165295, gnorm=1.173, loss_scale=16, train_wall=290, gb_free=19.9, wall=119091
2022-03-07 21:50:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:52:32 | INFO | train_inner | epoch 189:    108 / 196 loss=3.855, nll_loss=2.128, ppl=4.37, wps=20906.9, ups=0.32, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=1.143, loss_scale=16, train_wall=291, gb_free=19.9, wall=119404
2022-03-07 21:57:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:57:10 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 11.001 | nll_loss 9.941 | ppl 983.14 | wps 41479.9 | wpb 510.9 | bsz 1 | num_updates 36788 | best_loss 7.96
2022-03-07 21:57:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 36788 updates
2022-03-07 21:57:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:57:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 21:57:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 189 @ 36788 updates, score 11.001) (writing took 3.346326450817287 seconds)
2022-03-07 21:57:13 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-07 21:57:13 | INFO | train | epoch 189 | loss 3.883 | nll_loss 2.161 | ppl 4.47 | wps 20710.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36788 | lr 0.000164872 | gnorm 1.144 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 119685
2022-03-07 21:57:13 | INFO | fairseq.trainer | begin training epoch 190
2022-03-07 21:57:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:57:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:57:54 | INFO | train_inner | epoch 190:     13 / 196 loss=3.908, nll_loss=2.189, ppl=4.56, wps=20340.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36800, lr=0.000164845, gnorm=1.144, loss_scale=16, train_wall=290, gb_free=19.9, wall=119726
2022-03-07 22:03:04 | INFO | train_inner | epoch 190:    113 / 196 loss=3.855, nll_loss=2.128, ppl=4.37, wps=21115.9, ups=0.32, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=1.143, loss_scale=16, train_wall=288, gb_free=19.9, wall=120036
2022-03-07 22:04:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:07:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:07:26 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 10.988 | nll_loss 9.928 | ppl 974.34 | wps 42017.5 | wpb 510.9 | bsz 1 | num_updates 36982 | best_loss 7.96
2022-03-07 22:07:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 36982 updates
2022-03-07 22:07:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:07:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 190 @ 36982 updates, score 10.988) (writing took 8.855816294439137 seconds)
2022-03-07 22:07:35 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-07 22:07:35 | INFO | train | epoch 190 | loss 3.88 | nll_loss 2.157 | ppl 4.46 | wps 20427.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36982 | lr 0.000164439 | gnorm 1.149 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 120307
2022-03-07 22:07:35 | INFO | fairseq.trainer | begin training epoch 191
2022-03-07 22:07:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:08:31 | INFO | train_inner | epoch 191:     18 / 196 loss=3.902, nll_loss=2.182, ppl=4.54, wps=20004.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37000, lr=0.000164399, gnorm=1.152, loss_scale=16, train_wall=290, gb_free=19.9, wall=120363
2022-03-07 22:11:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:13:44 | INFO | train_inner | epoch 191:    119 / 196 loss=3.856, nll_loss=2.129, ppl=4.38, wps=20912.9, ups=0.32, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=1.135, loss_scale=16, train_wall=291, gb_free=19.9, wall=120676
2022-03-07 22:17:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:17:47 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 10.963 | nll_loss 9.892 | ppl 949.9 | wps 41759 | wpb 510.9 | bsz 1 | num_updates 37177 | best_loss 7.96
2022-03-07 22:17:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 37177 updates
2022-03-07 22:17:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:17:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:17:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 191 @ 37177 updates, score 10.963) (writing took 3.4179912908002734 seconds)
2022-03-07 22:17:51 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-07 22:17:51 | INFO | train | epoch 191 | loss 3.877 | nll_loss 2.153 | ppl 4.45 | wps 20712.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 37177 | lr 0.000164007 | gnorm 1.142 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 120923
2022-03-07 22:17:51 | INFO | fairseq.trainer | begin training epoch 192
2022-03-07 22:17:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:18:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:19:05 | INFO | train_inner | epoch 192:     24 / 196 loss=3.898, nll_loss=2.178, ppl=4.52, wps=20338.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37200, lr=0.000163956, gnorm=1.156, loss_scale=16, train_wall=290, gb_free=19.9, wall=120998
2022-03-07 22:24:16 | INFO | train_inner | epoch 192:    124 / 196 loss=3.858, nll_loss=2.131, ppl=4.38, wps=21106.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=37300, lr=0.000163737, gnorm=1.142, loss_scale=16, train_wall=288, gb_free=19.9, wall=121308
2022-03-07 22:24:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:27:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:28:04 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 11.01 | nll_loss 9.944 | ppl 985.24 | wps 41698.5 | wpb 510.9 | bsz 1 | num_updates 37371 | best_loss 7.96
2022-03-07 22:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 37371 updates
2022-03-07 22:28:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:28:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:28:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 192 @ 37371 updates, score 11.01) (writing took 3.3544504875317216 seconds)
2022-03-07 22:28:07 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-07 22:28:07 | INFO | train | epoch 192 | loss 3.874 | nll_loss 2.151 | ppl 4.44 | wps 20601.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37371 | lr 0.000163581 | gnorm 1.158 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 121539
2022-03-07 22:28:07 | INFO | fairseq.trainer | begin training epoch 193
2022-03-07 22:28:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:29:37 | INFO | train_inner | epoch 193:     29 / 196 loss=3.888, nll_loss=2.167, ppl=4.49, wps=20338.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37400, lr=0.000163517, gnorm=1.168, loss_scale=16, train_wall=290, gb_free=19.9, wall=121629
2022-03-07 22:31:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:34:51 | INFO | train_inner | epoch 193:    130 / 196 loss=3.857, nll_loss=2.131, ppl=4.38, wps=20907.3, ups=0.32, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=1.137, loss_scale=16, train_wall=291, gb_free=19.9, wall=121943
2022-03-07 22:38:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:38:20 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 10.973 | nll_loss 9.907 | ppl 959.95 | wps 41691.4 | wpb 510.9 | bsz 1 | num_updates 37565 | best_loss 7.96
2022-03-07 22:38:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 37565 updates
2022-03-07 22:38:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:38:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:38:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 193 @ 37565 updates, score 10.973) (writing took 3.317394749261439 seconds)
2022-03-07 22:38:23 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-07 22:38:23 | INFO | train | epoch 193 | loss 3.873 | nll_loss 2.149 | ppl 4.43 | wps 20601.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37565 | lr 0.000163158 | gnorm 1.147 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 122156
2022-03-07 22:38:23 | INFO | fairseq.trainer | begin training epoch 194
2022-03-07 22:38:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:40:12 | INFO | train_inner | epoch 194:     35 / 196 loss=3.886, nll_loss=2.164, ppl=4.48, wps=20338.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=37600, lr=0.000163082, gnorm=1.167, loss_scale=16, train_wall=291, gb_free=19.9, wall=122264
2022-03-07 22:44:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:45:25 | INFO | train_inner | epoch 194:    136 / 196 loss=3.862, nll_loss=2.137, ppl=4.4, wps=20917.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=37700, lr=0.000162866, gnorm=1.133, loss_scale=16, train_wall=291, gb_free=19.9, wall=122578
2022-03-07 22:48:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:48:36 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 10.998 | nll_loss 9.932 | ppl 976.65 | wps 41611.8 | wpb 510.9 | bsz 1 | num_updates 37760 | best_loss 7.96
2022-03-07 22:48:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 37760 updates
2022-03-07 22:48:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:48:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:48:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 194 @ 37760 updates, score 10.998) (writing took 3.3006080146878958 seconds)
2022-03-07 22:48:39 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-07 22:48:39 | INFO | train | epoch 194 | loss 3.87 | nll_loss 2.146 | ppl 4.43 | wps 20721.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 37760 | lr 0.000162736 | gnorm 1.147 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 122771
2022-03-07 22:48:39 | INFO | fairseq.trainer | begin training epoch 195
2022-03-07 22:48:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:50:44 | INFO | train_inner | epoch 195:     40 / 196 loss=3.871, nll_loss=2.147, ppl=4.43, wps=20545.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=37800, lr=0.00016265, gnorm=1.151, loss_scale=16, train_wall=287, gb_free=19.9, wall=122896
2022-03-07 22:51:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:55:57 | INFO | train_inner | epoch 195:    141 / 196 loss=3.864, nll_loss=2.139, ppl=4.4, wps=20920.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=37900, lr=0.000162435, gnorm=1.144, loss_scale=16, train_wall=291, gb_free=19.9, wall=123209
2022-03-07 22:58:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:58:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:58:52 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 11.014 | nll_loss 9.956 | ppl 993.11 | wps 41789.9 | wpb 510.9 | bsz 1 | num_updates 37954 | best_loss 7.96
2022-03-07 22:58:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 37954 updates
2022-03-07 22:58:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 22:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 195 @ 37954 updates, score 11.014) (writing took 3.3763269893825054 seconds)
2022-03-07 22:58:55 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-07 22:58:55 | INFO | train | epoch 195 | loss 3.868 | nll_loss 2.143 | ppl 4.42 | wps 20613.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37954 | lr 0.00016232 | gnorm 1.148 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 123387
2022-03-07 22:58:55 | INFO | fairseq.trainer | begin training epoch 196
2022-03-07 22:58:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:01:18 | INFO | train_inner | epoch 196:     46 / 196 loss=3.869, nll_loss=2.145, ppl=4.42, wps=20343.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=38000, lr=0.000162221, gnorm=1.138, loss_scale=16, train_wall=290, gb_free=19.9, wall=123530
2022-03-07 23:05:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:06:32 | INFO | train_inner | epoch 196:    147 / 196 loss=3.864, nll_loss=2.139, ppl=4.4, wps=20908, ups=0.32, wpb=65532.4, bsz=128, num_updates=38100, lr=0.000162008, gnorm=1.158, loss_scale=16, train_wall=291, gb_free=19.9, wall=123844
2022-03-07 23:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:09:08 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 10.983 | nll_loss 9.918 | ppl 967.46 | wps 41562.2 | wpb 510.9 | bsz 1 | num_updates 38149 | best_loss 7.96
2022-03-07 23:09:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 38149 updates
2022-03-07 23:09:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 196 @ 38149 updates, score 10.983) (writing took 3.3350655511021614 seconds)
2022-03-07 23:09:11 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-07 23:09:11 | INFO | train | epoch 196 | loss 3.865 | nll_loss 2.141 | ppl 4.41 | wps 20711.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 38149 | lr 0.000161904 | gnorm 1.147 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 124004
2022-03-07 23:09:11 | INFO | fairseq.trainer | begin training epoch 197
2022-03-07 23:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:11:50 | INFO | train_inner | epoch 197:     51 / 196 loss=3.863, nll_loss=2.138, ppl=4.4, wps=20532.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=38200, lr=0.000161796, gnorm=1.156, loss_scale=16, train_wall=288, gb_free=19.9, wall=124162
2022-03-07 23:12:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:17:03 | INFO | train_inner | epoch 197:    152 / 196 loss=3.87, nll_loss=2.145, ppl=4.42, wps=20916.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=38300, lr=0.000161585, gnorm=1.161, loss_scale=16, train_wall=291, gb_free=19.9, wall=124475
2022-03-07 23:18:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:19:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:19:24 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 11.015 | nll_loss 9.952 | ppl 990.83 | wps 41318.6 | wpb 510.9 | bsz 1 | num_updates 38343 | best_loss 7.96
2022-03-07 23:19:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 38343 updates
2022-03-07 23:19:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:19:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:19:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 197 @ 38343 updates, score 11.015) (writing took 3.3775412095710635 seconds)
2022-03-07 23:19:28 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-07 23:19:28 | INFO | train | epoch 197 | loss 3.863 | nll_loss 2.138 | ppl 4.4 | wps 20605 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38343 | lr 0.000161494 | gnorm 1.156 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 124620
2022-03-07 23:19:28 | INFO | fairseq.trainer | begin training epoch 198
2022-03-07 23:19:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:22:25 | INFO | train_inner | epoch 198:     57 / 196 loss=3.855, nll_loss=2.129, ppl=4.37, wps=20337.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=38400, lr=0.000161374, gnorm=1.136, loss_scale=16, train_wall=290, gb_free=19.9, wall=124797
2022-03-07 23:25:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:27:38 | INFO | train_inner | epoch 198:    158 / 196 loss=3.868, nll_loss=2.144, ppl=4.42, wps=20919.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=38500, lr=0.000161165, gnorm=1.153, loss_scale=16, train_wall=291, gb_free=19.9, wall=125110
2022-03-07 23:29:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:29:40 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 11.017 | nll_loss 9.955 | ppl 992.43 | wps 41438.2 | wpb 510.9 | bsz 1 | num_updates 38538 | best_loss 7.96
2022-03-07 23:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 38538 updates
2022-03-07 23:29:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:29:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:29:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 198 @ 38538 updates, score 11.017) (writing took 3.3254574947059155 seconds)
2022-03-07 23:29:44 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-07 23:29:44 | INFO | train | epoch 198 | loss 3.861 | nll_loss 2.136 | ppl 4.4 | wps 20717.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 38538 | lr 0.000161085 | gnorm 1.155 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 125236
2022-03-07 23:29:44 | INFO | fairseq.trainer | begin training epoch 199
2022-03-07 23:29:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:32:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:32:59 | INFO | train_inner | epoch 199:     63 / 196 loss=3.85, nll_loss=2.123, ppl=4.36, wps=20340.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=38600, lr=0.000160956, gnorm=1.165, loss_scale=16, train_wall=291, gb_free=19.9, wall=125431
2022-03-07 23:38:10 | INFO | train_inner | epoch 199:    163 / 196 loss=3.867, nll_loss=2.143, ppl=4.42, wps=21116.6, ups=0.32, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=1.153, loss_scale=16, train_wall=288, gb_free=19.9, wall=125742
2022-03-07 23:38:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:39:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:39:56 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 11.013 | nll_loss 9.953 | ppl 991.04 | wps 41786.4 | wpb 510.9 | bsz 1 | num_updates 38732 | best_loss 7.96
2022-03-07 23:39:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 38732 updates
2022-03-07 23:39:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:40:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:40:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 199 @ 38732 updates, score 11.013) (writing took 3.4215060202404857 seconds)
2022-03-07 23:40:00 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-07 23:40:00 | INFO | train | epoch 199 | loss 3.857 | nll_loss 2.131 | ppl 4.38 | wps 20603.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38732 | lr 0.000160681 | gnorm 1.15 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 125852
2022-03-07 23:40:00 | INFO | fairseq.trainer | begin training epoch 200
2022-03-07 23:40:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:43:31 | INFO | train_inner | epoch 200:     68 / 196 loss=3.846, nll_loss=2.119, ppl=4.34, wps=20330.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=38800, lr=0.00016054, gnorm=1.146, loss_scale=16, train_wall=291, gb_free=19.9, wall=126063
2022-03-07 23:45:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:48:44 | INFO | train_inner | epoch 200:    169 / 196 loss=3.869, nll_loss=2.145, ppl=4.42, wps=20910.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=38900, lr=0.000160334, gnorm=1.143, loss_scale=16, train_wall=291, gb_free=19.9, wall=126377
2022-03-07 23:50:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:50:13 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 10.984 | nll_loss 9.922 | ppl 970.26 | wps 41748.6 | wpb 510.9 | bsz 1 | num_updates 38927 | best_loss 7.96
2022-03-07 23:50:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 38927 updates
2022-03-07 23:50:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:50:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-07 23:50:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 200 @ 38927 updates, score 10.984) (writing took 3.3386176731437445 seconds)
2022-03-07 23:50:16 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-07 23:50:16 | INFO | train | epoch 200 | loss 3.856 | nll_loss 2.13 | ppl 4.38 | wps 20709.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 38927 | lr 0.000160278 | gnorm 1.144 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 126468
2022-03-07 23:50:16 | INFO | fairseq.trainer | begin training epoch 201
2022-03-07 23:50:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:52:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:54:06 | INFO | train_inner | epoch 201:     74 / 196 loss=3.843, nll_loss=2.115, ppl=4.33, wps=20337.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=39000, lr=0.000160128, gnorm=1.15, loss_scale=16, train_wall=291, gb_free=19.9, wall=126698
2022-03-07 23:59:16 | INFO | train_inner | epoch 201:    174 / 196 loss=3.867, nll_loss=2.143, ppl=4.42, wps=21122.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=39100, lr=0.000159923, gnorm=1.153, loss_scale=32, train_wall=288, gb_free=19.9, wall=127008
2022-03-07 23:59:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:00:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:00:29 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 11.002 | nll_loss 9.938 | ppl 981.24 | wps 41686 | wpb 510.9 | bsz 1 | num_updates 39121 | best_loss 7.96
2022-03-08 00:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 39121 updates
2022-03-08 00:00:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:00:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 201 @ 39121 updates, score 11.002) (writing took 3.3447811445221305 seconds)
2022-03-08 00:00:32 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-08 00:00:32 | INFO | train | epoch 201 | loss 3.854 | nll_loss 2.128 | ppl 4.37 | wps 20607.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39121 | lr 0.00015988 | gnorm 1.153 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 127084
2022-03-08 00:00:32 | INFO | fairseq.trainer | begin training epoch 202
2022-03-08 00:00:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:04:38 | INFO | train_inner | epoch 202:     79 / 196 loss=3.833, nll_loss=2.104, ppl=4.3, wps=20334.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39200, lr=0.000159719, gnorm=1.164, loss_scale=16, train_wall=291, gb_free=19.9, wall=127330
2022-03-08 00:06:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:09:51 | INFO | train_inner | epoch 202:    180 / 196 loss=3.876, nll_loss=2.153, ppl=4.45, wps=20910, ups=0.32, wpb=65536, bsz=128, num_updates=39300, lr=0.000159516, gnorm=1.168, loss_scale=16, train_wall=291, gb_free=19.9, wall=127643
2022-03-08 00:10:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:10:45 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 10.985 | nll_loss 9.924 | ppl 971.48 | wps 41756.7 | wpb 510.9 | bsz 1 | num_updates 39316 | best_loss 7.96
2022-03-08 00:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 39316 updates
2022-03-08 00:10:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:10:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 202 @ 39316 updates, score 10.985) (writing took 3.235404913313687 seconds)
2022-03-08 00:10:48 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-08 00:10:48 | INFO | train | epoch 202 | loss 3.852 | nll_loss 2.125 | ppl 4.36 | wps 20712.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 39316 | lr 0.000159483 | gnorm 1.169 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 127701
2022-03-08 00:10:48 | INFO | fairseq.trainer | begin training epoch 203
2022-03-08 00:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:12:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:15:12 | INFO | train_inner | epoch 203:     85 / 196 loss=3.829, nll_loss=2.099, ppl=4.28, wps=20342.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=39400, lr=0.000159313, gnorm=1.163, loss_scale=16, train_wall=291, gb_free=19.9, wall=127965
2022-03-08 00:19:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:20:26 | INFO | train_inner | epoch 203:    186 / 196 loss=3.871, nll_loss=2.147, ppl=4.43, wps=20907, ups=0.32, wpb=65532.4, bsz=128, num_updates=39500, lr=0.000159111, gnorm=1.145, loss_scale=16, train_wall=291, gb_free=19.9, wall=128278
2022-03-08 00:20:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:21:01 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 11.034 | nll_loss 9.973 | ppl 1004.94 | wps 41703.9 | wpb 510.9 | bsz 1 | num_updates 39510 | best_loss 7.96
2022-03-08 00:21:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 39510 updates
2022-03-08 00:21:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:21:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:21:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 203 @ 39510 updates, score 11.034) (writing took 3.280493024736643 seconds)
2022-03-08 00:21:05 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-08 00:21:05 | INFO | train | epoch 203 | loss 3.849 | nll_loss 2.122 | ppl 4.35 | wps 20604.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39510 | lr 0.000159091 | gnorm 1.151 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 128317
2022-03-08 00:21:05 | INFO | fairseq.trainer | begin training epoch 204
2022-03-08 00:21:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:25:44 | INFO | train_inner | epoch 204:     90 / 196 loss=3.824, nll_loss=2.093, ppl=4.27, wps=20531.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39600, lr=0.00015891, gnorm=1.142, loss_scale=16, train_wall=288, gb_free=19.9, wall=128596
2022-03-08 00:26:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:30:58 | INFO | train_inner | epoch 204:    191 / 196 loss=3.875, nll_loss=2.152, ppl=4.44, wps=20908.1, ups=0.32, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=1.158, loss_scale=16, train_wall=291, gb_free=19.9, wall=128910
2022-03-08 00:31:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:31:18 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 11.069 | nll_loss 10.005 | ppl 1027.34 | wps 41756.3 | wpb 510.9 | bsz 1 | num_updates 39705 | best_loss 7.96
2022-03-08 00:31:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 39705 updates
2022-03-08 00:31:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:31:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:31:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 204 @ 39705 updates, score 11.069) (writing took 3.2534512374550104 seconds)
2022-03-08 00:31:21 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-08 00:31:21 | INFO | train | epoch 204 | loss 3.847 | nll_loss 2.12 | ppl 4.35 | wps 20708.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 39705 | lr 0.0001587 | gnorm 1.149 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 128933
2022-03-08 00:31:21 | INFO | fairseq.trainer | begin training epoch 205
2022-03-08 00:31:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:33:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:36:19 | INFO | train_inner | epoch 205:     96 / 196 loss=3.819, nll_loss=2.088, ppl=4.25, wps=20337.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39800, lr=0.000158511, gnorm=1.137, loss_scale=16, train_wall=291, gb_free=19.9, wall=129231
2022-03-08 00:39:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:41:34 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 11.047 | nll_loss 9.984 | ppl 1012.91 | wps 41719.4 | wpb 510.9 | bsz 1 | num_updates 39899 | best_loss 7.96
2022-03-08 00:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 39899 updates
2022-03-08 00:41:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:41:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 205 @ 39899 updates, score 11.047) (writing took 3.2884636716917157 seconds)
2022-03-08 00:41:37 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-08 00:41:37 | INFO | train | epoch 205 | loss 3.846 | nll_loss 2.119 | ppl 4.34 | wps 20604.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39899 | lr 0.000158314 | gnorm 1.153 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 129549
2022-03-08 00:41:37 | INFO | fairseq.trainer | begin training epoch 206
2022-03-08 00:41:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:41:40 | INFO | train_inner | epoch 206:      1 / 196 loss=3.874, nll_loss=2.151, ppl=4.44, wps=20341.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=39900, lr=0.000158312, gnorm=1.168, loss_scale=16, train_wall=291, gb_free=19.9, wall=129553
2022-03-08 00:46:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:46:54 | INFO | train_inner | epoch 206:    102 / 196 loss=3.812, nll_loss=2.08, ppl=4.23, wps=20909.2, ups=0.32, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=1.143, loss_scale=16, train_wall=291, gb_free=19.9, wall=129866
2022-03-08 00:51:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:51:50 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 11.028 | nll_loss 9.966 | ppl 1000.24 | wps 41596.2 | wpb 510.9 | bsz 1 | num_updates 40094 | best_loss 7.96
2022-03-08 00:51:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 40094 updates
2022-03-08 00:51:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:51:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 00:51:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 206 @ 40094 updates, score 11.028) (writing took 3.2525488128885627 seconds)
2022-03-08 00:51:53 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-08 00:51:53 | INFO | train | epoch 206 | loss 3.843 | nll_loss 2.115 | ppl 4.33 | wps 20709.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 40094 | lr 0.000157928 | gnorm 1.15 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 130166
2022-03-08 00:51:53 | INFO | fairseq.trainer | begin training epoch 207
2022-03-08 00:51:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:52:12 | INFO | train_inner | epoch 207:      6 / 196 loss=3.871, nll_loss=2.148, ppl=4.43, wps=20533.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=40100, lr=0.000157917, gnorm=1.156, loss_scale=16, train_wall=288, gb_free=19.9, wall=130184
2022-03-08 00:53:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:57:26 | INFO | train_inner | epoch 207:    107 / 196 loss=3.811, nll_loss=2.078, ppl=4.22, wps=20901.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=40200, lr=0.00015772, gnorm=1.155, loss_scale=16, train_wall=291, gb_free=19.9, wall=130498
2022-03-08 01:00:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:02:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:02:06 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 11.066 | nll_loss 10.006 | ppl 1028.55 | wps 41753.4 | wpb 510.9 | bsz 1 | num_updates 40288 | best_loss 7.96
2022-03-08 01:02:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 40288 updates
2022-03-08 01:02:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:02:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:02:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 207 @ 40288 updates, score 11.066) (writing took 3.271077231504023 seconds)
2022-03-08 01:02:10 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-08 01:02:10 | INFO | train | epoch 207 | loss 3.84 | nll_loss 2.112 | ppl 4.32 | wps 20599.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40288 | lr 0.000157548 | gnorm 1.16 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 130782
2022-03-08 01:02:10 | INFO | fairseq.trainer | begin training epoch 208
2022-03-08 01:02:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:02:47 | INFO | train_inner | epoch 208:     12 / 196 loss=3.867, nll_loss=2.143, ppl=4.42, wps=20336.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=40300, lr=0.000157524, gnorm=1.163, loss_scale=16, train_wall=291, gb_free=19.9, wall=130819
2022-03-08 01:07:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:08:01 | INFO | train_inner | epoch 208:    113 / 196 loss=3.815, nll_loss=2.083, ppl=4.24, wps=20903.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=40400, lr=0.000157329, gnorm=1.158, loss_scale=16, train_wall=291, gb_free=19.9, wall=131133
2022-03-08 01:12:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:12:23 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 10.984 | nll_loss 9.92 | ppl 968.82 | wps 41527.8 | wpb 510.9 | bsz 1 | num_updates 40483 | best_loss 7.96
2022-03-08 01:12:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 40483 updates
2022-03-08 01:12:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:12:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:12:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 208 @ 40483 updates, score 10.984) (writing took 3.2633609203621745 seconds)
2022-03-08 01:12:26 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-08 01:12:26 | INFO | train | epoch 208 | loss 3.838 | nll_loss 2.11 | ppl 4.32 | wps 20712 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 40483 | lr 0.000157168 | gnorm 1.154 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 131398
2022-03-08 01:12:26 | INFO | fairseq.trainer | begin training epoch 209
2022-03-08 01:12:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:13:19 | INFO | train_inner | epoch 209:     17 / 196 loss=3.857, nll_loss=2.132, ppl=4.38, wps=20544.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=40500, lr=0.000157135, gnorm=1.152, loss_scale=16, train_wall=288, gb_free=19.9, wall=131451
2022-03-08 01:13:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:18:32 | INFO | train_inner | epoch 209:    118 / 196 loss=3.811, nll_loss=2.078, ppl=4.22, wps=20900, ups=0.32, wpb=65532.4, bsz=128, num_updates=40600, lr=0.000156941, gnorm=1.153, loss_scale=16, train_wall=291, gb_free=19.9, wall=131764
2022-03-08 01:20:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:22:39 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 11.037 | nll_loss 9.971 | ppl 1003.9 | wps 41801.1 | wpb 510.9 | bsz 1 | num_updates 40677 | best_loss 7.96
2022-03-08 01:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 40677 updates
2022-03-08 01:22:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 209 @ 40677 updates, score 11.037) (writing took 3.2555986307561398 seconds)
2022-03-08 01:22:42 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-08 01:22:42 | INFO | train | epoch 209 | loss 3.835 | nll_loss 2.106 | ppl 4.31 | wps 20602.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40677 | lr 0.000156793 | gnorm 1.16 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 132014
2022-03-08 01:22:42 | INFO | fairseq.trainer | begin training epoch 210
2022-03-08 01:22:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:23:54 | INFO | train_inner | epoch 210:     23 / 196 loss=3.857, nll_loss=2.131, ppl=4.38, wps=20310.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=40700, lr=0.000156748, gnorm=1.167, loss_scale=16, train_wall=291, gb_free=19.9, wall=132086
2022-03-08 01:27:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:29:10 | INFO | train_inner | epoch 210:    124 / 196 loss=3.82, nll_loss=2.089, ppl=4.25, wps=20731.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=40800, lr=0.000156556, gnorm=1.145, loss_scale=16, train_wall=292, gb_free=19.9, wall=132402
2022-03-08 01:32:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:33:00 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 11.054 | nll_loss 9.994 | ppl 1019.94 | wps 41442.7 | wpb 510.9 | bsz 1 | num_updates 40872 | best_loss 7.96
2022-03-08 01:33:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 40872 updates
2022-03-08 01:33:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:33:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:33:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 210 @ 40872 updates, score 11.054) (writing took 3.2536370772868395 seconds)
2022-03-08 01:33:03 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-08 01:33:03 | INFO | train | epoch 210 | loss 3.835 | nll_loss 2.106 | ppl 4.3 | wps 20544.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 40872 | lr 0.000156418 | gnorm 1.151 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 132636
2022-03-08 01:33:03 | INFO | fairseq.trainer | begin training epoch 211
2022-03-08 01:33:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:34:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:34:34 | INFO | train_inner | epoch 211:     29 / 196 loss=3.846, nll_loss=2.119, ppl=4.34, wps=20218.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=40900, lr=0.000156365, gnorm=1.158, loss_scale=16, train_wall=291, gb_free=19.9, wall=132726
2022-03-08 01:39:44 | INFO | train_inner | epoch 211:    129 / 196 loss=3.819, nll_loss=2.088, ppl=4.25, wps=21109.4, ups=0.32, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=1.16, loss_scale=16, train_wall=288, gb_free=19.9, wall=133036
2022-03-08 01:40:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:43:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:43:16 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 11.043 | nll_loss 9.983 | ppl 1011.83 | wps 41384.2 | wpb 510.9 | bsz 1 | num_updates 41066 | best_loss 7.96
2022-03-08 01:43:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 41066 updates
2022-03-08 01:43:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:43:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 211 @ 41066 updates, score 11.043) (writing took 3.2793777510523796 seconds)
2022-03-08 01:43:20 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-08 01:43:20 | INFO | train | epoch 211 | loss 3.832 | nll_loss 2.103 | ppl 4.3 | wps 20598.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41066 | lr 0.000156048 | gnorm 1.162 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 133252
2022-03-08 01:43:20 | INFO | fairseq.trainer | begin training epoch 212
2022-03-08 01:43:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:45:05 | INFO | train_inner | epoch 212:     34 / 196 loss=3.834, nll_loss=2.106, ppl=4.3, wps=20333.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41100, lr=0.000155984, gnorm=1.155, loss_scale=16, train_wall=291, gb_free=19.9, wall=133358
2022-03-08 01:47:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:50:19 | INFO | train_inner | epoch 212:    135 / 196 loss=3.826, nll_loss=2.096, ppl=4.27, wps=20913.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=41200, lr=0.000155794, gnorm=1.158, loss_scale=16, train_wall=291, gb_free=19.9, wall=133671
2022-03-08 01:53:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:53:33 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 11 | nll_loss 9.935 | ppl 978.81 | wps 41376 | wpb 510.9 | bsz 1 | num_updates 41261 | best_loss 7.96
2022-03-08 01:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 41261 updates
2022-03-08 01:53:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:53:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 01:53:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 212 @ 41261 updates, score 11.0) (writing took 3.345762461423874 seconds)
2022-03-08 01:53:36 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-08 01:53:36 | INFO | train | epoch 212 | loss 3.829 | nll_loss 2.1 | ppl 4.29 | wps 20711.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 41261 | lr 0.000155679 | gnorm 1.157 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 133868
2022-03-08 01:53:36 | INFO | fairseq.trainer | begin training epoch 213
2022-03-08 01:53:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:54:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:55:40 | INFO | train_inner | epoch 213:     40 / 196 loss=3.835, nll_loss=2.107, ppl=4.31, wps=20335.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=41300, lr=0.000155606, gnorm=1.16, loss_scale=16, train_wall=291, gb_free=19.9, wall=133992
2022-03-08 02:00:51 | INFO | train_inner | epoch 213:    140 / 196 loss=3.822, nll_loss=2.092, ppl=4.26, wps=21083.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=41400, lr=0.000155417, gnorm=1.176, loss_scale=16, train_wall=289, gb_free=19.9, wall=134303
2022-03-08 02:01:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:03:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:03:50 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 11.063 | nll_loss 10.008 | ppl 1029.39 | wps 41460.8 | wpb 510.9 | bsz 1 | num_updates 41455 | best_loss 7.96
2022-03-08 02:03:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 41455 updates
2022-03-08 02:03:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:03:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:03:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 213 @ 41455 updates, score 11.063) (writing took 3.3339650612324476 seconds)
2022-03-08 02:03:53 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-08 02:03:53 | INFO | train | epoch 213 | loss 3.827 | nll_loss 2.097 | ppl 4.28 | wps 20562.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41455 | lr 0.000155314 | gnorm 1.168 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 134486
2022-03-08 02:03:53 | INFO | fairseq.trainer | begin training epoch 214
2022-03-08 02:03:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:06:13 | INFO | train_inner | epoch 214:     45 / 196 loss=3.829, nll_loss=2.1, ppl=4.29, wps=20288.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=41500, lr=0.00015523, gnorm=1.164, loss_scale=16, train_wall=291, gb_free=19.9, wall=134625
2022-03-08 02:08:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:11:27 | INFO | train_inner | epoch 214:    146 / 196 loss=3.82, nll_loss=2.089, ppl=4.25, wps=20905.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=41600, lr=0.000155043, gnorm=1.162, loss_scale=16, train_wall=291, gb_free=19.9, wall=134939
2022-03-08 02:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:14:06 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 11.071 | nll_loss 10.012 | ppl 1032.21 | wps 41725 | wpb 510.9 | bsz 1 | num_updates 41650 | best_loss 7.96
2022-03-08 02:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 41650 updates
2022-03-08 02:14:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:14:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 214 @ 41650 updates, score 11.071) (writing took 3.382179629057646 seconds)
2022-03-08 02:14:10 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-08 02:14:10 | INFO | train | epoch 214 | loss 3.825 | nll_loss 2.095 | ppl 4.27 | wps 20702.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 41650 | lr 0.00015495 | gnorm 1.163 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 135102
2022-03-08 02:14:10 | INFO | fairseq.trainer | begin training epoch 215
2022-03-08 02:14:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:15:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:16:48 | INFO | train_inner | epoch 215:     51 / 196 loss=3.827, nll_loss=2.098, ppl=4.28, wps=20327.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41700, lr=0.000154857, gnorm=1.152, loss_scale=16, train_wall=291, gb_free=19.9, wall=135260
2022-03-08 02:21:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:22:02 | INFO | train_inner | epoch 215:    152 / 196 loss=3.821, nll_loss=2.091, ppl=4.26, wps=20915.4, ups=0.32, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=1.182, loss_scale=16, train_wall=291, gb_free=19.9, wall=135574
2022-03-08 02:24:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:24:23 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 11.006 | nll_loss 9.941 | ppl 982.88 | wps 41650.8 | wpb 510.9 | bsz 1 | num_updates 41844 | best_loss 7.96
2022-03-08 02:24:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 41844 updates
2022-03-08 02:24:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:24:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:24:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 215 @ 41844 updates, score 11.006) (writing took 3.3520504385232925 seconds)
2022-03-08 02:24:26 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-08 02:24:26 | INFO | train | epoch 215 | loss 3.823 | nll_loss 2.093 | ppl 4.27 | wps 20607 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41844 | lr 0.000154591 | gnorm 1.173 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 135718
2022-03-08 02:24:26 | INFO | fairseq.trainer | begin training epoch 216
2022-03-08 02:24:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:27:20 | INFO | train_inner | epoch 216:     56 / 196 loss=3.819, nll_loss=2.088, ppl=4.25, wps=20528.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=41900, lr=0.000154487, gnorm=1.169, loss_scale=16, train_wall=288, gb_free=19.9, wall=135892
2022-03-08 02:28:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:32:33 | INFO | train_inner | epoch 216:    157 / 196 loss=3.832, nll_loss=2.103, ppl=4.3, wps=20908.7, ups=0.32, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=1.152, loss_scale=16, train_wall=291, gb_free=19.9, wall=136206
2022-03-08 02:34:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:34:39 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 11.032 | nll_loss 9.975 | ppl 1006.62 | wps 41610.6 | wpb 510.9 | bsz 1 | num_updates 42039 | best_loss 7.96
2022-03-08 02:34:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 42039 updates
2022-03-08 02:34:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:34:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:34:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 216 @ 42039 updates, score 11.032) (writing took 3.323566308245063 seconds)
2022-03-08 02:34:42 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-08 02:34:42 | INFO | train | epoch 216 | loss 3.821 | nll_loss 2.09 | ppl 4.26 | wps 20706.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42039 | lr 0.000154232 | gnorm 1.154 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 136335
2022-03-08 02:34:42 | INFO | fairseq.trainer | begin training epoch 217
2022-03-08 02:34:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:35:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 02:37:58 | INFO | train_inner | epoch 217:     63 / 196 loss=3.812, nll_loss=2.081, ppl=4.23, wps=20145.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42100, lr=0.00015412, gnorm=1.165, loss_scale=8, train_wall=293, gb_free=19.9, wall=136530
2022-03-08 02:43:08 | INFO | train_inner | epoch 217:    163 / 196 loss=3.827, nll_loss=2.097, ppl=4.28, wps=21125.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=42200, lr=0.000153937, gnorm=1.155, loss_scale=8, train_wall=288, gb_free=19.9, wall=136840
2022-03-08 02:44:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:44:55 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 11.064 | nll_loss 10.004 | ppl 1026.87 | wps 41820.7 | wpb 510.9 | bsz 1 | num_updates 42233 | best_loss 7.96
2022-03-08 02:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 42233 updates
2022-03-08 02:44:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 217 @ 42233 updates, score 11.064) (writing took 3.335029987618327 seconds)
2022-03-08 02:44:58 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-08 02:44:58 | INFO | train | epoch 217 | loss 3.819 | nll_loss 2.088 | ppl 4.25 | wps 20612.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42233 | lr 0.000153877 | gnorm 1.165 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 136951
2022-03-08 02:44:58 | INFO | fairseq.trainer | begin training epoch 218
2022-03-08 02:44:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:48:26 | INFO | train_inner | epoch 218:     67 / 196 loss=3.805, nll_loss=2.072, ppl=4.2, wps=20539.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=42300, lr=0.000153755, gnorm=1.166, loss_scale=16, train_wall=288, gb_free=19.9, wall=137159
2022-03-08 02:50:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:53:40 | INFO | train_inner | epoch 218:    168 / 196 loss=3.831, nll_loss=2.102, ppl=4.29, wps=20912.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=42400, lr=0.000153574, gnorm=1.172, loss_scale=16, train_wall=291, gb_free=19.9, wall=137472
2022-03-08 02:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:55:11 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 11.039 | nll_loss 9.981 | ppl 1010.39 | wps 41993.2 | wpb 510.9 | bsz 1 | num_updates 42428 | best_loss 7.96
2022-03-08 02:55:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 42428 updates
2022-03-08 02:55:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:55:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 02:55:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 218 @ 42428 updates, score 11.039) (writing took 3.342893434688449 seconds)
2022-03-08 02:55:14 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-08 02:55:14 | INFO | train | epoch 218 | loss 3.817 | nll_loss 2.086 | ppl 4.25 | wps 20712.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42428 | lr 0.000153523 | gnorm 1.169 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 137567
2022-03-08 02:55:14 | INFO | fairseq.trainer | begin training epoch 219
2022-03-08 02:55:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:57:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 02:59:01 | INFO | train_inner | epoch 219:     73 / 196 loss=3.802, nll_loss=2.069, ppl=4.2, wps=20343, ups=0.31, wpb=65367, bsz=127.7, num_updates=42500, lr=0.000153393, gnorm=1.175, loss_scale=8, train_wall=290, gb_free=19.9, wall=137793
2022-03-08 03:04:11 | INFO | train_inner | epoch 219:    173 / 196 loss=3.833, nll_loss=2.105, ppl=4.3, wps=21121.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=42600, lr=0.000153213, gnorm=1.161, loss_scale=16, train_wall=288, gb_free=19.9, wall=138104
2022-03-08 03:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:05:27 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 11.069 | nll_loss 10.017 | ppl 1036.02 | wps 41550.1 | wpb 510.9 | bsz 1 | num_updates 42623 | best_loss 7.96
2022-03-08 03:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 42623 updates
2022-03-08 03:05:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:05:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 219 @ 42623 updates, score 11.069) (writing took 3.36825954541564 seconds)
2022-03-08 03:05:31 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-08 03:05:31 | INFO | train | epoch 219 | loss 3.815 | nll_loss 2.084 | ppl 4.24 | wps 20713.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42623 | lr 0.000153172 | gnorm 1.164 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 138183
2022-03-08 03:05:31 | INFO | fairseq.trainer | begin training epoch 220
2022-03-08 03:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:09:30 | INFO | train_inner | epoch 220:     77 / 196 loss=3.795, nll_loss=2.061, ppl=4.17, wps=20530.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42700, lr=0.000153033, gnorm=1.155, loss_scale=16, train_wall=288, gb_free=19.9, wall=138422
2022-03-08 03:10:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:14:43 | INFO | train_inner | epoch 220:    178 / 196 loss=3.834, nll_loss=2.106, ppl=4.3, wps=20915.5, ups=0.32, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=1.175, loss_scale=16, train_wall=291, gb_free=19.9, wall=138735
2022-03-08 03:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:15:43 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 11.042 | nll_loss 9.987 | ppl 1015.01 | wps 41611.2 | wpb 510.9 | bsz 1 | num_updates 42818 | best_loss 7.96
2022-03-08 03:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 42818 updates
2022-03-08 03:15:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:15:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 220 @ 42818 updates, score 11.042) (writing took 3.3414715761318803 seconds)
2022-03-08 03:15:47 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-08 03:15:47 | INFO | train | epoch 220 | loss 3.813 | nll_loss 2.081 | ppl 4.23 | wps 20711.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42818 | lr 0.000152822 | gnorm 1.163 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 138799
2022-03-08 03:15:47 | INFO | fairseq.trainer | begin training epoch 221
2022-03-08 03:15:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:17:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:20:04 | INFO | train_inner | epoch 221:     83 / 196 loss=3.793, nll_loss=2.058, ppl=4.17, wps=20338.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=42900, lr=0.000152676, gnorm=1.157, loss_scale=16, train_wall=290, gb_free=19.9, wall=139057
2022-03-08 03:24:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:25:18 | INFO | train_inner | epoch 221:    184 / 196 loss=3.831, nll_loss=2.103, ppl=4.29, wps=20912.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=43000, lr=0.000152499, gnorm=1.166, loss_scale=16, train_wall=291, gb_free=19.9, wall=139370
2022-03-08 03:25:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:26:00 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 11.054 | nll_loss 10.002 | ppl 1025.46 | wps 41880.7 | wpb 510.9 | bsz 1 | num_updates 43012 | best_loss 7.96
2022-03-08 03:26:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 43012 updates
2022-03-08 03:26:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:26:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:26:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 221 @ 43012 updates, score 11.054) (writing took 3.3775319773703814 seconds)
2022-03-08 03:26:03 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-08 03:26:03 | INFO | train | epoch 221 | loss 3.811 | nll_loss 2.079 | ppl 4.23 | wps 20607.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43012 | lr 0.000152477 | gnorm 1.162 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 139415
2022-03-08 03:26:03 | INFO | fairseq.trainer | begin training epoch 222
2022-03-08 03:26:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:30:36 | INFO | train_inner | epoch 222:     88 / 196 loss=3.785, nll_loss=2.049, ppl=4.14, wps=20530.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=43100, lr=0.000152322, gnorm=1.152, loss_scale=16, train_wall=288, gb_free=19.9, wall=139688
2022-03-08 03:31:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:35:49 | INFO | train_inner | epoch 222:    189 / 196 loss=3.838, nll_loss=2.11, ppl=4.32, wps=20917.7, ups=0.32, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=1.18, loss_scale=16, train_wall=291, gb_free=19.9, wall=140002
2022-03-08 03:36:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:36:16 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 11.058 | nll_loss 9.997 | ppl 1021.7 | wps 41581.2 | wpb 510.9 | bsz 1 | num_updates 43207 | best_loss 7.96
2022-03-08 03:36:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 43207 updates
2022-03-08 03:36:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 222 @ 43207 updates, score 11.058) (writing took 3.350161487236619 seconds)
2022-03-08 03:36:19 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-08 03:36:19 | INFO | train | epoch 222 | loss 3.809 | nll_loss 2.076 | ppl 4.22 | wps 20712.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 43207 | lr 0.000152133 | gnorm 1.167 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 140031
2022-03-08 03:36:19 | INFO | fairseq.trainer | begin training epoch 223
2022-03-08 03:36:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:37:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:41:11 | INFO | train_inner | epoch 223:     94 / 196 loss=3.78, nll_loss=2.044, ppl=4.12, wps=20331.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43300, lr=0.000151969, gnorm=1.181, loss_scale=16, train_wall=291, gb_free=19.9, wall=140323
2022-03-08 03:44:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:46:24 | INFO | train_inner | epoch 223:    195 / 196 loss=3.835, nll_loss=2.107, ppl=4.31, wps=20914.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=43400, lr=0.000151794, gnorm=1.189, loss_scale=16, train_wall=291, gb_free=19.9, wall=140637
2022-03-08 03:46:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:46:32 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 11.081 | nll_loss 10.028 | ppl 1044.27 | wps 41873.4 | wpb 510.9 | bsz 1 | num_updates 43401 | best_loss 7.96
2022-03-08 03:46:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 43401 updates
2022-03-08 03:46:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:46:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:46:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 223 @ 43401 updates, score 11.081) (writing took 3.295168266631663 seconds)
2022-03-08 03:46:35 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-08 03:46:35 | INFO | train | epoch 223 | loss 3.807 | nll_loss 2.074 | ppl 4.21 | wps 20606.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43401 | lr 0.000151792 | gnorm 1.185 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 140647
2022-03-08 03:46:35 | INFO | fairseq.trainer | begin training epoch 224
2022-03-08 03:46:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:51:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:51:46 | INFO | train_inner | epoch 224:    100 / 196 loss=3.776, nll_loss=2.039, ppl=4.11, wps=20349.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=43500, lr=0.00015162, gnorm=1.159, loss_scale=16, train_wall=290, gb_free=19.9, wall=140958
2022-03-08 03:56:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:56:48 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 11.074 | nll_loss 10.021 | ppl 1038.67 | wps 41966.4 | wpb 510.9 | bsz 1 | num_updates 43596 | best_loss 7.96
2022-03-08 03:56:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 43596 updates
2022-03-08 03:56:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:56:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 03:56:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 224 @ 43596 updates, score 11.074) (writing took 3.3485824782401323 seconds)
2022-03-08 03:56:51 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-08 03:56:51 | INFO | train | epoch 224 | loss 3.805 | nll_loss 2.072 | ppl 4.21 | wps 20719.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 43596 | lr 0.000151453 | gnorm 1.169 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 141263
2022-03-08 03:56:51 | INFO | fairseq.trainer | begin training epoch 225
2022-03-08 03:56:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:57:04 | INFO | train_inner | epoch 225:      4 / 196 loss=3.831, nll_loss=2.103, ppl=4.3, wps=20545, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43600, lr=0.000151446, gnorm=1.18, loss_scale=16, train_wall=288, gb_free=19.9, wall=141276
2022-03-08 03:58:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:02:17 | INFO | train_inner | epoch 225:    105 / 196 loss=3.776, nll_loss=2.039, ppl=4.11, wps=20899.4, ups=0.32, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=1.158, loss_scale=16, train_wall=291, gb_free=19.9, wall=141589
2022-03-08 04:04:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:06:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:07:04 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 11.063 | nll_loss 10.009 | ppl 1030.26 | wps 41405 | wpb 510.9 | bsz 1 | num_updates 43790 | best_loss 7.96
2022-03-08 04:07:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 43790 updates
2022-03-08 04:07:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:07:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 225 @ 43790 updates, score 11.063) (writing took 3.368701801635325 seconds)
2022-03-08 04:07:08 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-08 04:07:08 | INFO | train | epoch 225 | loss 3.802 | nll_loss 2.07 | ppl 4.2 | wps 20600.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43790 | lr 0.000151117 | gnorm 1.164 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 141880
2022-03-08 04:07:08 | INFO | fairseq.trainer | begin training epoch 226
2022-03-08 04:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:07:39 | INFO | train_inner | epoch 226:     10 / 196 loss=3.825, nll_loss=2.096, ppl=4.28, wps=20337, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43800, lr=0.000151099, gnorm=1.171, loss_scale=16, train_wall=290, gb_free=19.9, wall=141911
2022-03-08 04:11:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:12:52 | INFO | train_inner | epoch 226:    111 / 196 loss=3.782, nll_loss=2.046, ppl=4.13, wps=20914, ups=0.32, wpb=65532.4, bsz=128, num_updates=43900, lr=0.000150927, gnorm=1.168, loss_scale=16, train_wall=291, gb_free=19.9, wall=142224
2022-03-08 04:17:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:17:20 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 11.034 | nll_loss 9.975 | ppl 1006.5 | wps 41061.2 | wpb 510.9 | bsz 1 | num_updates 43985 | best_loss 7.96
2022-03-08 04:17:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 43985 updates
2022-03-08 04:17:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:17:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:17:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 226 @ 43985 updates, score 11.034) (writing took 3.3443266386166215 seconds)
2022-03-08 04:17:24 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-08 04:17:24 | INFO | train | epoch 226 | loss 3.802 | nll_loss 2.069 | ppl 4.2 | wps 20712.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 43985 | lr 0.000150781 | gnorm 1.176 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 142496
2022-03-08 04:17:24 | INFO | fairseq.trainer | begin training epoch 227
2022-03-08 04:17:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:18:10 | INFO | train_inner | epoch 227:     15 / 196 loss=3.82, nll_loss=2.089, ppl=4.26, wps=20536.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=44000, lr=0.000150756, gnorm=1.181, loss_scale=16, train_wall=288, gb_free=19.9, wall=142543
2022-03-08 04:18:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:23:24 | INFO | train_inner | epoch 227:    116 / 196 loss=3.78, nll_loss=2.044, ppl=4.12, wps=20907.9, ups=0.32, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=1.169, loss_scale=16, train_wall=291, gb_free=19.9, wall=142856
2022-03-08 04:25:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:27:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:27:37 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 11.063 | nll_loss 10.011 | ppl 1031.57 | wps 41587.2 | wpb 510.9 | bsz 1 | num_updates 44179 | best_loss 7.96
2022-03-08 04:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 44179 updates
2022-03-08 04:27:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:27:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 227 @ 44179 updates, score 11.063) (writing took 3.3070557825267315 seconds)
2022-03-08 04:27:40 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-08 04:27:40 | INFO | train | epoch 227 | loss 3.799 | nll_loss 2.066 | ppl 4.19 | wps 20604.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44179 | lr 0.00015045 | gnorm 1.17 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 143112
2022-03-08 04:27:40 | INFO | fairseq.trainer | begin training epoch 228
2022-03-08 04:27:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:28:45 | INFO | train_inner | epoch 228:     21 / 196 loss=3.817, nll_loss=2.087, ppl=4.25, wps=20334.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44200, lr=0.000150414, gnorm=1.172, loss_scale=16, train_wall=291, gb_free=19.9, wall=143177
2022-03-08 04:32:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:33:59 | INFO | train_inner | epoch 228:    122 / 196 loss=3.777, nll_loss=2.04, ppl=4.11, wps=20900, ups=0.32, wpb=65532.4, bsz=128, num_updates=44300, lr=0.000150244, gnorm=1.162, loss_scale=16, train_wall=291, gb_free=19.9, wall=143491
2022-03-08 04:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:37:53 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 11.092 | nll_loss 10.035 | ppl 1049.38 | wps 41749.2 | wpb 510.9 | bsz 1 | num_updates 44374 | best_loss 7.96
2022-03-08 04:37:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 44374 updates
2022-03-08 04:37:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:37:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:37:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 228 @ 44374 updates, score 11.092) (writing took 3.3691380927339196 seconds)
2022-03-08 04:37:56 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-08 04:37:56 | INFO | train | epoch 228 | loss 3.797 | nll_loss 2.063 | ppl 4.18 | wps 20707.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 44374 | lr 0.000150119 | gnorm 1.167 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 143728
2022-03-08 04:37:56 | INFO | fairseq.trainer | begin training epoch 229
2022-03-08 04:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:39:20 | INFO | train_inner | epoch 229:     27 / 196 loss=3.811, nll_loss=2.08, ppl=4.23, wps=20341.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=44400, lr=0.000150075, gnorm=1.171, loss_scale=16, train_wall=290, gb_free=19.9, wall=143812
2022-03-08 04:44:31 | INFO | train_inner | epoch 229:    127 / 196 loss=3.784, nll_loss=2.049, ppl=4.14, wps=21109.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=44500, lr=0.000149906, gnorm=1.159, loss_scale=16, train_wall=288, gb_free=19.9, wall=144123
2022-03-08 04:45:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:48:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:48:09 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 11.044 | nll_loss 9.986 | ppl 1014.4 | wps 41679.2 | wpb 510.9 | bsz 1 | num_updates 44568 | best_loss 7.96
2022-03-08 04:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 44568 updates
2022-03-08 04:48:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:48:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 229 @ 44568 updates, score 11.044) (writing took 3.3543989146128297 seconds)
2022-03-08 04:48:13 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-08 04:48:13 | INFO | train | epoch 229 | loss 3.796 | nll_loss 2.062 | ppl 4.18 | wps 20598 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44568 | lr 0.000149792 | gnorm 1.17 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 144345
2022-03-08 04:48:13 | INFO | fairseq.trainer | begin training epoch 230
2022-03-08 04:48:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:49:52 | INFO | train_inner | epoch 230:     32 / 196 loss=3.806, nll_loss=2.074, ppl=4.21, wps=20337.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=44600, lr=0.000149738, gnorm=1.181, loss_scale=16, train_wall=291, gb_free=19.9, wall=144444
2022-03-08 04:52:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:55:05 | INFO | train_inner | epoch 230:    133 / 196 loss=3.783, nll_loss=2.048, ppl=4.13, wps=20909.7, ups=0.32, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=1.169, loss_scale=16, train_wall=291, gb_free=19.9, wall=144758
2022-03-08 04:58:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:58:25 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 11.046 | nll_loss 9.993 | ppl 1019.29 | wps 41606.3 | wpb 510.9 | bsz 1 | num_updates 44763 | best_loss 7.96
2022-03-08 04:58:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 44763 updates
2022-03-08 04:58:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:58:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 04:58:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 230 @ 44763 updates, score 11.046) (writing took 3.371551346965134 seconds)
2022-03-08 04:58:29 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-08 04:58:29 | INFO | train | epoch 230 | loss 3.794 | nll_loss 2.06 | ppl 4.17 | wps 20714 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 44763 | lr 0.000149465 | gnorm 1.176 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 144961
2022-03-08 04:58:29 | INFO | fairseq.trainer | begin training epoch 231
2022-03-08 04:58:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:59:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:00:27 | INFO | train_inner | epoch 231:     38 / 196 loss=3.804, nll_loss=2.072, ppl=4.2, wps=20341.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44800, lr=0.000149404, gnorm=1.181, loss_scale=16, train_wall=290, gb_free=19.9, wall=145079
2022-03-08 05:05:37 | INFO | train_inner | epoch 231:    138 / 196 loss=3.784, nll_loss=2.049, ppl=4.14, wps=21116.7, ups=0.32, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=1.185, loss_scale=16, train_wall=288, gb_free=19.9, wall=145389
2022-03-08 05:05:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:08:42 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 11.043 | nll_loss 9.986 | ppl 1014.08 | wps 41875.6 | wpb 510.9 | bsz 1 | num_updates 44957 | best_loss 7.96
2022-03-08 05:08:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 44957 updates
2022-03-08 05:08:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:08:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 231 @ 44957 updates, score 11.043) (writing took 3.338694415986538 seconds)
2022-03-08 05:08:45 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-08 05:08:45 | INFO | train | epoch 231 | loss 3.792 | nll_loss 2.058 | ppl 4.16 | wps 20606.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44957 | lr 0.000149142 | gnorm 1.185 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 145577
2022-03-08 05:08:45 | INFO | fairseq.trainer | begin training epoch 232
2022-03-08 05:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:10:58 | INFO | train_inner | epoch 232:     43 / 196 loss=3.795, nll_loss=2.062, ppl=4.17, wps=20339.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45000, lr=0.000149071, gnorm=1.185, loss_scale=16, train_wall=290, gb_free=19.9, wall=145711
2022-03-08 05:12:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:16:12 | INFO | train_inner | epoch 232:    144 / 196 loss=3.786, nll_loss=2.052, ppl=4.15, wps=20911.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=45100, lr=0.000148906, gnorm=1.17, loss_scale=16, train_wall=291, gb_free=19.9, wall=146024
2022-03-08 05:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:18:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:18:58 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 11.056 | nll_loss 9.997 | ppl 1021.91 | wps 41710.6 | wpb 510.9 | bsz 1 | num_updates 45151 | best_loss 7.96
2022-03-08 05:18:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 45151 updates
2022-03-08 05:18:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:19:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:19:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 232 @ 45151 updates, score 11.056) (writing took 3.339345178566873 seconds)
2022-03-08 05:19:01 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-08 05:19:01 | INFO | train | epoch 232 | loss 3.79 | nll_loss 2.056 | ppl 4.16 | wps 20606.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45151 | lr 0.000148822 | gnorm 1.176 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 146193
2022-03-08 05:19:01 | INFO | fairseq.trainer | begin training epoch 233
2022-03-08 05:19:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:21:33 | INFO | train_inner | epoch 233:     49 / 196 loss=3.791, nll_loss=2.057, ppl=4.16, wps=20340.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45200, lr=0.000148741, gnorm=1.184, loss_scale=8, train_wall=290, gb_free=19.9, wall=146345
2022-03-08 05:26:44 | INFO | train_inner | epoch 233:    149 / 196 loss=3.788, nll_loss=2.054, ppl=4.15, wps=21110.6, ups=0.32, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=1.203, loss_scale=16, train_wall=288, gb_free=19.9, wall=146656
2022-03-08 05:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:29:14 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 11.025 | nll_loss 9.973 | ppl 1005.15 | wps 41673.5 | wpb 510.9 | bsz 1 | num_updates 45347 | best_loss 7.96
2022-03-08 05:29:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 45347 updates
2022-03-08 05:29:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:29:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:29:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 233 @ 45347 updates, score 11.025) (writing took 3.291612533852458 seconds)
2022-03-08 05:29:17 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-08 05:29:17 | INFO | train | epoch 233 | loss 3.789 | nll_loss 2.055 | ppl 4.16 | wps 20816.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 45347 | lr 0.0001485 | gnorm 1.191 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 146809
2022-03-08 05:29:17 | INFO | fairseq.trainer | begin training epoch 234
2022-03-08 05:29:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:30:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:32:05 | INFO | train_inner | epoch 234:     54 / 196 loss=3.787, nll_loss=2.053, ppl=4.15, wps=20335.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=45400, lr=0.000148413, gnorm=1.176, loss_scale=16, train_wall=291, gb_free=19.9, wall=146977
2022-03-08 05:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:37:19 | INFO | train_inner | epoch 234:    155 / 196 loss=3.79, nll_loss=2.055, ppl=4.16, wps=20899.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=45500, lr=0.00014825, gnorm=1.18, loss_scale=16, train_wall=291, gb_free=19.9, wall=147291
2022-03-08 05:39:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:39:30 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 11.06 | nll_loss 10.005 | ppl 1027.28 | wps 41770 | wpb 510.9 | bsz 1 | num_updates 45541 | best_loss 7.96
2022-03-08 05:39:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 45541 updates
2022-03-08 05:39:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:39:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:39:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 234 @ 45541 updates, score 11.06) (writing took 3.355141489766538 seconds)
2022-03-08 05:39:34 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-08 05:39:34 | INFO | train | epoch 234 | loss 3.786 | nll_loss 2.051 | ppl 4.14 | wps 20596 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45541 | lr 0.000148183 | gnorm 1.177 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 147426
2022-03-08 05:39:34 | INFO | fairseq.trainer | begin training epoch 235
2022-03-08 05:39:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:42:37 | INFO | train_inner | epoch 235:     59 / 196 loss=3.777, nll_loss=2.041, ppl=4.11, wps=20527.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=45600, lr=0.000148087, gnorm=1.175, loss_scale=16, train_wall=288, gb_free=19.9, wall=147609
2022-03-08 05:43:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:47:50 | INFO | train_inner | epoch 235:    160 / 196 loss=3.796, nll_loss=2.063, ppl=4.18, wps=20907.6, ups=0.32, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=1.192, loss_scale=16, train_wall=291, gb_free=19.9, wall=147923
2022-03-08 05:49:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:49:47 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 11.085 | nll_loss 10.036 | ppl 1050.08 | wps 41757.2 | wpb 510.9 | bsz 1 | num_updates 45736 | best_loss 7.96
2022-03-08 05:49:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 45736 updates
2022-03-08 05:49:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:49:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 05:49:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 235 @ 45736 updates, score 11.085) (writing took 3.3322744332253933 seconds)
2022-03-08 05:49:50 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-08 05:49:50 | INFO | train | epoch 235 | loss 3.784 | nll_loss 2.049 | ppl 4.14 | wps 20704.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 45736 | lr 0.000147867 | gnorm 1.181 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 148042
2022-03-08 05:49:50 | INFO | fairseq.trainer | begin training epoch 236
2022-03-08 05:49:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:50:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:53:12 | INFO | train_inner | epoch 236:     65 / 196 loss=3.772, nll_loss=2.035, ppl=4.1, wps=20329, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=45800, lr=0.000147764, gnorm=1.173, loss_scale=16, train_wall=291, gb_free=19.9, wall=148244
2022-03-08 05:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:58:25 | INFO | train_inner | epoch 236:    166 / 196 loss=3.796, nll_loss=2.063, ppl=4.18, wps=20912.6, ups=0.32, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=1.179, loss_scale=16, train_wall=291, gb_free=19.9, wall=148558
2022-03-08 05:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:00:03 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 11.059 | nll_loss 10.009 | ppl 1030.21 | wps 41562.3 | wpb 510.9 | bsz 1 | num_updates 45930 | best_loss 7.96
2022-03-08 06:00:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 45930 updates
2022-03-08 06:00:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 06:00:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 06:00:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 236 @ 45930 updates, score 11.059) (writing took 3.3529977444559336 seconds)
2022-03-08 06:00:06 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-08 06:00:06 | INFO | train | epoch 236 | loss 3.782 | nll_loss 2.047 | ppl 4.13 | wps 20603.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45930 | lr 0.000147554 | gnorm 1.18 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 148659
2022-03-08 06:00:06 | INFO | fairseq.trainer | begin training epoch 237
2022-03-08 06:00:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:03:44 | INFO | train_inner | epoch 237:     70 / 196 loss=3.765, nll_loss=2.027, ppl=4.07, wps=20525.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=46000, lr=0.000147442, gnorm=1.175, loss_scale=16, train_wall=288, gb_free=19.9, wall=148876
2022-03-08 06:04:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:08:57 | INFO | train_inner | epoch 237:    171 / 196 loss=3.795, nll_loss=2.062, ppl=4.18, wps=20913.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=46100, lr=0.000147282, gnorm=1.169, loss_scale=16, train_wall=291, gb_free=19.9, wall=149189
2022-03-08 06:10:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:10:19 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 11.061 | nll_loss 10 | ppl 1023.89 | wps 41719.8 | wpb 510.9 | bsz 1 | num_updates 46125 | best_loss 7.96
2022-03-08 06:10:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 46125 updates
2022-03-08 06:10:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 06:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 06:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 237 @ 46125 updates, score 11.061) (writing took 3.3787949979305267 seconds)
2022-03-08 06:10:23 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-08 06:10:23 | INFO | train | epoch 237 | loss 3.781 | nll_loss 2.045 | ppl 4.13 | wps 20707.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 46125 | lr 0.000147242 | gnorm 1.17 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 149275
2022-03-08 06:10:23 | INFO | fairseq.trainer | begin training epoch 238
2022-03-08 06:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:11:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:14:19 | INFO | train_inner | epoch 238:     76 / 196 loss=3.762, nll_loss=2.024, ppl=4.07, wps=20334.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=46200, lr=0.000147122, gnorm=1.176, loss_scale=16, train_wall=291, gb_free=19.9, wall=149511
2022-03-08 06:17:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:19:32 | INFO | train_inner | epoch 238:    177 / 196 loss=3.798, nll_loss=2.065, ppl=4.18, wps=20915.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=46300, lr=0.000146964, gnorm=1.189, loss_scale=16, train_wall=291, gb_free=19.9, wall=149824
2022-03-08 06:20:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:20:35 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 11.096 | nll_loss 10.044 | ppl 1055.72 | wps 41423.2 | wpb 510.9 | bsz 1 | num_updates 46319 | best_loss 7.96
2022-03-08 06:20:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 46319 updates
2022-03-08 06:20:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 06:20:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt
2022-03-08 06:20:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.08_#2/checkpoint_last.pt (epoch 238 @ 46319 updates, score 11.096) (writing took 3.1897239554673433 seconds)
2022-03-08 06:20:39 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-08 06:20:39 | INFO | train | epoch 238 | loss 3.778 | nll_loss 2.043 | ppl 4.12 | wps 20612.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 46319 | lr 0.000146933 | gnorm 1.182 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 149891
2022-03-08 06:20:39 | INFO | fairseq.trainer | begin training epoch 239
2022-03-08 06:20:39 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
