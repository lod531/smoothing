Sender: LSF System <lsfadmin@eu-g3-024>
Subject: Job 202287084: <w2_jelinek_0.13_-0.03_0.9> in cluster <euler> Exited

Job <w2_jelinek_0.13_-0.03_0.9> was submitted from host <eu-login-22> by user <andriusb> in cluster <euler> at Fri Jan 28 07:38:49 2022
Job was executed on host(s) <eu-g3-024>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Jan 28 07:39:21 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Jan 28 07:39:21 2022
Terminated at Fri Jan 28 07:39:31 2022
Results reported at Fri Jan 28 07:39:31 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-2-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.12, -0.02, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 16 --save-interval 40 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   4.27 sec.
    Max Memory :                                 399 MB
    Average Memory :                             366.67 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               19601.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                5
    Run time :                                   9 sec.
    Turnaround time :                            42 sec.

The output (if any) follows:

2022-01-28 07:39:30 | ERROR | fairseq.dataclass.utils | Error when composing. Overrides: ['common.no_progress_bar=False', 'common.log_interval=100', 'common.log_format=null', 'common.log_file=null', 'common.tensorboard_logdir=null', 'common.wandb_project=null', 'common.azureml_logging=False', 'common.seed=1', 'common.cpu=False', 'common.tpu=False', 'common.bf16=False', 'common.memory_efficient_bf16=False', 'common.fp16=False', 'common.memory_efficient_fp16=False', 'common.fp16_no_flatten_grads=False', 'common.fp16_init_scale=128', 'common.fp16_scale_window=null', 'common.fp16_scale_tolerance=0.0', 'common.on_cpu_convert_precision=False', 'common.min_loss_scale=0.0001', 'common.threshold_loss_scale=null', 'common.amp=False', 'common.amp_batch_retries=2', 'common.amp_init_scale=128', 'common.amp_scale_window=null', 'common.user_dir=null', 'common.empty_cache_freq=0', 'common.all_gather_list_size=16384', 'common.model_parallel_size=1', 'common.quantization_config_path=null', 'common.profile=False', 'common.reset_logging=False', 'common.suppress_crashes=False', 'common.use_plasma_view=False', "common.plasma_path='/tmp/plasma'", 'common_eval.path=null', 'common_eval.post_process=null', 'common_eval.quiet=False', "common_eval.model_overrides='{}'", 'common_eval.results_path=null', 'distributed_training.distributed_world_size=1', 'distributed_training.distributed_num_procs=1', 'distributed_training.distributed_rank=0', "distributed_training.distributed_backend='nccl'", 'distributed_training.distributed_init_method=null', 'distributed_training.distributed_port=-1', 'distributed_training.device_id=0', 'distributed_training.distributed_no_spawn=False', "distributed_training.ddp_backend='pytorch_ddp'", "distributed_training.ddp_comm_hook='none'", 'distributed_training.bucket_cap_mb=25', 'distributed_training.fix_batches_to_gpus=False', 'distributed_training.find_unused_parameters=False', 'distributed_training.gradient_as_bucket_view=False', 'distributed_training.fast_stat_sync=False', 'distributed_training.heartbeat_timeout=-1', 'distributed_training.broadcast_buffers=False', 'distributed_training.slowmo_momentum=null', "distributed_training.slowmo_algorithm='LocalSGD'", 'distributed_training.localsgd_frequency=3', 'distributed_training.nprocs_per_node=1', 'distributed_training.pipeline_model_parallel=False', 'distributed_training.pipeline_balance=null', 'distributed_training.pipeline_devices=null', 'distributed_training.pipeline_chunks=0', 'distributed_training.pipeline_encoder_balance=null', 'distributed_training.pipeline_encoder_devices=null', 'distributed_training.pipeline_decoder_balance=null', 'distributed_training.pipeline_decoder_devices=null', "distributed_training.pipeline_checkpoint='never'", "distributed_training.zero_sharding='none'", 'distributed_training.fp16=False', 'distributed_training.memory_efficient_fp16=False', 'distributed_training.tpu=False', 'distributed_training.no_reshard_after_forward=False', 'distributed_training.fp32_reduce_scatter=False', 'distributed_training.cpu_offload=False', 'distributed_training.use_sharded_state=False', 'dataset.num_workers=1', 'dataset.skip_invalid_size_inputs_valid_test=False', 'dataset.max_tokens=2048', 'dataset.batch_size=null', 'dataset.required_batch_size_multiple=8', 'dataset.required_seq_len_multiple=1', 'dataset.dataset_impl=null', 'dataset.data_buffer_size=10', "dataset.train_subset='train'", "dataset.valid_subset='valid'", 'dataset.combine_valid_subsets=null', 'dataset.ignore_unused_valid_subsets=False', 'dataset.validate_interval=1', 'dataset.validate_interval_updates=0', 'dataset.validate_after_updates=0', 'dataset.fixed_validation_seed=null', 'dataset.disable_validation=False', 'dataset.max_tokens_valid=2048', 'dataset.batch_size_valid=null', 'dataset.max_valid_steps=null', 'dataset.curriculum=0', "dataset.gen_subset='test'", 'dataset.num_shards=1', 'dataset.shard_id=0', 'optimization.max_epoch=0', 'optimization.max_update=50000', 'optimization.stop_time_hours=0.0', 'optimization.clip_norm=0.0', 'optimization.sentence_avg=False', 'optimization.update_freq=[16]', 'optimization.lr=[0.0005]', 'optimization.stop_min_lr=-1.0', 'optimization.use_bmuf=False', "checkpoint.save_dir='/cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9'", "checkpoint.restore_file='checkpoint_last.pt'", 'checkpoint.finetune_from_model=null', 'checkpoint.reset_dataloader=False', 'checkpoint.reset_lr_scheduler=False', 'checkpoint.reset_meters=False', 'checkpoint.reset_optimizer=False', "checkpoint.optimizer_overrides='{}'", 'checkpoint.save_interval=40', 'checkpoint.save_interval_updates=0', 'checkpoint.keep_interval_updates=-1', 'checkpoint.keep_interval_updates_pattern=-1', 'checkpoint.keep_last_epochs=-1', 'checkpoint.keep_best_checkpoints=-1', 'checkpoint.no_save=False', 'checkpoint.no_epoch_checkpoints=False', 'checkpoint.no_last_checkpoints=False', 'checkpoint.no_save_optimizer_state=False', "checkpoint.best_checkpoint_metric='loss'", 'checkpoint.maximize_best_checkpoint_metric=False', 'checkpoint.patience=-1', "checkpoint.checkpoint_suffix=''", 'checkpoint.checkpoint_shard_count=1', 'checkpoint.load_checkpoint_on_all_dp_ranks=False', 'checkpoint.write_checkpoints_asynchronously=False', 'checkpoint.model_parallel_size=1', 'bmuf.block_lr=1.0', 'bmuf.block_momentum=0.875', 'bmuf.global_sync_iter=50', 'bmuf.warmup_iterations=500', 'bmuf.use_nbm=False', 'bmuf.average_sync=False', 'bmuf.distributed_world_size=1', 'generation.beam=5', 'generation.nbest=1', 'generation.max_len_a=0.0', 'generation.max_len_b=200', 'generation.min_len=1', 'generation.match_source_len=False', 'generation.unnormalized=False', 'generation.no_early_stop=False', 'generation.no_beamable_mm=False', 'generation.lenpen=1.0', 'generation.unkpen=0.0', 'generation.replace_unk=null', 'generation.sacrebleu=False', 'generation.score_reference=False', 'generation.prefix_size=0', 'generation.no_repeat_ngram_size=0', 'generation.sampling=False', 'generation.sampling_topk=-1', 'generation.sampling_topp=-1.0', 'generation.constraints=null', 'generation.temperature=1.0', 'generation.diverse_beam_groups=-1', 'generation.diverse_beam_strength=0.5', 'generation.diversity_rate=-1.0', 'generation.print_alignment=null', 'generation.print_step=False', 'generation.lm_path=null', 'generation.lm_weight=0.0', 'generation.iter_decode_eos_penalty=0.0', 'generation.iter_decode_max_iter=10', 'generation.iter_decode_force_max_iter=False', 'generation.iter_decode_with_beam=1', 'generation.iter_decode_with_external_reranker=False', 'generation.retain_iter_history=False', 'generation.retain_dropout=False', 'generation.retain_dropout_modules=null', 'generation.decoding_format=null', 'generation.no_seed_provided=False', 'eval_lm.output_word_probs=False', 'eval_lm.output_word_stats=False', 'eval_lm.context_window=0', 'eval_lm.softmax_batch=9223372036854775807', 'interactive.buffer_size=0', "interactive.input='-'", 'ema.store_ema=False', 'ema.ema_decay=0.9999', 'ema.ema_start_update=0', 'ema.ema_seed_model=null', 'ema.ema_update_freq=1', 'ema.ema_fp32=False', 'task=language_modeling', 'task._name=language_modeling', "task.data='data-bin/wikitext-2-raw-full'", "task.sample_break_mode='none'", 'task.tokens_per_sample=512', 'task.output_dictionary_size=-1', 'task.self_target=False', 'task.future_target=False', 'task.past_target=False', 'task.add_bos_token=False', 'task.max_target_positions=null', "task.shorten_method='none'", "task.shorten_data_split_list=''", 'task.pad_to_fixed_length=False', 'task.pad_to_fixed_bsz=False', 'task.seed=1', 'task.batch_size=null', 'task.batch_size_valid=null', 'task.dataset_impl=null', 'task.data_buffer_size=10', 'task.tpu=False', 'task.use_plasma_view=False', "task.plasma_path='/tmp/plasma'", 'criterion=jelinek_mercer_smoothing', 'criterion._name=jelinek_mercer_smoothing', "criterion.alphas='(0.12, -0.02, 0.9)'", 'criterion.jelinek_n=2', 'criterion.sentence_avg=False', 'optimizer=adam', 'optimizer._name=adam', "optimizer.adam_betas='(0.9, 0.98)'", 'optimizer.adam_eps=1e-08', 'optimizer.weight_decay=0.5', 'optimizer.use_old_adam=False', 'optimizer.fp16_adam_stats=False', 'optimizer.tpu=False', 'optimizer.lr=[0.0005]', 'lr_scheduler=inverse_sqrt', 'lr_scheduler._name=inverse_sqrt', 'lr_scheduler.warmup_updates=4000', 'lr_scheduler.warmup_init_lr=1e-07', 'lr_scheduler.lr=[0.0005]', 'scoring=bleu', 'scoring._name=bleu', 'scoring.pad=1', 'scoring.eos=2', 'scoring.unk=3', 'model=transformer_lm', 'model._name=transformer_lm', "model.activation_fn='relu'", 'model.dropout=0.5', 'model.attention_dropout=0.0', 'model.activation_dropout=0.0', 'model.relu_dropout=0.0', 'model.decoder_embed_dim=512', 'model.decoder_output_dim=512', 'model.decoder_input_dim=512', 'model.decoder_ffn_embed_dim=2048', 'model.decoder_layers=6', 'model.decoder_attention_heads=8', 'model.decoder_normalize_before=False', 'model.no_decoder_final_norm=False', 'model.adaptive_softmax_cutoff=null', 'model.adaptive_softmax_dropout=0.0', 'model.adaptive_softmax_factor=4.0', 'model.no_token_positional_embeddings=False', 'model.share_decoder_input_output_embed=True', 'model.character_embeddings=False', "model.character_filters='[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]'", 'model.character_embedding_dim=4', 'model.char_embedder_highway_layers=2', 'model.adaptive_input=False', 'model.adaptive_input_factor=4.0', 'model.adaptive_input_cutoff=null', 'model.tie_adaptive_weights=False', 'model.tie_adaptive_proj=False', 'model.decoder_learned_pos=False', 'model.layernorm_embedding=False', 'model.no_scale_embedding=False', 'model.checkpoint_activations=False', 'model.offload_activations=False', 'model.decoder_layerdrop=0.0', 'model.decoder_layers_to_keep=null', 'model.quant_noise_pq=0.0', 'model.quant_noise_pq_block_size=8', 'model.quant_noise_scalar=0.0', 'model.min_params_to_wrap=100000000', 'model.base_layers=0', 'model.base_sublayers=1', 'model.base_shuffle=1', 'model.scale_fc=False', 'model.scale_attn=False', 'model.scale_heads=False', 'model.scale_resids=False', 'model.add_bos_token=False', 'model.tokens_per_sample=512', 'model.max_target_positions=null', 'model.tpu=False']
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 532, in cli_main
    cfg = convert_namespace_to_omegaconf(args)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/dataclass/utils.py", line 389, in convert_namespace_to_omegaconf
    composed_cfg = compose("config", overrides=overrides, strict=False)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/experimental/compose.py", line 31, in compose
    cfg = gh.hydra.compose_config(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/hydra.py", line 507, in compose_config
    cfg = self.config_loader.load_configuration(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 151, in load_configuration
    return self._load_configuration(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 256, in _load_configuration
    cfg = self._merge_defaults_into_config(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 804, in _merge_defaults_into_config
    hydra_cfg = merge_defaults_list_into_config(hydra_cfg, system_list)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 777, in merge_defaults_list_into_config
    merged_cfg = self._merge_config(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 695, in _merge_config
    loaded_cfg, _ = self._load_config_impl(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 589, in _load_config_impl
    ret = self.repository.load_config(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/config_repository.py", line 45, in load_config
    ret = source.load_config(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/_internal/core_plugins/importlib_resources_config_source.py", line 64, in load_config
    config=self._embed_config(cfg, header["package"]),
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/hydra/plugins/config_source.py", line 201, in _embed_config
    OmegaConf.update(cfg, package, node, merge=False)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/omegaconf.py", line 613, in update
    root.__setattr__(last_key, value)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 282, in __setattr__
    self.__set_impl(key, value)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 266, in __set_impl
    self._set_item_impl(key, value)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/basecontainer.py", line 475, in _set_item_impl
    assign(key, value)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/basecontainer.py", line 452, in assign
    v = copy.deepcopy(value_to_assign)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 93, in __deepcopy__
    res.__dict__[k] = copy.deepcopy(v, memo=memo)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/copy.py", line 230, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 91, in __deepcopy__
    res = DictConfig({})
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 81, in __init__
    self._set_value(content)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 527, in _set_value
    self._validate_set(key=None, value=value)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 143, in _validate_set
    self._validate_set_merge_impl(key, value, is_assign=True)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 148, in _validate_set_merge_impl
    vk = get_value_kind(value)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/_utils.py", line 352, in get_value_kind
    ) -> Union[ValueKind, Tuple[ValueKind, Optional[List[Match[str]]]]]:
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/typing.py", line 802, in __getitem__
    def __getitem__(self, params):
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-024>
Subject: Job 202287093: <w2_jelinek_0.13_-0.03_0.9> in cluster <euler> Exited

Job <w2_jelinek_0.13_-0.03_0.9> was submitted from host <eu-login-22> by user <andriusb> in cluster <euler> at Fri Jan 28 07:39:43 2022
Job was executed on host(s) <eu-g3-024>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Jan 28 07:39:53 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Jan 28 07:39:53 2022
Terminated at Sat Jan 29 03:40:02 2022
Results reported at Sat Jan 29 03:40:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-2-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.13, -0.03, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 16 --save-interval 40 --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   71958.00 sec.
    Max Memory :                                 5757 MB
    Average Memory :                             3348.73 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14243.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   72009 sec.
    Turnaround time :                            72019 sec.

The output (if any) follows:

2022-01-28 07:39:58 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 40, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-2-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.13, -0.03, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-28 07:39:58 | INFO | fairseq.tasks.language_modeling | dictionary: 76624 types
2022-01-28 07:39:59 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
Calculating frequency stats:
  0%|          | 0/36718 [00:00<?, ?it/s]  3%|▎         | 1128/36718 [00:00<00:03, 11255.80it/s]  6%|▌         | 2254/36718 [00:00<00:03, 10336.82it/s]  9%|▉         | 3459/36718 [00:00<00:03, 11066.90it/s] 13%|█▎        | 4675/36718 [00:00<00:02, 11477.16it/s] 16%|█▋        | 5979/36718 [00:00<00:02, 12025.80it/s] 20%|█▉        | 7186/36718 [00:00<00:02, 11263.99it/s] 23%|██▎       | 8323/36718 [00:00<00:02, 11202.09it/s] 26%|██▌       | 9451/36718 [00:00<00:02, 11085.52it/s] 29%|██▉       | 10565/36718 [00:00<00:02, 11062.32it/s] 32%|███▏      | 11675/36718 [00:01<00:02, 10930.75it/s] 35%|███▍      | 12815/36718 [00:01<00:02, 11065.84it/s] 38%|███▊      | 13995/36718 [00:01<00:02, 11266.58it/s] 41%|████▏     | 15208/36718 [00:01<00:01, 11522.35it/s] 45%|████▍     | 16362/36718 [00:01<00:01, 11006.20it/s] 48%|████▊     | 17491/36718 [00:01<00:01, 11083.48it/s] 51%|█████     | 18604/36718 [00:01<00:01, 10911.77it/s] 54%|█████▍    | 19879/36718 [00:01<00:01, 11442.87it/s] 57%|█████▋    | 21028/36718 [00:01<00:01, 11186.63it/s] 60%|██████    | 22151/36718 [00:01<00:01, 10887.56it/s] 64%|██████▎   | 23348/36718 [00:02<00:01, 11194.27it/s] 67%|██████▋   | 24769/36718 [00:02<00:00, 12067.69it/s] 71%|███████   | 25982/36718 [00:02<00:00, 11805.14it/s] 74%|███████▍  | 27168/36718 [00:02<00:00, 11109.12it/s] 77%|███████▋  | 28305/36718 [00:02<00:00, 11181.96it/s] 80%|████████  | 29431/36718 [00:02<00:00, 11109.79it/s] 83%|████████▎ | 30549/36718 [00:02<00:00, 11119.39it/s] 86%|████████▌ | 31665/36718 [00:02<00:00, 10987.52it/s] 89%|████████▉ | 32767/36718 [00:02<00:00, 10657.87it/s] 92%|█████████▏| 33836/36718 [00:03<00:00, 10578.40it/s] 95%|█████████▌| 35010/36718 [00:03<00:00, 10908.27it/s] 98%|█████████▊| 36104/36718 [00:03<00:00, 10840.42it/s]100%|██████████| 36718/36718 [00:03<00:00, 11127.97it/s]

gathering stats for n=1
  0%|          | 0/36718 [00:00<?, ?it/s]  5%|▌         | 1898/36718 [00:00<00:01, 18978.39it/s] 11%|█         | 4027/36718 [00:00<00:01, 20337.53it/s] 17%|█▋        | 6330/36718 [00:00<00:01, 21556.07it/s] 23%|██▎       | 8486/36718 [00:00<00:01, 20211.77it/s] 29%|██▊       | 10535/36718 [00:00<00:01, 20306.12it/s] 34%|███▍      | 12594/36718 [00:00<00:01, 20392.05it/s] 40%|████      | 14704/36718 [00:00<00:01, 20610.62it/s] 46%|████▌     | 16770/36718 [00:00<00:00, 20360.61it/s] 52%|█████▏    | 18910/36718 [00:00<00:00, 20678.33it/s] 57%|█████▋    | 21000/36718 [00:01<00:00, 20742.75it/s] 63%|██████▎   | 23077/36718 [00:01<00:00, 20699.83it/s] 69%|██████▉   | 25472/36718 [00:01<00:00, 21675.94it/s] 75%|███████▌  | 27642/36718 [00:01<00:00, 20674.38it/s] 81%|████████  | 29788/36718 [00:01<00:00, 20894.97it/s] 87%|████████▋ | 31886/36718 [00:01<00:00, 20387.37it/s] 92%|█████████▏| 33933/36718 [00:01<00:00, 20201.82it/s] 98%|█████████▊| 35998/36718 [00:01<00:00, 20331.14it/s]100%|██████████| 36718/36718 [00:01<00:00, 20564.52it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 72.22it/s]2022-01-28 07:40:13 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(76624, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=76624, bias=False)
  )
)
2022-01-28 07:40:13 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-01-28 07:40:13 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-01-28 07:40:13 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-01-28 07:40:13 | INFO | fairseq_cli.train | num. shared model params: 58,145,792 (num. trained: 58,145,792)
2022-01-28 07:40:13 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-28 07:40:13 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-2-raw-full/valid
2022-01-28 07:40:13 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-28 07:40:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-28 07:40:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-01-28 07:40:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-28 07:40:13 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-28 07:40:13 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-01-28 07:40:13 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint_last.pt
2022-01-28 07:40:13 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint_last.pt
2022-01-28 07:40:13 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-28 07:40:13 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
2022-01-28 07:40:13 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-01-28 07:40:13 | INFO | fairseq.trainer | begin training epoch 1
2022-01-28 07:40:13 | INFO | fairseq_cli.train | Start iterating over samples

2022-01-28 07:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/home/andriusb/fq/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-01-28 07:46:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.748 | ppl 27525.5 | wps 8010.5 | wpb 2034.1 | bsz 4 | num_updates 64
2022-01-28 07:46:09 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-28 07:46:09 | INFO | train | epoch 001 | loss 16.13 | ppl 71700.7 | wps 5920.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 64 | lr 8.0984e-06 | gnorm 3.131 | train_wall 324 | gb_free 6.1 | wall 356
KL Stats: Epoch 1 Divergences: Uniform: 0.5169244850152765 Unigram: 3.6864665222337285
2022-01-28 07:46:09 | INFO | fairseq.trainer | begin training epoch 2
2022-01-28 07:46:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:49:12 | INFO | train_inner | epoch 002:     36 / 64 loss=15.608, ppl=49960.4, wps=6097.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=100, lr=1.25975e-05, gnorm=2.565, train_wall=507, gb_free=6.1, wall=539
2022-01-28 07:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:52:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.781 | ppl 14073.1 | wps 8003.3 | wpb 2034.1 | bsz 4 | num_updates 128
2022-01-28 07:52:01 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-28 07:52:01 | INFO | train | epoch 002 | loss 14.481 | ppl 22868.1 | wps 5932.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 128 | lr 1.60968e-05 | gnorm 1.442 | train_wall 323 | gb_free 6.1 | wall 708
KL Stats: Epoch 2 Divergences: Uniform: 0.5316284961082435 Unigram: 2.418873189133747
2022-01-28 07:52:01 | INFO | fairseq.trainer | begin training epoch 3
2022-01-28 07:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:57:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 07:57:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.986 | ppl 8114.34 | wps 7993.4 | wpb 2034.1 | bsz 4 | num_updates 192
2022-01-28 07:57:53 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-28 07:57:53 | INFO | train | epoch 003 | loss 13.61 | ppl 12506.9 | wps 5938.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 192 | lr 2.40952e-05 | gnorm 1.161 | train_wall 323 | gb_free 6.1 | wall 1060
KL Stats: Epoch 3 Divergences: Uniform: 0.5107932935489108 Unigram: 1.7402920584862782
2022-01-28 07:57:53 | INFO | fairseq.trainer | begin training epoch 4
2022-01-28 07:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 07:58:34 | INFO | train_inner | epoch 004:      8 / 64 loss=13.739, ppl=13670.5, wps=5809, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=200, lr=2.5095e-05, gnorm=1.189, train_wall=504, gb_free=6.1, wall=1100
2022-01-28 08:03:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:03:45 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.175 | ppl 4623.72 | wps 8013.6 | wpb 2034.1 | bsz 4 | num_updates 256
2022-01-28 08:03:45 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-28 08:03:45 | INFO | train | epoch 004 | loss 12.692 | ppl 6615.44 | wps 5935.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 256 | lr 3.20936e-05 | gnorm 0.922 | train_wall 323 | gb_free 6.1 | wall 1411
KL Stats: Epoch 4 Divergences: Uniform: 0.5878531604251277 Unigram: 1.1387668840101015
2022-01-28 08:03:45 | INFO | fairseq.trainer | begin training epoch 5
2022-01-28 08:03:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:07:28 | INFO | train_inner | epoch 005:     44 / 64 loss=12.357, ppl=5246.13, wps=6109.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=300, lr=3.75925e-05, gnorm=0.814, train_wall=505, gb_free=6.1, wall=1635
2022-01-28 08:09:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:09:36 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.674 | ppl 3266.83 | wps 7980.4 | wpb 2034.1 | bsz 4 | num_updates 320
2022-01-28 08:09:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-28 08:09:36 | INFO | train | epoch 005 | loss 11.93 | ppl 3902.41 | wps 5941 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 320 | lr 4.0092e-05 | gnorm 0.669 | train_wall 322 | gb_free 6.1 | wall 1763
KL Stats: Epoch 5 Divergences: Uniform: 0.8086663543834085 Unigram: 0.6981961098555401
2022-01-28 08:09:36 | INFO | fairseq.trainer | begin training epoch 6
2022-01-28 08:09:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:15:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:15:28 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.426 | ppl 2750.73 | wps 8015.1 | wpb 2034.1 | bsz 4 | num_updates 384
2022-01-28 08:15:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-28 08:15:28 | INFO | train | epoch 006 | loss 11.515 | ppl 2926.21 | wps 5938.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 384 | lr 4.80904e-05 | gnorm 0.576 | train_wall 323 | gb_free 6.1 | wall 2115
KL Stats: Epoch 6 Divergences: Uniform: 1.0793817175582838 Unigram: 0.5151864799244691
2022-01-28 08:15:28 | INFO | fairseq.trainer | begin training epoch 7
2022-01-28 08:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:16:49 | INFO | train_inner | epoch 007:     16 / 64 loss=11.536, ppl=2969.92, wps=5811.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=400, lr=5.009e-05, gnorm=0.574, train_wall=504, gb_free=6.1, wall=2196
2022-01-28 08:20:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:21:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.277 | ppl 2481.64 | wps 7994.7 | wpb 2034.1 | bsz 4 | num_updates 448
2022-01-28 08:21:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-28 08:21:20 | INFO | train | epoch 007 | loss 11.317 | ppl 2550.97 | wps 5936.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 448 | lr 5.60888e-05 | gnorm 0.524 | train_wall 323 | gb_free 6.1 | wall 2467
KL Stats: Epoch 7 Divergences: Uniform: 1.2770906235978068 Unigram: 0.5422378315747816
2022-01-28 08:21:20 | INFO | fairseq.trainer | begin training epoch 8
2022-01-28 08:21:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:25:45 | INFO | train_inner | epoch 008:     52 / 64 loss=11.254, ppl=2441.79, wps=6103.1, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=500, lr=6.25875e-05, gnorm=0.518, train_wall=506, gb_free=6.1, wall=2732
2022-01-28 08:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:27:12 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.166 | ppl 2297.14 | wps 8022.7 | wpb 2034.1 | bsz 4 | num_updates 512
2022-01-28 08:27:12 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-28 08:27:12 | INFO | train | epoch 008 | loss 11.2 | ppl 2352.56 | wps 5926.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 512 | lr 6.40872e-05 | gnorm 0.514 | train_wall 323 | gb_free 6.1 | wall 2819
KL Stats: Epoch 8 Divergences: Uniform: 1.3737081502524155 Unigram: 0.6398919955444928
2022-01-28 08:27:12 | INFO | fairseq.trainer | begin training epoch 9
2022-01-28 08:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:32:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:33:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.042 | ppl 2107.79 | wps 8002.3 | wpb 2034.1 | bsz 4 | num_updates 576
2022-01-28 08:33:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-28 08:33:05 | INFO | train | epoch 009 | loss 11.086 | ppl 2174.5 | wps 5923.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 576 | lr 7.20856e-05 | gnorm 0.485 | train_wall 324 | gb_free 6.1 | wall 3172
KL Stats: Epoch 9 Divergences: Uniform: 1.4078041581751979 Unigram: 0.7738907050938274
2022-01-28 08:33:05 | INFO | fairseq.trainer | begin training epoch 10
2022-01-28 08:33:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:35:07 | INFO | train_inner | epoch 010:     24 / 64 loss=11.076, ppl=2159.43, wps=5801.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=600, lr=7.5085e-05, gnorm=0.486, train_wall=505, gb_free=6.1, wall=3294
2022-01-28 08:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:38:56 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.93 | ppl 1950.7 | wps 7979.3 | wpb 2034.1 | bsz 4 | num_updates 640
2022-01-28 08:38:56 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-28 08:38:56 | INFO | train | epoch 010 | loss 10.969 | ppl 2004.54 | wps 5938.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 640 | lr 8.0084e-05 | gnorm 0.485 | train_wall 323 | gb_free 6.1 | wall 3523
KL Stats: Epoch 10 Divergences: Uniform: 1.426890956524791 Unigram: 0.9245305716382944
2022-01-28 08:38:56 | INFO | fairseq.trainer | begin training epoch 11
2022-01-28 08:38:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:44:02 | INFO | train_inner | epoch 011:     60 / 64 loss=10.888, ppl=1895.2, wps=6110.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=700, lr=8.75825e-05, gnorm=0.495, train_wall=505, gb_free=6.1, wall=3828
2022-01-28 08:44:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:44:48 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.818 | ppl 1805.82 | wps 8010.4 | wpb 2034.1 | bsz 4 | num_updates 704
2022-01-28 08:44:48 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-28 08:44:48 | INFO | train | epoch 011 | loss 10.846 | ppl 1840.35 | wps 5941.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 704 | lr 8.80824e-05 | gnorm 0.497 | train_wall 322 | gb_free 6.1 | wall 3875
KL Stats: Epoch 11 Divergences: Uniform: 1.4421891599986165 Unigram: 1.078376338686141
2022-01-28 08:44:48 | INFO | fairseq.trainer | begin training epoch 12
2022-01-28 08:44:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:50:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:50:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.702 | ppl 1665.63 | wps 7998 | wpb 2034.1 | bsz 4 | num_updates 768
2022-01-28 08:50:40 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-28 08:50:40 | INFO | train | epoch 012 | loss 10.724 | ppl 1691.1 | wps 5936.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 768 | lr 9.60808e-05 | gnorm 0.489 | train_wall 323 | gb_free 6.1 | wall 4227
KL Stats: Epoch 12 Divergences: Uniform: 1.4521320596357161 Unigram: 1.2272913270244268
2022-01-28 08:50:40 | INFO | fairseq.trainer | begin training epoch 13
2022-01-28 08:50:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 08:53:23 | INFO | train_inner | epoch 013:     32 / 64 loss=10.699, ppl=1662.21, wps=5807.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=800, lr=0.00010008, gnorm=0.503, train_wall=504, gb_free=6.1, wall=4390
2022-01-28 08:56:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 08:56:32 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.612 | ppl 1564.59 | wps 8034.9 | wpb 2034.1 | bsz 4 | num_updates 832
2022-01-28 08:56:32 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-28 08:56:32 | INFO | train | epoch 013 | loss 10.606 | ppl 1558.17 | wps 5933.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 832 | lr 0.000104079 | gnorm 0.52 | train_wall 323 | gb_free 6.1 | wall 4579
KL Stats: Epoch 13 Divergences: Uniform: 1.4738050576479658 Unigram: 1.3605327805147878
2022-01-28 08:56:32 | INFO | fairseq.trainer | begin training epoch 14
2022-01-28 08:56:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:01:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:02:24 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.515 | ppl 1463.14 | wps 7974.4 | wpb 2034.1 | bsz 4 | num_updates 896
2022-01-28 09:02:24 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-28 09:02:24 | INFO | train | epoch 014 | loss 10.492 | ppl 1440.31 | wps 5933.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 896 | lr 0.000112078 | gnorm 0.559 | train_wall 323 | gb_free 6.1 | wall 4931
KL Stats: Epoch 14 Divergences: Uniform: 1.50056003201447 Unigram: 1.48222559809526
2022-01-28 09:02:24 | INFO | fairseq.trainer | begin training epoch 15
2022-01-28 09:02:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:02:44 | INFO | train_inner | epoch 015:      4 / 64 loss=10.515, ppl=1463.58, wps=5807.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=900, lr=0.000112578, gnorm=0.537, train_wall=504, gb_free=6.1, wall=4951
2022-01-28 09:07:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:08:16 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.441 | ppl 1390.37 | wps 8006.1 | wpb 2034.1 | bsz 4 | num_updates 960
2022-01-28 09:08:16 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-28 09:08:16 | INFO | train | epoch 015 | loss 10.379 | ppl 1331.79 | wps 5932.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 960 | lr 0.000120076 | gnorm 0.552 | train_wall 323 | gb_free 6.1 | wall 5283
KL Stats: Epoch 15 Divergences: Uniform: 1.521368074988557 Unigram: 1.595460762725742
2022-01-28 09:08:16 | INFO | fairseq.trainer | begin training epoch 16
2022-01-28 09:08:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:11:40 | INFO | train_inner | epoch 016:     40 / 64 loss=10.337, ppl=1293.82, wps=6103.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1000, lr=0.000125075, gnorm=0.571, train_wall=505, gb_free=6.1, wall=5487
2022-01-28 09:13:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:14:08 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.358 | ppl 1312.4 | wps 7988.9 | wpb 2034.1 | bsz 4 | num_updates 1024
2022-01-28 09:14:08 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-28 09:14:08 | INFO | train | epoch 016 | loss 10.272 | ppl 1236.05 | wps 5933.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1024 | lr 0.000128074 | gnorm 0.563 | train_wall 323 | gb_free 6.1 | wall 5635
KL Stats: Epoch 16 Divergences: Uniform: 1.546512583865095 Unigram: 1.7033095807432335
2022-01-28 09:14:08 | INFO | fairseq.trainer | begin training epoch 17
2022-01-28 09:14:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:19:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:20:00 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.267 | ppl 1232.28 | wps 7996.7 | wpb 2034.1 | bsz 4 | num_updates 1088
2022-01-28 09:20:00 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-28 09:20:00 | INFO | train | epoch 017 | loss 10.164 | ppl 1147.42 | wps 5927.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1088 | lr 0.000136073 | gnorm 0.557 | train_wall 323 | gb_free 6.1 | wall 5987
KL Stats: Epoch 17 Divergences: Uniform: 1.5781389455820685 Unigram: 1.7986097434230555
2022-01-28 09:20:00 | INFO | fairseq.trainer | begin training epoch 18
2022-01-28 09:20:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:21:01 | INFO | train_inner | epoch 018:     12 / 64 loss=10.178, ppl=1158.77, wps=5803.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1100, lr=0.000137573, gnorm=0.557, train_wall=504, gb_free=6.1, wall=6048
2022-01-28 09:25:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:25:52 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.204 | ppl 1179.82 | wps 7992.1 | wpb 2034.1 | bsz 4 | num_updates 1152
2022-01-28 09:25:52 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-28 09:25:52 | INFO | train | epoch 018 | loss 10.064 | ppl 1070.45 | wps 5932.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1152 | lr 0.000144071 | gnorm 0.583 | train_wall 323 | gb_free 6.1 | wall 6339
KL Stats: Epoch 18 Divergences: Uniform: 1.6086620456422112 Unigram: 1.8946558962490523
2022-01-28 09:25:52 | INFO | fairseq.trainer | begin training epoch 19
2022-01-28 09:25:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:29:57 | INFO | train_inner | epoch 019:     48 / 64 loss=10.014, ppl=1034.17, wps=6106, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=1200, lr=0.00015007, gnorm=0.548, train_wall=505, gb_free=6.1, wall=6584
2022-01-28 09:31:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 09:31:44 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.125 | ppl 1116.31 | wps 8009.7 | wpb 2034.1 | bsz 4 | num_updates 1216
2022-01-28 09:31:44 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-28 09:31:44 | INFO | train | epoch 019 | loss 9.96 | ppl 996.18 | wps 5934.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1216 | lr 0.00015207 | gnorm 0.533 | train_wall 323 | gb_free 6.1 | wall 6691
KL Stats: Epoch 19 Divergences: Uniform: 1.6363303187260667 Unigram: 1.9881139058723907
2022-01-28 09:31:44 | INFO | fairseq.trainer | begin training epoch 20
2022-01-28 09:31:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:37:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:37:36 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.039 | ppl 1052.09 | wps 7998.7 | wpb 2034.1 | bsz 4 | num_updates 1280
2022-01-28 09:37:36 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-28 09:37:36 | INFO | train | epoch 020 | loss 9.863 | ppl 931.36 | wps 5931.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1280 | lr 0.000160068 | gnorm 0.562 | train_wall 323 | gb_free 6.1 | wall 7043
KL Stats: Epoch 20 Divergences: Uniform: 1.6672466032990763 Unigram: 2.0769412243466774
2022-01-28 09:37:36 | INFO | fairseq.trainer | begin training epoch 21
2022-01-28 09:37:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:39:18 | INFO | train_inner | epoch 021:     20 / 64 loss=9.858, ppl=928, wps=5804.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=1300, lr=0.000162568, gnorm=0.557, train_wall=504, gb_free=6.1, wall=7145
2022-01-28 09:43:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:43:28 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.996 | ppl 1021.02 | wps 8019 | wpb 2034.1 | bsz 4 | num_updates 1344
2022-01-28 09:43:28 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-28 09:43:29 | INFO | train | epoch 021 | loss 9.768 | ppl 871.93 | wps 5932.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1344 | lr 0.000168066 | gnorm 0.54 | train_wall 323 | gb_free 6.1 | wall 7395
KL Stats: Epoch 21 Divergences: Uniform: 1.6925452075108285 Unigram: 2.1662969787121877
2022-01-28 09:43:29 | INFO | fairseq.trainer | begin training epoch 22
2022-01-28 09:43:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:48:14 | INFO | train_inner | epoch 022:     56 / 64 loss=9.715, ppl=840.64, wps=6103.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1400, lr=0.000175065, gnorm=0.553, train_wall=506, gb_free=6.1, wall=7681
2022-01-28 09:48:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 09:49:21 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.93 | ppl 975.38 | wps 7979.3 | wpb 2034.1 | bsz 4 | num_updates 1408
2022-01-28 09:49:21 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-28 09:49:21 | INFO | train | epoch 022 | loss 9.678 | ppl 819.37 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1408 | lr 0.000176065 | gnorm 0.56 | train_wall 323 | gb_free 6.1 | wall 7748
KL Stats: Epoch 22 Divergences: Uniform: 1.7177498463890153 Unigram: 2.2554335888364396
2022-01-28 09:49:21 | INFO | fairseq.trainer | begin training epoch 23
2022-01-28 09:49:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:54:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 09:55:13 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.864 | ppl 931.56 | wps 7996 | wpb 2034.1 | bsz 4 | num_updates 1472
2022-01-28 09:55:13 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-28 09:55:13 | INFO | train | epoch 023 | loss 9.591 | ppl 771.04 | wps 5927.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1472 | lr 0.000184063 | gnorm 0.526 | train_wall 323 | gb_free 6.1 | wall 8100
KL Stats: Epoch 23 Divergences: Uniform: 1.7442182698781048 Unigram: 2.3367123546676534
2022-01-28 09:55:13 | INFO | fairseq.trainer | begin training epoch 24
2022-01-28 09:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 09:57:36 | INFO | train_inner | epoch 024:     28 / 64 loss=9.576, ppl=763.07, wps=5800.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1500, lr=0.000187563, gnorm=0.538, train_wall=505, gb_free=6.1, wall=8243
2022-01-28 10:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:01:05 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.801 | ppl 891.77 | wps 8011.7 | wpb 2034.1 | bsz 4 | num_updates 1536
2022-01-28 10:01:05 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-28 10:01:05 | INFO | train | epoch 024 | loss 9.507 | ppl 727.64 | wps 5927.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1536 | lr 0.000192062 | gnorm 0.57 | train_wall 323 | gb_free 6.1 | wall 8452
KL Stats: Epoch 24 Divergences: Uniform: 1.7661836588589075 Unigram: 2.411621680219492
2022-01-28 10:01:05 | INFO | fairseq.trainer | begin training epoch 25
2022-01-28 10:01:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:06:31 | INFO | train_inner | epoch 025:     64 / 64 loss=9.453, ppl=701, wps=6097.1, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=1600, lr=0.00020006, gnorm=0.559, train_wall=505, gb_free=6.1, wall=8777
2022-01-28 10:06:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:06:58 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.77 | ppl 873.27 | wps 7980.6 | wpb 2034.1 | bsz 4 | num_updates 1600
2022-01-28 10:06:58 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-28 10:06:58 | INFO | train | epoch 025 | loss 9.426 | ppl 687.72 | wps 5925.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1600 | lr 0.00020006 | gnorm 0.548 | train_wall 323 | gb_free 6.1 | wall 8805
KL Stats: Epoch 25 Divergences: Uniform: 1.7933228071985698 Unigram: 2.4918876770491925
2022-01-28 10:06:58 | INFO | fairseq.trainer | begin training epoch 26
2022-01-28 10:06:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:12:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:12:50 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.715 | ppl 840.54 | wps 7995.3 | wpb 2034.1 | bsz 4 | num_updates 1664
2022-01-28 10:12:50 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-28 10:12:50 | INFO | train | epoch 026 | loss 9.344 | ppl 649.83 | wps 5933.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1664 | lr 0.000208058 | gnorm 0.553 | train_wall 323 | gb_free 6.1 | wall 9157
KL Stats: Epoch 26 Divergences: Uniform: 1.802567855337171 Unigram: 2.5643974793946747
2022-01-28 10:12:50 | INFO | fairseq.trainer | begin training epoch 27
2022-01-28 10:12:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:15:53 | INFO | train_inner | epoch 027:     36 / 64 loss=9.317, ppl=637.67, wps=5807.2, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=1700, lr=0.000212558, gnorm=0.555, train_wall=505, gb_free=6.1, wall=9340
2022-01-28 10:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:18:42 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.689 | ppl 825.31 | wps 7991.7 | wpb 2034.1 | bsz 4 | num_updates 1728
2022-01-28 10:18:42 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-28 10:18:42 | INFO | train | epoch 027 | loss 9.265 | ppl 615.1 | wps 5926.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1728 | lr 0.000216057 | gnorm 0.545 | train_wall 323 | gb_free 6.1 | wall 9509
KL Stats: Epoch 27 Divergences: Uniform: 1.8300707005840695 Unigram: 2.6362560721328405
2022-01-28 10:18:42 | INFO | fairseq.trainer | begin training epoch 28
2022-01-28 10:18:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:24:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:24:35 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.657 | ppl 807.53 | wps 8003.5 | wpb 2034.1 | bsz 4 | num_updates 1792
2022-01-28 10:24:35 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-28 10:24:35 | INFO | train | epoch 028 | loss 9.185 | ppl 582.12 | wps 5926.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1792 | lr 0.000224055 | gnorm 0.532 | train_wall 323 | gb_free 6.1 | wall 9862
KL Stats: Epoch 28 Divergences: Uniform: 1.8573594191737879 Unigram: 2.706883117880654
2022-01-28 10:24:35 | INFO | fairseq.trainer | begin training epoch 29
2022-01-28 10:24:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:25:16 | INFO | train_inner | epoch 029:      8 / 64 loss=9.201, ppl=588.59, wps=5798, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=1800, lr=0.000225055, gnorm=0.537, train_wall=505, gb_free=6.1, wall=9902
2022-01-28 10:29:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:30:27 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.613 | ppl 782.96 | wps 8005.9 | wpb 2034.1 | bsz 4 | num_updates 1856
2022-01-28 10:30:27 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-28 10:30:27 | INFO | train | epoch 029 | loss 9.107 | ppl 551.4 | wps 5933.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1856 | lr 0.000232054 | gnorm 0.539 | train_wall 323 | gb_free 6.1 | wall 10214
KL Stats: Epoch 29 Divergences: Uniform: 1.8759952029597744 Unigram: 2.772751238417001
2022-01-28 10:30:27 | INFO | fairseq.trainer | begin training epoch 30
2022-01-28 10:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:34:11 | INFO | train_inner | epoch 030:     44 / 64 loss=9.074, ppl=539.11, wps=6102.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1900, lr=0.000237553, gnorm=0.536, train_wall=506, gb_free=6.1, wall=10438
2022-01-28 10:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:36:19 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.597 | ppl 774.26 | wps 7969.8 | wpb 2034.1 | bsz 4 | num_updates 1920
2022-01-28 10:36:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-28 10:36:19 | INFO | train | epoch 030 | loss 9.03 | ppl 522.76 | wps 5927 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1920 | lr 0.000240052 | gnorm 0.542 | train_wall 323 | gb_free 6.1 | wall 10566
KL Stats: Epoch 30 Divergences: Uniform: 1.8968624852786211 Unigram: 2.842893926926946
2022-01-28 10:36:19 | INFO | fairseq.trainer | begin training epoch 31
2022-01-28 10:36:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:41:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:42:11 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.552 | ppl 750.63 | wps 7994.3 | wpb 2034.1 | bsz 4 | num_updates 1984
2022-01-28 10:42:12 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-28 10:42:12 | INFO | train | epoch 031 | loss 8.95 | ppl 494.44 | wps 5927.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1984 | lr 0.00024805 | gnorm 0.514 | train_wall 323 | gb_free 6.1 | wall 10918
KL Stats: Epoch 31 Divergences: Uniform: 1.9153981198219 Unigram: 2.9051181343043058
2022-01-28 10:42:12 | INFO | fairseq.trainer | begin training epoch 32
2022-01-28 10:42:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:43:33 | INFO | train_inner | epoch 032:     16 / 64 loss=8.951, ppl=494.77, wps=5801.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2000, lr=0.00025005, gnorm=0.517, train_wall=504, gb_free=6.1, wall=11000
2022-01-28 10:47:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:48:03 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.51 | ppl 729.02 | wps 8007.6 | wpb 2034.1 | bsz 4 | num_updates 2048
2022-01-28 10:48:03 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-28 10:48:03 | INFO | train | epoch 032 | loss 8.876 | ppl 469.79 | wps 5940.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2048 | lr 0.000256049 | gnorm 0.528 | train_wall 323 | gb_free 6.1 | wall 11270
KL Stats: Epoch 32 Divergences: Uniform: 1.9390946501263289 Unigram: 2.9690437941248558
2022-01-28 10:48:03 | INFO | fairseq.trainer | begin training epoch 33
2022-01-28 10:48:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:52:28 | INFO | train_inner | epoch 033:     52 / 64 loss=8.839, ppl=457.99, wps=6103.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2100, lr=0.000262548, gnorm=0.536, train_wall=505, gb_free=6.1, wall=11535
2022-01-28 10:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:53:56 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.497 | ppl 722.33 | wps 8024.7 | wpb 2034.1 | bsz 4 | num_updates 2112
2022-01-28 10:53:56 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-28 10:53:56 | INFO | train | epoch 033 | loss 8.801 | ppl 446.18 | wps 5923.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2112 | lr 0.000264047 | gnorm 0.536 | train_wall 324 | gb_free 6.1 | wall 11623
KL Stats: Epoch 33 Divergences: Uniform: 1.962794155636418 Unigram: 3.043794863286465
2022-01-28 10:53:56 | INFO | fairseq.trainer | begin training epoch 34
2022-01-28 10:53:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 10:59:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 10:59:48 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.477 | ppl 712.65 | wps 7992.8 | wpb 2034.1 | bsz 4 | num_updates 2176
2022-01-28 10:59:48 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-28 10:59:48 | INFO | train | epoch 034 | loss 8.725 | ppl 423.17 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2176 | lr 0.000272046 | gnorm 0.529 | train_wall 323 | gb_free 6.1 | wall 11975
KL Stats: Epoch 34 Divergences: Uniform: 1.9801370643585245 Unigram: 3.1083229124625418
2022-01-28 10:59:48 | INFO | fairseq.trainer | begin training epoch 35
2022-01-28 10:59:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:01:50 | INFO | train_inner | epoch 035:     24 / 64 loss=8.713, ppl=419.53, wps=5802.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2200, lr=0.000275045, gnorm=0.535, train_wall=504, gb_free=6.1, wall=12097
2022-01-28 11:05:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:05:40 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.431 | ppl 690.13 | wps 8001.4 | wpb 2034.1 | bsz 4 | num_updates 2240
2022-01-28 11:05:40 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-28 11:05:40 | INFO | train | epoch 035 | loss 8.654 | ppl 402.72 | wps 5930.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2240 | lr 0.000280044 | gnorm 0.537 | train_wall 323 | gb_free 6.1 | wall 12327
KL Stats: Epoch 35 Divergences: Uniform: 1.9996702044580548 Unigram: 3.1665759878162802
2022-01-28 11:05:40 | INFO | fairseq.trainer | begin training epoch 36
2022-01-28 11:05:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:10:45 | INFO | train_inner | epoch 036:     60 / 64 loss=8.609, ppl=390.46, wps=6107.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=2300, lr=0.000287543, gnorm=0.531, train_wall=505, gb_free=6.1, wall=12632
2022-01-28 11:11:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:11:32 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.415 | ppl 682.79 | wps 7997.3 | wpb 2034.1 | bsz 4 | num_updates 2304
2022-01-28 11:11:32 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-28 11:11:32 | INFO | train | epoch 036 | loss 8.579 | ppl 382.44 | wps 5939.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2304 | lr 0.000288042 | gnorm 0.53 | train_wall 323 | gb_free 6.1 | wall 12678
KL Stats: Epoch 36 Divergences: Uniform: 2.0235018398938474 Unigram: 3.236677473299354
2022-01-28 11:11:32 | INFO | fairseq.trainer | begin training epoch 37
2022-01-28 11:11:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:16:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:17:24 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.441 | ppl 695.18 | wps 8002.8 | wpb 2034.1 | bsz 4 | num_updates 2368
2022-01-28 11:17:24 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-28 11:17:24 | INFO | train | epoch 037 | loss 8.508 | ppl 364.15 | wps 5934.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2368 | lr 0.000296041 | gnorm 0.529 | train_wall 323 | gb_free 6.1 | wall 13030
KL Stats: Epoch 37 Divergences: Uniform: 2.0405086836072006 Unigram: 3.3021134875162517
2022-01-28 11:17:24 | INFO | fairseq.trainer | begin training epoch 38
2022-01-28 11:17:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:20:07 | INFO | train_inner | epoch 038:     32 / 64 loss=8.487, ppl=358.79, wps=5808.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2400, lr=0.00030004, gnorm=0.527, train_wall=504, gb_free=6.1, wall=13193
2022-01-28 11:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:23:16 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.418 | ppl 684.09 | wps 8010.5 | wpb 2034.1 | bsz 4 | num_updates 2432
2022-01-28 11:23:16 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-28 11:23:16 | INFO | train | epoch 038 | loss 8.44 | ppl 347.19 | wps 5934.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2432 | lr 0.000304039 | gnorm 0.53 | train_wall 323 | gb_free 6.1 | wall 13382
KL Stats: Epoch 38 Divergences: Uniform: 2.0655242755473373 Unigram: 3.3587745936563476
2022-01-28 11:23:16 | INFO | fairseq.trainer | begin training epoch 39
2022-01-28 11:23:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:28:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:29:08 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.386 | ppl 668.92 | wps 8017 | wpb 2034.1 | bsz 4 | num_updates 2496
2022-01-28 11:29:08 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-28 11:29:08 | INFO | train | epoch 039 | loss 8.37 | ppl 330.86 | wps 5928 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2496 | lr 0.000312038 | gnorm 0.526 | train_wall 323 | gb_free 6.1 | wall 13735
KL Stats: Epoch 39 Divergences: Uniform: 2.073710962977877 Unigram: 3.427631425159114
2022-01-28 11:29:08 | INFO | fairseq.trainer | begin training epoch 40
2022-01-28 11:29:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:29:28 | INFO | train_inner | epoch 040:      4 / 64 loss=8.392, ppl=335.9, wps=5802.7, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=2500, lr=0.000312538, gnorm=0.531, train_wall=504, gb_free=6.1, wall=13755
2022-01-28 11:34:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:35:00 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.367 | ppl 660.11 | wps 8005.3 | wpb 2034.1 | bsz 4 | num_updates 2560
2022-01-28 11:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2560 updates
2022-01-28 11:35:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint40.pt
2022-01-28 11:35:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint40.pt
2022-01-28 11:35:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint40.pt (epoch 40 @ 2560 updates, score 9.367) (writing took 4.933037809096277 seconds)
2022-01-28 11:35:05 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-28 11:35:05 | INFO | train | epoch 040 | loss 8.3 | ppl 315.22 | wps 5851.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2560 | lr 0.000320036 | gnorm 0.525 | train_wall 323 | gb_free 6.1 | wall 14092
KL Stats: Epoch 40 Divergences: Uniform: 2.102068217129798 Unigram: 3.492684433103613
2022-01-28 11:35:05 | INFO | fairseq.trainer | begin training epoch 41
2022-01-28 11:35:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:38:28 | INFO | train_inner | epoch 041:     40 / 64 loss=8.276, ppl=310.07, wps=6052.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2600, lr=0.000325035, gnorm=0.521, train_wall=505, gb_free=6.1, wall=14295
2022-01-28 11:40:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:40:57 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.352 | ppl 653.34 | wps 7986.8 | wpb 2034.1 | bsz 4 | num_updates 2624 | best_loss 9.352
2022-01-28 11:40:57 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-28 11:40:57 | INFO | train | epoch 041 | loss 8.236 | ppl 301.48 | wps 5933.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2624 | lr 0.000328034 | gnorm 0.522 | train_wall 323 | gb_free 6.1 | wall 14444
KL Stats: Epoch 41 Divergences: Uniform: 2.1148534694143364 Unigram: 3.5513183017651135
2022-01-28 11:40:57 | INFO | fairseq.trainer | begin training epoch 42
2022-01-28 11:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:46:49 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.335 | ppl 645.97 | wps 8017.6 | wpb 2034.1 | bsz 4 | num_updates 2688 | best_loss 9.335
2022-01-28 11:46:49 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-28 11:46:49 | INFO | train | epoch 042 | loss 8.171 | ppl 288.21 | wps 5933.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2688 | lr 0.000336033 | gnorm 0.53 | train_wall 323 | gb_free 6.1 | wall 14796
KL Stats: Epoch 42 Divergences: Uniform: 2.1325955904170346 Unigram: 3.620265095275188
2022-01-28 11:46:49 | INFO | fairseq.trainer | begin training epoch 43
2022-01-28 11:46:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:47:50 | INFO | train_inner | epoch 043:     12 / 64 loss=8.177, ppl=289.51, wps=5803.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2700, lr=0.000337533, gnorm=0.53, train_wall=504, gb_free=6.1, wall=14857
2022-01-28 11:52:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:52:41 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.356 | ppl 655.24 | wps 8002.4 | wpb 2034.1 | bsz 4 | num_updates 2752 | best_loss 9.356
2022-01-28 11:52:41 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-28 11:52:41 | INFO | train | epoch 043 | loss 8.105 | ppl 275.39 | wps 5934.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2752 | lr 0.000344031 | gnorm 0.533 | train_wall 323 | gb_free 6.1 | wall 15148
KL Stats: Epoch 43 Divergences: Uniform: 2.153057500091702 Unigram: 3.6856633585080325
2022-01-28 11:52:41 | INFO | fairseq.trainer | begin training epoch 44
2022-01-28 11:52:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 11:56:45 | INFO | train_inner | epoch 044:     48 / 64 loss=8.072, ppl=269.02, wps=6103.9, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2800, lr=0.00035003, gnorm=0.536, train_wall=505, gb_free=6.1, wall=15392
2022-01-28 11:58:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 11:58:33 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.393 | ppl 672.51 | wps 7983.1 | wpb 2034.1 | bsz 4 | num_updates 2816 | best_loss 9.367
2022-01-28 11:58:33 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-28 11:58:33 | INFO | train | epoch 044 | loss 8.046 | ppl 264.25 | wps 5926.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2816 | lr 0.00035203 | gnorm 0.535 | train_wall 323 | gb_free 6.1 | wall 15500
KL Stats: Epoch 44 Divergences: Uniform: 2.1715524010376774 Unigram: 3.741304441181967
2022-01-28 11:58:33 | INFO | fairseq.trainer | begin training epoch 45
2022-01-28 11:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:03:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:04:26 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.365 | ppl 659.49 | wps 7986.1 | wpb 2034.1 | bsz 4 | num_updates 2880 | best_loss 9.365
2022-01-28 12:04:26 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-28 12:04:26 | INFO | train | epoch 045 | loss 7.982 | ppl 252.79 | wps 5925.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2880 | lr 0.000360028 | gnorm 0.538 | train_wall 323 | gb_free 6.1 | wall 15852
KL Stats: Epoch 45 Divergences: Uniform: 2.1859779591778357 Unigram: 3.8086080259076764
2022-01-28 12:04:26 | INFO | fairseq.trainer | begin training epoch 46
2022-01-28 12:04:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:06:08 | INFO | train_inner | epoch 046:     20 / 64 loss=7.981, ppl=252.7, wps=5799.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2900, lr=0.000362528, gnorm=0.536, train_wall=505, gb_free=6.1, wall=15954
2022-01-28 12:09:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:10:18 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.363 | ppl 658.48 | wps 7997.9 | wpb 2034.1 | bsz 4 | num_updates 2944 | best_loss 9.363
2022-01-28 12:10:18 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-28 12:10:18 | INFO | train | epoch 046 | loss 7.923 | ppl 242.69 | wps 5933.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2944 | lr 0.000368026 | gnorm 0.547 | train_wall 323 | gb_free 6.1 | wall 16204
KL Stats: Epoch 46 Divergences: Uniform: 2.203520768386123 Unigram: 3.8641329493143797
2022-01-28 12:10:18 | INFO | fairseq.trainer | begin training epoch 47
2022-01-28 12:10:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:15:04 | INFO | train_inner | epoch 047:     56 / 64 loss=7.892, ppl=237.49, wps=6096, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3000, lr=0.000375025, gnorm=0.539, train_wall=506, gb_free=6.1, wall=16490
2022-01-28 12:15:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:16:10 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.342 | ppl 649.01 | wps 7988.9 | wpb 2034.1 | bsz 4 | num_updates 3008 | best_loss 9.342
2022-01-28 12:16:10 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-28 12:16:10 | INFO | train | epoch 047 | loss 7.863 | ppl 232.79 | wps 5919.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3008 | lr 0.000376025 | gnorm 0.533 | train_wall 324 | gb_free 6.1 | wall 16557
KL Stats: Epoch 47 Divergences: Uniform: 2.2209736604559334 Unigram: 3.923440554382216
2022-01-28 12:16:10 | INFO | fairseq.trainer | begin training epoch 48
2022-01-28 12:16:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:21:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:22:02 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.358 | ppl 656.28 | wps 8009.5 | wpb 2034.1 | bsz 4 | num_updates 3072 | best_loss 9.358
2022-01-28 12:22:02 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-28 12:22:02 | INFO | train | epoch 048 | loss 7.804 | ppl 223.49 | wps 5937.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3072 | lr 0.000384023 | gnorm 0.54 | train_wall 323 | gb_free 6.1 | wall 16909
KL Stats: Epoch 48 Divergences: Uniform: 2.236870947912752 Unigram: 3.9911552345387626
2022-01-28 12:22:02 | INFO | fairseq.trainer | begin training epoch 49
2022-01-28 12:22:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:24:25 | INFO | train_inner | epoch 049:     28 / 64 loss=7.787, ppl=220.87, wps=5809, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3100, lr=0.000387523, gnorm=0.541, train_wall=504, gb_free=6.1, wall=17052
2022-01-28 12:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:27:54 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.369 | ppl 661.33 | wps 7990.1 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 9.367
2022-01-28 12:27:54 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-28 12:27:54 | INFO | train | epoch 049 | loss 7.748 | ppl 214.93 | wps 5935.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3136 | lr 0.000392022 | gnorm 0.541 | train_wall 323 | gb_free 6.1 | wall 17261
KL Stats: Epoch 49 Divergences: Uniform: 2.245229567414752 Unigram: 4.05194965426889
2022-01-28 12:27:54 | INFO | fairseq.trainer | begin training epoch 50
2022-01-28 12:27:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:33:19 | INFO | train_inner | epoch 050:     64 / 64 loss=7.722, ppl=211.18, wps=6105.9, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=3200, lr=0.00040002, gnorm=0.553, train_wall=504, gb_free=6.1, wall=17585
2022-01-28 12:33:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:33:46 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.421 | ppl 685.35 | wps 8005 | wpb 2034.1 | bsz 4 | num_updates 3200 | best_loss 9.367
2022-01-28 12:33:46 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-28 12:33:46 | INFO | train | epoch 050 | loss 7.696 | ppl 207.34 | wps 5935.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3200 | lr 0.00040002 | gnorm 0.561 | train_wall 323 | gb_free 6.1 | wall 17613
KL Stats: Epoch 50 Divergences: Uniform: 2.260132424946737 Unigram: 4.104412388706335
2022-01-28 12:33:46 | INFO | fairseq.trainer | begin training epoch 51
2022-01-28 12:33:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:39:38 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.401 | ppl 676.07 | wps 8002.4 | wpb 2034.1 | bsz 4 | num_updates 3264 | best_loss 9.367
2022-01-28 12:39:38 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-28 12:39:38 | INFO | train | epoch 051 | loss 7.64 | ppl 199.49 | wps 5925.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3264 | lr 0.000408018 | gnorm 0.551 | train_wall 323 | gb_free 6.1 | wall 17965
KL Stats: Epoch 51 Divergences: Uniform: 2.283045674873044 Unigram: 4.162804632995376
2022-01-28 12:39:38 | INFO | fairseq.trainer | begin training epoch 52
2022-01-28 12:39:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:42:42 | INFO | train_inner | epoch 052:     36 / 64 loss=7.616, ppl=196.22, wps=5804.6, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=3300, lr=0.000412518, gnorm=0.55, train_wall=506, gb_free=6.1, wall=18149
2022-01-28 12:45:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:45:30 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.405 | ppl 677.83 | wps 8020.6 | wpb 2034.1 | bsz 4 | num_updates 3328 | best_loss 9.367
2022-01-28 12:45:30 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-28 12:45:30 | INFO | train | epoch 052 | loss 7.586 | ppl 192.08 | wps 5938.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3328 | lr 0.000416017 | gnorm 0.546 | train_wall 323 | gb_free 6.1 | wall 18317
KL Stats: Epoch 52 Divergences: Uniform: 2.294699855661371 Unigram: 4.230774434576868
2022-01-28 12:45:30 | INFO | fairseq.trainer | begin training epoch 53
2022-01-28 12:45:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:50:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 12:51:22 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.382 | ppl 667.03 | wps 7970.3 | wpb 2034.1 | bsz 4 | num_updates 3392 | best_loss 9.367
2022-01-28 12:51:22 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-28 12:51:22 | INFO | train | epoch 053 | loss 7.533 | ppl 185.26 | wps 5936.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3392 | lr 0.000424015 | gnorm 0.548 | train_wall 323 | gb_free 6.1 | wall 18669
KL Stats: Epoch 53 Divergences: Uniform: 2.3077732802389783 Unigram: 4.290048816603023
2022-01-28 12:51:22 | INFO | fairseq.trainer | begin training epoch 54
2022-01-28 12:51:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 12:52:03 | INFO | train_inner | epoch 054:      8 / 64 loss=7.546, ppl=186.85, wps=5809.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=3400, lr=0.000425015, gnorm=0.55, train_wall=504, gb_free=6.1, wall=18710
2022-01-28 12:56:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 12:57:14 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.454 | ppl 701.59 | wps 7999.6 | wpb 2034.1 | bsz 4 | num_updates 3456 | best_loss 9.367
2022-01-28 12:57:14 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-28 12:57:14 | INFO | train | epoch 054 | loss 7.482 | ppl 178.8 | wps 5938.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3456 | lr 0.000432014 | gnorm 0.552 | train_wall 323 | gb_free 6.1 | wall 19021
KL Stats: Epoch 54 Divergences: Uniform: 2.3174684222306454 Unigram: 4.348289253973083
2022-01-28 12:57:14 | INFO | fairseq.trainer | begin training epoch 55
2022-01-28 12:57:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:00:58 | INFO | train_inner | epoch 055:     44 / 64 loss=7.454, ppl=175.39, wps=6105.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3500, lr=0.000437513, gnorm=0.552, train_wall=505, gb_free=6.1, wall=19245
2022-01-28 13:02:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:03:06 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.491 | ppl 719.6 | wps 8007.3 | wpb 2034.1 | bsz 4 | num_updates 3520 | best_loss 9.367
2022-01-28 13:03:06 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-28 13:03:06 | INFO | train | epoch 055 | loss 7.434 | ppl 172.98 | wps 5929.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3520 | lr 0.000440012 | gnorm 0.566 | train_wall 323 | gb_free 6.1 | wall 19373
KL Stats: Epoch 55 Divergences: Uniform: 2.323466984790746 Unigram: 4.422722683342308
2022-01-28 13:03:06 | INFO | fairseq.trainer | begin training epoch 56
2022-01-28 13:03:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:08:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:08:58 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.519 | ppl 733.88 | wps 8006.1 | wpb 2034.1 | bsz 4 | num_updates 3584 | best_loss 9.367
2022-01-28 13:08:58 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-28 13:08:58 | INFO | train | epoch 056 | loss 7.386 | ppl 167.24 | wps 5926.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3584 | lr 0.00044801 | gnorm 0.568 | train_wall 323 | gb_free 6.1 | wall 19725
KL Stats: Epoch 56 Divergences: Uniform: 2.334257899524369 Unigram: 4.4708703420345275
2022-01-28 13:08:58 | INFO | fairseq.trainer | begin training epoch 57
2022-01-28 13:08:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:10:20 | INFO | train_inner | epoch 057:     16 / 64 loss=7.389, ppl=167.63, wps=5802.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3600, lr=0.00045001, gnorm=0.572, train_wall=504, gb_free=6.1, wall=19807
2022-01-28 13:14:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:14:51 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.553 | ppl 751.27 | wps 7985.2 | wpb 2034.1 | bsz 4 | num_updates 3648 | best_loss 9.367
2022-01-28 13:14:51 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-28 13:14:51 | INFO | train | epoch 057 | loss 7.337 | ppl 161.73 | wps 5928.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3648 | lr 0.000456009 | gnorm 0.583 | train_wall 323 | gb_free 6.1 | wall 20078
KL Stats: Epoch 57 Divergences: Uniform: 2.3589889079372037 Unigram: 4.547549765426425
2022-01-28 13:14:51 | INFO | fairseq.trainer | begin training epoch 58
2022-01-28 13:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:19:16 | INFO | train_inner | epoch 058:     52 / 64 loss=7.311, ppl=158.83, wps=6097.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3700, lr=0.000462508, gnorm=0.573, train_wall=506, gb_free=6.1, wall=20343
2022-01-28 13:20:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:20:43 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.567 | ppl 758.63 | wps 8008.9 | wpb 2034.1 | bsz 4 | num_updates 3712 | best_loss 9.367
2022-01-28 13:20:43 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-28 13:20:43 | INFO | train | epoch 058 | loss 7.29 | ppl 156.46 | wps 5927.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3712 | lr 0.000464007 | gnorm 0.565 | train_wall 323 | gb_free 6.1 | wall 20430
KL Stats: Epoch 58 Divergences: Uniform: 2.367617559073186 Unigram: 4.610482745629864
2022-01-28 13:20:43 | INFO | fairseq.trainer | begin training epoch 59
2022-01-28 13:20:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:26:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:26:35 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.608 | ppl 780.63 | wps 7999.2 | wpb 2034.1 | bsz 4 | num_updates 3776 | best_loss 9.367
2022-01-28 13:26:35 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-28 13:26:35 | INFO | train | epoch 059 | loss 7.244 | ppl 151.56 | wps 5934.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3776 | lr 0.000472006 | gnorm 0.578 | train_wall 323 | gb_free 6.1 | wall 20782
KL Stats: Epoch 59 Divergences: Uniform: 2.3816293091230056 Unigram: 4.6726149979348826
2022-01-28 13:26:35 | INFO | fairseq.trainer | begin training epoch 60
2022-01-28 13:26:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:28:37 | INFO | train_inner | epoch 060:     24 / 64 loss=7.238, ppl=150.94, wps=5805.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3800, lr=0.000475005, gnorm=0.578, train_wall=504, gb_free=6.1, wall=20904
2022-01-28 13:32:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 13:32:27 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.634 | ppl 794.72 | wps 7997.4 | wpb 2034.1 | bsz 4 | num_updates 3840 | best_loss 9.367
2022-01-28 13:32:27 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-28 13:32:27 | INFO | train | epoch 060 | loss 7.199 | ppl 146.88 | wps 5932 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3840 | lr 0.000480004 | gnorm 0.583 | train_wall 323 | gb_free 6.1 | wall 21134
KL Stats: Epoch 60 Divergences: Uniform: 2.3877652308383204 Unigram: 4.7497061631227675
2022-01-28 13:32:27 | INFO | fairseq.trainer | begin training epoch 61
2022-01-28 13:32:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:37:33 | INFO | train_inner | epoch 061:     60 / 64 loss=7.18, ppl=144.97, wps=6102.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3900, lr=0.000487503, gnorm=0.587, train_wall=505, gb_free=6.1, wall=21440
2022-01-28 13:37:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:38:19 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.694 | ppl 828.03 | wps 8027.3 | wpb 2034.1 | bsz 4 | num_updates 3904 | best_loss 9.367
2022-01-28 13:38:19 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-28 13:38:19 | INFO | train | epoch 061 | loss 7.156 | ppl 142.65 | wps 5929.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3904 | lr 0.000488002 | gnorm 0.586 | train_wall 323 | gb_free 6.1 | wall 21486
KL Stats: Epoch 61 Divergences: Uniform: 2.401559052973179 Unigram: 4.806224766912007
2022-01-28 13:38:19 | INFO | fairseq.trainer | begin training epoch 62
2022-01-28 13:38:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:43:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:44:11 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.634 | ppl 794.31 | wps 8000.9 | wpb 2034.1 | bsz 4 | num_updates 3968 | best_loss 9.367
2022-01-28 13:44:11 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-28 13:44:11 | INFO | train | epoch 062 | loss 7.113 | ppl 138.44 | wps 5932 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3968 | lr 0.000496001 | gnorm 0.595 | train_wall 323 | gb_free 6.1 | wall 21838
KL Stats: Epoch 62 Divergences: Uniform: 2.4125084248427227 Unigram: 4.889265873354909
2022-01-28 13:44:11 | INFO | fairseq.trainer | begin training epoch 63
2022-01-28 13:44:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:46:54 | INFO | train_inner | epoch 063:     32 / 64 loss=7.087, ppl=135.94, wps=5807.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4000, lr=0.0005, gnorm=0.594, train_wall=504, gb_free=6.1, wall=22001
2022-01-28 13:49:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:50:03 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.646 | ppl 800.95 | wps 7990.9 | wpb 2034.1 | bsz 4 | num_updates 4032 | best_loss 9.367
2022-01-28 13:50:03 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-28 13:50:03 | INFO | train | epoch 063 | loss 7.069 | ppl 134.23 | wps 5932.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4032 | lr 0.000498012 | gnorm 0.603 | train_wall 323 | gb_free 6.1 | wall 22190
KL Stats: Epoch 63 Divergences: Uniform: 2.4266430179225145 Unigram: 4.950346350934417
2022-01-28 13:50:04 | INFO | fairseq.trainer | begin training epoch 64
2022-01-28 13:50:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 13:55:56 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.759 | ppl 866.5 | wps 8001.7 | wpb 2034.1 | bsz 4 | num_updates 4096 | best_loss 9.367
2022-01-28 13:55:56 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-28 13:55:56 | INFO | train | epoch 064 | loss 7.022 | ppl 129.99 | wps 5929.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4096 | lr 0.000494106 | gnorm 0.59 | train_wall 323 | gb_free 6.1 | wall 22543
KL Stats: Epoch 64 Divergences: Uniform: 2.4292281594638343 Unigram: 5.013765749061473
2022-01-28 13:55:56 | INFO | fairseq.trainer | begin training epoch 65
2022-01-28 13:55:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 13:56:16 | INFO | train_inner | epoch 065:      4 / 64 loss=7.05, ppl=132.53, wps=5802.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4100, lr=0.000493865, gnorm=0.597, train_wall=504, gb_free=6.1, wall=22563
2022-01-28 14:01:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:01:48 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.732 | ppl 850.65 | wps 8022.6 | wpb 2034.1 | bsz 4 | num_updates 4160 | best_loss 9.367
2022-01-28 14:01:48 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-28 14:01:48 | INFO | train | epoch 065 | loss 6.979 | ppl 126.18 | wps 5929.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4160 | lr 0.00049029 | gnorm 0.612 | train_wall 323 | gb_free 6.1 | wall 22895
KL Stats: Epoch 65 Divergences: Uniform: 2.4475339169085317 Unigram: 5.100365946677013
2022-01-28 14:01:48 | INFO | fairseq.trainer | begin training epoch 66
2022-01-28 14:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:05:12 | INFO | train_inner | epoch 066:     40 / 64 loss=6.954, ppl=123.99, wps=6103.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4200, lr=0.00048795, gnorm=0.608, train_wall=505, gb_free=6.1, wall=23098
2022-01-28 14:07:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:07:40 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.801 | ppl 892 | wps 8000 | wpb 2034.1 | bsz 4 | num_updates 4224 | best_loss 9.367
2022-01-28 14:07:40 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-28 14:07:40 | INFO | train | epoch 066 | loss 6.936 | ppl 122.46 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4224 | lr 0.000486562 | gnorm 0.593 | train_wall 323 | gb_free 6.1 | wall 23247
KL Stats: Epoch 66 Divergences: Uniform: 2.446110323064866 Unigram: 5.157076418690823
2022-01-28 14:07:40 | INFO | fairseq.trainer | begin training epoch 67
2022-01-28 14:07:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:13:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:13:32 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.781 | ppl 879.99 | wps 8008.3 | wpb 2034.1 | bsz 4 | num_updates 4288 | best_loss 9.367
2022-01-28 14:13:32 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-28 14:13:32 | INFO | train | epoch 067 | loss 6.895 | ppl 118.99 | wps 5934.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4288 | lr 0.000482917 | gnorm 0.612 | train_wall 323 | gb_free 6.1 | wall 23599
KL Stats: Epoch 67 Divergences: Uniform: 2.4636294817427133 Unigram: 5.241463945856954
2022-01-28 14:13:32 | INFO | fairseq.trainer | begin training epoch 68
2022-01-28 14:13:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:14:33 | INFO | train_inner | epoch 068:     12 / 64 loss=6.904, ppl=119.73, wps=5807.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4300, lr=0.000482243, gnorm=0.603, train_wall=504, gb_free=6.1, wall=23660
2022-01-28 14:18:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:19:24 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.913 | ppl 963.85 | wps 8003.6 | wpb 2034.1 | bsz 4 | num_updates 4352 | best_loss 9.367
2022-01-28 14:19:24 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-28 14:19:24 | INFO | train | epoch 068 | loss 6.856 | ppl 115.86 | wps 5933.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4352 | lr 0.000479353 | gnorm 0.624 | train_wall 323 | gb_free 6.1 | wall 23951
KL Stats: Epoch 68 Divergences: Uniform: 2.4756600379520557 Unigram: 5.32484187013141
2022-01-28 14:19:24 | INFO | fairseq.trainer | begin training epoch 69
2022-01-28 14:19:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:23:29 | INFO | train_inner | epoch 069:     48 / 64 loss=6.837, ppl=114.31, wps=6101.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4400, lr=0.000476731, gnorm=0.62, train_wall=506, gb_free=6.1, wall=24195
2022-01-28 14:24:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:25:16 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.863 | ppl 931.43 | wps 7995 | wpb 2034.1 | bsz 4 | num_updates 4416 | best_loss 9.367
2022-01-28 14:25:16 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-28 14:25:16 | INFO | train | epoch 069 | loss 6.816 | ppl 112.65 | wps 5923.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4416 | lr 0.000475867 | gnorm 0.616 | train_wall 324 | gb_free 6.1 | wall 24303
KL Stats: Epoch 69 Divergences: Uniform: 2.4843896615256535 Unigram: 5.396955005444266
2022-01-28 14:25:16 | INFO | fairseq.trainer | begin training epoch 70
2022-01-28 14:25:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 14:31:09 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.985 | ppl 1013.67 | wps 8006.9 | wpb 2034.1 | bsz 4 | num_updates 4480 | best_loss 9.367
2022-01-28 14:31:09 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-28 14:31:09 | INFO | train | epoch 070 | loss 6.779 | ppl 109.8 | wps 5926.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4480 | lr 0.000472456 | gnorm 0.62 | train_wall 323 | gb_free 6.1 | wall 24656
KL Stats: Epoch 70 Divergences: Uniform: 2.4911885078462346 Unigram: 5.457953583860842
2022-01-28 14:31:09 | INFO | fairseq.trainer | begin training epoch 71
2022-01-28 14:31:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:32:51 | INFO | train_inner | epoch 071:     20 / 64 loss=6.773, ppl=109.34, wps=5798.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4500, lr=0.000471405, gnorm=0.62, train_wall=505, gb_free=6.1, wall=24758
2022-01-28 14:36:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:37:01 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.949 | ppl 988.19 | wps 8001.9 | wpb 2034.1 | bsz 4 | num_updates 4544 | best_loss 9.367
2022-01-28 14:37:01 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-28 14:37:01 | INFO | train | epoch 071 | loss 6.741 | ppl 106.96 | wps 5926.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4544 | lr 0.000469117 | gnorm 0.628 | train_wall 323 | gb_free 6.1 | wall 25008
KL Stats: Epoch 71 Divergences: Uniform: 2.499401971810244 Unigram: 5.562723332069421
2022-01-28 14:37:01 | INFO | fairseq.trainer | begin training epoch 72
2022-01-28 14:37:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:41:47 | INFO | train_inner | epoch 072:     56 / 64 loss=6.725, ppl=105.8, wps=6098, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4600, lr=0.000466252, gnorm=0.631, train_wall=506, gb_free=6.1, wall=25294
2022-01-28 14:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:42:54 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.896 | ppl 952.64 | wps 7976 | wpb 2034.1 | bsz 4 | num_updates 4608 | best_loss 9.367
2022-01-28 14:42:54 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-01-28 14:42:54 | INFO | train | epoch 072 | loss 6.705 | ppl 104.3 | wps 5923.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4608 | lr 0.000465847 | gnorm 0.631 | train_wall 323 | gb_free 6.1 | wall 25361
KL Stats: Epoch 72 Divergences: Uniform: 2.5187664802371796 Unigram: 5.637720809848479
2022-01-28 14:42:54 | INFO | fairseq.trainer | begin training epoch 73
2022-01-28 14:42:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:48:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:48:46 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.926 | ppl 972.55 | wps 8018.4 | wpb 2034.1 | bsz 4 | num_updates 4672 | best_loss 9.367
2022-01-28 14:48:46 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-01-28 14:48:46 | INFO | train | epoch 073 | loss 6.669 | ppl 101.73 | wps 5932.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4672 | lr 0.000462646 | gnorm 0.631 | train_wall 323 | gb_free 6.1 | wall 25713
KL Stats: Epoch 73 Divergences: Uniform: 2.517357420191149 Unigram: 5.706059595345922
2022-01-28 14:48:46 | INFO | fairseq.trainer | begin training epoch 74
2022-01-28 14:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 14:51:08 | INFO | train_inner | epoch 074:     28 / 64 loss=6.659, ppl=101.03, wps=5804.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4700, lr=0.000461266, gnorm=0.64, train_wall=504, gb_free=6.1, wall=25855
2022-01-28 14:54:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 14:54:38 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.961 | ppl 996.41 | wps 8014.3 | wpb 2034.1 | bsz 4 | num_updates 4736 | best_loss 9.367
2022-01-28 14:54:38 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-01-28 14:54:38 | INFO | train | epoch 074 | loss 6.637 | ppl 99.56 | wps 5934.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4736 | lr 0.000459509 | gnorm 0.649 | train_wall 323 | gb_free 6.1 | wall 26065
KL Stats: Epoch 74 Divergences: Uniform: 2.524241842273284 Unigram: 5.798887056823947
2022-01-28 14:54:38 | INFO | fairseq.trainer | begin training epoch 75
2022-01-28 14:54:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:00:02 | INFO | train_inner | epoch 075:     64 / 64 loss=6.625, ppl=98.73, wps=6104.2, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=4800, lr=0.000456435, gnorm=0.64, train_wall=504, gb_free=6.1, wall=26389
2022-01-28 15:00:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:00:30 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.043 | ppl 1055.33 | wps 8009.4 | wpb 2034.1 | bsz 4 | num_updates 4800 | best_loss 9.367
2022-01-28 15:00:30 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-01-28 15:00:30 | INFO | train | epoch 075 | loss 6.604 | ppl 97.29 | wps 5934.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4800 | lr 0.000456435 | gnorm 0.637 | train_wall 323 | gb_free 6.1 | wall 26417
KL Stats: Epoch 75 Divergences: Uniform: 2.5310139362937973 Unigram: 5.884554863399365
2022-01-28 15:00:30 | INFO | fairseq.trainer | begin training epoch 76
2022-01-28 15:00:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:06:22 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.014 | ppl 1034.21 | wps 8012.1 | wpb 2034.1 | bsz 4 | num_updates 4864 | best_loss 9.367
2022-01-28 15:06:22 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-01-28 15:06:22 | INFO | train | epoch 076 | loss 6.576 | ppl 95.4 | wps 5932.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.675 | train_wall 323 | gb_free 6.1 | wall 26769
KL Stats: Epoch 76 Divergences: Uniform: 2.5426168469286368 Unigram: 5.986159256496167
2022-01-28 15:06:22 | INFO | fairseq.trainer | begin training epoch 77
2022-01-28 15:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:09:25 | INFO | train_inner | epoch 077:     36 / 64 loss=6.549, ppl=93.65, wps=5806.4, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=4900, lr=0.000451754, gnorm=0.677, train_wall=506, gb_free=6.1, wall=26952
2022-01-28 15:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:12:14 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.921 | ppl 969.52 | wps 7960.3 | wpb 2034.1 | bsz 4 | num_updates 4928 | best_loss 9.367
2022-01-28 15:12:14 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-01-28 15:12:14 | INFO | train | epoch 077 | loss 6.539 | ppl 93.02 | wps 5927.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4928 | lr 0.000450469 | gnorm 0.665 | train_wall 323 | gb_free 6.1 | wall 27121
KL Stats: Epoch 77 Divergences: Uniform: 2.548875589980266 Unigram: 6.066487943866863
2022-01-28 15:12:14 | INFO | fairseq.trainer | begin training epoch 78
2022-01-28 15:12:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:18:06 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.905 | ppl 958.52 | wps 8005 | wpb 2034.1 | bsz 4 | num_updates 4992 | best_loss 9.367
2022-01-28 15:18:06 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-01-28 15:18:06 | INFO | train | epoch 078 | loss 6.512 | ppl 91.25 | wps 5936.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4992 | lr 0.000447572 | gnorm 0.668 | train_wall 323 | gb_free 6.1 | wall 27473
KL Stats: Epoch 78 Divergences: Uniform: 2.5561531498016277 Unigram: 6.143895811681159
2022-01-28 15:18:06 | INFO | fairseq.trainer | begin training epoch 79
2022-01-28 15:18:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:18:47 | INFO | train_inner | epoch 079:      8 / 64 loss=6.525, ppl=92.11, wps=5806, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5000, lr=0.000447214, gnorm=0.663, train_wall=504, gb_free=6.1, wall=27514
2022-01-28 15:23:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 15:23:58 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.111 | ppl 1105.95 | wps 8018.2 | wpb 2034.1 | bsz 4 | num_updates 5056 | best_loss 9.367
2022-01-28 15:23:58 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-01-28 15:23:58 | INFO | train | epoch 079 | loss 6.48 | ppl 89.27 | wps 5930 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5056 | lr 0.00044473 | gnorm 0.675 | train_wall 323 | gb_free 6.1 | wall 27825
KL Stats: Epoch 79 Divergences: Uniform: 2.5617771129633553 Unigram: 6.222472716795832
2022-01-28 15:23:58 | INFO | fairseq.trainer | begin training epoch 80
2022-01-28 15:23:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:27:43 | INFO | train_inner | epoch 080:     44 / 64 loss=6.462, ppl=88.13, wps=6093, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5100, lr=0.000442807, gnorm=0.667, train_wall=506, gb_free=6.1, wall=28050
2022-01-28 15:29:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:29:51 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.063 | ppl 1069.58 | wps 7990.9 | wpb 2034.1 | bsz 4 | num_updates 5120 | best_loss 9.367
2022-01-28 15:29:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5120 updates
2022-01-28 15:29:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint80.pt
2022-01-28 15:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint80.pt
2022-01-28 15:29:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint80.pt (epoch 80 @ 5120 updates, score 10.063) (writing took 3.166621347889304 seconds)
2022-01-28 15:29:54 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-01-28 15:29:54 | INFO | train | epoch 080 | loss 6.45 | ppl 87.41 | wps 5863.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5120 | lr 0.000441942 | gnorm 0.66 | train_wall 324 | gb_free 6.1 | wall 28181
KL Stats: Epoch 80 Divergences: Uniform: 2.5685473239744603 Unigram: 6.311747552657519
2022-01-28 15:29:54 | INFO | fairseq.trainer | begin training epoch 81
2022-01-28 15:29:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:35:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:35:47 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.042 | ppl 1054.38 | wps 8000.2 | wpb 2034.1 | bsz 4 | num_updates 5184 | best_loss 9.367
2022-01-28 15:35:47 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-01-28 15:35:47 | INFO | train | epoch 081 | loss 6.424 | ppl 85.88 | wps 5927.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5184 | lr 0.000439205 | gnorm 0.706 | train_wall 323 | gb_free 6.1 | wall 28534
KL Stats: Epoch 81 Divergences: Uniform: 2.586120807801783 Unigram: 6.419481944148931
2022-01-28 15:35:47 | INFO | fairseq.trainer | begin training epoch 82
2022-01-28 15:35:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:37:08 | INFO | train_inner | epoch 082:     16 / 64 loss=6.431, ppl=86.25, wps=5768.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5200, lr=0.000438529, gnorm=0.695, train_wall=505, gb_free=6.1, wall=28615
2022-01-28 15:41:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:41:39 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.956 | ppl 993.06 | wps 7989.7 | wpb 2034.1 | bsz 4 | num_updates 5248 | best_loss 9.367
2022-01-28 15:41:39 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-01-28 15:41:39 | INFO | train | epoch 082 | loss 6.395 | ppl 84.18 | wps 5925.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5248 | lr 0.000436519 | gnorm 0.679 | train_wall 323 | gb_free 6.1 | wall 28886
KL Stats: Epoch 82 Divergences: Uniform: 2.5887308823390374 Unigram: 6.523143008715092
2022-01-28 15:41:39 | INFO | fairseq.trainer | begin training epoch 83
2022-01-28 15:41:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:46:04 | INFO | train_inner | epoch 083:     52 / 64 loss=6.379, ppl=83.23, wps=6095.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5300, lr=0.000434372, gnorm=0.691, train_wall=506, gb_free=6.1, wall=29151
2022-01-28 15:47:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:47:32 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.026 | ppl 1042.6 | wps 7987.4 | wpb 2034.1 | bsz 4 | num_updates 5312 | best_loss 9.367
2022-01-28 15:47:32 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-01-28 15:47:32 | INFO | train | epoch 083 | loss 6.367 | ppl 82.56 | wps 5928.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5312 | lr 0.000433881 | gnorm 0.704 | train_wall 323 | gb_free 6.1 | wall 29238
KL Stats: Epoch 83 Divergences: Uniform: 2.587438491096615 Unigram: 6.606385415572883
2022-01-28 15:47:32 | INFO | fairseq.trainer | begin training epoch 84
2022-01-28 15:47:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:53:24 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.034 | ppl 1048.35 | wps 7985.1 | wpb 2034.1 | bsz 4 | num_updates 5376 | best_loss 9.367
2022-01-28 15:53:24 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-01-28 15:53:24 | INFO | train | epoch 084 | loss 6.34 | ppl 81 | wps 5926.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5376 | lr 0.000431291 | gnorm 0.713 | train_wall 323 | gb_free 6.1 | wall 29591
KL Stats: Epoch 84 Divergences: Uniform: 2.598686895950392 Unigram: 6.685301621543319
2022-01-28 15:53:24 | INFO | fairseq.trainer | begin training epoch 85
2022-01-28 15:53:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 15:55:26 | INFO | train_inner | epoch 085:     24 / 64 loss=6.33, ppl=80.47, wps=5804.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5400, lr=0.000430331, gnorm=0.713, train_wall=504, gb_free=6.1, wall=29713
2022-01-28 15:58:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 15:59:15 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.064 | ppl 1070.35 | wps 8005.6 | wpb 2034.1 | bsz 4 | num_updates 5440 | best_loss 9.367
2022-01-28 15:59:15 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-01-28 15:59:15 | INFO | train | epoch 085 | loss 6.315 | ppl 79.63 | wps 5942.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5440 | lr 0.000428746 | gnorm 0.705 | train_wall 322 | gb_free 6.1 | wall 29942
KL Stats: Epoch 85 Divergences: Uniform: 2.5992686857502036 Unigram: 6.77236984318963
2022-01-28 15:59:15 | INFO | fairseq.trainer | begin training epoch 86
2022-01-28 15:59:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:04:21 | INFO | train_inner | epoch 086:     60 / 64 loss=6.309, ppl=79.26, wps=6103.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5500, lr=0.000426401, gnorm=0.708, train_wall=505, gb_free=6.1, wall=30248
2022-01-28 16:04:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 16:05:08 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.117 | ppl 1110.64 | wps 8017.6 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 9.367
2022-01-28 16:05:08 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-01-28 16:05:08 | INFO | train | epoch 086 | loss 6.287 | ppl 78.06 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5504 | lr 0.000426246 | gnorm 0.717 | train_wall 323 | gb_free 6.1 | wall 30295
KL Stats: Epoch 86 Divergences: Uniform: 2.606613524479703 Unigram: 6.882195494516765
2022-01-28 16:05:08 | INFO | fairseq.trainer | begin training epoch 87
2022-01-28 16:05:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:10:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:11:00 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.079 | ppl 1081.83 | wps 7991 | wpb 2034.1 | bsz 4 | num_updates 5568 | best_loss 9.367
2022-01-28 16:11:00 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-01-28 16:11:00 | INFO | train | epoch 087 | loss 6.264 | ppl 76.87 | wps 5930.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5568 | lr 0.00042379 | gnorm 0.734 | train_wall 323 | gb_free 6.1 | wall 30647
KL Stats: Epoch 87 Divergences: Uniform: 2.6129017451896277 Unigram: 6.969836264676747
2022-01-28 16:11:00 | INFO | fairseq.trainer | begin training epoch 88
2022-01-28 16:11:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:13:44 | INFO | train_inner | epoch 088:     32 / 64 loss=6.25, ppl=76.12, wps=5799.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5600, lr=0.000422577, gnorm=0.733, train_wall=505, gb_free=6.1, wall=30810
2022-01-28 16:16:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:16:53 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.059 | ppl 1066.75 | wps 8020.2 | wpb 2034.1 | bsz 4 | num_updates 5632 | best_loss 9.367
2022-01-28 16:16:53 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-01-28 16:16:53 | INFO | train | epoch 088 | loss 6.239 | ppl 75.51 | wps 5924.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5632 | lr 0.000421375 | gnorm 0.73 | train_wall 324 | gb_free 6.1 | wall 30999
KL Stats: Epoch 88 Divergences: Uniform: 2.625967640163525 Unigram: 7.075057991652609
2022-01-28 16:16:53 | INFO | fairseq.trainer | begin training epoch 89
2022-01-28 16:16:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:22:45 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.14 | ppl 1128.14 | wps 7979.8 | wpb 2034.1 | bsz 4 | num_updates 5696 | best_loss 9.367
2022-01-28 16:22:45 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-01-28 16:22:45 | INFO | train | epoch 089 | loss 6.218 | ppl 74.43 | wps 5928.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5696 | lr 0.000419001 | gnorm 0.77 | train_wall 323 | gb_free 6.1 | wall 31352
KL Stats: Epoch 89 Divergences: Uniform: 2.628592012831226 Unigram: 7.159730663206436
2022-01-28 16:22:45 | INFO | fairseq.trainer | begin training epoch 90
2022-01-28 16:22:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:23:05 | INFO | train_inner | epoch 090:      4 / 64 loss=6.229, ppl=75.01, wps=5802.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5700, lr=0.000418854, gnorm=0.756, train_wall=504, gb_free=6.1, wall=31372
2022-01-28 16:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:28:37 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.221 | ppl 1193.84 | wps 7984.8 | wpb 2034.1 | bsz 4 | num_updates 5760 | best_loss 9.367
2022-01-28 16:28:37 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-01-28 16:28:37 | INFO | train | epoch 090 | loss 6.191 | ppl 73.05 | wps 5937.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5760 | lr 0.000416667 | gnorm 0.746 | train_wall 323 | gb_free 6.1 | wall 31703
KL Stats: Epoch 90 Divergences: Uniform: 2.622083873892698 Unigram: 7.259036415902657
2022-01-28 16:28:37 | INFO | fairseq.trainer | begin training epoch 91
2022-01-28 16:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:32:01 | INFO | train_inner | epoch 091:     40 / 64 loss=6.172, ppl=72.09, wps=6102.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5800, lr=0.000415227, gnorm=0.742, train_wall=505, gb_free=6.1, wall=31908
2022-01-28 16:34:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:34:29 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.249 | ppl 1217.15 | wps 7982.2 | wpb 2034.1 | bsz 4 | num_updates 5824 | best_loss 9.367
2022-01-28 16:34:29 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-01-28 16:34:29 | INFO | train | epoch 091 | loss 6.166 | ppl 71.79 | wps 5927.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5824 | lr 0.000414371 | gnorm 0.735 | train_wall 323 | gb_free 6.1 | wall 32056
KL Stats: Epoch 91 Divergences: Uniform: 2.625470370226207 Unigram: 7.3749069289212965
2022-01-28 16:34:29 | INFO | fairseq.trainer | begin training epoch 92
2022-01-28 16:34:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:39:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:40:21 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.128 | ppl 1119.13 | wps 7999 | wpb 2034.1 | bsz 4 | num_updates 5888 | best_loss 9.367
2022-01-28 16:40:21 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-01-28 16:40:21 | INFO | train | epoch 092 | loss 6.147 | ppl 70.87 | wps 5930.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5888 | lr 0.000412113 | gnorm 0.771 | train_wall 323 | gb_free 6.1 | wall 32408
KL Stats: Epoch 92 Divergences: Uniform: 2.6397141088552596 Unigram: 7.48493445379915
2022-01-28 16:40:21 | INFO | fairseq.trainer | begin training epoch 93
2022-01-28 16:40:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:41:22 | INFO | train_inner | epoch 093:     12 / 64 loss=6.156, ppl=71.3, wps=5805.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5900, lr=0.000411693, gnorm=0.761, train_wall=504, gb_free=6.1, wall=32469
2022-01-28 16:45:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:46:14 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.123 | ppl 1115.21 | wps 8005.6 | wpb 2034.1 | bsz 4 | num_updates 5952 | best_loss 9.367
2022-01-28 16:46:14 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-01-28 16:46:14 | INFO | train | epoch 093 | loss 6.127 | ppl 69.9 | wps 5926.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5952 | lr 0.000409891 | gnorm 0.791 | train_wall 323 | gb_free 6.1 | wall 32760
KL Stats: Epoch 93 Divergences: Uniform: 2.6415075282229354 Unigram: 7.5816981967057435
2022-01-28 16:46:14 | INFO | fairseq.trainer | begin training epoch 94
2022-01-28 16:46:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:50:18 | INFO | train_inner | epoch 094:     48 / 64 loss=6.109, ppl=69.04, wps=6095.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6000, lr=0.000408248, gnorm=0.784, train_wall=506, gb_free=6.1, wall=33005
2022-01-28 16:51:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 16:52:06 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.169 | ppl 1151.06 | wps 8001.6 | wpb 2034.1 | bsz 4 | num_updates 6016 | best_loss 9.367
2022-01-28 16:52:06 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-01-28 16:52:06 | INFO | train | epoch 094 | loss 6.1 | ppl 68.59 | wps 5926.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6016 | lr 0.000407705 | gnorm 0.781 | train_wall 323 | gb_free 6.1 | wall 33113
KL Stats: Epoch 94 Divergences: Uniform: 2.643601129492071 Unigram: 7.7014284658791805
2022-01-28 16:52:06 | INFO | fairseq.trainer | begin training epoch 95
2022-01-28 16:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:57:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 16:57:58 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.173 | ppl 1154.24 | wps 8020.3 | wpb 2034.1 | bsz 4 | num_updates 6080 | best_loss 9.367
2022-01-28 16:57:58 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-01-28 16:57:58 | INFO | train | epoch 095 | loss 6.08 | ppl 67.64 | wps 5934.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6080 | lr 0.000405554 | gnorm 0.784 | train_wall 323 | gb_free 6.1 | wall 33465
KL Stats: Epoch 95 Divergences: Uniform: 2.6470719101267077 Unigram: 7.803303406389882
2022-01-28 16:57:58 | INFO | fairseq.trainer | begin training epoch 96
2022-01-28 16:57:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 16:59:40 | INFO | train_inner | epoch 096:     20 / 64 loss=6.078, ppl=67.55, wps=5805.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6100, lr=0.000404888, gnorm=0.782, train_wall=504, gb_free=6.1, wall=33567
2022-01-28 17:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:03:50 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.114 | ppl 1108.17 | wps 7992.4 | wpb 2034.1 | bsz 4 | num_updates 6144 | best_loss 9.367
2022-01-28 17:03:50 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-01-28 17:03:50 | INFO | train | epoch 096 | loss 6.06 | ppl 66.7 | wps 5928.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6144 | lr 0.000403436 | gnorm 0.802 | train_wall 323 | gb_free 6.1 | wall 33817
KL Stats: Epoch 96 Divergences: Uniform: 2.6658430888924767 Unigram: 7.919673027803884
2022-01-28 17:03:50 | INFO | fairseq.trainer | begin training epoch 97
2022-01-28 17:03:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:08:36 | INFO | train_inner | epoch 097:     56 / 64 loss=6.053, ppl=66.38, wps=6100.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=6200, lr=0.00040161, gnorm=0.813, train_wall=506, gb_free=6.1, wall=34102
2022-01-28 17:09:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 17:09:43 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.116 | ppl 1109.41 | wps 7998.9 | wpb 2034.1 | bsz 4 | num_updates 6208 | best_loss 9.367
2022-01-28 17:09:43 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-01-28 17:09:43 | INFO | train | epoch 097 | loss 6.039 | ppl 65.76 | wps 5929.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6208 | lr 0.000401351 | gnorm 0.81 | train_wall 323 | gb_free 6.1 | wall 34169
KL Stats: Epoch 97 Divergences: Uniform: 2.662856502211258 Unigram: 8.04961052313414
2022-01-28 17:09:43 | INFO | fairseq.trainer | begin training epoch 98
2022-01-28 17:09:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:15:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:15:35 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.212 | ppl 1186.07 | wps 7994.2 | wpb 2034.1 | bsz 4 | num_updates 6272 | best_loss 9.367
2022-01-28 17:15:35 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-01-28 17:15:35 | INFO | train | epoch 098 | loss 6.017 | ppl 64.74 | wps 5921.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6272 | lr 0.000399298 | gnorm 0.801 | train_wall 324 | gb_free 6.1 | wall 34522
KL Stats: Epoch 98 Divergences: Uniform: 2.655399709715083 Unigram: 8.114884787541309
2022-01-28 17:15:35 | INFO | fairseq.trainer | begin training epoch 99
2022-01-28 17:15:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:17:58 | INFO | train_inner | epoch 099:     28 / 64 loss=6.008, ppl=64.36, wps=5797.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=6300, lr=0.00039841, gnorm=0.814, train_wall=505, gb_free=6.1, wall=34665
2022-01-28 17:21:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:21:28 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.159 | ppl 1143.35 | wps 7985.7 | wpb 2034.1 | bsz 4 | num_updates 6336 | best_loss 9.367
2022-01-28 17:21:28 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-01-28 17:21:28 | INFO | train | epoch 099 | loss 5.998 | ppl 63.92 | wps 5927 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6336 | lr 0.000397276 | gnorm 0.839 | train_wall 323 | gb_free 6.1 | wall 34874
KL Stats: Epoch 99 Divergences: Uniform: 2.6747686315842114 Unigram: 8.253008023823732
2022-01-28 17:21:28 | INFO | fairseq.trainer | begin training epoch 100
2022-01-28 17:21:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:26:53 | INFO | train_inner | epoch 100:     64 / 64 loss=5.998, ppl=63.9, wps=6097.7, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=6400, lr=0.000395285, gnorm=0.858, train_wall=505, gb_free=6.1, wall=35199
2022-01-28 17:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 17:27:20 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.207 | ppl 1181.98 | wps 7994.5 | wpb 2034.1 | bsz 4 | num_updates 6400 | best_loss 9.367
2022-01-28 17:27:20 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-01-28 17:27:20 | INFO | train | epoch 100 | loss 5.979 | ppl 63.07 | wps 5928.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6400 | lr 0.000395285 | gnorm 0.864 | train_wall 323 | gb_free 6.1 | wall 35227
KL Stats: Epoch 100 Divergences: Uniform: 2.6711342964392473 Unigram: 8.364389433524263
2022-01-28 17:27:20 | INFO | fairseq.trainer | begin training epoch 101
2022-01-28 17:27:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:32:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:33:12 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.192 | ppl 1169.79 | wps 8000.5 | wpb 2034.1 | bsz 4 | num_updates 6464 | best_loss 9.367
2022-01-28 17:33:12 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-01-28 17:33:12 | INFO | train | epoch 101 | loss 5.956 | ppl 62.1 | wps 5937.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6464 | lr 0.000393323 | gnorm 0.841 | train_wall 323 | gb_free 6.1 | wall 35579
KL Stats: Epoch 101 Divergences: Uniform: 2.68327020043222 Unigram: 8.500462256904395
2022-01-28 17:33:12 | INFO | fairseq.trainer | begin training epoch 102
2022-01-28 17:33:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:36:15 | INFO | train_inner | epoch 102:     36 / 64 loss=5.942, ppl=61.46, wps=5810.8, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=6500, lr=0.000392232, gnorm=0.853, train_wall=505, gb_free=6.1, wall=35762
2022-01-28 17:38:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:39:04 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.239 | ppl 1208.27 | wps 8001.8 | wpb 2034.1 | bsz 4 | num_updates 6528 | best_loss 9.367
2022-01-28 17:39:04 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-01-28 17:39:04 | INFO | train | epoch 102 | loss 5.939 | ppl 61.36 | wps 5936.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6528 | lr 0.00039139 | gnorm 0.862 | train_wall 323 | gb_free 6.1 | wall 35930
KL Stats: Epoch 102 Divergences: Uniform: 2.682569737229091 Unigram: 8.600492129048611
2022-01-28 17:39:04 | INFO | fairseq.trainer | begin training epoch 103
2022-01-28 17:39:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:44:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:44:56 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.22 | ppl 1193 | wps 8028.2 | wpb 2034.1 | bsz 4 | num_updates 6592 | best_loss 9.367
2022-01-28 17:44:56 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-01-28 17:44:56 | INFO | train | epoch 103 | loss 5.92 | ppl 60.53 | wps 5934.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6592 | lr 0.000389486 | gnorm 0.888 | train_wall 323 | gb_free 6.1 | wall 36282
KL Stats: Epoch 103 Divergences: Uniform: 2.6899839524846048 Unigram: 8.723934454649118
2022-01-28 17:44:56 | INFO | fairseq.trainer | begin training epoch 104
2022-01-28 17:44:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:45:36 | INFO | train_inner | epoch 104:      8 / 64 loss=5.927, ppl=60.85, wps=5807.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6600, lr=0.000389249, gnorm=0.876, train_wall=504, gb_free=6.1, wall=36323
2022-01-28 17:50:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:50:48 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.248 | ppl 1216.01 | wps 7997.6 | wpb 2034.1 | bsz 4 | num_updates 6656 | best_loss 9.367
2022-01-28 17:50:48 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-01-28 17:50:48 | INFO | train | epoch 104 | loss 5.901 | ppl 59.78 | wps 5933.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6656 | lr 0.000387609 | gnorm 0.892 | train_wall 323 | gb_free 6.1 | wall 36634
KL Stats: Epoch 104 Divergences: Uniform: 2.6820601869888745 Unigram: 8.831775217018178
2022-01-28 17:50:48 | INFO | fairseq.trainer | begin training epoch 105
2022-01-28 17:50:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 17:54:32 | INFO | train_inner | epoch 105:     44 / 64 loss=5.887, ppl=59.17, wps=6098.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6700, lr=0.000386334, gnorm=0.881, train_wall=506, gb_free=6.1, wall=36859
2022-01-28 17:56:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 17:56:40 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.178 | ppl 1158.62 | wps 8006.5 | wpb 2034.1 | bsz 4 | num_updates 6720 | best_loss 9.367
2022-01-28 17:56:40 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-01-28 17:56:40 | INFO | train | epoch 105 | loss 5.882 | ppl 58.96 | wps 5919.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6720 | lr 0.000385758 | gnorm 0.881 | train_wall 324 | gb_free 6.1 | wall 36987
KL Stats: Epoch 105 Divergences: Uniform: 2.686318610225632 Unigram: 8.950397005940458
2022-01-28 17:56:40 | INFO | fairseq.trainer | begin training epoch 106
2022-01-28 17:56:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:02:33 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.266 | ppl 1231.38 | wps 7983.4 | wpb 2034.1 | bsz 4 | num_updates 6784 | best_loss 9.367
2022-01-28 18:02:33 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-01-28 18:02:33 | INFO | train | epoch 106 | loss 5.861 | ppl 58.13 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6784 | lr 0.000383934 | gnorm 0.864 | train_wall 323 | gb_free 6.1 | wall 37339
KL Stats: Epoch 106 Divergences: Uniform: 2.6882989422459103 Unigram: 9.073318889074415
2022-01-28 18:02:33 | INFO | fairseq.trainer | begin training epoch 107
2022-01-28 18:02:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:03:54 | INFO | train_inner | epoch 107:     16 / 64 loss=5.866, ppl=58.31, wps=5799.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6800, lr=0.000383482, gnorm=0.874, train_wall=505, gb_free=6.1, wall=37421
2022-01-28 18:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:08:25 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.179 | ppl 1159.17 | wps 7990.9 | wpb 2034.1 | bsz 4 | num_updates 6848 | best_loss 9.367
2022-01-28 18:08:25 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-01-28 18:08:25 | INFO | train | epoch 107 | loss 5.845 | ppl 57.46 | wps 5923.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6848 | lr 0.000382136 | gnorm 0.913 | train_wall 323 | gb_free 6.1 | wall 37692
KL Stats: Epoch 107 Divergences: Uniform: 2.6996137273613208 Unigram: 9.232225221260569
2022-01-28 18:08:25 | INFO | fairseq.trainer | begin training epoch 108
2022-01-28 18:08:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:12:50 | INFO | train_inner | epoch 108:     52 / 64 loss=5.838, ppl=57.19, wps=6096.9, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=6900, lr=0.000380693, gnorm=0.902, train_wall=506, gb_free=6.1, wall=37957
2022-01-28 18:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:14:18 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.377 | ppl 1329.86 | wps 7986.8 | wpb 2034.1 | bsz 4 | num_updates 6912 | best_loss 9.367
2022-01-28 18:14:18 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-01-28 18:14:18 | INFO | train | epoch 108 | loss 5.828 | ppl 56.82 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6912 | lr 0.000380363 | gnorm 0.89 | train_wall 323 | gb_free 6.1 | wall 38044
KL Stats: Epoch 108 Divergences: Uniform: 2.696439390546525 Unigram: 9.322189066497897
2022-01-28 18:14:18 | INFO | fairseq.trainer | begin training epoch 109
2022-01-28 18:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:19:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:20:10 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.349 | ppl 1303.94 | wps 7975.1 | wpb 2034.1 | bsz 4 | num_updates 6976 | best_loss 9.367
2022-01-28 18:20:10 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-01-28 18:20:10 | INFO | train | epoch 109 | loss 5.813 | ppl 56.2 | wps 5934 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6976 | lr 0.000378614 | gnorm 0.925 | train_wall 323 | gb_free 6.1 | wall 38396
KL Stats: Epoch 109 Divergences: Uniform: 2.698321076324131 Unigram: 9.47556386923081
2022-01-28 18:20:10 | INFO | fairseq.trainer | begin training epoch 110
2022-01-28 18:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:22:12 | INFO | train_inner | epoch 110:     24 / 64 loss=5.804, ppl=55.89, wps=5803.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7000, lr=0.000377964, gnorm=0.918, train_wall=504, gb_free=6.1, wall=38519
2022-01-28 18:25:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 18:26:02 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.37 | ppl 1323.31 | wps 7979.1 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 9.367
2022-01-28 18:26:02 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-01-28 18:26:02 | INFO | train | epoch 110 | loss 5.792 | ppl 55.4 | wps 5921.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.923 | train_wall 324 | gb_free 6.1 | wall 38749
KL Stats: Epoch 110 Divergences: Uniform: 2.7050529262832255 Unigram: 9.579249918328568
2022-01-28 18:26:02 | INFO | fairseq.trainer | begin training epoch 111
2022-01-28 18:26:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:31:09 | INFO | train_inner | epoch 111:     60 / 64 loss=5.79, ppl=55.32, wps=6091.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7100, lr=0.000375293, gnorm=0.932, train_wall=506, gb_free=6.1, wall=39055
2022-01-28 18:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:31:55 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.286 | ppl 1248.4 | wps 8007.3 | wpb 2034.1 | bsz 4 | num_updates 7104 | best_loss 9.367
2022-01-28 18:31:55 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-01-28 18:31:55 | INFO | train | epoch 111 | loss 5.776 | ppl 54.79 | wps 5922.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7104 | lr 0.000375188 | gnorm 0.943 | train_wall 324 | gb_free 6.1 | wall 39102
KL Stats: Epoch 111 Divergences: Uniform: 2.7140120972118216 Unigram: 9.750399037143996
2022-01-28 18:31:55 | INFO | fairseq.trainer | begin training epoch 112
2022-01-28 18:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:37:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 18:37:48 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.388 | ppl 1339.61 | wps 7982.9 | wpb 2034.1 | bsz 4 | num_updates 7168 | best_loss 9.367
2022-01-28 18:37:48 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-01-28 18:37:48 | INFO | train | epoch 112 | loss 5.761 | ppl 54.23 | wps 5923.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7168 | lr 0.000373509 | gnorm 1.001 | train_wall 323 | gb_free 6.1 | wall 39454
KL Stats: Epoch 112 Divergences: Uniform: 2.7003583018112014 Unigram: 9.856821697233414
2022-01-28 18:37:48 | INFO | fairseq.trainer | begin training epoch 113
2022-01-28 18:37:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:40:30 | INFO | train_inner | epoch 113:     32 / 64 loss=5.748, ppl=53.73, wps=5803, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7200, lr=0.000372678, gnorm=0.989, train_wall=504, gb_free=6.1, wall=39617
2022-01-28 18:43:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:43:39 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.373 | ppl 1325.89 | wps 7996.2 | wpb 2034.1 | bsz 4 | num_updates 7232 | best_loss 9.367
2022-01-28 18:43:39 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-01-28 18:43:39 | INFO | train | epoch 113 | loss 5.739 | ppl 53.41 | wps 5939.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7232 | lr 0.000371853 | gnorm 0.943 | train_wall 323 | gb_free 6.1 | wall 39806
KL Stats: Epoch 113 Divergences: Uniform: 2.7046132554385696 Unigram: 9.96958856290571
2022-01-28 18:43:39 | INFO | fairseq.trainer | begin training epoch 114
2022-01-28 18:43:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:49:31 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.407 | ppl 1357.99 | wps 8024.5 | wpb 2034.1 | bsz 4 | num_updates 7296 | best_loss 9.367
2022-01-28 18:49:31 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-01-28 18:49:31 | INFO | train | epoch 114 | loss 5.725 | ppl 52.91 | wps 5943.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7296 | lr 0.000370218 | gnorm 0.973 | train_wall 322 | gb_free 6.1 | wall 40157
KL Stats: Epoch 114 Divergences: Uniform: 2.7093534721462555 Unigram: 10.101818916060054
2022-01-28 18:49:31 | INFO | fairseq.trainer | begin training epoch 115
2022-01-28 18:49:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:49:51 | INFO | train_inner | epoch 115:      4 / 64 loss=5.737, ppl=53.35, wps=5813.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7300, lr=0.000370117, gnorm=0.965, train_wall=503, gb_free=6.1, wall=40178
2022-01-28 18:54:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 18:55:23 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.293 | ppl 1254.77 | wps 8006.9 | wpb 2034.1 | bsz 4 | num_updates 7360 | best_loss 9.367
2022-01-28 18:55:23 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-01-28 18:55:23 | INFO | train | epoch 115 | loss 5.709 | ppl 52.3 | wps 5936 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7360 | lr 0.000368605 | gnorm 0.977 | train_wall 323 | gb_free 6.1 | wall 40509
KL Stats: Epoch 115 Divergences: Uniform: 2.711619078081235 Unigram: 10.254155125556604
2022-01-28 18:55:23 | INFO | fairseq.trainer | begin training epoch 116
2022-01-28 18:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 18:58:46 | INFO | train_inner | epoch 116:     40 / 64 loss=5.695, ppl=51.8, wps=6107.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7400, lr=0.000367607, gnorm=0.992, train_wall=505, gb_free=6.1, wall=40713
2022-01-28 19:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 19:01:15 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.375 | ppl 1327.9 | wps 7992.3 | wpb 2034.1 | bsz 4 | num_updates 7424 | best_loss 9.367
2022-01-28 19:01:15 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-01-28 19:01:15 | INFO | train | epoch 116 | loss 5.695 | ppl 51.79 | wps 5933.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7424 | lr 0.000367013 | gnorm 1.007 | train_wall 323 | gb_free 6.1 | wall 40861
KL Stats: Epoch 116 Divergences: Uniform: 2.7195903096154592 Unigram: 10.390149412819717
2022-01-28 19:01:15 | INFO | fairseq.trainer | begin training epoch 117
2022-01-28 19:01:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 19:07:07 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.476 | ppl 1424.54 | wps 8008.7 | wpb 2034.1 | bsz 4 | num_updates 7488 | best_loss 9.367
2022-01-28 19:07:07 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-01-28 19:07:07 | INFO | train | epoch 117 | loss 5.678 | ppl 51.2 | wps 5930.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7488 | lr 0.000365441 | gnorm 1.015 | train_wall 323 | gb_free 6.1 | wall 41214
KL Stats: Epoch 117 Divergences: Uniform: 2.715423236039285 Unigram: 10.516585485066408
2022-01-28 19:07:07 | INFO | fairseq.trainer | begin training epoch 118
2022-01-28 19:07:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:08:08 | INFO | train_inner | epoch 118:     12 / 64 loss=5.684, ppl=51.4, wps=5804.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7500, lr=0.000365148, gnorm=1.012, train_wall=504, gb_free=6.1, wall=41275
2022-01-28 19:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 19:12:59 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.377 | ppl 1329.93 | wps 8002 | wpb 2034.1 | bsz 4 | num_updates 7552 | best_loss 9.367
2022-01-28 19:12:59 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-01-28 19:12:59 | INFO | train | epoch 118 | loss 5.662 | ppl 50.62 | wps 5933.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7552 | lr 0.000363889 | gnorm 1.003 | train_wall 323 | gb_free 6.1 | wall 41566
KL Stats: Epoch 118 Divergences: Uniform: 2.7180771513404287 Unigram: 10.66273133319799
2022-01-28 19:12:59 | INFO | fairseq.trainer | begin training epoch 119
2022-01-28 19:12:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:17:03 | INFO | train_inner | epoch 119:     48 / 64 loss=5.65, ppl=50.2, wps=6101.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7600, lr=0.000362738, gnorm=1.009, train_wall=505, gb_free=6.1, wall=41810
2022-01-28 19:18:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:18:51 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.342 | ppl 1297.93 | wps 8007 | wpb 2034.1 | bsz 4 | num_updates 7616 | best_loss 9.367
2022-01-28 19:18:51 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-01-28 19:18:51 | INFO | train | epoch 119 | loss 5.647 | ppl 50.1 | wps 5928.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7616 | lr 0.000362357 | gnorm 1.044 | train_wall 323 | gb_free 6.1 | wall 41918
KL Stats: Epoch 119 Divergences: Uniform: 2.7246230238022617 Unigram: 10.809088548659432
2022-01-28 19:18:51 | INFO | fairseq.trainer | begin training epoch 120
2022-01-28 19:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:24:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:24:43 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.407 | ppl 1357.28 | wps 8003.3 | wpb 2034.1 | bsz 4 | num_updates 7680 | best_loss 9.367
2022-01-28 19:24:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7680 updates
2022-01-28 19:24:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint120.pt
2022-01-28 19:24:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint120.pt
2022-01-28 19:24:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint120.pt (epoch 120 @ 7680 updates, score 10.407) (writing took 3.069957137107849 seconds)
2022-01-28 19:24:46 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-01-28 19:24:46 | INFO | train | epoch 120 | loss 5.629 | ppl 49.47 | wps 5878.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7680 | lr 0.000360844 | gnorm 1.015 | train_wall 323 | gb_free 6.1 | wall 42273
KL Stats: Epoch 120 Divergences: Uniform: 2.729175880862192 Unigram: 10.97104313253986
2022-01-28 19:24:46 | INFO | fairseq.trainer | begin training epoch 121
2022-01-28 19:24:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:26:28 | INFO | train_inner | epoch 121:     20 / 64 loss=5.632, ppl=49.58, wps=5774.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7700, lr=0.000360375, gnorm=1.052, train_wall=504, gb_free=6.1, wall=42375
2022-01-28 19:30:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:30:38 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.366 | ppl 1319.82 | wps 8000.5 | wpb 2034.1 | bsz 4 | num_updates 7744 | best_loss 9.367
2022-01-28 19:30:38 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-01-28 19:30:38 | INFO | train | epoch 121 | loss 5.616 | ppl 49.04 | wps 5936.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7744 | lr 0.00035935 | gnorm 1.088 | train_wall 323 | gb_free 6.1 | wall 42625
KL Stats: Epoch 121 Divergences: Uniform: 2.7259869901434453 Unigram: 11.091360335978624
2022-01-28 19:30:38 | INFO | fairseq.trainer | begin training epoch 122
2022-01-28 19:30:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:35:23 | INFO | train_inner | epoch 122:     56 / 64 loss=5.613, ppl=48.93, wps=6104.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7800, lr=0.000358057, gnorm=1.079, train_wall=505, gb_free=6.1, wall=42910
2022-01-28 19:36:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:36:30 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.311 | ppl 1270.58 | wps 8005.2 | wpb 2034.1 | bsz 4 | num_updates 7808 | best_loss 9.367
2022-01-28 19:36:30 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-01-28 19:36:30 | INFO | train | epoch 122 | loss 5.604 | ppl 48.64 | wps 5934 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7808 | lr 0.000357874 | gnorm 1.07 | train_wall 323 | gb_free 6.1 | wall 42977
KL Stats: Epoch 122 Divergences: Uniform: 2.7261597727120743 Unigram: 11.227254933587085
2022-01-28 19:36:30 | INFO | fairseq.trainer | begin training epoch 123
2022-01-28 19:36:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:42:22 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.435 | ppl 1384.41 | wps 7999.2 | wpb 2034.1 | bsz 4 | num_updates 7872 | best_loss 9.367
2022-01-28 19:42:22 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-01-28 19:42:22 | INFO | train | epoch 123 | loss 5.583 | ppl 47.94 | wps 5933.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7872 | lr 0.000356416 | gnorm 1.062 | train_wall 323 | gb_free 6.1 | wall 43329
KL Stats: Epoch 123 Divergences: Uniform: 2.730179082114486 Unigram: 11.399337170021406
2022-01-28 19:42:22 | INFO | fairseq.trainer | begin training epoch 124
2022-01-28 19:42:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:44:45 | INFO | train_inner | epoch 124:     28 / 64 loss=5.576, ppl=47.69, wps=5809.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7900, lr=0.000355784, gnorm=1.058, train_wall=504, gb_free=6.1, wall=43471
2022-01-28 19:47:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:48:14 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.455 | ppl 1403.24 | wps 7996.3 | wpb 2034.1 | bsz 4 | num_updates 7936 | best_loss 9.367
2022-01-28 19:48:14 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-01-28 19:48:14 | INFO | train | epoch 124 | loss 5.567 | ppl 47.41 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7936 | lr 0.000354976 | gnorm 1.056 | train_wall 323 | gb_free 6.1 | wall 43681
KL Stats: Epoch 124 Divergences: Uniform: 2.7250670971893705 Unigram: 11.536971631919972
2022-01-28 19:48:14 | INFO | fairseq.trainer | begin training epoch 125
2022-01-28 19:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:53:39 | INFO | train_inner | epoch 125:     64 / 64 loss=5.57, ppl=47.51, wps=6096.5, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=8000, lr=0.000353553, gnorm=1.11, train_wall=505, gb_free=6.1, wall=44006
2022-01-28 19:53:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 19:54:07 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.455 | ppl 1403.22 | wps 8005.4 | wpb 2034.1 | bsz 4 | num_updates 8000 | best_loss 9.367
2022-01-28 19:54:07 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-01-28 19:54:07 | INFO | train | epoch 125 | loss 5.556 | ppl 47.06 | wps 5924 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8000 | lr 0.000353553 | gnorm 1.146 | train_wall 324 | gb_free 6.1 | wall 44033
KL Stats: Epoch 125 Divergences: Uniform: 2.733256253040583 Unigram: 11.69152771289011
2022-01-28 19:54:07 | INFO | fairseq.trainer | begin training epoch 126
2022-01-28 19:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 19:59:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 19:59:59 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.4 | ppl 1350.91 | wps 7977.3 | wpb 2034.1 | bsz 4 | num_updates 8064 | best_loss 9.367
2022-01-28 19:59:59 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-01-28 19:59:59 | INFO | train | epoch 126 | loss 5.54 | ppl 46.51 | wps 5922.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8064 | lr 0.000352148 | gnorm 1.053 | train_wall 323 | gb_free 6.1 | wall 44386
KL Stats: Epoch 126 Divergences: Uniform: 2.733161453876927 Unigram: 11.85844756633083
2022-01-28 19:59:59 | INFO | fairseq.trainer | begin training epoch 127
2022-01-28 19:59:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:03:02 | INFO | train_inner | epoch 127:     36 / 64 loss=5.528, ppl=46.13, wps=5802.6, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=8100, lr=0.000351364, gnorm=1.09, train_wall=506, gb_free=6.1, wall=44569
2022-01-28 20:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:05:51 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.445 | ppl 1393.84 | wps 8020.3 | wpb 2034.1 | bsz 4 | num_updates 8128 | best_loss 9.367
2022-01-28 20:05:51 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-01-28 20:05:51 | INFO | train | epoch 127 | loss 5.528 | ppl 46.14 | wps 5940 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8128 | lr 0.000350758 | gnorm 1.161 | train_wall 323 | gb_free 6.1 | wall 44738
KL Stats: Epoch 127 Divergences: Uniform: 2.736209134763471 Unigram: 12.027714906667955
2022-01-28 20:05:51 | INFO | fairseq.trainer | begin training epoch 128
2022-01-28 20:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:11:43 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.374 | ppl 1327.37 | wps 7973 | wpb 2034.1 | bsz 4 | num_updates 8192 | best_loss 9.367
2022-01-28 20:11:43 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-01-28 20:11:43 | INFO | train | epoch 128 | loss 5.512 | ppl 45.62 | wps 5927.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8192 | lr 0.000349386 | gnorm 1.095 | train_wall 323 | gb_free 6.1 | wall 45090
KL Stats: Epoch 128 Divergences: Uniform: 2.7327886564277617 Unigram: 12.170537882575024
2022-01-28 20:11:43 | INFO | fairseq.trainer | begin training epoch 129
2022-01-28 20:11:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:12:24 | INFO | train_inner | epoch 129:      8 / 64 loss=5.52, ppl=45.88, wps=5805.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8200, lr=0.000349215, gnorm=1.118, train_wall=504, gb_free=6.1, wall=45131
2022-01-28 20:17:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:17:35 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.417 | ppl 1366.91 | wps 7995.4 | wpb 2034.1 | bsz 4 | num_updates 8256 | best_loss 9.367
2022-01-28 20:17:35 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-01-28 20:17:35 | INFO | train | epoch 129 | loss 5.499 | ppl 45.23 | wps 5930.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8256 | lr 0.000348029 | gnorm 1.136 | train_wall 323 | gb_free 6.1 | wall 45442
KL Stats: Epoch 129 Divergences: Uniform: 2.722948059208172 Unigram: 12.31550442712126
2022-01-28 20:17:35 | INFO | fairseq.trainer | begin training epoch 130
2022-01-28 20:17:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:21:20 | INFO | train_inner | epoch 130:     44 / 64 loss=5.486, ppl=44.81, wps=6097.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=8300, lr=0.000347105, gnorm=1.143, train_wall=506, gb_free=6.1, wall=45667
2022-01-28 20:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:23:28 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.369 | ppl 1322.58 | wps 7986.8 | wpb 2034.1 | bsz 4 | num_updates 8320 | best_loss 9.367
2022-01-28 20:23:28 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-01-28 20:23:28 | INFO | train | epoch 130 | loss 5.485 | ppl 44.78 | wps 5921 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8320 | lr 0.000346688 | gnorm 1.193 | train_wall 324 | gb_free 6.1 | wall 45795
KL Stats: Epoch 130 Divergences: Uniform: 2.737905984870921 Unigram: 12.50025586578193
2022-01-28 20:23:28 | INFO | fairseq.trainer | begin training epoch 131
2022-01-28 20:23:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:28:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 20:29:21 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.405 | ppl 1355.53 | wps 7987.7 | wpb 2034.1 | bsz 4 | num_updates 8384 | best_loss 9.367
2022-01-28 20:29:21 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-01-28 20:29:21 | INFO | train | epoch 131 | loss 5.47 | ppl 44.32 | wps 5919 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8384 | lr 0.000345362 | gnorm 1.119 | train_wall 324 | gb_free 6.1 | wall 46148
KL Stats: Epoch 131 Divergences: Uniform: 2.738012293101521 Unigram: 12.658430806383047
2022-01-28 20:29:21 | INFO | fairseq.trainer | begin training epoch 132
2022-01-28 20:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:30:43 | INFO | train_inner | epoch 132:     16 / 64 loss=5.474, ppl=44.44, wps=5794.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8400, lr=0.000345033, gnorm=1.148, train_wall=505, gb_free=6.1, wall=46229
2022-01-28 20:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:35:13 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.54 | ppl 1489.21 | wps 7987.5 | wpb 2034.1 | bsz 4 | num_updates 8448 | best_loss 9.367
2022-01-28 20:35:13 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-01-28 20:35:13 | INFO | train | epoch 132 | loss 5.459 | ppl 43.99 | wps 5930.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8448 | lr 0.000344051 | gnorm 1.242 | train_wall 323 | gb_free 6.1 | wall 46500
KL Stats: Epoch 132 Divergences: Uniform: 2.7320599589832506 Unigram: 12.807607592342697
2022-01-28 20:35:13 | INFO | fairseq.trainer | begin training epoch 133
2022-01-28 20:35:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:39:38 | INFO | train_inner | epoch 133:     52 / 64 loss=5.453, ppl=43.81, wps=6104.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8500, lr=0.000342997, gnorm=1.258, train_wall=505, gb_free=6.1, wall=46765
2022-01-28 20:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:41:05 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.46 | ppl 1408.11 | wps 7996.9 | wpb 2034.1 | bsz 4 | num_updates 8512 | best_loss 9.367
2022-01-28 20:41:05 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-01-28 20:41:05 | INFO | train | epoch 133 | loss 5.447 | ppl 43.61 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8512 | lr 0.000342755 | gnorm 1.216 | train_wall 323 | gb_free 6.1 | wall 46852
KL Stats: Epoch 133 Divergences: Uniform: 2.737196039634972 Unigram: 12.937187047518998
2022-01-28 20:41:05 | INFO | fairseq.trainer | begin training epoch 134
2022-01-28 20:41:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:46:57 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.523 | ppl 1471 | wps 7991 | wpb 2034.1 | bsz 4 | num_updates 8576 | best_loss 9.367
2022-01-28 20:46:57 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-01-28 20:46:57 | INFO | train | epoch 134 | loss 5.432 | ppl 43.17 | wps 5925.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8576 | lr 0.000341474 | gnorm 1.27 | train_wall 323 | gb_free 6.1 | wall 47204
KL Stats: Epoch 134 Divergences: Uniform: 2.739526317429659 Unigram: 13.10970713813941
2022-01-28 20:46:57 | INFO | fairseq.trainer | begin training epoch 135
2022-01-28 20:46:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:49:00 | INFO | train_inner | epoch 135:     24 / 64 loss=5.43, ppl=43.12, wps=5800.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8600, lr=0.000340997, gnorm=1.237, train_wall=505, gb_free=6.1, wall=47327
2022-01-28 20:52:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:52:50 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.474 | ppl 1421.85 | wps 7995.6 | wpb 2034.1 | bsz 4 | num_updates 8640 | best_loss 9.367
2022-01-28 20:52:50 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-01-28 20:52:50 | INFO | train | epoch 135 | loss 5.417 | ppl 42.72 | wps 5932.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8640 | lr 0.000340207 | gnorm 1.193 | train_wall 323 | gb_free 6.1 | wall 47556
KL Stats: Epoch 135 Divergences: Uniform: 2.733694058336579 Unigram: 13.271948532057083
2022-01-28 20:52:50 | INFO | fairseq.trainer | begin training epoch 136
2022-01-28 20:52:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 20:57:55 | INFO | train_inner | epoch 136:     60 / 64 loss=5.415, ppl=42.67, wps=6106.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=8700, lr=0.000339032, gnorm=1.269, train_wall=505, gb_free=6.1, wall=47862
2022-01-28 20:58:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 20:58:42 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.54 | ppl 1489.34 | wps 7995.6 | wpb 2034.1 | bsz 4 | num_updates 8704 | best_loss 9.367
2022-01-28 20:58:42 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-01-28 20:58:42 | INFO | train | epoch 136 | loss 5.406 | ppl 42.39 | wps 5932 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8704 | lr 0.000338954 | gnorm 1.311 | train_wall 323 | gb_free 6.1 | wall 47908
KL Stats: Epoch 136 Divergences: Uniform: 2.7360222295184586 Unigram: 13.429611592942537
2022-01-28 20:58:42 | INFO | fairseq.trainer | begin training epoch 137
2022-01-28 20:58:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 21:04:34 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.543 | ppl 1492.15 | wps 7981.7 | wpb 2034.1 | bsz 4 | num_updates 8768 | best_loss 9.367
2022-01-28 21:04:34 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-01-28 21:04:34 | INFO | train | epoch 137 | loss 5.391 | ppl 41.95 | wps 5923.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8768 | lr 0.000337715 | gnorm 1.24 | train_wall 323 | gb_free 6.1 | wall 48261
KL Stats: Epoch 137 Divergences: Uniform: 2.737093146197594 Unigram: 13.598219139651093
2022-01-28 21:04:34 | INFO | fairseq.trainer | begin training epoch 138
2022-01-28 21:04:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:07:18 | INFO | train_inner | epoch 138:     32 / 64 loss=5.381, ppl=41.68, wps=5795.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8800, lr=0.0003371, gnorm=1.245, train_wall=505, gb_free=6.1, wall=48424
2022-01-28 21:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 21:10:27 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.584 | ppl 1534.83 | wps 7956.8 | wpb 2034.1 | bsz 4 | num_updates 8832 | best_loss 9.367
2022-01-28 21:10:27 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-01-28 21:10:27 | INFO | train | epoch 138 | loss 5.377 | ppl 41.57 | wps 5922.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8832 | lr 0.000336489 | gnorm 1.291 | train_wall 323 | gb_free 6.1 | wall 48614
KL Stats: Epoch 138 Divergences: Uniform: 2.738551012420993 Unigram: 13.766226203400349
2022-01-28 21:10:27 | INFO | fairseq.trainer | begin training epoch 139
2022-01-28 21:10:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:15:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:16:19 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.601 | ppl 1553.25 | wps 7989.8 | wpb 2034.1 | bsz 4 | num_updates 8896 | best_loss 9.367
2022-01-28 21:16:19 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-01-28 21:16:19 | INFO | train | epoch 139 | loss 5.364 | ppl 41.17 | wps 5924.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8896 | lr 0.000335276 | gnorm 1.308 | train_wall 323 | gb_free 6.1 | wall 48966
KL Stats: Epoch 139 Divergences: Uniform: 2.7409421850510083 Unigram: 13.9386935202323
2022-01-28 21:16:19 | INFO | fairseq.trainer | begin training epoch 140
2022-01-28 21:16:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:16:40 | INFO | train_inner | epoch 140:      4 / 64 loss=5.372, ppl=41.42, wps=5797.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8900, lr=0.000335201, gnorm=1.314, train_wall=505, gb_free=6.1, wall=48987
2022-01-28 21:21:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:22:12 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.582 | ppl 1533.2 | wps 7991.7 | wpb 2034.1 | bsz 4 | num_updates 8960 | best_loss 9.367
2022-01-28 21:22:12 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-01-28 21:22:12 | INFO | train | epoch 140 | loss 5.352 | ppl 40.85 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8960 | lr 0.000334077 | gnorm 1.345 | train_wall 323 | gb_free 6.1 | wall 49318
KL Stats: Epoch 140 Divergences: Uniform: 2.7314647625035517 Unigram: 14.091007070968905
2022-01-28 21:22:12 | INFO | fairseq.trainer | begin training epoch 141
2022-01-28 21:22:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:25:35 | INFO | train_inner | epoch 141:     40 / 64 loss=5.342, ppl=40.55, wps=6102.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9000, lr=0.000333333, gnorm=1.312, train_wall=505, gb_free=6.1, wall=49522
2022-01-28 21:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:28:04 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.525 | ppl 1473.76 | wps 7980.2 | wpb 2034.1 | bsz 4 | num_updates 9024 | best_loss 9.367
2022-01-28 21:28:04 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-01-28 21:28:04 | INFO | train | epoch 141 | loss 5.336 | ppl 40.4 | wps 5930.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9024 | lr 0.00033289 | gnorm 1.29 | train_wall 323 | gb_free 6.1 | wall 49671
KL Stats: Epoch 141 Divergences: Uniform: 2.740188911656406 Unigram: 14.28377036862092
2022-01-28 21:28:04 | INFO | fairseq.trainer | begin training epoch 142
2022-01-28 21:28:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:33:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:33:56 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.606 | ppl 1558.66 | wps 8012 | wpb 2034.1 | bsz 4 | num_updates 9088 | best_loss 9.367
2022-01-28 21:33:56 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-01-28 21:33:56 | INFO | train | epoch 142 | loss 5.325 | ppl 40.09 | wps 5935.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9088 | lr 0.000331716 | gnorm 1.342 | train_wall 323 | gb_free 6.1 | wall 50022
KL Stats: Epoch 142 Divergences: Uniform: 2.7458855215835403 Unigram: 14.47332772476325
2022-01-28 21:33:56 | INFO | fairseq.trainer | begin training epoch 143
2022-01-28 21:33:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:34:57 | INFO | train_inner | epoch 143:     12 / 64 loss=5.326, ppl=40.11, wps=5807.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9100, lr=0.000331497, gnorm=1.336, train_wall=504, gb_free=6.1, wall=50084
2022-01-28 21:39:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:39:47 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.605 | ppl 1557.15 | wps 7995.6 | wpb 2034.1 | bsz 4 | num_updates 9152 | best_loss 9.367
2022-01-28 21:39:47 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-01-28 21:39:47 | INFO | train | epoch 143 | loss 5.316 | ppl 39.83 | wps 5937.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9152 | lr 0.000330554 | gnorm 1.336 | train_wall 323 | gb_free 6.1 | wall 50374
KL Stats: Epoch 143 Divergences: Uniform: 2.7438881317880277 Unigram: 14.645408963501003
2022-01-28 21:39:47 | INFO | fairseq.trainer | begin training epoch 144
2022-01-28 21:39:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:43:52 | INFO | train_inner | epoch 144:     48 / 64 loss=5.312, ppl=39.71, wps=6107, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9200, lr=0.00032969, gnorm=1.395, train_wall=505, gb_free=6.1, wall=50619
2022-01-28 21:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:45:40 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.651 | ppl 1608.32 | wps 7960.4 | wpb 2034.1 | bsz 4 | num_updates 9216 | best_loss 9.367
2022-01-28 21:45:40 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-01-28 21:45:40 | INFO | train | epoch 144 | loss 5.304 | ppl 39.51 | wps 5930.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9216 | lr 0.000329404 | gnorm 1.427 | train_wall 323 | gb_free 6.1 | wall 50726
KL Stats: Epoch 144 Divergences: Uniform: 2.734198400300183 Unigram: 14.78501748086292
2022-01-28 21:45:40 | INFO | fairseq.trainer | begin training epoch 145
2022-01-28 21:45:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:51:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:51:31 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.496 | ppl 1444.27 | wps 7982.6 | wpb 2034.1 | bsz 4 | num_updates 9280 | best_loss 9.367
2022-01-28 21:51:31 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-01-28 21:51:31 | INFO | train | epoch 145 | loss 5.286 | ppl 39.01 | wps 5936.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9280 | lr 0.000328266 | gnorm 1.282 | train_wall 323 | gb_free 6.1 | wall 51078
KL Stats: Epoch 145 Divergences: Uniform: 2.7351098204526805 Unigram: 14.996704175215111
2022-01-28 21:51:31 | INFO | fairseq.trainer | begin training epoch 146
2022-01-28 21:51:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 21:53:13 | INFO | train_inner | epoch 146:     20 / 64 loss=5.283, ppl=38.93, wps=5806.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9300, lr=0.000327913, gnorm=1.302, train_wall=504, gb_free=6.1, wall=51180
2022-01-28 21:56:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 21:57:24 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.639 | ppl 1594.69 | wps 7983.1 | wpb 2034.1 | bsz 4 | num_updates 9344 | best_loss 9.367
2022-01-28 21:57:24 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-01-28 21:57:24 | INFO | train | epoch 146 | loss 5.275 | ppl 38.71 | wps 5927.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9344 | lr 0.00032714 | gnorm 1.323 | train_wall 323 | gb_free 6.1 | wall 51431
KL Stats: Epoch 146 Divergences: Uniform: 2.7359233782971852 Unigram: 15.184274732722635
2022-01-28 21:57:24 | INFO | fairseq.trainer | begin training epoch 147
2022-01-28 21:57:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:02:09 | INFO | train_inner | epoch 147:     56 / 64 loss=5.276, ppl=38.73, wps=6098.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9400, lr=0.000326164, gnorm=1.459, train_wall=506, gb_free=6.1, wall=51716
2022-01-28 22:02:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:03:16 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.527 | ppl 1475.28 | wps 7993.2 | wpb 2034.1 | bsz 4 | num_updates 9408 | best_loss 9.367
2022-01-28 22:03:16 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-01-28 22:03:16 | INFO | train | epoch 147 | loss 5.266 | ppl 38.48 | wps 5928.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9408 | lr 0.000326025 | gnorm 1.557 | train_wall 323 | gb_free 6.1 | wall 51783
KL Stats: Epoch 147 Divergences: Uniform: 2.742071600845005 Unigram: 15.356470605822283
2022-01-28 22:03:16 | INFO | fairseq.trainer | begin training epoch 148
2022-01-28 22:03:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:08:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:09:09 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.626 | ppl 1580.63 | wps 8016.4 | wpb 2034.1 | bsz 4 | num_updates 9472 | best_loss 9.367
2022-01-28 22:09:09 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-01-28 22:09:09 | INFO | train | epoch 148 | loss 5.248 | ppl 38.01 | wps 5924.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9472 | lr 0.000324922 | gnorm 1.333 | train_wall 324 | gb_free 6.1 | wall 52135
KL Stats: Epoch 148 Divergences: Uniform: 2.7375221730760084 Unigram: 15.55571298561147
2022-01-28 22:09:09 | INFO | fairseq.trainer | begin training epoch 149
2022-01-28 22:09:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:11:31 | INFO | train_inner | epoch 149:     28 / 64 loss=5.245, ppl=37.91, wps=5802.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9500, lr=0.000324443, gnorm=1.419, train_wall=504, gb_free=6.1, wall=52278
2022-01-28 22:14:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:15:00 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.629 | ppl 1583.75 | wps 7993.6 | wpb 2034.1 | bsz 4 | num_updates 9536 | best_loss 9.367
2022-01-28 22:15:00 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-01-28 22:15:00 | INFO | train | epoch 149 | loss 5.237 | ppl 37.72 | wps 5939.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9536 | lr 0.00032383 | gnorm 1.512 | train_wall 323 | gb_free 6.1 | wall 52487
KL Stats: Epoch 149 Divergences: Uniform: 2.7402400337639143 Unigram: 15.721850329592552
2022-01-28 22:15:00 | INFO | fairseq.trainer | begin training epoch 150
2022-01-28 22:15:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:20:25 | INFO | train_inner | epoch 150:     64 / 64 loss=5.236, ppl=37.69, wps=6105.1, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=9600, lr=0.000322749, gnorm=1.462, train_wall=504, gb_free=6.1, wall=52812
2022-01-28 22:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:20:52 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.717 | ppl 1683.37 | wps 7998 | wpb 2034.1 | bsz 4 | num_updates 9600 | best_loss 9.367
2022-01-28 22:20:52 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-01-28 22:20:52 | INFO | train | epoch 150 | loss 5.227 | ppl 37.44 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9600 | lr 0.000322749 | gnorm 1.467 | train_wall 323 | gb_free 6.1 | wall 52839
KL Stats: Epoch 150 Divergences: Uniform: 2.7414448973447865 Unigram: 15.874724532041453
2022-01-28 22:20:52 | INFO | fairseq.trainer | begin training epoch 151
2022-01-28 22:20:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:26:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:26:45 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.638 | ppl 1593.5 | wps 8022 | wpb 2034.1 | bsz 4 | num_updates 9664 | best_loss 9.367
2022-01-28 22:26:45 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-01-28 22:26:45 | INFO | train | epoch 151 | loss 5.214 | ppl 37.11 | wps 5924.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9664 | lr 0.000321678 | gnorm 1.461 | train_wall 324 | gb_free 6.1 | wall 53192
KL Stats: Epoch 151 Divergences: Uniform: 2.736597328465407 Unigram: 16.083918237495677
2022-01-28 22:26:45 | INFO | fairseq.trainer | begin training epoch 152
2022-01-28 22:26:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:29:48 | INFO | train_inner | epoch 152:     36 / 64 loss=5.202, ppl=36.82, wps=5801.6, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=9700, lr=0.000321081, gnorm=1.525, train_wall=506, gb_free=6.1, wall=53375
2022-01-28 22:32:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:32:37 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.665 | ppl 1624.18 | wps 7967.8 | wpb 2034.1 | bsz 4 | num_updates 9728 | best_loss 9.367
2022-01-28 22:32:37 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-01-28 22:32:37 | INFO | train | epoch 152 | loss 5.205 | ppl 36.88 | wps 5927.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9728 | lr 0.000320618 | gnorm 1.57 | train_wall 323 | gb_free 6.1 | wall 53544
KL Stats: Epoch 152 Divergences: Uniform: 2.7453821112831274 Unigram: 16.26758734088869
2022-01-28 22:32:37 | INFO | fairseq.trainer | begin training epoch 153
2022-01-28 22:32:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:38:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 22:38:30 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.549 | ppl 1498.11 | wps 8017.1 | wpb 2034.1 | bsz 4 | num_updates 9792 | best_loss 9.367
2022-01-28 22:38:30 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-01-28 22:38:30 | INFO | train | epoch 153 | loss 5.19 | ppl 36.49 | wps 5926.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9792 | lr 0.000319569 | gnorm 1.547 | train_wall 323 | gb_free 6.1 | wall 53896
KL Stats: Epoch 153 Divergences: Uniform: 2.749404184245767 Unigram: 16.454319637720182
2022-01-28 22:38:30 | INFO | fairseq.trainer | begin training epoch 154
2022-01-28 22:38:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:39:10 | INFO | train_inner | epoch 154:      8 / 64 loss=5.198, ppl=36.7, wps=5799.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9800, lr=0.000319438, gnorm=1.546, train_wall=505, gb_free=6.1, wall=53937
2022-01-28 22:43:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:44:21 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.574 | ppl 1524.39 | wps 8016.9 | wpb 2034.1 | bsz 4 | num_updates 9856 | best_loss 9.367
2022-01-28 22:44:21 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-01-28 22:44:21 | INFO | train | epoch 154 | loss 5.18 | ppl 36.25 | wps 5946.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9856 | lr 0.000318529 | gnorm 1.558 | train_wall 322 | gb_free 6.1 | wall 54248
KL Stats: Epoch 154 Divergences: Uniform: 2.7507568626894825 Unigram: 16.650948018642517
2022-01-28 22:44:21 | INFO | fairseq.trainer | begin training epoch 155
2022-01-28 22:44:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:48:05 | INFO | train_inner | epoch 155:     44 / 64 loss=5.17, ppl=36, wps=6112.5, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=9900, lr=0.000317821, gnorm=1.496, train_wall=505, gb_free=6.1, wall=54472
2022-01-28 22:49:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:50:13 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.565 | ppl 1514.75 | wps 7998.3 | wpb 2034.1 | bsz 4 | num_updates 9920 | best_loss 9.367
2022-01-28 22:50:13 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-01-28 22:50:13 | INFO | train | epoch 155 | loss 5.171 | ppl 36.01 | wps 5930.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9920 | lr 0.0003175 | gnorm 1.527 | train_wall 323 | gb_free 6.1 | wall 54600
KL Stats: Epoch 155 Divergences: Uniform: 2.744201875082186 Unigram: 16.821423922122662
2022-01-28 22:50:13 | INFO | fairseq.trainer | begin training epoch 156
2022-01-28 22:50:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:55:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 22:56:05 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.635 | ppl 1590.73 | wps 7998.8 | wpb 2034.1 | bsz 4 | num_updates 9984 | best_loss 9.367
2022-01-28 22:56:05 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-01-28 22:56:05 | INFO | train | epoch 156 | loss 5.157 | ppl 35.67 | wps 5928.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9984 | lr 0.000316481 | gnorm 1.603 | train_wall 323 | gb_free 6.1 | wall 54952
KL Stats: Epoch 156 Divergences: Uniform: 2.7301974865896037 Unigram: 17.02416653131518
2022-01-28 22:56:05 | INFO | fairseq.trainer | begin training epoch 157
2022-01-28 22:56:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 22:57:27 | INFO | train_inner | epoch 157:     16 / 64 loss=5.164, ppl=35.86, wps=5804, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10000, lr=0.000316228, gnorm=1.585, train_wall=504, gb_free=6.1, wall=55034
2022-01-28 23:01:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:01:57 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.613 | ppl 1565.67 | wps 8025.1 | wpb 2034.1 | bsz 4 | num_updates 10048 | best_loss 9.367
2022-01-28 23:01:57 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-01-28 23:01:57 | INFO | train | epoch 157 | loss 5.146 | ppl 35.4 | wps 5945 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10048 | lr 0.000315472 | gnorm 1.613 | train_wall 322 | gb_free 6.1 | wall 55304
KL Stats: Epoch 157 Divergences: Uniform: 2.7301723413133008 Unigram: 17.238524510180003
2022-01-28 23:01:57 | INFO | fairseq.trainer | begin training epoch 158
2022-01-28 23:01:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:06:21 | INFO | train_inner | epoch 158:     52 / 64 loss=5.137, ppl=35.18, wps=6116.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10100, lr=0.000314658, gnorm=1.684, train_wall=504, gb_free=6.1, wall=55568
2022-01-28 23:07:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:07:48 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.576 | ppl 1526.98 | wps 7988.1 | wpb 2034.1 | bsz 4 | num_updates 10112 | best_loss 9.367
2022-01-28 23:07:48 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-01-28 23:07:48 | INFO | train | epoch 158 | loss 5.136 | ppl 35.16 | wps 5943.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10112 | lr 0.000314472 | gnorm 1.662 | train_wall 322 | gb_free 6.1 | wall 55655
KL Stats: Epoch 158 Divergences: Uniform: 2.7443999055751527 Unigram: 17.433084849405436
2022-01-28 23:07:48 | INFO | fairseq.trainer | begin training epoch 159
2022-01-28 23:07:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:13:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:13:39 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.621 | ppl 1574.69 | wps 8029.7 | wpb 2034.1 | bsz 4 | num_updates 10176 | best_loss 9.367
2022-01-28 23:13:39 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-01-28 23:13:39 | INFO | train | epoch 159 | loss 5.123 | ppl 34.84 | wps 5946.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10176 | lr 0.000313481 | gnorm 1.651 | train_wall 322 | gb_free 6.1 | wall 56006
KL Stats: Epoch 159 Divergences: Uniform: 2.7235408106540184 Unigram: 17.622354291567948
2022-01-28 23:13:39 | INFO | fairseq.trainer | begin training epoch 160
2022-01-28 23:13:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:15:42 | INFO | train_inner | epoch 160:     24 / 64 loss=5.123, ppl=34.85, wps=5816.2, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=10200, lr=0.000313112, gnorm=1.695, train_wall=503, gb_free=6.1, wall=56128
2022-01-28 23:19:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:19:31 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.756 | ppl 1729.25 | wps 7997.1 | wpb 2034.1 | bsz 4 | num_updates 10240 | best_loss 9.367
2022-01-28 23:19:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 10240 updates
2022-01-28 23:19:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint160.pt
2022-01-28 23:19:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint160.pt
2022-01-28 23:19:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint160.pt (epoch 160 @ 10240 updates, score 10.756) (writing took 3.0585827631875873 seconds)
2022-01-28 23:19:34 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-01-28 23:19:34 | INFO | train | epoch 160 | loss 5.113 | ppl 34.61 | wps 5888.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10240 | lr 0.0003125 | gnorm 1.736 | train_wall 323 | gb_free 6.1 | wall 56361
KL Stats: Epoch 160 Divergences: Uniform: 2.7421163657586733 Unigram: 17.799406679799386
2022-01-28 23:19:34 | INFO | fairseq.trainer | begin training epoch 161
2022-01-28 23:19:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:24:39 | INFO | train_inner | epoch 161:     60 / 64 loss=5.11, ppl=34.54, wps=6080.9, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10300, lr=0.000311588, gnorm=1.694, train_wall=504, gb_free=6.1, wall=56666
2022-01-28 23:24:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:25:25 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.61 | ppl 1562.67 | wps 8015.5 | wpb 2034.1 | bsz 4 | num_updates 10304 | best_loss 9.367
2022-01-28 23:25:25 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-01-28 23:25:25 | INFO | train | epoch 161 | loss 5.103 | ppl 34.37 | wps 5947.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10304 | lr 0.000311528 | gnorm 1.71 | train_wall 322 | gb_free 6.1 | wall 56712
KL Stats: Epoch 161 Divergences: Uniform: 2.7495445115165187 Unigram: 17.987277000891172
2022-01-28 23:25:25 | INFO | fairseq.trainer | begin training epoch 162
2022-01-28 23:25:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:30:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 23:31:17 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.641 | ppl 1597.26 | wps 7987.9 | wpb 2034.1 | bsz 4 | num_updates 10368 | best_loss 9.367
2022-01-28 23:31:17 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-01-28 23:31:17 | INFO | train | epoch 162 | loss 5.089 | ppl 34.04 | wps 5935.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10368 | lr 0.000310565 | gnorm 1.711 | train_wall 323 | gb_free 6.1 | wall 57064
KL Stats: Epoch 162 Divergences: Uniform: 2.743163057607489 Unigram: 18.19016766706891
2022-01-28 23:31:17 | INFO | fairseq.trainer | begin training epoch 163
2022-01-28 23:31:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:34:00 | INFO | train_inner | epoch 163:     32 / 64 loss=5.075, ppl=33.72, wps=5807.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10400, lr=0.000310087, gnorm=1.701, train_wall=504, gb_free=6.1, wall=57227
2022-01-28 23:36:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:37:09 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.612 | ppl 1565.05 | wps 7986.9 | wpb 2034.1 | bsz 4 | num_updates 10432 | best_loss 9.367
2022-01-28 23:37:09 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-01-28 23:37:09 | INFO | train | epoch 163 | loss 5.08 | ppl 33.82 | wps 5929 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10432 | lr 0.000309611 | gnorm 1.666 | train_wall 323 | gb_free 6.1 | wall 57416
KL Stats: Epoch 163 Divergences: Uniform: 2.744849435604788 Unigram: 18.34116206992429
2022-01-28 23:37:09 | INFO | fairseq.trainer | begin training epoch 164
2022-01-28 23:37:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:42:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:43:02 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.762 | ppl 1736.8 | wps 8006.4 | wpb 2034.1 | bsz 4 | num_updates 10496 | best_loss 9.367
2022-01-28 23:43:02 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-01-28 23:43:02 | INFO | train | epoch 164 | loss 5.068 | ppl 33.55 | wps 5927.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10496 | lr 0.000308665 | gnorm 1.787 | train_wall 323 | gb_free 6.1 | wall 57769
KL Stats: Epoch 164 Divergences: Uniform: 2.73502835579782 Unigram: 18.52818826183551
2022-01-28 23:43:02 | INFO | fairseq.trainer | begin training epoch 165
2022-01-28 23:43:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:43:22 | INFO | train_inner | epoch 165:      4 / 64 loss=5.084, ppl=33.91, wps=5802, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10500, lr=0.000308607, gnorm=1.733, train_wall=504, gb_free=6.1, wall=57789
2022-01-28 23:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-28 23:48:54 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.612 | ppl 1565.47 | wps 7982.3 | wpb 2034.1 | bsz 4 | num_updates 10560 | best_loss 9.367
2022-01-28 23:48:54 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-01-28 23:48:54 | INFO | train | epoch 165 | loss 5.058 | ppl 33.31 | wps 5930.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10560 | lr 0.000307729 | gnorm 1.68 | train_wall 323 | gb_free 6.1 | wall 58121
KL Stats: Epoch 165 Divergences: Uniform: 2.735934260999474 Unigram: 18.709675970471174
2022-01-28 23:48:54 | INFO | fairseq.trainer | begin training epoch 166
2022-01-28 23:48:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-28 23:52:18 | INFO | train_inner | epoch 166:     40 / 64 loss=5.05, ppl=33.13, wps=6103.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10600, lr=0.000307148, gnorm=1.768, train_wall=505, gb_free=6.1, wall=58324
2022-01-28 23:54:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-28 23:54:46 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.649 | ppl 1605.7 | wps 8012 | wpb 2034.1 | bsz 4 | num_updates 10624 | best_loss 9.367
2022-01-28 23:54:46 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-01-28 23:54:46 | INFO | train | epoch 166 | loss 5.047 | ppl 33.07 | wps 5938.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10624 | lr 0.0003068 | gnorm 1.869 | train_wall 323 | gb_free 6.1 | wall 58472
KL Stats: Epoch 166 Divergences: Uniform: 2.740085196293993 Unigram: 18.95232767578342
2022-01-28 23:54:46 | INFO | fairseq.trainer | begin training epoch 167
2022-01-28 23:54:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:00:38 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.7 | ppl 1663.97 | wps 8001.9 | wpb 2034.1 | bsz 4 | num_updates 10688 | best_loss 9.367
2022-01-29 00:00:38 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-01-29 00:00:38 | INFO | train | epoch 167 | loss 5.036 | ppl 32.8 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10688 | lr 0.00030588 | gnorm 1.855 | train_wall 323 | gb_free 6.1 | wall 58825
KL Stats: Epoch 167 Divergences: Uniform: 2.7391298090252834 Unigram: 19.129267411924076
2022-01-29 00:00:38 | INFO | fairseq.trainer | begin training epoch 168
2022-01-29 00:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:01:39 | INFO | train_inner | epoch 168:     12 / 64 loss=5.038, ppl=32.85, wps=5808.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10700, lr=0.000305709, gnorm=1.857, train_wall=504, gb_free=6.1, wall=58886
2022-01-29 00:06:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:06:30 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.61 | ppl 1562.58 | wps 8010.9 | wpb 2034.1 | bsz 4 | num_updates 10752 | best_loss 9.367
2022-01-29 00:06:30 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-01-29 00:06:30 | INFO | train | epoch 168 | loss 5.027 | ppl 32.6 | wps 5939.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10752 | lr 0.000304969 | gnorm 1.981 | train_wall 323 | gb_free 6.1 | wall 59176
KL Stats: Epoch 168 Divergences: Uniform: 2.7466199481640716 Unigram: 19.295536181792333
2022-01-29 00:06:30 | INFO | fairseq.trainer | begin training epoch 169
2022-01-29 00:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:10:34 | INFO | train_inner | epoch 169:     48 / 64 loss=5.023, ppl=32.51, wps=6109.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10800, lr=0.00030429, gnorm=1.94, train_wall=505, gb_free=6.1, wall=59421
2022-01-29 00:11:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:12:21 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.768 | ppl 1743.52 | wps 8019.2 | wpb 2034.1 | bsz 4 | num_updates 10816 | best_loss 9.367
2022-01-29 00:12:21 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-01-29 00:12:21 | INFO | train | epoch 169 | loss 5.014 | ppl 32.32 | wps 5937.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10816 | lr 0.000304065 | gnorm 1.88 | train_wall 323 | gb_free 6.1 | wall 59528
KL Stats: Epoch 169 Divergences: Uniform: 2.7426921347750595 Unigram: 19.495751151920054
2022-01-29 00:12:21 | INFO | fairseq.trainer | begin training epoch 170
2022-01-29 00:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:17:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:18:13 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.688 | ppl 1649.74 | wps 7987.7 | wpb 2034.1 | bsz 4 | num_updates 10880 | best_loss 9.367
2022-01-29 00:18:13 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-01-29 00:18:13 | INFO | train | epoch 170 | loss 5.004 | ppl 32.09 | wps 5936.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10880 | lr 0.00030317 | gnorm 1.915 | train_wall 323 | gb_free 6.1 | wall 59880
KL Stats: Epoch 170 Divergences: Uniform: 2.7347041103398593 Unigram: 19.712106400789075
2022-01-29 00:18:13 | INFO | fairseq.trainer | begin training epoch 171
2022-01-29 00:18:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:19:55 | INFO | train_inner | epoch 171:     20 / 64 loss=4.999, ppl=31.98, wps=5809.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10900, lr=0.000302891, gnorm=1.936, train_wall=504, gb_free=6.1, wall=59982
2022-01-29 00:23:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:24:05 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.747 | ppl 1718.28 | wps 8001.7 | wpb 2034.1 | bsz 4 | num_updates 10944 | best_loss 9.367
2022-01-29 00:24:05 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-01-29 00:24:05 | INFO | train | epoch 171 | loss 4.997 | ppl 31.94 | wps 5937.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10944 | lr 0.000302282 | gnorm 1.953 | train_wall 323 | gb_free 6.1 | wall 60232
KL Stats: Epoch 171 Divergences: Uniform: 2.7365619670641035 Unigram: 19.900431833212185
2022-01-29 00:24:05 | INFO | fairseq.trainer | begin training epoch 172
2022-01-29 00:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:28:50 | INFO | train_inner | epoch 172:     56 / 64 loss=4.996, ppl=31.91, wps=6104.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11000, lr=0.000301511, gnorm=1.969, train_wall=505, gb_free=6.1, wall=60517
2022-01-29 00:29:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:29:57 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.923 | ppl 1941.83 | wps 8001.6 | wpb 2034.1 | bsz 4 | num_updates 11008 | best_loss 9.367
2022-01-29 00:29:57 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-01-29 00:29:57 | INFO | train | epoch 172 | loss 4.984 | ppl 31.65 | wps 5930.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11008 | lr 0.000301402 | gnorm 1.986 | train_wall 323 | gb_free 6.1 | wall 60584
KL Stats: Epoch 172 Divergences: Uniform: 2.7307469917835787 Unigram: 20.066726329852568
2022-01-29 00:29:57 | INFO | fairseq.trainer | begin training epoch 173
2022-01-29 00:29:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:35:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:35:50 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.736 | ppl 1704.99 | wps 8013.6 | wpb 2034.1 | bsz 4 | num_updates 11072 | best_loss 9.367
2022-01-29 00:35:50 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-01-29 00:35:50 | INFO | train | epoch 173 | loss 4.977 | ppl 31.5 | wps 5920.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11072 | lr 0.000300529 | gnorm 2.117 | train_wall 324 | gb_free 6.1 | wall 60937
KL Stats: Epoch 173 Divergences: Uniform: 2.7430002699943845 Unigram: 20.288489547031908
2022-01-29 00:35:50 | INFO | fairseq.trainer | begin training epoch 174
2022-01-29 00:35:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:38:13 | INFO | train_inner | epoch 174:     28 / 64 loss=4.969, ppl=31.33, wps=5797.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11100, lr=0.00030015, gnorm=2.111, train_wall=505, gb_free=6.1, wall=61079
2022-01-29 00:41:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:41:42 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 10.677 | ppl 1637.12 | wps 7972.7 | wpb 2034.1 | bsz 4 | num_updates 11136 | best_loss 9.367
2022-01-29 00:41:42 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-01-29 00:41:42 | INFO | train | epoch 174 | loss 4.965 | ppl 31.24 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11136 | lr 0.000299665 | gnorm 2.102 | train_wall 323 | gb_free 6.1 | wall 61289
KL Stats: Epoch 174 Divergences: Uniform: 2.737023145317555 Unigram: 20.45758776912359
2022-01-29 00:41:42 | INFO | fairseq.trainer | begin training epoch 175
2022-01-29 00:41:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:47:08 | INFO | train_inner | epoch 175:     64 / 64 loss=4.966, ppl=31.26, wps=6093.8, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=11200, lr=0.000298807, gnorm=2.015, train_wall=505, gb_free=6.1, wall=61614
2022-01-29 00:47:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 00:47:35 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.847 | ppl 1842.32 | wps 7965.5 | wpb 2034.1 | bsz 4 | num_updates 11200 | best_loss 9.367
2022-01-29 00:47:35 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-01-29 00:47:35 | INFO | train | epoch 175 | loss 4.956 | ppl 31.03 | wps 5918.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11200 | lr 0.000298807 | gnorm 2.008 | train_wall 324 | gb_free 6.1 | wall 61642
KL Stats: Epoch 175 Divergences: Uniform: 2.7447805233230285 Unigram: 20.639289999029344
2022-01-29 00:47:35 | INFO | fairseq.trainer | begin training epoch 176
2022-01-29 00:47:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:53:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:53:27 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.664 | ppl 1622.51 | wps 8010.5 | wpb 2034.1 | bsz 4 | num_updates 11264 | best_loss 9.367
2022-01-29 00:53:27 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-01-29 00:53:27 | INFO | train | epoch 176 | loss 4.949 | ppl 30.89 | wps 5926.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11264 | lr 0.000297957 | gnorm 2.174 | train_wall 323 | gb_free 6.1 | wall 61994
KL Stats: Epoch 176 Divergences: Uniform: 2.7298858761522857 Unigram: 20.84896805378473
2022-01-29 00:53:27 | INFO | fairseq.trainer | begin training epoch 177
2022-01-29 00:53:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 00:56:31 | INFO | train_inner | epoch 177:     36 / 64 loss=4.937, ppl=30.63, wps=5798.9, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=11300, lr=0.000297482, gnorm=2.212, train_wall=506, gb_free=6.1, wall=62178
2022-01-29 00:58:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 00:59:20 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 10.814 | ppl 1799.85 | wps 7996.1 | wpb 2034.1 | bsz 4 | num_updates 11328 | best_loss 9.367
2022-01-29 00:59:20 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-01-29 00:59:20 | INFO | train | epoch 177 | loss 4.938 | ppl 30.65 | wps 5923.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11328 | lr 0.000297114 | gnorm 2.245 | train_wall 323 | gb_free 6.1 | wall 62347
KL Stats: Epoch 177 Divergences: Uniform: 2.7213256514659734 Unigram: 20.996369788131343
2022-01-29 00:59:20 | INFO | fairseq.trainer | begin training epoch 178
2022-01-29 00:59:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:04:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:05:12 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 10.866 | ppl 1866.26 | wps 8001.1 | wpb 2034.1 | bsz 4 | num_updates 11392 | best_loss 9.367
2022-01-29 01:05:12 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-01-29 01:05:12 | INFO | train | epoch 178 | loss 4.928 | ppl 30.44 | wps 5934.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11392 | lr 0.000296278 | gnorm 2.082 | train_wall 323 | gb_free 6.1 | wall 62699
KL Stats: Epoch 178 Divergences: Uniform: 2.7339255363212076 Unigram: 21.185788215132643
2022-01-29 01:05:12 | INFO | fairseq.trainer | begin training epoch 179
2022-01-29 01:05:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:05:53 | INFO | train_inner | epoch 179:      8 / 64 loss=4.934, ppl=30.58, wps=5804.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11400, lr=0.000296174, gnorm=2.104, train_wall=504, gb_free=6.1, wall=62740
2022-01-29 01:10:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:11:04 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.762 | ppl 1736.76 | wps 7976.2 | wpb 2034.1 | bsz 4 | num_updates 11456 | best_loss 9.367
2022-01-29 01:11:04 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-01-29 01:11:04 | INFO | train | epoch 179 | loss 4.918 | ppl 30.23 | wps 5927.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11456 | lr 0.00029545 | gnorm 2.013 | train_wall 323 | gb_free 6.1 | wall 63051
KL Stats: Epoch 179 Divergences: Uniform: 2.728660544553334 Unigram: 21.383154281975163
2022-01-29 01:11:04 | INFO | fairseq.trainer | begin training epoch 180
2022-01-29 01:11:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:14:48 | INFO | train_inner | epoch 180:     44 / 64 loss=4.915, ppl=30.17, wps=6100.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11500, lr=0.000294884, gnorm=2.151, train_wall=505, gb_free=6.1, wall=63275
2022-01-29 01:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:16:56 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.797 | ppl 1779.25 | wps 7985.2 | wpb 2034.1 | bsz 4 | num_updates 11520 | best_loss 9.367
2022-01-29 01:16:56 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-01-29 01:16:56 | INFO | train | epoch 180 | loss 4.912 | ppl 30.11 | wps 5930.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11520 | lr 0.000294628 | gnorm 2.216 | train_wall 323 | gb_free 6.1 | wall 63403
KL Stats: Epoch 180 Divergences: Uniform: 2.7402117590982544 Unigram: 21.61108750276866
2022-01-29 01:16:56 | INFO | fairseq.trainer | begin training epoch 181
2022-01-29 01:16:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:22:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:22:49 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.658 | ppl 1616.1 | wps 7971.2 | wpb 2034.1 | bsz 4 | num_updates 11584 | best_loss 9.367
2022-01-29 01:22:49 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-01-29 01:22:49 | INFO | train | epoch 181 | loss 4.898 | ppl 29.82 | wps 5926.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11584 | lr 0.000293813 | gnorm 2.228 | train_wall 323 | gb_free 6.1 | wall 63756
KL Stats: Epoch 181 Divergences: Uniform: 2.728833092089711 Unigram: 21.75707679677422
2022-01-29 01:22:49 | INFO | fairseq.trainer | begin training epoch 182
2022-01-29 01:22:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:24:11 | INFO | train_inner | epoch 182:     16 / 64 loss=4.898, ppl=29.83, wps=5798.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11600, lr=0.00029361, gnorm=2.167, train_wall=505, gb_free=6.1, wall=63837
2022-01-29 01:28:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 01:28:42 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 10.766 | ppl 1741.53 | wps 7984.5 | wpb 2034.1 | bsz 4 | num_updates 11648 | best_loss 9.367
2022-01-29 01:28:42 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-01-29 01:28:42 | INFO | train | epoch 182 | loss 4.891 | ppl 29.68 | wps 5923.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11648 | lr 0.000293005 | gnorm 2.064 | train_wall 323 | gb_free 6.1 | wall 64108
KL Stats: Epoch 182 Divergences: Uniform: 2.7278726226850663 Unigram: 21.94735537282213
2022-01-29 01:28:42 | INFO | fairseq.trainer | begin training epoch 183
2022-01-29 01:28:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:33:06 | INFO | train_inner | epoch 183:     52 / 64 loss=4.888, ppl=29.61, wps=6099.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=11700, lr=0.000292353, gnorm=2.135, train_wall=506, gb_free=6.1, wall=64373
2022-01-29 01:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:34:34 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.797 | ppl 1779.7 | wps 7998.1 | wpb 2034.1 | bsz 4 | num_updates 11712 | best_loss 9.367
2022-01-29 01:34:34 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-01-29 01:34:34 | INFO | train | epoch 183 | loss 4.882 | ppl 29.49 | wps 5933.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11712 | lr 0.000292203 | gnorm 2.162 | train_wall 323 | gb_free 6.1 | wall 64460
KL Stats: Epoch 183 Divergences: Uniform: 2.7240640596448187 Unigram: 22.196301343861126
2022-01-29 01:34:34 | INFO | fairseq.trainer | begin training epoch 184
2022-01-29 01:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:39:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 01:40:26 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.97 | ppl 2005.16 | wps 8001.7 | wpb 2034.1 | bsz 4 | num_updates 11776 | best_loss 9.367
2022-01-29 01:40:26 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-01-29 01:40:26 | INFO | train | epoch 184 | loss 4.873 | ppl 29.3 | wps 5929.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11776 | lr 0.000291408 | gnorm 2.3 | train_wall 323 | gb_free 6.1 | wall 64813
KL Stats: Epoch 184 Divergences: Uniform: 2.7247425310885807 Unigram: 22.36908245636948
2022-01-29 01:40:26 | INFO | fairseq.trainer | begin training epoch 185
2022-01-29 01:40:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:42:28 | INFO | train_inner | epoch 185:     24 / 64 loss=4.874, ppl=29.32, wps=5805.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11800, lr=0.000291111, gnorm=2.245, train_wall=504, gb_free=6.1, wall=64935
2022-01-29 01:45:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 01:46:18 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 10.848 | ppl 1842.59 | wps 7977.8 | wpb 2034.1 | bsz 4 | num_updates 11840 | best_loss 9.367
2022-01-29 01:46:18 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-01-29 01:46:18 | INFO | train | epoch 185 | loss 4.864 | ppl 29.12 | wps 5931.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11840 | lr 0.000290619 | gnorm 2.337 | train_wall 323 | gb_free 6.1 | wall 65165
KL Stats: Epoch 185 Divergences: Uniform: 2.723895753482898 Unigram: 22.618660231351956
2022-01-29 01:46:18 | INFO | fairseq.trainer | begin training epoch 186
2022-01-29 01:46:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:51:23 | INFO | train_inner | epoch 186:     60 / 64 loss=4.857, ppl=28.98, wps=6102.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11900, lr=0.000289886, gnorm=2.233, train_wall=505, gb_free=6.1, wall=65470
2022-01-29 01:51:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:52:10 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.883 | ppl 1888.53 | wps 7983.5 | wpb 2034.1 | bsz 4 | num_updates 11904 | best_loss 9.367
2022-01-29 01:52:10 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-01-29 01:52:10 | INFO | train | epoch 186 | loss 4.853 | ppl 28.9 | wps 5933.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11904 | lr 0.000289837 | gnorm 2.134 | train_wall 323 | gb_free 6.1 | wall 65517
KL Stats: Epoch 186 Divergences: Uniform: 2.726902930089965 Unigram: 22.786553116623583
2022-01-29 01:52:10 | INFO | fairseq.trainer | begin training epoch 187
2022-01-29 01:52:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 01:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 01:58:01 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 10.916 | ppl 1932.03 | wps 7991 | wpb 2034.1 | bsz 4 | num_updates 11968 | best_loss 9.367
2022-01-29 01:58:01 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-01-29 01:58:01 | INFO | train | epoch 187 | loss 4.846 | ppl 28.76 | wps 5940.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11968 | lr 0.000289061 | gnorm 2.389 | train_wall 322 | gb_free 6.1 | wall 65868
KL Stats: Epoch 187 Divergences: Uniform: 2.728147494015141 Unigram: 22.973807782318346
2022-01-29 01:58:01 | INFO | fairseq.trainer | begin training epoch 188
2022-01-29 01:58:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:00:44 | INFO | train_inner | epoch 188:     32 / 64 loss=4.838, ppl=28.6, wps=5812.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12000, lr=0.000288675, gnorm=2.324, train_wall=503, gb_free=6.1, wall=66031
2022-01-29 02:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:03:53 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.956 | ppl 1986.95 | wps 7980.1 | wpb 2034.1 | bsz 4 | num_updates 12032 | best_loss 9.367
2022-01-29 02:03:53 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-01-29 02:03:53 | INFO | train | epoch 188 | loss 4.832 | ppl 28.48 | wps 5934.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12032 | lr 0.000288291 | gnorm 2.321 | train_wall 323 | gb_free 6.1 | wall 66220
KL Stats: Epoch 188 Divergences: Uniform: 2.7199366569942334 Unigram: 23.2355814839383
2022-01-29 02:03:53 | INFO | fairseq.trainer | begin training epoch 189
2022-01-29 02:03:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:09:45 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 11.205 | ppl 2361.01 | wps 7978.8 | wpb 2034.1 | bsz 4 | num_updates 12096 | best_loss 9.367
2022-01-29 02:09:45 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-01-29 02:09:45 | INFO | train | epoch 189 | loss 4.825 | ppl 28.35 | wps 5933.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12096 | lr 0.000287527 | gnorm 2.479 | train_wall 323 | gb_free 6.1 | wall 66572
KL Stats: Epoch 189 Divergences: Uniform: 2.7280080740683563 Unigram: 23.37946828545013
2022-01-29 02:09:45 | INFO | fairseq.trainer | begin training epoch 190
2022-01-29 02:09:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:10:06 | INFO | train_inner | epoch 190:      4 / 64 loss=4.832, ppl=28.48, wps=5805.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12100, lr=0.00028748, gnorm=2.494, train_wall=504, gb_free=6.1, wall=66593
2022-01-29 02:15:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:15:37 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 10.982 | ppl 2022.95 | wps 7995.1 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 9.367
2022-01-29 02:15:37 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-01-29 02:15:37 | INFO | train | epoch 190 | loss 4.822 | ppl 28.28 | wps 5934.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12160 | lr 0.00028677 | gnorm 2.605 | train_wall 323 | gb_free 6.1 | wall 66924
KL Stats: Epoch 190 Divergences: Uniform: 2.7283522627695476 Unigram: 23.58050901647467
2022-01-29 02:15:37 | INFO | fairseq.trainer | begin training epoch 191
2022-01-29 02:15:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:19:01 | INFO | train_inner | epoch 191:     40 / 64 loss=4.806, ppl=27.98, wps=6106.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12200, lr=0.000286299, gnorm=2.43, train_wall=505, gb_free=6.1, wall=67128
2022-01-29 02:21:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 02:21:30 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 10.963 | ppl 1995.8 | wps 7984.9 | wpb 2034.1 | bsz 4 | num_updates 12224 | best_loss 9.367
2022-01-29 02:21:30 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-01-29 02:21:30 | INFO | train | epoch 191 | loss 4.81 | ppl 28.06 | wps 5928.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12224 | lr 0.000286018 | gnorm 2.538 | train_wall 323 | gb_free 6.1 | wall 67276
KL Stats: Epoch 191 Divergences: Uniform: 2.7300162198963664 Unigram: 23.819467839166105
2022-01-29 02:21:30 | INFO | fairseq.trainer | begin training epoch 192
2022-01-29 02:21:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:26:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 02:27:23 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 11.048 | ppl 2117.31 | wps 7963.9 | wpb 2034.1 | bsz 4 | num_updates 12288 | best_loss 9.367
2022-01-29 02:27:23 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-01-29 02:27:23 | INFO | train | epoch 192 | loss 4.795 | ppl 27.77 | wps 5915.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12288 | lr 0.000285272 | gnorm 2.34 | train_wall 324 | gb_free 6.1 | wall 67630
KL Stats: Epoch 192 Divergences: Uniform: 2.72546672122615 Unigram: 23.992973614627925
2022-01-29 02:27:23 | INFO | fairseq.trainer | begin training epoch 193
2022-01-29 02:27:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:28:24 | INFO | train_inner | epoch 193:     12 / 64 loss=4.806, ppl=27.98, wps=5791.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12300, lr=0.000285133, gnorm=2.582, train_wall=505, gb_free=6.1, wall=67691
2022-01-29 02:32:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:33:15 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 10.809 | ppl 1793.82 | wps 7999 | wpb 2034.1 | bsz 4 | num_updates 12352 | best_loss 9.367
2022-01-29 02:33:15 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-01-29 02:33:15 | INFO | train | epoch 193 | loss 4.793 | ppl 27.73 | wps 5932.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12352 | lr 0.000284532 | gnorm 2.662 | train_wall 323 | gb_free 6.1 | wall 67982
KL Stats: Epoch 193 Divergences: Uniform: 2.724800129757952 Unigram: 24.152270996459475
2022-01-29 02:33:15 | INFO | fairseq.trainer | begin training epoch 194
2022-01-29 02:33:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:37:20 | INFO | train_inner | epoch 194:     48 / 64 loss=4.786, ppl=27.58, wps=6098.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12400, lr=0.000283981, gnorm=2.448, train_wall=506, gb_free=6.1, wall=68227
2022-01-29 02:38:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:39:08 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 11.198 | ppl 2349.89 | wps 7994.2 | wpb 2034.1 | bsz 4 | num_updates 12416 | best_loss 9.367
2022-01-29 02:39:08 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-01-29 02:39:08 | INFO | train | epoch 194 | loss 4.781 | ppl 27.5 | wps 5920.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12416 | lr 0.000283798 | gnorm 2.342 | train_wall 324 | gb_free 6.1 | wall 68334
KL Stats: Epoch 194 Divergences: Uniform: 2.723028349654176 Unigram: 24.39675058855039
2022-01-29 02:39:08 | INFO | fairseq.trainer | begin training epoch 195
2022-01-29 02:39:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:44:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:45:00 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 11.023 | ppl 2081.03 | wps 7964.5 | wpb 2034.1 | bsz 4 | num_updates 12480 | best_loss 9.367
2022-01-29 02:45:00 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-01-29 02:45:00 | INFO | train | epoch 195 | loss 4.778 | ppl 27.44 | wps 5922.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12480 | lr 0.000283069 | gnorm 2.779 | train_wall 324 | gb_free 6.1 | wall 68687
KL Stats: Epoch 195 Divergences: Uniform: 2.720361185560637 Unigram: 24.535263295179995
2022-01-29 02:45:00 | INFO | fairseq.trainer | begin training epoch 196
2022-01-29 02:45:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:46:42 | INFO | train_inner | epoch 196:     20 / 64 loss=4.774, ppl=27.37, wps=5796.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12500, lr=0.000282843, gnorm=2.621, train_wall=505, gb_free=6.1, wall=68789
2022-01-29 02:50:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-01-29 02:50:52 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 10.887 | ppl 1893.21 | wps 7996 | wpb 2034.1 | bsz 4 | num_updates 12544 | best_loss 9.367
2022-01-29 02:50:52 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-01-29 02:50:52 | INFO | train | epoch 196 | loss 4.77 | ppl 27.28 | wps 5931.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12544 | lr 0.000282346 | gnorm 2.557 | train_wall 323 | gb_free 6.1 | wall 69039
KL Stats: Epoch 196 Divergences: Uniform: 2.710763329566451 Unigram: 24.73946316275023
2022-01-29 02:50:52 | INFO | fairseq.trainer | begin training epoch 197
2022-01-29 02:50:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 02:55:38 | INFO | train_inner | epoch 197:     56 / 64 loss=4.767, ppl=27.23, wps=6099, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12600, lr=0.000281718, gnorm=2.682, train_wall=506, gb_free=6.1, wall=69325
2022-01-29 02:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 02:56:45 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 11.079 | ppl 2163.57 | wps 7991.5 | wpb 2034.1 | bsz 4 | num_updates 12608 | best_loss 9.367
2022-01-29 02:56:45 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-01-29 02:56:45 | INFO | train | epoch 197 | loss 4.759 | ppl 27.07 | wps 5927.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12608 | lr 0.000281629 | gnorm 2.633 | train_wall 323 | gb_free 6.1 | wall 69392
KL Stats: Epoch 197 Divergences: Uniform: 2.718251972901149 Unigram: 24.94591036786834
2022-01-29 02:56:45 | INFO | fairseq.trainer | begin training epoch 198
2022-01-29 02:56:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 03:02:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 03:02:37 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 10.989 | ppl 2032.26 | wps 7995 | wpb 2034.1 | bsz 4 | num_updates 12672 | best_loss 9.367
2022-01-29 03:02:37 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-01-29 03:02:37 | INFO | train | epoch 198 | loss 4.748 | ppl 26.88 | wps 5926.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12672 | lr 0.000280917 | gnorm 2.771 | train_wall 323 | gb_free 6.1 | wall 69744
KL Stats: Epoch 198 Divergences: Uniform: 2.7219717767150784 Unigram: 25.177664482905712
2022-01-29 03:02:37 | INFO | fairseq.trainer | begin training epoch 199
2022-01-29 03:02:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 03:05:00 | INFO | train_inner | epoch 199:     28 / 64 loss=4.743, ppl=26.78, wps=5801.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12700, lr=0.000280607, gnorm=2.669, train_wall=505, gb_free=6.1, wall=69887
2022-01-29 03:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 03:08:29 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 10.988 | ppl 2030.52 | wps 7974.5 | wpb 2034.1 | bsz 4 | num_updates 12736 | best_loss 9.367
2022-01-29 03:08:29 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-01-29 03:08:29 | INFO | train | epoch 199 | loss 4.74 | ppl 26.73 | wps 5931.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12736 | lr 0.00028021 | gnorm 2.554 | train_wall 323 | gb_free 6.1 | wall 70096
KL Stats: Epoch 199 Divergences: Uniform: 2.7106503800080186 Unigram: 25.386164889143036
2022-01-29 03:08:29 | INFO | fairseq.trainer | begin training epoch 200
2022-01-29 03:08:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 03:13:55 | INFO | train_inner | epoch 200:     64 / 64 loss=4.748, ppl=26.87, wps=6094.4, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=12800, lr=0.000279508, gnorm=2.854, train_wall=505, gb_free=6.1, wall=70422
2022-01-29 03:13:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 03:14:22 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 11.353 | ppl 2615.48 | wps 8009.6 | wpb 2034.1 | bsz 4 | num_updates 12800 | best_loss 9.367
2022-01-29 03:14:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 12800 updates
2022-01-29 03:14:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint200.pt
2022-01-29 03:14:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint200.pt
2022-01-29 03:14:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.13_-0.03_0.9/checkpoint200.pt (epoch 200 @ 12800 updates, score 11.353) (writing took 3.1079904749058187 seconds)
2022-01-29 03:14:25 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-01-29 03:14:25 | INFO | train | epoch 200 | loss 4.738 | ppl 26.68 | wps 5868.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12800 | lr 0.000279508 | gnorm 2.999 | train_wall 324 | gb_free 6.1 | wall 70452
KL Stats: Epoch 200 Divergences: Uniform: 2.711376080553213 Unigram: 25.54915263840108
2022-01-29 03:14:25 | INFO | fairseq.trainer | begin training epoch 201
2022-01-29 03:14:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 03:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 03:20:18 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 11.095 | ppl 2187.58 | wps 7962.2 | wpb 2034.1 | bsz 4 | num_updates 12864 | best_loss 9.367
2022-01-29 03:20:18 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-01-29 03:20:18 | INFO | train | epoch 201 | loss 4.727 | ppl 26.49 | wps 5928.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12864 | lr 0.000278812 | gnorm 2.791 | train_wall 323 | gb_free 6.1 | wall 70804
KL Stats: Epoch 201 Divergences: Uniform: 2.715478218347826 Unigram: 25.736104949124908
2022-01-29 03:20:18 | INFO | fairseq.trainer | begin training epoch 202
2022-01-29 03:20:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 03:23:21 | INFO | train_inner | epoch 202:     36 / 64 loss=4.711, ppl=26.19, wps=5769.2, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=12900, lr=0.000278423, gnorm=2.758, train_wall=506, gb_free=6.1, wall=70988
2022-01-29 03:25:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 03:26:10 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 10.967 | ppl 2002.04 | wps 7970.8 | wpb 2034.1 | bsz 4 | num_updates 12928 | best_loss 9.367
2022-01-29 03:26:10 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-01-29 03:26:10 | INFO | train | epoch 202 | loss 4.714 | ppl 26.24 | wps 5926.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12928 | lr 0.000278121 | gnorm 2.641 | train_wall 323 | gb_free 6.1 | wall 71157
KL Stats: Epoch 202 Divergences: Uniform: 2.72277265001391 Unigram: 25.99310339645338
2022-01-29 03:26:10 | INFO | fairseq.trainer | begin training epoch 203
2022-01-29 03:26:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 03:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 03:32:02 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 11.252 | ppl 2438.56 | wps 8011.6 | wpb 2034.1 | bsz 4 | num_updates 12992 | best_loss 9.367
2022-01-29 03:32:02 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-01-29 03:32:02 | INFO | train | epoch 203 | loss 4.711 | ppl 26.19 | wps 5928.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12992 | lr 0.000277435 | gnorm 3.008 | train_wall 323 | gb_free 6.1 | wall 71509
KL Stats: Epoch 203 Divergences: Uniform: 2.714578173290715 Unigram: 26.169824946471653
2022-01-29 03:32:02 | INFO | fairseq.trainer | begin training epoch 204
2022-01-29 03:32:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-29 03:32:43 | INFO | train_inner | epoch 204:      8 / 64 loss=4.719, ppl=26.33, wps=5802.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=13000, lr=0.00027735, gnorm=2.876, train_wall=504, gb_free=6.1, wall=71550
2022-01-29 03:37:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-29 03:37:54 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 11.195 | ppl 2344.85 | wps 7987.2 | wpb 2034.1 | bsz 4 | num_updates 13056 | best_loss 9.367
2022-01-29 03:37:54 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-01-29 03:37:54 | INFO | train | epoch 204 | loss 4.699 | ppl 25.98 | wps 5930.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 13056 | lr 0.000276755 | gnorm 3.042 | train_wall 323 | gb_free 6.1 | wall 71861
KL Stats: Epoch 204 Divergences: Uniform: 2.707221342942686 Unigram: 26.367916818306387
2022-01-29 03:37:54 | INFO | fairseq.trainer | begin training epoch 205
2022-01-29 03:37:54 | INFO | fairseq_cli.train | Start iterating over samples
User defined signal 2
