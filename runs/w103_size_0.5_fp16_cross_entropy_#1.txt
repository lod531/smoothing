Sender: LSF System <lsfadmin@eu-g2-15>
Subject: Job 205864803: <w103_size_0.5_fp16_cross_entropy_#1> in cluster <euler> Exited

Job <w103_size_0.5_fp16_cross_entropy_#1> was submitted from host <eu-login-43> by user <andriusb> in cluster <euler> at Sun Feb 20 17:06:58 2022
Job was executed on host(s) <eu-g2-15>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Sun Feb 20 17:07:23 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Feb 20 17:07:23 2022
Terminated at Sun Feb 20 17:12:23 2022
Results reported at Sun Feb 20 17:12:23 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 6657561 --fp16 --no-epoch-checkpoints --no-last-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   302.00 sec.
    Max Memory :                                 11613 MB
    Average Memory :                             5366.75 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               8387.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   299 sec.
    Turnaround time :                            325 sec.

The output (if any) follows:

2022-02-20 17:07:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6657561, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6657561, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-20 17:07:29 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-20 17:07:35 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-20 17:07:35 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-20 17:07:35 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-20 17:07:35 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-20 17:07:35 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-20 17:07:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-20 17:07:35 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-20 17:07:38 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-20 17:07:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-20 17:07:38 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-02-20 17:07:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-20 17:07:38 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-20 17:07:38 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-02-20 17:07:38 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-20 17:07:44 | INFO | fairseq.trainer | Loaded checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 2 @ 781 updates)
2022-02-20 17:07:44 | INFO | fairseq.trainer | loading train data for epoch 2
2022-02-20 17:07:46 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-20 17:07:47 | INFO | fairseq.trainer | begin training epoch 2
2022-02-20 17:07:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 17:09:32 | INFO | train_inner | epoch 002:     19 / 788 loss=9.136, ppl=562.81, wps=11828.8, ups=0.18, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.736, loss_scale=32, train_wall=100, gb_free=3.7, wall=113
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/cross_entropy.py", line 35, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 328, in extract_features_scriptable
    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g2-04>
Subject: Job 205864899: <w103_size_0.5_fp16_cross_entropy_#1> in cluster <euler> Exited

Job <w103_size_0.5_fp16_cross_entropy_#1> was submitted from host <eu-login-43> by user <andriusb> in cluster <euler> at Sun Feb 20 17:13:39 2022
Job was executed on host(s) <eu-g2-04>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Sun Feb 20 17:19:57 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Feb 20 17:19:57 2022
Terminated at Sun Feb 20 18:31:32 2022
Results reported at Sun Feb 20 18:31:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 6657561 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

Exited with exit code 134.

Resource usage summary:

    CPU time :                                   4381.94 sec.
    Max Memory :                                 13410 MB
    Average Memory :                             5263.90 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               6590.00 MB
    Max Swap :                                   7 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   4295 sec.
    Turnaround time :                            4673 sec.

The output (if any) follows:

2022-02-20 17:20:12 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6657561, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6657561, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-20 17:20:12 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-20 17:20:18 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-20 17:20:18 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-20 17:20:18 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-20 17:20:18 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-20 17:20:18 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-20 17:20:18 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-20 17:20:18 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-20 17:20:29 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-20 17:20:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-20 17:20:29 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-02-20 17:20:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-20 17:20:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-20 17:20:29 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-02-20 17:20:29 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-20 17:20:36 | INFO | fairseq.trainer | Loaded checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 2 @ 781 updates)
2022-02-20 17:20:36 | INFO | fairseq.trainer | loading train data for epoch 2
2022-02-20 17:20:38 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-20 17:20:38 | INFO | fairseq.trainer | begin training epoch 2
2022-02-20 17:20:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 17:22:21 | INFO | train_inner | epoch 002:     19 / 788 loss=9.136, ppl=562.81, wps=12183.5, ups=0.19, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.736, loss_scale=32, train_wall=97, gb_free=3.7, wall=112
2022-02-20 17:31:19 | INFO | train_inner | epoch 002:    119 / 788 loss=9.02, ppl=519.03, wps=12169.4, ups=0.19, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.833, loss_scale=32, train_wall=513, gb_free=3.7, wall=651
2022-02-20 17:32:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-20 17:33:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 17:40:29 | INFO | train_inner | epoch 002:    221 / 788 loss=8.808, ppl=448.2, wps=11916.4, ups=0.18, wpb=65534.7, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.932, loss_scale=16, train_wall=524, gb_free=3.7, wall=1201
2022-02-20 17:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 17:49:34 | INFO | train_inner | epoch 002:    322 / 788 loss=8.633, ppl=397.03, wps=12025.5, ups=0.18, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.918, loss_scale=16, train_wall=520, gb_free=3.7, wall=1746
2022-02-20 17:58:33 | INFO | train_inner | epoch 002:    422 / 788 loss=8.467, ppl=353.8, wps=12173.7, ups=0.19, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.917, loss_scale=16, train_wall=514, gb_free=3.7, wall=2284
2022-02-20 18:07:30 | INFO | train_inner | epoch 002:    522 / 788 loss=8.317, ppl=319, wps=12201.1, ups=0.19, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.967, loss_scale=32, train_wall=512, gb_free=3.7, wall=2821
2022-02-20 18:14:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-20 18:16:34 | INFO | train_inner | epoch 002:    623 / 788 loss=8.193, ppl=292.65, wps=12051.4, ups=0.18, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.962, loss_scale=32, train_wall=519, gb_free=3.7, wall=3365
2022-02-20 18:25:32 | INFO | train_inner | epoch 002:    723 / 788 loss=8.054, ppl=265.81, wps=12176.3, ups=0.19, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.977, loss_scale=64, train_wall=513, gb_free=3.7, wall=3903
2022-02-20 18:27:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-20 18:31:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 18:31:27 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.769 | ppl 218.19 | wps 27864.2 | wpb 510.9 | bsz 1 | num_updates 1564 | best_loss 7.769
2022-02-20 18:31:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1564 updates
2022-02-20 18:31:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 13701312 vs 13701204
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x2b52df6856a7 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x2b524661e500 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x2b524661a6d3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x2b524661f609 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x2b5246620141 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x2b5246620935 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x6543f3 (0x2b5243af03f3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x2c2c60 (0x2b524375ec60 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x2c3dce (0x2b524375fdce in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
<omitting python frames>
frame #46: __libc_start_main + 0xf5 (0x2b52316d7555 in /lib64/libc.so.6)
frame #47: /cluster/home/andriusb/fq/env/bin/python() [0x40071e]

/cluster/shadow/.lsbatch/1645373619.205864899: line 8: 22747 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 6657561 --fp16 --no-epoch-checkpoints --max-update 50000
Sender: LSF System <lsfadmin@eu-g2-16>
Subject: Job 205868294: <w103_size_0.5_fp16_cross_entropy_#1> in cluster <euler> Exited

Job <w103_size_0.5_fp16_cross_entropy_#1> was submitted from host <eu-login-43> by user <andriusb> in cluster <euler> at Sun Feb 20 19:49:34 2022
Job was executed on host(s) <eu-g2-16>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Sun Feb 20 19:50:06 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Feb 20 19:50:06 2022
Terminated at Sun Feb 20 21:00:37 2022
Results reported at Sun Feb 20 21:00:37 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 6657561 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

Exited with exit code 134.

Resource usage summary:

    CPU time :                                   4323.54 sec.
    Max Memory :                                 11216 MB
    Average Memory :                             3626.59 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               8784.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   4231 sec.
    Turnaround time :                            4263 sec.

The output (if any) follows:

2022-02-20 19:50:17 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6657561, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6657561, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-20 19:50:17 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-20 19:50:23 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-20 19:50:23 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-20 19:50:23 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-20 19:50:23 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-20 19:50:23 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-20 19:50:23 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-20 19:50:23 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-20 19:50:27 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-20 19:50:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-20 19:50:27 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-02-20 19:50:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-20 19:50:27 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-20 19:50:27 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-02-20 19:50:27 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-20 19:50:27 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-20 19:50:27 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-20 19:50:29 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-20 19:50:29 | INFO | fairseq.trainer | begin training epoch 1
2022-02-20 19:50:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 19:50:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-20 19:50:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-20 19:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 19:51:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 19:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-20 20:00:06 | INFO | train_inner | epoch 001:    105 / 788 loss=17.524, ppl=188436, wps=12310.8, ups=0.19, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.75, loss_scale=4, train_wall=551, gb_free=3.7, wall=579
2022-02-20 20:08:55 | INFO | train_inner | epoch 001:    205 / 788 loss=14.948, ppl=31608, wps=12393.3, ups=0.19, wpb=65534.7, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.74, loss_scale=8, train_wall=504, gb_free=3.7, wall=1108
2022-02-20 20:17:45 | INFO | train_inner | epoch 001:    305 / 788 loss=12.684, ppl=6581, wps=12368.4, ups=0.19, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.169, loss_scale=16, train_wall=505, gb_free=3.7, wall=1637
2022-02-20 20:26:35 | INFO | train_inner | epoch 001:    405 / 788 loss=10.944, ppl=1970.1, wps=12366.7, ups=0.19, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.686, loss_scale=16, train_wall=505, gb_free=3.7, wall=2167
2022-02-20 20:35:24 | INFO | train_inner | epoch 001:    505 / 788 loss=10.176, ppl=1156.7, wps=12381.9, ups=0.19, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.517, loss_scale=32, train_wall=505, gb_free=3.7, wall=2697
2022-02-20 20:39:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-20 20:44:19 | INFO | train_inner | epoch 001:    606 / 788 loss=9.802, ppl=892.44, wps=12245.1, ups=0.19, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.568, loss_scale=32, train_wall=510, gb_free=3.7, wall=3232
2022-02-20 20:48:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 20:53:14 | INFO | train_inner | epoch 001:    707 / 788 loss=9.515, ppl=731.6, wps=12255.5, ups=0.19, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.695, loss_scale=16, train_wall=510, gb_free=3.7, wall=3767
2022-02-20 21:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 21:00:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.093 | ppl 546.21 | wps 29103.6 | wpb 510.9 | bsz 1 | num_updates 781
2022-02-20 21:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 781 updates
2022-02-20 21:00:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 2827694528 vs 2827694416
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x2b4f42b106a7 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x2b4ea9aa9500 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x2b4ea9aa56d3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x2b4ea9aaa609 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x2b4ea9aab141 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x2b4ea9aab935 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x6543f3 (0x2b4ea6f7b3f3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x2c2c60 (0x2b4ea6be9c60 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x2c3dce (0x2b4ea6beadce in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
<omitting python frames>
frame #46: __libc_start_main + 0xf5 (0x2b4e94b62555 in /lib64/libc.so.6)
frame #47: /cluster/home/andriusb/fq/env/bin/python() [0x40071e]

/cluster/shadow/.lsbatch/1645382974.205868294: line 8: 192314 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 6657561 --fp16 --no-epoch-checkpoints --max-update 50000
Sender: LSF System <lsfadmin@eu-g3-048>
Subject: Job 205907919: <w103_size_0.5_fp16_cross_entropy_#1> in cluster <euler> Exited

Job <w103_size_0.5_fp16_cross_entropy_#1> was submitted from host <eu-login-29> by user <andriusb> in cluster <euler> at Mon Feb 21 06:45:03 2022
Job was executed on host(s) <eu-g3-048>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Mon Feb 21 06:45:28 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Mon Feb 21 06:45:28 2022
Terminated at Wed Feb 23 06:45:42 2022
Results reported at Wed Feb 23 06:45:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 6657561 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   172669.00 sec.
    Max Memory :                                 11543 MB
    Average Memory :                             2349.14 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               8457.00 MB
    Max Swap :                                   1910 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   172814 sec.
    Turnaround time :                            172840 sec.

The output (if any) follows:

2022-02-21 06:45:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6657561, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6657561, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-21 06:45:45 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-21 06:45:50 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-21 06:45:50 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-21 06:45:50 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-21 06:45:50 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-21 06:45:50 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-21 06:45:50 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-21 06:45:50 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-21 06:46:00 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-21 06:46:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-21 06:46:00 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-02-21 06:46:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-21 06:46:00 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-21 06:46:00 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-02-21 06:46:00 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-21 06:46:00 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-21 06:46:00 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-21 06:46:02 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-21 06:46:02 | INFO | fairseq.trainer | begin training epoch 1
2022-02-21 06:46:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 06:46:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-21 06:46:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 06:46:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 06:46:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 06:49:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-21 06:55:41 | INFO | train_inner | epoch 001:    105 / 788 loss=17.524, ppl=188436, wps=12335.6, ups=0.19, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.75, loss_scale=4, train_wall=555, gb_free=3.7, wall=581
2022-02-21 07:04:28 | INFO | train_inner | epoch 001:    205 / 788 loss=14.948, ppl=31608, wps=12440.3, ups=0.19, wpb=65534.7, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.74, loss_scale=8, train_wall=504, gb_free=3.7, wall=1108
2022-02-21 07:13:14 | INFO | train_inner | epoch 001:    305 / 788 loss=12.684, ppl=6581, wps=12459.9, ups=0.19, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.169, loss_scale=16, train_wall=503, gb_free=3.7, wall=1634
2022-02-21 07:22:00 | INFO | train_inner | epoch 001:    405 / 788 loss=10.944, ppl=1970.09, wps=12464.2, ups=0.19, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.686, loss_scale=16, train_wall=503, gb_free=3.7, wall=2159
2022-02-21 07:30:46 | INFO | train_inner | epoch 001:    505 / 788 loss=10.176, ppl=1156.66, wps=12462.3, ups=0.19, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.516, loss_scale=32, train_wall=503, gb_free=3.7, wall=2685
2022-02-21 07:34:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 07:39:36 | INFO | train_inner | epoch 001:    606 / 788 loss=9.802, ppl=892.39, wps=12347.7, ups=0.19, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.569, loss_scale=32, train_wall=508, gb_free=3.7, wall=3216
2022-02-21 07:43:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 07:48:27 | INFO | train_inner | epoch 001:    707 / 788 loss=9.515, ppl=731.58, wps=12351.5, ups=0.19, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.696, loss_scale=16, train_wall=508, gb_free=3.7, wall=3747
2022-02-21 07:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 07:55:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.093 | ppl 546.08 | wps 30396.5 | wpb 510.9 | bsz 1 | num_updates 781
2022-02-21 07:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 781 updates
2022-02-21 07:55:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 07:55:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 07:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 1 @ 781 updates, score 9.093) (writing took 13.990465367212892 seconds)
2022-02-21 07:55:51 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-21 07:55:51 | INFO | train | epoch 001 | loss 11.924 | ppl 3884.65 | wps 12351.7 | ups 0.19 | wpb 65497.2 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 1.245 | loss_scale 32 | train_wall 3990 | gb_free 3.7 | wall 4191
2022-02-21 07:55:51 | INFO | fairseq.trainer | begin training epoch 2
2022-02-21 07:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 07:57:31 | INFO | train_inner | epoch 002:     19 / 788 loss=9.255, ppl=610.78, wps=11983.4, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.733, loss_scale=32, train_wall=500, gb_free=3.7, wall=4291
2022-02-21 08:06:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 08:06:22 | INFO | train_inner | epoch 002:    120 / 788 loss=9.018, ppl=518.32, wps=12349.2, ups=0.19, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.807, loss_scale=32, train_wall=508, gb_free=3.7, wall=4822
2022-02-21 08:15:07 | INFO | train_inner | epoch 002:    220 / 788 loss=8.806, ppl=447.46, wps=12476.6, ups=0.19, wpb=65534.7, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.939, loss_scale=32, train_wall=503, gb_free=3.7, wall=5347
2022-02-21 08:17:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 08:23:58 | INFO | train_inner | epoch 002:    321 / 788 loss=8.633, ppl=396.87, wps=12351, ups=0.19, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.924, loss_scale=32, train_wall=508, gb_free=3.7, wall=5878
2022-02-21 08:24:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 08:32:48 | INFO | train_inner | epoch 002:    422 / 788 loss=8.466, ppl=353.69, wps=12365.7, ups=0.19, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.924, loss_scale=16, train_wall=507, gb_free=3.7, wall=6408
2022-02-21 08:41:33 | INFO | train_inner | epoch 002:    522 / 788 loss=8.317, ppl=318.8, wps=12482.6, ups=0.19, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.959, loss_scale=32, train_wall=502, gb_free=3.7, wall=6933
2022-02-21 08:46:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 08:50:23 | INFO | train_inner | epoch 002:    623 / 788 loss=8.195, ppl=293.03, wps=12358.5, ups=0.19, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.982, loss_scale=32, train_wall=508, gb_free=3.7, wall=7463
2022-02-21 08:59:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 08:59:13 | INFO | train_inner | epoch 002:    724 / 788 loss=8.053, ppl=265.52, wps=12365.8, ups=0.19, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.957, loss_scale=32, train_wall=507, gb_free=3.7, wall=7993
2022-02-21 09:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 09:04:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.767 | ppl 217.8 | wps 30258.8 | wpb 510.9 | bsz 1 | num_updates 1564 | best_loss 7.767
2022-02-21 09:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1564 updates
2022-02-21 09:04:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 09:05:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 09:05:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 2 @ 1564 updates, score 7.767) (writing took 14.210179136134684 seconds)
2022-02-21 09:05:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-21 09:05:08 | INFO | train | epoch 002 | loss 8.469 | ppl 354.27 | wps 12336.8 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 1564 | lr 0.000195561 | gnorm 0.925 | loss_scale 32 | train_wall 3958 | gb_free 3.7 | wall 8348
2022-02-21 09:05:08 | INFO | fairseq.trainer | begin training epoch 3
2022-02-21 09:05:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 09:08:18 | INFO | train_inner | epoch 003:     36 / 788 loss=7.904, ppl=239.49, wps=11980.9, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.961, loss_scale=32, train_wall=500, gb_free=3.7, wall=8537
2022-02-21 09:11:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 09:17:08 | INFO | train_inner | epoch 003:    137 / 788 loss=7.768, ppl=218.03, wps=12366.8, ups=0.19, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.972, loss_scale=32, train_wall=507, gb_free=3.7, wall=9067
2022-02-21 09:22:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 09:25:57 | INFO | train_inner | epoch 003:    238 / 788 loss=7.659, ppl=202.14, wps=12375.7, ups=0.19, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.973, loss_scale=32, train_wall=507, gb_free=3.7, wall=9597
2022-02-21 09:34:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 09:34:47 | INFO | train_inner | epoch 003:    339 / 788 loss=7.573, ppl=190.46, wps=12370.3, ups=0.19, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.931, loss_scale=32, train_wall=507, gb_free=3.7, wall=10127
2022-02-21 09:43:33 | INFO | train_inner | epoch 003:    439 / 788 loss=7.473, ppl=177.69, wps=12447.7, ups=0.19, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.922, loss_scale=32, train_wall=504, gb_free=3.7, wall=10653
2022-02-21 09:46:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 09:52:23 | INFO | train_inner | epoch 003:    540 / 788 loss=7.378, ppl=166.31, wps=12366, ups=0.19, wpb=65534.7, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.893, loss_scale=32, train_wall=507, gb_free=3.7, wall=11183
2022-02-21 09:58:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 10:01:13 | INFO | train_inner | epoch 003:    641 / 788 loss=7.301, ppl=157.66, wps=12378.4, ups=0.19, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.879, loss_scale=32, train_wall=507, gb_free=3.7, wall=11712
2022-02-21 10:09:57 | INFO | train_inner | epoch 003:    741 / 788 loss=7.232, ppl=150.32, wps=12494.9, ups=0.19, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.875, loss_scale=64, train_wall=502, gb_free=3.7, wall=12237
2022-02-21 10:10:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 10:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 10:14:09 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.02 | ppl 129.81 | wps 30070.5 | wpb 510.9 | bsz 1 | num_updates 2346 | best_loss 7.02
2022-02-21 10:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2346 updates
2022-02-21 10:14:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 10:14:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 10:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 3 @ 2346 updates, score 7.02) (writing took 14.087495513260365 seconds)
2022-02-21 10:14:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-21 10:14:23 | INFO | train | epoch 003 | loss 7.481 | ppl 178.64 | wps 12328.3 | ups 0.19 | wpb 65497.2 | bsz 127.9 | num_updates 2346 | lr 0.000293291 | gnorm 0.92 | loss_scale 32 | train_wall 3956 | gb_free 3.7 | wall 12503
2022-02-21 10:14:23 | INFO | fairseq.trainer | begin training epoch 4
2022-02-21 10:14:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 10:19:06 | INFO | train_inner | epoch 004:     54 / 788 loss=7.106, ppl=137.77, wps=11877.7, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.873, loss_scale=32, train_wall=505, gb_free=3.7, wall=12786
2022-02-21 10:22:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 10:27:56 | INFO | train_inner | epoch 004:    155 / 788 loss=6.991, ppl=127.22, wps=12365.3, ups=0.19, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.861, loss_scale=32, train_wall=507, gb_free=3.7, wall=13316
2022-02-21 10:34:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 10:36:46 | INFO | train_inner | epoch 004:    256 / 788 loss=6.946, ppl=123.29, wps=12373.4, ups=0.19, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.858, loss_scale=32, train_wall=507, gb_free=3.7, wall=13846
2022-02-21 10:43:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 10:45:36 | INFO | train_inner | epoch 004:    357 / 788 loss=6.881, ppl=117.83, wps=12372.9, ups=0.19, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.849, loss_scale=16, train_wall=507, gb_free=3.7, wall=14375
2022-02-21 10:54:20 | INFO | train_inner | epoch 004:    457 / 788 loss=6.824, ppl=113.32, wps=12499.4, ups=0.19, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.809, loss_scale=16, train_wall=502, gb_free=3.7, wall=14900
2022-02-21 11:00:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 11:03:10 | INFO | train_inner | epoch 004:    558 / 788 loss=6.777, ppl=109.68, wps=12370.5, ups=0.19, wpb=65534.7, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.831, loss_scale=16, train_wall=507, gb_free=3.7, wall=15430
2022-02-21 11:11:54 | INFO | train_inner | epoch 004:    658 / 788 loss=6.714, ppl=105, wps=12503.9, ups=0.19, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.813, loss_scale=32, train_wall=502, gb_free=3.7, wall=15954
2022-02-21 11:14:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 11:20:44 | INFO | train_inner | epoch 004:    759 / 788 loss=6.672, ppl=102, wps=12375.2, ups=0.19, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.801, loss_scale=16, train_wall=507, gb_free=3.7, wall=16483
2022-02-21 11:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 11:23:20 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.539 | ppl 93.01 | wps 30673.2 | wpb 510.9 | bsz 1 | num_updates 3129 | best_loss 6.539
2022-02-21 11:23:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3129 updates
2022-02-21 11:23:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 11:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 11:23:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 4 @ 3129 updates, score 6.539) (writing took 13.998604104854167 seconds)
2022-02-21 11:23:34 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-21 11:23:34 | INFO | train | epoch 004 | loss 6.838 | ppl 114.41 | wps 12353 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 3129 | lr 0.000391147 | gnorm 0.835 | loss_scale 16 | train_wall 3953 | gb_free 3.7 | wall 16654
2022-02-21 11:23:34 | INFO | fairseq.trainer | begin training epoch 5
2022-02-21 11:23:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 11:29:47 | INFO | train_inner | epoch 005:     71 / 788 loss=6.544, ppl=93.31, wps=12001.4, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.801, loss_scale=32, train_wall=500, gb_free=3.7, wall=17027
2022-02-21 11:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 11:38:37 | INFO | train_inner | epoch 005:    172 / 788 loss=6.476, ppl=89.02, wps=12370, ups=0.19, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.792, loss_scale=16, train_wall=507, gb_free=3.7, wall=17557
2022-02-21 11:46:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 11:47:26 | INFO | train_inner | epoch 005:    273 / 788 loss=6.444, ppl=87.09, wps=12378.2, ups=0.19, wpb=65534.7, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.811, loss_scale=16, train_wall=507, gb_free=3.7, wall=18086
2022-02-21 11:56:11 | INFO | train_inner | epoch 005:    373 / 788 loss=6.413, ppl=85.22, wps=12501, ups=0.19, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.784, loss_scale=16, train_wall=502, gb_free=3.7, wall=18610
2022-02-21 12:04:55 | INFO | train_inner | epoch 005:    473 / 788 loss=6.379, ppl=83.22, wps=12499.6, ups=0.19, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.75, loss_scale=32, train_wall=502, gb_free=3.7, wall=19135
2022-02-21 12:09:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-21 12:13:44 | INFO | train_inner | epoch 005:    574 / 788 loss=6.359, ppl=82.07, wps=12387.9, ups=0.19, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.773, loss_scale=32, train_wall=507, gb_free=3.7, wall=19664
2022-02-21 12:16:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 12:22:33 | INFO | train_inner | epoch 005:    675 / 788 loss=6.332, ppl=80.57, wps=12377.2, ups=0.19, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.75, loss_scale=16, train_wall=507, gb_free=3.7, wall=20193
2022-02-21 12:31:17 | INFO | train_inner | epoch 005:    775 / 788 loss=6.29, ppl=78.27, wps=12506.2, ups=0.19, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.732, loss_scale=32, train_wall=502, gb_free=3.7, wall=20717
2022-02-21 12:31:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 12:32:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 12:32:30 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.234 | ppl 75.27 | wps 30134.6 | wpb 510.9 | bsz 1 | num_updates 3912 | best_loss 6.234
2022-02-21 12:32:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3912 updates
2022-02-21 12:32:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 12:32:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 12:32:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 5 @ 3912 updates, score 6.234) (writing took 14.514131574891508 seconds)
2022-02-21 12:32:45 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-21 12:32:45 | INFO | train | epoch 005 | loss 6.395 | ppl 84.13 | wps 12356 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 3912 | lr 0.000489002 | gnorm 0.775 | loss_scale 16 | train_wall 3952 | gb_free 3.7 | wall 20805
2022-02-21 12:32:45 | INFO | fairseq.trainer | begin training epoch 6
2022-02-21 12:32:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 12:40:26 | INFO | train_inner | epoch 006:     88 / 788 loss=6.124, ppl=69.77, wps=11884.8, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.747, loss_scale=16, train_wall=504, gb_free=3.7, wall=21266
2022-02-21 12:46:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 12:49:17 | INFO | train_inner | epoch 006:    189 / 788 loss=6.121, ppl=69.62, wps=12353.8, ups=0.19, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.737, loss_scale=16, train_wall=507, gb_free=3.7, wall=21797
2022-02-21 12:58:05 | INFO | train_inner | epoch 006:    289 / 788 loss=6.094, ppl=68.32, wps=12405.9, ups=0.19, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.7, loss_scale=32, train_wall=505, gb_free=3.7, wall=22325
2022-02-21 13:02:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 13:06:56 | INFO | train_inner | epoch 006:    390 / 788 loss=6.085, ppl=67.9, wps=12336.9, ups=0.19, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.677, loss_scale=16, train_wall=508, gb_free=3.7, wall=22856
2022-02-21 13:15:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 13:15:51 | INFO | train_inner | epoch 006:    491 / 788 loss=6.057, ppl=66.59, wps=12265.3, ups=0.19, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.714, loss_scale=16, train_wall=510, gb_free=3.7, wall=23390
2022-02-21 13:24:37 | INFO | train_inner | epoch 006:    591 / 788 loss=6.038, ppl=65.71, wps=12457.9, ups=0.19, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.648, loss_scale=16, train_wall=503, gb_free=3.7, wall=23916
2022-02-21 13:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 13:33:31 | INFO | train_inner | epoch 006:    692 / 788 loss=6.016, ppl=64.7, wps=12266.5, ups=0.19, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.685, loss_scale=16, train_wall=510, gb_free=3.7, wall=24451
2022-02-21 13:41:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 13:41:59 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.983 | ppl 63.26 | wps 30648.9 | wpb 510.9 | bsz 1 | num_updates 4696 | best_loss 5.983
2022-02-21 13:41:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4696 updates
2022-02-21 13:41:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 13:42:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 13:42:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 6 @ 4696 updates, score 5.983) (writing took 14.59629337489605 seconds)
2022-02-21 13:42:13 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-21 13:42:13 | INFO | train | epoch 006 | loss 6.064 | ppl 66.89 | wps 12319.3 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 4696 | lr 0.000461462 | gnorm 0.688 | loss_scale 32 | train_wall 3964 | gb_free 3.7 | wall 24973
2022-02-21 13:42:13 | INFO | fairseq.trainer | begin training epoch 7
2022-02-21 13:42:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 13:42:34 | INFO | train_inner | epoch 007:      4 / 788 loss=5.992, ppl=63.63, wps=12005.8, ups=0.18, wpb=65232.6, bsz=127.4, num_updates=4700, lr=0.000461266, gnorm=0.635, loss_scale=32, train_wall=499, gb_free=3.7, wall=24994
2022-02-21 13:42:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 13:51:34 | INFO | train_inner | epoch 007:    105 / 788 loss=5.818, ppl=56.41, wps=12144.7, ups=0.19, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.681, loss_scale=16, train_wall=512, gb_free=3.7, wall=25534
2022-02-21 13:56:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 14:00:53 | INFO | train_inner | epoch 007:    206 / 788 loss=5.823, ppl=56.59, wps=11720.4, ups=0.18, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.64, loss_scale=16, train_wall=517, gb_free=3.7, wall=26093
2022-02-21 14:09:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 14:09:55 | INFO | train_inner | epoch 007:    307 / 788 loss=5.817, ppl=56.37, wps=12098.6, ups=0.18, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.649, loss_scale=16, train_wall=515, gb_free=3.7, wall=26635
2022-02-21 14:18:48 | INFO | train_inner | epoch 007:    407 / 788 loss=5.826, ppl=56.73, wps=12297.2, ups=0.19, wpb=65534.7, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.639, loss_scale=16, train_wall=509, gb_free=3.7, wall=27167
2022-02-21 14:24:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 14:27:46 | INFO | train_inner | epoch 007:    508 / 788 loss=5.805, ppl=55.91, wps=12168.8, ups=0.19, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.659, loss_scale=16, train_wall=514, gb_free=3.7, wall=27706
2022-02-21 14:36:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 14:36:45 | INFO | train_inner | epoch 007:    609 / 788 loss=5.803, ppl=55.81, wps=12166.3, ups=0.19, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.613, loss_scale=16, train_wall=514, gb_free=3.7, wall=28245
2022-02-21 14:45:38 | INFO | train_inner | epoch 007:    709 / 788 loss=5.791, ppl=55.37, wps=12287.6, ups=0.19, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.6, loss_scale=16, train_wall=508, gb_free=3.7, wall=28778
2022-02-21 14:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 14:52:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 14:52:47 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.84 | ppl 57.28 | wps 28337.5 | wpb 510.9 | bsz 1 | num_updates 5478 | best_loss 5.84
2022-02-21 14:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5478 updates
2022-02-21 14:52:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 14:52:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 14:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 7 @ 5478 updates, score 5.84) (writing took 17.14888234063983 seconds)
2022-02-21 14:53:05 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-21 14:53:05 | INFO | train | epoch 007 | loss 5.81 | ppl 56.08 | wps 12047.5 | ups 0.18 | wpb 65497.2 | bsz 127.9 | num_updates 5478 | lr 0.000427257 | gnorm 0.64 | loss_scale 16 | train_wall 4011 | gb_free 3.7 | wall 29224
2022-02-21 14:53:05 | INFO | fairseq.trainer | begin training epoch 8
2022-02-21 14:53:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 14:55:02 | INFO | train_inner | epoch 008:     22 / 788 loss=5.741, ppl=53.5, wps=11564, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.613, loss_scale=16, train_wall=513, gb_free=3.7, wall=29342
2022-02-21 15:03:56 | INFO | train_inner | epoch 008:    122 / 788 loss=5.614, ppl=48.96, wps=12272.5, ups=0.19, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.611, loss_scale=32, train_wall=510, gb_free=3.7, wall=29876
2022-02-21 15:05:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 15:12:56 | INFO | train_inner | epoch 008:    223 / 788 loss=5.628, ppl=49.47, wps=12157.1, ups=0.19, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.606, loss_scale=16, train_wall=515, gb_free=3.7, wall=30415
2022-02-21 15:21:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 15:21:54 | INFO | train_inner | epoch 008:    324 / 788 loss=5.637, ppl=49.78, wps=12169.2, ups=0.19, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.614, loss_scale=16, train_wall=514, gb_free=3.7, wall=30954
2022-02-21 15:30:47 | INFO | train_inner | epoch 008:    424 / 788 loss=5.635, ppl=49.68, wps=12291.5, ups=0.19, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.633, loss_scale=16, train_wall=509, gb_free=3.7, wall=31487
2022-02-21 15:35:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 15:39:45 | INFO | train_inner | epoch 008:    525 / 788 loss=5.624, ppl=49.31, wps=12180.9, ups=0.19, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.612, loss_scale=16, train_wall=514, gb_free=3.7, wall=32025
2022-02-21 15:47:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 15:48:44 | INFO | train_inner | epoch 008:    626 / 788 loss=5.635, ppl=49.68, wps=12168.4, ups=0.19, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.62, loss_scale=16, train_wall=514, gb_free=3.7, wall=32564
2022-02-21 15:57:37 | INFO | train_inner | epoch 008:    726 / 788 loss=5.635, ppl=49.71, wps=12296.8, ups=0.19, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.583, loss_scale=16, train_wall=509, gb_free=3.7, wall=33097
2022-02-21 15:59:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 16:03:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 16:03:12 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.763 | ppl 54.31 | wps 28596.1 | wpb 510.9 | bsz 1 | num_updates 6261 | best_loss 5.763
2022-02-21 16:03:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6261 updates
2022-02-21 16:03:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 16:03:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 16:03:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 8 @ 6261 updates, score 5.763) (writing took 20.495530693791807 seconds)
2022-02-21 16:03:33 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-21 16:03:33 | INFO | train | epoch 008 | loss 5.628 | ppl 49.46 | wps 12129 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 6261 | lr 0.000399648 | gnorm 0.609 | loss_scale 16 | train_wall 4011 | gb_free 3.7 | wall 33453
2022-02-21 16:03:33 | INFO | fairseq.trainer | begin training epoch 9
2022-02-21 16:03:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 16:07:01 | INFO | train_inner | epoch 009:     39 / 788 loss=5.555, ppl=47, wps=11564.3, ups=0.18, wpb=65232.6, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.59, loss_scale=16, train_wall=512, gb_free=3.7, wall=33661
2022-02-21 16:11:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 16:16:00 | INFO | train_inner | epoch 009:    140 / 788 loss=5.464, ppl=44.13, wps=12156.7, ups=0.19, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.607, loss_scale=16, train_wall=515, gb_free=3.7, wall=34200
2022-02-21 16:23:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 16:25:01 | INFO | train_inner | epoch 009:    241 / 788 loss=5.474, ppl=44.44, wps=12118.3, ups=0.18, wpb=65534.7, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.607, loss_scale=16, train_wall=515, gb_free=3.7, wall=34741
2022-02-21 16:33:58 | INFO | train_inner | epoch 009:    341 / 788 loss=5.494, ppl=45.08, wps=12206.1, ups=0.19, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.594, loss_scale=16, train_wall=511, gb_free=3.7, wall=35277
2022-02-21 16:37:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 16:42:58 | INFO | train_inner | epoch 009:    442 / 788 loss=5.488, ppl=44.87, wps=12136.4, ups=0.19, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.583, loss_scale=16, train_wall=515, gb_free=3.7, wall=35817
2022-02-21 16:49:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 16:51:57 | INFO | train_inner | epoch 009:    543 / 788 loss=5.508, ppl=45.5, wps=12149.6, ups=0.19, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.596, loss_scale=16, train_wall=515, gb_free=3.7, wall=36357
2022-02-21 16:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 17:00:57 | INFO | train_inner | epoch 009:    644 / 788 loss=5.512, ppl=45.63, wps=12145.7, ups=0.19, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.593, loss_scale=8, train_wall=515, gb_free=3.7, wall=36896
2022-02-21 17:09:52 | INFO | train_inner | epoch 009:    744 / 788 loss=5.506, ppl=45.45, wps=12254.8, ups=0.19, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.58, loss_scale=16, train_wall=510, gb_free=3.7, wall=37431
2022-02-21 17:13:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 17:13:52 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.672 | ppl 50.97 | wps 28508.7 | wpb 510.9 | bsz 1 | num_updates 7044 | best_loss 5.672
2022-02-21 17:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7044 updates
2022-02-21 17:13:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 17:14:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 17:14:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 9 @ 7044 updates, score 5.672) (writing took 15.594872394576669 seconds)
2022-02-21 17:14:08 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-21 17:14:08 | INFO | train | epoch 009 | loss 5.49 | ppl 44.95 | wps 12109.7 | ups 0.18 | wpb 65497.3 | bsz 127.9 | num_updates 7044 | lr 0.000376782 | gnorm 0.597 | loss_scale 16 | train_wall 4016 | gb_free 3.7 | wall 37688
2022-02-21 17:14:08 | INFO | fairseq.trainer | begin training epoch 10
2022-02-21 17:14:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 17:18:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 17:19:15 | INFO | train_inner | epoch 010:     57 / 788 loss=5.399, ppl=42.2, wps=11571.5, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.613, loss_scale=16, train_wall=514, gb_free=3.7, wall=37995
2022-02-21 17:28:12 | INFO | train_inner | epoch 010:    157 / 788 loss=5.345, ppl=40.64, wps=12210.9, ups=0.19, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.593, loss_scale=16, train_wall=511, gb_free=3.7, wall=38532
2022-02-21 17:32:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 17:33:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 17:37:17 | INFO | train_inner | epoch 010:    259 / 788 loss=5.359, ppl=41.05, wps=12033, ups=0.18, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.597, loss_scale=8, train_wall=520, gb_free=3.7, wall=39076
2022-02-21 17:46:09 | INFO | train_inner | epoch 010:    359 / 788 loss=5.371, ppl=41.39, wps=12305.9, ups=0.19, wpb=65534.7, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.566, loss_scale=16, train_wall=509, gb_free=3.7, wall=39609
2022-02-21 17:49:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 17:55:07 | INFO | train_inner | epoch 010:    460 / 788 loss=5.388, ppl=41.87, wps=12179.2, ups=0.19, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.62, loss_scale=8, train_wall=514, gb_free=3.7, wall=40147
2022-02-21 18:02:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 18:04:05 | INFO | train_inner | epoch 010:    561 / 788 loss=5.403, ppl=42.3, wps=12183.8, ups=0.19, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.611, loss_scale=8, train_wall=514, gb_free=3.7, wall=40685
2022-02-21 18:12:57 | INFO | train_inner | epoch 010:    661 / 788 loss=5.405, ppl=42.37, wps=12314.9, ups=0.19, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.582, loss_scale=8, train_wall=508, gb_free=3.7, wall=41217
2022-02-21 18:21:50 | INFO | train_inner | epoch 010:    761 / 788 loss=5.419, ppl=42.8, wps=12310.4, ups=0.19, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.595, loss_scale=16, train_wall=509, gb_free=3.7, wall=41749
2022-02-21 18:24:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 18:24:19 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.633 | ppl 49.62 | wps 29117.9 | wpb 510.9 | bsz 1 | num_updates 7827 | best_loss 5.633
2022-02-21 18:24:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7827 updates
2022-02-21 18:24:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 18:24:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 18:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 10 @ 7827 updates, score 5.633) (writing took 15.799293044954538 seconds)
2022-02-21 18:24:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-21 18:24:34 | INFO | train | epoch 010 | loss 5.38 | ppl 41.65 | wps 12133.6 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 7827 | lr 0.000357439 | gnorm 0.593 | loss_scale 16 | train_wall 4012 | gb_free 3.7 | wall 41914
2022-02-21 18:24:35 | INFO | fairseq.trainer | begin training epoch 11
2022-02-21 18:24:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 18:24:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 18:31:08 | INFO | train_inner | epoch 011:     74 / 788 loss=5.273, ppl=38.66, wps=11675, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.597, loss_scale=8, train_wall=511, gb_free=3.7, wall=42308
2022-02-21 18:40:01 | INFO | train_inner | epoch 011:    174 / 788 loss=5.254, ppl=38.15, wps=12300.2, ups=0.19, wpb=65534.7, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.572, loss_scale=16, train_wall=509, gb_free=3.7, wall=42841
2022-02-21 18:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 18:48:59 | INFO | train_inner | epoch 011:    275 / 788 loss=5.275, ppl=38.71, wps=12183, ups=0.19, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.582, loss_scale=16, train_wall=514, gb_free=3.7, wall=43379
2022-02-21 18:51:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 18:57:56 | INFO | train_inner | epoch 011:    376 / 788 loss=5.276, ppl=38.74, wps=12196.8, ups=0.19, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.586, loss_scale=8, train_wall=514, gb_free=3.7, wall=43916
2022-02-21 19:06:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 19:06:54 | INFO | train_inner | epoch 011:    477 / 788 loss=5.305, ppl=39.54, wps=12183.3, ups=0.19, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.599, loss_scale=8, train_wall=514, gb_free=3.7, wall=44454
2022-02-21 19:15:47 | INFO | train_inner | epoch 011:    577 / 788 loss=5.314, ppl=39.77, wps=12308.3, ups=0.19, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.577, loss_scale=8, train_wall=509, gb_free=3.7, wall=44987
2022-02-21 19:22:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 19:24:45 | INFO | train_inner | epoch 011:    678 / 788 loss=5.324, ppl=40.05, wps=12183.4, ups=0.19, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.601, loss_scale=8, train_wall=514, gb_free=3.7, wall=45524
2022-02-21 19:33:37 | INFO | train_inner | epoch 011:    778 / 788 loss=5.324, ppl=40.06, wps=12308.9, ups=0.19, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.584, loss_scale=8, train_wall=509, gb_free=3.7, wall=46057
2022-02-21 19:34:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 19:34:36 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.599 | ppl 48.46 | wps 28350.9 | wpb 510.9 | bsz 1 | num_updates 8610 | best_loss 5.599
2022-02-21 19:34:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8610 updates
2022-02-21 19:34:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 19:34:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 19:34:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 11 @ 8610 updates, score 5.599) (writing took 14.882346617989242 seconds)
2022-02-21 19:34:51 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-21 19:34:51 | INFO | train | epoch 011 | loss 5.29 | ppl 39.11 | wps 12163.8 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 8610 | lr 0.000340799 | gnorm 0.588 | loss_scale 16 | train_wall 4007 | gb_free 3.7 | wall 46130
2022-02-21 19:34:51 | INFO | fairseq.trainer | begin training epoch 12
2022-02-21 19:34:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 19:42:50 | INFO | train_inner | epoch 012:     90 / 788 loss=5.173, ppl=36.07, wps=11804.7, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.602, loss_scale=16, train_wall=506, gb_free=3.7, wall=46610
2022-02-21 19:45:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 19:51:48 | INFO | train_inner | epoch 012:    191 / 788 loss=5.179, ppl=36.24, wps=12182.3, ups=0.19, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.574, loss_scale=16, train_wall=514, gb_free=3.7, wall=47147
2022-02-21 19:51:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 20:00:45 | INFO | train_inner | epoch 012:    292 / 788 loss=5.197, ppl=36.69, wps=12190.4, ups=0.19, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.611, loss_scale=8, train_wall=514, gb_free=3.7, wall=47685
2022-02-21 20:08:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 20:09:43 | INFO | train_inner | epoch 012:    393 / 788 loss=5.222, ppl=37.32, wps=12188.6, ups=0.19, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.593, loss_scale=8, train_wall=514, gb_free=3.7, wall=48223
2022-02-21 20:18:36 | INFO | train_inner | epoch 012:    493 / 788 loss=5.216, ppl=37.16, wps=12306.3, ups=0.19, wpb=65534.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.604, loss_scale=8, train_wall=509, gb_free=3.7, wall=48755
2022-02-21 20:23:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 20:27:33 | INFO | train_inner | epoch 012:    594 / 788 loss=5.24, ppl=37.79, wps=12184.1, ups=0.19, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.582, loss_scale=8, train_wall=514, gb_free=3.7, wall=49293
2022-02-21 20:35:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 20:36:31 | INFO | train_inner | epoch 012:    695 / 788 loss=5.237, ppl=37.72, wps=12196.2, ups=0.19, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.618, loss_scale=8, train_wall=513, gb_free=3.7, wall=49831
2022-02-21 20:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 20:44:51 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.564 | ppl 47.31 | wps 29168.1 | wpb 510.9 | bsz 1 | num_updates 9393 | best_loss 5.564
2022-02-21 20:44:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9393 updates
2022-02-21 20:44:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 20:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 20:45:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 12 @ 9393 updates, score 5.564) (writing took 14.653290546499193 seconds)
2022-02-21 20:45:06 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-21 20:45:06 | INFO | train | epoch 012 | loss 5.213 | ppl 37.09 | wps 12166.2 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 9393 | lr 0.000326286 | gnorm 0.597 | loss_scale 8 | train_wall 4006 | gb_free 3.7 | wall 50346
2022-02-21 20:45:06 | INFO | fairseq.trainer | begin training epoch 13
2022-02-21 20:45:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 20:45:43 | INFO | train_inner | epoch 013:      7 / 788 loss=5.242, ppl=37.84, wps=11807.7, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=9400, lr=0.000326164, gnorm=0.587, loss_scale=8, train_wall=506, gb_free=3.7, wall=50383
2022-02-21 20:49:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 20:54:41 | INFO | train_inner | epoch 013:    108 / 788 loss=5.091, ppl=34.09, wps=12191.9, ups=0.19, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.611, loss_scale=8, train_wall=514, gb_free=3.7, wall=50921
2022-02-21 21:03:33 | INFO | train_inner | epoch 013:    208 / 788 loss=5.105, ppl=34.42, wps=12313, ups=0.19, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.584, loss_scale=16, train_wall=508, gb_free=3.7, wall=51453
2022-02-21 21:03:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 21:12:30 | INFO | train_inner | epoch 013:    309 / 788 loss=5.139, ppl=35.23, wps=12195.3, ups=0.19, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.59, loss_scale=8, train_wall=513, gb_free=3.7, wall=51990
2022-02-21 21:16:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 21:21:28 | INFO | train_inner | epoch 013:    410 / 788 loss=5.14, ppl=35.26, wps=12194.8, ups=0.19, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.611, loss_scale=8, train_wall=514, gb_free=3.7, wall=52528
2022-02-21 21:29:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 21:30:25 | INFO | train_inner | epoch 013:    511 / 788 loss=5.153, ppl=35.59, wps=12191.1, ups=0.19, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.615, loss_scale=8, train_wall=514, gb_free=3.7, wall=53065
2022-02-21 21:39:17 | INFO | train_inner | epoch 013:    611 / 788 loss=5.183, ppl=36.34, wps=12319.1, ups=0.19, wpb=65534.7, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.594, loss_scale=8, train_wall=508, gb_free=3.7, wall=53597
2022-02-21 21:47:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 21:48:15 | INFO | train_inner | epoch 013:    712 / 788 loss=5.187, ppl=36.43, wps=12190.8, ups=0.19, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.596, loss_scale=8, train_wall=514, gb_free=3.7, wall=54135
2022-02-21 21:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 21:55:04 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.552 | ppl 46.91 | wps 29180.5 | wpb 510.9 | bsz 1 | num_updates 10176 | best_loss 5.552
2022-02-21 21:55:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10176 updates
2022-02-21 21:55:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 21:55:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 21:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 13 @ 10176 updates, score 5.552) (writing took 15.029077114537358 seconds)
2022-02-21 21:55:20 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-21 21:55:20 | INFO | train | epoch 013 | loss 5.147 | ppl 35.42 | wps 12171 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 10176 | lr 0.000313481 | gnorm 0.6 | loss_scale 8 | train_wall 4005 | gb_free 3.7 | wall 54559
2022-02-21 21:55:20 | INFO | fairseq.trainer | begin training epoch 14
2022-02-21 21:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 21:57:27 | INFO | train_inner | epoch 014:     24 / 788 loss=5.154, ppl=35.61, wps=11811.3, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.606, loss_scale=8, train_wall=506, gb_free=3.7, wall=54687
2022-02-21 22:00:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 22:06:25 | INFO | train_inner | epoch 014:    125 / 788 loss=5.031, ppl=32.7, wps=12194.5, ups=0.19, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.605, loss_scale=8, train_wall=513, gb_free=3.7, wall=55224
2022-02-21 22:15:17 | INFO | train_inner | epoch 014:    225 / 788 loss=5.048, ppl=33.07, wps=12314.5, ups=0.19, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.598, loss_scale=16, train_wall=509, gb_free=3.7, wall=55757
2022-02-21 22:15:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 22:24:14 | INFO | train_inner | epoch 014:    326 / 788 loss=5.079, ppl=33.8, wps=12194, ups=0.19, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.61, loss_scale=8, train_wall=514, gb_free=3.7, wall=56294
2022-02-21 22:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 22:33:12 | INFO | train_inner | epoch 014:    427 / 788 loss=5.096, ppl=34.21, wps=12191, ups=0.19, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.602, loss_scale=8, train_wall=514, gb_free=3.7, wall=56832
2022-02-21 22:42:04 | INFO | train_inner | epoch 014:    527 / 788 loss=5.108, ppl=34.48, wps=12311.1, ups=0.19, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.609, loss_scale=8, train_wall=509, gb_free=3.7, wall=57364
2022-02-21 22:50:57 | INFO | train_inner | epoch 014:    627 / 788 loss=5.112, ppl=34.58, wps=12311.7, ups=0.19, wpb=65534.7, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.612, loss_scale=16, train_wall=509, gb_free=3.7, wall=57896
2022-02-21 22:52:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 22:59:54 | INFO | train_inner | epoch 014:    728 / 788 loss=5.132, ppl=35.07, wps=12192, ups=0.19, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.584, loss_scale=8, train_wall=514, gb_free=3.7, wall=58434
2022-02-21 23:05:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-21 23:05:19 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.542 | ppl 46.61 | wps 28521.5 | wpb 510.9 | bsz 1 | num_updates 10960 | best_loss 5.542
2022-02-21 23:05:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 10960 updates
2022-02-21 23:05:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 23:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-21 23:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 14 @ 10960 updates, score 5.542) (writing took 15.13552365731448 seconds)
2022-02-21 23:05:34 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-21 23:05:34 | INFO | train | epoch 014 | loss 5.088 | ppl 34.02 | wps 12184.8 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 10960 | lr 0.000302061 | gnorm 0.604 | loss_scale 16 | train_wall 4005 | gb_free 3.7 | wall 58774
2022-02-21 23:05:34 | INFO | fairseq.trainer | begin training epoch 15
2022-02-21 23:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-21 23:09:07 | INFO | train_inner | epoch 015:     40 / 788 loss=5.065, ppl=33.48, wps=11801.8, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.613, loss_scale=16, train_wall=506, gb_free=3.7, wall=58987
2022-02-21 23:15:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-21 23:16:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 23:18:10 | INFO | train_inner | epoch 015:    142 / 788 loss=4.982, ppl=31.61, wps=12068.2, ups=0.18, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.618, loss_scale=8, train_wall=519, gb_free=3.7, wall=59530
2022-02-21 23:27:02 | INFO | train_inner | epoch 015:    242 / 788 loss=5.002, ppl=32.05, wps=12312.5, ups=0.19, wpb=65534.7, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.594, loss_scale=8, train_wall=509, gb_free=3.7, wall=60062
2022-02-21 23:28:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 23:36:00 | INFO | train_inner | epoch 015:    343 / 788 loss=5.023, ppl=32.52, wps=12188.9, ups=0.19, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.618, loss_scale=8, train_wall=514, gb_free=3.7, wall=60600
2022-02-21 23:44:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-21 23:44:57 | INFO | train_inner | epoch 015:    444 / 788 loss=5.047, ppl=33.06, wps=12198.5, ups=0.19, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.606, loss_scale=8, train_wall=513, gb_free=3.7, wall=61137
2022-02-21 23:53:50 | INFO | train_inner | epoch 015:    544 / 788 loss=5.06, ppl=33.36, wps=12305.5, ups=0.19, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.587, loss_scale=8, train_wall=509, gb_free=3.7, wall=61669
2022-02-21 23:56:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 00:02:47 | INFO | train_inner | epoch 015:    645 / 788 loss=5.065, ppl=33.48, wps=12192.6, ups=0.19, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.632, loss_scale=8, train_wall=513, gb_free=3.7, wall=62207
2022-02-22 00:11:39 | INFO | train_inner | epoch 015:    745 / 788 loss=5.081, ppl=33.85, wps=12315.8, ups=0.19, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.596, loss_scale=16, train_wall=509, gb_free=3.7, wall=62739
2022-02-22 00:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 00:15:33 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.532 | ppl 46.28 | wps 27936.7 | wpb 510.9 | bsz 1 | num_updates 11743 | best_loss 5.532
2022-02-22 00:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11743 updates
2022-02-22 00:15:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 00:15:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 00:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 15 @ 11743 updates, score 5.532) (writing took 14.87162890471518 seconds)
2022-02-22 00:15:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-22 00:15:48 | INFO | train | epoch 015 | loss 5.037 | ppl 32.83 | wps 12168.3 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 11743 | lr 0.000291817 | gnorm 0.607 | loss_scale 16 | train_wall 4005 | gb_free 3.7 | wall 62988
2022-02-22 00:15:48 | INFO | fairseq.trainer | begin training epoch 16
2022-02-22 00:15:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 00:18:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 00:20:57 | INFO | train_inner | epoch 016:     58 / 788 loss=5.001, ppl=32.03, wps=11697.9, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.617, loss_scale=8, train_wall=511, gb_free=3.7, wall=63297
2022-02-22 00:29:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 00:29:54 | INFO | train_inner | epoch 016:    159 / 788 loss=4.945, ppl=30.81, wps=12204.2, ups=0.19, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.607, loss_scale=8, train_wall=513, gb_free=3.7, wall=63834
2022-02-22 00:38:46 | INFO | train_inner | epoch 016:    259 / 788 loss=4.957, ppl=31.07, wps=12324.4, ups=0.19, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.594, loss_scale=8, train_wall=508, gb_free=3.7, wall=64365
2022-02-22 00:42:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 00:47:43 | INFO | train_inner | epoch 016:    360 / 788 loss=4.98, ppl=31.56, wps=12203, ups=0.19, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.604, loss_scale=8, train_wall=513, gb_free=3.7, wall=64902
2022-02-22 00:56:34 | INFO | train_inner | epoch 016:    460 / 788 loss=4.998, ppl=31.96, wps=12331.2, ups=0.19, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.629, loss_scale=16, train_wall=508, gb_free=3.7, wall=65434
2022-02-22 01:01:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 01:05:31 | INFO | train_inner | epoch 016:    561 / 788 loss=5.014, ppl=32.3, wps=12202, ups=0.19, wpb=65534.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.594, loss_scale=8, train_wall=513, gb_free=3.7, wall=65971
2022-02-22 01:14:23 | INFO | train_inner | epoch 016:    661 / 788 loss=5.023, ppl=32.51, wps=12320.9, ups=0.19, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.611, loss_scale=16, train_wall=508, gb_free=3.7, wall=66503
2022-02-22 01:23:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 01:23:20 | INFO | train_inner | epoch 016:    762 / 788 loss=5.047, ppl=33.05, wps=12206.2, ups=0.19, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.605, loss_scale=8, train_wall=513, gb_free=3.7, wall=67040
2022-02-22 01:25:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 01:25:44 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.524 | ppl 46.01 | wps 28915.9 | wpb 510.9 | bsz 1 | num_updates 12526 | best_loss 5.524
2022-02-22 01:25:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12526 updates
2022-02-22 01:25:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 01:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 01:25:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 16 @ 12526 updates, score 5.524) (writing took 14.856295570731163 seconds)
2022-02-22 01:25:58 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-22 01:25:58 | INFO | train | epoch 016 | loss 4.991 | ppl 31.8 | wps 12181.4 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 12526 | lr 0.000282549 | gnorm 0.61 | loss_scale 8 | train_wall 4000 | gb_free 3.7 | wall 67198
2022-02-22 01:25:59 | INFO | fairseq.trainer | begin training epoch 17
2022-02-22 01:25:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 01:32:32 | INFO | train_inner | epoch 017:     74 / 788 loss=4.91, ppl=30.06, wps=11822.8, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.628, loss_scale=8, train_wall=506, gb_free=3.7, wall=67592
2022-02-22 01:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 01:41:29 | INFO | train_inner | epoch 017:    175 / 788 loss=4.896, ppl=29.76, wps=12201.4, ups=0.19, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.617, loss_scale=8, train_wall=513, gb_free=3.7, wall=68129
2022-02-22 01:50:21 | INFO | train_inner | epoch 017:    275 / 788 loss=4.932, ppl=30.52, wps=12322.2, ups=0.19, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.626, loss_scale=8, train_wall=508, gb_free=3.7, wall=68661
2022-02-22 01:51:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 01:59:18 | INFO | train_inner | epoch 017:    376 / 788 loss=4.947, ppl=30.84, wps=12207.8, ups=0.19, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.635, loss_scale=8, train_wall=513, gb_free=3.7, wall=69197
2022-02-22 02:08:09 | INFO | train_inner | epoch 017:    476 / 788 loss=4.956, ppl=31.04, wps=12327.9, ups=0.19, wpb=65534.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.599, loss_scale=16, train_wall=508, gb_free=3.7, wall=69729
2022-02-22 02:12:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 02:17:07 | INFO | train_inner | epoch 017:    577 / 788 loss=4.983, ppl=31.62, wps=12200.3, ups=0.19, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.617, loss_scale=8, train_wall=513, gb_free=3.7, wall=70266
2022-02-22 02:25:58 | INFO | train_inner | epoch 017:    677 / 788 loss=4.98, ppl=31.57, wps=12325, ups=0.19, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.614, loss_scale=16, train_wall=508, gb_free=3.7, wall=70798
2022-02-22 02:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 02:34:55 | INFO | train_inner | epoch 017:    778 / 788 loss=5.005, ppl=32.12, wps=12211.9, ups=0.19, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.635, loss_scale=8, train_wall=513, gb_free=3.7, wall=71335
2022-02-22 02:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 02:35:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.518 | ppl 45.82 | wps 29254.8 | wpb 510.9 | bsz 1 | num_updates 13310 | best_loss 5.518
2022-02-22 02:35:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13310 updates
2022-02-22 02:35:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 02:36:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 02:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 17 @ 13310 updates, score 5.518) (writing took 15.951030567288399 seconds)
2022-02-22 02:36:09 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-22 02:36:09 | INFO | train | epoch 017 | loss 4.949 | ppl 30.9 | wps 12194.9 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 13310 | lr 0.000274101 | gnorm 0.62 | loss_scale 8 | train_wall 4000 | gb_free 3.7 | wall 71409
2022-02-22 02:36:09 | INFO | fairseq.trainer | begin training epoch 18
2022-02-22 02:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 02:40:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 02:44:14 | INFO | train_inner | epoch 018:     91 / 788 loss=4.856, ppl=28.96, wps=11672.5, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.602, loss_scale=8, train_wall=511, gb_free=3.7, wall=71894
2022-02-22 02:53:06 | INFO | train_inner | epoch 018:    191 / 788 loss=4.867, ppl=29.19, wps=12321.1, ups=0.19, wpb=65534.7, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.622, loss_scale=16, train_wall=508, gb_free=3.7, wall=72425
2022-02-22 02:56:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 03:02:03 | INFO | train_inner | epoch 018:    292 / 788 loss=4.887, ppl=29.6, wps=12194.8, ups=0.19, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.637, loss_scale=8, train_wall=513, gb_free=3.7, wall=72963
2022-02-22 03:08:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 03:11:00 | INFO | train_inner | epoch 018:    393 / 788 loss=4.905, ppl=29.96, wps=12205.6, ups=0.19, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.619, loss_scale=8, train_wall=513, gb_free=3.7, wall=73500
2022-02-22 03:19:52 | INFO | train_inner | epoch 018:    493 / 788 loss=4.919, ppl=30.25, wps=12323.8, ups=0.19, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.614, loss_scale=16, train_wall=508, gb_free=3.7, wall=74032
2022-02-22 03:22:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 03:28:49 | INFO | train_inner | epoch 018:    594 / 788 loss=4.94, ppl=30.7, wps=12206.4, ups=0.19, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.624, loss_scale=8, train_wall=513, gb_free=3.7, wall=74569
2022-02-22 03:37:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 03:37:46 | INFO | train_inner | epoch 018:    695 / 788 loss=4.962, ppl=31.16, wps=12201, ups=0.19, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.628, loss_scale=8, train_wall=513, gb_free=3.7, wall=75106
2022-02-22 03:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 03:46:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.515 | ppl 45.74 | wps 28420 | wpb 510.9 | bsz 1 | num_updates 14093 | best_loss 5.515
2022-02-22 03:46:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14093 updates
2022-02-22 03:46:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 03:46:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 03:46:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 18 @ 14093 updates, score 5.515) (writing took 16.369843780994415 seconds)
2022-02-22 03:46:23 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-22 03:46:23 | INFO | train | epoch 018 | loss 4.911 | ppl 30.08 | wps 12172 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 14093 | lr 0.000266378 | gnorm 0.621 | loss_scale 8 | train_wall 4002 | gb_free 3.7 | wall 75622
2022-02-22 03:46:23 | INFO | fairseq.trainer | begin training epoch 19
2022-02-22 03:46:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 03:47:00 | INFO | train_inner | epoch 019:      7 / 788 loss=4.953, ppl=30.98, wps=11778, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=14100, lr=0.000266312, gnorm=0.617, loss_scale=8, train_wall=506, gb_free=3.7, wall=75660
2022-02-22 03:55:53 | INFO | train_inner | epoch 019:    107 / 788 loss=4.799, ppl=27.83, wps=12298.7, ups=0.19, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.605, loss_scale=16, train_wall=509, gb_free=3.7, wall=76192
2022-02-22 03:56:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 04:04:50 | INFO | train_inner | epoch 019:    208 / 788 loss=4.827, ppl=28.39, wps=12200.4, ups=0.19, wpb=65534.7, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.629, loss_scale=8, train_wall=513, gb_free=3.7, wall=76730
2022-02-22 04:13:42 | INFO | train_inner | epoch 019:    308 / 788 loss=4.858, ppl=29.01, wps=12311.7, ups=0.19, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.636, loss_scale=16, train_wall=508, gb_free=3.7, wall=77262
2022-02-22 04:16:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 04:22:40 | INFO | train_inner | epoch 019:    409 / 788 loss=4.879, ppl=29.43, wps=12195, ups=0.19, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.626, loss_scale=8, train_wall=513, gb_free=3.7, wall=77799
2022-02-22 04:31:32 | INFO | train_inner | epoch 019:    509 / 788 loss=4.905, ppl=29.97, wps=12319, ups=0.19, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.621, loss_scale=16, train_wall=508, gb_free=3.7, wall=78331
2022-02-22 04:31:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 04:40:29 | INFO | train_inner | epoch 019:    610 / 788 loss=4.904, ppl=29.94, wps=12201.7, ups=0.19, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.626, loss_scale=8, train_wall=513, gb_free=3.7, wall=78868
2022-02-22 04:43:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 04:49:26 | INFO | train_inner | epoch 019:    711 / 788 loss=4.92, ppl=30.28, wps=12200.9, ups=0.19, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.636, loss_scale=8, train_wall=513, gb_free=3.7, wall=79406
2022-02-22 04:55:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 04:56:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 04:56:21 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.511 | ppl 45.59 | wps 28221.3 | wpb 510.9 | bsz 1 | num_updates 14876 | best_loss 5.511
2022-02-22 04:56:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14876 updates
2022-02-22 04:56:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 04:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 04:56:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 19 @ 14876 updates, score 5.511) (writing took 14.471311999484897 seconds)
2022-02-22 04:56:35 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-22 04:56:35 | INFO | train | epoch 019 | loss 4.876 | ppl 29.37 | wps 12173.4 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 14876 | lr 0.000259273 | gnorm 0.63 | loss_scale 8 | train_wall 4002 | gb_free 3.7 | wall 79835
2022-02-22 04:56:35 | INFO | fairseq.trainer | begin training epoch 20
2022-02-22 04:56:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 04:58:43 | INFO | train_inner | epoch 020:     24 / 788 loss=4.894, ppl=29.73, wps=11709.9, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=14900, lr=0.000259064, gnorm=0.653, loss_scale=8, train_wall=511, gb_free=3.7, wall=79963
2022-02-22 05:07:35 | INFO | train_inner | epoch 020:    124 / 788 loss=4.774, ppl=27.36, wps=12321.6, ups=0.19, wpb=65534.7, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.625, loss_scale=16, train_wall=508, gb_free=3.7, wall=80495
2022-02-22 05:11:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 05:16:32 | INFO | train_inner | epoch 020:    225 / 788 loss=4.801, ppl=27.88, wps=12199.1, ups=0.19, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.639, loss_scale=8, train_wall=513, gb_free=3.7, wall=81032
2022-02-22 05:25:24 | INFO | train_inner | epoch 020:    325 / 788 loss=4.827, ppl=28.38, wps=12315.9, ups=0.19, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.638, loss_scale=16, train_wall=508, gb_free=3.7, wall=81564
2022-02-22 05:28:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 05:34:22 | INFO | train_inner | epoch 020:    426 / 788 loss=4.853, ppl=28.89, wps=12190.8, ups=0.19, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.646, loss_scale=8, train_wall=513, gb_free=3.7, wall=82101
2022-02-22 05:42:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 05:43:19 | INFO | train_inner | epoch 020:    527 / 788 loss=4.861, ppl=29.07, wps=12196.2, ups=0.19, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.619, loss_scale=8, train_wall=513, gb_free=3.7, wall=82639
2022-02-22 05:52:11 | INFO | train_inner | epoch 020:    627 / 788 loss=4.88, ppl=29.45, wps=12319.8, ups=0.19, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.63, loss_scale=8, train_wall=508, gb_free=3.7, wall=83171
2022-02-22 05:54:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 06:01:09 | INFO | train_inner | epoch 020:    728 / 788 loss=4.892, ppl=29.69, wps=12190.5, ups=0.19, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.646, loss_scale=8, train_wall=513, gb_free=3.7, wall=83708
2022-02-22 06:05:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 06:06:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 06:06:33 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.512 | ppl 45.64 | wps 28380.7 | wpb 510.9 | bsz 1 | num_updates 15659 | best_loss 5.511
2022-02-22 06:06:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15659 updates
2022-02-22 06:06:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 06:06:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 06:06:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 20 @ 15659 updates, score 5.512) (writing took 7.253375880420208 seconds)
2022-02-22 06:06:41 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-22 06:06:41 | INFO | train | epoch 020 | loss 4.844 | ppl 28.71 | wps 12195.2 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 15659 | lr 0.000252707 | gnorm 0.634 | loss_scale 8 | train_wall 4002 | gb_free 3.7 | wall 84040
2022-02-22 06:06:41 | INFO | fairseq.trainer | begin training epoch 21
2022-02-22 06:06:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 06:10:19 | INFO | train_inner | epoch 021:     41 / 788 loss=4.834, ppl=28.53, wps=11855.2, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.652, loss_scale=8, train_wall=511, gb_free=3.7, wall=84259
2022-02-22 06:19:11 | INFO | train_inner | epoch 021:    141 / 788 loss=4.751, ppl=26.93, wps=12315.8, ups=0.19, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.636, loss_scale=16, train_wall=508, gb_free=3.7, wall=84791
2022-02-22 06:25:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 06:28:08 | INFO | train_inner | epoch 021:    242 / 788 loss=4.784, ppl=27.55, wps=12194.5, ups=0.19, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.632, loss_scale=8, train_wall=513, gb_free=3.7, wall=85328
2022-02-22 06:37:00 | INFO | train_inner | epoch 021:    342 / 788 loss=4.79, ppl=27.67, wps=12325.7, ups=0.19, wpb=65534.7, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.635, loss_scale=16, train_wall=508, gb_free=3.7, wall=85860
2022-02-22 06:38:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 06:45:57 | INFO | train_inner | epoch 021:    443 / 788 loss=4.824, ppl=28.33, wps=12200.9, ups=0.19, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.653, loss_scale=8, train_wall=513, gb_free=3.7, wall=86397
2022-02-22 06:50:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 06:54:54 | INFO | train_inner | epoch 021:    544 / 788 loss=4.836, ppl=28.56, wps=12200.9, ups=0.19, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.65, loss_scale=8, train_wall=513, gb_free=3.7, wall=86934
2022-02-22 07:02:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 07:03:51 | INFO | train_inner | epoch 021:    645 / 788 loss=4.847, ppl=28.78, wps=12208.2, ups=0.19, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.631, loss_scale=8, train_wall=513, gb_free=3.7, wall=87471
2022-02-22 07:12:43 | INFO | train_inner | epoch 021:    745 / 788 loss=4.874, ppl=29.32, wps=12325.2, ups=0.19, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.644, loss_scale=8, train_wall=508, gb_free=3.7, wall=88003
2022-02-22 07:13:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 07:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 07:16:37 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.511 | ppl 45.59 | wps 28976.4 | wpb 510.9 | bsz 1 | num_updates 16442 | best_loss 5.511
2022-02-22 07:16:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16442 updates
2022-02-22 07:16:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 07:16:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt
2022-02-22 07:16:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 21 @ 16442 updates, score 5.511) (writing took 15.194161518476903 seconds)
2022-02-22 07:16:52 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-22 07:16:52 | INFO | train | epoch 021 | loss 4.814 | ppl 28.12 | wps 12177.3 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 16442 | lr 0.000246617 | gnorm 0.642 | loss_scale 8 | train_wall 4001 | gb_free 3.7 | wall 88252
2022-02-22 07:16:52 | INFO | fairseq.trainer | begin training epoch 22
2022-02-22 07:16:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 07:22:01 | INFO | train_inner | epoch 022:     58 / 788 loss=4.774, ppl=27.35, wps=11700.5, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.639, loss_scale=8, train_wall=511, gb_free=3.7, wall=88560
2022-02-22 07:29:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 07:30:58 | INFO | train_inner | epoch 022:    159 / 788 loss=4.732, ppl=26.57, wps=12197.5, ups=0.19, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.679, loss_scale=8, train_wall=513, gb_free=3.7, wall=89098
2022-02-22 07:39:50 | INFO | train_inner | epoch 022:    259 / 788 loss=4.746, ppl=26.83, wps=12321.9, ups=0.19, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.67, loss_scale=8, train_wall=508, gb_free=3.7, wall=89629
2022-02-22 07:44:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 07:48:47 | INFO | train_inner | epoch 022:    360 / 788 loss=4.781, ppl=27.48, wps=12201.6, ups=0.19, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.645, loss_scale=8, train_wall=513, gb_free=3.7, wall=90167
2022-02-22 07:57:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 07:57:44 | INFO | train_inner | epoch 022:    461 / 788 loss=4.789, ppl=27.65, wps=12200.9, ups=0.19, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.659, loss_scale=8, train_wall=513, gb_free=3.7, wall=90704
2022-02-22 08:06:36 | INFO | train_inner | epoch 022:    561 / 788 loss=4.819, ppl=28.23, wps=12327.6, ups=0.19, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.637, loss_scale=8, train_wall=508, gb_free=3.7, wall=91235
2022-02-22 08:10:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 08:15:32 | INFO | train_inner | epoch 022:    662 / 788 loss=4.826, ppl=28.37, wps=12207.8, ups=0.19, wpb=65534.7, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.634, loss_scale=8, train_wall=513, gb_free=3.7, wall=91772
2022-02-22 08:24:24 | INFO | train_inner | epoch 022:    762 / 788 loss=4.841, ppl=28.67, wps=12324.4, ups=0.19, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.647, loss_scale=16, train_wall=508, gb_free=3.7, wall=92304
2022-02-22 08:26:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 08:26:48 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.513 | ppl 45.67 | wps 28675.1 | wpb 510.9 | bsz 1 | num_updates 17226 | best_loss 5.511
2022-02-22 08:26:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17226 updates
2022-02-22 08:26:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 08:26:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 08:26:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 22 @ 17226 updates, score 5.513) (writing took 7.508518172428012 seconds)
2022-02-22 08:26:55 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-22 08:26:55 | INFO | train | epoch 022 | loss 4.786 | ppl 27.6 | wps 12217.1 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 17226 | lr 0.000240939 | gnorm 0.652 | loss_scale 16 | train_wall 4001 | gb_free 3.7 | wall 92455
2022-02-22 08:26:55 | INFO | fairseq.trainer | begin training epoch 23
2022-02-22 08:26:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 08:30:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 08:33:34 | INFO | train_inner | epoch 023:     75 / 788 loss=4.727, ppl=26.48, wps=11859.5, ups=0.18, wpb=65232.6, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.675, loss_scale=8, train_wall=511, gb_free=3.7, wall=92854
2022-02-22 08:42:26 | INFO | train_inner | epoch 023:    175 / 788 loss=4.702, ppl=26.03, wps=12323, ups=0.19, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.647, loss_scale=16, train_wall=508, gb_free=3.7, wall=93386
2022-02-22 08:48:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 08:51:23 | INFO | train_inner | epoch 023:    276 / 788 loss=4.718, ppl=26.33, wps=12202, ups=0.19, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.645, loss_scale=8, train_wall=513, gb_free=3.7, wall=93923
2022-02-22 09:00:15 | INFO | train_inner | epoch 023:    376 / 788 loss=4.754, ppl=26.98, wps=12328.5, ups=0.19, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.675, loss_scale=16, train_wall=508, gb_free=3.7, wall=94454
2022-02-22 09:02:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 09:09:12 | INFO | train_inner | epoch 023:    477 / 788 loss=4.773, ppl=27.35, wps=12207, ups=0.19, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.658, loss_scale=8, train_wall=513, gb_free=3.7, wall=94991
2022-02-22 09:14:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 09:18:08 | INFO | train_inner | epoch 023:    578 / 788 loss=4.788, ppl=27.62, wps=12207.4, ups=0.19, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.643, loss_scale=8, train_wall=513, gb_free=3.7, wall=95528
2022-02-22 09:27:00 | INFO | train_inner | epoch 023:    678 / 788 loss=4.808, ppl=28.01, wps=12333.6, ups=0.19, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.647, loss_scale=16, train_wall=508, gb_free=3.7, wall=96060
2022-02-22 09:32:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 09:35:57 | INFO | train_inner | epoch 023:    779 / 788 loss=4.827, ppl=28.39, wps=12205.8, ups=0.19, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.648, loss_scale=8, train_wall=513, gb_free=3.7, wall=96596
2022-02-22 09:36:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 09:36:50 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.518 | ppl 45.82 | wps 28180.2 | wpb 510.9 | bsz 1 | num_updates 18009 | best_loss 5.511
2022-02-22 09:36:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18009 updates
2022-02-22 09:36:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 09:36:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 09:36:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 23 @ 18009 updates, score 5.518) (writing took 7.14892753213644 seconds)
2022-02-22 09:36:57 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-22 09:36:57 | INFO | train | epoch 023 | loss 4.76 | ppl 27.09 | wps 12205.3 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 18009 | lr 0.000235643 | gnorm 0.655 | loss_scale 8 | train_wall 4000 | gb_free 3.7 | wall 96657
2022-02-22 09:36:57 | INFO | fairseq.trainer | begin training epoch 24
2022-02-22 09:36:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 09:45:01 | INFO | train_inner | epoch 024:     91 / 788 loss=4.667, ppl=25.4, wps=11986.8, ups=0.18, wpb=65232.6, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.655, loss_scale=16, train_wall=505, gb_free=3.7, wall=97141
2022-02-22 09:48:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 09:53:58 | INFO | train_inner | epoch 024:    192 / 788 loss=4.684, ppl=25.71, wps=12204.9, ups=0.19, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.638, loss_scale=8, train_wall=513, gb_free=3.7, wall=97678
2022-02-22 10:02:50 | INFO | train_inner | epoch 024:    292 / 788 loss=4.727, ppl=26.49, wps=12327.6, ups=0.19, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.661, loss_scale=16, train_wall=508, gb_free=3.7, wall=98209
2022-02-22 10:06:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 10:11:46 | INFO | train_inner | epoch 024:    393 / 788 loss=4.735, ppl=26.62, wps=12207.4, ups=0.19, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.652, loss_scale=8, train_wall=513, gb_free=3.7, wall=98746
2022-02-22 10:20:38 | INFO | train_inner | epoch 024:    493 / 788 loss=4.742, ppl=26.75, wps=12326.8, ups=0.19, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.655, loss_scale=16, train_wall=508, gb_free=3.7, wall=99278
2022-02-22 10:20:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 10:29:35 | INFO | train_inner | epoch 024:    594 / 788 loss=4.765, ppl=27.19, wps=12203.4, ups=0.19, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.656, loss_scale=8, train_wall=513, gb_free=3.7, wall=99815
2022-02-22 10:38:27 | INFO | train_inner | epoch 024:    694 / 788 loss=4.78, ppl=27.47, wps=12324.7, ups=0.19, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.656, loss_scale=16, train_wall=508, gb_free=3.7, wall=100347
2022-02-22 10:43:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 10:46:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 10:46:52 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.52 | ppl 45.88 | wps 28489.6 | wpb 510.9 | bsz 1 | num_updates 18793 | best_loss 5.511
2022-02-22 10:46:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18793 updates
2022-02-22 10:46:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 10:46:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 10:46:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 24 @ 18793 updates, score 5.52) (writing took 7.114065666683018 seconds)
2022-02-22 10:46:59 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-22 10:46:59 | INFO | train | epoch 024 | loss 4.736 | ppl 26.64 | wps 12219.3 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 18793 | lr 0.000230676 | gnorm 0.653 | loss_scale 8 | train_wall 3999 | gb_free 3.7 | wall 100859
2022-02-22 10:46:59 | INFO | fairseq.trainer | begin training epoch 25
2022-02-22 10:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 10:47:37 | INFO | train_inner | epoch 025:      7 / 788 loss=4.783, ppl=27.54, wps=11862, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=18800, lr=0.000230633, gnorm=0.659, loss_scale=8, train_wall=511, gb_free=3.7, wall=100896
2022-02-22 10:56:29 | INFO | train_inner | epoch 025:    107 / 788 loss=4.631, ppl=24.77, wps=12316.2, ups=0.19, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.646, loss_scale=16, train_wall=508, gb_free=3.7, wall=101429
2022-02-22 10:57:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 11:05:26 | INFO | train_inner | epoch 025:    208 / 788 loss=4.662, ppl=25.31, wps=12207.8, ups=0.19, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.668, loss_scale=8, train_wall=513, gb_free=3.7, wall=101965
2022-02-22 11:14:17 | INFO | train_inner | epoch 025:    308 / 788 loss=4.689, ppl=25.79, wps=12327.6, ups=0.19, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.653, loss_scale=16, train_wall=508, gb_free=3.7, wall=102497
2022-02-22 11:15:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 11:23:15 | INFO | train_inner | epoch 025:    409 / 788 loss=4.722, ppl=26.38, wps=12197.6, ups=0.19, wpb=65534.7, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.663, loss_scale=8, train_wall=513, gb_free=3.7, wall=103034
2022-02-22 11:28:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 11:32:12 | INFO | train_inner | epoch 025:    510 / 788 loss=4.72, ppl=26.35, wps=12192.5, ups=0.19, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.648, loss_scale=8, train_wall=513, gb_free=3.7, wall=103572
2022-02-22 11:41:04 | INFO | train_inner | epoch 025:    610 / 788 loss=4.753, ppl=26.96, wps=12310.8, ups=0.19, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.673, loss_scale=16, train_wall=508, gb_free=3.7, wall=104104
2022-02-22 11:41:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 11:50:02 | INFO | train_inner | epoch 025:    711 / 788 loss=4.773, ppl=27.35, wps=12187.3, ups=0.19, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.649, loss_scale=8, train_wall=514, gb_free=3.7, wall=104642
2022-02-22 11:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 11:56:57 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.535 | ppl 46.35 | wps 28905.7 | wpb 510.9 | bsz 1 | num_updates 19577 | best_loss 5.511
2022-02-22 11:56:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19577 updates
2022-02-22 11:56:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 11:57:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 11:57:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 25 @ 19577 updates, score 5.535) (writing took 7.50280056335032 seconds)
2022-02-22 11:57:05 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-22 11:57:05 | INFO | train | epoch 025 | loss 4.712 | ppl 26.22 | wps 12210.1 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 19577 | lr 0.00022601 | gnorm 0.658 | loss_scale 16 | train_wall 4002 | gb_free 3.7 | wall 105065
2022-02-22 11:57:05 | INFO | fairseq.trainer | begin training epoch 26
2022-02-22 11:57:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 11:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 11:59:13 | INFO | train_inner | epoch 026:     24 / 788 loss=4.734, ppl=26.62, wps=11844.3, ups=0.18, wpb=65233.9, bsz=127.4, num_updates=19600, lr=0.000225877, gnorm=0.66, loss_scale=8, train_wall=511, gb_free=3.7, wall=105193
2022-02-22 12:08:06 | INFO | train_inner | epoch 026:    124 / 788 loss=4.609, ppl=24.4, wps=12305.7, ups=0.19, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.686, loss_scale=8, train_wall=509, gb_free=3.7, wall=105725
2022-02-22 12:15:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 12:17:04 | INFO | train_inner | epoch 026:    225 / 788 loss=4.647, ppl=25.05, wps=12177.5, ups=0.19, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.65, loss_scale=8, train_wall=514, gb_free=3.7, wall=106263
2022-02-22 12:25:56 | INFO | train_inner | epoch 026:    325 / 788 loss=4.668, ppl=25.42, wps=12304.6, ups=0.19, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.661, loss_scale=8, train_wall=509, gb_free=3.7, wall=106796
2022-02-22 12:32:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 12:34:54 | INFO | train_inner | epoch 026:    426 / 788 loss=4.69, ppl=25.81, wps=12184.7, ups=0.19, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.659, loss_scale=8, train_wall=514, gb_free=3.7, wall=107334
2022-02-22 12:43:47 | INFO | train_inner | epoch 026:    526 / 788 loss=4.705, ppl=26.08, wps=12301.2, ups=0.19, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.667, loss_scale=8, train_wall=509, gb_free=3.7, wall=107867
2022-02-22 12:44:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 12:52:45 | INFO | train_inner | epoch 026:    627 / 788 loss=4.737, ppl=26.66, wps=12183.1, ups=0.19, wpb=65534.7, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.69, loss_scale=8, train_wall=514, gb_free=3.7, wall=108405
2022-02-22 13:01:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 13:01:43 | INFO | train_inner | epoch 026:    728 / 788 loss=4.755, ppl=27, wps=12175.8, ups=0.19, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.658, loss_scale=8, train_wall=514, gb_free=3.7, wall=108943
2022-02-22 13:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 13:07:08 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.539 | ppl 46.48 | wps 28095.4 | wpb 510.9 | bsz 1 | num_updates 20360 | best_loss 5.511
2022-02-22 13:07:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20360 updates
2022-02-22 13:07:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 13:07:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 13:07:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 26 @ 20360 updates, score 5.539) (writing took 7.506714717485011 seconds)
2022-02-22 13:07:16 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-22 13:07:16 | INFO | train | epoch 026 | loss 4.691 | ppl 25.82 | wps 12179.5 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 20360 | lr 0.000221621 | gnorm 0.666 | loss_scale 8 | train_wall 4006 | gb_free 3.7 | wall 109275
2022-02-22 13:07:16 | INFO | fairseq.trainer | begin training epoch 27
2022-02-22 13:07:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 13:10:49 | INFO | train_inner | epoch 027:     40 / 788 loss=4.69, ppl=25.82, wps=11955.7, ups=0.18, wpb=65232.6, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.675, loss_scale=8, train_wall=506, gb_free=3.7, wall=109489
2022-02-22 13:18:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 13:19:47 | INFO | train_inner | epoch 027:    141 / 788 loss=4.601, ppl=24.27, wps=12173.1, ups=0.19, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.667, loss_scale=8, train_wall=514, gb_free=3.7, wall=110027
2022-02-22 13:28:40 | INFO | train_inner | epoch 027:    241 / 788 loss=4.63, ppl=24.76, wps=12305.3, ups=0.19, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.68, loss_scale=8, train_wall=508, gb_free=3.7, wall=110559
2022-02-22 13:37:33 | INFO | train_inner | epoch 027:    341 / 788 loss=4.665, ppl=25.37, wps=12295.7, ups=0.19, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.675, loss_scale=16, train_wall=509, gb_free=3.7, wall=111092
2022-02-22 13:39:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 13:46:28 | INFO | train_inner | epoch 027:    442 / 788 loss=4.667, ppl=25.4, wps=12254.8, ups=0.19, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.684, loss_scale=8, train_wall=511, gb_free=3.7, wall=111627
2022-02-22 13:52:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 13:55:16 | INFO | train_inner | epoch 027:    543 / 788 loss=4.697, ppl=25.95, wps=12401.1, ups=0.19, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.662, loss_scale=8, train_wall=506, gb_free=3.7, wall=112156
2022-02-22 14:03:58 | INFO | train_inner | epoch 027:    643 / 788 loss=4.706, ppl=26.1, wps=12564.8, ups=0.19, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.674, loss_scale=16, train_wall=499, gb_free=3.7, wall=112677
2022-02-22 14:06:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 14:12:44 | INFO | train_inner | epoch 027:    744 / 788 loss=4.726, ppl=26.46, wps=12447.7, ups=0.19, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.676, loss_scale=8, train_wall=504, gb_free=3.7, wall=113204
2022-02-22 14:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 14:16:38 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.549 | ppl 46.82 | wps 30658.7 | wpb 510.9 | bsz 1 | num_updates 21144 | best_loss 5.511
2022-02-22 14:16:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21144 updates
2022-02-22 14:16:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 14:16:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 14:16:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 27 @ 21144 updates, score 5.549) (writing took 5.995400425978005 seconds)
2022-02-22 14:16:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-22 14:16:44 | INFO | train | epoch 027 | loss 4.67 | ppl 25.45 | wps 12318.2 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 21144 | lr 0.000217474 | gnorm 0.677 | loss_scale 8 | train_wall 3972 | gb_free 3.7 | wall 113444
2022-02-22 14:16:44 | INFO | fairseq.trainer | begin training epoch 28
2022-02-22 14:16:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 14:21:37 | INFO | train_inner | epoch 028:     56 / 788 loss=4.646, ppl=25.04, wps=12249.5, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.682, loss_scale=16, train_wall=497, gb_free=3.7, wall=113736
2022-02-22 14:29:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 14:30:24 | INFO | train_inner | epoch 028:    157 / 788 loss=4.591, ppl=24.11, wps=12432, ups=0.19, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.674, loss_scale=16, train_wall=505, gb_free=3.7, wall=114264
2022-02-22 14:38:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 14:39:11 | INFO | train_inner | epoch 028:    258 / 788 loss=4.622, ppl=24.62, wps=12436.6, ups=0.19, wpb=65534.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.68, loss_scale=8, train_wall=504, gb_free=3.7, wall=114790
2022-02-22 14:47:53 | INFO | train_inner | epoch 028:    358 / 788 loss=4.639, ppl=24.91, wps=12558.3, ups=0.19, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.668, loss_scale=8, train_wall=500, gb_free=3.7, wall=115312
2022-02-22 14:56:35 | INFO | train_inner | epoch 028:    458 / 788 loss=4.644, ppl=24.99, wps=12556, ups=0.19, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.687, loss_scale=16, train_wall=500, gb_free=3.7, wall=115834
2022-02-22 14:57:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 15:05:21 | INFO | train_inner | epoch 028:    559 / 788 loss=4.682, ppl=25.68, wps=12442.4, ups=0.19, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.673, loss_scale=8, train_wall=504, gb_free=3.7, wall=116361
2022-02-22 15:12:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 15:14:08 | INFO | train_inner | epoch 028:    660 / 788 loss=4.695, ppl=25.89, wps=12442.8, ups=0.19, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.694, loss_scale=8, train_wall=504, gb_free=3.7, wall=116888
2022-02-22 15:22:49 | INFO | train_inner | epoch 028:    760 / 788 loss=4.702, ppl=26.04, wps=12568.8, ups=0.19, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.679, loss_scale=8, train_wall=499, gb_free=3.7, wall=117409
2022-02-22 15:25:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 15:25:20 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.555 | ppl 47 | wps 30749.5 | wpb 510.9 | bsz 1 | num_updates 21928 | best_loss 5.511
2022-02-22 15:25:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 21928 updates
2022-02-22 15:25:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 15:25:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 15:25:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 28 @ 21928 updates, score 5.555) (writing took 5.822356935590506 seconds)
2022-02-22 15:25:26 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-22 15:25:26 | INFO | train | epoch 028 | loss 4.65 | ppl 25.11 | wps 12457.8 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 21928 | lr 0.00021355 | gnorm 0.678 | loss_scale 16 | train_wall 3933 | gb_free 3.7 | wall 117566
2022-02-22 15:25:26 | INFO | fairseq.trainer | begin training epoch 29
2022-02-22 15:25:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 15:26:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 15:31:47 | INFO | train_inner | epoch 029:     73 / 788 loss=4.599, ppl=24.23, wps=12135.6, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.669, loss_scale=8, train_wall=502, gb_free=3.7, wall=117947
2022-02-22 15:40:29 | INFO | train_inner | epoch 029:    173 / 788 loss=4.578, ppl=23.88, wps=12565, ups=0.19, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.672, loss_scale=16, train_wall=499, gb_free=3.7, wall=118468
2022-02-22 15:43:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 15:49:15 | INFO | train_inner | epoch 029:    274 / 788 loss=4.603, ppl=24.29, wps=12446, ups=0.19, wpb=65534.7, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.692, loss_scale=8, train_wall=504, gb_free=3.7, wall=118995
2022-02-22 15:57:57 | INFO | train_inner | epoch 029:    374 / 788 loss=4.623, ppl=24.64, wps=12565.7, ups=0.19, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.677, loss_scale=16, train_wall=499, gb_free=3.7, wall=119516
2022-02-22 16:06:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 16:06:43 | INFO | train_inner | epoch 029:    475 / 788 loss=4.634, ppl=24.83, wps=12441.6, ups=0.19, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.672, loss_scale=16, train_wall=504, gb_free=3.7, wall=120043
2022-02-22 16:11:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 16:15:30 | INFO | train_inner | epoch 029:    576 / 788 loss=4.659, ppl=25.26, wps=12435.9, ups=0.19, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.682, loss_scale=8, train_wall=504, gb_free=3.7, wall=120570
2022-02-22 16:24:12 | INFO | train_inner | epoch 029:    676 / 788 loss=4.671, ppl=25.48, wps=12568.3, ups=0.19, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.695, loss_scale=16, train_wall=499, gb_free=3.7, wall=121092
2022-02-22 16:32:53 | INFO | train_inner | epoch 029:    776 / 788 loss=4.703, ppl=26.05, wps=12570.5, ups=0.19, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.671, loss_scale=16, train_wall=499, gb_free=3.7, wall=121613
2022-02-22 16:33:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 16:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 16:34:00 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.559 | ppl 47.16 | wps 30828.5 | wpb 510.9 | bsz 1 | num_updates 22711 | best_loss 5.511
2022-02-22 16:34:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22711 updates
2022-02-22 16:34:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 16:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 16:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 29 @ 22711 updates, score 5.559) (writing took 5.842831303365529 seconds)
2022-02-22 16:34:06 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-22 16:34:06 | INFO | train | epoch 029 | loss 4.632 | ppl 24.79 | wps 12447 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 22711 | lr 0.000209837 | gnorm 0.679 | loss_scale 8 | train_wall 3931 | gb_free 3.7 | wall 121686
2022-02-22 16:34:06 | INFO | fairseq.trainer | begin training epoch 30
2022-02-22 16:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 16:41:50 | INFO | train_inner | epoch 030:     89 / 788 loss=4.557, ppl=23.53, wps=12144.1, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.679, loss_scale=8, train_wall=502, gb_free=3.7, wall=122150
2022-02-22 16:44:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 16:50:37 | INFO | train_inner | epoch 030:    190 / 788 loss=4.572, ppl=23.79, wps=12444.5, ups=0.19, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.689, loss_scale=8, train_wall=504, gb_free=3.7, wall=122677
2022-02-22 16:59:18 | INFO | train_inner | epoch 030:    290 / 788 loss=4.585, ppl=24, wps=12568.9, ups=0.19, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.686, loss_scale=16, train_wall=499, gb_free=3.7, wall=123198
2022-02-22 17:00:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 17:08:05 | INFO | train_inner | epoch 030:    391 / 788 loss=4.604, ppl=24.32, wps=12442.5, ups=0.19, wpb=65534.7, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.685, loss_scale=8, train_wall=504, gb_free=3.7, wall=123725
2022-02-22 17:16:47 | INFO | train_inner | epoch 030:    491 / 788 loss=4.615, ppl=24.51, wps=12568.4, ups=0.19, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.678, loss_scale=16, train_wall=499, gb_free=3.7, wall=124246
2022-02-22 17:18:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 17:25:33 | INFO | train_inner | epoch 030:    592 / 788 loss=4.654, ppl=25.18, wps=12447.8, ups=0.19, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.673, loss_scale=8, train_wall=504, gb_free=3.7, wall=124773
2022-02-22 17:34:15 | INFO | train_inner | epoch 030:    692 / 788 loss=4.664, ppl=25.36, wps=12567.3, ups=0.19, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.687, loss_scale=16, train_wall=499, gb_free=3.7, wall=125294
2022-02-22 17:40:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 17:42:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 17:42:40 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.555 | ppl 47.02 | wps 30781.8 | wpb 510.9 | bsz 1 | num_updates 23495 | best_loss 5.511
2022-02-22 17:42:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23495 updates
2022-02-22 17:42:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 17:42:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 17:42:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 30 @ 23495 updates, score 5.555) (writing took 5.876862915232778 seconds)
2022-02-22 17:42:46 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-22 17:42:46 | INFO | train | epoch 030 | loss 4.613 | ppl 24.48 | wps 12465.4 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 23495 | lr 0.000206306 | gnorm 0.683 | loss_scale 16 | train_wall 3930 | gb_free 3.7 | wall 125805
2022-02-22 17:42:46 | INFO | fairseq.trainer | begin training epoch 31
2022-02-22 17:42:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 17:43:12 | INFO | train_inner | epoch 031:      5 / 788 loss=4.664, ppl=25.35, wps=12139.5, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=23500, lr=0.000206284, gnorm=0.688, loss_scale=16, train_wall=502, gb_free=3.7, wall=125832
2022-02-22 17:48:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 17:51:59 | INFO | train_inner | epoch 031:    106 / 788 loss=4.508, ppl=22.76, wps=12444.6, ups=0.19, wpb=65534.7, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.671, loss_scale=8, train_wall=504, gb_free=3.7, wall=126358
2022-02-22 18:00:40 | INFO | train_inner | epoch 031:    206 / 788 loss=4.557, ppl=23.53, wps=12572.3, ups=0.19, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.698, loss_scale=16, train_wall=499, gb_free=3.7, wall=126880
2022-02-22 18:09:22 | INFO | train_inner | epoch 031:    306 / 788 loss=4.572, ppl=23.78, wps=12563.5, ups=0.19, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.692, loss_scale=16, train_wall=499, gb_free=3.7, wall=127401
2022-02-22 18:11:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 18:12:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 18:18:13 | INFO | train_inner | epoch 031:    408 / 788 loss=4.593, ppl=24.14, wps=12331.8, ups=0.19, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.706, loss_scale=8, train_wall=509, gb_free=3.7, wall=127933
2022-02-22 18:26:54 | INFO | train_inner | epoch 031:    508 / 788 loss=4.62, ppl=24.58, wps=12578.1, ups=0.19, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.693, loss_scale=16, train_wall=499, gb_free=3.7, wall=128454
2022-02-22 18:28:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 18:35:40 | INFO | train_inner | epoch 031:    609 / 788 loss=4.634, ppl=24.83, wps=12449.9, ups=0.19, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.713, loss_scale=8, train_wall=504, gb_free=3.7, wall=128980
2022-02-22 18:42:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 18:44:27 | INFO | train_inner | epoch 031:    710 / 788 loss=4.648, ppl=25.07, wps=12450.5, ups=0.19, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.705, loss_scale=8, train_wall=504, gb_free=3.7, wall=129506
2022-02-22 18:51:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 18:51:18 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.559 | ppl 47.16 | wps 30861.3 | wpb 510.9 | bsz 1 | num_updates 24278 | best_loss 5.511
2022-02-22 18:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24278 updates
2022-02-22 18:51:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 18:51:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 18:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 31 @ 24278 updates, score 5.559) (writing took 5.834476042538881 seconds)
2022-02-22 18:51:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-22 18:51:24 | INFO | train | epoch 031 | loss 4.596 | ppl 24.19 | wps 12452.8 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 24278 | lr 0.000202952 | gnorm 0.698 | loss_scale 8 | train_wall 3930 | gb_free 3.7 | wall 129924
2022-02-22 18:51:24 | INFO | fairseq.trainer | begin training epoch 32
2022-02-22 18:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 18:53:19 | INFO | train_inner | epoch 032:     22 / 788 loss=4.62, ppl=24.59, wps=12259, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=24300, lr=0.00020286, gnorm=0.701, loss_scale=8, train_wall=497, gb_free=3.7, wall=130039
2022-02-22 18:54:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 19:02:05 | INFO | train_inner | epoch 032:    123 / 788 loss=4.512, ppl=22.81, wps=12450, ups=0.19, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.693, loss_scale=8, train_wall=504, gb_free=3.7, wall=130565
2022-02-22 19:05:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 19:10:52 | INFO | train_inner | epoch 032:    224 / 788 loss=4.526, ppl=23.03, wps=12452.7, ups=0.19, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.705, loss_scale=8, train_wall=504, gb_free=3.7, wall=131091
2022-02-22 19:19:33 | INFO | train_inner | epoch 032:    324 / 788 loss=4.563, ppl=23.64, wps=12576.3, ups=0.19, wpb=65534.7, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.697, loss_scale=16, train_wall=499, gb_free=3.7, wall=131612
2022-02-22 19:20:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 19:28:19 | INFO | train_inner | epoch 032:    425 / 788 loss=4.576, ppl=23.85, wps=12453.3, ups=0.19, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.711, loss_scale=8, train_wall=504, gb_free=3.7, wall=132139
2022-02-22 19:37:00 | INFO | train_inner | epoch 032:    525 / 788 loss=4.599, ppl=24.24, wps=12573.5, ups=0.19, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.686, loss_scale=16, train_wall=499, gb_free=3.7, wall=132660
2022-02-22 19:38:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 19:45:47 | INFO | train_inner | epoch 032:    626 / 788 loss=4.621, ppl=24.61, wps=12450.4, ups=0.19, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.7, loss_scale=8, train_wall=504, gb_free=3.7, wall=133186
2022-02-22 19:54:28 | INFO | train_inner | epoch 032:    726 / 788 loss=4.641, ppl=24.95, wps=12571.2, ups=0.19, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.692, loss_scale=16, train_wall=499, gb_free=3.7, wall=133708
2022-02-22 19:55:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 19:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 19:59:56 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.587 | ppl 48.06 | wps 30856 | wpb 510.9 | bsz 1 | num_updates 25061 | best_loss 5.511
2022-02-22 19:59:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25061 updates
2022-02-22 19:59:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 20:00:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 20:00:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 32 @ 25061 updates, score 5.587) (writing took 5.860353937372565 seconds)
2022-02-22 20:00:02 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-22 20:00:02 | INFO | train | epoch 032 | loss 4.58 | ppl 23.92 | wps 12455.1 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 25061 | lr 0.000199756 | gnorm 0.697 | loss_scale 8 | train_wall 3929 | gb_free 3.7 | wall 134041
2022-02-22 20:00:02 | INFO | fairseq.trainer | begin training epoch 33
2022-02-22 20:00:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 20:03:25 | INFO | train_inner | epoch 033:     39 / 788 loss=4.58, ppl=23.91, wps=12148.7, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=25100, lr=0.000199601, gnorm=0.705, loss_scale=8, train_wall=501, gb_free=3.7, wall=134245
2022-02-22 20:12:06 | INFO | train_inner | epoch 033:    139 / 788 loss=4.506, ppl=22.72, wps=12567.8, ups=0.19, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.685, loss_scale=16, train_wall=499, gb_free=3.7, wall=134766
2022-02-22 20:17:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 20:20:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 20:20:58 | INFO | train_inner | epoch 033:    241 / 788 loss=4.522, ppl=22.97, wps=12331.8, ups=0.19, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.703, loss_scale=8, train_wall=509, gb_free=3.7, wall=135297
2022-02-22 20:29:39 | INFO | train_inner | epoch 033:    341 / 788 loss=4.544, ppl=23.34, wps=12580, ups=0.19, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.693, loss_scale=8, train_wall=499, gb_free=3.7, wall=135818
2022-02-22 20:38:20 | INFO | train_inner | epoch 033:    441 / 788 loss=4.567, ppl=23.7, wps=12576.7, ups=0.19, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.728, loss_scale=16, train_wall=499, gb_free=3.7, wall=136340
2022-02-22 20:39:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 20:47:06 | INFO | train_inner | epoch 033:    542 / 788 loss=4.595, ppl=24.17, wps=12456.6, ups=0.19, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.718, loss_scale=8, train_wall=504, gb_free=3.7, wall=136866
2022-02-22 20:55:47 | INFO | train_inner | epoch 033:    642 / 788 loss=4.607, ppl=24.38, wps=12574.6, ups=0.19, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.704, loss_scale=16, train_wall=499, gb_free=3.7, wall=137387
2022-02-22 21:02:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 21:04:34 | INFO | train_inner | epoch 033:    743 / 788 loss=4.62, ppl=24.59, wps=12449.2, ups=0.19, wpb=65534.7, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.708, loss_scale=16, train_wall=504, gb_free=3.7, wall=137913
2022-02-22 21:08:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 21:08:33 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.58 | ppl 47.83 | wps 30844.2 | wpb 510.9 | bsz 1 | num_updates 25845 | best_loss 5.511
2022-02-22 21:08:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25845 updates
2022-02-22 21:08:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 21:08:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 21:08:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 33 @ 25845 updates, score 5.58) (writing took 5.889378556981683 seconds)
2022-02-22 21:08:39 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-22 21:08:39 | INFO | train | epoch 033 | loss 4.564 | ppl 23.66 | wps 12472.2 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 25845 | lr 0.000196703 | gnorm 0.708 | loss_scale 16 | train_wall 3929 | gb_free 3.7 | wall 138158
2022-02-22 21:08:39 | INFO | fairseq.trainer | begin training epoch 34
2022-02-22 21:08:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 21:13:26 | INFO | train_inner | epoch 034:     55 / 788 loss=4.533, ppl=23.15, wps=12260.5, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.713, loss_scale=32, train_wall=497, gb_free=3.7, wall=138445
2022-02-22 21:13:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 21:22:12 | INFO | train_inner | epoch 034:    156 / 788 loss=4.479, ppl=22.3, wps=12447.5, ups=0.19, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.699, loss_scale=16, train_wall=504, gb_free=3.7, wall=138972
2022-02-22 21:24:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 21:30:59 | INFO | train_inner | epoch 034:    257 / 788 loss=4.513, ppl=22.83, wps=12445.8, ups=0.19, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.711, loss_scale=16, train_wall=504, gb_free=3.7, wall=139498
2022-02-22 21:35:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 21:39:45 | INFO | train_inner | epoch 034:    358 / 788 loss=4.536, ppl=23.19, wps=12450.5, ups=0.19, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.705, loss_scale=16, train_wall=504, gb_free=3.7, wall=140025
2022-02-22 21:47:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 21:48:31 | INFO | train_inner | epoch 034:    459 / 788 loss=4.567, ppl=23.7, wps=12449, ups=0.19, wpb=65534.7, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.702, loss_scale=16, train_wall=504, gb_free=3.7, wall=140551
2022-02-22 21:49:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 21:57:18 | INFO | train_inner | epoch 034:    560 / 788 loss=4.577, ppl=23.87, wps=12449, ups=0.19, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.719, loss_scale=8, train_wall=504, gb_free=3.7, wall=141078
2022-02-22 22:05:59 | INFO | train_inner | epoch 034:    660 / 788 loss=4.591, ppl=24.1, wps=12575.9, ups=0.19, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.73, loss_scale=16, train_wall=499, gb_free=3.7, wall=141599
2022-02-22 22:11:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 22:14:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 22:14:51 | INFO | train_inner | epoch 034:    762 / 788 loss=4.609, ppl=24.4, wps=12326.5, ups=0.19, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.711, loss_scale=8, train_wall=509, gb_free=3.7, wall=142130
2022-02-22 22:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 22:17:11 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.586 | ppl 48.03 | wps 30797.3 | wpb 510.9 | bsz 1 | num_updates 26626 | best_loss 5.511
2022-02-22 22:17:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 26626 updates
2022-02-22 22:17:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 22:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 22:17:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 34 @ 26626 updates, score 5.586) (writing took 5.8819604981690645 seconds)
2022-02-22 22:17:17 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-22 22:17:17 | INFO | train | epoch 034 | loss 4.549 | ppl 23.41 | wps 12421.5 | ups 0.19 | wpb 65497.2 | bsz 127.9 | num_updates 26626 | lr 0.000193797 | gnorm 0.71 | loss_scale 8 | train_wall 3930 | gb_free 3.7 | wall 142277
2022-02-22 22:17:17 | INFO | fairseq.trainer | begin training epoch 35
2022-02-22 22:17:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 22:23:42 | INFO | train_inner | epoch 035:     74 / 788 loss=4.495, ppl=22.55, wps=12267, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.71, loss_scale=8, train_wall=496, gb_free=3.7, wall=142662
2022-02-22 22:32:24 | INFO | train_inner | epoch 035:    174 / 788 loss=4.481, ppl=22.33, wps=12571.7, ups=0.19, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.705, loss_scale=16, train_wall=499, gb_free=3.7, wall=143184
2022-02-22 22:36:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 22:41:10 | INFO | train_inner | epoch 035:    275 / 788 loss=4.504, ppl=22.68, wps=12449.7, ups=0.19, wpb=65534.7, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.706, loss_scale=16, train_wall=504, gb_free=3.7, wall=143710
2022-02-22 22:48:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 22:49:56 | INFO | train_inner | epoch 035:    376 / 788 loss=4.523, ppl=22.99, wps=12454.2, ups=0.19, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.708, loss_scale=16, train_wall=504, gb_free=3.7, wall=144236
2022-02-22 22:58:38 | INFO | train_inner | epoch 035:    476 / 788 loss=4.551, ppl=23.44, wps=12573, ups=0.19, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.722, loss_scale=16, train_wall=499, gb_free=3.7, wall=144757
2022-02-22 22:59:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 23:07:24 | INFO | train_inner | epoch 035:    577 / 788 loss=4.566, ppl=23.69, wps=12449.6, ups=0.19, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.717, loss_scale=16, train_wall=504, gb_free=3.7, wall=145284
2022-02-22 23:11:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 23:11:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-22 23:16:16 | INFO | train_inner | epoch 035:    679 / 788 loss=4.584, ppl=23.99, wps=12322.9, ups=0.19, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.715, loss_scale=8, train_wall=509, gb_free=3.7, wall=145816
2022-02-22 23:24:57 | INFO | train_inner | epoch 035:    779 / 788 loss=4.598, ppl=24.21, wps=12569.5, ups=0.19, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.71, loss_scale=16, train_wall=499, gb_free=3.7, wall=146337
2022-02-22 23:25:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-22 23:25:49 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.598 | ppl 48.42 | wps 30789.7 | wpb 510.9 | bsz 1 | num_updates 27409 | best_loss 5.511
2022-02-22 23:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 27409 updates
2022-02-22 23:25:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 23:25:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-22 23:25:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 35 @ 27409 updates, score 5.598) (writing took 5.87026035040617 seconds)
2022-02-22 23:25:55 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-22 23:25:55 | INFO | train | epoch 035 | loss 4.535 | ppl 23.18 | wps 12453.5 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 27409 | lr 0.000191009 | gnorm 0.711 | loss_scale 16 | train_wall 3929 | gb_free 3.7 | wall 146395
2022-02-22 23:25:55 | INFO | fairseq.trainer | begin training epoch 36
2022-02-22 23:25:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-22 23:33:49 | INFO | train_inner | epoch 036:     91 / 788 loss=4.454, ppl=21.91, wps=12259.7, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.709, loss_scale=16, train_wall=497, gb_free=3.7, wall=146869
2022-02-22 23:34:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 23:42:36 | INFO | train_inner | epoch 036:    192 / 788 loss=4.465, ppl=22.08, wps=12439.5, ups=0.19, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.711, loss_scale=16, train_wall=504, gb_free=3.7, wall=147396
2022-02-22 23:45:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-22 23:51:23 | INFO | train_inner | epoch 036:    293 / 788 loss=4.487, ppl=22.42, wps=12445.3, ups=0.19, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.726, loss_scale=16, train_wall=504, gb_free=3.7, wall=147923
2022-02-22 23:57:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 00:00:09 | INFO | train_inner | epoch 036:    394 / 788 loss=4.516, ppl=22.88, wps=12453.7, ups=0.19, wpb=65534.7, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.719, loss_scale=16, train_wall=504, gb_free=3.7, wall=148449
2022-02-23 00:08:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 00:08:55 | INFO | train_inner | epoch 036:    495 / 788 loss=4.54, ppl=23.27, wps=12450.7, ups=0.19, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.718, loss_scale=16, train_wall=504, gb_free=3.7, wall=148975
2022-02-23 00:09:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-23 00:17:42 | INFO | train_inner | epoch 036:    596 / 788 loss=4.559, ppl=23.56, wps=12453.5, ups=0.19, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.718, loss_scale=8, train_wall=504, gb_free=3.7, wall=149501
2022-02-23 00:26:23 | INFO | train_inner | epoch 036:    696 / 788 loss=4.569, ppl=23.74, wps=12569.7, ups=0.19, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.721, loss_scale=16, train_wall=499, gb_free=3.7, wall=150023
2022-02-23 00:32:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 00:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-23 00:34:27 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.604 | ppl 48.63 | wps 30843.1 | wpb 510.9 | bsz 1 | num_updates 28191 | best_loss 5.511
2022-02-23 00:34:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 28191 updates
2022-02-23 00:34:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 00:34:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 00:34:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 36 @ 28191 updates, score 5.604) (writing took 5.9974145879969 seconds)
2022-02-23 00:34:33 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-23 00:34:33 | INFO | train | epoch 036 | loss 4.521 | ppl 22.96 | wps 12436 | ups 0.19 | wpb 65497.2 | bsz 127.9 | num_updates 28191 | lr 0.000188341 | gnorm 0.719 | loss_scale 16 | train_wall 3930 | gb_free 3.7 | wall 150513
2022-02-23 00:34:34 | INFO | fairseq.trainer | begin training epoch 37
2022-02-23 00:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-23 00:35:20 | INFO | train_inner | epoch 037:      9 / 788 loss=4.578, ppl=23.88, wps=12138.6, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=28200, lr=0.000188311, gnorm=0.734, loss_scale=16, train_wall=502, gb_free=3.7, wall=150560
2022-02-23 00:43:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 00:44:07 | INFO | train_inner | epoch 037:    110 / 788 loss=4.432, ppl=21.59, wps=12448.8, ups=0.19, wpb=65536, bsz=128, num_updates=28300, lr=0.000187978, gnorm=0.72, loss_scale=16, train_wall=504, gb_free=3.7, wall=151087
2022-02-23 00:52:48 | INFO | train_inner | epoch 037:    210 / 788 loss=4.457, ppl=21.97, wps=12568.6, ups=0.19, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.73, loss_scale=16, train_wall=499, gb_free=3.7, wall=151608
2022-02-23 00:55:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 01:01:35 | INFO | train_inner | epoch 037:    311 / 788 loss=4.477, ppl=22.26, wps=12445.1, ups=0.19, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.713, loss_scale=16, train_wall=504, gb_free=3.7, wall=152135
2022-02-23 01:06:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 01:10:22 | INFO | train_inner | epoch 037:    412 / 788 loss=4.504, ppl=22.69, wps=12446.6, ups=0.19, wpb=65534.7, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.724, loss_scale=16, train_wall=504, gb_free=3.7, wall=152661
2022-02-23 01:17:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 01:19:08 | INFO | train_inner | epoch 037:    513 / 788 loss=4.524, ppl=23, wps=12445.4, ups=0.19, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.732, loss_scale=16, train_wall=504, gb_free=3.7, wall=153188
2022-02-23 01:27:49 | INFO | train_inner | epoch 037:    613 / 788 loss=4.551, ppl=23.43, wps=12574.3, ups=0.19, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.727, loss_scale=16, train_wall=499, gb_free=3.7, wall=153709
2022-02-23 01:29:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 01:36:36 | INFO | train_inner | epoch 037:    714 / 788 loss=4.563, ppl=23.64, wps=12449.1, ups=0.19, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.736, loss_scale=16, train_wall=504, gb_free=3.7, wall=154235
2022-02-23 01:40:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 01:42:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-23 01:43:06 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.612 | ppl 48.92 | wps 30879.6 | wpb 510.9 | bsz 1 | num_updates 28973 | best_loss 5.511
2022-02-23 01:43:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 28973 updates
2022-02-23 01:43:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 01:43:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 01:43:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 37 @ 28973 updates, score 5.612) (writing took 5.862286979332566 seconds)
2022-02-23 01:43:12 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-23 01:43:12 | INFO | train | epoch 037 | loss 4.508 | ppl 22.75 | wps 12435.5 | ups 0.19 | wpb 65497.2 | bsz 127.9 | num_updates 28973 | lr 0.000185782 | gnorm 0.726 | loss_scale 16 | train_wall 3930 | gb_free 3.7 | wall 154632
2022-02-23 01:43:12 | INFO | fairseq.trainer | begin training epoch 38
2022-02-23 01:43:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-23 01:45:33 | INFO | train_inner | epoch 038:     27 / 788 loss=4.535, ppl=23.19, wps=12140.6, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=29000, lr=0.000185695, gnorm=0.719, loss_scale=16, train_wall=502, gb_free=3.7, wall=154773
2022-02-23 01:52:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 01:54:20 | INFO | train_inner | epoch 038:    128 / 788 loss=4.417, ppl=21.37, wps=12437.9, ups=0.19, wpb=65534.7, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.732, loss_scale=16, train_wall=504, gb_free=3.7, wall=155300
2022-02-23 01:57:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-23 02:03:06 | INFO | train_inner | epoch 038:    229 / 788 loss=4.444, ppl=21.77, wps=12447, ups=0.19, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.73, loss_scale=8, train_wall=504, gb_free=3.7, wall=155826
2022-02-23 02:11:48 | INFO | train_inner | epoch 038:    329 / 788 loss=4.475, ppl=22.24, wps=12573.9, ups=0.19, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.734, loss_scale=16, train_wall=499, gb_free=3.7, wall=156347
2022-02-23 02:19:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-23 02:20:34 | INFO | train_inner | epoch 038:    430 / 788 loss=4.497, ppl=22.58, wps=12448.3, ups=0.19, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.734, loss_scale=8, train_wall=504, gb_free=3.7, wall=156874
2022-02-23 02:29:15 | INFO | train_inner | epoch 038:    530 / 788 loss=4.514, ppl=22.85, wps=12579.5, ups=0.19, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.741, loss_scale=8, train_wall=499, gb_free=3.7, wall=157395
2022-02-23 02:37:56 | INFO | train_inner | epoch 038:    630 / 788 loss=4.538, ppl=23.24, wps=12573.4, ups=0.19, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.743, loss_scale=16, train_wall=499, gb_free=3.7, wall=157916
2022-02-23 02:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 02:46:43 | INFO | train_inner | epoch 038:    731 / 788 loss=4.558, ppl=23.55, wps=12448.3, ups=0.19, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.736, loss_scale=16, train_wall=504, gb_free=3.7, wall=158443
2022-02-23 02:51:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-23 02:51:44 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.617 | ppl 49.08 | wps 30712.8 | wpb 510.9 | bsz 1 | num_updates 29757 | best_loss 5.511
2022-02-23 02:51:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 29757 updates
2022-02-23 02:51:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 02:51:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 02:51:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 38 @ 29757 updates, score 5.617) (writing took 5.903074198402464 seconds)
2022-02-23 02:51:50 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-23 02:51:50 | INFO | train | epoch 038 | loss 4.495 | ppl 22.55 | wps 12469.1 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 29757 | lr 0.000183318 | gnorm 0.734 | loss_scale 16 | train_wall 3930 | gb_free 3.7 | wall 158750
2022-02-23 02:51:51 | INFO | fairseq.trainer | begin training epoch 39
2022-02-23 02:51:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-23 02:53:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 02:55:40 | INFO | train_inner | epoch 039:     44 / 788 loss=4.498, ppl=22.59, wps=12145.3, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=29800, lr=0.000183186, gnorm=0.718, loss_scale=16, train_wall=502, gb_free=3.7, wall=158980
2022-02-23 03:04:21 | INFO | train_inner | epoch 039:    144 / 788 loss=4.422, ppl=21.44, wps=12572.1, ups=0.19, wpb=65536, bsz=128, num_updates=29900, lr=0.000182879, gnorm=0.728, loss_scale=16, train_wall=499, gb_free=3.7, wall=159501
2022-02-23 03:04:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 03:13:08 | INFO | train_inner | epoch 039:    245 / 788 loss=4.444, ppl=21.76, wps=12449.5, ups=0.19, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.732, loss_scale=16, train_wall=504, gb_free=3.7, wall=160027
2022-02-23 03:16:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 03:21:54 | INFO | train_inner | epoch 039:    346 / 788 loss=4.46, ppl=22, wps=12448.3, ups=0.19, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.735, loss_scale=16, train_wall=504, gb_free=3.7, wall=160554
2022-02-23 03:27:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 03:30:41 | INFO | train_inner | epoch 039:    447 / 788 loss=4.493, ppl=22.52, wps=12447.9, ups=0.19, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.726, loss_scale=16, train_wall=504, gb_free=3.7, wall=161080
2022-02-23 03:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 03:39:27 | INFO | train_inner | epoch 039:    548 / 788 loss=4.507, ppl=22.74, wps=12446.6, ups=0.19, wpb=65536, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.738, loss_scale=16, train_wall=504, gb_free=3.7, wall=161607
2022-02-23 03:48:08 | INFO | train_inner | epoch 039:    648 / 788 loss=4.521, ppl=22.96, wps=12572.7, ups=0.19, wpb=65534.7, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.734, loss_scale=16, train_wall=499, gb_free=3.7, wall=162128
2022-02-23 03:49:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-23 03:56:55 | INFO | train_inner | epoch 039:    749 / 788 loss=4.54, ppl=23.26, wps=12453.5, ups=0.19, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.735, loss_scale=8, train_wall=504, gb_free=3.7, wall=162654
2022-02-23 04:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-23 04:00:23 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.614 | ppl 48.97 | wps 30875.4 | wpb 510.9 | bsz 1 | num_updates 30539 | best_loss 5.511
2022-02-23 04:00:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 30539 updates
2022-02-23 04:00:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 04:00:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 04:00:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 39 @ 30539 updates, score 5.614) (writing took 5.893950432538986 seconds)
2022-02-23 04:00:28 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-23 04:00:28 | INFO | train | epoch 039 | loss 4.482 | ppl 22.35 | wps 12437.6 | ups 0.19 | wpb 65497.2 | bsz 127.9 | num_updates 30539 | lr 0.000180956 | gnorm 0.731 | loss_scale 8 | train_wall 3930 | gb_free 3.7 | wall 162868
2022-02-23 04:00:29 | INFO | fairseq.trainer | begin training epoch 40
2022-02-23 04:00:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-23 04:05:47 | INFO | train_inner | epoch 040:     61 / 788 loss=4.444, ppl=21.77, wps=12263.7, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=30600, lr=0.000180775, gnorm=0.728, loss_scale=16, train_wall=497, gb_free=3.7, wall=163186
2022-02-23 04:12:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 04:14:33 | INFO | train_inner | epoch 040:    162 / 788 loss=4.411, ppl=21.27, wps=12451.6, ups=0.19, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.75, loss_scale=16, train_wall=504, gb_free=3.7, wall=163713
2022-02-23 04:23:14 | INFO | train_inner | epoch 040:    262 / 788 loss=4.436, ppl=21.65, wps=12573.8, ups=0.19, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.732, loss_scale=16, train_wall=499, gb_free=3.7, wall=164234
2022-02-23 04:23:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 04:32:00 | INFO | train_inner | epoch 040:    363 / 788 loss=4.458, ppl=21.97, wps=12452.8, ups=0.19, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.73, loss_scale=16, train_wall=504, gb_free=3.7, wall=164760
2022-02-23 04:34:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 04:40:47 | INFO | train_inner | epoch 040:    464 / 788 loss=4.484, ppl=22.38, wps=12454, ups=0.19, wpb=65536, bsz=128, num_updates=31000, lr=0.000179605, gnorm=0.743, loss_scale=16, train_wall=504, gb_free=3.7, wall=165286
2022-02-23 04:46:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 04:49:33 | INFO | train_inner | epoch 040:    565 / 788 loss=4.503, ppl=22.68, wps=12452.2, ups=0.19, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.751, loss_scale=16, train_wall=504, gb_free=3.7, wall=165813
2022-02-23 04:57:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 04:58:19 | INFO | train_inner | epoch 040:    666 / 788 loss=4.515, ppl=22.86, wps=12447.7, ups=0.19, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.743, loss_scale=16, train_wall=504, gb_free=3.7, wall=166339
2022-02-23 05:07:00 | INFO | train_inner | epoch 040:    766 / 788 loss=4.527, ppl=23.06, wps=12577, ups=0.19, wpb=65534.7, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.737, loss_scale=16, train_wall=499, gb_free=3.7, wall=166860
2022-02-23 05:08:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-23 05:09:00 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.627 | ppl 49.42 | wps 30869.3 | wpb 510.9 | bsz 1 | num_updates 31322 | best_loss 5.511
2022-02-23 05:09:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 31322 updates
2022-02-23 05:09:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 05:09:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 05:09:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 40 @ 31322 updates, score 5.627) (writing took 6.124416291713715 seconds)
2022-02-23 05:09:06 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-23 05:09:06 | INFO | train | epoch 040 | loss 4.471 | ppl 22.17 | wps 12454.9 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 31322 | lr 0.00017868 | gnorm 0.74 | loss_scale 32 | train_wall 3930 | gb_free 3.7 | wall 166986
2022-02-23 05:09:06 | INFO | fairseq.trainer | begin training epoch 41
2022-02-23 05:09:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-23 05:09:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 05:15:58 | INFO | train_inner | epoch 041:     79 / 788 loss=4.413, ppl=21.3, wps=12137.3, ups=0.19, wpb=65233.9, bsz=127.4, num_updates=31400, lr=0.000178458, gnorm=0.747, loss_scale=16, train_wall=502, gb_free=3.7, wall=167398
2022-02-23 05:20:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 05:24:44 | INFO | train_inner | epoch 041:    180 / 788 loss=4.408, ppl=21.22, wps=12450.2, ups=0.19, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=0.75, loss_scale=16, train_wall=504, gb_free=3.7, wall=167924
2022-02-23 05:27:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-23 05:33:30 | INFO | train_inner | epoch 041:    281 / 788 loss=4.43, ppl=21.56, wps=12457.1, ups=0.19, wpb=65534.7, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.744, loss_scale=8, train_wall=504, gb_free=3.7, wall=168450
2022-02-23 05:42:11 | INFO | train_inner | epoch 041:    381 / 788 loss=4.452, ppl=21.88, wps=12578.4, ups=0.19, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.745, loss_scale=16, train_wall=499, gb_free=3.7, wall=168971
2022-02-23 05:50:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 05:50:58 | INFO | train_inner | epoch 041:    482 / 788 loss=4.474, ppl=22.22, wps=12456.5, ups=0.19, wpb=65536, bsz=128, num_updates=31800, lr=0.000177332, gnorm=0.745, loss_scale=16, train_wall=504, gb_free=3.7, wall=169497
2022-02-23 05:58:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-23 05:59:44 | INFO | train_inner | epoch 041:    583 / 788 loss=4.486, ppl=22.41, wps=12449.8, ups=0.19, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.741, loss_scale=8, train_wall=504, gb_free=3.7, wall=170024
2022-02-23 06:08:25 | INFO | train_inner | epoch 041:    683 / 788 loss=4.503, ppl=22.67, wps=12579.2, ups=0.19, wpb=65536, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.759, loss_scale=8, train_wall=499, gb_free=3.7, wall=170545
2022-02-23 06:17:06 | INFO | train_inner | epoch 041:    783 / 788 loss=4.521, ppl=22.96, wps=12579.5, ups=0.19, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.741, loss_scale=16, train_wall=499, gb_free=3.7, wall=171066
2022-02-23 06:17:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-23 06:17:37 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.622 | ppl 49.24 | wps 30784.4 | wpb 510.9 | bsz 1 | num_updates 32105 | best_loss 5.511
2022-02-23 06:17:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 32105 updates
2022-02-23 06:17:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 06:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt
2022-02-23 06:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 41 @ 32105 updates, score 5.622) (writing took 6.183634704910219 seconds)
2022-02-23 06:17:43 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-23 06:17:43 | INFO | train | epoch 041 | loss 4.459 | ppl 21.99 | wps 12457.3 | ups 0.19 | wpb 65497.3 | bsz 127.9 | num_updates 32105 | lr 0.000176487 | gnorm 0.747 | loss_scale 16 | train_wall 3929 | gb_free 3.7 | wall 171103
2022-02-23 06:17:43 | INFO | fairseq.trainer | begin training epoch 42
2022-02-23 06:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-23 06:20:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-23 06:21:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-23 06:26:08 | INFO | train_inner | epoch 042:     97 / 788 loss=4.375, ppl=20.76, wps=12027.5, ups=0.18, wpb=65232.6, bsz=127.4, num_updates=32200, lr=0.000176227, gnorm=0.753, loss_scale=8, train_wall=506, gb_free=3.7, wall=171608
2022-02-23 06:34:50 | INFO | train_inner | epoch 042:    197 / 788 loss=4.402, ppl=21.14, wps=12571, ups=0.19, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.755, loss_scale=16, train_wall=499, gb_free=3.7, wall=172129
2022-02-23 06:43:31 | INFO | train_inner | epoch 042:    297 / 788 loss=4.427, ppl=21.51, wps=12579.1, ups=0.19, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.746, loss_scale=16, train_wall=499, gb_free=3.7, wall=172650
2022-02-23 06:43:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
User defined signal 2
