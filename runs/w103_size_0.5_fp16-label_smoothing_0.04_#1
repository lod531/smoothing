Sender: LSF System <lsfadmin@eu-g3-032>
Subject: Job 205633400: <w103_size_0.5_fp16_label_smoothing_0.04_#1> in cluster <euler> Exited

Job <w103_size_0.5_fp16_label_smoothing_0.04_#1> was submitted from host <eu-login-37> by user <andriusb> in cluster <euler> at Fri Feb 18 08:23:19 2022
Job was executed on host(s) <eu-g3-032>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Feb 18 08:23:47 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 18 08:23:47 2022
Terminated at Sun Feb 20 07:25:57 2022
Results reported at Sun Feb 20 07:25:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.04 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 1321671 --fp16 --max-update 50000
------------------------------------------------------------

Exited with exit code 134.

Resource usage summary:

    CPU time :                                   169241.30 sec.
    Max Memory :                                 13781 MB
    Average Memory :                             3664.03 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               6219.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   169329 sec.
    Turnaround time :                            169358 sec.

The output (if any) follows:

2022-02-18 08:23:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.04, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-18 08:23:55 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-18 08:24:00 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-18 08:24:00 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-18 08:24:00 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-18 08:24:00 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-18 08:24:00 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-18 08:24:00 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-18 08:24:00 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-18 08:24:03 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-18 08:24:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-18 08:24:03 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-02-18 08:24:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-18 08:24:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-18 08:24:03 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-02-18 08:24:03 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-02-18 08:24:03 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-02-18 08:24:03 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-18 08:24:04 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-18 08:24:05 | INFO | fairseq.trainer | begin training epoch 1
2022-02-18 08:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 08:24:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-18 08:24:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 08:24:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 08:24:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-18 08:34:30 | INFO | train_inner | epoch 001:    104 / 788 loss=17.447, nll_loss=17.362, ppl=168467, wps=11316.8, ups=0.17, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.248, loss_scale=8, train_wall=602, gb_free=3.3, wall=627
2022-02-18 08:37:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-18 08:44:14 | INFO | train_inner | epoch 001:    205 / 788 loss=15.134, nll_loss=14.953, ppl=31719.5, wps=11212.6, ups=0.17, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.587, loss_scale=8, train_wall=562, gb_free=3.3, wall=1212
2022-02-18 08:53:53 | INFO | train_inner | epoch 001:    305 / 788 loss=12.931, nll_loss=12.647, ppl=6415.72, wps=11328.9, ups=0.17, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.112, loss_scale=16, train_wall=556, gb_free=3.3, wall=1790
2022-02-18 09:03:32 | INFO | train_inner | epoch 001:    405 / 788 loss=11.309, nll_loss=10.921, ppl=1939.23, wps=11325, ups=0.17, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.66, loss_scale=32, train_wall=556, gb_free=3.3, wall=2369
2022-02-18 09:13:10 | INFO | train_inner | epoch 001:    505 / 788 loss=10.643, nll_loss=10.186, ppl=1164.64, wps=11323.3, ups=0.17, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.516, loss_scale=32, train_wall=557, gb_free=3.3, wall=2948
2022-02-18 09:14:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 09:22:55 | INFO | train_inner | epoch 001:    606 / 788 loss=10.293, nll_loss=9.802, ppl=892.44, wps=11213, ups=0.17, wpb=65534.7, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.56, loss_scale=32, train_wall=562, gb_free=3.3, wall=3532
2022-02-18 09:27:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 09:32:39 | INFO | train_inner | epoch 001:    707 / 788 loss=10.03, nll_loss=9.518, ppl=733.4, wps=11212.4, ups=0.17, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.648, loss_scale=32, train_wall=562, gb_free=3.3, wall=4117
2022-02-18 09:40:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 09:40:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 09:40:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.627 | nll_loss 9.091 | ppl 545.16 | wps 27897.9 | wpb 510.9 | bsz 1 | num_updates 780
2022-02-18 09:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 780 updates
2022-02-18 09:40:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint1.pt
2022-02-18 09:40:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint1.pt
2022-02-18 09:40:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint1.pt (epoch 1 @ 780 updates, score 9.627) (writing took 19.689266154542565 seconds)
2022-02-18 09:40:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-18 09:40:53 | INFO | train | epoch 001 | loss 12.263 | nll_loss 11.903 | ppl 3828.43 | wps 11198.1 | ups 0.17 | wpb 65497.1 | bsz 127.9 | num_updates 780 | lr 9.75805e-05 | gnorm 1.139 | loss_scale 32 | train_wall 4405 | gb_free 3.3 | wall 4610
2022-02-18 09:40:53 | INFO | fairseq.trainer | begin training epoch 2
2022-02-18 09:40:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 09:42:49 | INFO | train_inner | epoch 002:     20 / 788 loss=9.798, nll_loss=9.272, ppl=618.31, wps=10704, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.697, loss_scale=32, train_wall=559, gb_free=3.3, wall=4726
2022-02-18 09:52:28 | INFO | train_inner | epoch 002:    120 / 788 loss=9.573, nll_loss=9.033, ppl=523.8, wps=11322.7, ups=0.17, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.791, loss_scale=32, train_wall=556, gb_free=3.3, wall=5305
2022-02-18 09:53:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:02:12 | INFO | train_inner | epoch 002:    221 / 788 loss=9.379, nll_loss=8.827, ppl=454.2, wps=11211.4, ups=0.17, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.822, loss_scale=32, train_wall=562, gb_free=3.3, wall=5889
2022-02-18 10:06:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:11:57 | INFO | train_inner | epoch 002:    322 / 788 loss=9.205, nll_loss=8.644, ppl=400.11, wps=11212.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.91, loss_scale=32, train_wall=562, gb_free=3.3, wall=6474
2022-02-18 10:18:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:21:41 | INFO | train_inner | epoch 002:    423 / 788 loss=9.06, nll_loss=8.492, ppl=360.05, wps=11218.6, ups=0.17, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.891, loss_scale=32, train_wall=562, gb_free=3.3, wall=7058
2022-02-18 10:31:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:31:25 | INFO | train_inner | epoch 002:    524 / 788 loss=8.906, nll_loss=8.329, ppl=321.62, wps=11211.3, ups=0.17, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.879, loss_scale=32, train_wall=562, gb_free=3.3, wall=7643
2022-02-18 10:41:04 | INFO | train_inner | epoch 002:    624 / 788 loss=8.782, nll_loss=8.199, ppl=293.76, wps=11323.9, ups=0.17, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.949, loss_scale=32, train_wall=556, gb_free=3.3, wall=8221
2022-02-18 10:43:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:50:48 | INFO | train_inner | epoch 002:    725 / 788 loss=8.658, nll_loss=8.068, ppl=268.4, wps=11220.4, ups=0.17, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.922, loss_scale=32, train_wall=562, gb_free=3.3, wall=8805
2022-02-18 10:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 10:56:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.378 | nll_loss 7.77 | ppl 218.25 | wps 27637.4 | wpb 510.9 | bsz 1 | num_updates 1563 | best_loss 8.378
2022-02-18 10:56:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1563 updates
2022-02-18 10:56:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint2.pt
2022-02-18 10:57:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint2.pt
2022-02-18 10:57:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint2.pt (epoch 2 @ 1563 updates, score 8.378) (writing took 19.76632368657738 seconds)
2022-02-18 10:57:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-18 10:57:18 | INFO | train | epoch 002 | loss 9.055 | nll_loss 8.486 | ppl 358.63 | wps 11185.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 1563 | lr 0.000195436 | gnorm 0.88 | loss_scale 64 | train_wall 4381 | gb_free 3.3 | wall 9195
2022-02-18 10:57:18 | INFO | fairseq.trainer | begin training epoch 3
2022-02-18 10:57:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 10:57:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:00:58 | INFO | train_inner | epoch 003:     38 / 788 loss=8.519, nll_loss=7.921, ppl=242.36, wps=10697.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.917, loss_scale=32, train_wall=559, gb_free=3.3, wall=9415
2022-02-18 11:10:41 | INFO | train_inner | epoch 003:    138 / 788 loss=8.397, nll_loss=7.793, ppl=221.74, wps=11237.8, ups=0.17, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.881, loss_scale=64, train_wall=560, gb_free=3.3, wall=9998
2022-02-18 11:12:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:20:29 | INFO | train_inner | epoch 003:    239 / 788 loss=8.288, nll_loss=7.678, ppl=204.77, wps=11146.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.917, loss_scale=32, train_wall=565, gb_free=3.3, wall=10586
2022-02-18 11:24:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:30:13 | INFO | train_inner | epoch 003:    340 / 788 loss=8.195, nll_loss=7.58, ppl=191.3, wps=11221.6, ups=0.17, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.864, loss_scale=32, train_wall=562, gb_free=3.3, wall=11170
2022-02-18 11:37:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:39:57 | INFO | train_inner | epoch 003:    441 / 788 loss=8.112, nll_loss=7.492, ppl=180.04, wps=11217.9, ups=0.17, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.874, loss_scale=32, train_wall=562, gb_free=3.3, wall=11755
2022-02-18 11:49:36 | INFO | train_inner | epoch 003:    541 / 788 loss=8.03, nll_loss=7.405, ppl=169.52, wps=11328.4, ups=0.17, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.83, loss_scale=32, train_wall=556, gb_free=3.3, wall=12333
2022-02-18 11:49:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:59:20 | INFO | train_inner | epoch 003:    642 / 788 loss=7.943, nll_loss=7.314, ppl=159.12, wps=11215.6, ups=0.17, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.839, loss_scale=32, train_wall=562, gb_free=3.3, wall=12917
2022-02-18 12:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 12:09:04 | INFO | train_inner | epoch 003:    743 / 788 loss=7.873, nll_loss=7.24, ppl=151.16, wps=11218, ups=0.17, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.803, loss_scale=32, train_wall=562, gb_free=3.3, wall=13502
2022-02-18 12:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 12:13:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.655 | nll_loss 7.003 | ppl 128.24 | wps 27790.4 | wpb 510.9 | bsz 1 | num_updates 2345 | best_loss 7.655
2022-02-18 12:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2345 updates
2022-02-18 12:13:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint3.pt
2022-02-18 12:13:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint3.pt
2022-02-18 12:13:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint3.pt (epoch 3 @ 2345 updates, score 7.655) (writing took 19.83880739659071 seconds)
2022-02-18 12:13:50 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-18 12:13:50 | INFO | train | epoch 003 | loss 8.117 | nll_loss 7.498 | ppl 180.76 | wps 11154.5 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 2345 | lr 0.000293166 | gnorm 0.86 | loss_scale 32 | train_wall 4387 | gb_free 3.3 | wall 13787
2022-02-18 12:13:50 | INFO | fairseq.trainer | begin training epoch 4
2022-02-18 12:13:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 12:15:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 12:19:14 | INFO | train_inner | epoch 004:     56 / 788 loss=7.755, nll_loss=7.115, ppl=138.65, wps=10704, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.82, loss_scale=32, train_wall=559, gb_free=3.3, wall=14111
2022-02-18 12:28:52 | INFO | train_inner | epoch 004:    156 / 788 loss=7.662, nll_loss=7.018, ppl=129.6, wps=11330.2, ups=0.17, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.79, loss_scale=64, train_wall=556, gb_free=3.3, wall=14689
2022-02-18 12:29:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 12:38:36 | INFO | train_inner | epoch 004:    257 / 788 loss=7.598, nll_loss=6.95, ppl=123.62, wps=11215.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.786, loss_scale=32, train_wall=562, gb_free=3.3, wall=15274
2022-02-18 12:41:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 12:44:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 12:48:27 | INFO | train_inner | epoch 004:    359 / 788 loss=7.558, nll_loss=6.908, ppl=120.07, wps=11107.6, ups=0.17, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.775, loss_scale=16, train_wall=567, gb_free=3.3, wall=15864
2022-02-18 12:58:05 | INFO | train_inner | epoch 004:    459 / 788 loss=7.487, nll_loss=6.833, ppl=114.01, wps=11330.4, ups=0.17, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.752, loss_scale=32, train_wall=556, gb_free=3.3, wall=16442
2022-02-18 13:01:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 13:07:49 | INFO | train_inner | epoch 004:    560 / 788 loss=7.44, nll_loss=6.784, ppl=110.17, wps=11219.1, ups=0.17, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.762, loss_scale=16, train_wall=562, gb_free=3.3, wall=17026
2022-02-18 13:17:27 | INFO | train_inner | epoch 004:    660 / 788 loss=7.389, nll_loss=6.73, ppl=106.14, wps=11331.1, ups=0.17, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.758, loss_scale=32, train_wall=556, gb_free=3.3, wall=17605
2022-02-18 13:27:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 13:27:12 | INFO | train_inner | epoch 004:    761 / 788 loss=7.332, nll_loss=6.67, ppl=101.81, wps=11212.2, ups=0.17, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.723, loss_scale=32, train_wall=562, gb_free=3.3, wall=18189
2022-02-18 13:29:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 13:29:53 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.195 | nll_loss 6.51 | ppl 91.11 | wps 28379.9 | wpb 510.9 | bsz 1 | num_updates 3127 | best_loss 7.195
2022-02-18 13:29:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3127 updates
2022-02-18 13:29:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint4.pt
2022-02-18 13:30:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint4.pt
2022-02-18 13:30:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint4.pt (epoch 4 @ 3127 updates, score 7.195) (writing took 20.37313615065068 seconds)
2022-02-18 13:30:14 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-18 13:30:14 | INFO | train | epoch 004 | loss 7.504 | nll_loss 6.851 | ppl 115.41 | wps 11173.1 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 3127 | lr 0.000390897 | gnorm 0.765 | loss_scale 32 | train_wall 4380 | gb_free 3.3 | wall 18371
2022-02-18 13:30:14 | INFO | fairseq.trainer | begin training epoch 5
2022-02-18 13:30:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 13:37:17 | INFO | train_inner | epoch 005:     73 / 788 loss=7.211, nll_loss=6.542, ppl=93.2, wps=10783.2, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.722, loss_scale=32, train_wall=554, gb_free=3.3, wall=18794
2022-02-18 13:42:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 13:47:03 | INFO | train_inner | epoch 005:    174 / 788 loss=7.162, nll_loss=6.49, ppl=89.9, wps=11188.9, ups=0.17, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.714, loss_scale=32, train_wall=563, gb_free=3.3, wall=19380
2022-02-18 13:55:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 13:56:48 | INFO | train_inner | epoch 005:    275 / 788 loss=7.137, nll_loss=6.464, ppl=88.31, wps=11196.4, ups=0.17, wpb=65534.7, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.736, loss_scale=32, train_wall=562, gb_free=3.3, wall=19965
2022-02-18 13:56:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 14:06:34 | INFO | train_inner | epoch 005:    376 / 788 loss=7.099, nll_loss=6.424, ppl=85.85, wps=11192.2, ups=0.17, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.676, loss_scale=16, train_wall=563, gb_free=3.3, wall=20551
2022-02-18 14:14:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 14:16:19 | INFO | train_inner | epoch 005:    477 / 788 loss=7.065, nll_loss=6.388, ppl=83.74, wps=11195.8, ups=0.17, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.704, loss_scale=16, train_wall=563, gb_free=3.3, wall=21136
2022-02-18 14:26:02 | INFO | train_inner | epoch 005:    577 / 788 loss=7.043, nll_loss=6.365, ppl=82.4, wps=11236.2, ups=0.17, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.692, loss_scale=16, train_wall=560, gb_free=3.3, wall=21719
2022-02-18 14:31:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 14:35:54 | INFO | train_inner | epoch 005:    678 / 788 loss=7.014, nll_loss=6.335, ppl=80.74, wps=11079.9, ups=0.17, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.654, loss_scale=16, train_wall=568, gb_free=3.3, wall=22311
2022-02-18 14:45:37 | INFO | train_inner | epoch 005:    778 / 788 loss=6.979, nll_loss=6.298, ppl=78.7, wps=11241.7, ups=0.17, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.671, loss_scale=32, train_wall=560, gb_free=3.3, wall=22894
2022-02-18 14:46:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 14:46:40 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.892 | nll_loss 6.2 | ppl 73.5 | wps 27269.4 | wpb 510.9 | bsz 1 | num_updates 3910 | best_loss 6.892
2022-02-18 14:46:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3910 updates
2022-02-18 14:46:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint5.pt
2022-02-18 14:46:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint5.pt
2022-02-18 14:47:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint5.pt (epoch 5 @ 3910 updates, score 6.892) (writing took 20.393753508105874 seconds)
2022-02-18 14:47:00 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-18 14:47:00 | INFO | train | epoch 005 | loss 7.08 | nll_loss 6.404 | ppl 84.69 | wps 11132.8 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 3910 | lr 0.000488752 | gnorm 0.694 | loss_scale 32 | train_wall 4399 | gb_free 3.3 | wall 22978
2022-02-18 14:47:01 | INFO | fairseq.trainer | begin training epoch 6
2022-02-18 14:47:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 14:55:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 14:55:49 | INFO | train_inner | epoch 006:     91 / 788 loss=6.835, nll_loss=6.146, ppl=70.82, wps=10649, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.651, loss_scale=16, train_wall=561, gb_free=3.3, wall=23506
2022-02-18 15:05:30 | INFO | train_inner | epoch 006:    191 / 788 loss=6.816, nll_loss=6.126, ppl=69.83, wps=11289.7, ups=0.17, wpb=65534.7, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.639, loss_scale=16, train_wall=558, gb_free=3.3, wall=24087
2022-02-18 15:10:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 15:15:16 | INFO | train_inner | epoch 006:    292 / 788 loss=6.802, nll_loss=6.111, ppl=69.12, wps=11177.1, ups=0.17, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.646, loss_scale=16, train_wall=564, gb_free=3.3, wall=24673
2022-02-18 15:24:56 | INFO | train_inner | epoch 006:    392 / 788 loss=6.785, nll_loss=6.094, ppl=68.31, wps=11295, ups=0.17, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.612, loss_scale=32, train_wall=558, gb_free=3.3, wall=25254
2022-02-18 15:31:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 15:34:43 | INFO | train_inner | epoch 006:    493 / 788 loss=6.765, nll_loss=6.073, ppl=67.33, wps=11165.9, ups=0.17, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.588, loss_scale=16, train_wall=564, gb_free=3.3, wall=25840
2022-02-18 15:44:24 | INFO | train_inner | epoch 006:    593 / 788 loss=6.745, nll_loss=6.052, ppl=66.34, wps=11287, ups=0.17, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.592, loss_scale=32, train_wall=558, gb_free=3.3, wall=26421
2022-02-18 15:52:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 15:54:11 | INFO | train_inner | epoch 006:    694 / 788 loss=6.733, nll_loss=6.04, ppl=65.8, wps=11165.6, ups=0.17, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.581, loss_scale=16, train_wall=564, gb_free=3.3, wall=27008
2022-02-18 16:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 16:03:22 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.677 | nll_loss 5.972 | ppl 62.75 | wps 26954.7 | wpb 510.9 | bsz 1 | num_updates 4694 | best_loss 6.677
2022-02-18 16:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4694 updates
2022-02-18 16:03:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint6.pt
2022-02-18 16:03:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint6.pt
2022-02-18 16:03:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint6.pt (epoch 6 @ 4694 updates, score 6.677) (writing took 20.10558295249939 seconds)
2022-02-18 16:03:42 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-18 16:03:43 | INFO | train | epoch 006 | loss 6.772 | nll_loss 6.08 | ppl 67.65 | wps 11157.9 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 4694 | lr 0.00046156 | gnorm 0.613 | loss_scale 16 | train_wall 4396 | gb_free 3.3 | wall 27580
2022-02-18 16:03:43 | INFO | fairseq.trainer | begin training epoch 7
2022-02-18 16:03:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 16:04:17 | INFO | train_inner | epoch 007:      6 / 788 loss=6.699, nll_loss=6.004, ppl=64.17, wps=10754, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=4700, lr=0.000461266, gnorm=0.591, loss_scale=16, train_wall=556, gb_free=3.3, wall=27615
2022-02-18 16:13:59 | INFO | train_inner | epoch 007:    106 / 788 loss=6.543, nll_loss=5.84, ppl=57.29, wps=11271.6, ups=0.17, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.56, loss_scale=32, train_wall=559, gb_free=3.3, wall=28196
2022-02-18 16:15:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 16:23:46 | INFO | train_inner | epoch 007:    207 / 788 loss=6.55, nll_loss=5.847, ppl=57.57, wps=11156.1, ups=0.17, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.564, loss_scale=16, train_wall=565, gb_free=3.3, wall=28784
2022-02-18 16:33:28 | INFO | train_inner | epoch 007:    307 / 788 loss=6.537, nll_loss=5.833, ppl=57, wps=11273.4, ups=0.17, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.553, loss_scale=32, train_wall=559, gb_free=3.3, wall=29365
2022-02-18 16:40:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 16:43:15 | INFO | train_inner | epoch 007:    408 / 788 loss=6.543, nll_loss=5.84, ppl=57.28, wps=11158.2, ups=0.17, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.556, loss_scale=32, train_wall=565, gb_free=3.3, wall=29952
2022-02-18 16:45:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 16:53:02 | INFO | train_inner | epoch 007:    509 / 788 loss=6.539, nll_loss=5.836, ppl=57.13, wps=11157.9, ups=0.17, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.557, loss_scale=16, train_wall=565, gb_free=3.3, wall=30540
2022-02-18 17:02:44 | INFO | train_inner | epoch 007:    609 / 788 loss=6.529, nll_loss=5.825, ppl=56.69, wps=11265.6, ups=0.17, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.531, loss_scale=32, train_wall=559, gb_free=3.3, wall=31121
2022-02-18 17:10:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 17:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 17:12:37 | INFO | train_inner | epoch 007:    711 / 788 loss=6.528, nll_loss=5.825, ppl=56.69, wps=11045.9, ups=0.17, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.546, loss_scale=16, train_wall=570, gb_free=3.3, wall=31715
2022-02-18 17:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 17:20:11 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.542 | nll_loss 5.826 | ppl 56.73 | wps 27036.9 | wpb 510.9 | bsz 1 | num_updates 5477 | best_loss 6.542
2022-02-18 17:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5477 updates
2022-02-18 17:20:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint7.pt
2022-02-18 17:20:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint7.pt
2022-02-18 17:20:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint7.pt (epoch 7 @ 5477 updates, score 6.542) (writing took 20.0591029683128 seconds)
2022-02-18 17:20:31 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-18 17:20:31 | INFO | train | epoch 007 | loss 6.537 | nll_loss 5.833 | ppl 57.01 | wps 11129.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 5477 | lr 0.000427296 | gnorm 0.552 | loss_scale 16 | train_wall 4402 | gb_free 3.3 | wall 32188
2022-02-18 17:20:31 | INFO | fairseq.trainer | begin training epoch 8
2022-02-18 17:20:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 17:22:45 | INFO | train_inner | epoch 008:     23 / 788 loss=6.48, nll_loss=5.774, ppl=54.71, wps=10740.1, ups=0.16, wpb=65232.6, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.536, loss_scale=16, train_wall=557, gb_free=3.3, wall=32322
2022-02-18 17:25:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 17:32:33 | INFO | train_inner | epoch 008:    124 / 788 loss=6.353, nll_loss=5.641, ppl=49.89, wps=11146.2, ups=0.17, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.548, loss_scale=16, train_wall=565, gb_free=3.3, wall=32910
2022-02-18 17:42:14 | INFO | train_inner | epoch 008:    224 / 788 loss=6.367, nll_loss=5.655, ppl=50.38, wps=11269, ups=0.17, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.526, loss_scale=32, train_wall=559, gb_free=3.3, wall=33491
2022-02-18 17:51:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 17:52:01 | INFO | train_inner | epoch 008:    325 / 788 loss=6.38, nll_loss=5.668, ppl=50.85, wps=11159.7, ups=0.17, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.54, loss_scale=32, train_wall=565, gb_free=3.3, wall=34079
2022-02-18 18:01:43 | INFO | train_inner | epoch 008:    425 / 788 loss=6.37, nll_loss=5.658, ppl=50.51, wps=11274.5, ups=0.17, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.534, loss_scale=32, train_wall=559, gb_free=3.3, wall=34660
2022-02-18 18:03:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 18:11:30 | INFO | train_inner | epoch 008:    526 / 788 loss=6.381, nll_loss=5.67, ppl=50.92, wps=11166, ups=0.17, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.533, loss_scale=32, train_wall=564, gb_free=3.3, wall=35247
2022-02-18 18:16:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 18:21:16 | INFO | train_inner | epoch 008:    627 / 788 loss=6.377, nll_loss=5.666, ppl=50.77, wps=11172.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.511, loss_scale=32, train_wall=564, gb_free=3.3, wall=35834
2022-02-18 18:29:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 18:31:03 | INFO | train_inner | epoch 008:    728 / 788 loss=6.37, nll_loss=5.658, ppl=50.51, wps=11169.6, ups=0.17, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.53, loss_scale=32, train_wall=564, gb_free=3.3, wall=36420
2022-02-18 18:35:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 18:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 18:36:57 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.454 | nll_loss 5.728 | ppl 53.01 | wps 27267.5 | wpb 510.9 | bsz 1 | num_updates 6259 | best_loss 6.454
2022-02-18 18:36:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6259 updates
2022-02-18 18:36:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint8.pt
2022-02-18 18:37:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint8.pt
2022-02-18 18:37:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint8.pt (epoch 8 @ 6259 updates, score 6.454) (writing took 20.17935322318226 seconds)
2022-02-18 18:37:17 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-18 18:37:17 | INFO | train | epoch 008 | loss 6.372 | nll_loss 5.661 | ppl 50.58 | wps 11118.9 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 6259 | lr 0.000399712 | gnorm 0.53 | loss_scale 16 | train_wall 4401 | gb_free 3.3 | wall 36794
2022-02-18 18:37:17 | INFO | fairseq.trainer | begin training epoch 9
2022-02-18 18:37:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 18:41:15 | INFO | train_inner | epoch 009:     41 / 788 loss=6.315, nll_loss=5.601, ppl=48.54, wps=10660.8, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.516, loss_scale=16, train_wall=561, gb_free=3.3, wall=37032
2022-02-18 18:50:54 | INFO | train_inner | epoch 009:    141 / 788 loss=6.227, nll_loss=5.507, ppl=45.49, wps=11323.7, ups=0.17, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.525, loss_scale=32, train_wall=556, gb_free=3.3, wall=37611
2022-02-18 19:00:33 | INFO | train_inner | epoch 009:    241 / 788 loss=6.229, nll_loss=5.509, ppl=45.54, wps=11319.5, ups=0.17, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.521, loss_scale=32, train_wall=557, gb_free=3.3, wall=38190
2022-02-18 19:01:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 19:09:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 19:10:23 | INFO | train_inner | epoch 009:    343 / 788 loss=6.243, nll_loss=5.525, ppl=46.03, wps=11108.3, ups=0.17, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.53, loss_scale=16, train_wall=567, gb_free=3.3, wall=38780
2022-02-18 19:20:01 | INFO | train_inner | epoch 009:    443 / 788 loss=6.257, nll_loss=5.54, ppl=46.51, wps=11326.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.509, loss_scale=16, train_wall=556, gb_free=3.3, wall=39358
2022-02-18 19:29:40 | INFO | train_inner | epoch 009:    543 / 788 loss=6.259, nll_loss=5.542, ppl=46.58, wps=11331, ups=0.17, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.525, loss_scale=32, train_wall=556, gb_free=3.3, wall=39937
2022-02-18 19:32:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 19:39:24 | INFO | train_inner | epoch 009:    644 / 788 loss=6.267, nll_loss=5.55, ppl=46.85, wps=11215.2, ups=0.17, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.545, loss_scale=16, train_wall=562, gb_free=3.3, wall=40521
2022-02-18 19:49:03 | INFO | train_inner | epoch 009:    744 / 788 loss=6.26, nll_loss=5.543, ppl=46.64, wps=11325.9, ups=0.17, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.518, loss_scale=32, train_wall=556, gb_free=3.3, wall=41100
2022-02-18 19:53:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 19:53:22 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.382 | nll_loss 5.651 | ppl 50.23 | wps 27849.8 | wpb 510.9 | bsz 1 | num_updates 7044 | best_loss 6.382
2022-02-18 19:53:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7044 updates
2022-02-18 19:53:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint9.pt
2022-02-18 19:53:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint9.pt
2022-02-18 19:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint9.pt (epoch 9 @ 7044 updates, score 6.382) (writing took 20.061286280862987 seconds)
2022-02-18 19:53:43 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-18 19:53:43 | INFO | train | epoch 009 | loss 6.248 | nll_loss 5.53 | ppl 46.2 | wps 11212.8 | ups 0.17 | wpb 65497.4 | bsz 127.9 | num_updates 7044 | lr 0.000376782 | gnorm 0.524 | loss_scale 32 | train_wall 4381 | gb_free 3.3 | wall 41380
2022-02-18 19:53:43 | INFO | fairseq.trainer | begin training epoch 10
2022-02-18 19:53:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 19:58:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 19:59:12 | INFO | train_inner | epoch 010:     57 / 788 loss=6.178, nll_loss=5.457, ppl=43.91, wps=10696.8, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.53, loss_scale=32, train_wall=559, gb_free=3.3, wall=41710
2022-02-18 20:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 20:08:57 | INFO | train_inner | epoch 010:    158 / 788 loss=6.117, nll_loss=5.392, ppl=42, wps=11214.2, ups=0.17, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.542, loss_scale=16, train_wall=562, gb_free=3.3, wall=42294
2022-02-18 20:18:36 | INFO | train_inner | epoch 010:    258 / 788 loss=6.13, nll_loss=5.405, ppl=42.38, wps=11323.7, ups=0.17, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.506, loss_scale=32, train_wall=556, gb_free=3.3, wall=42873
2022-02-18 20:26:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 20:28:20 | INFO | train_inner | epoch 010:    359 / 788 loss=6.147, nll_loss=5.424, ppl=42.92, wps=11206.9, ups=0.17, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.528, loss_scale=32, train_wall=562, gb_free=3.3, wall=43458
2022-02-18 20:36:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 20:38:05 | INFO | train_inner | epoch 010:    460 / 788 loss=6.153, nll_loss=5.43, ppl=43.11, wps=11204.9, ups=0.17, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.522, loss_scale=16, train_wall=562, gb_free=3.3, wall=44043
2022-02-18 20:47:44 | INFO | train_inner | epoch 010:    560 / 788 loss=6.165, nll_loss=5.443, ppl=43.49, wps=11318.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.523, loss_scale=16, train_wall=557, gb_free=3.3, wall=44622
2022-02-18 20:50:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 20:57:29 | INFO | train_inner | epoch 010:    661 / 788 loss=6.172, nll_loss=5.45, ppl=43.72, wps=11207.2, ups=0.17, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.517, loss_scale=16, train_wall=562, gb_free=3.3, wall=45206
2022-02-18 21:03:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 21:07:14 | INFO | train_inner | epoch 010:    762 / 788 loss=6.177, nll_loss=5.456, ppl=43.89, wps=11208.6, ups=0.17, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.524, loss_scale=16, train_wall=562, gb_free=3.3, wall=45791
2022-02-18 21:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 21:09:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.336 | nll_loss 5.605 | ppl 48.68 | wps 28495.2 | wpb 510.9 | bsz 1 | num_updates 7826 | best_loss 6.336
2022-02-18 21:09:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7826 updates
2022-02-18 21:09:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint10.pt
2022-02-18 21:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint10.pt
2022-02-18 21:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint10.pt (epoch 10 @ 7826 updates, score 6.336) (writing took 19.93241059780121 seconds)
2022-02-18 21:10:09 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-18 21:10:09 | INFO | train | epoch 010 | loss 6.149 | nll_loss 5.426 | ppl 42.99 | wps 11166.9 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 7826 | lr 0.000357462 | gnorm 0.523 | loss_scale 16 | train_wall 4382 | gb_free 3.3 | wall 45967
2022-02-18 21:10:09 | INFO | fairseq.trainer | begin training epoch 11
2022-02-18 21:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 21:17:18 | INFO | train_inner | epoch 011:     74 / 788 loss=6.06, nll_loss=5.332, ppl=40.29, wps=10796, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.514, loss_scale=32, train_wall=554, gb_free=3.3, wall=46395
2022-02-18 21:24:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 21:27:03 | INFO | train_inner | epoch 011:    175 / 788 loss=6.031, nll_loss=5.302, ppl=39.45, wps=11194.3, ups=0.17, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.528, loss_scale=16, train_wall=563, gb_free=3.3, wall=46981
2022-02-18 21:36:42 | INFO | train_inner | epoch 011:    275 / 788 loss=6.051, nll_loss=5.323, ppl=40.02, wps=11316.2, ups=0.17, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.511, loss_scale=16, train_wall=557, gb_free=3.3, wall=47560
2022-02-18 21:43:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 21:46:27 | INFO | train_inner | epoch 011:    376 / 788 loss=6.071, nll_loss=5.344, ppl=40.62, wps=11204.3, ups=0.17, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.519, loss_scale=16, train_wall=562, gb_free=3.3, wall=48145
2022-02-18 21:56:06 | INFO | train_inner | epoch 011:    476 / 788 loss=6.075, nll_loss=5.347, ppl=40.71, wps=11319.2, ups=0.17, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.498, loss_scale=32, train_wall=557, gb_free=3.3, wall=48724
2022-02-18 22:02:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:05:51 | INFO | train_inner | epoch 011:    577 / 788 loss=6.088, nll_loss=5.362, ppl=41.12, wps=11205.1, ups=0.17, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.522, loss_scale=16, train_wall=562, gb_free=3.3, wall=49309
2022-02-18 22:15:30 | INFO | train_inner | epoch 011:    677 / 788 loss=6.097, nll_loss=5.371, ppl=41.38, wps=11316.5, ups=0.17, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.534, loss_scale=32, train_wall=557, gb_free=3.3, wall=49888
2022-02-18 22:23:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:25:15 | INFO | train_inner | epoch 011:    778 / 788 loss=6.105, nll_loss=5.38, ppl=41.65, wps=11202.4, ups=0.17, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.507, loss_scale=16, train_wall=562, gb_free=3.3, wall=50473
2022-02-18 22:26:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 22:26:19 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.294 | nll_loss 5.565 | ppl 47.35 | wps 27801.2 | wpb 510.9 | bsz 1 | num_updates 8610 | best_loss 6.294
2022-02-18 22:26:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8610 updates
2022-02-18 22:26:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint11.pt
2022-02-18 22:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint11.pt
2022-02-18 22:26:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint11.pt (epoch 11 @ 8610 updates, score 6.294) (writing took 19.885630569420755 seconds)
2022-02-18 22:26:38 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-18 22:26:38 | INFO | train | epoch 011 | loss 6.069 | nll_loss 5.342 | ppl 40.55 | wps 11189.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 8610 | lr 0.000340799 | gnorm 0.517 | loss_scale 16 | train_wall 4385 | gb_free 3.3 | wall 50556
2022-02-18 22:26:39 | INFO | fairseq.trainer | begin training epoch 12
2022-02-18 22:26:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 22:35:20 | INFO | train_inner | epoch 012:     90 / 788 loss=5.957, nll_loss=5.224, ppl=37.39, wps=10790.4, ups=0.17, wpb=65232.6, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.504, loss_scale=16, train_wall=554, gb_free=3.3, wall=51077
2022-02-18 22:38:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:45:05 | INFO | train_inner | epoch 012:    191 / 788 loss=5.971, nll_loss=5.238, ppl=37.73, wps=11202.4, ups=0.17, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.525, loss_scale=16, train_wall=562, gb_free=3.3, wall=51662
2022-02-18 22:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:54:50 | INFO | train_inner | epoch 012:    292 / 788 loss=5.991, nll_loss=5.259, ppl=38.3, wps=11205.8, ups=0.17, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.51, loss_scale=16, train_wall=562, gb_free=3.3, wall=52247
2022-02-18 23:04:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:04:35 | INFO | train_inner | epoch 012:    393 / 788 loss=6.005, nll_loss=5.274, ppl=38.68, wps=11197.5, ups=0.17, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.514, loss_scale=16, train_wall=563, gb_free=3.3, wall=52832
2022-02-18 23:14:14 | INFO | train_inner | epoch 012:    493 / 788 loss=6.005, nll_loss=5.274, ppl=38.7, wps=11312.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.517, loss_scale=16, train_wall=557, gb_free=3.3, wall=53412
2022-02-18 23:16:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:23:59 | INFO | train_inner | epoch 012:    594 / 788 loss=6.015, nll_loss=5.285, ppl=38.99, wps=11206.5, ups=0.17, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.544, loss_scale=16, train_wall=562, gb_free=3.3, wall=53997
2022-02-18 23:33:38 | INFO | train_inner | epoch 012:    694 / 788 loss=6.032, nll_loss=5.303, ppl=39.47, wps=11315, ups=0.17, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.503, loss_scale=32, train_wall=557, gb_free=3.3, wall=54576
2022-02-18 23:35:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:42:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 23:42:48 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.274 | nll_loss 5.538 | ppl 46.47 | wps 27972.8 | wpb 510.9 | bsz 1 | num_updates 9393 | best_loss 6.274
2022-02-18 23:42:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9393 updates
2022-02-18 23:42:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint12.pt
2022-02-18 23:42:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint12.pt
2022-02-18 23:43:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint12.pt (epoch 12 @ 9393 updates, score 6.274) (writing took 19.91621823888272 seconds)
2022-02-18 23:43:08 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-18 23:43:08 | INFO | train | epoch 012 | loss 6.001 | nll_loss 5.27 | ppl 38.6 | wps 11174.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 9393 | lr 0.000326286 | gnorm 0.515 | loss_scale 16 | train_wall 4385 | gb_free 3.3 | wall 55145
2022-02-18 23:43:08 | INFO | fairseq.trainer | begin training epoch 13
2022-02-18 23:43:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 23:43:48 | INFO | train_inner | epoch 013:      7 / 788 loss=6.033, nll_loss=5.304, ppl=39.51, wps=10694.6, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=9400, lr=0.000326164, gnorm=0.506, loss_scale=16, train_wall=560, gb_free=3.3, wall=55186
2022-02-18 23:51:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:53:33 | INFO | train_inner | epoch 013:    108 / 788 loss=5.881, nll_loss=5.144, ppl=35.36, wps=11203.9, ups=0.17, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.519, loss_scale=16, train_wall=562, gb_free=3.3, wall=55771
2022-02-19 00:03:12 | INFO | train_inner | epoch 013:    208 / 788 loss=5.912, nll_loss=5.176, ppl=36.16, wps=11320, ups=0.17, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.533, loss_scale=16, train_wall=557, gb_free=3.3, wall=56350
2022-02-19 00:04:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:12:57 | INFO | train_inner | epoch 013:    309 / 788 loss=5.934, nll_loss=5.198, ppl=36.72, wps=11211.3, ups=0.17, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.524, loss_scale=16, train_wall=562, gb_free=3.3, wall=56934
2022-02-19 00:19:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:22:41 | INFO | train_inner | epoch 013:    410 / 788 loss=5.94, nll_loss=5.205, ppl=36.89, wps=11208.8, ups=0.17, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.524, loss_scale=16, train_wall=562, gb_free=3.3, wall=57519
2022-02-19 00:31:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:32:26 | INFO | train_inner | epoch 013:    511 / 788 loss=5.956, nll_loss=5.223, ppl=37.34, wps=11206.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.517, loss_scale=16, train_wall=562, gb_free=3.3, wall=58104
2022-02-19 00:42:05 | INFO | train_inner | epoch 013:    611 / 788 loss=5.974, nll_loss=5.242, ppl=37.83, wps=11317.1, ups=0.17, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.512, loss_scale=16, train_wall=557, gb_free=3.3, wall=58683
2022-02-19 00:46:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:51:50 | INFO | train_inner | epoch 013:    712 / 788 loss=5.98, nll_loss=5.247, ppl=37.99, wps=11208.7, ups=0.17, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.531, loss_scale=16, train_wall=562, gb_free=3.3, wall=59267
2022-02-19 00:59:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 00:59:15 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.253 | nll_loss 5.516 | ppl 45.75 | wps 28168.3 | wpb 510.9 | bsz 1 | num_updates 10176 | best_loss 6.253
2022-02-19 00:59:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10176 updates
2022-02-19 00:59:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint13.pt
2022-02-19 00:59:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint13.pt
2022-02-19 00:59:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint13.pt (epoch 13 @ 10176 updates, score 6.253) (writing took 19.865399131551385 seconds)
2022-02-19 00:59:35 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-19 00:59:35 | INFO | train | epoch 013 | loss 5.944 | nll_loss 5.21 | ppl 37.01 | wps 11180.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 10176 | lr 0.000313481 | gnorm 0.524 | loss_scale 16 | train_wall 4383 | gb_free 3.3 | wall 59732
2022-02-19 00:59:35 | INFO | fairseq.trainer | begin training epoch 14
2022-02-19 00:59:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 00:59:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:02:00 | INFO | train_inner | epoch 014:     25 / 788 loss=5.946, nll_loss=5.212, ppl=37.06, wps=10702.2, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.533, loss_scale=16, train_wall=559, gb_free=3.3, wall=59877
2022-02-19 01:11:38 | INFO | train_inner | epoch 014:    125 / 788 loss=5.83, nll_loss=5.09, ppl=34.06, wps=11325.1, ups=0.17, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.504, loss_scale=16, train_wall=556, gb_free=3.3, wall=60456
2022-02-19 01:12:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:21:23 | INFO | train_inner | epoch 014:    226 / 788 loss=5.866, nll_loss=5.127, ppl=34.94, wps=11215, ups=0.17, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.531, loss_scale=16, train_wall=562, gb_free=3.3, wall=61040
2022-02-19 01:27:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:31:07 | INFO | train_inner | epoch 014:    327 / 788 loss=5.882, nll_loss=5.144, ppl=35.36, wps=11219.1, ups=0.17, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.528, loss_scale=16, train_wall=562, gb_free=3.3, wall=61624
2022-02-19 01:40:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:40:51 | INFO | train_inner | epoch 014:    428 / 788 loss=5.896, nll_loss=5.16, ppl=35.74, wps=11221, ups=0.17, wpb=65534.7, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.512, loss_scale=16, train_wall=561, gb_free=3.3, wall=62208
2022-02-19 01:50:29 | INFO | train_inner | epoch 014:    528 / 788 loss=5.919, nll_loss=5.184, ppl=36.34, wps=11326.4, ups=0.17, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.525, loss_scale=16, train_wall=556, gb_free=3.3, wall=62787
2022-02-19 01:53:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 02:00:14 | INFO | train_inner | epoch 014:    629 / 788 loss=5.924, nll_loss=5.188, ppl=36.46, wps=11214.6, ups=0.17, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.531, loss_scale=16, train_wall=562, gb_free=3.3, wall=63371
2022-02-19 02:09:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 02:09:58 | INFO | train_inner | epoch 014:    730 / 788 loss=5.93, nll_loss=5.195, ppl=36.64, wps=11214, ups=0.17, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.516, loss_scale=16, train_wall=562, gb_free=3.3, wall=63956
2022-02-19 02:15:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 02:15:39 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.235 | nll_loss 5.499 | ppl 45.23 | wps 27923.5 | wpb 510.9 | bsz 1 | num_updates 10958 | best_loss 6.235
2022-02-19 02:15:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 10958 updates
2022-02-19 02:15:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint14.pt
2022-02-19 02:15:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint14.pt
2022-02-19 02:15:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint14.pt (epoch 14 @ 10958 updates, score 6.235) (writing took 19.979198148474097 seconds)
2022-02-19 02:15:59 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-19 02:15:59 | INFO | train | epoch 014 | loss 5.894 | nll_loss 5.157 | ppl 35.67 | wps 11173 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 10958 | lr 0.000302089 | gnorm 0.52 | loss_scale 16 | train_wall 4380 | gb_free 3.3 | wall 64316
2022-02-19 02:15:59 | INFO | fairseq.trainer | begin training epoch 15
2022-02-19 02:15:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 02:20:02 | INFO | train_inner | epoch 015:     42 / 788 loss=5.877, nll_loss=5.14, ppl=35.25, wps=10804.2, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.507, loss_scale=16, train_wall=554, gb_free=3.3, wall=64559
2022-02-19 02:22:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 02:29:47 | INFO | train_inner | epoch 015:    143 / 788 loss=5.798, nll_loss=5.056, ppl=33.27, wps=11210.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.525, loss_scale=16, train_wall=562, gb_free=3.3, wall=65144
2022-02-19 02:35:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 02:39:31 | INFO | train_inner | epoch 015:    244 / 788 loss=5.824, nll_loss=5.083, ppl=33.89, wps=11216.7, ups=0.17, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.534, loss_scale=16, train_wall=562, gb_free=3.3, wall=65728
2022-02-19 02:48:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 02:49:15 | INFO | train_inner | epoch 015:    345 / 788 loss=5.853, nll_loss=5.113, ppl=34.62, wps=11216.1, ups=0.17, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.528, loss_scale=16, train_wall=562, gb_free=3.3, wall=66313
2022-02-19 02:58:54 | INFO | train_inner | epoch 015:    445 / 788 loss=5.847, nll_loss=5.107, ppl=34.47, wps=11327.9, ups=0.17, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.523, loss_scale=16, train_wall=556, gb_free=3.3, wall=66891
2022-02-19 03:02:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:08:38 | INFO | train_inner | epoch 015:    546 / 788 loss=5.869, nll_loss=5.131, ppl=35.04, wps=11218, ups=0.17, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.538, loss_scale=16, train_wall=562, gb_free=3.3, wall=67475
2022-02-19 03:14:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:18:22 | INFO | train_inner | epoch 015:    647 / 788 loss=5.88, nll_loss=5.142, ppl=35.31, wps=11211.9, ups=0.17, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.526, loss_scale=16, train_wall=562, gb_free=3.3, wall=68060
2022-02-19 03:28:01 | INFO | train_inner | epoch 015:    747 / 788 loss=5.887, nll_loss=5.15, ppl=35.49, wps=11328.7, ups=0.17, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.517, loss_scale=32, train_wall=556, gb_free=3.3, wall=68638
2022-02-19 03:29:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:31:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 03:32:03 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.217 | nll_loss 5.489 | ppl 44.9 | wps 28415.5 | wpb 510.9 | bsz 1 | num_updates 11740 | best_loss 6.217
2022-02-19 03:32:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11740 updates
2022-02-19 03:32:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint15.pt
2022-02-19 03:32:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint15.pt
2022-02-19 03:32:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint15.pt (epoch 15 @ 11740 updates, score 6.217) (writing took 19.920543695800006 seconds)
2022-02-19 03:32:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-19 03:32:23 | INFO | train | epoch 015 | loss 5.85 | nll_loss 5.11 | ppl 34.55 | wps 11173.7 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 11740 | lr 0.000291854 | gnorm 0.526 | loss_scale 16 | train_wall 4380 | gb_free 3.3 | wall 68900
2022-02-19 03:32:23 | INFO | fairseq.trainer | begin training epoch 16
2022-02-19 03:32:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 03:38:10 | INFO | train_inner | epoch 016:     60 / 788 loss=5.8, nll_loss=5.058, ppl=33.3, wps=10710.8, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.521, loss_scale=16, train_wall=559, gb_free=3.3, wall=69247
2022-02-19 03:42:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:47:55 | INFO | train_inner | epoch 016:    161 / 788 loss=5.769, nll_loss=5.026, ppl=32.57, wps=11210.5, ups=0.17, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.531, loss_scale=16, train_wall=562, gb_free=3.3, wall=69832
2022-02-19 03:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:57:39 | INFO | train_inner | epoch 016:    262 / 788 loss=5.791, nll_loss=5.048, ppl=33.08, wps=11218, ups=0.17, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.52, loss_scale=16, train_wall=562, gb_free=3.3, wall=70416
2022-02-19 04:07:17 | INFO | train_inner | epoch 016:    362 / 788 loss=5.801, nll_loss=5.058, ppl=33.32, wps=11332.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.528, loss_scale=16, train_wall=556, gb_free=3.3, wall=70994
2022-02-19 04:09:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:17:01 | INFO | train_inner | epoch 016:    463 / 788 loss=5.812, nll_loss=5.071, ppl=33.61, wps=11217.3, ups=0.17, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.522, loss_scale=16, train_wall=562, gb_free=3.3, wall=71579
2022-02-19 04:21:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:26:46 | INFO | train_inner | epoch 016:    564 / 788 loss=5.83, nll_loss=5.089, ppl=34.05, wps=11219.2, ups=0.17, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.519, loss_scale=16, train_wall=562, gb_free=3.3, wall=72163
2022-02-19 04:34:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:36:29 | INFO | train_inner | epoch 016:    665 / 788 loss=5.843, nll_loss=5.103, ppl=34.37, wps=11227.6, ups=0.17, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.55, loss_scale=16, train_wall=561, gb_free=3.3, wall=72747
2022-02-19 04:46:08 | INFO | train_inner | epoch 016:    765 / 788 loss=5.862, nll_loss=5.124, ppl=34.87, wps=11331, ups=0.17, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.512, loss_scale=16, train_wall=556, gb_free=3.3, wall=73325
2022-02-19 04:48:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 04:48:26 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.211 | nll_loss 5.478 | ppl 44.58 | wps 27860 | wpb 510.9 | bsz 1 | num_updates 12523 | best_loss 6.211
2022-02-19 04:48:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12523 updates
2022-02-19 04:48:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint16.pt
2022-02-19 04:48:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint16.pt
2022-02-19 04:48:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint16.pt (epoch 16 @ 12523 updates, score 6.211) (writing took 19.708754518069327 seconds)
2022-02-19 04:48:46 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-19 04:48:46 | INFO | train | epoch 016 | loss 5.81 | nll_loss 5.069 | ppl 33.56 | wps 11191.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 12523 | lr 0.000282583 | gnorm 0.524 | loss_scale 32 | train_wall 4379 | gb_free 3.3 | wall 73483
2022-02-19 04:48:46 | INFO | fairseq.trainer | begin training epoch 17
2022-02-19 04:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 04:49:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:56:17 | INFO | train_inner | epoch 017:     78 / 788 loss=5.74, nll_loss=4.994, ppl=31.87, wps=10706.6, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.546, loss_scale=16, train_wall=559, gb_free=3.3, wall=73934
2022-02-19 05:03:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 05:06:01 | INFO | train_inner | epoch 017:    179 / 788 loss=5.729, nll_loss=4.982, ppl=31.61, wps=11222.4, ups=0.17, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.526, loss_scale=16, train_wall=561, gb_free=3.3, wall=74518
2022-02-19 05:15:39 | INFO | train_inner | epoch 017:    279 / 788 loss=5.749, nll_loss=5.004, ppl=32.08, wps=11329.8, ups=0.17, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.515, loss_scale=16, train_wall=556, gb_free=3.3, wall=75097
2022-02-19 05:18:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 05:25:23 | INFO | train_inner | epoch 017:    380 / 788 loss=5.762, nll_loss=5.018, ppl=32.39, wps=11222, ups=0.17, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.533, loss_scale=16, train_wall=562, gb_free=3.3, wall=75681
2022-02-19 05:30:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 05:35:07 | INFO | train_inner | epoch 017:    481 / 788 loss=5.79, nll_loss=5.047, ppl=33.06, wps=11226.8, ups=0.17, wpb=65534.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.517, loss_scale=16, train_wall=561, gb_free=3.3, wall=76264
2022-02-19 05:43:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 05:44:51 | INFO | train_inner | epoch 017:    582 / 788 loss=5.795, nll_loss=5.052, ppl=33.17, wps=11222.4, ups=0.17, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.548, loss_scale=16, train_wall=562, gb_free=3.3, wall=76848
2022-02-19 05:54:29 | INFO | train_inner | epoch 017:    682 / 788 loss=5.818, nll_loss=5.076, ppl=33.74, wps=11339, ups=0.17, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.517, loss_scale=16, train_wall=556, gb_free=3.3, wall=77426
2022-02-19 05:55:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:04:13 | INFO | train_inner | epoch 017:    783 / 788 loss=5.83, nll_loss=5.089, ppl=34.04, wps=11219.5, ups=0.17, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.53, loss_scale=16, train_wall=562, gb_free=3.3, wall=78010
2022-02-19 06:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 06:04:47 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.2 | nll_loss 5.464 | ppl 44.13 | wps 27694.4 | wpb 510.9 | bsz 1 | num_updates 13305 | best_loss 6.2
2022-02-19 06:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13305 updates
2022-02-19 06:04:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint17.pt
2022-02-19 06:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint17.pt
2022-02-19 06:05:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint17.pt (epoch 17 @ 13305 updates, score 6.2) (writing took 19.992609767243266 seconds)
2022-02-19 06:05:07 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-19 06:05:07 | INFO | train | epoch 017 | loss 5.775 | nll_loss 5.031 | ppl 32.69 | wps 11178.8 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 13305 | lr 0.000274153 | gnorm 0.53 | loss_scale 16 | train_wall 4378 | gb_free 3.3 | wall 78065
2022-02-19 06:05:07 | INFO | fairseq.trainer | begin training epoch 18
2022-02-19 06:05:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 06:09:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:14:22 | INFO | train_inner | epoch 018:     96 / 788 loss=5.691, nll_loss=4.943, ppl=30.77, wps=10711.2, ups=0.16, wpb=65232.6, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.52, loss_scale=16, train_wall=559, gb_free=3.3, wall=78619
2022-02-19 06:23:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:24:06 | INFO | train_inner | epoch 018:    197 / 788 loss=5.701, nll_loss=4.953, ppl=30.98, wps=11218.3, ups=0.17, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.553, loss_scale=16, train_wall=562, gb_free=3.3, wall=79204
2022-02-19 06:33:45 | INFO | train_inner | epoch 018:    297 / 788 loss=5.717, nll_loss=4.97, ppl=31.34, wps=11329.9, ups=0.17, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.531, loss_scale=16, train_wall=556, gb_free=3.3, wall=79782
2022-02-19 06:36:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:43:29 | INFO | train_inner | epoch 018:    398 / 788 loss=5.746, nll_loss=5.001, ppl=32.01, wps=11221.5, ups=0.17, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.536, loss_scale=16, train_wall=561, gb_free=3.3, wall=80366
2022-02-19 06:49:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:53:13 | INFO | train_inner | epoch 018:    499 / 788 loss=5.762, nll_loss=5.017, ppl=32.38, wps=11226, ups=0.17, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.541, loss_scale=16, train_wall=561, gb_free=3.3, wall=80950
2022-02-19 07:02:50 | INFO | train_inner | epoch 018:    599 / 788 loss=5.756, nll_loss=5.011, ppl=32.25, wps=11340.9, ups=0.17, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.527, loss_scale=32, train_wall=556, gb_free=3.3, wall=81528
2022-02-19 07:05:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 07:12:34 | INFO | train_inner | epoch 018:    700 / 788 loss=5.787, nll_loss=5.044, ppl=33, wps=11226.4, ups=0.17, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.515, loss_scale=16, train_wall=561, gb_free=3.3, wall=82112
2022-02-19 07:19:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 07:21:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 07:21:08 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.196 | nll_loss 5.463 | ppl 44.12 | wps 27859.1 | wpb 510.9 | bsz 1 | num_updates 14087 | best_loss 6.196
2022-02-19 07:21:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14087 updates
2022-02-19 07:21:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint18.pt
2022-02-19 07:21:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint18.pt
2022-02-19 07:21:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint18.pt (epoch 18 @ 14087 updates, score 6.196) (writing took 19.98595156893134 seconds)
2022-02-19 07:21:28 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-19 07:21:28 | INFO | train | epoch 018 | loss 5.743 | nll_loss 4.997 | ppl 31.94 | wps 11180.9 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 14087 | lr 0.000266435 | gnorm 0.531 | loss_scale 16 | train_wall 4378 | gb_free 3.3 | wall 82646
2022-02-19 07:21:28 | INFO | fairseq.trainer | begin training epoch 19
2022-02-19 07:21:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 07:22:43 | INFO | train_inner | epoch 019:     13 / 788 loss=5.77, nll_loss=5.026, ppl=32.57, wps=10708.6, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=14100, lr=0.000266312, gnorm=0.522, loss_scale=16, train_wall=559, gb_free=3.3, wall=82721
2022-02-19 07:32:22 | INFO | train_inner | epoch 019:    113 / 788 loss=5.654, nll_loss=4.903, ppl=29.92, wps=11334.1, ups=0.17, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.518, loss_scale=32, train_wall=556, gb_free=3.3, wall=83299
2022-02-19 07:33:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 07:42:05 | INFO | train_inner | epoch 019:    214 / 788 loss=5.678, nll_loss=4.929, ppl=30.46, wps=11226.8, ups=0.17, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.541, loss_scale=16, train_wall=561, gb_free=3.3, wall=83883
2022-02-19 07:45:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 07:51:50 | INFO | train_inner | epoch 019:    315 / 788 loss=5.698, nll_loss=4.949, ppl=30.89, wps=11218.6, ups=0.17, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.538, loss_scale=16, train_wall=562, gb_free=3.3, wall=84467
2022-02-19 07:59:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 08:01:33 | INFO | train_inner | epoch 019:    416 / 788 loss=5.71, nll_loss=4.962, ppl=31.18, wps=11224.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.534, loss_scale=16, train_wall=561, gb_free=3.3, wall=85051
2022-02-19 08:11:12 | INFO | train_inner | epoch 019:    516 / 788 loss=5.739, nll_loss=4.993, ppl=31.85, wps=11333.7, ups=0.17, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.548, loss_scale=16, train_wall=556, gb_free=3.3, wall=85629
2022-02-19 08:13:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 08:20:56 | INFO | train_inner | epoch 019:    617 / 788 loss=5.741, nll_loss=4.995, ppl=31.89, wps=11214.1, ups=0.17, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.537, loss_scale=16, train_wall=562, gb_free=3.3, wall=86213
2022-02-19 08:27:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 08:30:40 | INFO | train_inner | epoch 019:    718 / 788 loss=5.754, nll_loss=5.009, ppl=32.2, wps=11217, ups=0.17, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.523, loss_scale=16, train_wall=562, gb_free=3.3, wall=86798
2022-02-19 08:37:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 08:37:31 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.186 | nll_loss 5.449 | ppl 43.68 | wps 27366.4 | wpb 510.9 | bsz 1 | num_updates 14870 | best_loss 6.186
2022-02-19 08:37:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14870 updates
2022-02-19 08:37:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint19.pt
2022-02-19 08:37:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint19.pt
2022-02-19 08:37:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint19.pt (epoch 19 @ 14870 updates, score 6.186) (writing took 19.9500518636778 seconds)
2022-02-19 08:37:51 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-19 08:37:51 | INFO | train | epoch 019 | loss 5.714 | nll_loss 4.967 | ppl 31.27 | wps 11191.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 14870 | lr 0.000259325 | gnorm 0.536 | loss_scale 16 | train_wall 4379 | gb_free 3.3 | wall 87228
2022-02-19 08:37:51 | INFO | fairseq.trainer | begin training epoch 20
2022-02-19 08:37:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 08:40:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 08:40:50 | INFO | train_inner | epoch 020:     31 / 788 loss=5.722, nll_loss=4.975, ppl=31.45, wps=10697.7, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=14900, lr=0.000259064, gnorm=0.554, loss_scale=16, train_wall=559, gb_free=3.3, wall=87407
2022-02-19 08:50:29 | INFO | train_inner | epoch 020:    131 / 788 loss=5.626, nll_loss=4.874, ppl=29.32, wps=11327.9, ups=0.17, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.531, loss_scale=16, train_wall=556, gb_free=3.3, wall=87986
2022-02-19 08:54:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:00:13 | INFO | train_inner | epoch 020:    232 / 788 loss=5.65, nll_loss=4.899, ppl=29.84, wps=11218.5, ups=0.17, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.532, loss_scale=16, train_wall=562, gb_free=3.3, wall=88570
2022-02-19 09:09:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:09:57 | INFO | train_inner | epoch 020:    333 / 788 loss=5.661, nll_loss=4.91, ppl=30.06, wps=11224.1, ups=0.17, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.535, loss_scale=16, train_wall=561, gb_free=3.3, wall=89154
2022-02-19 09:19:36 | INFO | train_inner | epoch 020:    433 / 788 loss=5.689, nll_loss=4.94, ppl=30.69, wps=11321.7, ups=0.17, wpb=65534.7, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.558, loss_scale=16, train_wall=557, gb_free=3.3, wall=89733
2022-02-19 09:22:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:29:20 | INFO | train_inner | epoch 020:    534 / 788 loss=5.708, nll_loss=4.96, ppl=31.14, wps=11213.7, ups=0.17, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.531, loss_scale=16, train_wall=562, gb_free=3.3, wall=90317
2022-02-19 09:35:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:39:05 | INFO | train_inner | epoch 020:    635 / 788 loss=5.72, nll_loss=4.972, ppl=31.39, wps=11210.9, ups=0.17, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.539, loss_scale=16, train_wall=562, gb_free=3.3, wall=90902
2022-02-19 09:48:43 | INFO | train_inner | epoch 020:    735 / 788 loss=5.747, nll_loss=5.001, ppl=32.02, wps=11325.3, ups=0.17, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.561, loss_scale=32, train_wall=556, gb_free=3.3, wall=91481
2022-02-19 09:50:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:53:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 09:53:55 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.183 | nll_loss 5.442 | ppl 43.49 | wps 28309.1 | wpb 510.9 | bsz 1 | num_updates 15652 | best_loss 6.183
2022-02-19 09:53:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15652 updates
2022-02-19 09:53:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint20.pt
2022-02-19 09:54:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint20.pt
2022-02-19 09:54:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint20.pt (epoch 20 @ 15652 updates, score 6.183) (writing took 20.178977742791176 seconds)
2022-02-19 09:54:15 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-19 09:54:15 | INFO | train | epoch 020 | loss 5.687 | nll_loss 4.937 | ppl 30.64 | wps 11172.8 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 15652 | lr 0.000252764 | gnorm 0.541 | loss_scale 16 | train_wall 4380 | gb_free 3.3 | wall 91812
2022-02-19 09:54:15 | INFO | fairseq.trainer | begin training epoch 21
2022-02-19 09:54:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 09:58:53 | INFO | train_inner | epoch 021:     48 / 788 loss=5.67, nll_loss=4.92, ppl=30.28, wps=10701.7, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.536, loss_scale=16, train_wall=559, gb_free=3.3, wall=92090
2022-02-19 10:03:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 10:08:37 | INFO | train_inner | epoch 021:    149 / 788 loss=5.608, nll_loss=4.854, ppl=28.93, wps=11220.1, ups=0.17, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.546, loss_scale=16, train_wall=561, gb_free=3.3, wall=92674
2022-02-19 10:17:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 10:18:21 | INFO | train_inner | epoch 021:    250 / 788 loss=5.634, nll_loss=4.881, ppl=29.47, wps=11220.7, ups=0.17, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.545, loss_scale=16, train_wall=562, gb_free=3.3, wall=93258
2022-02-19 10:27:59 | INFO | train_inner | epoch 021:    350 / 788 loss=5.649, nll_loss=4.897, ppl=29.8, wps=11332.5, ups=0.17, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.534, loss_scale=16, train_wall=556, gb_free=3.3, wall=93837
2022-02-19 10:32:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 10:34:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 10:37:49 | INFO | train_inner | epoch 021:    452 / 788 loss=5.674, nll_loss=4.924, ppl=30.35, wps=11116.6, ups=0.17, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.553, loss_scale=8, train_wall=567, gb_free=3.3, wall=94426
2022-02-19 10:47:27 | INFO | train_inner | epoch 021:    552 / 788 loss=5.681, nll_loss=4.931, ppl=30.51, wps=11337.7, ups=0.17, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.542, loss_scale=16, train_wall=556, gb_free=3.3, wall=95004
2022-02-19 10:57:05 | INFO | train_inner | epoch 021:    652 / 788 loss=5.699, nll_loss=4.95, ppl=30.92, wps=11336.4, ups=0.17, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.532, loss_scale=16, train_wall=556, gb_free=3.3, wall=95582
2022-02-19 11:00:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:06:49 | INFO | train_inner | epoch 021:    753 / 788 loss=5.712, nll_loss=4.964, ppl=31.22, wps=11224.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.534, loss_scale=16, train_wall=561, gb_free=3.3, wall=96166
2022-02-19 11:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 11:10:16 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.178 | nll_loss 5.441 | ppl 43.45 | wps 28506.6 | wpb 510.9 | bsz 1 | num_updates 16435 | best_loss 6.178
2022-02-19 11:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16435 updates
2022-02-19 11:10:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint21.pt
2022-02-19 11:10:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint21.pt
2022-02-19 11:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint21.pt (epoch 21 @ 16435 updates, score 6.178) (writing took 20.01342035178095 seconds)
2022-02-19 11:10:36 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-19 11:10:36 | INFO | train | epoch 021 | loss 5.663 | nll_loss 4.912 | ppl 30.11 | wps 11194.1 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 16435 | lr 0.000246669 | gnorm 0.541 | loss_scale 16 | train_wall 4378 | gb_free 3.3 | wall 96394
2022-02-19 11:10:36 | INFO | fairseq.trainer | begin training epoch 22
2022-02-19 11:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 11:16:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:16:58 | INFO | train_inner | epoch 022:     66 / 788 loss=5.613, nll_loss=4.86, ppl=29.04, wps=10708, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.544, loss_scale=16, train_wall=559, gb_free=3.3, wall=96775
2022-02-19 11:26:36 | INFO | train_inner | epoch 022:    166 / 788 loss=5.589, nll_loss=4.834, ppl=28.52, wps=11335.1, ups=0.17, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.554, loss_scale=16, train_wall=556, gb_free=3.3, wall=97353
2022-02-19 11:28:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:36:21 | INFO | train_inner | epoch 022:    267 / 788 loss=5.609, nll_loss=4.855, ppl=28.94, wps=11212.9, ups=0.17, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.531, loss_scale=16, train_wall=562, gb_free=3.3, wall=97938
2022-02-19 11:41:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:46:04 | INFO | train_inner | epoch 022:    368 / 788 loss=5.638, nll_loss=4.886, ppl=29.57, wps=11232.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.545, loss_scale=16, train_wall=561, gb_free=3.3, wall=98521
2022-02-19 11:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:55:47 | INFO | train_inner | epoch 022:    469 / 788 loss=5.648, nll_loss=4.897, ppl=29.79, wps=11239.6, ups=0.17, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.54, loss_scale=16, train_wall=561, gb_free=3.3, wall=99104
2022-02-19 12:05:24 | INFO | train_inner | epoch 022:    569 / 788 loss=5.654, nll_loss=4.903, ppl=29.91, wps=11354.4, ups=0.17, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.562, loss_scale=16, train_wall=555, gb_free=3.3, wall=99682
2022-02-19 12:07:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 12:15:07 | INFO | train_inner | epoch 022:    670 / 788 loss=5.681, nll_loss=4.932, ppl=30.52, wps=11238.1, ups=0.17, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.549, loss_scale=16, train_wall=561, gb_free=3.3, wall=100265
2022-02-19 12:19:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 12:24:49 | INFO | train_inner | epoch 022:    771 / 788 loss=5.701, nll_loss=4.952, ppl=30.96, wps=11269.1, ups=0.17, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.525, loss_scale=16, train_wall=559, gb_free=3.3, wall=100846
2022-02-19 12:26:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 12:26:32 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.173 | nll_loss 5.431 | ppl 43.13 | wps 28601.7 | wpb 510.9 | bsz 1 | num_updates 17217 | best_loss 6.173
2022-02-19 12:26:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17217 updates
2022-02-19 12:26:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint22.pt
2022-02-19 12:26:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint22.pt
2022-02-19 12:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint22.pt (epoch 22 @ 17217 updates, score 6.173) (writing took 19.558124826289713 seconds)
2022-02-19 12:26:52 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-19 12:26:52 | INFO | train | epoch 022 | loss 5.64 | nll_loss 4.888 | ppl 29.61 | wps 11194.9 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 17217 | lr 0.000241002 | gnorm 0.543 | loss_scale 16 | train_wall 4373 | gb_free 3.3 | wall 100969
2022-02-19 12:26:52 | INFO | fairseq.trainer | begin training epoch 23
2022-02-19 12:26:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 12:34:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 12:34:55 | INFO | train_inner | epoch 023:     84 / 788 loss=5.582, nll_loss=4.826, ppl=28.37, wps=10764.4, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.538, loss_scale=16, train_wall=556, gb_free=3.3, wall=101452
2022-02-19 12:44:30 | INFO | train_inner | epoch 023:    184 / 788 loss=5.576, nll_loss=4.82, ppl=28.25, wps=11393.5, ups=0.17, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.566, loss_scale=16, train_wall=553, gb_free=3.3, wall=102028
2022-02-19 12:46:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 12:54:11 | INFO | train_inner | epoch 023:    285 / 788 loss=5.592, nll_loss=4.838, ppl=28.59, wps=11280.5, ups=0.17, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.557, loss_scale=16, train_wall=559, gb_free=3.3, wall=102609
2022-02-19 12:59:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 13:03:53 | INFO | train_inner | epoch 023:    386 / 788 loss=5.604, nll_loss=4.849, ppl=28.83, wps=11272.9, ups=0.17, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.547, loss_scale=16, train_wall=559, gb_free=3.3, wall=103190
2022-02-19 13:12:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 13:13:34 | INFO | train_inner | epoch 023:    487 / 788 loss=5.633, nll_loss=4.88, ppl=29.45, wps=11279.4, ups=0.17, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.534, loss_scale=16, train_wall=559, gb_free=3.3, wall=103771
2022-02-19 13:23:09 | INFO | train_inner | epoch 023:    587 / 788 loss=5.647, nll_loss=4.895, ppl=29.76, wps=11391.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.558, loss_scale=16, train_wall=553, gb_free=3.3, wall=104346
2022-02-19 13:25:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 13:32:50 | INFO | train_inner | epoch 023:    688 / 788 loss=5.664, nll_loss=4.913, ppl=30.12, wps=11280, ups=0.17, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.547, loss_scale=16, train_wall=559, gb_free=3.3, wall=104927
2022-02-19 13:38:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 13:42:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 13:42:30 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.172 | nll_loss 5.435 | ppl 43.27 | wps 28556.9 | wpb 510.9 | bsz 1 | num_updates 17999 | best_loss 6.172
2022-02-19 13:42:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 17999 updates
2022-02-19 13:42:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint23.pt
2022-02-19 13:42:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint23.pt
2022-02-19 13:42:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint23.pt (epoch 23 @ 17999 updates, score 6.172) (writing took 19.372136309742928 seconds)
2022-02-19 13:42:50 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-19 13:42:50 | INFO | train | epoch 023 | loss 5.619 | nll_loss 4.866 | ppl 29.16 | wps 11236.6 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 17999 | lr 0.000235709 | gnorm 0.549 | loss_scale 16 | train_wall 4357 | gb_free 3.3 | wall 105527
2022-02-19 13:42:50 | INFO | fairseq.trainer | begin training epoch 24
2022-02-19 13:42:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 13:42:56 | INFO | train_inner | epoch 024:      1 / 788 loss=5.67, nll_loss=4.92, ppl=30.27, wps=10768.9, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=18000, lr=0.000235702, gnorm=0.544, loss_scale=16, train_wall=556, gb_free=3.3, wall=105533
2022-02-19 13:43:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 13:52:37 | INFO | train_inner | epoch 024:    102 / 788 loss=5.528, nll_loss=4.77, ppl=27.28, wps=11277.6, ups=0.17, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.576, loss_scale=8, train_wall=559, gb_free=3.3, wall=106114
2022-02-19 14:02:12 | INFO | train_inner | epoch 024:    202 / 788 loss=5.549, nll_loss=4.791, ppl=27.69, wps=11383.9, ups=0.17, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.54, loss_scale=16, train_wall=554, gb_free=3.3, wall=106690
2022-02-19 14:08:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:11:54 | INFO | train_inner | epoch 024:    303 / 788 loss=5.586, nll_loss=4.83, ppl=28.45, wps=11276.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.562, loss_scale=16, train_wall=559, gb_free=3.3, wall=107271
2022-02-19 14:21:29 | INFO | train_inner | epoch 024:    403 / 788 loss=5.593, nll_loss=4.838, ppl=28.61, wps=11387.5, ups=0.17, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.55, loss_scale=32, train_wall=553, gb_free=3.3, wall=107846
2022-02-19 14:21:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:31:10 | INFO | train_inner | epoch 024:    504 / 788 loss=5.623, nll_loss=4.87, ppl=29.25, wps=11279.5, ups=0.17, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.552, loss_scale=16, train_wall=559, gb_free=3.3, wall=108428
2022-02-19 14:34:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:40:51 | INFO | train_inner | epoch 024:    605 / 788 loss=5.628, nll_loss=4.875, ppl=29.34, wps=11281.7, ups=0.17, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.549, loss_scale=16, train_wall=559, gb_free=3.3, wall=109008
2022-02-19 14:46:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:50:32 | INFO | train_inner | epoch 024:    706 / 788 loss=5.648, nll_loss=4.896, ppl=29.78, wps=11275.1, ups=0.17, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.557, loss_scale=16, train_wall=559, gb_free=3.3, wall=109590
2022-02-19 14:58:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 14:58:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.174 | nll_loss 5.432 | ppl 43.17 | wps 28506 | wpb 510.9 | bsz 1 | num_updates 18782 | best_loss 6.172
2022-02-19 14:58:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18782 updates
2022-02-19 14:58:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint24.pt
2022-02-19 14:58:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint24.pt
2022-02-19 14:58:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint24.pt (epoch 24 @ 18782 updates, score 6.174) (writing took 12.845215869136155 seconds)
2022-02-19 14:58:42 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-19 14:58:42 | INFO | train | epoch 024 | loss 5.6 | nll_loss 4.846 | ppl 28.75 | wps 11265.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 18782 | lr 0.000230743 | gnorm 0.555 | loss_scale 16 | train_wall 4358 | gb_free 3.3 | wall 110079
2022-02-19 14:58:42 | INFO | fairseq.trainer | begin training epoch 25
2022-02-19 14:58:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 14:59:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:00:31 | INFO | train_inner | epoch 025:     19 / 788 loss=5.628, nll_loss=4.875, ppl=29.35, wps=10888.3, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=18800, lr=0.000230633, gnorm=0.553, loss_scale=16, train_wall=556, gb_free=3.3, wall=110189
2022-02-19 15:10:07 | INFO | train_inner | epoch 025:    119 / 788 loss=5.521, nll_loss=4.762, ppl=27.13, wps=11387.3, ups=0.17, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.552, loss_scale=16, train_wall=553, gb_free=3.3, wall=110764
2022-02-19 15:12:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:19:48 | INFO | train_inner | epoch 025:    220 / 788 loss=5.547, nll_loss=4.789, ppl=27.65, wps=11276.4, ups=0.17, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.549, loss_scale=16, train_wall=559, gb_free=3.3, wall=111345
2022-02-19 15:25:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:29:29 | INFO | train_inner | epoch 025:    321 / 788 loss=5.564, nll_loss=4.807, ppl=28, wps=11279.5, ups=0.17, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.555, loss_scale=16, train_wall=559, gb_free=3.3, wall=111926
2022-02-19 15:37:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:39:10 | INFO | train_inner | epoch 025:    422 / 788 loss=5.581, nll_loss=4.825, ppl=28.35, wps=11281.9, ups=0.17, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.558, loss_scale=16, train_wall=559, gb_free=3.3, wall=112507
2022-02-19 15:48:45 | INFO | train_inner | epoch 025:    522 / 788 loss=5.601, nll_loss=4.847, ppl=28.77, wps=11392, ups=0.17, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.56, loss_scale=16, train_wall=553, gb_free=3.3, wall=113083
2022-02-19 15:50:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:50:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 15:58:32 | INFO | train_inner | epoch 025:    624 / 788 loss=5.615, nll_loss=4.861, ppl=29.06, wps=11173, ups=0.17, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.592, loss_scale=8, train_wall=564, gb_free=3.3, wall=113669
2022-02-19 16:08:08 | INFO | train_inner | epoch 025:    724 / 788 loss=5.623, nll_loss=4.87, ppl=29.24, wps=11381, ups=0.17, wpb=65534.7, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.539, loss_scale=16, train_wall=554, gb_free=3.3, wall=114245
2022-02-19 16:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 16:14:21 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.175 | nll_loss 5.43 | ppl 43.1 | wps 28505.6 | wpb 510.9 | bsz 1 | num_updates 19564 | best_loss 6.172
2022-02-19 16:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19564 updates
2022-02-19 16:14:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint25.pt
2022-02-19 16:14:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint25.pt
2022-02-19 16:14:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint25.pt (epoch 25 @ 19564 updates, score 6.175) (writing took 12.829451439902186 seconds)
2022-02-19 16:14:34 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-19 16:14:34 | INFO | train | epoch 025 | loss 5.582 | nll_loss 4.827 | ppl 28.38 | wps 11252.5 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 19564 | lr 0.000226085 | gnorm 0.558 | loss_scale 16 | train_wall 4357 | gb_free 3.3 | wall 114631
2022-02-19 16:14:34 | INFO | fairseq.trainer | begin training epoch 26
2022-02-19 16:14:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 16:15:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 16:18:07 | INFO | train_inner | epoch 026:     37 / 788 loss=5.586, nll_loss=4.831, ppl=28.46, wps=10889.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=19600, lr=0.000225877, gnorm=0.552, loss_scale=16, train_wall=556, gb_free=3.3, wall=114844
2022-02-19 16:27:42 | INFO | train_inner | epoch 026:    137 / 788 loss=5.502, nll_loss=4.742, ppl=26.75, wps=11387.1, ups=0.17, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.553, loss_scale=16, train_wall=553, gb_free=3.3, wall=115420
2022-02-19 16:28:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 16:37:24 | INFO | train_inner | epoch 026:    238 / 788 loss=5.537, nll_loss=4.779, ppl=27.45, wps=11274.2, ups=0.17, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.564, loss_scale=16, train_wall=559, gb_free=3.3, wall=116001
2022-02-19 16:41:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 16:47:05 | INFO | train_inner | epoch 026:    339 / 788 loss=5.549, nll_loss=4.792, ppl=27.7, wps=11278, ups=0.17, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.567, loss_scale=16, train_wall=559, gb_free=3.3, wall=116582
2022-02-19 16:53:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 16:56:46 | INFO | train_inner | epoch 026:    440 / 788 loss=5.567, nll_loss=4.811, ppl=28.07, wps=11280.1, ups=0.17, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.554, loss_scale=16, train_wall=559, gb_free=3.3, wall=117163
2022-02-19 17:06:21 | INFO | train_inner | epoch 026:    540 / 788 loss=5.582, nll_loss=4.827, ppl=28.38, wps=11387.4, ups=0.17, wpb=65534.7, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.549, loss_scale=32, train_wall=553, gb_free=3.3, wall=117739
2022-02-19 17:06:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 17:15:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 17:16:08 | INFO | train_inner | epoch 026:    642 / 788 loss=5.604, nll_loss=4.849, ppl=28.83, wps=11168.7, ups=0.17, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.564, loss_scale=8, train_wall=564, gb_free=3.3, wall=118325
2022-02-19 17:25:43 | INFO | train_inner | epoch 026:    742 / 788 loss=5.614, nll_loss=4.86, ppl=29.04, wps=11397.3, ups=0.17, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.564, loss_scale=8, train_wall=553, gb_free=3.3, wall=118900
2022-02-19 17:30:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 17:30:13 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.169 | nll_loss 5.427 | ppl 43.01 | wps 28573.3 | wpb 510.9 | bsz 1 | num_updates 20346 | best_loss 6.169
2022-02-19 17:30:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20346 updates
2022-02-19 17:30:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint26.pt
2022-02-19 17:30:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint26.pt
2022-02-19 17:30:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint26.pt (epoch 26 @ 20346 updates, score 6.169) (writing took 19.4638541508466 seconds)
2022-02-19 17:30:32 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-19 17:30:32 | INFO | train | epoch 026 | loss 5.565 | nll_loss 4.809 | ppl 28.02 | wps 11236.4 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 20346 | lr 0.000221697 | gnorm 0.558 | loss_scale 16 | train_wall 4357 | gb_free 3.3 | wall 119190
2022-02-19 17:30:32 | INFO | fairseq.trainer | begin training epoch 27
2022-02-19 17:30:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 17:35:43 | INFO | train_inner | epoch 027:     54 / 788 loss=5.547, nll_loss=4.789, ppl=27.65, wps=10872.8, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.549, loss_scale=16, train_wall=551, gb_free=3.3, wall=119500
2022-02-19 17:41:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 17:45:24 | INFO | train_inner | epoch 027:    155 / 788 loss=5.492, nll_loss=4.731, ppl=26.55, wps=11276, ups=0.17, wpb=65534.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.571, loss_scale=16, train_wall=559, gb_free=3.3, wall=120081
2022-02-19 17:54:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 17:55:05 | INFO | train_inner | epoch 027:    256 / 788 loss=5.511, nll_loss=4.751, ppl=26.94, wps=11280.1, ups=0.17, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.565, loss_scale=16, train_wall=559, gb_free=3.3, wall=120662
2022-02-19 18:04:41 | INFO | train_inner | epoch 027:    356 / 788 loss=5.549, nll_loss=4.791, ppl=27.69, wps=11385.7, ups=0.17, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.579, loss_scale=16, train_wall=554, gb_free=3.3, wall=121238
2022-02-19 18:08:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 18:14:22 | INFO | train_inner | epoch 027:    457 / 788 loss=5.553, nll_loss=4.796, ppl=27.78, wps=11275.2, ups=0.17, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.564, loss_scale=16, train_wall=559, gb_free=3.3, wall=121819
2022-02-19 18:21:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 18:24:03 | INFO | train_inner | epoch 027:    558 / 788 loss=5.58, nll_loss=4.824, ppl=28.33, wps=11278.1, ups=0.17, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.559, loss_scale=16, train_wall=559, gb_free=3.3, wall=122400
2022-02-19 18:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 18:33:44 | INFO | train_inner | epoch 027:    659 / 788 loss=5.584, nll_loss=4.828, ppl=28.41, wps=11279.1, ups=0.17, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.569, loss_scale=8, train_wall=559, gb_free=3.3, wall=122981
2022-02-19 18:43:19 | INFO | train_inner | epoch 027:    759 / 788 loss=5.601, nll_loss=4.847, ppl=28.78, wps=11392.9, ups=0.17, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.559, loss_scale=8, train_wall=553, gb_free=3.3, wall=123557
2022-02-19 18:46:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 18:46:11 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.17 | nll_loss 5.436 | ppl 43.29 | wps 28371.7 | wpb 510.9 | bsz 1 | num_updates 21129 | best_loss 6.169
2022-02-19 18:46:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21129 updates
2022-02-19 18:46:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint27.pt
2022-02-19 18:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint27.pt
2022-02-19 18:46:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint27.pt (epoch 27 @ 21129 updates, score 6.17) (writing took 12.878735704347491 seconds)
2022-02-19 18:46:24 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-19 18:46:24 | INFO | train | epoch 027 | loss 5.549 | nll_loss 4.792 | ppl 27.7 | wps 11266.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 21129 | lr 0.000217551 | gnorm 0.565 | loss_scale 16 | train_wall 4357 | gb_free 3.3 | wall 123742
2022-02-19 18:46:24 | INFO | fairseq.trainer | begin training epoch 28
2022-02-19 18:46:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 18:53:13 | INFO | train_inner | epoch 028:     71 / 788 loss=5.502, nll_loss=4.742, ppl=26.75, wps=10990.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.565, loss_scale=16, train_wall=551, gb_free=3.3, wall=124150
2022-02-19 18:57:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:02:54 | INFO | train_inner | epoch 028:    172 / 788 loss=5.484, nll_loss=4.722, ppl=26.39, wps=11280.4, ups=0.17, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.555, loss_scale=16, train_wall=559, gb_free=3.3, wall=124731
2022-02-19 19:11:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:12:35 | INFO | train_inner | epoch 028:    273 / 788 loss=5.499, nll_loss=4.739, ppl=26.7, wps=11284.5, ups=0.17, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.561, loss_scale=16, train_wall=559, gb_free=3.3, wall=125312
2022-02-19 19:22:10 | INFO | train_inner | epoch 028:    373 / 788 loss=5.523, nll_loss=4.764, ppl=27.18, wps=11398, ups=0.17, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.56, loss_scale=16, train_wall=553, gb_free=3.3, wall=125887
2022-02-19 19:23:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:31:51 | INFO | train_inner | epoch 028:    474 / 788 loss=5.547, nll_loss=4.789, ppl=27.65, wps=11280.8, ups=0.17, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.563, loss_scale=16, train_wall=559, gb_free=3.3, wall=126468
2022-02-19 19:36:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:37:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 19:41:37 | INFO | train_inner | epoch 028:    576 / 788 loss=5.567, nll_loss=4.811, ppl=28.07, wps=11179.2, ups=0.17, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.566, loss_scale=8, train_wall=564, gb_free=3.3, wall=127054
2022-02-19 19:51:12 | INFO | train_inner | epoch 028:    676 / 788 loss=5.575, nll_loss=4.819, ppl=28.22, wps=11393.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.574, loss_scale=16, train_wall=553, gb_free=3.3, wall=127629
2022-02-19 20:00:48 | INFO | train_inner | epoch 028:    776 / 788 loss=5.587, nll_loss=4.831, ppl=28.47, wps=11384.3, ups=0.17, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.576, loss_scale=16, train_wall=554, gb_free=3.3, wall=128205
2022-02-19 20:01:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 20:02:02 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.174 | nll_loss 5.425 | ppl 42.96 | wps 28584.8 | wpb 510.9 | bsz 1 | num_updates 21912 | best_loss 6.169
2022-02-19 20:02:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 21912 updates
2022-02-19 20:02:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint28.pt
2022-02-19 20:02:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint28.pt
2022-02-19 20:02:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint28.pt (epoch 28 @ 21912 updates, score 6.174) (writing took 12.952986091375351 seconds)
2022-02-19 20:02:15 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-19 20:02:15 | INFO | train | epoch 028 | loss 5.534 | nll_loss 4.776 | ppl 27.4 | wps 11269.7 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 21912 | lr 0.000213628 | gnorm 0.566 | loss_scale 32 | train_wall 4356 | gb_free 3.3 | wall 128292
2022-02-19 20:02:15 | INFO | fairseq.trainer | begin training epoch 29
2022-02-19 20:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 20:02:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:10:48 | INFO | train_inner | epoch 029:     89 / 788 loss=5.476, nll_loss=4.714, ppl=26.24, wps=10868.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.566, loss_scale=16, train_wall=557, gb_free=3.3, wall=128805
2022-02-19 20:14:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:20:30 | INFO | train_inner | epoch 029:    190 / 788 loss=5.47, nll_loss=4.708, ppl=26.13, wps=11260.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.562, loss_scale=16, train_wall=559, gb_free=3.3, wall=129387
2022-02-19 20:27:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:30:12 | INFO | train_inner | epoch 029:    291 / 788 loss=5.497, nll_loss=4.736, ppl=26.65, wps=11261.1, ups=0.17, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.569, loss_scale=16, train_wall=559, gb_free=3.3, wall=129969
2022-02-19 20:39:48 | INFO | train_inner | epoch 029:    391 / 788 loss=5.516, nll_loss=4.756, ppl=27.02, wps=11374.6, ups=0.17, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.567, loss_scale=16, train_wall=554, gb_free=3.3, wall=130545
2022-02-19 20:40:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:49:30 | INFO | train_inner | epoch 029:    492 / 788 loss=5.536, nll_loss=4.777, ppl=27.42, wps=11260.9, ups=0.17, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.591, loss_scale=16, train_wall=559, gb_free=3.3, wall=131127
2022-02-19 20:53:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:53:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 20:59:17 | INFO | train_inner | epoch 029:    594 / 788 loss=5.539, nll_loss=4.781, ppl=27.49, wps=11155.8, ups=0.17, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.558, loss_scale=8, train_wall=565, gb_free=3.3, wall=131715
2022-02-19 21:08:53 | INFO | train_inner | epoch 029:    694 / 788 loss=5.566, nll_loss=4.81, ppl=28.04, wps=11378.6, ups=0.17, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.601, loss_scale=16, train_wall=554, gb_free=3.3, wall=132291
2022-02-19 21:17:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 21:18:00 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.175 | nll_loss 5.44 | ppl 43.42 | wps 28495.5 | wpb 510.9 | bsz 1 | num_updates 22694 | best_loss 6.169
2022-02-19 21:18:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22694 updates
2022-02-19 21:18:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint29.pt
2022-02-19 21:18:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint29.pt
2022-02-19 21:18:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint29.pt (epoch 29 @ 22694 updates, score 6.175) (writing took 12.937987706623971 seconds)
2022-02-19 21:18:13 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-19 21:18:13 | INFO | train | epoch 029 | loss 5.52 | nll_loss 4.761 | ppl 27.11 | wps 11237.1 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 22694 | lr 0.000209915 | gnorm 0.571 | loss_scale 32 | train_wall 4361 | gb_free 3.3 | wall 132850
2022-02-19 21:18:13 | INFO | fairseq.trainer | begin training epoch 30
2022-02-19 21:18:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 21:18:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 21:18:53 | INFO | train_inner | epoch 030:      7 / 788 loss=5.566, nll_loss=4.809, ppl=28.04, wps=10873.9, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=22700, lr=0.000209888, gnorm=0.565, loss_scale=16, train_wall=557, gb_free=3.3, wall=132891
2022-02-19 21:28:30 | INFO | train_inner | epoch 030:    107 / 788 loss=5.438, nll_loss=4.674, ppl=25.52, wps=11372.8, ups=0.17, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.583, loss_scale=16, train_wall=554, gb_free=3.3, wall=133467
2022-02-19 21:30:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 21:36:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 21:38:17 | INFO | train_inner | epoch 030:    209 / 788 loss=5.469, nll_loss=4.707, ppl=26.11, wps=11149.5, ups=0.17, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.561, loss_scale=8, train_wall=565, gb_free=3.3, wall=134055
2022-02-19 21:47:53 | INFO | train_inner | epoch 030:    309 / 788 loss=5.49, nll_loss=4.728, ppl=26.51, wps=11374.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.578, loss_scale=8, train_wall=554, gb_free=3.3, wall=134631
2022-02-19 21:57:30 | INFO | train_inner | epoch 030:    409 / 788 loss=5.51, nll_loss=4.749, ppl=26.9, wps=11371.1, ups=0.17, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.568, loss_scale=16, train_wall=554, gb_free=3.3, wall=135207
2022-02-19 22:01:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 22:07:12 | INFO | train_inner | epoch 030:    510 / 788 loss=5.521, nll_loss=4.761, ppl=27.12, wps=11254.4, ups=0.17, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.595, loss_scale=16, train_wall=560, gb_free=3.3, wall=135789
2022-02-19 22:13:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 22:16:54 | INFO | train_inner | epoch 030:    611 / 788 loss=5.533, nll_loss=4.774, ppl=27.36, wps=11258.5, ups=0.17, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.572, loss_scale=16, train_wall=560, gb_free=3.3, wall=136372
2022-02-19 22:26:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 22:26:36 | INFO | train_inner | epoch 030:    712 / 788 loss=5.552, nll_loss=4.794, ppl=27.74, wps=11259.3, ups=0.17, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.574, loss_scale=16, train_wall=560, gb_free=3.3, wall=136954
2022-02-19 22:33:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 22:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 22:33:59 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.175 | nll_loss 5.441 | ppl 43.44 | wps 28497 | wpb 510.9 | bsz 1 | num_updates 23475 | best_loss 6.169
2022-02-19 22:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23475 updates
2022-02-19 22:33:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint30.pt
2022-02-19 22:34:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint30.pt
2022-02-19 22:34:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint30.pt (epoch 30 @ 23475 updates, score 6.175) (writing took 12.932064446620643 seconds)
2022-02-19 22:34:12 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-19 22:34:12 | INFO | train | epoch 030 | loss 5.507 | nll_loss 4.747 | ppl 26.85 | wps 11219.3 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 23475 | lr 0.000206394 | gnorm 0.577 | loss_scale 8 | train_wall 4363 | gb_free 3.3 | wall 137410
2022-02-19 22:34:12 | INFO | fairseq.trainer | begin training epoch 31
2022-02-19 22:34:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 22:36:36 | INFO | train_inner | epoch 031:     25 / 788 loss=5.525, nll_loss=4.766, ppl=27.2, wps=10872.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=23500, lr=0.000206284, gnorm=0.579, loss_scale=8, train_wall=557, gb_free=3.3, wall=137554
2022-02-19 22:46:12 | INFO | train_inner | epoch 031:    125 / 788 loss=5.427, nll_loss=4.662, ppl=25.31, wps=11374, ups=0.17, wpb=65534.7, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.579, loss_scale=16, train_wall=554, gb_free=3.3, wall=138130
2022-02-19 22:55:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 22:55:54 | INFO | train_inner | epoch 031:    226 / 788 loss=5.451, nll_loss=4.687, ppl=25.76, wps=11260.8, ups=0.17, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.568, loss_scale=8, train_wall=559, gb_free=3.3, wall=138712
2022-02-19 23:05:31 | INFO | train_inner | epoch 031:    326 / 788 loss=5.482, nll_loss=4.72, ppl=26.35, wps=11372, ups=0.17, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.586, loss_scale=8, train_wall=554, gb_free=3.3, wall=139288
2022-02-19 23:13:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 23:15:13 | INFO | train_inner | epoch 031:    427 / 788 loss=5.498, nll_loss=4.737, ppl=26.67, wps=11259.8, ups=0.17, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.577, loss_scale=8, train_wall=559, gb_free=3.3, wall=139870
2022-02-19 23:24:49 | INFO | train_inner | epoch 031:    527 / 788 loss=5.514, nll_loss=4.755, ppl=26.99, wps=11374.6, ups=0.17, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.579, loss_scale=8, train_wall=554, gb_free=3.3, wall=140446
2022-02-19 23:34:25 | INFO | train_inner | epoch 031:    627 / 788 loss=5.539, nll_loss=4.78, ppl=27.48, wps=11367.4, ups=0.17, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.579, loss_scale=16, train_wall=554, gb_free=3.3, wall=141023
2022-02-19 23:38:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 23:44:08 | INFO | train_inner | epoch 031:    728 / 788 loss=5.543, nll_loss=4.785, ppl=27.57, wps=11258.3, ups=0.17, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.554, loss_scale=16, train_wall=560, gb_free=3.3, wall=141605
2022-02-19 23:46:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 23:49:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 23:49:58 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.175 | nll_loss 5.436 | ppl 43.3 | wps 28373 | wpb 510.9 | bsz 1 | num_updates 24259 | best_loss 6.169
2022-02-19 23:49:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24259 updates
2022-02-19 23:49:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint31.pt
2022-02-19 23:50:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint31.pt
2022-02-19 23:50:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint31.pt (epoch 31 @ 24259 updates, score 6.175) (writing took 13.055311226285994 seconds)
2022-02-19 23:50:12 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-19 23:50:12 | INFO | train | epoch 031 | loss 5.495 | nll_loss 4.734 | ppl 26.61 | wps 11262.6 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 24259 | lr 0.000203032 | gnorm 0.575 | loss_scale 8 | train_wall 4363 | gb_free 3.3 | wall 141969
2022-02-19 23:50:12 | INFO | fairseq.trainer | begin training epoch 32
2022-02-19 23:50:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 23:54:08 | INFO | train_inner | epoch 032:     41 / 788 loss=5.491, nll_loss=4.73, ppl=26.54, wps=10868.2, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=24300, lr=0.00020286, gnorm=0.582, loss_scale=8, train_wall=557, gb_free=3.3, wall=142205
2022-02-20 00:02:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 00:03:50 | INFO | train_inner | epoch 032:    142 / 788 loss=5.427, nll_loss=4.662, ppl=25.32, wps=11259.1, ups=0.17, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.578, loss_scale=8, train_wall=559, gb_free=3.3, wall=142787
2022-02-20 00:13:26 | INFO | train_inner | epoch 032:    242 / 788 loss=5.447, nll_loss=4.683, ppl=25.69, wps=11373.4, ups=0.17, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.57, loss_scale=8, train_wall=554, gb_free=3.3, wall=143363
2022-02-20 00:23:03 | INFO | train_inner | epoch 032:    342 / 788 loss=5.469, nll_loss=4.706, ppl=26.1, wps=11369.3, ups=0.17, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.597, loss_scale=16, train_wall=554, gb_free=3.3, wall=143940
2022-02-20 00:26:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 00:32:45 | INFO | train_inner | epoch 032:    443 / 788 loss=5.49, nll_loss=4.729, ppl=26.52, wps=11258.8, ups=0.17, wpb=65534.7, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.577, loss_scale=16, train_wall=560, gb_free=3.3, wall=144522
2022-02-20 00:34:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 00:42:27 | INFO | train_inner | epoch 032:    544 / 788 loss=5.506, nll_loss=4.746, ppl=26.83, wps=11260.3, ups=0.17, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.589, loss_scale=8, train_wall=559, gb_free=3.3, wall=145104
2022-02-20 00:52:03 | INFO | train_inner | epoch 032:    644 / 788 loss=5.522, nll_loss=4.763, ppl=27.15, wps=11374.4, ups=0.17, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.584, loss_scale=16, train_wall=554, gb_free=3.3, wall=145680
2022-02-20 00:59:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 00:59:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 01:01:51 | INFO | train_inner | epoch 032:    746 / 788 loss=5.528, nll_loss=4.768, ppl=27.26, wps=11150.8, ups=0.17, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.594, loss_scale=8, train_wall=565, gb_free=3.3, wall=146268
2022-02-20 01:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 01:05:57 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.166 | nll_loss 5.427 | ppl 43.02 | wps 28430 | wpb 510.9 | bsz 1 | num_updates 25042 | best_loss 6.166
2022-02-20 01:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25042 updates
2022-02-20 01:05:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint32.pt
2022-02-20 01:06:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint32.pt
2022-02-20 01:06:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint32.pt (epoch 32 @ 25042 updates, score 6.166) (writing took 19.6839282149449 seconds)
2022-02-20 01:06:17 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-20 01:06:17 | INFO | train | epoch 032 | loss 5.483 | nll_loss 4.721 | ppl 26.37 | wps 11232.8 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 25042 | lr 0.000199832 | gnorm 0.583 | loss_scale 8 | train_wall 4362 | gb_free 3.3 | wall 146534
2022-02-20 01:06:17 | INFO | fairseq.trainer | begin training epoch 33
2022-02-20 01:06:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 01:11:51 | INFO | train_inner | epoch 033:     58 / 788 loss=5.446, nll_loss=4.683, ppl=25.68, wps=10855.3, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=25100, lr=0.000199601, gnorm=0.571, loss_scale=8, train_wall=551, gb_free=3.3, wall=146869
2022-02-20 01:13:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 01:21:34 | INFO | train_inner | epoch 033:    159 / 788 loss=5.415, nll_loss=4.649, ppl=25.08, wps=11259.7, ups=0.17, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.586, loss_scale=8, train_wall=559, gb_free=3.3, wall=147451
2022-02-20 01:29:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 01:31:16 | INFO | train_inner | epoch 033:    260 / 788 loss=5.443, nll_loss=4.679, ppl=25.61, wps=11257.6, ups=0.17, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.588, loss_scale=8, train_wall=560, gb_free=3.3, wall=148033
2022-02-20 01:40:52 | INFO | train_inner | epoch 033:    360 / 788 loss=5.464, nll_loss=4.7, ppl=26, wps=11372.5, ups=0.17, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.589, loss_scale=8, train_wall=554, gb_free=3.3, wall=148609
2022-02-20 01:50:28 | INFO | train_inner | epoch 033:    460 / 788 loss=5.488, nll_loss=4.726, ppl=26.47, wps=11371.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.581, loss_scale=16, train_wall=554, gb_free=3.3, wall=149186
2022-02-20 01:54:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 01:57:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 02:00:16 | INFO | train_inner | epoch 033:    562 / 788 loss=5.498, nll_loss=4.737, ppl=26.67, wps=11144.9, ups=0.17, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.582, loss_scale=8, train_wall=565, gb_free=3.3, wall=149774
2022-02-20 02:09:52 | INFO | train_inner | epoch 033:    662 / 788 loss=5.503, nll_loss=4.743, ppl=26.77, wps=11374.7, ups=0.17, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.584, loss_scale=8, train_wall=554, gb_free=3.3, wall=150350
2022-02-20 02:10:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 02:19:34 | INFO | train_inner | epoch 033:    763 / 788 loss=5.531, nll_loss=4.772, ppl=27.32, wps=11262.5, ups=0.17, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.577, loss_scale=8, train_wall=559, gb_free=3.3, wall=150932
2022-02-20 02:21:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 02:22:03 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.177 | nll_loss 5.433 | ppl 43.2 | wps 28388.9 | wpb 510.9 | bsz 1 | num_updates 25825 | best_loss 6.166
2022-02-20 02:22:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25825 updates
2022-02-20 02:22:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint33.pt
2022-02-20 02:22:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint33.pt
2022-02-20 02:22:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint33.pt (epoch 33 @ 25825 updates, score 6.177) (writing took 12.952548164874315 seconds)
2022-02-20 02:22:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-20 02:22:16 | INFO | train | epoch 033 | loss 5.472 | nll_loss 4.709 | ppl 26.16 | wps 11248.5 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 25825 | lr 0.000196779 | gnorm 0.583 | loss_scale 8 | train_wall 4363 | gb_free 3.3 | wall 151094
2022-02-20 02:22:16 | INFO | fairseq.trainer | begin training epoch 34
2022-02-20 02:22:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 02:25:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 02:29:35 | INFO | train_inner | epoch 034:     76 / 788 loss=5.424, nll_loss=4.659, ppl=25.27, wps=10867.8, ups=0.17, wpb=65232.6, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.611, loss_scale=8, train_wall=557, gb_free=3.3, wall=151532
2022-02-20 02:39:11 | INFO | train_inner | epoch 034:    176 / 788 loss=5.404, nll_loss=4.637, ppl=24.88, wps=11371.2, ups=0.17, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.589, loss_scale=16, train_wall=554, gb_free=3.3, wall=152108
2022-02-20 02:48:47 | INFO | train_inner | epoch 034:    276 / 788 loss=5.435, nll_loss=4.67, ppl=25.46, wps=11368.1, ups=0.17, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.575, loss_scale=16, train_wall=554, gb_free=3.3, wall=152685
2022-02-20 02:50:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 02:58:30 | INFO | train_inner | epoch 034:    377 / 788 loss=5.444, nll_loss=4.68, ppl=25.63, wps=11248.8, ups=0.17, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.593, loss_scale=16, train_wall=560, gb_free=3.3, wall=153267
2022-02-20 02:59:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 03:08:12 | INFO | train_inner | epoch 034:    478 / 788 loss=5.472, nll_loss=4.71, ppl=26.17, wps=11264.4, ups=0.17, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.584, loss_scale=8, train_wall=559, gb_free=3.3, wall=153849
2022-02-20 03:12:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 03:17:54 | INFO | train_inner | epoch 034:    579 / 788 loss=5.495, nll_loss=4.733, ppl=26.6, wps=11259.5, ups=0.17, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.595, loss_scale=8, train_wall=559, gb_free=3.3, wall=154431
2022-02-20 03:27:30 | INFO | train_inner | epoch 034:    679 / 788 loss=5.508, nll_loss=4.748, ppl=26.87, wps=11369.1, ups=0.17, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.593, loss_scale=16, train_wall=554, gb_free=3.3, wall=155008
2022-02-20 03:29:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 03:37:12 | INFO | train_inner | epoch 034:    780 / 788 loss=5.518, nll_loss=4.758, ppl=27.07, wps=11260.3, ups=0.17, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.588, loss_scale=8, train_wall=560, gb_free=3.3, wall=155590
2022-02-20 03:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 03:38:04 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.178 | nll_loss 5.431 | ppl 43.15 | wps 28425.6 | wpb 510.9 | bsz 1 | num_updates 26608 | best_loss 6.166
2022-02-20 03:38:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 26608 updates
2022-02-20 03:38:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint34.pt
2022-02-20 03:38:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint34.pt
2022-02-20 03:38:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint34.pt (epoch 34 @ 26608 updates, score 6.178) (writing took 12.935929762199521 seconds)
2022-02-20 03:38:16 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-20 03:38:16 | INFO | train | epoch 034 | loss 5.461 | nll_loss 4.698 | ppl 25.96 | wps 11246.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 26608 | lr 0.000193863 | gnorm 0.591 | loss_scale 8 | train_wall 4363 | gb_free 3.3 | wall 155654
2022-02-20 03:38:17 | INFO | fairseq.trainer | begin training epoch 35
2022-02-20 03:38:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 03:47:07 | INFO | train_inner | epoch 035:     92 / 788 loss=5.383, nll_loss=4.615, ppl=24.5, wps=10967.5, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.58, loss_scale=16, train_wall=552, gb_free=3.3, wall=156184
2022-02-20 03:54:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 03:55:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 03:56:55 | INFO | train_inner | epoch 035:    194 / 788 loss=5.411, nll_loss=4.644, ppl=25.01, wps=11141.1, ups=0.17, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.596, loss_scale=8, train_wall=565, gb_free=3.3, wall=156773
2022-02-20 04:06:32 | INFO | train_inner | epoch 035:    294 / 788 loss=5.424, nll_loss=4.659, ppl=25.26, wps=11368.7, ups=0.17, wpb=65534.7, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.598, loss_scale=8, train_wall=554, gb_free=3.3, wall=157349
2022-02-20 04:11:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 04:16:14 | INFO | train_inner | epoch 035:    395 / 788 loss=5.448, nll_loss=4.684, ppl=25.71, wps=11259.4, ups=0.17, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.583, loss_scale=8, train_wall=559, gb_free=3.3, wall=157931
2022-02-20 04:25:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 04:25:56 | INFO | train_inner | epoch 035:    496 / 788 loss=5.457, nll_loss=4.694, ppl=25.88, wps=11258.3, ups=0.17, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.594, loss_scale=8, train_wall=560, gb_free=3.3, wall=158513
2022-02-20 04:35:32 | INFO | train_inner | epoch 035:    596 / 788 loss=5.484, nll_loss=4.722, ppl=26.39, wps=11370.4, ups=0.17, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.593, loss_scale=8, train_wall=554, gb_free=3.3, wall=159090
2022-02-20 04:45:09 | INFO | train_inner | epoch 035:    696 / 788 loss=5.491, nll_loss=4.73, ppl=26.54, wps=11371.4, ups=0.17, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.6, loss_scale=16, train_wall=554, gb_free=3.3, wall=159666
2022-02-20 04:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 04:53:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 04:54:04 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.181 | nll_loss 5.436 | ppl 43.28 | wps 28494 | wpb 510.9 | bsz 1 | num_updates 27391 | best_loss 6.166
2022-02-20 04:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 27391 updates
2022-02-20 04:54:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint35.pt
2022-02-20 04:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint35.pt
2022-02-20 04:54:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint35.pt (epoch 35 @ 27391 updates, score 6.181) (writing took 12.892005554400384 seconds)
2022-02-20 04:54:17 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-20 04:54:17 | INFO | train | epoch 035 | loss 5.451 | nll_loss 4.687 | ppl 25.76 | wps 11246 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 27391 | lr 0.000191072 | gnorm 0.591 | loss_scale 16 | train_wall 4364 | gb_free 3.3 | wall 160214
2022-02-20 04:54:17 | INFO | fairseq.trainer | begin training epoch 36
2022-02-20 04:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 04:54:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 04:55:14 | INFO | train_inner | epoch 036:     10 / 788 loss=5.505, nll_loss=4.744, ppl=26.8, wps=10769.4, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=27400, lr=0.00019104, gnorm=0.589, loss_scale=8, train_wall=562, gb_free=3.3, wall=160272
2022-02-20 05:04:51 | INFO | train_inner | epoch 036:    110 / 788 loss=5.371, nll_loss=4.603, ppl=24.29, wps=11367.3, ups=0.17, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.577, loss_scale=8, train_wall=554, gb_free=3.3, wall=160848
2022-02-20 05:07:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 05:14:33 | INFO | train_inner | epoch 036:    211 / 788 loss=5.399, nll_loss=4.632, ppl=24.8, wps=11264.8, ups=0.17, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.598, loss_scale=8, train_wall=559, gb_free=3.3, wall=161430
2022-02-20 05:21:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 05:24:14 | INFO | train_inner | epoch 036:    312 / 788 loss=5.426, nll_loss=4.661, ppl=25.29, wps=11271.3, ups=0.17, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.583, loss_scale=8, train_wall=559, gb_free=3.3, wall=162011
2022-02-20 05:33:50 | INFO | train_inner | epoch 036:    412 / 788 loss=5.437, nll_loss=4.673, ppl=25.5, wps=11381.7, ups=0.17, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.596, loss_scale=8, train_wall=554, gb_free=3.3, wall=162587
2022-02-20 05:38:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 05:43:31 | INFO | train_inner | epoch 036:    513 / 788 loss=5.468, nll_loss=4.705, ppl=26.08, wps=11270.5, ups=0.17, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.591, loss_scale=8, train_wall=559, gb_free=3.3, wall=163169
2022-02-20 05:53:07 | INFO | train_inner | epoch 036:    613 / 788 loss=5.468, nll_loss=4.705, ppl=26.09, wps=11385.8, ups=0.17, wpb=65534.7, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.599, loss_scale=16, train_wall=553, gb_free=3.3, wall=163744
2022-02-20 05:56:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 06:02:48 | INFO | train_inner | epoch 036:    714 / 788 loss=5.488, nll_loss=4.726, ppl=26.47, wps=11272, ups=0.17, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.595, loss_scale=8, train_wall=559, gb_free=3.3, wall=164326
2022-02-20 06:09:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 06:09:59 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.18 | nll_loss 5.445 | ppl 43.56 | wps 28414.4 | wpb 510.9 | bsz 1 | num_updates 28174 | best_loss 6.166
2022-02-20 06:10:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 28174 updates
2022-02-20 06:10:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint36.pt
2022-02-20 06:10:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint36.pt
2022-02-20 06:10:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint36.pt (epoch 36 @ 28174 updates, score 6.18) (writing took 12.872787329368293 seconds)
2022-02-20 06:10:12 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-20 06:10:12 | INFO | train | epoch 036 | loss 5.442 | nll_loss 4.678 | ppl 25.59 | wps 11257.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 28174 | lr 0.000188398 | gnorm 0.594 | loss_scale 16 | train_wall 4360 | gb_free 3.3 | wall 164770
2022-02-20 06:10:12 | INFO | fairseq.trainer | begin training epoch 37
2022-02-20 06:10:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 06:12:42 | INFO | train_inner | epoch 037:     26 / 788 loss=5.463, nll_loss=4.7, ppl=25.99, wps=10985.4, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=28200, lr=0.000188311, gnorm=0.6, loss_scale=16, train_wall=551, gb_free=3.3, wall=164920
2022-02-20 06:14:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 06:22:24 | INFO | train_inner | epoch 037:    127 / 788 loss=5.376, nll_loss=4.607, ppl=24.37, wps=11270.2, ups=0.17, wpb=65536, bsz=128, num_updates=28300, lr=0.000187978, gnorm=0.595, loss_scale=8, train_wall=559, gb_free=3.3, wall=165501
2022-02-20 06:29:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 06:32:05 | INFO | train_inner | epoch 037:    228 / 788 loss=5.397, nll_loss=4.629, ppl=24.75, wps=11271.1, ups=0.17, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.599, loss_scale=8, train_wall=559, gb_free=3.3, wall=166082
2022-02-20 06:41:41 | INFO | train_inner | epoch 037:    328 / 788 loss=5.423, nll_loss=4.657, ppl=25.23, wps=11385.3, ups=0.17, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.599, loss_scale=8, train_wall=554, gb_free=3.3, wall=166658
2022-02-20 06:42:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 06:51:22 | INFO | train_inner | epoch 037:    429 / 788 loss=5.432, nll_loss=4.667, ppl=25.4, wps=11271.7, ups=0.17, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.604, loss_scale=8, train_wall=559, gb_free=3.3, wall=167240
2022-02-20 06:54:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 07:01:03 | INFO | train_inner | epoch 037:    530 / 788 loss=5.441, nll_loss=4.677, ppl=25.58, wps=11277.9, ups=0.17, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.599, loss_scale=8, train_wall=559, gb_free=3.3, wall=167821
2022-02-20 07:08:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 07:10:45 | INFO | train_inner | epoch 037:    631 / 788 loss=5.474, nll_loss=4.711, ppl=26.19, wps=11274.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.599, loss_scale=8, train_wall=559, gb_free=3.3, wall=168402
2022-02-20 07:20:20 | INFO | train_inner | epoch 037:    731 / 788 loss=5.483, nll_loss=4.721, ppl=26.37, wps=11383.2, ups=0.17, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.593, loss_scale=8, train_wall=554, gb_free=3.3, wall=168978
2022-02-20 07:21:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 07:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 07:25:53 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.185 | nll_loss 5.45 | ppl 43.7 | wps 28511.7 | wpb 510.9 | bsz 1 | num_updates 28956 | best_loss 6.166
2022-02-20 07:25:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 28956 updates
2022-02-20 07:25:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1/checkpoint37.pt
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 3184896 vs 3184784
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x2abc00c066a7 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x2abb67b9f500 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x2abb67b9b6d3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x2abb67ba0609 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x2abb67ba1141 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x2abb67ba1935 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x6543f3 (0x2abb650713f3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x2c2c60 (0x2abb64cdfc60 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x2c3dce (0x2abb64ce0dce in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
<omitting python frames>
frame #46: __libc_start_main + 0xf5 (0x2abb52c58555 in /lib64/libc.so.6)
frame #47: /cluster/home/andriusb/fq/env/bin/python() [0x40071e]

/cluster/shadow/.lsbatch/1645168999.205633400: line 8: 119934 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.04 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 1321671 --fp16 --max-update 50000
