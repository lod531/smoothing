Sender: LSF System <lsfadmin@eu-g2-04>
Subject: Job 208519851: <de_dropout_0.3_jelinek_0.0_0.05_0.95_#1> in cluster <euler> Done

Job <de_dropout_0.3_jelinek_0.0_0.05_0.95_#1> was submitted from host <eu-login-09> by user <andriusb> in cluster <euler> at Tue Mar 15 09:42:19 2022
Job was executed on host(s) <eu-g2-04>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 14:17:11 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 14:17:11 2022
Terminated at Wed Mar 16 10:55:23 2022
Results reported at Wed Mar 16 10:55:23 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/de --save-dir /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.0, 0.05, 0.95)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   77788.40 sec.
    Max Memory :                                 3918 MB
    Average Memory :                             2983.24 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16082.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   74292 sec.
    Turnaround time :                            90784 sec.

The output (if any) follows:

2022-03-15 14:17:18 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/de', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.0, 0.05, 0.95)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 14:17:19 | INFO | fairseq.tasks.language_modeling | dictionary: 34528 types
2022-03-15 14:17:20 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
Calculating frequency stats:
  0%|          | 0/45920 [00:00<?, ?it/s]  0%|          | 48/45920 [00:00<01:37, 470.52it/s]  0%|          | 135/45920 [00:00<01:05, 697.77it/s]  0%|          | 205/45920 [00:00<01:19, 574.22it/s]  1%|          | 265/45920 [00:00<01:34, 482.33it/s]  1%|          | 327/45920 [00:00<01:31, 495.74it/s]  1%|          | 417/45920 [00:00<01:14, 608.05it/s]  1%|          | 482/45920 [00:00<01:16, 596.39it/s]  1%|          | 567/45920 [00:00<01:07, 667.23it/s]  1%|▏         | 637/45920 [00:01<01:08, 661.15it/s]  2%|▏         | 705/45920 [00:01<01:34, 480.10it/s]  2%|▏         | 761/45920 [00:01<01:30, 498.02it/s]  2%|▏         | 817/45920 [00:01<01:28, 510.51it/s]  2%|▏         | 880/45920 [00:01<01:25, 528.80it/s]  2%|▏         | 957/45920 [00:01<01:15, 591.85it/s]  2%|▏         | 1036/45920 [00:01<01:09, 641.60it/s]  2%|▏         | 1110/45920 [00:01<01:07, 667.79it/s]  3%|▎         | 1200/45920 [00:02<01:00, 734.00it/s]  3%|▎         | 1276/45920 [00:02<01:00, 736.74it/s]  3%|▎         | 1351/45920 [00:02<01:01, 729.64it/s]  3%|▎         | 1425/45920 [00:02<01:02, 708.04it/s]  3%|▎         | 1514/45920 [00:02<00:58, 758.86it/s]  3%|▎         | 1591/45920 [00:02<00:58, 757.59it/s]  4%|▎         | 1668/45920 [00:02<01:09, 632.65it/s]  4%|▍         | 1735/45920 [00:02<01:10, 623.56it/s]  4%|▍         | 1813/45920 [00:02<01:06, 664.15it/s]  4%|▍         | 1905/45920 [00:03<01:00, 729.59it/s]  4%|▍         | 1981/45920 [00:03<01:01, 720.05it/s]  4%|▍         | 2055/45920 [00:03<01:02, 700.00it/s]  5%|▍         | 2127/45920 [00:03<01:09, 631.45it/s]  5%|▍         | 2192/45920 [00:03<01:17, 565.69it/s]  5%|▍         | 2267/45920 [00:03<01:11, 611.77it/s]  5%|▌         | 2331/45920 [00:03<01:19, 549.68it/s]  5%|▌         | 2401/45920 [00:03<01:14, 586.57it/s]  5%|▌         | 2470/45920 [00:03<01:10, 612.48it/s]  6%|▌         | 2537/45920 [00:04<01:09, 626.64it/s]  6%|▌         | 2602/45920 [00:04<01:08, 627.84it/s]  6%|▌         | 2691/45920 [00:04<01:01, 700.41it/s]  6%|▌         | 2763/45920 [00:04<01:01, 698.35it/s]  6%|▌         | 2834/45920 [00:04<01:04, 664.83it/s]  6%|▋         | 2932/45920 [00:04<00:57, 749.38it/s]  7%|▋         | 3008/45920 [00:04<01:09, 614.14it/s]  7%|▋         | 3078/45920 [00:04<01:07, 632.80it/s]  7%|▋         | 3145/45920 [00:04<01:08, 622.81it/s]  7%|▋         | 3230/45920 [00:05<01:02, 679.83it/s]  7%|▋         | 3310/45920 [00:05<01:00, 708.89it/s]  7%|▋         | 3392/45920 [00:05<00:58, 732.09it/s]  8%|▊         | 3467/45920 [00:05<00:59, 709.60it/s]  8%|▊         | 3540/45920 [00:05<01:04, 653.91it/s]  8%|▊         | 3607/45920 [00:05<01:13, 578.42it/s]  8%|▊         | 3668/45920 [00:05<01:18, 537.33it/s]  8%|▊         | 3726/45920 [00:05<01:17, 546.33it/s]  8%|▊         | 3792/45920 [00:06<01:21, 519.85it/s]  8%|▊         | 3865/45920 [00:06<01:13, 570.88it/s]  9%|▊         | 3927/45920 [00:06<01:19, 527.84it/s]  9%|▊         | 3982/45920 [00:06<01:18, 532.28it/s]  9%|▉         | 4043/45920 [00:06<01:15, 552.04it/s]  9%|▉         | 4100/45920 [00:06<01:20, 522.52it/s]  9%|▉         | 4154/45920 [00:06<01:23, 497.95it/s]  9%|▉         | 4230/45920 [00:06<01:14, 557.20it/s]  9%|▉         | 4287/45920 [00:07<01:33, 444.87it/s]  9%|▉         | 4354/45920 [00:07<01:23, 497.60it/s] 10%|▉         | 4425/45920 [00:07<01:16, 542.89it/s] 10%|▉         | 4504/45920 [00:07<01:08, 604.41it/s] 10%|▉         | 4589/45920 [00:07<01:01, 670.71it/s] 10%|█         | 4660/45920 [00:07<01:12, 570.98it/s] 10%|█         | 4744/45920 [00:07<01:04, 637.96it/s] 10%|█         | 4813/45920 [00:07<01:04, 638.31it/s] 11%|█         | 4889/45920 [00:07<01:02, 661.56it/s] 11%|█         | 4970/45920 [00:08<00:59, 692.04it/s] 11%|█         | 5050/45920 [00:08<00:56, 719.34it/s] 11%|█         | 5124/45920 [00:08<01:04, 632.65it/s] 11%|█▏        | 5191/45920 [00:08<01:09, 587.67it/s] 11%|█▏        | 5254/45920 [00:08<01:08, 595.38it/s] 12%|█▏        | 5316/45920 [00:08<01:08, 590.58it/s] 12%|█▏        | 5402/45920 [00:08<01:01, 663.24it/s] 12%|█▏        | 5471/45920 [00:08<01:03, 635.95it/s] 12%|█▏        | 5536/45920 [00:09<01:05, 613.37it/s] 12%|█▏        | 5599/45920 [00:09<01:08, 590.17it/s] 12%|█▏        | 5659/45920 [00:09<01:10, 568.57it/s] 12%|█▏        | 5735/45920 [00:09<01:05, 616.13it/s] 13%|█▎        | 5798/45920 [00:09<01:10, 571.80it/s] 13%|█▎        | 5879/45920 [00:09<01:03, 633.96it/s] 13%|█▎        | 5944/45920 [00:09<01:08, 580.98it/s] 13%|█▎        | 6010/45920 [00:09<01:06, 600.29it/s] 13%|█▎        | 6072/45920 [00:09<01:13, 540.36it/s] 13%|█▎        | 6140/45920 [00:10<01:09, 575.91it/s] 14%|█▎        | 6213/45920 [00:10<01:04, 616.14it/s] 14%|█▎        | 6277/45920 [00:10<01:06, 594.48it/s] 14%|█▍        | 6338/45920 [00:10<01:06, 597.38it/s] 14%|█▍        | 6430/45920 [00:10<00:57, 687.67it/s] 14%|█▍        | 6500/45920 [00:10<00:58, 676.56it/s] 14%|█▍        | 6586/45920 [00:10<00:54, 727.90it/s] 15%|█▍        | 6660/45920 [00:10<01:08, 569.16it/s] 15%|█▍        | 6723/45920 [00:10<01:07, 581.45it/s] 15%|█▍        | 6799/45920 [00:11<01:02, 626.10it/s] 15%|█▍        | 6866/45920 [00:11<01:03, 611.55it/s] 15%|█▌        | 6938/45920 [00:11<01:01, 638.63it/s] 15%|█▌        | 7027/45920 [00:11<00:55, 705.98it/s] 15%|█▌        | 7107/45920 [00:11<00:53, 730.42it/s] 16%|█▌        | 7182/45920 [00:11<00:56, 689.06it/s] 16%|█▌        | 7253/45920 [00:11<00:59, 650.10it/s] 16%|█▌        | 7320/45920 [00:11<01:04, 596.44it/s] 16%|█▌        | 7382/45920 [00:11<01:04, 597.88it/s] 16%|█▌        | 7455/45920 [00:12<01:00, 632.74it/s] 16%|█▋        | 7520/45920 [00:12<01:01, 620.39it/s] 17%|█▋        | 7583/45920 [00:12<01:18, 491.36it/s] 17%|█▋        | 7637/45920 [00:12<01:42, 371.73it/s] 17%|█▋        | 7715/45920 [00:12<01:24, 453.04it/s] 17%|█▋        | 7770/45920 [00:12<01:27, 437.21it/s] 17%|█▋        | 7849/45920 [00:12<01:13, 516.66it/s] 17%|█▋        | 7935/45920 [00:13<01:03, 599.09it/s] 17%|█▋        | 8002/45920 [00:13<01:01, 613.22it/s] 18%|█▊        | 8069/45920 [00:13<01:02, 609.50it/s] 18%|█▊        | 8140/45920 [00:13<00:59, 635.93it/s] 18%|█▊        | 8207/45920 [00:13<01:01, 612.12it/s] 18%|█▊        | 8283/45920 [00:13<00:57, 651.18it/s] 18%|█▊        | 8365/45920 [00:13<00:53, 698.34it/s] 18%|█▊        | 8448/45920 [00:13<00:51, 733.50it/s] 19%|█▊        | 8523/45920 [00:13<01:02, 598.62it/s] 19%|█▊        | 8601/45920 [00:14<00:58, 642.89it/s] 19%|█▉        | 8670/45920 [00:14<00:57, 650.62it/s] 19%|█▉        | 8739/45920 [00:14<01:01, 607.19it/s] 19%|█▉        | 8816/45920 [00:14<00:57, 649.32it/s] 19%|█▉        | 8884/45920 [00:14<00:58, 633.54it/s] 19%|█▉        | 8949/45920 [00:14<01:03, 586.45it/s] 20%|█▉        | 9019/45920 [00:14<00:59, 615.19it/s] 20%|█▉        | 9100/45920 [00:14<00:55, 668.39it/s] 20%|█▉        | 9169/45920 [00:15<01:03, 577.66it/s] 20%|██        | 9263/45920 [00:15<00:54, 669.51it/s] 20%|██        | 9334/45920 [00:15<00:57, 632.78it/s] 20%|██        | 9412/45920 [00:15<00:54, 671.14it/s] 21%|██        | 9482/45920 [00:15<00:55, 659.38it/s] 21%|██        | 9550/45920 [00:15<00:56, 649.44it/s] 21%|██        | 9617/45920 [00:15<01:02, 581.87it/s] 21%|██        | 9678/45920 [00:15<01:06, 542.75it/s] 21%|██        | 9734/45920 [00:15<01:08, 528.70it/s] 21%|██▏       | 9796/45920 [00:16<01:05, 552.21it/s] 21%|██▏       | 9866/45920 [00:16<01:00, 591.16it/s] 22%|██▏       | 9927/45920 [00:16<01:07, 532.77it/s] 22%|██▏       | 10000/45920 [00:16<01:01, 583.91it/s] 22%|██▏       | 10061/45920 [00:16<01:03, 566.54it/s] 22%|██▏       | 10131/45920 [00:16<01:07, 527.67it/s] 22%|██▏       | 10205/45920 [00:16<01:01, 580.29it/s] 22%|██▏       | 10280/45920 [00:16<00:57, 624.77it/s] 23%|██▎       | 10353/45920 [00:16<00:54, 650.86it/s] 23%|██▎       | 10420/45920 [00:17<00:55, 642.08it/s] 23%|██▎       | 10509/45920 [00:17<00:57, 617.38it/s] 23%|██▎       | 10584/45920 [00:17<00:55, 641.12it/s] 23%|██▎       | 10650/45920 [00:17<01:01, 577.60it/s] 23%|██▎       | 10728/45920 [00:17<00:56, 628.07it/s] 24%|██▎       | 10808/45920 [00:17<00:52, 671.25it/s] 24%|██▎       | 10878/45920 [00:17<01:01, 571.65it/s] 24%|██▍       | 10939/45920 [00:17<01:00, 573.51it/s] 24%|██▍       | 11016/45920 [00:18<00:55, 624.24it/s] 24%|██▍       | 11082/45920 [00:18<00:59, 586.71it/s] 24%|██▍       | 11147/45920 [00:18<00:57, 602.77it/s] 24%|██▍       | 11227/45920 [00:18<00:52, 654.94it/s] 25%|██▍       | 11295/45920 [00:18<00:52, 656.47it/s] 25%|██▍       | 11364/45920 [00:18<01:04, 534.78it/s] 25%|██▍       | 11449/45920 [00:18<00:56, 611.29it/s] 25%|██▌       | 11515/45920 [00:18<00:58, 591.96it/s] 25%|██▌       | 11600/45920 [00:19<00:52, 650.22it/s] 25%|██▌       | 11668/45920 [00:19<00:52, 654.12it/s] 26%|██▌       | 11736/45920 [00:19<01:12, 474.57it/s] 26%|██▌       | 11792/45920 [00:19<01:11, 474.72it/s] 26%|██▌       | 11865/45920 [00:19<01:03, 533.24it/s] 26%|██▌       | 11925/45920 [00:19<01:02, 545.70it/s] 26%|██▌       | 11984/45920 [00:19<01:00, 556.61it/s] 26%|██▌       | 12050/45920 [00:19<00:58, 577.45it/s] 26%|██▋       | 12111/45920 [00:20<01:06, 508.97it/s] 27%|██▋       | 12174/45920 [00:20<01:02, 538.25it/s] 27%|██▋       | 12246/45920 [00:20<00:57, 585.25it/s] 27%|██▋       | 12320/45920 [00:20<00:54, 619.48it/s] 27%|██▋       | 12407/45920 [00:20<00:48, 687.06it/s] 27%|██▋       | 12487/45920 [00:20<00:46, 719.15it/s] 27%|██▋       | 12561/45920 [00:20<00:48, 687.82it/s] 28%|██▊       | 12638/45920 [00:20<00:46, 710.34it/s] 28%|██▊       | 12711/45920 [00:20<00:46, 710.89it/s] 28%|██▊       | 12783/45920 [00:21<00:49, 671.67it/s] 28%|██▊       | 12852/45920 [00:21<01:00, 542.15it/s] 28%|██▊       | 12934/45920 [00:21<00:54, 605.58it/s] 28%|██▊       | 12999/45920 [00:21<00:54, 602.70it/s] 28%|██▊       | 13063/45920 [00:21<00:55, 590.94it/s] 29%|██▊       | 13147/45920 [00:21<00:49, 656.54it/s] 29%|██▉       | 13218/45920 [00:21<00:48, 667.88it/s] 29%|██▉       | 13300/45920 [00:21<00:45, 710.18it/s] 29%|██▉       | 13373/45920 [00:21<00:51, 627.72it/s] 29%|██▉       | 13452/45920 [00:22<00:48, 670.41it/s] 29%|██▉       | 13522/45920 [00:22<00:53, 609.38it/s] 30%|██▉       | 13606/45920 [00:22<00:48, 668.16it/s] 30%|██▉       | 13676/45920 [00:22<00:53, 605.94it/s] 30%|██▉       | 13751/45920 [00:22<00:51, 622.60it/s] 30%|███       | 13840/45920 [00:22<00:46, 690.27it/s] 30%|███       | 13912/45920 [00:22<00:47, 667.36it/s] 30%|███       | 13981/45920 [00:22<00:57, 558.22it/s] 31%|███       | 14041/45920 [00:23<00:57, 556.49it/s] 31%|███       | 14100/45920 [00:23<01:02, 513.14it/s] 31%|███       | 14167/45920 [00:23<00:57, 549.98it/s] 31%|███       | 14225/45920 [00:23<01:07, 466.52it/s] 31%|███       | 14298/45920 [00:23<00:59, 528.81it/s] 31%|███▏      | 14368/45920 [00:23<00:55, 571.38it/s] 31%|███▏      | 14431/45920 [00:23<00:53, 586.58it/s] 32%|███▏      | 14511/45920 [00:23<00:48, 644.82it/s] 32%|███▏      | 14578/45920 [00:24<00:55, 566.40it/s] 32%|███▏      | 14642/45920 [00:24<00:53, 581.87it/s] 32%|███▏      | 14703/45920 [00:24<01:02, 501.93it/s] 32%|███▏      | 14777/45920 [00:24<00:56, 553.65it/s] 32%|███▏      | 14842/45920 [00:24<00:53, 578.47it/s] 32%|███▏      | 14913/45920 [00:24<00:50, 612.11it/s] 33%|███▎      | 14977/45920 [00:24<00:51, 598.84it/s] 33%|███▎      | 15039/45920 [00:24<00:52, 587.09it/s] 33%|███▎      | 15099/45920 [00:25<01:04, 476.75it/s] 33%|███▎      | 15180/45920 [00:25<00:55, 556.93it/s] 33%|███▎      | 15258/45920 [00:25<00:50, 612.26it/s] 33%|███▎      | 15324/45920 [00:25<00:56, 546.31it/s] 33%|███▎      | 15383/45920 [00:25<01:02, 492.25it/s] 34%|███▎      | 15439/45920 [00:25<01:00, 507.23it/s] 34%|███▍      | 15517/45920 [00:25<00:52, 573.68it/s] 34%|███▍      | 15594/45920 [00:25<00:48, 625.61it/s] 34%|███▍      | 15660/45920 [00:25<00:54, 553.92it/s] 34%|███▍      | 15741/45920 [00:26<00:49, 608.75it/s] 34%|███▍      | 15805/45920 [00:26<00:51, 581.15it/s] 35%|███▍      | 15866/45920 [00:26<00:53, 556.57it/s] 35%|███▍      | 15932/45920 [00:26<00:51, 582.86it/s] 35%|███▍      | 15994/45920 [00:26<00:50, 588.92it/s] 35%|███▌      | 16075/45920 [00:26<00:46, 647.82it/s] 35%|███▌      | 16141/45920 [00:26<00:51, 582.28it/s] 35%|███▌      | 16218/45920 [00:26<00:47, 631.65it/s] 35%|███▌      | 16284/45920 [00:27<01:00, 491.99it/s] 36%|███▌      | 16376/45920 [00:27<00:50, 588.06it/s] 36%|███▌      | 16443/45920 [00:27<00:48, 607.75it/s] 36%|███▌      | 16509/45920 [00:27<00:52, 555.76it/s] 36%|███▌      | 16582/45920 [00:27<00:49, 596.15it/s] 36%|███▋      | 16664/45920 [00:27<00:45, 649.11it/s] 36%|███▋      | 16747/45920 [00:27<00:41, 696.79it/s] 37%|███▋      | 16820/45920 [00:27<00:54, 536.97it/s] 37%|███▋      | 16881/45920 [00:28<00:54, 536.80it/s] 37%|███▋      | 16940/45920 [00:28<00:58, 499.50it/s] 37%|███▋      | 16996/45920 [00:28<00:56, 513.95it/s] 37%|███▋      | 17092/45920 [00:28<00:46, 625.35it/s] 37%|███▋      | 17181/45920 [00:28<00:41, 686.38it/s] 38%|███▊      | 17253/45920 [00:28<00:46, 619.49it/s] 38%|███▊      | 17319/45920 [00:28<00:47, 602.27it/s] 38%|███▊      | 17387/45920 [00:28<00:45, 620.60it/s] 38%|███▊      | 17467/45920 [00:28<00:42, 667.51it/s] 38%|███▊      | 17555/45920 [00:29<00:39, 722.71it/s] 38%|███▊      | 17630/45920 [00:29<00:39, 718.83it/s] 39%|███▊      | 17703/45920 [00:29<00:40, 689.65it/s] 39%|███▊      | 17773/45920 [00:29<00:41, 671.23it/s] 39%|███▉      | 17841/45920 [00:29<00:43, 651.50it/s] 39%|███▉      | 17907/45920 [00:29<00:43, 646.08it/s] 39%|███▉      | 17972/45920 [00:29<00:50, 548.18it/s] 39%|███▉      | 18030/45920 [00:29<00:50, 554.79it/s] 39%|███▉      | 18111/45920 [00:30<00:44, 621.22it/s] 40%|███▉      | 18194/45920 [00:30<00:41, 676.18it/s] 40%|███▉      | 18264/45920 [00:30<00:41, 670.57it/s] 40%|███▉      | 18345/45920 [00:30<00:38, 709.45it/s] 40%|████      | 18418/45920 [00:30<00:39, 698.40it/s] 40%|████      | 18489/45920 [00:30<00:44, 618.23it/s] 40%|████      | 18553/45920 [00:30<00:49, 551.31it/s] 41%|████      | 18611/45920 [00:30<01:05, 417.62it/s] 41%|████      | 18695/45920 [00:31<00:55, 494.83it/s] 41%|████      | 18752/45920 [00:31<00:58, 466.50it/s] 41%|████      | 18840/45920 [00:31<00:48, 560.91it/s] 41%|████      | 18920/45920 [00:31<00:43, 619.63it/s] 41%|████▏     | 18996/45920 [00:31<00:41, 655.29it/s] 42%|████▏     | 19069/45920 [00:31<00:39, 674.14it/s] 42%|████▏     | 19140/45920 [00:31<00:40, 654.25it/s] 42%|████▏     | 19208/45920 [00:31<00:43, 607.35it/s] 42%|████▏     | 19293/45920 [00:31<00:39, 671.10it/s] 42%|████▏     | 19363/45920 [00:32<00:44, 591.65it/s] 42%|████▏     | 19437/45920 [00:32<00:42, 626.38it/s] 42%|████▏     | 19503/45920 [00:32<00:43, 605.03it/s] 43%|████▎     | 19590/45920 [00:32<00:39, 673.15it/s] 43%|████▎     | 19660/45920 [00:32<00:39, 671.68it/s] 43%|████▎     | 19729/45920 [00:32<00:39, 667.58it/s] 43%|████▎     | 19810/45920 [00:32<00:36, 706.78it/s] 43%|████▎     | 19882/45920 [00:32<00:38, 681.53it/s] 43%|████▎     | 19951/45920 [00:32<00:38, 666.14it/s] 44%|████▎     | 20031/45920 [00:33<00:40, 635.40it/s] 44%|████▍     | 20096/45920 [00:33<00:43, 591.77it/s] 44%|████▍     | 20157/45920 [00:33<00:44, 584.75it/s] 44%|████▍     | 20235/45920 [00:33<00:40, 635.83it/s] 44%|████▍     | 20310/45920 [00:33<00:38, 664.00it/s] 44%|████▍     | 20378/45920 [00:33<00:39, 641.85it/s] 45%|████▍     | 20443/45920 [00:33<00:39, 637.02it/s] 45%|████▍     | 20508/45920 [00:33<00:40, 623.15it/s] 45%|████▍     | 20588/45920 [00:33<00:37, 671.92it/s] 45%|████▌     | 20679/45920 [00:34<00:34, 739.95it/s] 45%|████▌     | 20754/45920 [00:34<00:37, 676.02it/s] 45%|████▌     | 20826/45920 [00:34<00:36, 678.81it/s] 46%|████▌     | 20895/45920 [00:34<00:40, 623.88it/s] 46%|████▌     | 20960/45920 [00:34<00:39, 629.83it/s] 46%|████▌     | 21025/45920 [00:34<00:39, 634.49it/s] 46%|████▌     | 21090/45920 [00:34<00:43, 567.59it/s] 46%|████▌     | 21149/45920 [00:34<00:45, 538.89it/s] 46%|████▌     | 21208/45920 [00:35<00:45, 545.55it/s] 46%|████▋     | 21271/45920 [00:35<00:43, 566.98it/s] 46%|████▋     | 21329/45920 [00:35<00:44, 549.15it/s] 47%|████▋     | 21392/45920 [00:35<00:43, 567.70it/s] 47%|████▋     | 21454/45920 [00:35<00:42, 581.60it/s] 47%|████▋     | 21522/45920 [00:35<00:40, 609.08it/s] 47%|████▋     | 21609/45920 [00:35<00:35, 680.97it/s] 47%|████▋     | 21678/45920 [00:35<00:38, 628.35it/s] 47%|████▋     | 21742/45920 [00:35<00:41, 584.83it/s] 47%|████▋     | 21802/45920 [00:36<00:45, 535.46it/s] 48%|████▊     | 21868/45920 [00:36<00:42, 566.93it/s] 48%|████▊     | 21927/45920 [00:36<00:43, 552.89it/s] 48%|████▊     | 22000/45920 [00:36<00:39, 599.51it/s] 48%|████▊     | 22062/45920 [00:36<00:46, 516.67it/s] 48%|████▊     | 22127/45920 [00:36<00:43, 550.25it/s] 48%|████▊     | 22221/45920 [00:36<00:36, 652.89it/s] 49%|████▊     | 22307/45920 [00:36<00:33, 708.71it/s] 49%|████▊     | 22381/45920 [00:36<00:34, 673.67it/s] 49%|████▉     | 22459/45920 [00:37<00:33, 699.59it/s] 49%|████▉     | 22531/45920 [00:37<00:34, 680.36it/s] 49%|████▉     | 22601/45920 [00:37<00:34, 682.70it/s] 49%|████▉     | 22671/45920 [00:37<00:35, 657.33it/s] 50%|████▉     | 22742/45920 [00:37<00:34, 671.70it/s] 50%|████▉     | 22823/45920 [00:37<00:32, 709.43it/s] 50%|████▉     | 22895/45920 [00:37<00:36, 631.84it/s] 50%|█████     | 22969/45920 [00:37<00:34, 655.96it/s] 50%|█████     | 23068/45920 [00:37<00:30, 741.44it/s] 50%|█████     | 23144/45920 [00:38<00:34, 669.54it/s] 51%|█████     | 23214/45920 [00:38<00:35, 641.03it/s] 51%|█████     | 23281/45920 [00:38<00:34, 648.12it/s] 51%|█████     | 23354/45920 [00:38<00:33, 669.28it/s] 51%|█████     | 23422/45920 [00:38<00:34, 657.64it/s] 51%|█████     | 23489/45920 [00:38<00:36, 607.88it/s] 51%|█████▏    | 23551/45920 [00:38<00:37, 600.81it/s] 51%|█████▏    | 23612/45920 [00:38<00:45, 491.63it/s] 52%|█████▏    | 23675/45920 [00:39<00:42, 519.45it/s] 52%|█████▏    | 23755/45920 [00:39<00:37, 587.01it/s] 52%|█████▏    | 23817/45920 [00:39<00:37, 586.89it/s] 52%|█████▏    | 23892/45920 [00:39<00:34, 631.06it/s] 52%|█████▏    | 23958/45920 [00:39<00:41, 527.58it/s] 52%|█████▏    | 24035/45920 [00:39<00:37, 586.37it/s] 53%|█████▎    | 24109/45920 [00:39<00:35, 616.18it/s] 53%|█████▎    | 24174/45920 [00:39<00:36, 600.96it/s] 53%|█████▎    | 24237/45920 [00:39<00:39, 555.19it/s] 53%|█████▎    | 24299/45920 [00:40<00:37, 571.57it/s] 53%|█████▎    | 24371/45920 [00:40<00:35, 610.70it/s] 53%|█████▎    | 24444/45920 [00:40<00:33, 640.74it/s] 53%|█████▎    | 24524/45920 [00:40<00:31, 685.74it/s] 54%|█████▎    | 24602/45920 [00:40<00:29, 711.24it/s] 54%|█████▍    | 24696/45920 [00:40<00:27, 777.27it/s] 54%|█████▍    | 24775/45920 [00:40<00:30, 702.00it/s] 54%|█████▍    | 24848/45920 [00:40<00:30, 698.71it/s] 54%|█████▍    | 24920/45920 [00:40<00:29, 703.60it/s] 54%|█████▍    | 24992/45920 [00:41<00:30, 680.07it/s] 55%|█████▍    | 25061/45920 [00:41<00:33, 631.79it/s] 55%|█████▍    | 25126/45920 [00:41<00:34, 608.42it/s] 55%|█████▍    | 25194/45920 [00:41<00:33, 624.18it/s] 55%|█████▌    | 25273/45920 [00:41<00:30, 669.81it/s] 55%|█████▌    | 25349/45920 [00:41<00:30, 670.52it/s] 55%|█████▌    | 25436/45920 [00:41<00:28, 726.05it/s] 56%|█████▌    | 25518/45920 [00:41<00:27, 751.67it/s] 56%|█████▌    | 25594/45920 [00:41<00:30, 662.66it/s] 56%|█████▌    | 25663/45920 [00:42<00:34, 595.13it/s] 56%|█████▌    | 25738/45920 [00:42<00:32, 621.18it/s] 56%|█████▌    | 25815/45920 [00:42<00:30, 659.89it/s] 56%|█████▋    | 25889/45920 [00:42<00:29, 681.30it/s] 57%|█████▋    | 25962/45920 [00:42<00:28, 690.24it/s] 57%|█████▋    | 26033/45920 [00:42<00:28, 687.78it/s] 57%|█████▋    | 26103/45920 [00:42<00:31, 623.50it/s] 57%|█████▋    | 26167/45920 [00:42<00:37, 530.83it/s] 57%|█████▋    | 26242/45920 [00:43<00:33, 584.52it/s] 57%|█████▋    | 26317/45920 [00:43<00:31, 625.32it/s] 57%|█████▋    | 26386/45920 [00:43<00:30, 631.00it/s] 58%|█████▊    | 26452/45920 [00:43<00:30, 631.66it/s] 58%|█████▊    | 26525/45920 [00:43<00:30, 627.22it/s] 58%|█████▊    | 26596/45920 [00:43<00:29, 646.28it/s] 58%|█████▊    | 26676/45920 [00:43<00:28, 685.43it/s] 58%|█████▊    | 26748/45920 [00:43<00:27, 690.29it/s] 58%|█████▊    | 26818/45920 [00:43<00:27, 689.01it/s] 59%|█████▊    | 26888/45920 [00:44<00:31, 610.84it/s] 59%|█████▊    | 26951/45920 [00:44<00:31, 611.33it/s] 59%|█████▉    | 27031/45920 [00:44<00:28, 663.14it/s] 59%|█████▉    | 27104/45920 [00:44<00:27, 681.04it/s] 59%|█████▉    | 27174/45920 [00:44<00:41, 453.05it/s] 59%|█████▉    | 27240/45920 [00:44<00:37, 495.49it/s] 59%|█████▉    | 27312/45920 [00:44<00:33, 547.52it/s] 60%|█████▉    | 27394/45920 [00:44<00:30, 613.51it/s] 60%|█████▉    | 27463/45920 [00:45<00:31, 594.47it/s] 60%|█████▉    | 27528/45920 [00:45<00:31, 588.66it/s] 60%|██████    | 27605/45920 [00:45<00:28, 633.41it/s] 60%|██████    | 27672/45920 [00:45<00:28, 641.61it/s] 60%|██████    | 27739/45920 [00:45<00:29, 620.36it/s] 61%|██████    | 27803/45920 [00:45<00:29, 613.24it/s] 61%|██████    | 27866/45920 [00:45<00:34, 517.72it/s] 61%|██████    | 27921/45920 [00:45<00:39, 457.31it/s] 61%|██████    | 28012/45920 [00:45<00:32, 554.86it/s] 61%|██████    | 28078/45920 [00:46<00:30, 581.07it/s] 61%|██████▏   | 28140/45920 [00:46<00:33, 525.00it/s] 61%|██████▏   | 28196/45920 [00:46<00:34, 521.25it/s] 62%|██████▏   | 28264/45920 [00:46<00:31, 561.14it/s] 62%|██████▏   | 28324/45920 [00:46<00:30, 570.72it/s] 62%|██████▏   | 28394/45920 [00:46<00:28, 605.94it/s] 62%|██████▏   | 28474/45920 [00:46<00:26, 657.62it/s] 62%|██████▏   | 28541/45920 [00:46<00:27, 634.86it/s] 62%|██████▏   | 28615/45920 [00:46<00:26, 650.75it/s] 62%|██████▏   | 28681/45920 [00:47<00:30, 566.72it/s] 63%|██████▎   | 28744/45920 [00:47<00:29, 581.96it/s] 63%|██████▎   | 28821/45920 [00:47<00:27, 632.22it/s] 63%|██████▎   | 28893/45920 [00:47<00:26, 641.97it/s] 63%|██████▎   | 28967/45920 [00:47<00:25, 668.95it/s] 63%|██████▎   | 29042/45920 [00:47<00:24, 687.60it/s] 63%|██████▎   | 29112/45920 [00:47<00:24, 684.14it/s] 64%|██████▎   | 29181/45920 [00:47<00:25, 662.74it/s] 64%|██████▎   | 29249/45920 [00:47<00:25, 666.72it/s] 64%|██████▍   | 29317/45920 [00:48<00:31, 533.72it/s] 64%|██████▍   | 29375/45920 [00:48<00:33, 498.87it/s] 64%|██████▍   | 29453/45920 [00:48<00:29, 566.18it/s] 64%|██████▍   | 29517/45920 [00:48<00:28, 574.65it/s] 64%|██████▍   | 29596/45920 [00:48<00:25, 628.08it/s] 65%|██████▍   | 29662/45920 [00:48<00:25, 635.17it/s] 65%|██████▍   | 29729/45920 [00:48<00:25, 644.26it/s] 65%|██████▍   | 29795/45920 [00:48<00:27, 585.00it/s] 65%|██████▌   | 29857/45920 [00:49<00:27, 591.50it/s] 65%|██████▌   | 29918/45920 [00:49<00:27, 575.27it/s] 65%|██████▌   | 29993/45920 [00:49<00:26, 610.00it/s] 65%|██████▌   | 30055/45920 [00:49<00:26, 604.62it/s] 66%|██████▌   | 30129/45920 [00:49<00:24, 641.20it/s] 66%|██████▌   | 30200/45920 [00:49<00:24, 647.72it/s] 66%|██████▌   | 30266/45920 [00:49<00:28, 556.93it/s] 66%|██████▌   | 30337/45920 [00:49<00:26, 594.89it/s] 66%|██████▌   | 30399/45920 [00:49<00:26, 584.25it/s] 66%|██████▋   | 30482/45920 [00:50<00:23, 645.49it/s] 67%|██████▋   | 30562/45920 [00:50<00:22, 685.36it/s] 67%|██████▋   | 30635/45920 [00:50<00:21, 697.02it/s] 67%|██████▋   | 30706/45920 [00:50<00:22, 665.65it/s] 67%|██████▋   | 30780/45920 [00:50<00:22, 672.00it/s] 67%|██████▋   | 30861/45920 [00:50<00:21, 710.91it/s] 67%|██████▋   | 30933/45920 [00:50<00:21, 699.22it/s] 68%|██████▊   | 31004/45920 [00:50<00:21, 700.18it/s] 68%|██████▊   | 31087/45920 [00:50<00:21, 679.85it/s] 68%|██████▊   | 31167/45920 [00:51<00:20, 712.67it/s] 68%|██████▊   | 31239/45920 [00:51<00:21, 691.45it/s] 68%|██████▊   | 31309/45920 [00:51<00:21, 678.24it/s] 68%|██████▊   | 31383/45920 [00:51<00:20, 694.96it/s] 69%|██████▊   | 31465/45920 [00:51<00:19, 729.19it/s] 69%|██████▊   | 31539/45920 [00:51<00:30, 466.40it/s] 69%|██████▉   | 31611/45920 [00:51<00:27, 512.13it/s] 69%|██████▉   | 31673/45920 [00:51<00:27, 514.27it/s] 69%|██████▉   | 31732/45920 [00:52<00:27, 516.17it/s] 69%|██████▉   | 31797/45920 [00:52<00:28, 488.12it/s] 69%|██████▉   | 31869/45920 [00:52<00:25, 541.75it/s] 70%|██████▉   | 31928/45920 [00:52<00:25, 540.84it/s] 70%|██████▉   | 32001/45920 [00:52<00:23, 589.65it/s] 70%|██████▉   | 32083/45920 [00:52<00:21, 649.40it/s] 70%|███████   | 32151/45920 [00:52<00:21, 630.02it/s] 70%|███████   | 32216/45920 [00:52<00:22, 599.34it/s] 70%|███████   | 32303/45920 [00:52<00:20, 671.12it/s] 70%|███████   | 32372/45920 [00:53<00:21, 638.46it/s] 71%|███████   | 32444/45920 [00:53<00:20, 660.24it/s] 71%|███████   | 32525/45920 [00:53<00:19, 699.71it/s] 71%|███████   | 32597/45920 [00:53<00:20, 644.11it/s] 71%|███████   | 32678/45920 [00:53<00:19, 688.14it/s] 71%|███████▏  | 32749/45920 [00:53<00:23, 557.21it/s] 71%|███████▏  | 32814/45920 [00:53<00:22, 579.42it/s] 72%|███████▏  | 32890/45920 [00:53<00:20, 623.37it/s] 72%|███████▏  | 32965/45920 [00:54<00:19, 649.45it/s] 72%|███████▏  | 33033/45920 [00:54<00:19, 653.57it/s] 72%|███████▏  | 33107/45920 [00:54<00:18, 675.30it/s] 72%|███████▏  | 33176/45920 [00:54<00:18, 671.15it/s] 72%|███████▏  | 33255/45920 [00:54<00:18, 701.47it/s] 73%|███████▎  | 33342/45920 [00:54<00:16, 749.69it/s] 73%|███████▎  | 33418/45920 [00:54<00:16, 737.52it/s] 73%|███████▎  | 33498/45920 [00:54<00:16, 755.21it/s] 73%|███████▎  | 33575/45920 [00:54<00:16, 758.40it/s] 73%|███████▎  | 33652/45920 [00:55<00:19, 617.69it/s] 73%|███████▎  | 33719/45920 [00:55<00:19, 614.14it/s] 74%|███████▎  | 33803/45920 [00:55<00:18, 672.06it/s] 74%|███████▍  | 33874/45920 [00:55<00:18, 650.82it/s] 74%|███████▍  | 33943/45920 [00:55<00:18, 660.88it/s] 74%|███████▍  | 34015/45920 [00:55<00:17, 675.95it/s] 74%|███████▍  | 34085/45920 [00:55<00:17, 682.22it/s] 74%|███████▍  | 34155/45920 [00:55<00:18, 625.48it/s] 75%|███████▍  | 34220/45920 [00:55<00:21, 547.69it/s] 75%|███████▍  | 34285/45920 [00:56<00:20, 571.80it/s] 75%|███████▍  | 34354/45920 [00:56<00:19, 602.47it/s] 75%|███████▍  | 34417/45920 [00:56<00:19, 587.04it/s] 75%|███████▌  | 34480/45920 [00:56<00:19, 594.69it/s] 75%|███████▌  | 34558/45920 [00:56<00:17, 644.64it/s] 75%|███████▌  | 34632/45920 [00:56<00:16, 667.40it/s] 76%|███████▌  | 34700/45920 [00:56<00:19, 562.91it/s] 76%|███████▌  | 34790/45920 [00:56<00:17, 649.25it/s] 76%|███████▌  | 34859/45920 [00:56<00:16, 656.26it/s] 76%|███████▌  | 34928/45920 [00:57<00:17, 642.84it/s] 76%|███████▌  | 34995/45920 [00:57<00:17, 629.29it/s] 76%|███████▋  | 35060/45920 [00:57<00:19, 544.19it/s] 76%|███████▋  | 35118/45920 [00:57<00:20, 535.64it/s] 77%|███████▋  | 35174/45920 [00:57<00:20, 512.32it/s] 77%|███████▋  | 35243/45920 [00:57<00:19, 557.76it/s] 77%|███████▋  | 35329/45920 [00:57<00:16, 631.41it/s] 77%|███████▋  | 35394/45920 [00:57<00:18, 570.48it/s] 77%|███████▋  | 35465/45920 [00:57<00:17, 606.66it/s] 77%|███████▋  | 35528/45920 [00:58<00:17, 601.55it/s] 78%|███████▊  | 35590/45920 [00:58<00:17, 577.28it/s] 78%|███████▊  | 35674/45920 [00:58<00:15, 647.16it/s] 78%|███████▊  | 35741/45920 [00:58<00:16, 632.20it/s] 78%|███████▊  | 35806/45920 [00:58<00:16, 624.83it/s] 78%|███████▊  | 35876/45920 [00:58<00:15, 639.64it/s] 78%|███████▊  | 35947/45920 [00:58<00:15, 656.18it/s] 78%|███████▊  | 36014/45920 [00:58<00:15, 654.54it/s] 79%|███████▊  | 36081/45920 [00:58<00:15, 653.47it/s] 79%|███████▊  | 36160/45920 [00:59<00:14, 692.70it/s] 79%|███████▉  | 36230/45920 [00:59<00:16, 593.63it/s] 79%|███████▉  | 36302/45920 [00:59<00:15, 620.64it/s] 79%|███████▉  | 36367/45920 [00:59<00:16, 563.95it/s] 79%|███████▉  | 36426/45920 [00:59<00:16, 560.48it/s] 80%|███████▉  | 36508/45920 [00:59<00:15, 624.89it/s] 80%|███████▉  | 36573/45920 [00:59<00:15, 601.18it/s] 80%|███████▉  | 36651/45920 [00:59<00:14, 646.15it/s] 80%|███████▉  | 36717/45920 [01:00<00:17, 520.56it/s] 80%|████████  | 36794/45920 [01:00<00:16, 568.96it/s] 80%|████████  | 36855/45920 [01:00<00:16, 563.52it/s] 80%|████████  | 36932/45920 [01:00<00:14, 615.97it/s] 81%|████████  | 36997/45920 [01:00<00:14, 609.63it/s] 81%|████████  | 37060/45920 [01:00<00:16, 546.11it/s] 81%|████████  | 37155/45920 [01:00<00:13, 638.92it/s] 81%|████████  | 37222/45920 [01:00<00:13, 642.80it/s] 81%|████████  | 37292/45920 [01:00<00:13, 653.01it/s] 81%|████████▏ | 37359/45920 [01:01<00:13, 620.95it/s] 82%|████████▏ | 37432/45920 [01:01<00:13, 650.04it/s] 82%|████████▏ | 37499/45920 [01:01<00:12, 651.84it/s] 82%|████████▏ | 37565/45920 [01:01<00:12, 645.22it/s] 82%|████████▏ | 37631/45920 [01:01<00:12, 645.66it/s] 82%|████████▏ | 37696/45920 [01:01<00:14, 566.44it/s] 82%|████████▏ | 37775/45920 [01:01<00:13, 625.31it/s] 82%|████████▏ | 37853/45920 [01:01<00:12, 667.29it/s] 83%|████████▎ | 37924/45920 [01:01<00:11, 677.29it/s] 83%|████████▎ | 37998/45920 [01:02<00:12, 631.24it/s] 83%|████████▎ | 38063/45920 [01:02<00:13, 585.51it/s] 83%|████████▎ | 38124/45920 [01:02<00:13, 579.74it/s] 83%|████████▎ | 38184/45920 [01:02<00:14, 546.29it/s] 83%|████████▎ | 38240/45920 [01:02<00:14, 539.86it/s] 83%|████████▎ | 38312/45920 [01:02<00:13, 583.42it/s] 84%|████████▎ | 38380/45920 [01:02<00:12, 607.94it/s] 84%|████████▎ | 38448/45920 [01:02<00:11, 623.44it/s] 84%|████████▍ | 38515/45920 [01:02<00:11, 636.04it/s] 84%|████████▍ | 38580/45920 [01:03<00:12, 565.82it/s] 84%|████████▍ | 38639/45920 [01:03<00:13, 524.56it/s] 84%|████████▍ | 38693/45920 [01:03<00:13, 525.14it/s] 84%|████████▍ | 38747/45920 [01:03<00:13, 523.53it/s] 84%|████████▍ | 38801/45920 [01:03<00:13, 510.10it/s] 85%|████████▍ | 38880/45920 [01:03<00:12, 584.01it/s] 85%|████████▍ | 38940/45920 [01:03<00:12, 538.15it/s] 85%|████████▍ | 39002/45920 [01:03<00:12, 559.02it/s] 85%|████████▌ | 39063/45920 [01:04<00:12, 567.39it/s] 85%|████████▌ | 39139/45920 [01:04<00:10, 621.19it/s] 85%|████████▌ | 39208/45920 [01:04<00:10, 639.55it/s] 86%|████████▌ | 39273/45920 [01:04<00:11, 574.64it/s] 86%|████████▌ | 39333/45920 [01:04<00:13, 500.39it/s] 86%|████████▌ | 39395/45920 [01:04<00:12, 526.09it/s] 86%|████████▌ | 39477/45920 [01:04<00:10, 601.97it/s] 86%|████████▌ | 39540/45920 [01:04<00:10, 604.17it/s] 86%|████████▋ | 39607/45920 [01:04<00:10, 593.56it/s] 86%|████████▋ | 39668/45920 [01:05<00:12, 500.60it/s] 87%|████████▋ | 39738/45920 [01:05<00:11, 544.25it/s] 87%|████████▋ | 39798/45920 [01:05<00:10, 557.84it/s] 87%|████████▋ | 39858/45920 [01:05<00:10, 565.48it/s] 87%|████████▋ | 39947/45920 [01:05<00:09, 654.95it/s] 87%|████████▋ | 40031/45920 [01:05<00:08, 706.17it/s] 87%|████████▋ | 40104/45920 [01:05<00:09, 616.08it/s] 87%|████████▋ | 40169/45920 [01:05<00:09, 615.03it/s] 88%|████████▊ | 40237/45920 [01:05<00:09, 609.15it/s] 88%|████████▊ | 40300/45920 [01:06<00:10, 529.78it/s] 88%|████████▊ | 40364/45920 [01:06<00:10, 552.32it/s] 88%|████████▊ | 40431/45920 [01:06<00:09, 571.71it/s] 88%|████████▊ | 40509/45920 [01:06<00:08, 627.26it/s] 88%|████████▊ | 40574/45920 [01:06<00:08, 629.55it/s] 89%|████████▊ | 40666/45920 [01:06<00:07, 708.02it/s] 89%|████████▊ | 40739/45920 [01:06<00:07, 655.98it/s] 89%|████████▉ | 40810/45920 [01:06<00:08, 602.83it/s] 89%|████████▉ | 40873/45920 [01:07<00:08, 590.50it/s] 89%|████████▉ | 40945/45920 [01:07<00:08, 604.93it/s] 89%|████████▉ | 41015/45920 [01:07<00:07, 629.56it/s] 89%|████████▉ | 41096/45920 [01:07<00:07, 676.23it/s] 90%|████████▉ | 41165/45920 [01:07<00:07, 628.32it/s] 90%|████████▉ | 41230/45920 [01:07<00:07, 631.68it/s] 90%|████████▉ | 41295/45920 [01:07<00:08, 563.42it/s] 90%|█████████ | 41354/45920 [01:07<00:08, 562.82it/s] 90%|█████████ | 41419/45920 [01:07<00:07, 585.13it/s] 90%|█████████ | 41486/45920 [01:08<00:07, 604.38it/s] 90%|█████████ | 41548/45920 [01:08<00:07, 580.38it/s] 91%|█████████ | 41609/45920 [01:08<00:07, 588.28it/s] 91%|█████████ | 41691/45920 [01:08<00:06, 652.27it/s] 91%|█████████ | 41760/45920 [01:08<00:06, 663.07it/s] 91%|█████████ | 41857/45920 [01:08<00:05, 752.32it/s] 91%|█████████▏| 41948/45920 [01:08<00:04, 798.52it/s] 92%|█████████▏| 42029/45920 [01:08<00:05, 767.21it/s] 92%|█████████▏| 42107/45920 [01:08<00:05, 727.53it/s] 92%|█████████▏| 42181/45920 [01:09<00:06, 611.61it/s] 92%|█████████▏| 42246/45920 [01:09<00:06, 580.55it/s] 92%|█████████▏| 42317/45920 [01:09<00:05, 611.75it/s] 92%|█████████▏| 42381/45920 [01:09<00:05, 595.32it/s] 92%|█████████▏| 42458/45920 [01:09<00:05, 640.17it/s] 93%|█████████▎| 42538/45920 [01:09<00:04, 683.09it/s] 93%|█████████▎| 42608/45920 [01:09<00:04, 680.71it/s] 93%|█████████▎| 42678/45920 [01:09<00:04, 649.47it/s] 93%|█████████▎| 42744/45920 [01:09<00:05, 566.83it/s] 93%|█████████▎| 42814/45920 [01:10<00:05, 599.77it/s] 93%|█████████▎| 42889/45920 [01:10<00:04, 639.39it/s] 94%|█████████▎| 42955/45920 [01:10<00:05, 588.28it/s] 94%|█████████▎| 43016/45920 [01:10<00:05, 569.52it/s] 94%|█████████▍| 43075/45920 [01:10<00:05, 513.52it/s] 94%|█████████▍| 43140/45920 [01:10<00:05, 546.73it/s] 94%|█████████▍| 43205/45920 [01:10<00:04, 572.65it/s] 94%|█████████▍| 43279/45920 [01:10<00:04, 618.28it/s] 94%|█████████▍| 43346/45920 [01:10<00:04, 630.89it/s] 95%|█████████▍| 43411/45920 [01:11<00:04, 618.14it/s] 95%|█████████▍| 43474/45920 [01:11<00:04, 586.16it/s] 95%|█████████▍| 43535/45920 [01:11<00:04, 536.13it/s] 95%|█████████▍| 43604/45920 [01:11<00:04, 576.65it/s] 95%|█████████▌| 43686/45920 [01:11<00:03, 639.96it/s] 95%|█████████▌| 43767/45920 [01:11<00:03, 686.28it/s] 95%|█████████▌| 43838/45920 [01:11<00:03, 669.87it/s] 96%|█████████▌| 43917/45920 [01:11<00:02, 702.22it/s] 96%|█████████▌| 43989/45920 [01:11<00:02, 701.51it/s] 96%|█████████▌| 44060/45920 [01:12<00:02, 685.41it/s] 96%|█████████▌| 44138/45920 [01:12<00:02, 666.57it/s] 96%|█████████▋| 44206/45920 [01:12<00:03, 510.25it/s] 96%|█████████▋| 44280/45920 [01:12<00:02, 560.03it/s] 97%|█████████▋| 44348/45920 [01:12<00:02, 589.23it/s] 97%|█████████▋| 44417/45920 [01:12<00:02, 613.33it/s] 97%|█████████▋| 44497/45920 [01:12<00:02, 663.10it/s] 97%|█████████▋| 44571/45920 [01:12<00:01, 684.36it/s] 97%|█████████▋| 44642/45920 [01:13<00:01, 659.30it/s] 97%|█████████▋| 44710/45920 [01:13<00:02, 585.34it/s] 98%|█████████▊| 44772/45920 [01:13<00:02, 564.14it/s] 98%|█████████▊| 44840/45920 [01:13<00:01, 587.36it/s] 98%|█████████▊| 44901/45920 [01:13<00:01, 566.92it/s] 98%|█████████▊| 44959/45920 [01:13<00:01, 540.76it/s] 98%|█████████▊| 45021/45920 [01:13<00:01, 481.92it/s] 98%|█████████▊| 45084/45920 [01:13<00:01, 516.86it/s] 98%|█████████▊| 45152/45920 [01:14<00:01, 553.96it/s] 98%|█████████▊| 45227/45920 [01:14<00:01, 591.41it/s] 99%|█████████▊| 45288/45920 [01:14<00:01, 570.19it/s] 99%|█████████▉| 45362/45920 [01:14<00:00, 613.15it/s] 99%|█████████▉| 45437/45920 [01:14<00:00, 648.57it/s] 99%|█████████▉| 45516/45920 [01:14<00:00, 688.56it/s] 99%|█████████▉| 45586/45920 [01:14<00:00, 691.60it/s] 99%|█████████▉| 45656/45920 [01:14<00:00, 552.54it/s]100%|█████████▉| 45723/45920 [01:14<00:00, 577.72it/s]100%|█████████▉| 45805/45920 [01:15<00:00, 640.61it/s]100%|█████████▉| 45886/45920 [01:15<00:00, 680.62it/s]100%|██████████| 45920/45920 [01:15<00:00, 610.22it/s]

gathering stats for n=1
  0%|          | 0/45920 [00:00<?, ?it/s]  1%|          | 252/45920 [00:00<00:18, 2489.05it/s]  1%|          | 528/45920 [00:00<00:17, 2646.86it/s]  2%|▏         | 793/45920 [00:00<00:19, 2346.54it/s]  2%|▏         | 1072/45920 [00:00<00:17, 2508.22it/s]  3%|▎         | 1363/45920 [00:00<00:16, 2644.32it/s]  4%|▎         | 1655/45920 [00:00<00:16, 2734.07it/s]  4%|▍         | 1939/45920 [00:00<00:15, 2761.51it/s]  5%|▍         | 2217/45920 [00:00<00:16, 2710.59it/s]  5%|▌         | 2490/45920 [00:00<00:16, 2681.62it/s]  6%|▌         | 2768/45920 [00:01<00:15, 2697.98it/s]  7%|▋         | 3039/45920 [00:01<00:15, 2684.45it/s]  7%|▋         | 3308/45920 [00:01<00:16, 2656.24it/s]  8%|▊         | 3574/45920 [00:01<00:16, 2603.32it/s]  8%|▊         | 3835/45920 [00:01<00:17, 2457.43it/s]  9%|▉         | 4083/45920 [00:01<00:17, 2394.89it/s]  9%|▉         | 4324/45920 [00:01<00:18, 2290.14it/s] 10%|█         | 4600/45920 [00:01<00:17, 2411.12it/s] 11%|█         | 4888/45920 [00:01<00:16, 2543.81it/s] 11%|█         | 5148/45920 [00:02<00:15, 2557.59it/s] 12%|█▏        | 5418/45920 [00:02<00:15, 2598.42it/s] 12%|█▏        | 5679/45920 [00:02<00:16, 2506.80it/s] 13%|█▎        | 5934/45920 [00:02<00:16, 2484.06it/s] 13%|█▎        | 6189/45920 [00:02<00:15, 2500.94it/s] 14%|█▍        | 6486/45920 [00:02<00:14, 2637.45it/s] 15%|█▍        | 6751/45920 [00:02<00:14, 2614.58it/s] 15%|█▌        | 7037/45920 [00:02<00:14, 2684.57it/s] 16%|█▌        | 7307/45920 [00:02<00:14, 2671.79it/s] 16%|█▋        | 7575/45920 [00:02<00:15, 2520.20it/s] 17%|█▋        | 7829/45920 [00:03<00:16, 2377.60it/s] 18%|█▊        | 8096/45920 [00:03<00:15, 2457.75it/s] 18%|█▊        | 8384/45920 [00:03<00:14, 2576.67it/s] 19%|█▉        | 8645/45920 [00:03<00:14, 2568.67it/s] 19%|█▉        | 8904/45920 [00:03<00:14, 2506.58it/s] 20%|█▉        | 9157/45920 [00:03<00:14, 2511.59it/s] 21%|██        | 9420/45920 [00:03<00:14, 2533.03it/s] 21%|██        | 9675/45920 [00:03<00:14, 2505.71it/s] 22%|██▏       | 9927/45920 [00:03<00:14, 2476.16it/s] 22%|██▏       | 10175/45920 [00:04<00:14, 2418.94it/s] 23%|██▎       | 10455/45920 [00:04<00:14, 2528.48it/s] 23%|██▎       | 10714/45920 [00:04<00:13, 2542.31it/s] 24%|██▍       | 10984/45920 [00:04<00:13, 2588.36it/s] 24%|██▍       | 11244/45920 [00:04<00:13, 2586.99it/s] 25%|██▌       | 11504/45920 [00:04<00:13, 2582.76it/s] 26%|██▌       | 11763/45920 [00:04<00:13, 2485.92it/s] 26%|██▌       | 12013/45920 [00:04<00:13, 2434.51it/s] 27%|██▋       | 12261/45920 [00:04<00:13, 2438.39it/s] 27%|██▋       | 12558/45920 [00:04<00:12, 2579.84it/s] 28%|██▊       | 12817/45920 [00:05<00:13, 2536.62it/s] 28%|██▊       | 13079/45920 [00:05<00:12, 2557.79it/s] 29%|██▉       | 13384/45920 [00:05<00:12, 2701.46it/s] 30%|██▉       | 13655/45920 [00:05<00:12, 2633.75it/s] 30%|███       | 13932/45920 [00:05<00:11, 2671.47it/s] 31%|███       | 14200/45920 [00:05<00:12, 2515.41it/s] 32%|███▏      | 14469/45920 [00:05<00:12, 2559.41it/s] 32%|███▏      | 14727/45920 [00:05<00:12, 2464.94it/s] 33%|███▎      | 15005/45920 [00:05<00:12, 2531.99it/s] 33%|███▎      | 15267/45920 [00:05<00:11, 2556.40it/s] 34%|███▍      | 15524/45920 [00:06<00:12, 2426.61it/s] 34%|███▍      | 15779/45920 [00:06<00:12, 2460.91it/s] 35%|███▍      | 16027/45920 [00:06<00:12, 2433.89it/s] 35%|███▌      | 16272/45920 [00:06<00:12, 2369.30it/s] 36%|███▌      | 16528/45920 [00:06<00:12, 2421.01it/s] 37%|███▋      | 16800/45920 [00:06<00:11, 2507.40it/s] 37%|███▋      | 17052/45920 [00:06<00:11, 2426.26it/s] 38%|███▊      | 17340/45920 [00:06<00:11, 2556.30it/s] 38%|███▊      | 17656/45920 [00:06<00:10, 2729.25it/s] 39%|███▉      | 17931/45920 [00:07<00:10, 2691.76it/s] 40%|███▉      | 18202/45920 [00:07<00:10, 2690.19it/s] 40%|████      | 18472/45920 [00:07<00:10, 2650.79it/s] 41%|████      | 18738/45920 [00:07<00:11, 2447.59it/s] 41%|████▏     | 19035/45920 [00:07<00:10, 2588.31it/s] 42%|████▏     | 19298/45920 [00:07<00:10, 2596.99it/s] 43%|████▎     | 19567/45920 [00:07<00:10, 2622.22it/s] 43%|████▎     | 19850/45920 [00:07<00:09, 2680.09it/s] 44%|████▍     | 20120/45920 [00:07<00:10, 2553.65it/s] 44%|████▍     | 20403/45920 [00:07<00:09, 2632.22it/s] 45%|████▌     | 20680/45920 [00:08<00:09, 2671.29it/s] 46%|████▌     | 20949/45920 [00:08<00:09, 2627.81it/s] 46%|████▌     | 21213/45920 [00:08<00:09, 2499.44it/s] 47%|████▋     | 21465/45920 [00:08<00:10, 2443.82it/s] 47%|████▋     | 21732/45920 [00:08<00:09, 2507.60it/s] 48%|████▊     | 21985/45920 [00:08<00:09, 2452.92it/s] 48%|████▊     | 22263/45920 [00:08<00:09, 2543.83it/s] 49%|████▉     | 22563/45920 [00:08<00:08, 2674.55it/s] 50%|████▉     | 22832/45920 [00:08<00:08, 2672.28it/s] 50%|█████     | 23117/45920 [00:09<00:08, 2715.22it/s] 51%|█████     | 23390/45920 [00:09<00:08, 2716.45it/s] 52%|█████▏    | 23663/45920 [00:09<00:08, 2518.22it/s] 52%|█████▏    | 23929/45920 [00:09<00:08, 2556.13it/s] 53%|█████▎    | 24189/45920 [00:09<00:08, 2567.39it/s] 53%|█████▎    | 24448/45920 [00:09<00:08, 2546.92it/s] 54%|█████▍    | 24754/45920 [00:09<00:07, 2694.88it/s] 54%|█████▍    | 25025/45920 [00:09<00:07, 2687.72it/s] 55%|█████▌    | 25295/45920 [00:09<00:07, 2678.15it/s] 56%|█████▌    | 25586/45920 [00:09<00:07, 2736.24it/s] 56%|█████▋    | 25863/45920 [00:10<00:07, 2745.28it/s] 57%|█████▋    | 26138/45920 [00:10<00:07, 2664.91it/s] 58%|█████▊    | 26416/45920 [00:10<00:07, 2696.61it/s] 58%|█████▊    | 26687/45920 [00:10<00:07, 2664.98it/s] 59%|█████▊    | 26954/45920 [00:10<00:07, 2646.75it/s] 59%|█████▉    | 27219/45920 [00:10<00:07, 2473.34it/s] 60%|█████▉    | 27488/45920 [00:10<00:07, 2517.94it/s] 60%|██████    | 27754/45920 [00:10<00:07, 2539.69it/s] 61%|██████    | 28010/45920 [00:10<00:07, 2442.62it/s] 62%|██████▏   | 28256/45920 [00:11<00:07, 2418.90it/s] 62%|██████▏   | 28531/45920 [00:11<00:06, 2511.50it/s] 63%|██████▎   | 28793/45920 [00:11<00:06, 2542.10it/s] 63%|██████▎   | 29058/45920 [00:11<00:06, 2573.24it/s] 64%|██████▍   | 29317/45920 [00:11<00:06, 2517.79it/s] 64%|██████▍   | 29570/45920 [00:11<00:06, 2520.83it/s] 65%|██████▍   | 29830/45920 [00:11<00:06, 2542.45it/s] 66%|██████▌   | 30085/45920 [00:11<00:06, 2517.02it/s] 66%|██████▌   | 30349/45920 [00:11<00:06, 2539.71it/s] 67%|██████▋   | 30620/45920 [00:11<00:05, 2587.27it/s] 67%|██████▋   | 30895/45920 [00:12<00:05, 2633.20it/s] 68%|██████▊   | 31186/45920 [00:12<00:05, 2690.98it/s] 69%|██████▊   | 31466/45920 [00:12<00:05, 2722.20it/s] 69%|██████▉   | 31739/45920 [00:12<00:05, 2473.21it/s] 70%|██████▉   | 31991/45920 [00:12<00:05, 2482.88it/s] 70%|███████   | 32307/45920 [00:12<00:05, 2674.06it/s] 71%|███████   | 32578/45920 [00:12<00:05, 2664.93it/s] 72%|███████▏  | 32847/45920 [00:12<00:04, 2645.81it/s] 72%|███████▏  | 33134/45920 [00:12<00:04, 2706.58it/s] 73%|███████▎  | 33428/45920 [00:13<00:04, 2763.61it/s] 73%|███████▎  | 33706/45920 [00:13<00:04, 2753.76it/s] 74%|███████▍  | 33983/45920 [00:13<00:04, 2713.06it/s] 75%|███████▍  | 34255/45920 [00:13<00:04, 2594.34it/s] 75%|███████▌  | 34516/45920 [00:13<00:04, 2590.41it/s] 76%|███████▌  | 34812/45920 [00:13<00:04, 2681.01it/s] 76%|███████▋  | 35081/45920 [00:13<00:04, 2518.21it/s] 77%|███████▋  | 35342/45920 [00:13<00:04, 2543.08it/s] 78%|███████▊  | 35599/45920 [00:13<00:04, 2491.88it/s] 78%|███████▊  | 35871/45920 [00:13<00:03, 2547.54it/s] 79%|███████▊  | 36152/45920 [00:14<00:03, 2613.39it/s] 79%|███████▉  | 36415/45920 [00:14<00:03, 2511.75it/s] 80%|███████▉  | 36674/45920 [00:14<00:03, 2525.85it/s] 80%|████████  | 36933/45920 [00:14<00:03, 2540.91it/s] 81%|████████  | 37207/45920 [00:14<00:03, 2598.11it/s] 82%|████████▏ | 37468/45920 [00:14<00:03, 2564.21it/s] 82%|████████▏ | 37725/45920 [00:14<00:03, 2551.35it/s] 83%|████████▎ | 37998/45920 [00:14<00:03, 2572.34it/s] 83%|████████▎ | 38256/45920 [00:14<00:03, 2449.94it/s] 84%|████████▍ | 38512/45920 [00:15<00:02, 2481.04it/s] 84%|████████▍ | 38762/45920 [00:15<00:03, 2374.45it/s] 85%|████████▍ | 39003/45920 [00:15<00:02, 2381.61it/s] 85%|████████▌ | 39246/45920 [00:15<00:02, 2394.12it/s] 86%|████████▌ | 39487/45920 [00:15<00:02, 2393.33it/s] 87%|████████▋ | 39727/45920 [00:15<00:02, 2357.67it/s] 87%|████████▋ | 39998/45920 [00:15<00:02, 2458.18it/s] 88%|████████▊ | 40245/45920 [00:15<00:02, 2408.58it/s] 88%|████████▊ | 40500/45920 [00:15<00:02, 2446.48it/s] 89%|████████▉ | 40788/45920 [00:15<00:01, 2568.86it/s] 89%|████████▉ | 41046/45920 [00:16<00:01, 2559.99it/s] 90%|████████▉ | 41303/45920 [00:16<00:01, 2535.50it/s] 90%|█████████ | 41557/45920 [00:16<00:01, 2491.76it/s] 91%|█████████ | 41876/45920 [00:16<00:01, 2695.33it/s] 92%|█████████▏| 42147/45920 [00:16<00:01, 2659.46it/s] 92%|█████████▏| 42414/45920 [00:16<00:01, 2628.98it/s] 93%|█████████▎| 42693/45920 [00:16<00:01, 2675.63it/s] 94%|█████████▎| 42961/45920 [00:16<00:01, 2636.20it/s] 94%|█████████▍| 43225/45920 [00:16<00:01, 2561.88it/s] 95%|█████████▍| 43482/45920 [00:16<00:00, 2524.06it/s] 95%|█████████▌| 43772/45920 [00:17<00:00, 2631.21it/s] 96%|█████████▌| 44044/45920 [00:17<00:00, 2655.22it/s] 96%|█████████▋| 44311/45920 [00:17<00:00, 2563.39it/s] 97%|█████████▋| 44575/45920 [00:17<00:00, 2583.60it/s] 98%|█████████▊| 44835/45920 [00:17<00:00, 2483.20it/s] 98%|█████████▊| 45085/45920 [00:17<00:00, 2332.99it/s] 99%|█████████▊| 45337/45920 [00:17<00:00, 2383.96it/s] 99%|█████████▉| 45606/45920 [00:17<00:00, 2458.78it/s]100%|█████████▉| 45900/45920 [00:17<00:00, 2517.57it/s]100%|██████████| 45920/45920 [00:17<00:00, 2558.09it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 43.89it/s]2022-03-15 14:18:58 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(34528, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=34528, bias=False)
  )
)
2022-03-15 14:18:58 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 14:18:58 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 14:18:58 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 14:18:58 | INFO | fairseq_cli.train | num. shared model params: 36,592,640 (num. trained: 36,592,640)
2022-03-15 14:18:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 14:18:58 | INFO | fairseq.data.data_utils | loaded 2,294 examples from: data-bin/de/valid
2022-03-15 14:18:58 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 14:18:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 14:18:58 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 14:18:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 14:18:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 14:18:58 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 14:18:58 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-15 14:18:58 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-15 14:18:58 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 14:18:58 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
2022-03-15 14:18:58 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 14:18:58 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 14:19:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 14:19:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 14:19:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 14:19:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 14:25:42 | INFO | train_inner | epoch 001:    104 / 392 loss=14.681, ppl=26263.8, wps=17333, ups=0.26, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.193, loss_scale=8, train_wall=374, gb_free=9.6, wall=405
2022-03-15 14:32:01 | INFO | train_inner | epoch 001:    204 / 392 loss=13.106, ppl=8818.34, wps=17331.7, ups=0.26, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.739, loss_scale=16, train_wall=349, gb_free=9.6, wall=783
2022-03-15 14:38:14 | INFO | train_inner | epoch 001:    304 / 392 loss=12.143, ppl=4523.22, wps=17555.7, ups=0.27, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.464, loss_scale=32, train_wall=345, gb_free=9.6, wall=1156
2022-03-15 14:43:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 14:44:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.485 | ppl 2866.5 | wps 28920.9 | wpb 511.9 | bsz 1 | num_updates 388
2022-03-15 14:44:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-15 14:44:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 14:44:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 14:44:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.485) (writing took 2.5294596910243854 seconds)
2022-03-15 14:44:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 14:44:22 | INFO | train | epoch 001 | loss 12.939 | ppl 7852.02 | wps 16940.3 | ups 0.26 | wpb 65404.5 | bsz 127.7 | num_updates 388 | lr 4.85903e-05 | gnorm 0.964 | loss_scale 64 | train_wall 1364 | gb_free 9.6 | wall 1525
KL Stats: Epoch 1 Divergences: Uniform: 0.7153728208180513 Unigram: 0.8894846699291238
2022-03-15 14:44:22 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 14:44:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 14:45:07 | INFO | train_inner | epoch 002:     12 / 392 loss=11.647, ppl=3207.21, wps=15748.5, ups=0.24, wpb=65022.4, bsz=127, num_updates=400, lr=5.009e-05, gnorm=0.392, loss_scale=64, train_wall=338, gb_free=9.6, wall=1569
2022-03-15 14:49:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 14:51:22 | INFO | train_inner | epoch 002:    113 / 392 loss=11.415, ppl=2729.85, wps=17479, ups=0.27, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.375, loss_scale=32, train_wall=346, gb_free=9.6, wall=1944
2022-03-15 14:57:38 | INFO | train_inner | epoch 002:    213 / 392 loss=11.162, ppl=2291.6, wps=17400.3, ups=0.27, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.378, loss_scale=64, train_wall=348, gb_free=9.6, wall=2321
2022-03-15 15:02:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 15:03:58 | INFO | train_inner | epoch 002:    314 / 392 loss=10.869, ppl=1869.62, wps=17263.2, ups=0.26, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.438, loss_scale=32, train_wall=351, gb_free=9.6, wall=2700
2022-03-15 15:08:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 15:09:30 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.356 | ppl 1310.94 | wps 28826.1 | wpb 511.9 | bsz 1 | num_updates 778 | best_loss 10.356
2022-03-15 15:09:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 778 updates
2022-03-15 15:09:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 15:09:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 15:09:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 2 @ 778 updates, score 10.356) (writing took 2.6137803239980713 seconds)
2022-03-15 15:09:33 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 15:09:33 | INFO | train | epoch 002 | loss 11.055 | ppl 2127.55 | wps 16884.8 | ups 0.26 | wpb 65405.2 | bsz 127.7 | num_updates 778 | lr 9.73306e-05 | gnorm 0.405 | loss_scale 32 | train_wall 1351 | gb_free 9.6 | wall 3035
KL Stats: Epoch 2 Divergences: Uniform: 1.6029270874500128 Unigram: 0.533788023258917
2022-03-15 15:09:33 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 15:09:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 15:10:54 | INFO | train_inner | epoch 003:     22 / 392 loss=10.586, ppl=1537.36, wps=15618.4, ups=0.24, wpb=65029.1, bsz=127, num_updates=800, lr=0.00010008, gnorm=0.439, loss_scale=32, train_wall=341, gb_free=9.6, wall=3117
2022-03-15 15:12:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 15:17:05 | INFO | train_inner | epoch 003:    123 / 392 loss=10.338, ppl=1293.97, wps=17664.9, ups=0.27, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.477, loss_scale=32, train_wall=343, gb_free=9.6, wall=3488
2022-03-15 15:20:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 15:23:18 | INFO | train_inner | epoch 003:    224 / 392 loss=10.127, ppl=1118.57, wps=17580.5, ups=0.27, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.516, loss_scale=32, train_wall=344, gb_free=9.6, wall=3860
2022-03-15 15:29:31 | INFO | train_inner | epoch 003:    324 / 392 loss=9.935, ppl=978.87, wps=17568.8, ups=0.27, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.546, loss_scale=64, train_wall=344, gb_free=9.6, wall=4233
2022-03-15 15:29:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 15:33:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 15:34:25 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.578 | ppl 764.14 | wps 28655.5 | wpb 511.9 | bsz 1 | num_updates 1167 | best_loss 9.578
2022-03-15 15:34:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1167 updates
2022-03-15 15:34:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 15:34:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 15:34:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 3 @ 1167 updates, score 9.578) (writing took 2.559396324970294 seconds)
2022-03-15 15:34:28 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 15:34:28 | INFO | train | epoch 003 | loss 10.094 | ppl 1093.07 | wps 17020 | ups 0.26 | wpb 65404.8 | bsz 127.7 | num_updates 1167 | lr 0.000145946 | gnorm 0.521 | loss_scale 32 | train_wall 1336 | gb_free 9.6 | wall 4530
KL Stats: Epoch 3 Divergences: Uniform: 2.3083011758402248 Unigram: 1.2366646991746406
2022-03-15 15:34:28 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 15:34:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 15:36:31 | INFO | train_inner | epoch 004:     33 / 392 loss=9.76, ppl=867.33, wps=15486.1, ups=0.24, wpb=65025.8, bsz=127, num_updates=1200, lr=0.00015007, gnorm=0.568, loss_scale=32, train_wall=343, gb_free=9.6, wall=4653
2022-03-15 15:41:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 15:42:48 | INFO | train_inner | epoch 004:    134 / 392 loss=9.602, ppl=777.35, wps=17400.4, ups=0.27, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.615, loss_scale=32, train_wall=348, gb_free=9.6, wall=5030
2022-03-15 15:49:02 | INFO | train_inner | epoch 004:    234 / 392 loss=9.467, ppl=707.56, wps=17503.8, ups=0.27, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.636, loss_scale=32, train_wall=346, gb_free=9.6, wall=5404
2022-03-15 15:50:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 15:55:21 | INFO | train_inner | epoch 004:    335 / 392 loss=9.336, ppl=646.34, wps=17300.5, ups=0.26, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.638, loss_scale=32, train_wall=350, gb_free=9.6, wall=5783
2022-03-15 15:58:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 15:58:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 15:59:20 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.007 | ppl 514.48 | wps 30689.8 | wpb 511.9 | bsz 1 | num_updates 1556 | best_loss 9.007
2022-03-15 15:59:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1556 updates
2022-03-15 15:59:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 15:59:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 15:59:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 4 @ 1556 updates, score 9.007) (writing took 5.341869522002526 seconds)
2022-03-15 15:59:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 15:59:26 | INFO | train | epoch 004 | loss 9.453 | ppl 701.1 | wps 16986.6 | ups 0.26 | wpb 65404.8 | bsz 127.7 | num_updates 1556 | lr 0.000194561 | gnorm 0.626 | loss_scale 32 | train_wall 1339 | gb_free 9.6 | wall 6028
KL Stats: Epoch 4 Divergences: Uniform: 2.8712367624300867 Unigram: 1.630069753311372
2022-03-15 15:59:26 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 15:59:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 16:01:47 | INFO | train_inner | epoch 005:     44 / 392 loss=9.186, ppl=582.29, wps=16853.4, ups=0.26, wpb=65025.8, bsz=127, num_updates=1600, lr=0.00020006, gnorm=0.663, loss_scale=32, train_wall=312, gb_free=9.6, wall=6169
2022-03-15 16:07:12 | INFO | train_inner | epoch 005:    144 / 392 loss=9.044, ppl=528.01, wps=20147.5, ups=0.31, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.67, loss_scale=64, train_wall=300, gb_free=9.6, wall=6494
2022-03-15 16:07:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:12:43 | INFO | train_inner | epoch 005:    245 / 392 loss=8.912, ppl=481.75, wps=19771.2, ups=0.3, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.693, loss_scale=32, train_wall=306, gb_free=9.6, wall=6826
2022-03-15 16:16:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:18:47 | INFO | train_inner | epoch 005:    346 / 392 loss=8.786, ppl=441.45, wps=18032.6, ups=0.28, wpb=65532.7, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.682, loss_scale=32, train_wall=335, gb_free=9.6, wall=7189
2022-03-15 16:21:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 16:21:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.474 | ppl 355.52 | wps 31945 | wpb 511.9 | bsz 1 | num_updates 1946 | best_loss 8.474
2022-03-15 16:21:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1946 updates
2022-03-15 16:21:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 16:21:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 16:21:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 5 @ 1946 updates, score 8.474) (writing took 2.3480655000312254 seconds)
2022-03-15 16:21:56 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 16:21:56 | INFO | train | epoch 005 | loss 8.913 | ppl 481.94 | wps 18886.5 | ups 0.29 | wpb 65405.2 | bsz 127.7 | num_updates 1946 | lr 0.000243301 | gnorm 0.681 | loss_scale 32 | train_wall 1206 | gb_free 9.6 | wall 7379
KL Stats: Epoch 5 Divergences: Uniform: 3.301061868148313 Unigram: 1.9032075546436473
2022-03-15 16:21:56 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 16:21:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 16:24:56 | INFO | train_inner | epoch 006:     54 / 392 loss=8.644, ppl=399.97, wps=17624.2, ups=0.27, wpb=65029.1, bsz=127, num_updates=2000, lr=0.00025005, gnorm=0.68, loss_scale=64, train_wall=301, gb_free=9.6, wall=7558
2022-03-15 16:26:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:30:52 | INFO | train_inner | epoch 006:    155 / 392 loss=8.522, ppl=367.71, wps=18375.9, ups=0.28, wpb=65532.7, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.678, loss_scale=32, train_wall=329, gb_free=9.6, wall=7915
2022-03-15 16:34:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:36:55 | INFO | train_inner | epoch 006:    256 / 392 loss=8.423, ppl=343.27, wps=18058.6, ups=0.28, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.687, loss_scale=32, train_wall=335, gb_free=9.6, wall=8278
2022-03-15 16:42:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:42:50 | INFO | train_inner | epoch 006:    357 / 392 loss=8.327, ppl=321.17, wps=18478.1, ups=0.28, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.665, loss_scale=32, train_wall=328, gb_free=9.6, wall=8632
2022-03-15 16:44:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 16:45:36 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.067 | ppl 268.19 | wps 29546.3 | wpb 511.9 | bsz 1 | num_updates 2335 | best_loss 8.067
2022-03-15 16:45:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2335 updates
2022-03-15 16:45:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 16:45:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 16:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 6 @ 2335 updates, score 8.067) (writing took 2.5345526109449565 seconds)
2022-03-15 16:45:39 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 16:45:39 | INFO | train | epoch 006 | loss 8.436 | ppl 346.41 | wps 17883 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 2335 | lr 0.000291917 | gnorm 0.676 | loss_scale 32 | train_wall 1271 | gb_free 9.6 | wall 8801
KL Stats: Epoch 6 Divergences: Uniform: 3.6922236197341647 Unigram: 2.127270194240402
2022-03-15 16:45:39 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 16:45:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 16:49:32 | INFO | train_inner | epoch 007:     65 / 392 loss=8.219, ppl=298.03, wps=16189.9, ups=0.25, wpb=65029.1, bsz=127, num_updates=2400, lr=0.00030004, gnorm=0.661, loss_scale=32, train_wall=328, gb_free=9.6, wall=9034
2022-03-15 16:51:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:55:31 | INFO | train_inner | epoch 007:    166 / 392 loss=8.125, ppl=279.14, wps=18244.6, ups=0.28, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.637, loss_scale=32, train_wall=332, gb_free=9.6, wall=9393
2022-03-15 16:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:01:29 | INFO | train_inner | epoch 007:    267 / 392 loss=8.053, ppl=265.56, wps=18283.1, ups=0.28, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.65, loss_scale=32, train_wall=331, gb_free=9.6, wall=9752
2022-03-15 17:07:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:07:30 | INFO | train_inner | epoch 007:    368 / 392 loss=7.978, ppl=252.09, wps=18177.9, ups=0.28, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.63, loss_scale=32, train_wall=333, gb_free=9.6, wall=10112
2022-03-15 17:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:09:35 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.777 | ppl 219.3 | wps 29698.6 | wpb 511.9 | bsz 1 | num_updates 2724 | best_loss 7.777
2022-03-15 17:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2724 updates
2022-03-15 17:09:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 17:09:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 17:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 7 @ 2724 updates, score 7.777) (writing took 2.5110093720722944 seconds)
2022-03-15 17:09:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 17:09:38 | INFO | train | epoch 007 | loss 8.069 | ppl 268.49 | wps 17686.1 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 2724 | lr 0.000340532 | gnorm 0.644 | loss_scale 32 | train_wall 1286 | gb_free 9.6 | wall 10240
KL Stats: Epoch 7 Divergences: Uniform: 4.000266924623309 Unigram: 2.2990228734738665
2022-03-15 17:09:38 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 17:09:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:14:07 | INFO | train_inner | epoch 008:     76 / 392 loss=7.874, ppl=234.57, wps=16371.8, ups=0.25, wpb=65025.8, bsz=127, num_updates=2800, lr=0.00035003, gnorm=0.634, loss_scale=32, train_wall=324, gb_free=9.6, wall=10509
2022-03-15 17:15:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:15:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:20:13 | INFO | train_inner | epoch 008:    178 / 392 loss=7.82, ppl=226.02, wps=17923.5, ups=0.27, wpb=65532.7, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.626, loss_scale=16, train_wall=338, gb_free=9.6, wall=10875
2022-03-15 17:25:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:26:13 | INFO | train_inner | epoch 008:    279 / 392 loss=7.756, ppl=216.12, wps=18204.2, ups=0.28, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.613, loss_scale=16, train_wall=333, gb_free=9.6, wall=11235
2022-03-15 17:32:08 | INFO | train_inner | epoch 008:    379 / 392 loss=7.706, ppl=208.85, wps=18452.8, ups=0.28, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.604, loss_scale=16, train_wall=328, gb_free=9.6, wall=11590
2022-03-15 17:32:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:33:34 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.518 | ppl 183.31 | wps 29824.1 | wpb 511.9 | bsz 1 | num_updates 3113 | best_loss 7.518
2022-03-15 17:33:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3113 updates
2022-03-15 17:33:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 17:33:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 17:33:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 8 @ 3113 updates, score 7.518) (writing took 2.539000371005386 seconds)
2022-03-15 17:33:37 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 17:33:37 | INFO | train | epoch 008 | loss 7.776 | ppl 219.22 | wps 17677 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 3113 | lr 0.000389147 | gnorm 0.617 | loss_scale 16 | train_wall 1287 | gb_free 9.6 | wall 11679
KL Stats: Epoch 8 Divergences: Uniform: 4.21221950461986 Unigram: 2.4178946412523246
2022-03-15 17:33:37 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 17:33:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:38:46 | INFO | train_inner | epoch 009:     87 / 392 loss=7.592, ppl=192.92, wps=16342, ups=0.25, wpb=65029.1, bsz=127, num_updates=3200, lr=0.00040002, gnorm=0.63, loss_scale=32, train_wall=325, gb_free=9.6, wall=11988
2022-03-15 17:40:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:44:51 | INFO | train_inner | epoch 009:    188 / 392 loss=7.557, ppl=188.36, wps=17958, ups=0.27, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.6, loss_scale=16, train_wall=337, gb_free=9.6, wall=12353
2022-03-15 17:49:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:50:51 | INFO | train_inner | epoch 009:    289 / 392 loss=7.506, ppl=181.79, wps=18199.4, ups=0.28, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.6, loss_scale=16, train_wall=333, gb_free=9.6, wall=12713
2022-03-15 17:51:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 17:56:51 | INFO | train_inner | epoch 009:    390 / 392 loss=7.446, ppl=174.37, wps=18180.9, ups=0.28, wpb=65532.7, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.605, loss_scale=8, train_wall=333, gb_free=9.6, wall=13074
2022-03-15 17:56:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:57:38 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.262 | ppl 153.54 | wps 30095 | wpb 511.9 | bsz 1 | num_updates 3502 | best_loss 7.262
2022-03-15 17:57:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3502 updates
2022-03-15 17:57:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 17:57:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 17:57:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 9 @ 3502 updates, score 7.262) (writing took 2.9577081060269848 seconds)
2022-03-15 17:57:42 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 17:57:42 | INFO | train | epoch 009 | loss 7.521 | ppl 183.62 | wps 17611 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 3502 | lr 0.000437762 | gnorm 0.608 | loss_scale 8 | train_wall 1292 | gb_free 9.6 | wall 13124
KL Stats: Epoch 9 Divergences: Uniform: 4.356428234225696 Unigram: 2.5169596014021907
2022-03-15 17:57:42 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 17:57:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:03:32 | INFO | train_inner | epoch 010:     98 / 392 loss=7.329, ppl=160.83, wps=16247.7, ups=0.25, wpb=65029.1, bsz=127, num_updates=3600, lr=0.00045001, gnorm=0.6, loss_scale=16, train_wall=327, gb_free=9.6, wall=13474
2022-03-15 18:09:25 | INFO | train_inner | epoch 010:    198 / 392 loss=7.301, ppl=157.67, wps=18553.4, ups=0.28, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.598, loss_scale=32, train_wall=326, gb_free=9.6, wall=13827
2022-03-15 18:10:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:15:25 | INFO | train_inner | epoch 010:    299 / 392 loss=7.259, ppl=153.19, wps=18183.8, ups=0.28, wpb=65532.7, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.605, loss_scale=16, train_wall=333, gb_free=9.6, wall=14187
2022-03-15 18:18:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:20:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:21:32 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.048 | ppl 132.29 | wps 29973.9 | wpb 511.9 | bsz 1 | num_updates 3892 | best_loss 7.048
2022-03-15 18:21:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3892 updates
2022-03-15 18:21:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 18:21:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 18:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 10 @ 3892 updates, score 7.048) (writing took 2.454099240945652 seconds)
2022-03-15 18:21:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 18:21:34 | INFO | train | epoch 010 | loss 7.278 | ppl 155.25 | wps 17801.4 | ups 0.27 | wpb 65405.2 | bsz 127.7 | num_updates 3892 | lr 0.000486503 | gnorm 0.602 | loss_scale 16 | train_wall 1281 | gb_free 9.6 | wall 14557
KL Stats: Epoch 10 Divergences: Uniform: 4.483252671496698 Unigram: 2.6024186400851166
2022-03-15 18:21:34 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 18:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:22:02 | INFO | train_inner | epoch 011:      8 / 392 loss=7.217, ppl=148.82, wps=16387.9, ups=0.25, wpb=65029.1, bsz=127, num_updates=3900, lr=0.000487503, gnorm=0.612, loss_scale=16, train_wall=324, gb_free=9.6, wall=14584
2022-03-15 18:27:49 | INFO | train_inner | epoch 011:    108 / 392 loss=7.114, ppl=138.56, wps=18896, ups=0.29, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.595, loss_scale=32, train_wall=320, gb_free=9.6, wall=14931
2022-03-15 18:28:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:33:53 | INFO | train_inner | epoch 011:    209 / 392 loss=7.089, ppl=136.16, wps=17995.5, ups=0.27, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.594, loss_scale=16, train_wall=336, gb_free=9.6, wall=15295
2022-03-15 18:38:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:39:57 | INFO | train_inner | epoch 011:    310 / 392 loss=7.067, ppl=134.09, wps=17990.8, ups=0.27, wpb=65532.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.586, loss_scale=16, train_wall=337, gb_free=9.6, wall=15660
2022-03-15 18:44:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:45:32 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.895 | ppl 119.04 | wps 29697.6 | wpb 511.9 | bsz 1 | num_updates 4282 | best_loss 6.895
2022-03-15 18:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4282 updates
2022-03-15 18:45:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 18:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 18:45:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 11 @ 4282 updates, score 6.895) (writing took 2.58764667797368 seconds)
2022-03-15 18:45:34 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 18:45:34 | INFO | train | epoch 011 | loss 7.079 | ppl 135.21 | wps 17713.6 | ups 0.27 | wpb 65405.2 | bsz 127.7 | num_updates 4282 | lr 0.000483255 | gnorm 0.592 | loss_scale 16 | train_wall 1288 | gb_free 9.6 | wall 15997
KL Stats: Epoch 11 Divergences: Uniform: 4.571056804041779 Unigram: 2.6662132367023217
2022-03-15 18:45:34 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 18:45:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:46:39 | INFO | train_inner | epoch 012:     18 / 392 loss=7.019, ppl=129.65, wps=16202.6, ups=0.25, wpb=65029.1, bsz=127, num_updates=4300, lr=0.000482243, gnorm=0.588, loss_scale=32, train_wall=328, gb_free=9.6, wall=16061
2022-03-15 18:46:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:52:37 | INFO | train_inner | epoch 012:    119 / 392 loss=6.942, ppl=122.95, wps=18270.8, ups=0.28, wpb=65532.7, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.572, loss_scale=16, train_wall=331, gb_free=9.6, wall=16420
2022-03-15 18:55:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:58:36 | INFO | train_inner | epoch 012:    220 / 392 loss=6.929, ppl=121.83, wps=18255, ups=0.28, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.57, loss_scale=16, train_wall=332, gb_free=9.6, wall=16779
2022-03-15 19:03:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:04:40 | INFO | train_inner | epoch 012:    321 / 392 loss=6.903, ppl=119.68, wps=18002.1, ups=0.27, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.566, loss_scale=16, train_wall=336, gb_free=9.6, wall=17143
2022-03-15 19:08:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:09:36 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.782 | ppl 110.05 | wps 29404.3 | wpb 511.9 | bsz 1 | num_updates 4671 | best_loss 6.782
2022-03-15 19:09:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4671 updates
2022-03-15 19:09:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 19:09:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 19:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 12 @ 4671 updates, score 6.782) (writing took 2.5575166899943724 seconds)
2022-03-15 19:09:39 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 19:09:39 | INFO | train | epoch 012 | loss 6.92 | ppl 121.08 | wps 17612.2 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 4671 | lr 0.000462695 | gnorm 0.572 | loss_scale 16 | train_wall 1291 | gb_free 9.6 | wall 17441
KL Stats: Epoch 12 Divergences: Uniform: 4.653069435256198 Unigram: 2.7191473267031125
2022-03-15 19:09:39 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 19:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:11:18 | INFO | train_inner | epoch 013:     29 / 392 loss=6.865, ppl=116.54, wps=16365.4, ups=0.25, wpb=65029.1, bsz=127, num_updates=4700, lr=0.000461266, gnorm=0.57, loss_scale=16, train_wall=324, gb_free=9.6, wall=17540
2022-03-15 19:12:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:17:21 | INFO | train_inner | epoch 013:    130 / 392 loss=6.815, ppl=112.57, wps=18058, ups=0.28, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.556, loss_scale=16, train_wall=335, gb_free=9.6, wall=17903
2022-03-15 19:20:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:23:19 | INFO | train_inner | epoch 013:    231 / 392 loss=6.797, ppl=111.19, wps=18259.5, ups=0.28, wpb=65532.7, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.555, loss_scale=16, train_wall=331, gb_free=9.6, wall=18262
2022-03-15 19:28:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:29:20 | INFO | train_inner | epoch 013:    332 / 392 loss=6.794, ppl=110.94, wps=18156.7, ups=0.28, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.559, loss_scale=16, train_wall=333, gb_free=9.6, wall=18623
2022-03-15 19:32:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:33:30 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.692 | ppl 103.42 | wps 31410.4 | wpb 511.9 | bsz 1 | num_updates 5060 | best_loss 6.692
2022-03-15 19:33:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5060 updates
2022-03-15 19:33:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 19:33:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 19:33:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 13 @ 5060 updates, score 6.692) (writing took 2.3475692439824343 seconds)
2022-03-15 19:33:33 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 19:33:33 | INFO | train | epoch 013 | loss 6.798 | ppl 111.27 | wps 17745.3 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 5060 | lr 0.000444554 | gnorm 0.556 | loss_scale 16 | train_wall 1284 | gb_free 9.6 | wall 18875
KL Stats: Epoch 13 Divergences: Uniform: 4.716992880723842 Unigram: 2.7593068262536544
2022-03-15 19:33:33 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 19:33:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:35:53 | INFO | train_inner | epoch 014:     40 / 392 loss=6.743, ppl=107.15, wps=16553.7, ups=0.25, wpb=65025.8, bsz=127, num_updates=5100, lr=0.000442807, gnorm=0.557, loss_scale=16, train_wall=322, gb_free=9.6, wall=19016
2022-03-15 19:37:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:41:51 | INFO | train_inner | epoch 014:    141 / 392 loss=6.693, ppl=103.47, wps=18310.3, ups=0.28, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.542, loss_scale=16, train_wall=330, gb_free=9.6, wall=19373
2022-03-15 19:45:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:47:50 | INFO | train_inner | epoch 014:    242 / 392 loss=6.71, ppl=104.66, wps=18265.7, ups=0.28, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.554, loss_scale=16, train_wall=331, gb_free=9.6, wall=19732
2022-03-15 19:53:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:53:51 | INFO | train_inner | epoch 014:    343 / 392 loss=6.706, ppl=104.37, wps=18163, ups=0.28, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.546, loss_scale=16, train_wall=333, gb_free=9.6, wall=20093
2022-03-15 19:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:57:23 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.611 | ppl 97.73 | wps 29515.8 | wpb 511.9 | bsz 1 | num_updates 5449 | best_loss 6.611
2022-03-15 19:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5449 updates
2022-03-15 19:57:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 19:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 19:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 14 @ 5449 updates, score 6.611) (writing took 2.5108788840007037 seconds)
2022-03-15 19:57:25 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 19:57:25 | INFO | train | epoch 014 | loss 6.701 | ppl 104.04 | wps 17763 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 5449 | lr 0.000428392 | gnorm 0.549 | loss_scale 16 | train_wall 1280 | gb_free 9.6 | wall 20307
KL Stats: Epoch 14 Divergences: Uniform: 4.772664501606417 Unigram: 2.7901396891703167
2022-03-15 19:57:25 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 19:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:00:30 | INFO | train_inner | epoch 015:     51 / 392 loss=6.652, ppl=100.6, wps=16284.6, ups=0.25, wpb=65029.1, bsz=127, num_updates=5500, lr=0.000426401, gnorm=0.548, loss_scale=16, train_wall=326, gb_free=9.6, wall=20492
2022-03-15 20:02:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:06:34 | INFO | train_inner | epoch 015:    152 / 392 loss=6.626, ppl=98.76, wps=18016, ups=0.27, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.541, loss_scale=16, train_wall=336, gb_free=9.6, wall=20856
2022-03-15 20:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:12:33 | INFO | train_inner | epoch 015:    253 / 392 loss=6.623, ppl=98.6, wps=18269.9, ups=0.28, wpb=65532.7, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.538, loss_scale=16, train_wall=331, gb_free=9.6, wall=21215
2022-03-15 20:18:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:18:33 | INFO | train_inner | epoch 015:    354 / 392 loss=6.62, ppl=98.38, wps=18171.1, ups=0.28, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.541, loss_scale=16, train_wall=333, gb_free=9.6, wall=21576
2022-03-15 20:20:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:21:30 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.573 | ppl 95.18 | wps 29500.6 | wpb 511.9 | bsz 1 | num_updates 5838 | best_loss 6.573
2022-03-15 20:21:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5838 updates
2022-03-15 20:21:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 20:21:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 20:21:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 15 @ 5838 updates, score 6.573) (writing took 2.4949438729090616 seconds)
2022-03-15 20:21:33 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 20:21:33 | INFO | train | epoch 015 | loss 6.62 | ppl 98.39 | wps 17576.7 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 5838 | lr 0.000413874 | gnorm 0.542 | loss_scale 16 | train_wall 1294 | gb_free 9.6 | wall 21755
KL Stats: Epoch 15 Divergences: Uniform: 4.82850747031759 Unigram: 2.823685049297267
2022-03-15 20:21:33 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 20:21:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:25:17 | INFO | train_inner | epoch 016:     62 / 392 loss=6.568, ppl=94.9, wps=16103.9, ups=0.25, wpb=65029.1, bsz=127, num_updates=5900, lr=0.000411693, gnorm=0.541, loss_scale=16, train_wall=330, gb_free=9.6, wall=21979
2022-03-15 20:26:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:31:24 | INFO | train_inner | epoch 016:    163 / 392 loss=6.553, ppl=93.89, wps=17881.8, ups=0.27, wpb=65532.7, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.54, loss_scale=16, train_wall=338, gb_free=9.6, wall=22346
2022-03-15 20:35:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:37:28 | INFO | train_inner | epoch 016:    264 / 392 loss=6.558, ppl=94.24, wps=17973.4, ups=0.27, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.549, loss_scale=16, train_wall=337, gb_free=9.6, wall=22710
2022-03-15 20:43:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:43:29 | INFO | train_inner | epoch 016:    365 / 392 loss=6.546, ppl=93.46, wps=18178.8, ups=0.28, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.538, loss_scale=16, train_wall=333, gb_free=9.6, wall=23071
2022-03-15 20:45:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:45:46 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.527 | ppl 92.22 | wps 29712.9 | wpb 511.9 | bsz 1 | num_updates 6227 | best_loss 6.527
2022-03-15 20:45:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6227 updates
2022-03-15 20:45:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 20:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 20:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 16 @ 6227 updates, score 6.527) (writing took 2.477876786957495 seconds)
2022-03-15 20:45:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 20:45:48 | INFO | train | epoch 016 | loss 6.552 | ppl 93.86 | wps 17480 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 6227 | lr 0.000400738 | gnorm 0.542 | loss_scale 16 | train_wall 1302 | gb_free 9.6 | wall 23210
KL Stats: Epoch 16 Divergences: Uniform: 4.874804011466313 Unigram: 2.8485805858522686
2022-03-15 20:45:48 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 20:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:50:09 | INFO | train_inner | epoch 017:     73 / 392 loss=6.504, ppl=90.77, wps=16227.3, ups=0.25, wpb=65025.8, bsz=127, num_updates=6300, lr=0.00039841, gnorm=0.537, loss_scale=16, train_wall=328, gb_free=9.6, wall=23472
2022-03-15 20:51:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:56:15 | INFO | train_inner | epoch 017:    174 / 392 loss=6.494, ppl=90.13, wps=17917.5, ups=0.27, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.541, loss_scale=16, train_wall=338, gb_free=9.6, wall=23837
2022-03-15 21:01:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:02:20 | INFO | train_inner | epoch 017:    275 / 392 loss=6.497, ppl=90.3, wps=17946.4, ups=0.27, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.536, loss_scale=16, train_wall=337, gb_free=9.6, wall=24203
2022-03-15 21:08:20 | INFO | train_inner | epoch 017:    375 / 392 loss=6.499, ppl=90.42, wps=18241.8, ups=0.28, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.544, loss_scale=16, train_wall=332, gb_free=9.6, wall=24562
2022-03-15 21:09:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:10:01 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.477 | ppl 89.11 | wps 29510.1 | wpb 511.9 | bsz 1 | num_updates 6616 | best_loss 6.477
2022-03-15 21:10:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6616 updates
2022-03-15 21:10:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 21:10:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 21:10:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 17 @ 6616 updates, score 6.477) (writing took 2.5244498179527 seconds)
2022-03-15 21:10:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 21:10:04 | INFO | train | epoch 017 | loss 6.494 | ppl 90.13 | wps 17478.3 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 6616 | lr 0.000388779 | gnorm 0.54 | loss_scale 16 | train_wall 1302 | gb_free 9.6 | wall 24666
KL Stats: Epoch 17 Divergences: Uniform: 4.912633307090943 Unigram: 2.8698853939602302
2022-03-15 21:10:04 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 21:10:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:15:09 | INFO | train_inner | epoch 018:     84 / 392 loss=6.435, ppl=86.54, wps=15882.6, ups=0.24, wpb=65029.1, bsz=127, num_updates=6700, lr=0.000386334, gnorm=0.54, loss_scale=16, train_wall=335, gb_free=9.6, wall=24971
2022-03-15 21:18:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:21:16 | INFO | train_inner | epoch 018:    185 / 392 loss=6.443, ppl=86.98, wps=17840.2, ups=0.27, wpb=65532.7, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.53, loss_scale=16, train_wall=339, gb_free=9.6, wall=25339
2022-03-15 21:26:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:27:20 | INFO | train_inner | epoch 018:    286 / 392 loss=6.449, ppl=87.35, wps=18043.1, ups=0.28, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.536, loss_scale=16, train_wall=336, gb_free=9.6, wall=25702
2022-03-15 21:33:16 | INFO | train_inner | epoch 018:    386 / 392 loss=6.451, ppl=87.49, wps=18411, ups=0.28, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.539, loss_scale=16, train_wall=329, gb_free=9.6, wall=26058
2022-03-15 21:33:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:34:17 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.451 | ppl 87.47 | wps 29706.7 | wpb 511.9 | bsz 1 | num_updates 7006 | best_loss 6.451
2022-03-15 21:34:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7006 updates
2022-03-15 21:34:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 21:34:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 21:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 18 @ 7006 updates, score 6.451) (writing took 2.56743211299181 seconds)
2022-03-15 21:34:20 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 21:34:20 | INFO | train | epoch 018 | loss 6.442 | ppl 86.96 | wps 17515.8 | ups 0.27 | wpb 65405.2 | bsz 127.7 | num_updates 7006 | lr 0.000377803 | gnorm 0.536 | loss_scale 16 | train_wall 1303 | gb_free 9.6 | wall 26122
KL Stats: Epoch 18 Divergences: Uniform: 4.951455715543124 Unigram: 2.888900665344373
2022-03-15 21:34:20 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 21:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:35:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:40:00 | INFO | train_inner | epoch 019:     95 / 392 loss=6.38, ppl=83.31, wps=16093.1, ups=0.25, wpb=65029.1, bsz=127, num_updates=7100, lr=0.000375293, gnorm=0.533, loss_scale=16, train_wall=330, gb_free=9.6, wall=26462
2022-03-15 21:44:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:46:06 | INFO | train_inner | epoch 019:    196 / 392 loss=6.397, ppl=84.25, wps=17901.8, ups=0.27, wpb=65532.7, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.536, loss_scale=16, train_wall=338, gb_free=9.6, wall=26828
2022-03-15 21:52:07 | INFO | train_inner | epoch 019:    296 / 392 loss=6.405, ppl=84.73, wps=18119, ups=0.28, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.528, loss_scale=16, train_wall=334, gb_free=9.6, wall=27190
2022-03-15 21:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:58:33 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.409 | ppl 84.98 | wps 29705.4 | wpb 511.9 | bsz 1 | num_updates 7395 | best_loss 6.409
2022-03-15 21:58:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7395 updates
2022-03-15 21:58:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 21:58:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 21:58:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 19 @ 7395 updates, score 6.409) (writing took 2.8612039940198883 seconds)
2022-03-15 21:58:36 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-15 21:58:36 | INFO | train | epoch 019 | loss 6.397 | ppl 84.27 | wps 17479.3 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 7395 | lr 0.000367732 | gnorm 0.531 | loss_scale 16 | train_wall 1301 | gb_free 9.6 | wall 27578
KL Stats: Epoch 19 Divergences: Uniform: 4.98785470595546 Unigram: 2.9068883117038182
2022-03-15 21:58:36 | INFO | fairseq.trainer | begin training epoch 20
2022-03-15 21:58:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:58:54 | INFO | train_inner | epoch 020:      5 / 392 loss=6.403, ppl=84.64, wps=16006.1, ups=0.25, wpb=65029.1, bsz=127, num_updates=7400, lr=0.000367607, gnorm=0.534, loss_scale=16, train_wall=332, gb_free=9.6, wall=27596
2022-03-15 22:01:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:04:53 | INFO | train_inner | epoch 020:    106 / 392 loss=6.349, ppl=81.5, wps=18254.6, ups=0.28, wpb=65532.7, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.523, loss_scale=16, train_wall=332, gb_free=9.6, wall=27955
2022-03-15 22:08:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:10:55 | INFO | train_inner | epoch 020:    207 / 392 loss=6.352, ppl=81.71, wps=18079.7, ups=0.28, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.534, loss_scale=16, train_wall=335, gb_free=9.6, wall=28317
2022-03-15 22:16:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:16:59 | INFO | train_inner | epoch 020:    308 / 392 loss=6.363, ppl=82.31, wps=17990.6, ups=0.27, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.533, loss_scale=16, train_wall=337, gb_free=9.6, wall=28682
2022-03-15 22:21:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:22:42 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.392 | ppl 83.96 | wps 29694.9 | wpb 511.9 | bsz 1 | num_updates 7784 | best_loss 6.392
2022-03-15 22:22:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7784 updates
2022-03-15 22:22:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 22:22:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 22:22:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 20 @ 7784 updates, score 6.392) (writing took 2.6034802040085196 seconds)
2022-03-15 22:22:45 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-15 22:22:45 | INFO | train | epoch 020 | loss 6.356 | ppl 81.92 | wps 17554.9 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 7784 | lr 0.000358425 | gnorm 0.531 | loss_scale 16 | train_wall 1296 | gb_free 9.6 | wall 29027
KL Stats: Epoch 20 Divergences: Uniform: 5.0214762287023245 Unigram: 2.923045815895578
2022-03-15 22:22:45 | INFO | fairseq.trainer | begin training epoch 21
2022-03-15 22:22:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:23:43 | INFO | train_inner | epoch 021:     16 / 392 loss=6.354, ppl=81.8, wps=16119.7, ups=0.25, wpb=65029.1, bsz=127, num_updates=7800, lr=0.000358057, gnorm=0.541, loss_scale=16, train_wall=330, gb_free=9.6, wall=29085
2022-03-15 22:25:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:29:45 | INFO | train_inner | epoch 021:    117 / 392 loss=6.31, ppl=79.32, wps=18074.7, ups=0.28, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.536, loss_scale=16, train_wall=335, gb_free=9.6, wall=29448
2022-03-15 22:33:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:35:49 | INFO | train_inner | epoch 021:    218 / 392 loss=6.311, ppl=79.41, wps=18013.4, ups=0.27, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.535, loss_scale=16, train_wall=336, gb_free=9.6, wall=29812
2022-03-15 22:41:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:41:57 | INFO | train_inner | epoch 021:    319 / 392 loss=6.328, ppl=80.35, wps=17819.3, ups=0.27, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.528, loss_scale=16, train_wall=340, gb_free=9.6, wall=30179
2022-03-15 22:46:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:47:04 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.359 | ppl 82.06 | wps 29337.3 | wpb 511.9 | bsz 1 | num_updates 8173 | best_loss 6.359
2022-03-15 22:47:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8173 updates
2022-03-15 22:47:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 22:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 22:47:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 21 @ 8173 updates, score 6.359) (writing took 2.5274978379020467 seconds)
2022-03-15 22:47:07 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-15 22:47:07 | INFO | train | epoch 021 | loss 6.32 | ppl 79.91 | wps 17407.8 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 8173 | lr 0.000349791 | gnorm 0.537 | loss_scale 16 | train_wall 1307 | gb_free 9.6 | wall 30489
KL Stats: Epoch 21 Divergences: Uniform: 5.052852022093311 Unigram: 2.9355001027042515
2022-03-15 22:47:07 | INFO | fairseq.trainer | begin training epoch 22
2022-03-15 22:47:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:48:44 | INFO | train_inner | epoch 022:     27 / 392 loss=6.316, ppl=79.68, wps=15977.1, ups=0.25, wpb=65025.8, bsz=127, num_updates=8200, lr=0.000349215, gnorm=0.543, loss_scale=16, train_wall=333, gb_free=9.6, wall=30586
2022-03-15 22:49:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:54:46 | INFO | train_inner | epoch 022:    128 / 392 loss=6.274, ppl=77.39, wps=18092.1, ups=0.28, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.53, loss_scale=16, train_wall=334, gb_free=9.6, wall=30949
2022-03-15 22:58:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:00:51 | INFO | train_inner | epoch 022:    229 / 392 loss=6.294, ppl=78.49, wps=17988.5, ups=0.27, wpb=65532.7, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.534, loss_scale=16, train_wall=337, gb_free=9.6, wall=31313
2022-03-15 23:06:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:06:57 | INFO | train_inner | epoch 022:    330 / 392 loss=6.295, ppl=78.5, wps=17885.5, ups=0.27, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.528, loss_scale=16, train_wall=338, gb_free=9.6, wall=31679
2022-03-15 23:10:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:11:22 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.344 | ppl 81.26 | wps 29563.7 | wpb 511.9 | bsz 1 | num_updates 8562 | best_loss 6.344
2022-03-15 23:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8562 updates
2022-03-15 23:11:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 23:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 23:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 22 @ 8562 updates, score 6.344) (writing took 2.491832828032784 seconds)
2022-03-15 23:11:24 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-15 23:11:24 | INFO | train | epoch 022 | loss 6.287 | ppl 78.09 | wps 17454.8 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 8562 | lr 0.000341753 | gnorm 0.531 | loss_scale 16 | train_wall 1304 | gb_free 9.6 | wall 31946
KL Stats: Epoch 22 Divergences: Uniform: 5.084155408310408 Unigram: 2.953134849204243
2022-03-15 23:11:24 | INFO | fairseq.trainer | begin training epoch 23
2022-03-15 23:11:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:13:41 | INFO | train_inner | epoch 023:     38 / 392 loss=6.271, ppl=77.21, wps=16085.6, ups=0.25, wpb=65029.1, bsz=127, num_updates=8600, lr=0.000340997, gnorm=0.535, loss_scale=16, train_wall=331, gb_free=9.6, wall=32084
2022-03-15 23:15:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:19:42 | INFO | train_inner | epoch 023:    139 / 392 loss=6.247, ppl=75.95, wps=18149.9, ups=0.28, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.53, loss_scale=16, train_wall=334, gb_free=9.6, wall=32445
2022-03-15 23:22:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:25:45 | INFO | train_inner | epoch 023:    240 / 392 loss=6.258, ppl=76.54, wps=18094.8, ups=0.28, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.538, loss_scale=16, train_wall=335, gb_free=9.6, wall=32807
2022-03-15 23:30:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:31:49 | INFO | train_inner | epoch 023:    341 / 392 loss=6.267, ppl=77.02, wps=17961.3, ups=0.27, wpb=65532.7, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.534, loss_scale=16, train_wall=337, gb_free=9.6, wall=33172
2022-03-15 23:34:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:35:36 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.319 | ppl 79.85 | wps 29394.3 | wpb 511.9 | bsz 1 | num_updates 8951 | best_loss 6.319
2022-03-15 23:35:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 8951 updates
2022-03-15 23:35:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 23:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 23:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 23 @ 8951 updates, score 6.319) (writing took 2.550438793026842 seconds)
2022-03-15 23:35:38 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-15 23:35:38 | INFO | train | epoch 023 | loss 6.257 | ppl 76.49 | wps 17495.5 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 8951 | lr 0.000334244 | gnorm 0.535 | loss_scale 16 | train_wall 1301 | gb_free 9.6 | wall 33401
KL Stats: Epoch 23 Divergences: Uniform: 5.107158832869842 Unigram: 2.963302167569483
2022-03-15 23:35:38 | INFO | fairseq.trainer | begin training epoch 24
2022-03-15 23:35:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:38:36 | INFO | train_inner | epoch 024:     49 / 392 loss=6.247, ppl=75.95, wps=15995, ups=0.25, wpb=65029.1, bsz=127, num_updates=9000, lr=0.000333333, gnorm=0.536, loss_scale=16, train_wall=333, gb_free=9.6, wall=33578
2022-03-15 23:39:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:44:40 | INFO | train_inner | epoch 024:    150 / 392 loss=6.217, ppl=74.39, wps=18024, ups=0.28, wpb=65532.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.533, loss_scale=16, train_wall=336, gb_free=9.6, wall=33942
2022-03-15 23:47:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:50:40 | INFO | train_inner | epoch 024:    251 / 392 loss=6.237, ppl=75.41, wps=18189.3, ups=0.28, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.53, loss_scale=16, train_wall=333, gb_free=9.6, wall=34302
2022-03-15 23:55:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:56:42 | INFO | train_inner | epoch 024:    352 / 392 loss=6.238, ppl=75.5, wps=18110.1, ups=0.28, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.542, loss_scale=16, train_wall=334, gb_free=9.6, wall=34664
2022-03-15 23:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:59:47 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.307 | ppl 79.15 | wps 29323.9 | wpb 511.9 | bsz 1 | num_updates 9340 | best_loss 6.307
2022-03-15 23:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9340 updates
2022-03-15 23:59:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 23:59:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-15 23:59:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 24 @ 9340 updates, score 6.307) (writing took 2.5429182259831578 seconds)
2022-03-15 23:59:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-15 23:59:50 | INFO | train | epoch 024 | loss 6.229 | ppl 75.03 | wps 17528.5 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 9340 | lr 0.00032721 | gnorm 0.537 | loss_scale 16 | train_wall 1298 | gb_free 9.6 | wall 34852
KL Stats: Epoch 24 Divergences: Uniform: 5.1363849663435825 Unigram: 2.976982671220561
2022-03-15 23:59:50 | INFO | fairseq.trainer | begin training epoch 25
2022-03-15 23:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:03:27 | INFO | train_inner | epoch 025:     60 / 392 loss=6.2, ppl=73.52, wps=16029.1, ups=0.25, wpb=65025.8, bsz=127, num_updates=9400, lr=0.000326164, gnorm=0.538, loss_scale=16, train_wall=332, gb_free=9.6, wall=35070
2022-03-16 00:04:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:09:34 | INFO | train_inner | epoch 025:    161 / 392 loss=6.198, ppl=73.39, wps=17884.1, ups=0.27, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.526, loss_scale=16, train_wall=338, gb_free=9.6, wall=35436
2022-03-16 00:12:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:15:33 | INFO | train_inner | epoch 025:    262 / 392 loss=6.214, ppl=74.22, wps=18221.5, ups=0.28, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.539, loss_scale=16, train_wall=332, gb_free=9.6, wall=35796
2022-03-16 00:20:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:21:38 | INFO | train_inner | epoch 025:    363 / 392 loss=6.214, ppl=74.22, wps=17976.2, ups=0.27, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.524, loss_scale=16, train_wall=337, gb_free=9.6, wall=36160
2022-03-16 00:23:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:24:05 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.29 | ppl 78.22 | wps 29272.9 | wpb 511.9 | bsz 1 | num_updates 9729 | best_loss 6.29
2022-03-16 00:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9729 updates
2022-03-16 00:24:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 00:24:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 00:24:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 25 @ 9729 updates, score 6.29) (writing took 2.576493217027746 seconds)
2022-03-16 00:24:08 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-16 00:24:08 | INFO | train | epoch 025 | loss 6.204 | ppl 73.71 | wps 17454 | ups 0.27 | wpb 65404.8 | bsz 127.7 | num_updates 9729 | lr 0.000320602 | gnorm 0.531 | loss_scale 16 | train_wall 1303 | gb_free 9.6 | wall 36310
KL Stats: Epoch 25 Divergences: Uniform: 5.160400957088672 Unigram: 2.9872555559075584
2022-03-16 00:24:08 | INFO | fairseq.trainer | begin training epoch 26
2022-03-16 00:24:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:28:26 | INFO | train_inner | epoch 026:     71 / 392 loss=6.174, ppl=72.19, wps=15955, ups=0.25, wpb=65029.1, bsz=127, num_updates=9800, lr=0.000319438, gnorm=0.534, loss_scale=16, train_wall=333, gb_free=9.6, wall=36568
2022-03-16 00:28:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:33:24 | INFO | train_inner | epoch 026:    172 / 392 loss=6.174, ppl=72.21, wps=21969.7, ups=0.34, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.528, loss_scale=16, train_wall=274, gb_free=9.6, wall=36866
2022-03-16 00:35:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:38:19 | INFO | train_inner | epoch 026:    273 / 392 loss=6.184, ppl=72.72, wps=22198.4, ups=0.34, wpb=65532.7, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.528, loss_scale=16, train_wall=271, gb_free=9.6, wall=37161
2022-03-16 00:41:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:43:15 | INFO | train_inner | epoch 026:    374 / 392 loss=6.197, ppl=73.36, wps=22182, ups=0.34, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.534, loss_scale=16, train_wall=271, gb_free=9.6, wall=37457
2022-03-16 00:44:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:44:42 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.27 | ppl 77.16 | wps 34370.1 | wpb 511.9 | bsz 1 | num_updates 10118 | best_loss 6.27
2022-03-16 00:44:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10118 updates
2022-03-16 00:44:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 00:44:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 00:44:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 26 @ 10118 updates, score 6.27) (writing took 2.3596422349801287 seconds)
2022-03-16 00:44:45 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 00:44:45 | INFO | train | epoch 026 | loss 6.18 | ppl 72.5 | wps 20564 | ups 0.31 | wpb 65404.8 | bsz 127.7 | num_updates 10118 | lr 0.000314378 | gnorm 0.531 | loss_scale 16 | train_wall 1100 | gb_free 9.6 | wall 37547
KL Stats: Epoch 26 Divergences: Uniform: 5.1814070429972805 Unigram: 2.995849122243532
2022-03-16 00:44:45 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 00:44:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:48:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:48:47 | INFO | train_inner | epoch 027:     83 / 392 loss=6.145, ppl=70.75, wps=19544.7, ups=0.3, wpb=65025.8, bsz=127, num_updates=10200, lr=0.000313112, gnorm=0.537, loss_scale=16, train_wall=268, gb_free=9.6, wall=37790
2022-03-16 00:53:39 | INFO | train_inner | epoch 027:    183 / 392 loss=6.149, ppl=70.96, wps=22442.6, ups=0.34, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.527, loss_scale=16, train_wall=268, gb_free=9.6, wall=38082
2022-03-16 00:55:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:58:35 | INFO | train_inner | epoch 027:    284 / 392 loss=6.169, ppl=71.96, wps=22196.9, ups=0.34, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.541, loss_scale=16, train_wall=271, gb_free=9.6, wall=38377
2022-03-16 01:01:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:03:30 | INFO | train_inner | epoch 027:    385 / 392 loss=6.177, ppl=72.34, wps=22167.4, ups=0.34, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.535, loss_scale=16, train_wall=271, gb_free=9.6, wall=38673
2022-03-16 01:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:04:26 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.259 | ppl 76.59 | wps 34051.3 | wpb 511.9 | bsz 1 | num_updates 10507 | best_loss 6.259
2022-03-16 01:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10507 updates
2022-03-16 01:04:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 01:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 01:04:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 27 @ 10507 updates, score 6.259) (writing took 2.279397675069049 seconds)
2022-03-16 01:04:29 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 01:04:29 | INFO | train | epoch 027 | loss 6.158 | ppl 71.41 | wps 21493.4 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 10507 | lr 0.000308504 | gnorm 0.535 | loss_scale 16 | train_wall 1049 | gb_free 9.6 | wall 38731
KL Stats: Epoch 27 Divergences: Uniform: 5.202393430005544 Unigram: 3.0067482042327396
2022-03-16 01:04:29 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 01:04:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:08:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:09:04 | INFO | train_inner | epoch 028:     94 / 392 loss=6.118, ppl=69.46, wps=19474.3, ups=0.3, wpb=65025.8, bsz=127, num_updates=10600, lr=0.000307148, gnorm=0.539, loss_scale=16, train_wall=269, gb_free=9.6, wall=39006
2022-03-16 01:11:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:13:59 | INFO | train_inner | epoch 028:    195 / 392 loss=6.131, ppl=70.09, wps=22225.9, ups=0.34, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.548, loss_scale=8, train_wall=270, gb_free=9.6, wall=39301
2022-03-16 01:18:52 | INFO | train_inner | epoch 028:    295 / 392 loss=6.151, ppl=71.08, wps=22389.7, ups=0.34, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.531, loss_scale=16, train_wall=269, gb_free=9.6, wall=39594
2022-03-16 01:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:24:11 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.252 | ppl 76.19 | wps 34209.9 | wpb 511.9 | bsz 1 | num_updates 10897 | best_loss 6.252
2022-03-16 01:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10897 updates
2022-03-16 01:24:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 01:24:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 01:24:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 28 @ 10897 updates, score 6.252) (writing took 2.244622212019749 seconds)
2022-03-16 01:24:13 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 01:24:13 | INFO | train | epoch 028 | loss 6.138 | ppl 70.43 | wps 21531.7 | ups 0.33 | wpb 65405.2 | bsz 127.7 | num_updates 10897 | lr 0.000302933 | gnorm 0.538 | loss_scale 32 | train_wall 1050 | gb_free 9.6 | wall 39916
KL Stats: Epoch 28 Divergences: Uniform: 5.2260699688376775 Unigram: 3.014770463930458
2022-03-16 01:24:13 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 01:24:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:24:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:24:25 | INFO | train_inner | epoch 029:      4 / 392 loss=6.151, ppl=71.07, wps=19507.6, ups=0.3, wpb=65029.1, bsz=127, num_updates=10900, lr=0.000302891, gnorm=0.54, loss_scale=16, train_wall=269, gb_free=9.6, wall=39927
2022-03-16 01:29:18 | INFO | train_inner | epoch 029:    104 / 392 loss=6.092, ppl=68.23, wps=22379.5, ups=0.34, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.536, loss_scale=16, train_wall=269, gb_free=9.6, wall=40220
2022-03-16 01:30:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:31:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:34:16 | INFO | train_inner | epoch 029:    206 / 392 loss=6.117, ppl=69.4, wps=22014.2, ups=0.34, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.541, loss_scale=8, train_wall=273, gb_free=9.6, wall=40518
2022-03-16 01:39:08 | INFO | train_inner | epoch 029:    306 / 392 loss=6.126, ppl=69.84, wps=22412.5, ups=0.34, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.538, loss_scale=16, train_wall=268, gb_free=9.6, wall=40810
2022-03-16 01:43:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:43:54 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.228 | ppl 74.97 | wps 34090.1 | wpb 511.9 | bsz 1 | num_updates 11286 | best_loss 6.228
2022-03-16 01:43:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11286 updates
2022-03-16 01:43:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 01:43:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 01:43:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 29 @ 11286 updates, score 6.228) (writing took 2.238700401969254 seconds)
2022-03-16 01:43:57 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 01:43:57 | INFO | train | epoch 029 | loss 6.119 | ppl 69.48 | wps 21497.8 | ups 0.33 | wpb 65405.7 | bsz 127.7 | num_updates 11286 | lr 0.000297667 | gnorm 0.54 | loss_scale 16 | train_wall 1049 | gb_free 9.6 | wall 41099
KL Stats: Epoch 29 Divergences: Uniform: 5.243115234926699 Unigram: 3.0231099367363976
2022-03-16 01:43:57 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 01:43:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:44:41 | INFO | train_inner | epoch 030:     15 / 392 loss=6.134, ppl=70.24, wps=19532.9, ups=0.3, wpb=65029.1, bsz=127, num_updates=11300, lr=0.000297482, gnorm=0.548, loss_scale=16, train_wall=269, gb_free=9.6, wall=41143
2022-03-16 01:49:33 | INFO | train_inner | epoch 030:    115 / 392 loss=6.085, ppl=67.87, wps=22436.9, ups=0.34, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.533, loss_scale=16, train_wall=268, gb_free=9.6, wall=41435
2022-03-16 01:50:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:54:28 | INFO | train_inner | epoch 030:    216 / 392 loss=6.097, ppl=68.44, wps=22239.4, ups=0.34, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.55, loss_scale=16, train_wall=270, gb_free=9.6, wall=41730
2022-03-16 01:57:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:59:22 | INFO | train_inner | epoch 030:    317 / 392 loss=6.112, ppl=69.15, wps=22236.2, ups=0.34, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.534, loss_scale=16, train_wall=270, gb_free=9.6, wall=42025
2022-03-16 02:02:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:03:37 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.218 | ppl 74.45 | wps 34401.2 | wpb 511.9 | bsz 1 | num_updates 11675 | best_loss 6.218
2022-03-16 02:03:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11675 updates
2022-03-16 02:03:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 02:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 02:03:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 30 @ 11675 updates, score 6.218) (writing took 2.23769578197971 seconds)
2022-03-16 02:03:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 02:03:39 | INFO | train | epoch 030 | loss 6.1 | ppl 68.59 | wps 21520.8 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 11675 | lr 0.000292666 | gnorm 0.539 | loss_scale 16 | train_wall 1048 | gb_free 9.6 | wall 42281
KL Stats: Epoch 30 Divergences: Uniform: 5.262776292051234 Unigram: 3.0304908879812094
2022-03-16 02:03:39 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 02:03:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:04:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:04:55 | INFO | train_inner | epoch 031:     26 / 392 loss=6.096, ppl=68.41, wps=19541.1, ups=0.3, wpb=65025.8, bsz=127, num_updates=11700, lr=0.000292353, gnorm=0.539, loss_scale=16, train_wall=269, gb_free=9.6, wall=42357
2022-03-16 02:05:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:09:50 | INFO | train_inner | epoch 031:    127 / 392 loss=6.057, ppl=66.58, wps=22231.6, ups=0.34, wpb=65532.7, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.54, loss_scale=8, train_wall=270, gb_free=9.6, wall=42652
2022-03-16 02:14:42 | INFO | train_inner | epoch 031:    227 / 392 loss=6.099, ppl=68.55, wps=22417.5, ups=0.34, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.541, loss_scale=16, train_wall=268, gb_free=9.6, wall=42945
2022-03-16 02:15:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:19:37 | INFO | train_inner | epoch 031:    328 / 392 loss=6.088, ppl=68.02, wps=22259, ups=0.34, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.544, loss_scale=8, train_wall=270, gb_free=9.6, wall=43239
2022-03-16 02:22:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:23:19 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.212 | ppl 74.13 | wps 34142.5 | wpb 511.9 | bsz 1 | num_updates 12064 | best_loss 6.212
2022-03-16 02:23:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12064 updates
2022-03-16 02:23:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 02:23:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 02:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 31 @ 12064 updates, score 6.212) (writing took 2.2903557079844177 seconds)
2022-03-16 02:23:22 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 02:23:22 | INFO | train | epoch 031 | loss 6.083 | ppl 67.77 | wps 21514.4 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 12064 | lr 0.000287908 | gnorm 0.543 | loss_scale 16 | train_wall 1048 | gb_free 9.6 | wall 43464
KL Stats: Epoch 31 Divergences: Uniform: 5.28008287540696 Unigram: 3.0391668323841166
2022-03-16 02:23:22 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 02:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:25:07 | INFO | train_inner | epoch 032:     36 / 392 loss=6.082, ppl=67.75, wps=19705.3, ups=0.3, wpb=65029.1, bsz=127, num_updates=12100, lr=0.00028748, gnorm=0.545, loss_scale=16, train_wall=266, gb_free=9.6, wall=43569
2022-03-16 02:29:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:30:01 | INFO | train_inner | epoch 032:    137 / 392 loss=6.053, ppl=66.42, wps=22251.7, ups=0.34, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.534, loss_scale=16, train_wall=270, gb_free=9.6, wall=43864
2022-03-16 02:34:53 | INFO | train_inner | epoch 032:    237 / 392 loss=6.068, ppl=67.09, wps=22451.4, ups=0.34, wpb=65532.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.537, loss_scale=16, train_wall=268, gb_free=9.6, wall=44155
2022-03-16 02:35:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:39:48 | INFO | train_inner | epoch 032:    338 / 392 loss=6.073, ppl=67.31, wps=22221.6, ups=0.34, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.541, loss_scale=16, train_wall=271, gb_free=9.6, wall=44450
2022-03-16 02:41:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:42:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:43:01 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.206 | ppl 73.83 | wps 34213.7 | wpb 511.9 | bsz 1 | num_updates 12453 | best_loss 6.206
2022-03-16 02:43:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12453 updates
2022-03-16 02:43:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 02:43:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 02:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 32 @ 12453 updates, score 6.206) (writing took 2.264544478035532 seconds)
2022-03-16 02:43:03 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 02:43:03 | INFO | train | epoch 032 | loss 6.067 | ppl 67.04 | wps 21531 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 12453 | lr 0.000283376 | gnorm 0.539 | loss_scale 16 | train_wall 1047 | gb_free 9.6 | wall 44646
KL Stats: Epoch 32 Divergences: Uniform: 5.298445791358198 Unigram: 3.0465996632707975
2022-03-16 02:43:03 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 02:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:45:21 | INFO | train_inner | epoch 033:     47 / 392 loss=6.062, ppl=66.79, wps=19550.8, ups=0.3, wpb=65029.1, bsz=127, num_updates=12500, lr=0.000282843, gnorm=0.551, loss_scale=16, train_wall=268, gb_free=9.6, wall=44783
2022-03-16 02:47:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:50:16 | INFO | train_inner | epoch 033:    148 / 392 loss=6.037, ppl=65.66, wps=22187.9, ups=0.34, wpb=65532.7, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.546, loss_scale=8, train_wall=271, gb_free=9.6, wall=45078
2022-03-16 02:55:08 | INFO | train_inner | epoch 033:    248 / 392 loss=6.058, ppl=66.63, wps=22460.4, ups=0.34, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.543, loss_scale=16, train_wall=268, gb_free=9.6, wall=45370
2022-03-16 02:59:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:00:03 | INFO | train_inner | epoch 033:    349 / 392 loss=6.061, ppl=66.78, wps=22209.7, ups=0.34, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.542, loss_scale=16, train_wall=271, gb_free=9.6, wall=45665
2022-03-16 03:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:02:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.194 | ppl 73.23 | wps 34078.1 | wpb 511.9 | bsz 1 | num_updates 12843 | best_loss 6.194
2022-03-16 03:02:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12843 updates
2022-03-16 03:02:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 03:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 03:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 33 @ 12843 updates, score 6.194) (writing took 2.263591584051028 seconds)
2022-03-16 03:02:46 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 03:02:46 | INFO | train | epoch 033 | loss 6.052 | ppl 66.33 | wps 21562.9 | ups 0.33 | wpb 65405.2 | bsz 127.7 | num_updates 12843 | lr 0.00027904 | gnorm 0.546 | loss_scale 16 | train_wall 1048 | gb_free 9.6 | wall 45828
KL Stats: Epoch 33 Divergences: Uniform: 5.3132916310079015 Unigram: 3.052628100767517
2022-03-16 03:02:46 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 03:02:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:05:33 | INFO | train_inner | epoch 034:     57 / 392 loss=6.037, ppl=65.67, wps=19689.9, ups=0.3, wpb=65029.1, bsz=127, num_updates=12900, lr=0.000278423, gnorm=0.544, loss_scale=16, train_wall=266, gb_free=9.6, wall=45995
2022-03-16 03:07:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:10:28 | INFO | train_inner | epoch 034:    158 / 392 loss=6.027, ppl=65.19, wps=22191.9, ups=0.34, wpb=65532.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.538, loss_scale=16, train_wall=271, gb_free=9.6, wall=46291
2022-03-16 03:13:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:15:24 | INFO | train_inner | epoch 034:    259 / 392 loss=6.043, ppl=65.94, wps=22178.2, ups=0.34, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.54, loss_scale=16, train_wall=271, gb_free=9.6, wall=46586
2022-03-16 03:20:18 | INFO | train_inner | epoch 034:    359 / 392 loss=6.05, ppl=66.26, wps=22296, ups=0.34, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.544, loss_scale=32, train_wall=270, gb_free=9.6, wall=46880
2022-03-16 03:20:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:22:30 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.185 | ppl 72.77 | wps 34108 | wpb 511.9 | bsz 1 | num_updates 13232 | best_loss 6.185
2022-03-16 03:22:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13232 updates
2022-03-16 03:22:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 03:22:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 03:22:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 34 @ 13232 updates, score 6.185) (writing took 2.257939744973555 seconds)
2022-03-16 03:22:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 03:22:32 | INFO | train | epoch 034 | loss 6.037 | ppl 65.65 | wps 21451.2 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 13232 | lr 0.000274908 | gnorm 0.541 | loss_scale 16 | train_wall 1051 | gb_free 9.6 | wall 47015
KL Stats: Epoch 34 Divergences: Uniform: 5.331455638250074 Unigram: 3.060544534332775
2022-03-16 03:22:32 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 03:22:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:25:51 | INFO | train_inner | epoch 035:     68 / 392 loss=6.012, ppl=64.51, wps=19519.1, ups=0.3, wpb=65029.1, bsz=127, num_updates=13300, lr=0.000274204, gnorm=0.544, loss_scale=16, train_wall=269, gb_free=9.6, wall=47213
2022-03-16 03:27:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:30:46 | INFO | train_inner | epoch 035:    169 / 392 loss=6.012, ppl=64.54, wps=22193, ups=0.34, wpb=65532.7, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.541, loss_scale=16, train_wall=271, gb_free=9.6, wall=47509
2022-03-16 03:33:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:35:42 | INFO | train_inner | epoch 035:    270 / 392 loss=6.037, ppl=65.67, wps=22203.3, ups=0.34, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.539, loss_scale=16, train_wall=271, gb_free=9.6, wall=47804
2022-03-16 03:40:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:40:37 | INFO | train_inner | epoch 035:    371 / 392 loss=6.038, ppl=65.7, wps=22171.2, ups=0.34, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.543, loss_scale=16, train_wall=271, gb_free=9.6, wall=48099
2022-03-16 03:41:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:42:14 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.171 | ppl 72.05 | wps 34347.3 | wpb 511.9 | bsz 1 | num_updates 13621 | best_loss 6.171
2022-03-16 03:42:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13621 updates
2022-03-16 03:42:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 03:42:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 03:42:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 35 @ 13621 updates, score 6.171) (writing took 2.2450893860077485 seconds)
2022-03-16 03:42:16 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 03:42:16 | INFO | train | epoch 035 | loss 6.023 | ppl 65.03 | wps 21492.2 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 13621 | lr 0.000270954 | gnorm 0.542 | loss_scale 16 | train_wall 1049 | gb_free 9.6 | wall 48198
KL Stats: Epoch 35 Divergences: Uniform: 5.346679607709936 Unigram: 3.065972621285877
2022-03-16 03:42:16 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 03:42:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:46:08 | INFO | train_inner | epoch 036:     79 / 392 loss=5.989, ppl=63.52, wps=19680.9, ups=0.3, wpb=65025.8, bsz=127, num_updates=13700, lr=0.000270172, gnorm=0.551, loss_scale=16, train_wall=266, gb_free=9.6, wall=48430
2022-03-16 03:47:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:51:03 | INFO | train_inner | epoch 036:    180 / 392 loss=6.008, ppl=64.34, wps=22174.1, ups=0.34, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.539, loss_scale=16, train_wall=271, gb_free=9.6, wall=48725
2022-03-16 03:53:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:55:59 | INFO | train_inner | epoch 036:    281 / 392 loss=6.02, ppl=64.88, wps=22142.4, ups=0.34, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.54, loss_scale=16, train_wall=272, gb_free=9.6, wall=49021
2022-03-16 03:59:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:00:56 | INFO | train_inner | epoch 036:    382 / 392 loss=6.029, ppl=65.28, wps=22083.1, ups=0.34, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.542, loss_scale=16, train_wall=272, gb_free=9.6, wall=49318
2022-03-16 04:01:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:02:01 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.173 | ppl 72.15 | wps 34171.4 | wpb 511.9 | bsz 1 | num_updates 14010 | best_loss 6.171
2022-03-16 04:02:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14010 updates
2022-03-16 04:02:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 04:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 04:02:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt (epoch 36 @ 14010 updates, score 6.173) (writing took 1.2515282439999282 seconds)
2022-03-16 04:02:02 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 04:02:02 | INFO | train | epoch 036 | loss 6.01 | ppl 64.47 | wps 21454.9 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 14010 | lr 0.000267166 | gnorm 0.543 | loss_scale 16 | train_wall 1052 | gb_free 9.6 | wall 49384
KL Stats: Epoch 36 Divergences: Uniform: 5.361304827665662 Unigram: 3.0739537957937175
2022-03-16 04:02:02 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 04:02:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:06:25 | INFO | train_inner | epoch 037:     90 / 392 loss=5.981, ppl=63.16, wps=19728.9, ups=0.3, wpb=65029.1, bsz=127, num_updates=14100, lr=0.000266312, gnorm=0.549, loss_scale=16, train_wall=266, gb_free=9.6, wall=49648
2022-03-16 04:06:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:11:21 | INFO | train_inner | epoch 037:    191 / 392 loss=5.991, ppl=63.62, wps=22178.2, ups=0.34, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.539, loss_scale=16, train_wall=271, gb_free=9.6, wall=49943
2022-03-16 04:13:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:16:16 | INFO | train_inner | epoch 037:    292 / 392 loss=6.006, ppl=64.26, wps=22181.7, ups=0.34, wpb=65532.7, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.543, loss_scale=16, train_wall=271, gb_free=9.6, wall=50239
2022-03-16 04:19:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:21:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:21:44 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.168 | ppl 71.9 | wps 34083.7 | wpb 511.9 | bsz 1 | num_updates 14399 | best_loss 6.168
2022-03-16 04:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14399 updates
2022-03-16 04:21:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 04:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 04:21:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 37 @ 14399 updates, score 6.168) (writing took 2.231369946966879 seconds)
2022-03-16 04:21:47 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 04:21:47 | INFO | train | epoch 037 | loss 5.998 | ppl 63.91 | wps 21476.3 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 14399 | lr 0.000263532 | gnorm 0.543 | loss_scale 16 | train_wall 1050 | gb_free 9.6 | wall 50569
KL Stats: Epoch 37 Divergences: Uniform: 5.376235817890978 Unigram: 3.0798931650865726
2022-03-16 04:21:47 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 04:21:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:21:50 | INFO | train_inner | epoch 038:      1 / 392 loss=6.018, ppl=64.81, wps=19511.7, ups=0.3, wpb=65029.1, bsz=127, num_updates=14400, lr=0.000263523, gnorm=0.545, loss_scale=16, train_wall=269, gb_free=9.6, wall=50572
2022-03-16 04:23:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:26:45 | INFO | train_inner | epoch 038:    102 / 392 loss=5.964, ppl=62.43, wps=22154.9, ups=0.34, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.548, loss_scale=8, train_wall=271, gb_free=9.6, wall=50868
2022-03-16 04:31:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:31:41 | INFO | train_inner | epoch 038:    203 / 392 loss=5.982, ppl=63.22, wps=22146.2, ups=0.34, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.541, loss_scale=8, train_wall=271, gb_free=9.6, wall=51164
2022-03-16 04:36:34 | INFO | train_inner | epoch 038:    303 / 392 loss=5.999, ppl=63.94, wps=22363.9, ups=0.34, wpb=65532.7, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.539, loss_scale=8, train_wall=269, gb_free=9.6, wall=51457
2022-03-16 04:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:41:31 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.159 | ppl 71.48 | wps 34153.8 | wpb 511.9 | bsz 1 | num_updates 14789 | best_loss 6.159
2022-03-16 04:41:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14789 updates
2022-03-16 04:41:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 04:41:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 04:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 38 @ 14789 updates, score 6.159) (writing took 2.2385236109839752 seconds)
2022-03-16 04:41:33 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 04:41:33 | INFO | train | epoch 038 | loss 5.987 | ppl 63.42 | wps 21499.7 | ups 0.33 | wpb 65405.2 | bsz 127.7 | num_updates 14789 | lr 0.000260034 | gnorm 0.542 | loss_scale 16 | train_wall 1052 | gb_free 9.6 | wall 51755
KL Stats: Epoch 38 Divergences: Uniform: 5.390459439015986 Unigram: 3.0850171739509373
2022-03-16 04:41:33 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 04:41:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:42:05 | INFO | train_inner | epoch 039:     11 / 392 loss=5.997, ppl=63.85, wps=19658.1, ups=0.3, wpb=65029.1, bsz=127, num_updates=14800, lr=0.000259938, gnorm=0.542, loss_scale=16, train_wall=267, gb_free=9.6, wall=51788
2022-03-16 04:44:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:47:00 | INFO | train_inner | epoch 039:    112 / 392 loss=5.955, ppl=62.05, wps=22198.6, ups=0.34, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.543, loss_scale=16, train_wall=271, gb_free=9.6, wall=52083
2022-03-16 04:51:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:51:55 | INFO | train_inner | epoch 039:    213 / 392 loss=5.966, ppl=62.53, wps=22222.3, ups=0.34, wpb=65532.7, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.544, loss_scale=16, train_wall=270, gb_free=9.6, wall=52378
2022-03-16 04:56:47 | INFO | train_inner | epoch 039:    313 / 392 loss=5.99, ppl=63.56, wps=22450.2, ups=0.34, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.538, loss_scale=16, train_wall=268, gb_free=9.6, wall=52670
2022-03-16 04:58:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:59:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:00:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:01:13 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.146 | ppl 70.82 | wps 34161.8 | wpb 511.9 | bsz 1 | num_updates 15177 | best_loss 6.146
2022-03-16 05:01:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15177 updates
2022-03-16 05:01:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 05:01:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 05:01:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 39 @ 15177 updates, score 6.146) (writing took 2.223282436025329 seconds)
2022-03-16 05:01:16 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 05:01:16 | INFO | train | epoch 039 | loss 5.975 | ppl 62.89 | wps 21458.4 | ups 0.33 | wpb 65404.5 | bsz 127.7 | num_updates 15177 | lr 0.000256689 | gnorm 0.544 | loss_scale 8 | train_wall 1048 | gb_free 9.6 | wall 52938
KL Stats: Epoch 39 Divergences: Uniform: 5.403120087753529 Unigram: 3.0893552558404966
2022-03-16 05:01:16 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 05:01:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:02:23 | INFO | train_inner | epoch 040:     23 / 392 loss=5.977, ppl=62.99, wps=19383.2, ups=0.3, wpb=65029.1, bsz=127, num_updates=15200, lr=0.000256495, gnorm=0.55, loss_scale=8, train_wall=271, gb_free=9.6, wall=53005
2022-03-16 05:07:16 | INFO | train_inner | epoch 040:    123 / 392 loss=5.945, ppl=61.62, wps=22347.5, ups=0.34, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.541, loss_scale=16, train_wall=269, gb_free=9.6, wall=53298
2022-03-16 05:12:09 | INFO | train_inner | epoch 040:    223 / 392 loss=5.964, ppl=62.43, wps=22338.7, ups=0.34, wpb=65532.7, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.541, loss_scale=16, train_wall=269, gb_free=9.6, wall=53592
2022-03-16 05:12:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:17:04 | INFO | train_inner | epoch 040:    324 / 392 loss=5.977, ppl=62.97, wps=22205.9, ups=0.34, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.553, loss_scale=8, train_wall=271, gb_free=9.6, wall=53887
2022-03-16 05:20:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:20:59 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.146 | ppl 70.8 | wps 34132.4 | wpb 511.9 | bsz 1 | num_updates 15568 | best_loss 6.146
2022-03-16 05:20:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15568 updates
2022-03-16 05:20:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 05:21:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 05:21:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 40 @ 15568 updates, score 6.146) (writing took 2.2504036999307573 seconds)
2022-03-16 05:21:02 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 05:21:02 | INFO | train | epoch 040 | loss 5.964 | ppl 62.44 | wps 21562 | ups 0.33 | wpb 65405.5 | bsz 127.7 | num_updates 15568 | lr 0.000253445 | gnorm 0.547 | loss_scale 16 | train_wall 1051 | gb_free 9.6 | wall 54124
KL Stats: Epoch 40 Divergences: Uniform: 5.418699375866424 Unigram: 3.095431152672521
2022-03-16 05:21:02 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 05:21:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:22:35 | INFO | train_inner | epoch 041:     32 / 392 loss=5.966, ppl=62.49, wps=19646.2, ups=0.3, wpb=65029.1, bsz=127, num_updates=15600, lr=0.000253185, gnorm=0.548, loss_scale=16, train_wall=267, gb_free=9.6, wall=54218
2022-03-16 05:26:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:27:31 | INFO | train_inner | epoch 041:    133 / 392 loss=5.944, ppl=61.57, wps=22180.8, ups=0.34, wpb=65532.7, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.547, loss_scale=16, train_wall=271, gb_free=9.6, wall=54513
2022-03-16 05:32:24 | INFO | train_inner | epoch 041:    233 / 392 loss=5.955, ppl=62.02, wps=22385.6, ups=0.34, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.542, loss_scale=16, train_wall=269, gb_free=9.6, wall=54806
2022-03-16 05:32:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:37:19 | INFO | train_inner | epoch 041:    334 / 392 loss=5.967, ppl=62.56, wps=22209.9, ups=0.34, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.543, loss_scale=16, train_wall=271, gb_free=9.6, wall=55101
2022-03-16 05:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:40:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:40:43 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.144 | ppl 70.72 | wps 34249 | wpb 511.9 | bsz 1 | num_updates 15957 | best_loss 6.144
2022-03-16 05:40:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 15957 updates
2022-03-16 05:40:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 05:40:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 05:40:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 41 @ 15957 updates, score 6.144) (writing took 2.2596705690957606 seconds)
2022-03-16 05:40:46 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 05:40:46 | INFO | train | epoch 041 | loss 5.954 | ppl 62 | wps 21486.3 | ups 0.33 | wpb 65404.8 | bsz 127.7 | num_updates 15957 | lr 0.000250337 | gnorm 0.546 | loss_scale 16 | train_wall 1049 | gb_free 9.6 | wall 55308
KL Stats: Epoch 41 Divergences: Uniform: 5.432410159631673 Unigram: 3.100642919156866
2022-03-16 05:40:46 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 05:40:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:42:52 | INFO | train_inner | epoch 042:     43 / 392 loss=5.946, ppl=61.66, wps=19530, ups=0.3, wpb=65029.1, bsz=127, num_updates=16000, lr=0.00025, gnorm=0.553, loss_scale=16, train_wall=269, gb_free=9.6, wall=55434
2022-03-16 05:46:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:47:47 | INFO | train_inner | epoch 042:    144 / 392 loss=5.932, ppl=61.06, wps=22179.6, ups=0.34, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.54, loss_scale=16, train_wall=271, gb_free=9.6, wall=55730
2022-03-16 05:49:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:52:42 | INFO | train_inner | epoch 042:    245 / 392 loss=5.943, ppl=61.51, wps=22222.3, ups=0.34, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.548, loss_scale=8, train_wall=270, gb_free=9.6, wall=56024
2022-03-16 05:57:34 | INFO | train_inner | epoch 042:    345 / 392 loss=5.963, ppl=62.39, wps=22424, ups=0.34, wpb=65532.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.537, loss_scale=16, train_wall=268, gb_free=9.6, wall=56317
2022-03-16 05:59:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:00:27 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.139 | ppl 70.48 | wps 34199.6 | wpb 511.9 | bsz 1 | num_updates 16347 | best_loss 6.139
2022-03-16 06:00:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16347 updates
2022-03-16 06:00:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 06:00:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 06:00:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 42 @ 16347 updates, score 6.139) (writing took 2.3541579799493775 seconds)
2022-03-16 06:00:30 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 06:00:30 | INFO | train | epoch 042 | loss 5.944 | ppl 61.57 | wps 21545.2 | ups 0.33 | wpb 65405.2 | bsz 127.7 | num_updates 16347 | lr 0.000247332 | gnorm 0.543 | loss_scale 16 | train_wall 1049 | gb_free 9.6 | wall 56492
KL Stats: Epoch 42 Divergences: Uniform: 5.447152990278562 Unigram: 3.106874029195223
2022-03-16 06:00:30 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 06:00:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:03:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:03:08 | INFO | train_inner | epoch 043:     54 / 392 loss=5.932, ppl=61.06, wps=19498.6, ups=0.3, wpb=65029.1, bsz=127, num_updates=16400, lr=0.000246932, gnorm=0.553, loss_scale=16, train_wall=269, gb_free=9.6, wall=56650
2022-03-16 06:08:00 | INFO | train_inner | epoch 043:    154 / 392 loss=5.923, ppl=60.68, wps=22428.5, ups=0.34, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.549, loss_scale=16, train_wall=268, gb_free=9.6, wall=56942
2022-03-16 06:09:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:12:55 | INFO | train_inner | epoch 043:    255 / 392 loss=5.939, ppl=61.35, wps=22208.4, ups=0.34, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.549, loss_scale=16, train_wall=271, gb_free=9.6, wall=57237
2022-03-16 06:15:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:16:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:17:54 | INFO | train_inner | epoch 043:    357 / 392 loss=5.953, ppl=61.93, wps=21948.8, ups=0.33, wpb=65532.7, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.547, loss_scale=8, train_wall=274, gb_free=9.6, wall=57536
2022-03-16 06:19:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:20:16 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.125 | ppl 69.81 | wps 33584.5 | wpb 511.9 | bsz 1 | num_updates 16735 | best_loss 6.125
2022-03-16 06:20:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16735 updates
2022-03-16 06:20:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 06:20:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 06:20:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 43 @ 16735 updates, score 6.125) (writing took 2.211391312070191 seconds)
2022-03-16 06:20:18 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 06:20:18 | INFO | train | epoch 043 | loss 5.934 | ppl 61.16 | wps 21359.7 | ups 0.33 | wpb 65404.5 | bsz 127.7 | num_updates 16735 | lr 0.000244448 | gnorm 0.55 | loss_scale 8 | train_wall 1053 | gb_free 9.6 | wall 57680
KL Stats: Epoch 43 Divergences: Uniform: 5.458052968735279 Unigram: 3.1106961755893776
2022-03-16 06:20:18 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 06:20:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:23:35 | INFO | train_inner | epoch 044:     65 / 392 loss=5.917, ppl=60.43, wps=19082, ups=0.29, wpb=65029.1, bsz=127, num_updates=16800, lr=0.000243975, gnorm=0.558, loss_scale=8, train_wall=276, gb_free=9.6, wall=57877
2022-03-16 06:28:36 | INFO | train_inner | epoch 044:    165 / 392 loss=5.91, ppl=60.13, wps=21744.9, ups=0.33, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.545, loss_scale=16, train_wall=277, gb_free=9.6, wall=58178
2022-03-16 06:30:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:33:41 | INFO | train_inner | epoch 044:    266 / 392 loss=5.939, ppl=61.35, wps=21516.3, ups=0.33, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.543, loss_scale=16, train_wall=280, gb_free=9.6, wall=58483
2022-03-16 06:37:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:38:44 | INFO | train_inner | epoch 044:    367 / 392 loss=5.938, ppl=61.32, wps=21564.5, ups=0.33, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.546, loss_scale=16, train_wall=279, gb_free=9.6, wall=58787
2022-03-16 06:39:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:40:36 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.13 | ppl 70.06 | wps 33583.3 | wpb 511.9 | bsz 1 | num_updates 17125 | best_loss 6.125
2022-03-16 06:40:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17125 updates
2022-03-16 06:40:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 06:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 06:40:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt (epoch 44 @ 17125 updates, score 6.13) (writing took 1.2900230900850147 seconds)
2022-03-16 06:40:37 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 06:40:37 | INFO | train | epoch 044 | loss 5.926 | ppl 60.8 | wps 20920.6 | ups 0.32 | wpb 65405.2 | bsz 127.7 | num_updates 17125 | lr 0.000241649 | gnorm 0.549 | loss_scale 16 | train_wall 1084 | gb_free 9.6 | wall 58899
KL Stats: Epoch 44 Divergences: Uniform: 5.471955698709338 Unigram: 3.1165227204434944
2022-03-16 06:40:37 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 06:40:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:44:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:44:27 | INFO | train_inner | epoch 045:     76 / 392 loss=5.906, ppl=59.95, wps=18997.9, ups=0.29, wpb=65025.8, bsz=127, num_updates=17200, lr=0.000241121, gnorm=0.553, loss_scale=16, train_wall=278, gb_free=9.6, wall=59129
2022-03-16 06:49:28 | INFO | train_inner | epoch 045:    176 / 392 loss=5.904, ppl=59.89, wps=21758.3, ups=0.33, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.549, loss_scale=16, train_wall=277, gb_free=9.6, wall=59430
2022-03-16 06:50:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:54:24 | INFO | train_inner | epoch 045:    277 / 392 loss=5.929, ppl=60.91, wps=22166.3, ups=0.34, wpb=65532.7, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.553, loss_scale=16, train_wall=271, gb_free=9.6, wall=59726
2022-03-16 06:57:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:59:19 | INFO | train_inner | epoch 045:    378 / 392 loss=5.938, ppl=61.32, wps=22184.2, ups=0.34, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.55, loss_scale=16, train_wall=271, gb_free=9.6, wall=60021
2022-03-16 06:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:00:36 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.121 | ppl 69.62 | wps 34186.1 | wpb 511.9 | bsz 1 | num_updates 17514 | best_loss 6.121
2022-03-16 07:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17514 updates
2022-03-16 07:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 07:00:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 07:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 45 @ 17514 updates, score 6.121) (writing took 2.296300841961056 seconds)
2022-03-16 07:00:38 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 07:00:38 | INFO | train | epoch 045 | loss 5.918 | ppl 60.44 | wps 21185.6 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 17514 | lr 0.00023895 | gnorm 0.55 | loss_scale 16 | train_wall 1066 | gb_free 9.6 | wall 60100
KL Stats: Epoch 45 Divergences: Uniform: 5.48336746254905 Unigram: 3.120568896868963
2022-03-16 07:00:38 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 07:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:04:50 | INFO | train_inner | epoch 046:     86 / 392 loss=5.89, ppl=59.31, wps=19651.3, ups=0.3, wpb=65025.8, bsz=127, num_updates=17600, lr=0.000238366, gnorm=0.559, loss_scale=32, train_wall=267, gb_free=9.6, wall=60352
2022-03-16 07:05:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:09:49 | INFO | train_inner | epoch 046:    187 / 392 loss=5.906, ppl=59.97, wps=21926.7, ups=0.33, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.556, loss_scale=16, train_wall=274, gb_free=9.6, wall=60651
2022-03-16 07:11:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:14:54 | INFO | train_inner | epoch 046:    288 / 392 loss=5.917, ppl=60.42, wps=21444.8, ups=0.33, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.55, loss_scale=16, train_wall=281, gb_free=9.6, wall=60957
2022-03-16 07:16:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:19:59 | INFO | train_inner | epoch 046:    389 / 392 loss=5.923, ppl=60.66, wps=21537.6, ups=0.33, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.557, loss_scale=8, train_wall=280, gb_free=9.6, wall=61261
2022-03-16 07:20:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:20:44 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.117 | ppl 69.39 | wps 33697.3 | wpb 511.9 | bsz 1 | num_updates 17903 | best_loss 6.117
2022-03-16 07:20:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 17903 updates
2022-03-16 07:20:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 07:20:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 07:20:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 46 @ 17903 updates, score 6.117) (writing took 2.2230298870708793 seconds)
2022-03-16 07:20:46 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 07:20:46 | INFO | train | epoch 046 | loss 5.908 | ppl 60.07 | wps 21063.3 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 17903 | lr 0.00023634 | gnorm 0.556 | loss_scale 8 | train_wall 1072 | gb_free 9.6 | wall 61308
KL Stats: Epoch 46 Divergences: Uniform: 5.495855477612159 Unigram: 3.1248942832711832
2022-03-16 07:20:46 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 07:20:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:25:38 | INFO | train_inner | epoch 047:     97 / 392 loss=5.878, ppl=58.83, wps=19189.5, ups=0.3, wpb=65029.1, bsz=127, num_updates=18000, lr=0.000235702, gnorm=0.551, loss_scale=16, train_wall=274, gb_free=9.6, wall=61600
2022-03-16 07:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:30:42 | INFO | train_inner | epoch 047:    198 / 392 loss=5.893, ppl=59.43, wps=21495.3, ups=0.33, wpb=65532.7, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.548, loss_scale=16, train_wall=280, gb_free=9.6, wall=61905
2022-03-16 07:35:43 | INFO | train_inner | epoch 047:    298 / 392 loss=5.909, ppl=60.07, wps=21782.1, ups=0.33, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.549, loss_scale=16, train_wall=277, gb_free=9.6, wall=62206
2022-03-16 07:36:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:40:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:41:03 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.113 | ppl 69.23 | wps 33527.8 | wpb 511.9 | bsz 1 | num_updates 18293 | best_loss 6.113
2022-03-16 07:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18293 updates
2022-03-16 07:41:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 07:41:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 07:41:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 47 @ 18293 updates, score 6.113) (writing took 2.2658996590180323 seconds)
2022-03-16 07:41:05 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 07:41:05 | INFO | train | epoch 047 | loss 5.9 | ppl 59.72 | wps 20925.3 | ups 0.32 | wpb 65405.2 | bsz 127.7 | num_updates 18293 | lr 0.000233807 | gnorm 0.55 | loss_scale 16 | train_wall 1083 | gb_free 9.6 | wall 62527
KL Stats: Epoch 47 Divergences: Uniform: 5.505616089689496 Unigram: 3.1292235538347972
2022-03-16 07:41:05 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 07:41:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:41:26 | INFO | train_inner | epoch 048:      7 / 392 loss=5.917, ppl=60.43, wps=18957, ups=0.29, wpb=65029.1, bsz=127, num_updates=18300, lr=0.000233762, gnorm=0.556, loss_scale=16, train_wall=278, gb_free=9.6, wall=62549
2022-03-16 07:44:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:46:31 | INFO | train_inner | epoch 048:    108 / 392 loss=5.872, ppl=58.57, wps=21504.9, ups=0.33, wpb=65532.7, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.546, loss_scale=16, train_wall=280, gb_free=9.6, wall=62853
2022-03-16 07:50:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:51:36 | INFO | train_inner | epoch 048:    209 / 392 loss=5.887, ppl=59.18, wps=21524.1, ups=0.33, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.549, loss_scale=16, train_wall=280, gb_free=9.6, wall=63158
2022-03-16 07:56:37 | INFO | train_inner | epoch 048:    309 / 392 loss=5.906, ppl=59.95, wps=21734.9, ups=0.33, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.543, loss_scale=16, train_wall=277, gb_free=9.6, wall=63459
2022-03-16 07:57:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:01:24 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.111 | ppl 69.14 | wps 33602 | wpb 511.9 | bsz 1 | num_updates 18681 | best_loss 6.111
2022-03-16 08:01:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18681 updates
2022-03-16 08:01:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 08:01:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 08:01:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 48 @ 18681 updates, score 6.111) (writing took 2.2827517869882286 seconds)
2022-03-16 08:01:27 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 08:01:27 | INFO | train | epoch 048 | loss 5.893 | ppl 59.41 | wps 20770.2 | ups 0.32 | wpb 65404.5 | bsz 127.7 | num_updates 18681 | lr 0.000231366 | gnorm 0.548 | loss_scale 8 | train_wall 1085 | gb_free 9.6 | wall 63749
KL Stats: Epoch 48 Divergences: Uniform: 5.516079107833098 Unigram: 3.132473899203732
2022-03-16 08:01:27 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 08:01:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:02:24 | INFO | train_inner | epoch 049:     19 / 392 loss=5.9, ppl=59.72, wps=18724.3, ups=0.29, wpb=65029.1, bsz=127, num_updates=18700, lr=0.000231249, gnorm=0.553, loss_scale=8, train_wall=282, gb_free=9.6, wall=63807
2022-03-16 08:07:26 | INFO | train_inner | epoch 049:    119 / 392 loss=5.867, ppl=58.36, wps=21714.9, ups=0.33, wpb=65532.7, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.553, loss_scale=16, train_wall=277, gb_free=9.6, wall=64108
2022-03-16 08:12:28 | INFO | train_inner | epoch 049:    219 / 392 loss=5.885, ppl=59.1, wps=21704.6, ups=0.33, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.549, loss_scale=16, train_wall=278, gb_free=9.6, wall=64410
2022-03-16 08:13:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:17:33 | INFO | train_inner | epoch 049:    320 / 392 loss=5.895, ppl=59.5, wps=21488.7, ups=0.33, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.549, loss_scale=16, train_wall=280, gb_free=9.6, wall=64715
2022-03-16 08:20:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:21:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:21:46 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.113 | ppl 69.22 | wps 33729.5 | wpb 511.9 | bsz 1 | num_updates 19071 | best_loss 6.111
2022-03-16 08:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19071 updates
2022-03-16 08:21:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 08:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 08:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt (epoch 49 @ 19071 updates, score 6.113) (writing took 1.2849372919881716 seconds)
2022-03-16 08:21:48 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 08:21:48 | INFO | train | epoch 049 | loss 5.885 | ppl 59.11 | wps 20893.2 | ups 0.32 | wpb 65405.2 | bsz 127.7 | num_updates 19071 | lr 0.000228988 | gnorm 0.552 | loss_scale 16 | train_wall 1086 | gb_free 9.6 | wall 64970
KL Stats: Epoch 49 Divergences: Uniform: 5.528671535049989 Unigram: 3.1377446377536855
2022-03-16 08:21:48 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 08:21:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:22:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:23:18 | INFO | train_inner | epoch 050:     30 / 392 loss=5.886, ppl=59.16, wps=18825.8, ups=0.29, wpb=65029.1, bsz=127, num_updates=19100, lr=0.000228814, gnorm=0.554, loss_scale=8, train_wall=281, gb_free=9.6, wall=65061
2022-03-16 08:28:20 | INFO | train_inner | epoch 050:    130 / 392 loss=5.856, ppl=57.92, wps=21700, ups=0.33, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.55, loss_scale=8, train_wall=278, gb_free=9.6, wall=65363
2022-03-16 08:33:23 | INFO | train_inner | epoch 050:    230 / 392 loss=5.878, ppl=58.82, wps=21676.4, ups=0.33, wpb=65532.7, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.557, loss_scale=16, train_wall=278, gb_free=9.6, wall=65665
2022-03-16 08:35:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:38:28 | INFO | train_inner | epoch 050:    331 / 392 loss=5.892, ppl=59.4, wps=21504.5, ups=0.33, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.547, loss_scale=16, train_wall=280, gb_free=9.6, wall=65970
2022-03-16 08:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:42:07 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.105 | ppl 68.85 | wps 33697.9 | wpb 511.9 | bsz 1 | num_updates 19461 | best_loss 6.105
2022-03-16 08:42:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19461 updates
2022-03-16 08:42:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 08:42:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 08:42:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 50 @ 19461 updates, score 6.105) (writing took 2.218174342997372 seconds)
2022-03-16 08:42:09 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 08:42:09 | INFO | train | epoch 050 | loss 5.878 | ppl 58.8 | wps 20877.5 | ups 0.32 | wpb 65405.2 | bsz 127.7 | num_updates 19461 | lr 0.000226682 | gnorm 0.552 | loss_scale 16 | train_wall 1085 | gb_free 9.6 | wall 66192
KL Stats: Epoch 50 Divergences: Uniform: 5.539371648615648 Unigram: 3.1427180652156723
2022-03-16 08:42:09 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 08:42:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:42:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:44:09 | INFO | train_inner | epoch 051:     40 / 392 loss=5.884, ppl=59.06, wps=19044.3, ups=0.29, wpb=65029.1, bsz=127, num_updates=19500, lr=0.000226455, gnorm=0.555, loss_scale=16, train_wall=276, gb_free=9.6, wall=66311
2022-03-16 08:49:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:49:13 | INFO | train_inner | epoch 051:    141 / 392 loss=5.853, ppl=57.79, wps=21562.2, ups=0.33, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.552, loss_scale=16, train_wall=279, gb_free=9.6, wall=66615
2022-03-16 08:52:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:54:17 | INFO | train_inner | epoch 051:    242 / 392 loss=5.87, ppl=58.5, wps=21551.6, ups=0.33, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.545, loss_scale=8, train_wall=279, gb_free=9.6, wall=66919
2022-03-16 08:59:19 | INFO | train_inner | epoch 051:    342 / 392 loss=5.891, ppl=59.35, wps=21732.5, ups=0.33, wpb=65532.7, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.546, loss_scale=16, train_wall=277, gb_free=9.6, wall=67221
2022-03-16 09:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:02:25 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.103 | ppl 68.72 | wps 33605.2 | wpb 511.9 | bsz 1 | num_updates 19850 | best_loss 6.103
2022-03-16 09:02:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 19850 updates
2022-03-16 09:02:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:02:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:02:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 51 @ 19850 updates, score 6.103) (writing took 2.286942691076547 seconds)
2022-03-16 09:02:28 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 09:02:28 | INFO | train | epoch 051 | loss 5.871 | ppl 58.51 | wps 20885.3 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 19850 | lr 0.00022445 | gnorm 0.55 | loss_scale 16 | train_wall 1082 | gb_free 9.6 | wall 67410
KL Stats: Epoch 51 Divergences: Uniform: 5.549882127944131 Unigram: 3.1462130068303233
2022-03-16 09:02:28 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 09:02:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:04:58 | INFO | train_inner | epoch 052:     50 / 392 loss=5.863, ppl=58.22, wps=19141.5, ups=0.29, wpb=65029.1, bsz=127, num_updates=19900, lr=0.000224168, gnorm=0.553, loss_scale=16, train_wall=275, gb_free=9.6, wall=67561
2022-03-16 09:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:10:02 | INFO | train_inner | epoch 052:    151 / 392 loss=5.849, ppl=57.62, wps=21552, ups=0.33, wpb=65532.7, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.554, loss_scale=16, train_wall=279, gb_free=9.6, wall=67865
2022-03-16 09:12:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:15:07 | INFO | train_inner | epoch 052:    252 / 392 loss=5.866, ppl=58.34, wps=21498.8, ups=0.33, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.555, loss_scale=8, train_wall=280, gb_free=9.6, wall=68170
2022-03-16 09:20:09 | INFO | train_inner | epoch 052:    352 / 392 loss=5.882, ppl=58.96, wps=21719.4, ups=0.33, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.554, loss_scale=16, train_wall=277, gb_free=9.6, wall=68471
2022-03-16 09:22:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:22:46 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.098 | ppl 68.5 | wps 33520 | wpb 511.9 | bsz 1 | num_updates 20240 | best_loss 6.098
2022-03-16 09:22:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20240 updates
2022-03-16 09:22:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:22:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:22:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 52 @ 20240 updates, score 6.098) (writing took 2.258517325972207 seconds)
2022-03-16 09:22:48 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 09:22:48 | INFO | train | epoch 052 | loss 5.864 | ppl 58.22 | wps 20897 | ups 0.32 | wpb 65405.2 | bsz 127.7 | num_updates 20240 | lr 0.000222277 | gnorm 0.553 | loss_scale 16 | train_wall 1084 | gb_free 9.6 | wall 68631
KL Stats: Epoch 52 Divergences: Uniform: 5.559631740822121 Unigram: 3.14956249485416
2022-03-16 09:22:48 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 09:22:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:25:49 | INFO | train_inner | epoch 053:     60 / 392 loss=5.847, ppl=57.55, wps=19134.9, ups=0.29, wpb=65029.1, bsz=127, num_updates=20300, lr=0.000221948, gnorm=0.558, loss_scale=32, train_wall=275, gb_free=9.6, wall=68811
2022-03-16 09:25:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:25:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:30:40 | INFO | train_inner | epoch 053:    162 / 392 loss=5.851, ppl=57.7, wps=22532, ups=0.34, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.566, loss_scale=8, train_wall=266, gb_free=9.6, wall=69102
2022-03-16 09:35:17 | INFO | train_inner | epoch 053:    262 / 392 loss=5.861, ppl=58.1, wps=23604, ups=0.36, wpb=65532.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.55, loss_scale=16, train_wall=254, gb_free=9.6, wall=69380
2022-03-16 09:38:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:39:51 | INFO | train_inner | epoch 053:    363 / 392 loss=5.878, ppl=58.79, wps=23982.8, ups=0.37, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.553, loss_scale=16, train_wall=249, gb_free=9.6, wall=69653
2022-03-16 09:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:41:43 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.093 | ppl 68.27 | wps 36510.4 | wpb 511.9 | bsz 1 | num_updates 20629 | best_loss 6.093
2022-03-16 09:41:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20629 updates
2022-03-16 09:41:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:41:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:41:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 53 @ 20629 updates, score 6.093) (writing took 2.2965946350013837 seconds)
2022-03-16 09:41:46 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 09:41:46 | INFO | train | epoch 053 | loss 5.858 | ppl 58 | wps 22370.6 | ups 0.34 | wpb 65404.8 | bsz 127.7 | num_updates 20629 | lr 0.000220171 | gnorm 0.558 | loss_scale 16 | train_wall 1006 | gb_free 9.6 | wall 69768
KL Stats: Epoch 53 Divergences: Uniform: 5.569924796354481 Unigram: 3.152501020167995
2022-03-16 09:41:46 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 09:41:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:44:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:44:59 | INFO | train_inner | epoch 054:     72 / 392 loss=5.837, ppl=57.16, wps=21049.6, ups=0.32, wpb=65025.8, bsz=127, num_updates=20700, lr=0.000219793, gnorm=0.555, loss_scale=8, train_wall=247, gb_free=9.6, wall=69962
2022-03-16 09:49:29 | INFO | train_inner | epoch 054:    172 / 392 loss=5.846, ppl=57.52, wps=24327.2, ups=0.37, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.55, loss_scale=8, train_wall=246, gb_free=9.6, wall=70231
2022-03-16 09:53:58 | INFO | train_inner | epoch 054:    272 / 392 loss=5.858, ppl=57.98, wps=24318.1, ups=0.37, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.557, loss_scale=16, train_wall=246, gb_free=9.6, wall=70501
2022-03-16 09:56:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:58:30 | INFO | train_inner | epoch 054:    373 / 392 loss=5.869, ppl=58.46, wps=24110.2, ups=0.37, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.547, loss_scale=16, train_wall=248, gb_free=9.6, wall=70773
2022-03-16 09:59:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:59:55 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.087 | ppl 67.99 | wps 36444.6 | wpb 511.9 | bsz 1 | num_updates 21019 | best_loss 6.087
2022-03-16 09:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21019 updates
2022-03-16 09:59:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt
2022-03-16 09:59:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_best.pt (epoch 54 @ 21019 updates, score 6.087) (writing took 2.3017433900386095 seconds)
2022-03-16 09:59:57 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 09:59:57 | INFO | train | epoch 054 | loss 5.851 | ppl 57.73 | wps 23370.5 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 21019 | lr 0.000218119 | gnorm 0.552 | loss_scale 16 | train_wall 961 | gb_free 9.6 | wall 70859
KL Stats: Epoch 54 Divergences: Uniform: 5.579801565269667 Unigram: 3.1555982085873358
2022-03-16 09:59:57 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 09:59:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:02:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:02:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:03:41 | INFO | train_inner | epoch 055:     83 / 392 loss=5.827, ppl=56.76, wps=20951.3, ups=0.32, wpb=65029.1, bsz=127, num_updates=21100, lr=0.0002177, gnorm=0.562, loss_scale=8, train_wall=249, gb_free=9.6, wall=71083
2022-03-16 10:08:10 | INFO | train_inner | epoch 055:    183 / 392 loss=5.838, ppl=57.18, wps=24344.1, ups=0.37, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.545, loss_scale=8, train_wall=245, gb_free=9.6, wall=71352
2022-03-16 10:12:39 | INFO | train_inner | epoch 055:    283 / 392 loss=5.854, ppl=57.82, wps=24350.5, ups=0.37, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.55, loss_scale=16, train_wall=246, gb_free=9.6, wall=71621
2022-03-16 10:14:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:17:11 | INFO | train_inner | epoch 055:    384 / 392 loss=5.859, ppl=58.06, wps=24047, ups=0.37, wpb=65532.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.551, loss_scale=16, train_wall=249, gb_free=9.6, wall=71894
2022-03-16 10:17:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:18:06 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.093 | ppl 68.25 | wps 36498.7 | wpb 511.9 | bsz 1 | num_updates 21408 | best_loss 6.087
2022-03-16 10:18:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21408 updates
2022-03-16 10:18:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 10:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 10:18:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt (epoch 55 @ 21408 updates, score 6.093) (writing took 1.323327520978637 seconds)
2022-03-16 10:18:08 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 10:18:08 | INFO | train | epoch 055 | loss 5.845 | ppl 57.47 | wps 23329 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 21408 | lr 0.000216128 | gnorm 0.552 | loss_scale 16 | train_wall 961 | gb_free 9.6 | wall 71950
KL Stats: Epoch 55 Divergences: Uniform: 5.591639883970299 Unigram: 3.1599479278856912
2022-03-16 10:18:08 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 10:18:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:20:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:22:19 | INFO | train_inner | epoch 056:     93 / 392 loss=5.813, ppl=56.24, wps=21125, ups=0.32, wpb=65029.1, bsz=127, num_updates=21500, lr=0.000215666, gnorm=0.566, loss_scale=16, train_wall=247, gb_free=9.6, wall=72202
2022-03-16 10:26:57 | INFO | train_inner | epoch 056:    193 / 392 loss=5.837, ppl=57.15, wps=23597.7, ups=0.36, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.555, loss_scale=32, train_wall=254, gb_free=9.6, wall=72479
2022-03-16 10:27:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:31:36 | INFO | train_inner | epoch 056:    294 / 392 loss=5.853, ppl=57.8, wps=23458.8, ups=0.36, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.56, loss_scale=16, train_wall=255, gb_free=9.6, wall=72759
2022-03-16 10:33:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:36:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:36:41 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.088 | ppl 68.04 | wps 35527 | wpb 511.9 | bsz 1 | num_updates 21797 | best_loss 6.087
2022-03-16 10:36:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21797 updates
2022-03-16 10:36:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 10:36:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 10:36:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt (epoch 56 @ 21797 updates, score 6.088) (writing took 1.2529815000016242 seconds)
2022-03-16 10:36:43 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 10:36:43 | INFO | train | epoch 056 | loss 5.839 | ppl 57.24 | wps 22817.6 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 21797 | lr 0.000214191 | gnorm 0.56 | loss_scale 16 | train_wall 984 | gb_free 9.6 | wall 73065
KL Stats: Epoch 56 Divergences: Uniform: 5.599475699673443 Unigram: 3.1639394910469907
2022-03-16 10:36:43 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 10:36:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:36:51 | INFO | train_inner | epoch 057:      3 / 392 loss=5.857, ppl=57.96, wps=20662.6, ups=0.32, wpb=65025.8, bsz=127, num_updates=21800, lr=0.000214176, gnorm=0.564, loss_scale=16, train_wall=253, gb_free=9.6, wall=73073
2022-03-16 10:39:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:41:30 | INFO | train_inner | epoch 057:    104 / 392 loss=5.805, ppl=55.91, wps=23462.8, ups=0.36, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.553, loss_scale=16, train_wall=255, gb_free=9.6, wall=73353
2022-03-16 10:46:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:46:10 | INFO | train_inner | epoch 057:    205 / 392 loss=5.829, ppl=56.84, wps=23426.7, ups=0.36, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.557, loss_scale=16, train_wall=256, gb_free=9.6, wall=73632
2022-03-16 10:50:46 | INFO | train_inner | epoch 057:    305 / 392 loss=5.853, ppl=57.8, wps=23725.4, ups=0.36, wpb=65532.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.555, loss_scale=16, train_wall=253, gb_free=9.6, wall=73909
2022-03-16 10:52:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:53:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:54:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:55:21 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.089 | ppl 68.06 | wps 35468.6 | wpb 511.9 | bsz 1 | num_updates 22185 | best_loss 6.087
2022-03-16 10:55:21 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 10:55:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22185 updates
2022-03-16 10:55:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 10:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt
2022-03-16 10:55:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.0_0.05_0.95/checkpoint_last.pt (epoch 57 @ 22185 updates, score 6.089) (writing took 1.2408545369980857 seconds)
2022-03-16 10:55:22 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 10:55:22 | INFO | train | epoch 057 | loss 5.833 | ppl 56.99 | wps 22672 | ups 0.35 | wpb 65404.5 | bsz 127.7 | num_updates 22185 | lr 0.00021231 | gnorm 0.558 | loss_scale 8 | train_wall 989 | gb_free 9.6 | wall 74184
2022-03-16 10:55:22 | INFO | fairseq_cli.train | done training in 74184.1 seconds
