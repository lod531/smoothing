Sender: LSF System <lsfadmin@eu-g3-069>
Subject: Job 207133289: <w103_size_0.0625_fp16_label_smoothing_0.01_#1> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_label_smoothing_0.01_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:38:20 2022
Job was executed on host(s) <eu-g3-069>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 10:34:23 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 10:34:23 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.01 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   89384.96 sec.
    Max Memory :                                 8148 MB
    Average Memory :                             4117.51 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11852.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   89458 sec.
    Turnaround time :                            92822 sec.

The output (if any) follows:

2022-03-04 10:34:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.01, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 10:34:30 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 10:34:31 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 10:34:31 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 10:34:31 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 10:34:31 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 10:34:31 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 10:34:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 10:34:31 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 10:34:34 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 10:34:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 10:34:34 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-04 10:34:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 10:34:34 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 10:34:34 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 10:34:34 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 10:34:34 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 10:34:34 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 10:34:34 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 10:34:34 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 10:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:34:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 10:34:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:34:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:34:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:35:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 10:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:39:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.509 | nll_loss 14.475 | ppl 22780.3 | wps 45371.1 | wpb 510.9 | bsz 1 | num_updates 92
2022-03-04 10:39:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 92 updates
2022-03-04 10:39:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:39:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:39:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 1 @ 92 updates, score 14.509) (writing took 4.664631276391447 seconds)
2022-03-04 10:39:07 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 10:39:07 | INFO | train | epoch 001 | loss 15.953 | nll_loss 15.934 | ppl 62594.8 | wps 24382.9 | ups 0.37 | wpb 65489.2 | bsz 127.9 | num_updates 92 | lr 1.15977e-05 | gnorm 3.498 | loss_scale 4 | train_wall 242 | gb_free 21 | wall 273
2022-03-04 10:39:07 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 10:39:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:39:28 | INFO | train_inner | epoch 002:      8 / 97 loss=15.839, nll_loss=15.818, ppl=57786.6, wps=24468.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.358, loss_scale=4, train_wall=260, gb_free=21, wall=294
2022-03-04 10:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:43:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.867 | nll_loss 12.816 | ppl 7210.86 | wps 45197.5 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 12.867
2022-03-04 10:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 10:43:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:43:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 2 @ 189 updates, score 12.867) (writing took 4.847785300575197 seconds)
2022-03-04 10:43:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 10:43:25 | INFO | train | epoch 002 | loss 13.828 | nll_loss 13.788 | ppl 14147.3 | wps 24659.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.595 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 531
2022-03-04 10:43:25 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 10:43:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:43:53 | INFO | train_inner | epoch 003:     11 / 97 loss=13.662, nll_loss=13.62, ppl=12590.9, wps=24692, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.556, loss_scale=8, train_wall=233, gb_free=21, wall=559
2022-03-04 10:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:47:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.177 | nll_loss 11.104 | ppl 2201.34 | wps 45292.9 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.177
2022-03-04 10:47:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 10:47:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:47:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.177) (writing took 4.921513069421053 seconds)
2022-03-04 10:47:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 10:47:42 | INFO | train | epoch 003 | loss 12.008 | nll_loss 11.947 | ppl 3947.78 | wps 24672.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 1.127 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 788
2022-03-04 10:47:42 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 10:47:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:48:18 | INFO | train_inner | epoch 004:     14 / 97 loss=11.789, nll_loss=11.725, ppl=3384.74, wps=24701.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=1.054, loss_scale=16, train_wall=233, gb_free=21, wall=824
2022-03-04 10:51:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:51:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.362 | nll_loss 10.271 | ppl 1235.76 | wps 44946.7 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 10.362
2022-03-04 10:51:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 10:51:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:51:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:52:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 4 @ 383 updates, score 10.362) (writing took 4.8343073995783925 seconds)
2022-03-04 10:52:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 10:52:00 | INFO | train | epoch 004 | loss 10.685 | nll_loss 10.602 | ppl 1554.49 | wps 24675.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.601 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 1046
2022-03-04 10:52:00 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 10:52:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:52:43 | INFO | train_inner | epoch 005:     17 / 97 loss=10.561, nll_loss=10.476, ppl=1423.93, wps=24702.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.558, loss_scale=32, train_wall=233, gb_free=21, wall=1089
2022-03-04 10:56:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:56:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.99 | nll_loss 9.888 | ppl 947.81 | wps 45403.8 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 9.99
2022-03-04 10:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 10:56:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:56:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:56:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 5 @ 480 updates, score 9.99) (writing took 4.938962113112211 seconds)
2022-03-04 10:56:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:56:17 | INFO | train | epoch 005 | loss 10.129 | nll_loss 10.032 | ppl 1046.62 | wps 24664.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.495 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1303
2022-03-04 10:56:17 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:56:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:57:08 | INFO | train_inner | epoch 006:     20 / 97 loss=10.058, nll_loss=9.959, ppl=995.57, wps=24691, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.506, loss_scale=32, train_wall=233, gb_free=21, wall=1354
2022-03-04 10:57:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:00:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:00:30 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.689 | nll_loss 9.581 | ppl 765.83 | wps 45360.6 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 9.689
2022-03-04 11:00:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 11:00:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:00:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 6 @ 576 updates, score 9.689) (writing took 4.820190516300499 seconds)
2022-03-04 11:00:35 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 11:00:35 | INFO | train | epoch 006 | loss 9.788 | nll_loss 9.683 | ppl 821.93 | wps 24415.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.561 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1561
2022-03-04 11:00:35 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 11:00:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:01:36 | INFO | train_inner | epoch 007:     24 / 97 loss=9.714, nll_loss=9.607, ppl=779.99, wps=24467.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.573, loss_scale=32, train_wall=235, gb_free=21, wall=1622
2022-03-04 11:03:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:04:47 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.429 | nll_loss 9.315 | ppl 636.92 | wps 45548.1 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 9.429
2022-03-04 11:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 11:04:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 7 @ 672 updates, score 9.429) (writing took 4.938016824424267 seconds)
2022-03-04 11:04:52 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 11:04:52 | INFO | train | epoch 007 | loss 9.494 | nll_loss 9.383 | ppl 667.62 | wps 24413.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.627 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1818
2022-03-04 11:04:52 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 11:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:06:04 | INFO | train_inner | epoch 008:     28 / 97 loss=9.422, nll_loss=9.309, ppl=634.43, wps=24461.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.666, loss_scale=32, train_wall=235, gb_free=21, wall=1890
2022-03-04 11:09:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:09:05 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.201 | nll_loss 9.082 | ppl 542.11 | wps 45386.9 | wpb 510.9 | bsz 1 | num_updates 769 | best_loss 9.201
2022-03-04 11:09:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 769 updates
2022-03-04 11:09:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 8 @ 769 updates, score 9.201) (writing took 4.87616391479969 seconds)
2022-03-04 11:09:09 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 11:09:09 | INFO | train | epoch 008 | loss 9.231 | nll_loss 9.115 | ppl 554.4 | wps 24687.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 769 | lr 9.62058e-05 | gnorm 0.774 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2076
2022-03-04 11:09:10 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 11:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:09:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:10:31 | INFO | train_inner | epoch 009:     32 / 97 loss=9.155, nll_loss=9.038, ppl=525.56, wps=24477.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.781, loss_scale=32, train_wall=235, gb_free=21, wall=2157
2022-03-04 11:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:13:22 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.983 | nll_loss 8.86 | ppl 464.72 | wps 45391.4 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 8.983
2022-03-04 11:13:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 11:13:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 9 @ 865 updates, score 8.983) (writing took 4.978440296836197 seconds)
2022-03-04 11:13:27 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 11:13:27 | INFO | train | epoch 009 | loss 8.984 | nll_loss 8.864 | ppl 465.79 | wps 24403.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.831 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2333
2022-03-04 11:13:27 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 11:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:14:57 | INFO | train_inner | epoch 010:     35 / 97 loss=8.9, nll_loss=8.777, ppl=438.82, wps=24689.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.865, loss_scale=32, train_wall=233, gb_free=21, wall=2423
2022-03-04 11:15:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:17:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.807 | nll_loss 8.68 | ppl 410.21 | wps 45182.6 | wpb 510.9 | bsz 1 | num_updates 961 | best_loss 8.807
2022-03-04 11:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 961 updates
2022-03-04 11:17:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:17:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 10 @ 961 updates, score 8.807) (writing took 4.861311847344041 seconds)
2022-03-04 11:17:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 11:17:45 | INFO | train | epoch 010 | loss 8.753 | nll_loss 8.628 | ppl 395.54 | wps 24421.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 961 | lr 0.000120201 | gnorm 0.897 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2591
2022-03-04 11:17:45 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 11:17:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:19:24 | INFO | train_inner | epoch 011:     39 / 97 loss=8.665, nll_loss=8.537, ppl=371.56, wps=24462.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.899, loss_scale=32, train_wall=235, gb_free=21, wall=2690
2022-03-04 11:20:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:21:57 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.635 | nll_loss 8.503 | ppl 362.88 | wps 45389 | wpb 510.9 | bsz 1 | num_updates 1057 | best_loss 8.635
2022-03-04 11:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1057 updates
2022-03-04 11:21:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:21:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:22:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 11 @ 1057 updates, score 8.635) (writing took 4.9849478313699365 seconds)
2022-03-04 11:22:02 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 11:22:02 | INFO | train | epoch 011 | loss 8.543 | nll_loss 8.414 | ppl 341.06 | wps 24399.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1057 | lr 0.000132199 | gnorm 0.896 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2848
2022-03-04 11:22:02 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 11:22:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:23:52 | INFO | train_inner | epoch 012:     43 / 97 loss=8.462, nll_loss=8.33, ppl=321.89, wps=24452, ups=0.37, wpb=65495, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.912, loss_scale=32, train_wall=235, gb_free=21, wall=2958
2022-03-04 11:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:26:15 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.483 | nll_loss 8.35 | ppl 326.25 | wps 45436.6 | wpb 510.9 | bsz 1 | num_updates 1154 | best_loss 8.483
2022-03-04 11:26:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1154 updates
2022-03-04 11:26:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:26:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 12 @ 1154 updates, score 8.483) (writing took 4.98970246501267 seconds)
2022-03-04 11:26:20 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 11:26:20 | INFO | train | epoch 012 | loss 8.354 | nll_loss 8.22 | ppl 298.25 | wps 24650.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1154 | lr 0.000144321 | gnorm 0.958 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3106
2022-03-04 11:26:20 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 11:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:28:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:28:20 | INFO | train_inner | epoch 013:     47 / 97 loss=8.267, nll_loss=8.132, ppl=280.54, wps=24431.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.948, loss_scale=32, train_wall=236, gb_free=21, wall=3226
2022-03-04 11:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:30:33 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.359 | nll_loss 8.222 | ppl 298.52 | wps 45258.3 | wpb 510.9 | bsz 1 | num_updates 1250 | best_loss 8.359
2022-03-04 11:30:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1250 updates
2022-03-04 11:30:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:30:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:30:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 13 @ 1250 updates, score 8.359) (writing took 4.968619123101234 seconds)
2022-03-04 11:30:38 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 11:30:38 | INFO | train | epoch 013 | loss 8.174 | nll_loss 8.037 | ppl 262.62 | wps 24390.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1250 | lr 0.000156319 | gnorm 0.95 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3364
2022-03-04 11:30:38 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 11:30:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:32:46 | INFO | train_inner | epoch 014:     50 / 97 loss=8.087, nll_loss=7.949, ppl=247.09, wps=24678.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.985, loss_scale=32, train_wall=233, gb_free=21, wall=3492
2022-03-04 11:34:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:34:51 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.259 | nll_loss 8.119 | ppl 278.09 | wps 45351.9 | wpb 510.9 | bsz 1 | num_updates 1346 | best_loss 8.259
2022-03-04 11:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1346 updates
2022-03-04 11:34:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:34:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 14 @ 1346 updates, score 8.259) (writing took 4.96030605584383 seconds)
2022-03-04 11:34:55 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 11:34:55 | INFO | train | epoch 014 | loss 8.007 | nll_loss 7.867 | ppl 233.43 | wps 24391.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1346 | lr 0.000168316 | gnorm 0.975 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3622
2022-03-04 11:34:55 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 11:34:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:35:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:37:16 | INFO | train_inner | epoch 015:     55 / 97 loss=7.924, nll_loss=7.782, ppl=220.16, wps=24212.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=1.004, loss_scale=16, train_wall=238, gb_free=21, wall=3762
2022-03-04 11:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:39:08 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.146 | nll_loss 8.003 | ppl 256.52 | wps 45367.2 | wpb 510.9 | bsz 1 | num_updates 1442 | best_loss 8.146
2022-03-04 11:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1442 updates
2022-03-04 11:39:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:39:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 15 @ 1442 updates, score 8.146) (writing took 4.925467907451093 seconds)
2022-03-04 11:39:13 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 11:39:13 | INFO | train | epoch 015 | loss 7.844 | nll_loss 7.701 | ppl 208.05 | wps 24406.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1442 | lr 0.000180314 | gnorm 0.995 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 3879
2022-03-04 11:39:13 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 11:39:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:41:42 | INFO | train_inner | epoch 016:     58 / 97 loss=7.743, nll_loss=7.598, ppl=193.7, wps=24676.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=1, loss_scale=32, train_wall=233, gb_free=21, wall=4028
2022-03-04 11:43:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:43:26 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.043 | nll_loss 7.896 | ppl 238.24 | wps 45369.8 | wpb 510.9 | bsz 1 | num_updates 1539 | best_loss 8.043
2022-03-04 11:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1539 updates
2022-03-04 11:43:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:43:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:43:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 16 @ 1539 updates, score 8.043) (writing took 4.9215964414179325 seconds)
2022-03-04 11:43:31 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 11:43:31 | INFO | train | epoch 016 | loss 7.684 | nll_loss 7.538 | ppl 185.82 | wps 24643 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1539 | lr 0.000192437 | gnorm 1.012 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4137
2022-03-04 11:43:31 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 11:43:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:46:07 | INFO | train_inner | epoch 017:     61 / 97 loss=7.595, nll_loss=7.447, ppl=174.47, wps=24677, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=1.014, loss_scale=32, train_wall=233, gb_free=21, wall=4293
2022-03-04 11:47:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:47:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.948 | nll_loss 7.799 | ppl 222.73 | wps 45238.7 | wpb 510.9 | bsz 1 | num_updates 1635 | best_loss 7.948
2022-03-04 11:47:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1635 updates
2022-03-04 11:47:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:47:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:47:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 17 @ 1635 updates, score 7.948) (writing took 4.990750093013048 seconds)
2022-03-04 11:47:49 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 11:47:49 | INFO | train | epoch 017 | loss 7.523 | nll_loss 7.374 | ppl 165.91 | wps 24388.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1635 | lr 0.000204434 | gnorm 1.013 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4395
2022-03-04 11:47:49 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 11:47:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:50:35 | INFO | train_inner | epoch 018:     65 / 97 loss=7.416, nll_loss=7.265, ppl=153.8, wps=24428.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=1.005, loss_scale=32, train_wall=236, gb_free=21, wall=4561
2022-03-04 11:51:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:52:01 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.87 | nll_loss 7.718 | ppl 210.58 | wps 45388.6 | wpb 510.9 | bsz 1 | num_updates 1732 | best_loss 7.87
2022-03-04 11:52:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1732 updates
2022-03-04 11:52:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:52:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:52:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 18 @ 1732 updates, score 7.87) (writing took 4.907638486474752 seconds)
2022-03-04 11:52:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 11:52:06 | INFO | train | epoch 018 | loss 7.368 | nll_loss 7.216 | ppl 148.69 | wps 24646.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1732 | lr 0.000216557 | gnorm 1.003 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4653
2022-03-04 11:52:06 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 11:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:54:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:55:03 | INFO | train_inner | epoch 019:     69 / 97 loss=7.264, nll_loss=7.11, ppl=138.13, wps=24454.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.998, loss_scale=32, train_wall=235, gb_free=21, wall=4829
2022-03-04 11:56:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:56:19 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.79 | nll_loss 7.636 | ppl 198.94 | wps 45690.9 | wpb 510.9 | bsz 1 | num_updates 1828 | best_loss 7.79
2022-03-04 11:56:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1828 updates
2022-03-04 11:56:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:56:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 19 @ 1828 updates, score 7.79) (writing took 5.0391527926549315 seconds)
2022-03-04 11:56:24 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 11:56:24 | INFO | train | epoch 019 | loss 7.215 | nll_loss 7.06 | ppl 133.44 | wps 24391 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1828 | lr 0.000228554 | gnorm 1.011 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4910
2022-03-04 11:56:24 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 11:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:59:28 | INFO | train_inner | epoch 020:     72 / 97 loss=7.105, nll_loss=6.948, ppl=123.5, wps=24679.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=1.015, loss_scale=32, train_wall=233, gb_free=21, wall=5094
2022-03-04 12:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:00:37 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.732 | nll_loss 7.575 | ppl 190.73 | wps 45328.4 | wpb 510.9 | bsz 1 | num_updates 1925 | best_loss 7.732
2022-03-04 12:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1925 updates
2022-03-04 12:00:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 20 @ 1925 updates, score 7.732) (writing took 5.006457304582 seconds)
2022-03-04 12:00:42 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 12:00:42 | INFO | train | epoch 020 | loss 7.065 | nll_loss 6.908 | ppl 120.06 | wps 24654.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1925 | lr 0.000240677 | gnorm 0.998 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5168
2022-03-04 12:00:42 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 12:00:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:00:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:03:56 | INFO | train_inner | epoch 021:     76 / 97 loss=6.953, nll_loss=6.793, ppl=110.91, wps=24445.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.995, loss_scale=32, train_wall=235, gb_free=21, wall=5362
2022-03-04 12:04:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:04:54 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.664 | nll_loss 7.508 | ppl 181.96 | wps 45317.5 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 7.664
2022-03-04 12:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 12:04:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:04:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 21 @ 2021 updates, score 7.664) (writing took 5.025745987892151 seconds)
2022-03-04 12:05:00 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 12:05:00 | INFO | train | epoch 021 | loss 6.921 | nll_loss 6.76 | ppl 108.42 | wps 24398.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 1.013 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5426
2022-03-04 12:05:00 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 12:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:06:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:06:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:08:27 | INFO | train_inner | epoch 022:     81 / 97 loss=6.811, nll_loss=6.648, ppl=100.28, wps=24204.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=1.006, loss_scale=16, train_wall=238, gb_free=21, wall=5633
2022-03-04 12:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:09:12 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.645 | nll_loss 7.489 | ppl 179.66 | wps 45384.7 | wpb 510.9 | bsz 1 | num_updates 2116 | best_loss 7.645
2022-03-04 12:09:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2116 updates
2022-03-04 12:09:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 22 @ 2116 updates, score 7.645) (writing took 4.989705544896424 seconds)
2022-03-04 12:09:17 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 12:09:17 | INFO | train | epoch 022 | loss 6.782 | nll_loss 6.619 | ppl 98.29 | wps 24136.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 2116 | lr 0.000264547 | gnorm 0.999 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 5683
2022-03-04 12:09:17 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 12:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:12:52 | INFO | train_inner | epoch 023:     84 / 97 loss=6.672, nll_loss=6.506, ppl=90.89, wps=24689.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=1.008, loss_scale=32, train_wall=233, gb_free=21, wall=5898
2022-03-04 12:13:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:13:30 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.616 | nll_loss 7.455 | ppl 175.47 | wps 45248.3 | wpb 510.9 | bsz 1 | num_updates 2213 | best_loss 7.616
2022-03-04 12:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2213 updates
2022-03-04 12:13:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:13:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:13:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 23 @ 2213 updates, score 7.616) (writing took 4.945872534997761 seconds)
2022-03-04 12:13:35 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 12:13:35 | INFO | train | epoch 023 | loss 6.649 | nll_loss 6.482 | ppl 89.42 | wps 24662.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2213 | lr 0.00027667 | gnorm 1.013 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5941
2022-03-04 12:13:35 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 12:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:17:17 | INFO | train_inner | epoch 024:     87 / 97 loss=6.529, nll_loss=6.361, ppl=82.19, wps=24683.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.989, loss_scale=32, train_wall=233, gb_free=21, wall=6163
2022-03-04 12:17:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:17:48 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.591 | nll_loss 7.427 | ppl 172.03 | wps 45295.1 | wpb 510.9 | bsz 1 | num_updates 2310 | best_loss 7.591
2022-03-04 12:17:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2310 updates
2022-03-04 12:17:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:17:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 24 @ 2310 updates, score 7.591) (writing took 4.988410256803036 seconds)
2022-03-04 12:17:53 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 12:17:53 | INFO | train | epoch 024 | loss 6.515 | nll_loss 6.346 | ppl 81.37 | wps 24647.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2310 | lr 0.000288792 | gnorm 0.995 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6199
2022-03-04 12:17:53 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 12:17:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:18:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:21:45 | INFO | train_inner | epoch 025:     91 / 97 loss=6.399, nll_loss=6.227, ppl=74.93, wps=24432.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=1.036, loss_scale=32, train_wall=236, gb_free=21, wall=6432
2022-03-04 12:22:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:22:05 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.571 | nll_loss 7.407 | ppl 169.7 | wps 45211 | wpb 510.9 | bsz 1 | num_updates 2406 | best_loss 7.571
2022-03-04 12:22:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2406 updates
2022-03-04 12:22:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:22:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 25 @ 2406 updates, score 7.571) (writing took 4.910134165547788 seconds)
2022-03-04 12:22:10 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 12:22:10 | INFO | train | epoch 025 | loss 6.387 | nll_loss 6.216 | ppl 74.33 | wps 24393.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2406 | lr 0.00030079 | gnorm 1.038 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6456
2022-03-04 12:22:10 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 12:22:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:24:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:26:13 | INFO | train_inner | epoch 026:     95 / 97 loss=6.27, nll_loss=6.095, ppl=68.38, wps=24447, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=0.996, loss_scale=32, train_wall=236, gb_free=21, wall=6699
2022-03-04 12:26:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:26:23 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.589 | nll_loss 7.425 | ppl 171.89 | wps 45278.7 | wpb 510.9 | bsz 1 | num_updates 2502 | best_loss 7.571
2022-03-04 12:26:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2502 updates
2022-03-04 12:26:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:26:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 26 @ 2502 updates, score 7.589) (writing took 2.1289770556613803 seconds)
2022-03-04 12:26:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 12:26:25 | INFO | train | epoch 026 | loss 6.26 | nll_loss 6.086 | ppl 67.91 | wps 24666 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2502 | lr 0.000312787 | gnorm 0.99 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6711
2022-03-04 12:26:25 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 12:26:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:30:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:30:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:30:38 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.625 | nll_loss 7.459 | ppl 175.97 | wps 45255.6 | wpb 510.9 | bsz 1 | num_updates 2598 | best_loss 7.571
2022-03-04 12:30:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2598 updates
2022-03-04 12:30:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:30:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:30:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 27 @ 2598 updates, score 7.625) (writing took 2.1936601269990206 seconds)
2022-03-04 12:30:40 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 12:30:40 | INFO | train | epoch 027 | loss 6.14 | nll_loss 5.963 | ppl 62.39 | wps 24662.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2598 | lr 0.000324785 | gnorm 1.019 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6966
2022-03-04 12:30:40 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 12:30:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:30:45 | INFO | train_inner | epoch 028:      2 / 97 loss=6.14, nll_loss=5.963, ppl=62.39, wps=24052.2, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=1.022, loss_scale=32, train_wall=235, gb_free=21, wall=6972
2022-03-04 12:34:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:34:53 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.574 | nll_loss 7.406 | ppl 169.65 | wps 45232.4 | wpb 510.9 | bsz 1 | num_updates 2695 | best_loss 7.571
2022-03-04 12:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2695 updates
2022-03-04 12:34:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:34:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:34:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 28 @ 2695 updates, score 7.574) (writing took 2.15293375775218 seconds)
2022-03-04 12:34:55 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 12:34:55 | INFO | train | epoch 028 | loss 6.02 | nll_loss 5.84 | ppl 57.3 | wps 24918.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2695 | lr 0.000336908 | gnorm 1.016 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7221
2022-03-04 12:34:55 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 12:34:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:35:08 | INFO | train_inner | epoch 029:      5 / 97 loss=6.012, nll_loss=5.833, ppl=57, wps=24936.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=1.012, loss_scale=32, train_wall=233, gb_free=21, wall=7234
2022-03-04 12:36:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:36:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:39:08 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.627 | nll_loss 7.458 | ppl 175.86 | wps 45393.8 | wpb 510.9 | bsz 1 | num_updates 2790 | best_loss 7.571
2022-03-04 12:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2790 updates
2022-03-04 12:39:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 29 @ 2790 updates, score 7.627) (writing took 2.1595207760110497 seconds)
2022-03-04 12:39:10 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 12:39:10 | INFO | train | epoch 029 | loss 5.901 | nll_loss 5.719 | ppl 52.66 | wps 24420.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 2790 | lr 0.00034878 | gnorm 1.033 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 7476
2022-03-04 12:39:10 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 12:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:39:36 | INFO | train_inner | epoch 030:     10 / 97 loss=5.888, nll_loss=5.705, ppl=52.18, wps=24479, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=1.024, loss_scale=16, train_wall=238, gb_free=21, wall=7502
2022-03-04 12:43:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:43:23 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.646 | nll_loss 7.477 | ppl 178.1 | wps 45176.8 | wpb 510.9 | bsz 1 | num_updates 2887 | best_loss 7.571
2022-03-04 12:43:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2887 updates
2022-03-04 12:43:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:43:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:43:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 30 @ 2887 updates, score 7.646) (writing took 2.1716429851949215 seconds)
2022-03-04 12:43:25 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 12:43:25 | INFO | train | epoch 030 | loss 5.789 | nll_loss 5.604 | ppl 48.64 | wps 24900.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2887 | lr 0.000360903 | gnorm 1.034 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7731
2022-03-04 12:43:25 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 12:43:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:43:58 | INFO | train_inner | epoch 031:     13 / 97 loss=5.77, nll_loss=5.585, ppl=48.01, wps=24926.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=1.043, loss_scale=32, train_wall=233, gb_free=21, wall=7764
2022-03-04 12:47:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:47:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:47:38 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.669 | nll_loss 7.501 | ppl 181.12 | wps 45308.1 | wpb 510.9 | bsz 1 | num_updates 2983 | best_loss 7.571
2022-03-04 12:47:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2983 updates
2022-03-04 12:47:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:47:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:47:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 31 @ 2983 updates, score 7.669) (writing took 2.2163843885064125 seconds)
2022-03-04 12:47:40 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 12:47:40 | INFO | train | epoch 031 | loss 5.672 | nll_loss 5.484 | ppl 44.77 | wps 24649 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2983 | lr 0.0003729 | gnorm 1.046 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7986
2022-03-04 12:47:40 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 12:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:48:24 | INFO | train_inner | epoch 032:     17 / 97 loss=5.653, nll_loss=5.465, ppl=44.17, wps=24689.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=1.052, loss_scale=32, train_wall=236, gb_free=21, wall=8030
2022-03-04 12:48:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:51:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:51:53 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.719 | nll_loss 7.548 | ppl 187.1 | wps 45270 | wpb 510.9 | bsz 1 | num_updates 3079 | best_loss 7.571
2022-03-04 12:51:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3079 updates
2022-03-04 12:51:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:51:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:51:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 32 @ 3079 updates, score 7.719) (writing took 2.1513396613299847 seconds)
2022-03-04 12:51:55 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 12:51:55 | INFO | train | epoch 032 | loss 5.561 | nll_loss 5.372 | ppl 41.4 | wps 24671.9 | ups 0.38 | wpb 65493.3 | bsz 127.9 | num_updates 3079 | lr 0.000384898 | gnorm 1.063 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 8241
2022-03-04 12:51:55 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 12:51:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:52:49 | INFO | train_inner | epoch 033:     21 / 97 loss=5.538, nll_loss=5.347, ppl=40.71, wps=24713.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=1.068, loss_scale=16, train_wall=235, gb_free=21, wall=8295
2022-03-04 12:56:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:56:07 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.772 | nll_loss 7.601 | ppl 194.15 | wps 45236.6 | wpb 510.9 | bsz 1 | num_updates 3176 | best_loss 7.571
2022-03-04 12:56:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3176 updates
2022-03-04 12:56:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:56:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 33 @ 3176 updates, score 7.772) (writing took 2.1513087023049593 seconds)
2022-03-04 12:56:10 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 12:56:10 | INFO | train | epoch 033 | loss 5.455 | nll_loss 5.262 | ppl 38.38 | wps 24939.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3176 | lr 0.000397021 | gnorm 1.087 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 8496
2022-03-04 12:56:10 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 12:56:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:57:11 | INFO | train_inner | epoch 034:     24 / 97 loss=5.424, nll_loss=5.231, ppl=37.56, wps=24949.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=1.086, loss_scale=32, train_wall=233, gb_free=21, wall=8557
2022-03-04 12:58:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:00:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:00:22 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.828 | nll_loss 7.655 | ppl 201.53 | wps 45373.2 | wpb 510.9 | bsz 1 | num_updates 3272 | best_loss 7.571
2022-03-04 13:00:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3272 updates
2022-03-04 13:00:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:00:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:00:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 34 @ 3272 updates, score 7.828) (writing took 2.1233757538720965 seconds)
2022-03-04 13:00:24 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 13:00:24 | INFO | train | epoch 034 | loss 5.344 | nll_loss 5.149 | ppl 35.47 | wps 24680.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3272 | lr 0.000409018 | gnorm 1.075 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 8751
2022-03-04 13:00:24 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 13:00:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:01:36 | INFO | train_inner | epoch 035:     28 / 97 loss=5.32, nll_loss=5.124, ppl=34.87, wps=24729.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=1.079, loss_scale=16, train_wall=235, gb_free=21, wall=8822
2022-03-04 13:04:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:04:37 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.864 | nll_loss 7.691 | ppl 206.63 | wps 45363.8 | wpb 510.9 | bsz 1 | num_updates 3368 | best_loss 7.571
2022-03-04 13:04:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3368 updates
2022-03-04 13:04:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:04:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 35 @ 3368 updates, score 7.864) (writing took 2.1890398785471916 seconds)
2022-03-04 13:04:39 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 13:04:39 | INFO | train | epoch 035 | loss 5.239 | nll_loss 5.041 | ppl 32.93 | wps 24691.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3368 | lr 0.000421016 | gnorm 1.104 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9005
2022-03-04 13:04:39 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 13:04:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:06:01 | INFO | train_inner | epoch 036:     32 / 97 loss=5.197, nll_loss=4.998, ppl=31.96, wps=24728.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=1.107, loss_scale=16, train_wall=235, gb_free=21, wall=9087
2022-03-04 13:08:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:08:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.887 | nll_loss 7.714 | ppl 210.01 | wps 45437 | wpb 510.9 | bsz 1 | num_updates 3465 | best_loss 7.571
2022-03-04 13:08:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3465 updates
2022-03-04 13:08:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 36 @ 3465 updates, score 7.887) (writing took 2.118642019107938 seconds)
2022-03-04 13:08:54 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 13:08:54 | INFO | train | epoch 036 | loss 5.134 | nll_loss 4.933 | ppl 30.56 | wps 24946 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3465 | lr 0.000433138 | gnorm 1.11 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9260
2022-03-04 13:08:54 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 13:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:10:23 | INFO | train_inner | epoch 037:     35 / 97 loss=5.097, nll_loss=4.895, ppl=29.76, wps=24955.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=1.12, loss_scale=32, train_wall=233, gb_free=21, wall=9349
2022-03-04 13:11:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:13:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:13:06 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.945 | nll_loss 7.772 | ppl 218.6 | wps 45292.1 | wpb 510.9 | bsz 1 | num_updates 3561 | best_loss 7.571
2022-03-04 13:13:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3561 updates
2022-03-04 13:13:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:13:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 37 @ 3561 updates, score 7.945) (writing took 2.113338208757341 seconds)
2022-03-04 13:13:09 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 13:13:09 | INFO | train | epoch 037 | loss 5.024 | nll_loss 4.821 | ppl 28.26 | wps 24665.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3561 | lr 0.000445136 | gnorm 1.103 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9515
2022-03-04 13:13:09 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 13:13:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:14:48 | INFO | train_inner | epoch 038:     39 / 97 loss=4.986, nll_loss=4.782, ppl=27.5, wps=24706.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=1.1, loss_scale=16, train_wall=236, gb_free=21, wall=9614
2022-03-04 13:17:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:17:21 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.999 | nll_loss 7.825 | ppl 226.69 | wps 45367 | wpb 510.9 | bsz 1 | num_updates 3658 | best_loss 7.571
2022-03-04 13:17:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3658 updates
2022-03-04 13:17:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:17:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:17:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 38 @ 3658 updates, score 7.999) (writing took 2.1192583199590445 seconds)
2022-03-04 13:17:23 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 13:17:23 | INFO | train | epoch 038 | loss 4.925 | nll_loss 4.719 | ppl 26.34 | wps 24929.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3658 | lr 0.000457259 | gnorm 1.116 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 9770
2022-03-04 13:17:23 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 13:17:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:17:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:19:13 | INFO | train_inner | epoch 039:     43 / 97 loss=4.883, nll_loss=4.676, ppl=25.57, wps=24709.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.146, loss_scale=16, train_wall=235, gb_free=21, wall=9879
2022-03-04 13:21:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:21:36 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.029 | nll_loss 7.855 | ppl 231.57 | wps 45327.1 | wpb 510.9 | bsz 1 | num_updates 3754 | best_loss 7.571
2022-03-04 13:21:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3754 updates
2022-03-04 13:21:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:21:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:21:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 39 @ 3754 updates, score 8.029) (writing took 2.1792959682643414 seconds)
2022-03-04 13:21:38 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 13:21:38 | INFO | train | epoch 039 | loss 4.824 | nll_loss 4.615 | ppl 24.51 | wps 24680.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3754 | lr 0.000469256 | gnorm 1.164 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10024
2022-03-04 13:21:38 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 13:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:23:36 | INFO | train_inner | epoch 040:     46 / 97 loss=4.771, nll_loss=4.561, ppl=23.6, wps=24964, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.151, loss_scale=32, train_wall=233, gb_free=21, wall=10142
2022-03-04 13:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:25:51 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.071 | nll_loss 7.891 | ppl 237.45 | wps 45443 | wpb 510.9 | bsz 1 | num_updates 3851 | best_loss 7.571
2022-03-04 13:25:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3851 updates
2022-03-04 13:25:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:25:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:25:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 40 @ 3851 updates, score 8.071) (writing took 2.156263304874301 seconds)
2022-03-04 13:25:53 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 13:25:53 | INFO | train | epoch 040 | loss 4.724 | nll_loss 4.513 | ppl 22.83 | wps 24939.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3851 | lr 0.000481379 | gnorm 1.162 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 10279
2022-03-04 13:25:53 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 13:25:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:26:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:28:01 | INFO | train_inner | epoch 041:     50 / 97 loss=4.672, nll_loss=4.459, ppl=21.99, wps=24711.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.161, loss_scale=16, train_wall=235, gb_free=21, wall=10407
2022-03-04 13:30:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:30:05 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.118 | nll_loss 7.937 | ppl 244.98 | wps 45265.5 | wpb 510.9 | bsz 1 | num_updates 3947 | best_loss 7.571
2022-03-04 13:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3947 updates
2022-03-04 13:30:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:30:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:30:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 41 @ 3947 updates, score 8.118) (writing took 2.1321135330945253 seconds)
2022-03-04 13:30:08 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 13:30:08 | INFO | train | epoch 041 | loss 4.626 | nll_loss 4.412 | ppl 21.29 | wps 24682.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3947 | lr 0.000493376 | gnorm 1.157 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10534
2022-03-04 13:30:08 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 13:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:32:23 | INFO | train_inner | epoch 042:     53 / 97 loss=4.58, nll_loss=4.365, ppl=20.6, wps=24962.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.203, loss_scale=32, train_wall=233, gb_free=21, wall=10669
2022-03-04 13:33:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:34:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:34:20 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.19 | nll_loss 8.013 | ppl 258.25 | wps 45241.9 | wpb 510.9 | bsz 1 | num_updates 4043 | best_loss 7.571
2022-03-04 13:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4043 updates
2022-03-04 13:34:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:34:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:34:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 42 @ 4043 updates, score 8.19) (writing took 2.1426567025482655 seconds)
2022-03-04 13:34:22 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 13:34:22 | INFO | train | epoch 042 | loss 4.529 | nll_loss 4.312 | ppl 19.87 | wps 24676.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4043 | lr 0.000497334 | gnorm 1.191 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10788
2022-03-04 13:34:22 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 13:34:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:36:48 | INFO | train_inner | epoch 043:     57 / 97 loss=4.468, nll_loss=4.25, ppl=19.03, wps=24710.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.147, loss_scale=16, train_wall=235, gb_free=21, wall=10934
2022-03-04 13:38:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:38:35 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.253 | nll_loss 8.076 | ppl 269.78 | wps 45470.7 | wpb 510.9 | bsz 1 | num_updates 4140 | best_loss 7.571
2022-03-04 13:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4140 updates
2022-03-04 13:38:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:38:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 43 @ 4140 updates, score 8.253) (writing took 2.209234225563705 seconds)
2022-03-04 13:38:37 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 13:38:37 | INFO | train | epoch 043 | loss 4.429 | nll_loss 4.209 | ppl 18.5 | wps 24923.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4140 | lr 0.000491473 | gnorm 1.179 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11043
2022-03-04 13:38:37 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 13:38:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:41:11 | INFO | train_inner | epoch 044:     60 / 97 loss=4.365, nll_loss=4.143, ppl=17.67, wps=24946.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.172, loss_scale=32, train_wall=233, gb_free=21, wall=11197
2022-03-04 13:42:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:42:50 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.339 | nll_loss 8.161 | ppl 286.16 | wps 45342.4 | wpb 510.9 | bsz 1 | num_updates 4237 | best_loss 7.571
2022-03-04 13:42:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4237 updates
2022-03-04 13:42:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:42:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:42:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 44 @ 4237 updates, score 8.339) (writing took 2.1354059064760804 seconds)
2022-03-04 13:42:52 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 13:42:52 | INFO | train | epoch 044 | loss 4.322 | nll_loss 4.099 | ppl 17.14 | wps 24931 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4237 | lr 0.000485815 | gnorm 1.139 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 11298
2022-03-04 13:42:52 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 13:42:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:43:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:45:36 | INFO | train_inner | epoch 045:     64 / 97 loss=4.267, nll_loss=4.043, ppl=16.48, wps=24720.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=1.162, loss_scale=16, train_wall=235, gb_free=21, wall=11462
2022-03-04 13:47:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:47:05 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.382 | nll_loss 8.2 | ppl 294 | wps 45441.5 | wpb 510.9 | bsz 1 | num_updates 4333 | best_loss 7.571
2022-03-04 13:47:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4333 updates
2022-03-04 13:47:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:47:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:47:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 45 @ 4333 updates, score 8.382) (writing took 2.1430437145754695 seconds)
2022-03-04 13:47:07 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 13:47:07 | INFO | train | epoch 045 | loss 4.224 | nll_loss 3.999 | ppl 15.99 | wps 24683.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4333 | lr 0.000480403 | gnorm 1.148 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11553
2022-03-04 13:47:07 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 13:47:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:48:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:50:00 | INFO | train_inner | epoch 046:     68 / 97 loss=4.158, nll_loss=3.931, ppl=15.25, wps=24724.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.12, loss_scale=16, train_wall=235, gb_free=21, wall=11727
2022-03-04 13:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:51:19 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.422 | nll_loss 8.238 | ppl 301.98 | wps 45401.7 | wpb 510.9 | bsz 1 | num_updates 4429 | best_loss 7.571
2022-03-04 13:51:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4429 updates
2022-03-04 13:51:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:51:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 46 @ 4429 updates, score 8.422) (writing took 2.1447392096742988 seconds)
2022-03-04 13:51:21 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 13:51:21 | INFO | train | epoch 046 | loss 4.132 | nll_loss 3.904 | ppl 14.97 | wps 24689.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4429 | lr 0.000475168 | gnorm 1.174 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11808
2022-03-04 13:51:21 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 13:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:54:23 | INFO | train_inner | epoch 047:     71 / 97 loss=4.068, nll_loss=3.838, ppl=14.3, wps=24966.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=1.155, loss_scale=32, train_wall=233, gb_free=21, wall=11989
2022-03-04 13:55:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:55:34 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.543 | nll_loss 8.361 | ppl 328.9 | wps 45327.2 | wpb 510.9 | bsz 1 | num_updates 4526 | best_loss 7.571
2022-03-04 13:55:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4526 updates
2022-03-04 13:55:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:55:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:55:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 47 @ 4526 updates, score 8.543) (writing took 2.401046018116176 seconds)
2022-03-04 13:55:36 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 13:55:36 | INFO | train | epoch 047 | loss 4.038 | nll_loss 3.808 | ppl 14 | wps 24928.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4526 | lr 0.000470049 | gnorm 1.109 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 12062
2022-03-04 13:55:36 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 13:55:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:55:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:58:48 | INFO | train_inner | epoch 048:     75 / 97 loss=3.972, nll_loss=3.739, ppl=13.36, wps=24727.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.124, loss_scale=16, train_wall=235, gb_free=21, wall=12254
2022-03-04 13:59:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:59:49 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.625 | nll_loss 8.438 | ppl 346.7 | wps 45378.5 | wpb 510.9 | bsz 1 | num_updates 4622 | best_loss 7.571
2022-03-04 13:59:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4622 updates
2022-03-04 13:59:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:59:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 48 @ 4622 updates, score 8.625) (writing took 2.1969471387565136 seconds)
2022-03-04 13:59:51 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 13:59:51 | INFO | train | epoch 048 | loss 3.95 | nll_loss 3.717 | ppl 13.15 | wps 24707.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4622 | lr 0.000465141 | gnorm 1.149 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 12317
2022-03-04 13:59:51 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 13:59:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:02:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:03:12 | INFO | train_inner | epoch 049:     79 / 97 loss=3.893, nll_loss=3.658, ppl=12.62, wps=24742.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=1.15, loss_scale=16, train_wall=235, gb_free=21, wall=12519
2022-03-04 14:03:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:04:03 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.697 | nll_loss 8.506 | ppl 363.62 | wps 45417.7 | wpb 510.9 | bsz 1 | num_updates 4718 | best_loss 7.571
2022-03-04 14:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4718 updates
2022-03-04 14:04:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:04:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:04:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 49 @ 4718 updates, score 8.697) (writing took 2.2454170705750585 seconds)
2022-03-04 14:04:05 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 14:04:05 | INFO | train | epoch 049 | loss 3.866 | nll_loss 3.631 | ppl 12.39 | wps 24706.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4718 | lr 0.000460385 | gnorm 1.133 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 12571
2022-03-04 14:04:05 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 14:04:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:07:34 | INFO | train_inner | epoch 050:     82 / 97 loss=3.798, nll_loss=3.56, ppl=11.79, wps=25005.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=1.109, loss_scale=16, train_wall=233, gb_free=21, wall=12780
2022-03-04 14:08:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:08:17 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.75 | nll_loss 8.564 | ppl 378.46 | wps 45516.7 | wpb 510.9 | bsz 1 | num_updates 4815 | best_loss 7.571
2022-03-04 14:08:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4815 updates
2022-03-04 14:08:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:08:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:08:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 50 @ 4815 updates, score 8.75) (writing took 2.3283425690606236 seconds)
2022-03-04 14:08:20 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 14:08:20 | INFO | train | epoch 050 | loss 3.783 | nll_loss 3.544 | ppl 11.67 | wps 24974.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4815 | lr 0.000455724 | gnorm 1.113 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 12826
2022-03-04 14:08:20 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 14:08:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:10:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:11:59 | INFO | train_inner | epoch 051:     86 / 97 loss=3.716, nll_loss=3.476, ppl=11.13, wps=24751.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.126, loss_scale=16, train_wall=235, gb_free=21, wall=13045
2022-03-04 14:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:12:32 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.819 | nll_loss 8.632 | ppl 396.75 | wps 45542.3 | wpb 510.9 | bsz 1 | num_updates 4911 | best_loss 7.571
2022-03-04 14:12:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4911 updates
2022-03-04 14:12:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:12:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 51 @ 4911 updates, score 8.819) (writing took 2.361928611062467 seconds)
2022-03-04 14:12:34 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 14:12:34 | INFO | train | epoch 051 | loss 3.703 | nll_loss 3.462 | ppl 11.02 | wps 24711.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4911 | lr 0.000451248 | gnorm 1.129 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 13080
2022-03-04 14:12:34 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 14:12:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:16:21 | INFO | train_inner | epoch 052:     89 / 97 loss=3.638, nll_loss=3.395, ppl=10.52, wps=24980.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1.138, loss_scale=32, train_wall=233, gb_free=21, wall=13307
2022-03-04 14:16:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:16:46 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.9 | nll_loss 8.714 | ppl 419.89 | wps 45534.5 | wpb 510.9 | bsz 1 | num_updates 5008 | best_loss 7.571
2022-03-04 14:16:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5008 updates
2022-03-04 14:16:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:16:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 52 @ 5008 updates, score 8.9) (writing took 2.188222970813513 seconds)
2022-03-04 14:16:48 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 14:16:48 | INFO | train | epoch 052 | loss 3.628 | nll_loss 3.384 | ppl 10.44 | wps 24974.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5008 | lr 0.000446856 | gnorm 1.137 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 13334
2022-03-04 14:16:48 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 14:16:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:20:43 | INFO | train_inner | epoch 053:     92 / 97 loss=3.562, nll_loss=3.317, ppl=9.97, wps=24987.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=1.126, loss_scale=32, train_wall=233, gb_free=21, wall=13569
2022-03-04 14:20:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:20:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:21:01 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.984 | nll_loss 8.798 | ppl 445.12 | wps 45307.2 | wpb 510.9 | bsz 1 | num_updates 5104 | best_loss 7.571
2022-03-04 14:21:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5104 updates
2022-03-04 14:21:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:21:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 53 @ 5104 updates, score 8.984) (writing took 2.2130283610895276 seconds)
2022-03-04 14:21:03 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 14:21:03 | INFO | train | epoch 053 | loss 3.553 | nll_loss 3.307 | ppl 9.9 | wps 24705.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5104 | lr 0.000442634 | gnorm 1.129 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 13589
2022-03-04 14:21:03 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 14:21:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:25:08 | INFO | train_inner | epoch 054:     96 / 97 loss=3.488, nll_loss=3.24, ppl=9.45, wps=24775, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5200, lr=0.000438529, gnorm=1.13, loss_scale=16, train_wall=235, gb_free=21, wall=13834
2022-03-04 14:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:25:15 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.053 | nll_loss 8.862 | ppl 465.22 | wps 45559.2 | wpb 510.9 | bsz 1 | num_updates 5201 | best_loss 7.571
2022-03-04 14:25:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5201 updates
2022-03-04 14:25:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:25:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:25:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 54 @ 5201 updates, score 9.053) (writing took 2.252369811758399 seconds)
2022-03-04 14:25:17 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 14:25:17 | INFO | train | epoch 054 | loss 3.482 | nll_loss 3.234 | ppl 9.41 | wps 24992.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5201 | lr 0.000438487 | gnorm 1.126 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 13843
2022-03-04 14:25:17 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 14:25:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:29:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:29:29 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.163 | nll_loss 8.973 | ppl 502.45 | wps 45475 | wpb 510.9 | bsz 1 | num_updates 5298 | best_loss 7.571
2022-03-04 14:29:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5298 updates
2022-03-04 14:29:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:29:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 55 @ 5298 updates, score 9.163) (writing took 2.3595749717205763 seconds)
2022-03-04 14:29:31 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 14:29:31 | INFO | train | epoch 055 | loss 3.414 | nll_loss 3.164 | ppl 8.96 | wps 24964.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5298 | lr 0.000434454 | gnorm 1.151 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 14098
2022-03-04 14:29:32 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 14:29:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:29:37 | INFO | train_inner | epoch 056:      2 / 97 loss=3.411, nll_loss=3.161, ppl=8.94, wps=24315.9, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=5300, lr=0.000434372, gnorm=1.151, loss_scale=32, train_wall=233, gb_free=21, wall=14103
2022-03-04 14:31:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:33:44 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.212 | nll_loss 9.022 | ppl 519.74 | wps 45455.6 | wpb 510.9 | bsz 1 | num_updates 5394 | best_loss 7.571
2022-03-04 14:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5394 updates
2022-03-04 14:33:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:33:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:33:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 56 @ 5394 updates, score 9.212) (writing took 2.2836403334513307 seconds)
2022-03-04 14:33:46 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 14:33:46 | INFO | train | epoch 056 | loss 3.344 | nll_loss 3.091 | ppl 8.52 | wps 24717.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5394 | lr 0.000430571 | gnorm 1.116 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14352
2022-03-04 14:33:46 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 14:33:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:34:01 | INFO | train_inner | epoch 057:      6 / 97 loss=3.337, nll_loss=3.085, ppl=8.48, wps=24756.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=1.114, loss_scale=16, train_wall=235, gb_free=21, wall=14367
2022-03-04 14:37:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:37:58 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.308 | nll_loss 9.116 | ppl 554.93 | wps 45453.2 | wpb 510.9 | bsz 1 | num_updates 5491 | best_loss 7.571
2022-03-04 14:37:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5491 updates
2022-03-04 14:37:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:38:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:38:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 57 @ 5491 updates, score 9.308) (writing took 2.3038430670276284 seconds)
2022-03-04 14:38:00 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 14:38:00 | INFO | train | epoch 057 | loss 3.285 | nll_loss 3.03 | ppl 8.17 | wps 24955.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5491 | lr 0.000426751 | gnorm 1.128 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 14607
2022-03-04 14:38:00 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 14:38:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:38:23 | INFO | train_inner | epoch 058:      9 / 97 loss=3.276, nll_loss=3.021, ppl=8.11, wps=24977, ups=0.38, wpb=65495, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.129, loss_scale=32, train_wall=233, gb_free=21, wall=14630
2022-03-04 14:42:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:42:12 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.353 | nll_loss 9.16 | ppl 572.25 | wps 45508.7 | wpb 510.9 | bsz 1 | num_updates 5588 | best_loss 7.571
2022-03-04 14:42:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5588 updates
2022-03-04 14:42:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:42:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:42:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 58 @ 5588 updates, score 9.353) (writing took 2.287123299203813 seconds)
2022-03-04 14:42:15 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 14:42:15 | INFO | train | epoch 058 | loss 3.223 | nll_loss 2.966 | ppl 7.81 | wps 24976.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5588 | lr 0.000423031 | gnorm 1.148 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 14861
2022-03-04 14:42:15 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 14:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:42:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:42:48 | INFO | train_inner | epoch 059:     13 / 97 loss=3.214, nll_loss=2.957, ppl=7.77, wps=24758.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.14, loss_scale=32, train_wall=235, gb_free=21, wall=14894
2022-03-04 14:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:46:27 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.439 | nll_loss 9.248 | ppl 608.22 | wps 45343.8 | wpb 510.9 | bsz 1 | num_updates 5684 | best_loss 7.571
2022-03-04 14:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5684 updates
2022-03-04 14:46:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:46:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:46:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 59 @ 5684 updates, score 9.439) (writing took 2.3668904388323426 seconds)
2022-03-04 14:46:29 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 14:46:29 | INFO | train | epoch 059 | loss 3.159 | nll_loss 2.9 | ppl 7.47 | wps 24713.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5684 | lr 0.000419443 | gnorm 1.123 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 15115
2022-03-04 14:46:29 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 14:46:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:47:10 | INFO | train_inner | epoch 060:     16 / 97 loss=3.149, nll_loss=2.889, ppl=7.41, wps=24991.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.139, loss_scale=32, train_wall=233, gb_free=21, wall=15156
2022-03-04 14:48:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:50:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:50:41 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.535 | nll_loss 9.341 | ppl 648.68 | wps 45323.4 | wpb 510.9 | bsz 1 | num_updates 5780 | best_loss 7.571
2022-03-04 14:50:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5780 updates
2022-03-04 14:50:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:50:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:50:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 60 @ 5780 updates, score 9.535) (writing took 2.3534248592332006 seconds)
2022-03-04 14:50:44 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 14:50:44 | INFO | train | epoch 060 | loss 3.103 | nll_loss 2.842 | ppl 7.17 | wps 24700.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5780 | lr 0.000415945 | gnorm 1.132 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 15370
2022-03-04 14:50:44 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 14:50:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:51:35 | INFO | train_inner | epoch 061:     20 / 97 loss=3.09, nll_loss=2.828, ppl=7.1, wps=24733.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.122, loss_scale=32, train_wall=235, gb_free=21, wall=15421
2022-03-04 14:54:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:54:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:54:56 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.563 | nll_loss 9.369 | ppl 661.45 | wps 45397.3 | wpb 510.9 | bsz 1 | num_updates 5875 | best_loss 7.571
2022-03-04 14:54:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5875 updates
2022-03-04 14:54:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:54:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:54:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 61 @ 5875 updates, score 9.563) (writing took 2.3527134004980326 seconds)
2022-03-04 14:54:58 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 14:54:58 | INFO | train | epoch 061 | loss 3.046 | nll_loss 2.783 | ppl 6.88 | wps 24443.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 5875 | lr 0.000412568 | gnorm 1.119 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 15624
2022-03-04 14:54:58 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 14:54:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:56:02 | INFO | train_inner | epoch 062:     25 / 97 loss=3.031, nll_loss=2.767, ppl=6.81, wps=24512.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.11, loss_scale=16, train_wall=237, gb_free=21, wall=15688
2022-03-04 14:59:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:59:10 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.675 | nll_loss 9.484 | ppl 716.02 | wps 45392.2 | wpb 510.9 | bsz 1 | num_updates 5972 | best_loss 7.571
2022-03-04 14:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5972 updates
2022-03-04 14:59:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:59:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:59:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 62 @ 5972 updates, score 9.675) (writing took 2.3418219555169344 seconds)
2022-03-04 14:59:13 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 14:59:13 | INFO | train | epoch 062 | loss 2.997 | nll_loss 2.732 | ppl 6.65 | wps 24976.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5972 | lr 0.000409204 | gnorm 1.121 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 15879
2022-03-04 14:59:13 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 14:59:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:00:24 | INFO | train_inner | epoch 063:     28 / 97 loss=2.98, nll_loss=2.714, ppl=6.56, wps=24988.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.137, loss_scale=32, train_wall=233, gb_free=21, wall=15950
2022-03-04 15:03:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:03:25 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.737 | nll_loss 9.543 | ppl 745.98 | wps 45293.1 | wpb 510.9 | bsz 1 | num_updates 6069 | best_loss 7.571
2022-03-04 15:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6069 updates
2022-03-04 15:03:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:03:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:03:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 63 @ 6069 updates, score 9.737) (writing took 2.2643408309668303 seconds)
2022-03-04 15:03:27 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 15:03:27 | INFO | train | epoch 063 | loss 2.945 | nll_loss 2.678 | ppl 6.4 | wps 24952.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6069 | lr 0.000405921 | gnorm 1.152 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 16133
2022-03-04 15:03:27 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 15:03:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:04:46 | INFO | train_inner | epoch 064:     31 / 97 loss=2.93, nll_loss=2.662, ppl=6.33, wps=24976.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.156, loss_scale=32, train_wall=233, gb_free=21, wall=16212
2022-03-04 15:05:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:07:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:07:39 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.829 | nll_loss 9.636 | ppl 795.61 | wps 45320.8 | wpb 510.9 | bsz 1 | num_updates 6165 | best_loss 7.571
2022-03-04 15:07:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6165 updates
2022-03-04 15:07:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:07:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:07:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 64 @ 6165 updates, score 9.829) (writing took 2.3720875419676304 seconds)
2022-03-04 15:07:42 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 15:07:42 | INFO | train | epoch 064 | loss 2.893 | nll_loss 2.625 | ppl 6.17 | wps 24692.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6165 | lr 0.000402748 | gnorm 1.135 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 16388
2022-03-04 15:07:42 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 15:07:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:09:11 | INFO | train_inner | epoch 065:     35 / 97 loss=2.873, nll_loss=2.603, ppl=6.08, wps=24732.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.125, loss_scale=32, train_wall=235, gb_free=21, wall=16477
2022-03-04 15:11:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:11:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:11:54 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.878 | nll_loss 9.682 | ppl 821.59 | wps 45373.1 | wpb 510.9 | bsz 1 | num_updates 6260 | best_loss 7.571
2022-03-04 15:11:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6260 updates
2022-03-04 15:11:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:11:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 65 @ 6260 updates, score 9.878) (writing took 2.408868790604174 seconds)
2022-03-04 15:11:56 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 15:11:56 | INFO | train | epoch 065 | loss 2.844 | nll_loss 2.573 | ppl 5.95 | wps 24447.6 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 6260 | lr 0.00039968 | gnorm 1.13 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16642
2022-03-04 15:11:56 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 15:11:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:13:38 | INFO | train_inner | epoch 066:     40 / 97 loss=2.826, nll_loss=2.554, ppl=5.87, wps=24510.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.129, loss_scale=16, train_wall=237, gb_free=21, wall=16744
2022-03-04 15:16:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:16:08 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.959 | nll_loss 9.766 | ppl 870.61 | wps 45213.3 | wpb 510.9 | bsz 1 | num_updates 6357 | best_loss 7.571
2022-03-04 15:16:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6357 updates
2022-03-04 15:16:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:16:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:16:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 66 @ 6357 updates, score 9.959) (writing took 2.4025772409513593 seconds)
2022-03-04 15:16:11 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 15:16:11 | INFO | train | epoch 066 | loss 2.801 | nll_loss 2.529 | ppl 5.77 | wps 24949.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6357 | lr 0.000396619 | gnorm 1.124 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16897
2022-03-04 15:16:11 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 15:16:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:18:01 | INFO | train_inner | epoch 067:     43 / 97 loss=2.78, nll_loss=2.507, ppl=5.69, wps=24975.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.123, loss_scale=32, train_wall=233, gb_free=21, wall=17007
2022-03-04 15:18:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:20:23 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.016 | nll_loss 9.822 | ppl 904.92 | wps 45323.8 | wpb 510.9 | bsz 1 | num_updates 6453 | best_loss 7.571
2022-03-04 15:20:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6453 updates
2022-03-04 15:20:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:20:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:20:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 67 @ 6453 updates, score 10.016) (writing took 2.4740745574235916 seconds)
2022-03-04 15:20:25 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 15:20:25 | INFO | train | epoch 067 | loss 2.754 | nll_loss 2.481 | ppl 5.58 | wps 24701.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6453 | lr 0.000393658 | gnorm 1.125 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 17152
2022-03-04 15:20:25 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 15:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:22:25 | INFO | train_inner | epoch 068:     47 / 97 loss=2.736, nll_loss=2.461, ppl=5.51, wps=24737.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1.12, loss_scale=16, train_wall=235, gb_free=21, wall=17271
2022-03-04 15:24:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:24:38 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.112 | nll_loss 9.915 | ppl 965.28 | wps 45257.7 | wpb 510.9 | bsz 1 | num_updates 6550 | best_loss 7.571
2022-03-04 15:24:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6550 updates
2022-03-04 15:24:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:24:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:24:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 68 @ 6550 updates, score 10.112) (writing took 2.3755303993821144 seconds)
2022-03-04 15:24:40 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 15:24:40 | INFO | train | epoch 068 | loss 2.712 | nll_loss 2.436 | ppl 5.41 | wps 24960.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6550 | lr 0.000390732 | gnorm 1.136 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 17406
2022-03-04 15:24:40 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 15:24:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:26:48 | INFO | train_inner | epoch 069:     50 / 97 loss=2.693, nll_loss=2.417, ppl=5.34, wps=24973.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.129, loss_scale=32, train_wall=233, gb_free=21, wall=17534
2022-03-04 15:27:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:28:52 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.192 | nll_loss 9.998 | ppl 1022.82 | wps 45215.9 | wpb 510.9 | bsz 1 | num_updates 6646 | best_loss 7.571
2022-03-04 15:28:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6646 updates
2022-03-04 15:28:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:28:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:28:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 69 @ 6646 updates, score 10.192) (writing took 2.4204569179564714 seconds)
2022-03-04 15:28:55 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 15:28:55 | INFO | train | epoch 069 | loss 2.667 | nll_loss 2.39 | ppl 5.24 | wps 24689.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6646 | lr 0.0003879 | gnorm 1.123 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 17661
2022-03-04 15:28:55 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 15:28:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:31:12 | INFO | train_inner | epoch 070:     54 / 97 loss=2.642, nll_loss=2.364, ppl=5.15, wps=24732.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.143, loss_scale=16, train_wall=235, gb_free=21, wall=17799
2022-03-04 15:33:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:33:07 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.285 | nll_loss 10.089 | ppl 1089.4 | wps 45263.6 | wpb 510.9 | bsz 1 | num_updates 6743 | best_loss 7.571
2022-03-04 15:33:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6743 updates
2022-03-04 15:33:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:33:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:33:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 70 @ 6743 updates, score 10.285) (writing took 2.3943165335804224 seconds)
2022-03-04 15:33:09 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 15:33:09 | INFO | train | epoch 070 | loss 2.629 | nll_loss 2.35 | ppl 5.1 | wps 24955.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6743 | lr 0.0003851 | gnorm 1.134 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 17915
2022-03-04 15:33:09 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 15:33:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:34:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:35:37 | INFO | train_inner | epoch 071:     58 / 97 loss=2.61, nll_loss=2.33, ppl=5.03, wps=24743, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.123, loss_scale=16, train_wall=235, gb_free=21, wall=18063
2022-03-04 15:37:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:37:21 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.313 | nll_loss 10.118 | ppl 1111 | wps 45361.9 | wpb 510.9 | bsz 1 | num_updates 6839 | best_loss 7.571
2022-03-04 15:37:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6839 updates
2022-03-04 15:37:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:37:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:37:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 71 @ 6839 updates, score 10.313) (writing took 2.417305159382522 seconds)
2022-03-04 15:37:24 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 15:37:24 | INFO | train | epoch 071 | loss 2.587 | nll_loss 2.307 | ppl 4.95 | wps 24706.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6839 | lr 0.000382388 | gnorm 1.121 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 18170
2022-03-04 15:37:24 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 15:37:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:39:59 | INFO | train_inner | epoch 072:     61 / 97 loss=2.561, nll_loss=2.28, ppl=4.86, wps=24988.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.121, loss_scale=16, train_wall=233, gb_free=21, wall=18325
2022-03-04 15:41:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:41:36 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.393 | nll_loss 10.196 | ppl 1173.03 | wps 45229.4 | wpb 510.9 | bsz 1 | num_updates 6936 | best_loss 7.571
2022-03-04 15:41:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6936 updates
2022-03-04 15:41:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:41:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:41:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 72 @ 6936 updates, score 10.393) (writing took 2.366823857650161 seconds)
2022-03-04 15:41:38 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 15:41:38 | INFO | train | epoch 072 | loss 2.55 | nll_loss 2.268 | ppl 4.82 | wps 24973.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6936 | lr 0.000379704 | gnorm 1.133 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 18424
2022-03-04 15:41:38 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 15:41:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:44:21 | INFO | train_inner | epoch 073:     64 / 97 loss=2.525, nll_loss=2.242, ppl=4.73, wps=24990.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.137, loss_scale=32, train_wall=233, gb_free=21, wall=18587
2022-03-04 15:45:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:45:50 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.45 | nll_loss 10.253 | ppl 1220.54 | wps 45429.3 | wpb 510.9 | bsz 1 | num_updates 7033 | best_loss 7.571
2022-03-04 15:45:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7033 updates
2022-03-04 15:45:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:45:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:45:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 73 @ 7033 updates, score 10.45) (writing took 2.3521029967814684 seconds)
2022-03-04 15:45:53 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 15:45:53 | INFO | train | epoch 073 | loss 2.513 | nll_loss 2.23 | ppl 4.69 | wps 24962 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7033 | lr 0.000377077 | gnorm 1.141 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 18679
2022-03-04 15:45:53 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 15:45:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:46:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:46:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:48:48 | INFO | train_inner | epoch 074:     69 / 97 loss=2.497, nll_loss=2.213, ppl=4.64, wps=24515.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.145, loss_scale=16, train_wall=237, gb_free=21, wall=18855
2022-03-04 15:50:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:50:04 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.55 | nll_loss 10.354 | ppl 1308.5 | wps 45332.1 | wpb 510.9 | bsz 1 | num_updates 7128 | best_loss 7.571
2022-03-04 15:50:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7128 updates
2022-03-04 15:50:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:50:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:50:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 74 @ 7128 updates, score 10.55) (writing took 2.3594434056431055 seconds)
2022-03-04 15:50:07 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 15:50:07 | INFO | train | epoch 074 | loss 2.473 | nll_loss 2.188 | ppl 4.56 | wps 24461.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 7128 | lr 0.000374555 | gnorm 1.141 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 18933
2022-03-04 15:50:07 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 15:50:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:52:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:53:13 | INFO | train_inner | epoch 075:     73 / 97 loss=2.447, nll_loss=2.161, ppl=4.47, wps=24753.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.137, loss_scale=16, train_wall=235, gb_free=21, wall=19119
2022-03-04 15:54:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:54:19 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.613 | nll_loss 10.418 | ppl 1367.82 | wps 45323.8 | wpb 510.9 | bsz 1 | num_updates 7224 | best_loss 7.571
2022-03-04 15:54:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7224 updates
2022-03-04 15:54:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:54:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 75 @ 7224 updates, score 10.613) (writing took 2.398661312647164 seconds)
2022-03-04 15:54:21 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 15:54:21 | INFO | train | epoch 075 | loss 2.44 | nll_loss 2.153 | ppl 4.45 | wps 24715.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7224 | lr 0.000372058 | gnorm 1.128 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 19187
2022-03-04 15:54:21 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 15:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:57:35 | INFO | train_inner | epoch 076:     76 / 97 loss=2.417, nll_loss=2.13, ppl=4.38, wps=24998.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.128, loss_scale=16, train_wall=233, gb_free=21, wall=19381
2022-03-04 15:58:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:58:33 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.658 | nll_loss 10.465 | ppl 1413.57 | wps 45297.4 | wpb 510.9 | bsz 1 | num_updates 7321 | best_loss 7.571
2022-03-04 15:58:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7321 updates
2022-03-04 15:58:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:58:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:58:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 76 @ 7321 updates, score 10.658) (writing took 2.333650415763259 seconds)
2022-03-04 15:58:36 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 15:58:36 | INFO | train | epoch 076 | loss 2.409 | nll_loss 2.121 | ppl 4.35 | wps 24979.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7321 | lr 0.000369585 | gnorm 1.129 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 19442
2022-03-04 15:58:36 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 15:58:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:58:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:02:00 | INFO | train_inner | epoch 077:     80 / 97 loss=2.382, nll_loss=2.094, ppl=4.27, wps=24752.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.129, loss_scale=16, train_wall=235, gb_free=21, wall=19646
2022-03-04 16:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:02:48 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.726 | nll_loss 10.533 | ppl 1481.22 | wps 45411.2 | wpb 510.9 | bsz 1 | num_updates 7417 | best_loss 7.571
2022-03-04 16:02:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7417 updates
2022-03-04 16:02:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:02:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 77 @ 7417 updates, score 10.726) (writing took 2.3424337580800056 seconds)
2022-03-04 16:02:50 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 16:02:50 | INFO | train | epoch 077 | loss 2.375 | nll_loss 2.087 | ppl 4.25 | wps 24699.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7417 | lr 0.000367186 | gnorm 1.127 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 19696
2022-03-04 16:02:50 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 16:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:04:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:06:24 | INFO | train_inner | epoch 078:     84 / 97 loss=2.35, nll_loss=2.06, ppl=4.17, wps=24754.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.119, loss_scale=16, train_wall=235, gb_free=21, wall=19910
2022-03-04 16:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:07:02 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.799 | nll_loss 10.601 | ppl 1552.97 | wps 45446 | wpb 510.9 | bsz 1 | num_updates 7513 | best_loss 7.571
2022-03-04 16:07:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7513 updates
2022-03-04 16:07:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:07:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:07:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 78 @ 7513 updates, score 10.799) (writing took 2.3618987100198865 seconds)
2022-03-04 16:07:04 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 16:07:04 | INFO | train | epoch 078 | loss 2.34 | nll_loss 2.05 | ppl 4.14 | wps 24730.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7513 | lr 0.000364832 | gnorm 1.119 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 19950
2022-03-04 16:07:04 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 16:07:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:10:46 | INFO | train_inner | epoch 079:     87 / 97 loss=2.318, nll_loss=2.026, ppl=4.07, wps=24982.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.124, loss_scale=32, train_wall=233, gb_free=21, wall=20172
2022-03-04 16:11:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:11:16 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.859 | nll_loss 10.664 | ppl 1622.16 | wps 45289.1 | wpb 510.9 | bsz 1 | num_updates 7610 | best_loss 7.571
2022-03-04 16:11:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7610 updates
2022-03-04 16:11:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:11:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 79 @ 7610 updates, score 10.859) (writing took 2.4051064513623714 seconds)
2022-03-04 16:11:19 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 16:11:19 | INFO | train | epoch 079 | loss 2.312 | nll_loss 2.02 | ppl 4.06 | wps 24952.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7610 | lr 0.0003625 | gnorm 1.121 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 20205
2022-03-04 16:11:19 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 16:11:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:11:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:15:11 | INFO | train_inner | epoch 080:     91 / 97 loss=2.287, nll_loss=1.995, ppl=3.99, wps=24755.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.127, loss_scale=16, train_wall=235, gb_free=21, wall=20437
2022-03-04 16:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:15:31 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.924 | nll_loss 10.727 | ppl 1694.79 | wps 45494.4 | wpb 510.9 | bsz 1 | num_updates 7706 | best_loss 7.571
2022-03-04 16:15:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7706 updates
2022-03-04 16:15:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:15:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 80 @ 7706 updates, score 10.924) (writing took 2.3356989044696093 seconds)
2022-03-04 16:15:33 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 16:15:33 | INFO | train | epoch 080 | loss 2.281 | nll_loss 1.988 | ppl 3.97 | wps 24724.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7706 | lr 0.000360235 | gnorm 1.131 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 20459
2022-03-04 16:15:33 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 16:15:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:17:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:19:35 | INFO | train_inner | epoch 081:     95 / 97 loss=2.256, nll_loss=1.963, ppl=3.9, wps=24757.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=7800, lr=0.000358057, gnorm=1.133, loss_scale=16, train_wall=235, gb_free=21, wall=20702
2022-03-04 16:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:19:45 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.963 | nll_loss 10.767 | ppl 1742.29 | wps 45229.5 | wpb 510.9 | bsz 1 | num_updates 7802 | best_loss 7.571
2022-03-04 16:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7802 updates
2022-03-04 16:19:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:19:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:19:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 81 @ 7802 updates, score 10.963) (writing took 2.3434061082080007 seconds)
2022-03-04 16:19:48 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 16:19:48 | INFO | train | epoch 081 | loss 2.251 | nll_loss 1.957 | ppl 3.88 | wps 24716 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7802 | lr 0.000358012 | gnorm 1.131 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 20714
2022-03-04 16:19:48 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 16:19:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:23:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:24:00 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 11.061 | nll_loss 10.863 | ppl 1861.83 | wps 45429 | wpb 510.9 | bsz 1 | num_updates 7899 | best_loss 7.571
2022-03-04 16:24:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7899 updates
2022-03-04 16:24:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:24:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:24:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 82 @ 7899 updates, score 11.061) (writing took 2.37472474668175 seconds)
2022-03-04 16:24:02 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 16:24:02 | INFO | train | epoch 082 | loss 2.225 | nll_loss 1.93 | ppl 3.81 | wps 24970.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7899 | lr 0.000355807 | gnorm 1.123 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 20968
2022-03-04 16:24:02 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 16:24:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:24:05 | INFO | train_inner | epoch 083:      1 / 97 loss=2.226, nll_loss=1.931, ppl=3.81, wps=24308.8, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=7900, lr=0.000355784, gnorm=1.123, loss_scale=32, train_wall=232, gb_free=21, wall=20971
2022-03-04 16:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:28:14 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 11.128 | nll_loss 10.933 | ppl 1954.43 | wps 45449 | wpb 510.9 | bsz 1 | num_updates 7996 | best_loss 7.571
2022-03-04 16:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7996 updates
2022-03-04 16:28:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:28:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:28:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 83 @ 7996 updates, score 11.128) (writing took 2.394024958834052 seconds)
2022-03-04 16:28:16 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 16:28:16 | INFO | train | epoch 083 | loss 2.198 | nll_loss 1.902 | ppl 3.74 | wps 24968.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7996 | lr 0.000353642 | gnorm 1.123 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 21223
2022-03-04 16:28:16 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 16:28:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:28:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:28:29 | INFO | train_inner | epoch 084:      5 / 97 loss=2.194, nll_loss=1.897, ppl=3.73, wps=24751.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8000, lr=0.000353553, gnorm=1.121, loss_scale=32, train_wall=235, gb_free=21, wall=21235
2022-03-04 16:32:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:32:29 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 11.174 | nll_loss 10.977 | ppl 2016.03 | wps 45324.7 | wpb 510.9 | bsz 1 | num_updates 8092 | best_loss 7.571
2022-03-04 16:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8092 updates
2022-03-04 16:32:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:32:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 84 @ 8092 updates, score 11.174) (writing took 2.39775026217103 seconds)
2022-03-04 16:32:31 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 16:32:31 | INFO | train | epoch 084 | loss 2.169 | nll_loss 1.872 | ppl 3.66 | wps 24699.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8092 | lr 0.000351538 | gnorm 1.108 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 21477
2022-03-04 16:32:31 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 16:32:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:32:51 | INFO | train_inner | epoch 085:      8 / 97 loss=2.165, nll_loss=1.868, ppl=3.65, wps=24976.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.109, loss_scale=32, train_wall=233, gb_free=21, wall=21498
2022-03-04 16:33:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:36:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:36:43 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 11.256 | nll_loss 11.058 | ppl 2131.79 | wps 45363.2 | wpb 510.9 | bsz 1 | num_updates 8188 | best_loss 7.571
2022-03-04 16:36:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8188 updates
2022-03-04 16:36:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:36:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:36:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 85 @ 8188 updates, score 11.256) (writing took 2.3504328969866037 seconds)
2022-03-04 16:36:45 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 16:36:45 | INFO | train | epoch 085 | loss 2.145 | nll_loss 1.847 | ppl 3.6 | wps 24714.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8188 | lr 0.000349471 | gnorm 1.125 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 21731
2022-03-04 16:36:45 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 16:36:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:37:16 | INFO | train_inner | epoch 086:     12 / 97 loss=2.138, nll_loss=1.84, ppl=3.58, wps=24753.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.124, loss_scale=16, train_wall=235, gb_free=21, wall=21762
2022-03-04 16:40:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:40:57 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 11.31 | nll_loss 11.112 | ppl 2213.93 | wps 45406.2 | wpb 510.9 | bsz 1 | num_updates 8284 | best_loss 7.571
2022-03-04 16:40:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8284 updates
2022-03-04 16:40:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:41:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 86 @ 8284 updates, score 11.31) (writing took 2.334592259489 seconds)
2022-03-04 16:41:00 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 16:41:00 | INFO | train | epoch 086 | loss 2.117 | nll_loss 1.818 | ppl 3.53 | wps 24707.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8284 | lr 0.00034744 | gnorm 1.128 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 21986
2022-03-04 16:41:00 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 16:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:41:41 | INFO | train_inner | epoch 087:     16 / 97 loss=2.112, nll_loss=1.813, ppl=3.51, wps=24739.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.134, loss_scale=16, train_wall=235, gb_free=21, wall=22027
2022-03-04 16:45:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:45:12 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.351 | nll_loss 11.154 | ppl 2278.39 | wps 45326.1 | wpb 510.9 | bsz 1 | num_updates 8381 | best_loss 7.571
2022-03-04 16:45:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8381 updates
2022-03-04 16:45:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:45:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:45:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 87 @ 8381 updates, score 11.351) (writing took 2.4529301151633263 seconds)
2022-03-04 16:45:14 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 16:45:14 | INFO | train | epoch 087 | loss 2.096 | nll_loss 1.796 | ppl 3.47 | wps 24948.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8381 | lr 0.000345424 | gnorm 1.128 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 22241
2022-03-04 16:45:14 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 16:45:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:46:03 | INFO | train_inner | epoch 088:     19 / 97 loss=2.089, nll_loss=1.788, ppl=3.45, wps=24978.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.119, loss_scale=16, train_wall=233, gb_free=21, wall=22289
2022-03-04 16:46:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:49:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:49:26 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.467 | nll_loss 11.268 | ppl 2465.53 | wps 45466.7 | wpb 510.9 | bsz 1 | num_updates 8477 | best_loss 7.571
2022-03-04 16:49:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8477 updates
2022-03-04 16:49:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:49:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:49:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 88 @ 8477 updates, score 11.467) (writing took 2.335972134023905 seconds)
2022-03-04 16:49:29 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 16:49:29 | INFO | train | epoch 088 | loss 2.07 | nll_loss 1.769 | ppl 3.41 | wps 24714.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8477 | lr 0.000343462 | gnorm 1.125 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 22495
2022-03-04 16:49:29 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 16:49:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:50:28 | INFO | train_inner | epoch 089:     23 / 97 loss=2.063, nll_loss=1.762, ppl=3.39, wps=24752.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.121, loss_scale=16, train_wall=235, gb_free=21, wall=22554
2022-03-04 16:53:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:53:41 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.468 | nll_loss 11.273 | ppl 2474.09 | wps 45194.4 | wpb 510.9 | bsz 1 | num_updates 8574 | best_loss 7.571
2022-03-04 16:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8574 updates
2022-03-04 16:53:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:53:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 89 @ 8574 updates, score 11.468) (writing took 2.376466586254537 seconds)
2022-03-04 16:53:43 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 16:53:43 | INFO | train | epoch 089 | loss 2.048 | nll_loss 1.746 | ppl 3.35 | wps 24958.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8574 | lr 0.000341514 | gnorm 1.127 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 22749
2022-03-04 16:53:43 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 16:53:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:53:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:54:52 | INFO | train_inner | epoch 090:     27 / 97 loss=2.041, nll_loss=1.738, ppl=3.34, wps=24737.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.125, loss_scale=16, train_wall=235, gb_free=21, wall=22818
2022-03-04 16:57:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:57:55 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.529 | nll_loss 11.331 | ppl 2575.36 | wps 45346.2 | wpb 510.9 | bsz 1 | num_updates 8670 | best_loss 7.571
2022-03-04 16:57:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8670 updates
2022-03-04 16:57:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:57:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:57:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 90 @ 8670 updates, score 11.529) (writing took 2.3587699625641108 seconds)
2022-03-04 16:57:58 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 16:57:58 | INFO | train | epoch 090 | loss 2.023 | nll_loss 1.719 | ppl 3.29 | wps 24710.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8670 | lr 0.000339618 | gnorm 1.102 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 23004
2022-03-04 16:57:58 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 16:57:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:59:14 | INFO | train_inner | epoch 091:     30 / 97 loss=2.015, nll_loss=1.712, ppl=3.28, wps=24991.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8700, lr=0.000339032, gnorm=1.113, loss_scale=16, train_wall=233, gb_free=21, wall=23080
2022-03-04 17:00:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:02:10 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.596 | nll_loss 11.398 | ppl 2698.31 | wps 45349.6 | wpb 510.9 | bsz 1 | num_updates 8766 | best_loss 7.571
2022-03-04 17:02:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8766 updates
2022-03-04 17:02:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:02:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 91 @ 8766 updates, score 11.596) (writing took 2.426392164081335 seconds)
2022-03-04 17:02:12 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 17:02:12 | INFO | train | epoch 091 | loss 2.003 | nll_loss 1.699 | ppl 3.25 | wps 24705.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8766 | lr 0.000337753 | gnorm 1.119 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 23258
2022-03-04 17:02:12 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 17:02:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:03:39 | INFO | train_inner | epoch 092:     34 / 97 loss=1.995, nll_loss=1.691, ppl=3.23, wps=24736.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.121, loss_scale=16, train_wall=235, gb_free=21, wall=23345
2022-03-04 17:06:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:06:24 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.695 | nll_loss 11.499 | ppl 2893.41 | wps 45229.2 | wpb 510.9 | bsz 1 | num_updates 8863 | best_loss 7.571
2022-03-04 17:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8863 updates
2022-03-04 17:06:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:06:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:06:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 92 @ 8863 updates, score 11.695) (writing took 2.333137698471546 seconds)
2022-03-04 17:06:27 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 17:06:27 | INFO | train | epoch 092 | loss 1.983 | nll_loss 1.678 | ppl 3.2 | wps 24963.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8863 | lr 0.0003359 | gnorm 1.113 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 23513
2022-03-04 17:06:27 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 17:06:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:07:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:08:04 | INFO | train_inner | epoch 093:     38 / 97 loss=1.974, nll_loss=1.668, ppl=3.18, wps=24756.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.104, loss_scale=16, train_wall=235, gb_free=21, wall=23610
2022-03-04 17:10:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:10:39 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.71 | nll_loss 11.515 | ppl 2926.73 | wps 45417.5 | wpb 510.9 | bsz 1 | num_updates 8959 | best_loss 7.571
2022-03-04 17:10:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8959 updates
2022-03-04 17:10:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:10:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:10:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 93 @ 8959 updates, score 11.71) (writing took 2.35103399772197 seconds)
2022-03-04 17:10:41 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 17:10:41 | INFO | train | epoch 093 | loss 1.961 | nll_loss 1.655 | ppl 3.15 | wps 24712.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8959 | lr 0.000334095 | gnorm 1.115 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 23767
2022-03-04 17:10:41 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 17:10:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:12:26 | INFO | train_inner | epoch 094:     41 / 97 loss=1.951, nll_loss=1.644, ppl=3.13, wps=24981.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.119, loss_scale=16, train_wall=233, gb_free=21, wall=23872
2022-03-04 17:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:14:53 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.743 | nll_loss 11.548 | ppl 2994.18 | wps 45281 | wpb 510.9 | bsz 1 | num_updates 9056 | best_loss 7.571
2022-03-04 17:14:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9056 updates
2022-03-04 17:14:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:14:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:14:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 94 @ 9056 updates, score 11.743) (writing took 2.3428288782015443 seconds)
2022-03-04 17:14:56 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 17:14:56 | INFO | train | epoch 094 | loss 1.942 | nll_loss 1.635 | ppl 3.11 | wps 24960.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9056 | lr 0.000332301 | gnorm 1.117 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 24022
2022-03-04 17:14:56 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 17:14:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:16:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:16:51 | INFO | train_inner | epoch 095:     45 / 97 loss=1.938, nll_loss=1.631, ppl=3.1, wps=24740.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=9100, lr=0.000331497, gnorm=1.119, loss_scale=16, train_wall=235, gb_free=21, wall=24137
2022-03-04 17:19:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:19:08 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.802 | nll_loss 11.605 | ppl 3115.72 | wps 45282.9 | wpb 510.9 | bsz 1 | num_updates 9152 | best_loss 7.571
2022-03-04 17:19:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9152 updates
2022-03-04 17:19:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:19:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:19:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 95 @ 9152 updates, score 11.802) (writing took 2.451915103942156 seconds)
2022-03-04 17:19:10 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 17:19:10 | INFO | train | epoch 095 | loss 1.922 | nll_loss 1.615 | ppl 3.06 | wps 24690.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9152 | lr 0.000330554 | gnorm 1.123 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 24276
2022-03-04 17:19:10 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 17:19:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:21:13 | INFO | train_inner | epoch 096:     48 / 97 loss=1.911, nll_loss=1.603, ppl=3.04, wps=24965.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.115, loss_scale=16, train_wall=233, gb_free=21, wall=24399
2022-03-04 17:22:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:23:23 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.849 | nll_loss 11.653 | ppl 3221.33 | wps 45791.3 | wpb 510.9 | bsz 1 | num_updates 9248 | best_loss 7.571
2022-03-04 17:23:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9248 updates
2022-03-04 17:23:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:23:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:23:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 96 @ 9248 updates, score 11.849) (writing took 2.3550924537703395 seconds)
2022-03-04 17:23:25 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 17:23:25 | INFO | train | epoch 096 | loss 1.9 | nll_loss 1.591 | ppl 3.01 | wps 24695.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9248 | lr 0.000328834 | gnorm 1.108 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 24531
2022-03-04 17:23:25 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 17:23:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:25:38 | INFO | train_inner | epoch 097:     52 / 97 loss=1.892, nll_loss=1.583, ppl=3, wps=24742, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.11, loss_scale=16, train_wall=235, gb_free=21, wall=24664
2022-03-04 17:27:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:27:37 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.901 | nll_loss 11.705 | ppl 3338.4 | wps 45258.9 | wpb 510.9 | bsz 1 | num_updates 9345 | best_loss 7.571
2022-03-04 17:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9345 updates
2022-03-04 17:27:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:27:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 97 @ 9345 updates, score 11.901) (writing took 2.3174559781327844 seconds)
2022-03-04 17:27:40 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 17:27:40 | INFO | train | epoch 097 | loss 1.884 | nll_loss 1.575 | ppl 2.98 | wps 24946.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9345 | lr 0.000327122 | gnorm 1.121 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 24786
2022-03-04 17:27:40 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 17:27:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:29:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:30:02 | INFO | train_inner | epoch 098:     56 / 97 loss=1.871, nll_loss=1.562, ppl=2.95, wps=24724.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.115, loss_scale=16, train_wall=235, gb_free=21, wall=24929
2022-03-04 17:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:31:52 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.929 | nll_loss 11.735 | ppl 3409.44 | wps 45335.2 | wpb 510.9 | bsz 1 | num_updates 9441 | best_loss 7.571
2022-03-04 17:31:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9441 updates
2022-03-04 17:31:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:31:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:31:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 98 @ 9441 updates, score 11.929) (writing took 2.3232562877237797 seconds)
2022-03-04 17:31:54 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 17:31:54 | INFO | train | epoch 098 | loss 1.864 | nll_loss 1.554 | ppl 2.94 | wps 24707 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9441 | lr 0.000325455 | gnorm 1.102 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 25040
2022-03-04 17:31:54 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 17:31:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:34:25 | INFO | train_inner | epoch 099:     59 / 97 loss=1.858, nll_loss=1.547, ppl=2.92, wps=24991, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.11, loss_scale=16, train_wall=233, gb_free=21, wall=25191
2022-03-04 17:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:36:06 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 12.029 | nll_loss 11.836 | ppl 3655.83 | wps 45165.1 | wpb 510.9 | bsz 1 | num_updates 9538 | best_loss 7.571
2022-03-04 17:36:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9538 updates
2022-03-04 17:36:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:36:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 99 @ 9538 updates, score 12.029) (writing took 2.4135182900354266 seconds)
2022-03-04 17:36:09 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 17:36:09 | INFO | train | epoch 099 | loss 1.848 | nll_loss 1.537 | ppl 2.9 | wps 24954.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9538 | lr 0.000323796 | gnorm 1.109 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 25295
2022-03-04 17:36:09 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 17:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:38:47 | INFO | train_inner | epoch 100:     62 / 97 loss=1.836, nll_loss=1.525, ppl=2.88, wps=24971.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.102, loss_scale=32, train_wall=233, gb_free=21, wall=25453
2022-03-04 17:40:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:40:21 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 12.047 | nll_loss 11.855 | ppl 3703.3 | wps 45375.7 | wpb 510.9 | bsz 1 | num_updates 9635 | best_loss 7.571
2022-03-04 17:40:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9635 updates
2022-03-04 17:40:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:40:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 100 @ 9635 updates, score 12.047) (writing took 2.3350524455308914 seconds)
2022-03-04 17:40:23 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 17:40:23 | INFO | train | epoch 100 | loss 1.831 | nll_loss 1.519 | ppl 2.87 | wps 24954.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9635 | lr 0.000322162 | gnorm 1.105 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 25549
2022-03-04 17:40:23 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 17:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:40:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:43:12 | INFO | train_inner | epoch 101:     66 / 97 loss=1.821, nll_loss=1.509, ppl=2.85, wps=24744.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.114, loss_scale=16, train_wall=235, gb_free=21, wall=25718
2022-03-04 17:44:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:44:35 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 12.124 | nll_loss 11.929 | ppl 3900.47 | wps 45352.5 | wpb 510.9 | bsz 1 | num_updates 9731 | best_loss 7.571
2022-03-04 17:44:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9731 updates
2022-03-04 17:44:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:44:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:44:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 101 @ 9731 updates, score 12.124) (writing took 2.3562400517985225 seconds)
2022-03-04 17:44:38 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 17:44:38 | INFO | train | epoch 101 | loss 1.813 | nll_loss 1.501 | ppl 2.83 | wps 24698 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9731 | lr 0.000320569 | gnorm 1.108 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 25804
2022-03-04 17:44:38 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 17:44:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:47:34 | INFO | train_inner | epoch 102:     69 / 97 loss=1.803, nll_loss=1.49, ppl=2.81, wps=24970, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.097, loss_scale=32, train_wall=233, gb_free=21, wall=25980
2022-03-04 17:47:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:48:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:48:50 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 12.117 | nll_loss 11.922 | ppl 3880 | wps 45153.9 | wpb 510.9 | bsz 1 | num_updates 9827 | best_loss 7.571
2022-03-04 17:48:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9827 updates
2022-03-04 17:48:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:48:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:48:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 102 @ 9827 updates, score 12.117) (writing took 2.336909521371126 seconds)
2022-03-04 17:48:52 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 17:48:52 | INFO | train | epoch 102 | loss 1.796 | nll_loss 1.483 | ppl 2.79 | wps 24701.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9827 | lr 0.000318999 | gnorm 1.108 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 26058
2022-03-04 17:48:52 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 17:48:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:51:59 | INFO | train_inner | epoch 103:     73 / 97 loss=1.782, nll_loss=1.469, ppl=2.77, wps=24739.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.104, loss_scale=16, train_wall=235, gb_free=21, wall=26245
2022-03-04 17:53:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:53:04 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 12.21 | nll_loss 12.016 | ppl 4142.88 | wps 45311.8 | wpb 510.9 | bsz 1 | num_updates 9924 | best_loss 7.571
2022-03-04 17:53:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9924 updates
2022-03-04 17:53:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:53:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:53:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 103 @ 9924 updates, score 12.21) (writing took 2.430048911832273 seconds)
2022-03-04 17:53:07 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 17:53:07 | INFO | train | epoch 103 | loss 1.779 | nll_loss 1.466 | ppl 2.76 | wps 24946.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9924 | lr 0.000317436 | gnorm 1.09 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 26313
2022-03-04 17:53:07 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 17:53:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:53:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:56:23 | INFO | train_inner | epoch 104:     77 / 97 loss=1.77, nll_loss=1.456, ppl=2.74, wps=24733.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.087, loss_scale=16, train_wall=235, gb_free=21, wall=26509
2022-03-04 17:57:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:57:19 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 12.231 | nll_loss 12.037 | ppl 4201.51 | wps 45137.5 | wpb 510.9 | bsz 1 | num_updates 10020 | best_loss 7.571
2022-03-04 17:57:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10020 updates
2022-03-04 17:57:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:57:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 104 @ 10020 updates, score 12.231) (writing took 2.3909139493480325 seconds)
2022-03-04 17:57:21 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 17:57:21 | INFO | train | epoch 104 | loss 1.762 | nll_loss 1.448 | ppl 2.73 | wps 24697.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 10020 | lr 0.000315912 | gnorm 1.084 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 26568
2022-03-04 17:57:21 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 17:57:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:00:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:00:48 | INFO | train_inner | epoch 105:     81 / 97 loss=1.752, nll_loss=1.437, ppl=2.71, wps=24726.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.078, loss_scale=16, train_wall=235, gb_free=21, wall=26774
2022-03-04 18:01:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:01:34 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 12.25 | nll_loss 12.055 | ppl 4255.86 | wps 45287.9 | wpb 510.9 | bsz 1 | num_updates 10116 | best_loss 7.571
2022-03-04 18:01:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10116 updates
2022-03-04 18:01:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:01:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:01:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 105 @ 10116 updates, score 12.25) (writing took 2.4120151130482554 seconds)
2022-03-04 18:01:36 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 18:01:36 | INFO | train | epoch 105 | loss 1.748 | nll_loss 1.433 | ppl 2.7 | wps 24684.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 10116 | lr 0.000314409 | gnorm 1.081 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 26822
2022-03-04 18:01:36 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 18:01:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:05:11 | INFO | train_inner | epoch 106:     84 / 97 loss=1.74, nll_loss=1.425, ppl=2.68, wps=24964.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=10200, lr=0.000313112, gnorm=1.121, loss_scale=16, train_wall=233, gb_free=21, wall=27037
2022-03-04 18:05:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:05:48 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 12.312 | nll_loss 12.119 | ppl 4447.57 | wps 45233 | wpb 510.9 | bsz 1 | num_updates 10213 | best_loss 7.571
2022-03-04 18:05:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10213 updates
2022-03-04 18:05:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 106 @ 10213 updates, score 12.312) (writing took 2.4012435264885426 seconds)
2022-03-04 18:05:51 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 18:05:51 | INFO | train | epoch 106 | loss 1.737 | nll_loss 1.421 | ppl 2.68 | wps 24945.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10213 | lr 0.000312913 | gnorm 1.122 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 27077
2022-03-04 18:05:51 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 18:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:08:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:09:35 | INFO | train_inner | epoch 107:     88 / 97 loss=1.724, nll_loss=1.408, ppl=2.65, wps=24737.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.078, loss_scale=16, train_wall=235, gb_free=21, wall=27301
2022-03-04 18:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:10:03 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 12.373 | nll_loss 12.178 | ppl 4635.22 | wps 45276.3 | wpb 510.9 | bsz 1 | num_updates 10309 | best_loss 7.571
2022-03-04 18:10:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10309 updates
2022-03-04 18:10:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:10:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:10:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 107 @ 10309 updates, score 12.373) (writing took 2.430516330525279 seconds)
2022-03-04 18:10:05 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 18:10:05 | INFO | train | epoch 107 | loss 1.718 | nll_loss 1.401 | ppl 2.64 | wps 24697.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 10309 | lr 0.000311452 | gnorm 1.075 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 27332
2022-03-04 18:10:05 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 18:10:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:58 | INFO | train_inner | epoch 108:     91 / 97 loss=1.709, nll_loss=1.392, ppl=2.62, wps=24972, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.098, loss_scale=32, train_wall=233, gb_free=21, wall=27564
2022-03-04 18:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:14:18 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 12.37 | nll_loss 12.174 | ppl 4621.36 | wps 45299.4 | wpb 510.9 | bsz 1 | num_updates 10406 | best_loss 7.571
2022-03-04 18:14:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10406 updates
2022-03-04 18:14:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:14:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:14:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 108 @ 10406 updates, score 12.37) (writing took 2.3723686896264553 seconds)
2022-03-04 18:14:20 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 18:14:20 | INFO | train | epoch 108 | loss 1.706 | nll_loss 1.39 | ppl 2.62 | wps 24954.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10406 | lr 0.000309997 | gnorm 1.097 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 27586
2022-03-04 18:14:20 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 18:14:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:15:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:18:22 | INFO | train_inner | epoch 109:     95 / 97 loss=1.693, nll_loss=1.376, ppl=2.59, wps=24729.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10500, lr=0.000308607, gnorm=1.092, loss_scale=16, train_wall=235, gb_free=21, wall=27829
2022-03-04 18:18:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:18:32 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 12.442 | nll_loss 12.249 | ppl 4868.91 | wps 45185.4 | wpb 510.9 | bsz 1 | num_updates 10502 | best_loss 7.571
2022-03-04 18:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10502 updates
2022-03-04 18:18:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:18:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:18:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 109 @ 10502 updates, score 12.442) (writing took 2.349821414798498 seconds)
2022-03-04 18:18:35 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 18:18:35 | INFO | train | epoch 109 | loss 1.69 | nll_loss 1.372 | ppl 2.59 | wps 24693.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 10502 | lr 0.000308577 | gnorm 1.091 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 27841
2022-03-04 18:18:35 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 18:18:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:20:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:22:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:22:47 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 12.476 | nll_loss 12.282 | ppl 4978.93 | wps 44343.1 | wpb 510.9 | bsz 1 | num_updates 10598 | best_loss 7.571
2022-03-04 18:22:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10598 updates
2022-03-04 18:22:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:22:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:22:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 110 @ 10598 updates, score 12.476) (writing took 2.3425005273893476 seconds)
2022-03-04 18:22:49 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 18:22:49 | INFO | train | epoch 110 | loss 1.678 | nll_loss 1.36 | ppl 2.57 | wps 24687.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 10598 | lr 0.000307177 | gnorm 1.093 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 28095
2022-03-04 18:22:49 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 18:22:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:22:54 | INFO | train_inner | epoch 111:      2 / 97 loss=1.677, nll_loss=1.359, ppl=2.57, wps=24054.7, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=10600, lr=0.000307148, gnorm=1.092, loss_scale=16, train_wall=235, gb_free=21, wall=28101
2022-03-04 18:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:27:04 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 12.529 | nll_loss 12.335 | ppl 5168.34 | wps 44246.6 | wpb 510.9 | bsz 1 | num_updates 10695 | best_loss 7.571
2022-03-04 18:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10695 updates
2022-03-04 18:27:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:27:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 111 @ 10695 updates, score 12.529) (writing took 2.4619455486536026 seconds)
2022-03-04 18:27:06 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 18:27:06 | INFO | train | epoch 111 | loss 1.664 | nll_loss 1.346 | ppl 2.54 | wps 24704.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10695 | lr 0.00030578 | gnorm 1.093 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 28353
2022-03-04 18:27:06 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 18:27:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:27:19 | INFO | train_inner | epoch 112:      5 / 97 loss=1.662, nll_loss=1.343, ppl=2.54, wps=24729.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.093, loss_scale=32, train_wall=235, gb_free=21, wall=28365
2022-03-04 18:29:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:31:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:31:21 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 12.534 | nll_loss 12.342 | ppl 5192.45 | wps 44269.9 | wpb 510.9 | bsz 1 | num_updates 10791 | best_loss 7.571
2022-03-04 18:31:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10791 updates
2022-03-04 18:31:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:31:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:31:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 112 @ 10791 updates, score 12.534) (writing took 2.4038745602592826 seconds)
2022-03-04 18:31:23 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 18:31:23 | INFO | train | epoch 112 | loss 1.649 | nll_loss 1.33 | ppl 2.51 | wps 24464.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10791 | lr 0.000304417 | gnorm 1.078 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 28610
2022-03-04 18:31:23 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 18:31:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:31:47 | INFO | train_inner | epoch 113:      9 / 97 loss=1.645, nll_loss=1.326, ppl=2.51, wps=24497.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.072, loss_scale=16, train_wall=237, gb_free=21, wall=28633
2022-03-04 18:34:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:35:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:35:38 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 12.62 | nll_loss 12.428 | ppl 5508.72 | wps 44854.6 | wpb 510.9 | bsz 1 | num_updates 10887 | best_loss 7.571
2022-03-04 18:35:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10887 updates
2022-03-04 18:35:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:35:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:35:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 113 @ 10887 updates, score 12.62) (writing took 2.367227981798351 seconds)
2022-03-04 18:35:40 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 18:35:40 | INFO | train | epoch 113 | loss 1.637 | nll_loss 1.317 | ppl 2.49 | wps 24504.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10887 | lr 0.000303072 | gnorm 1.077 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 28866
2022-03-04 18:35:40 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 18:35:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:36:13 | INFO | train_inner | epoch 114:     13 / 97 loss=1.634, nll_loss=1.314, ppl=2.49, wps=24552.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.078, loss_scale=16, train_wall=237, gb_free=21, wall=28900
2022-03-04 18:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:39:54 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 12.649 | nll_loss 12.457 | ppl 5620.59 | wps 44860.9 | wpb 510.9 | bsz 1 | num_updates 10984 | best_loss 7.571
2022-03-04 18:39:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10984 updates
2022-03-04 18:39:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:39:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:39:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 114 @ 10984 updates, score 12.649) (writing took 2.4051399044692516 seconds)
2022-03-04 18:39:56 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 18:39:56 | INFO | train | epoch 114 | loss 1.625 | nll_loss 1.305 | ppl 2.47 | wps 24792.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10984 | lr 0.000301731 | gnorm 1.073 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 29122
2022-03-04 18:39:56 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 18:39:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:40:37 | INFO | train_inner | epoch 115:     16 / 97 loss=1.624, nll_loss=1.304, ppl=2.47, wps=24818.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.074, loss_scale=16, train_wall=234, gb_free=21, wall=29163
2022-03-04 18:41:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:44:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:44:10 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 12.644 | nll_loss 12.451 | ppl 5600.5 | wps 45344.2 | wpb 510.9 | bsz 1 | num_updates 11080 | best_loss 7.571
2022-03-04 18:44:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11080 updates
2022-03-04 18:44:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:44:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:44:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 115 @ 11080 updates, score 12.644) (writing took 2.408913447521627 seconds)
2022-03-04 18:44:12 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 18:44:12 | INFO | train | epoch 115 | loss 1.612 | nll_loss 1.291 | ppl 2.45 | wps 24563.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 11080 | lr 0.000300421 | gnorm 1.065 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 29378
2022-03-04 18:44:12 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 18:44:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:45:03 | INFO | train_inner | epoch 116:     20 / 97 loss=1.605, nll_loss=1.285, ppl=2.44, wps=24601.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.054, loss_scale=16, train_wall=236, gb_free=21, wall=29430
2022-03-04 18:47:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:48:26 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 12.738 | nll_loss 12.545 | ppl 5974.21 | wps 44868.3 | wpb 510.9 | bsz 1 | num_updates 11176 | best_loss 7.571
2022-03-04 18:48:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11176 updates
2022-03-04 18:48:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:48:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 116 @ 11176 updates, score 12.738) (writing took 2.3507455559447408 seconds)
2022-03-04 18:48:28 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 18:48:28 | INFO | train | epoch 116 | loss 1.602 | nll_loss 1.281 | ppl 2.43 | wps 24564.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 11176 | lr 0.000299128 | gnorm 1.07 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 29634
2022-03-04 18:48:28 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 18:48:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:49:30 | INFO | train_inner | epoch 117:     24 / 97 loss=1.6, nll_loss=1.279, ppl=2.43, wps=24602.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.081, loss_scale=16, train_wall=236, gb_free=21, wall=29696
2022-03-04 18:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:52:42 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 12.732 | nll_loss 12.539 | ppl 5951.96 | wps 44563.5 | wpb 510.9 | bsz 1 | num_updates 11273 | best_loss 7.571
2022-03-04 18:52:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11273 updates
2022-03-04 18:52:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:52:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:52:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 117 @ 11273 updates, score 12.732) (writing took 2.335856613703072 seconds)
2022-03-04 18:52:44 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 18:52:44 | INFO | train | epoch 117 | loss 1.591 | nll_loss 1.269 | ppl 2.41 | wps 24818.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11273 | lr 0.000297838 | gnorm 1.083 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 29890
2022-03-04 18:52:44 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 18:52:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:53:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:53:56 | INFO | train_inner | epoch 118:     28 / 97 loss=1.584, nll_loss=1.263, ppl=2.4, wps=24604.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.08, loss_scale=16, train_wall=236, gb_free=21, wall=29962
2022-03-04 18:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:56:58 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 12.826 | nll_loss 12.635 | ppl 6361.5 | wps 44503.5 | wpb 510.9 | bsz 1 | num_updates 11369 | best_loss 7.571
2022-03-04 18:56:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11369 updates
2022-03-04 18:56:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:57:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:57:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 118 @ 11369 updates, score 12.826) (writing took 2.461665994487703 seconds)
2022-03-04 18:57:00 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 18:57:00 | INFO | train | epoch 118 | loss 1.577 | nll_loss 1.255 | ppl 2.39 | wps 24528.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11369 | lr 0.000296578 | gnorm 1.063 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30147
2022-03-04 18:57:00 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 18:57:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:58:20 | INFO | train_inner | epoch 119:     31 / 97 loss=1.574, nll_loss=1.252, ppl=2.38, wps=24788.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.062, loss_scale=16, train_wall=234, gb_free=21, wall=30226
2022-03-04 18:59:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:01:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:01:15 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 12.817 | nll_loss 12.625 | ppl 6318.09 | wps 44203.7 | wpb 510.9 | bsz 1 | num_updates 11465 | best_loss 7.571
2022-03-04 19:01:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11465 updates
2022-03-04 19:01:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:01:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:01:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 119 @ 11465 updates, score 12.817) (writing took 2.4879316864535213 seconds)
2022-03-04 19:01:17 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 19:01:17 | INFO | train | epoch 119 | loss 1.568 | nll_loss 1.245 | ppl 2.37 | wps 24495.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11465 | lr 0.000295334 | gnorm 1.071 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30403
2022-03-04 19:01:17 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 19:01:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:02:47 | INFO | train_inner | epoch 120:     35 / 97 loss=1.562, nll_loss=1.239, ppl=2.36, wps=24526.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.062, loss_scale=16, train_wall=237, gb_free=21, wall=30493
2022-03-04 19:05:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:05:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:05:32 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 12.882 | nll_loss 12.691 | ppl 6611.2 | wps 44431.4 | wpb 510.9 | bsz 1 | num_updates 11561 | best_loss 7.571
2022-03-04 19:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11561 updates
2022-03-04 19:05:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 120 @ 11561 updates, score 12.882) (writing took 2.347581923007965 seconds)
2022-03-04 19:05:34 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 19:05:34 | INFO | train | epoch 120 | loss 1.554 | nll_loss 1.231 | ppl 2.35 | wps 24479.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11561 | lr 0.000294105 | gnorm 1.057 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30660
2022-03-04 19:05:34 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 19:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:07:14 | INFO | train_inner | epoch 121:     39 / 97 loss=1.552, nll_loss=1.229, ppl=2.34, wps=24510.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.064, loss_scale=16, train_wall=237, gb_free=21, wall=30760
2022-03-04 19:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:48 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 12.867 | nll_loss 12.677 | ppl 6548.25 | wps 45122.9 | wpb 510.9 | bsz 1 | num_updates 11658 | best_loss 7.571
2022-03-04 19:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11658 updates
2022-03-04 19:09:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:09:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:09:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 121 @ 11658 updates, score 12.867) (writing took 2.327129017561674 seconds)
2022-03-04 19:09:51 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 19:09:51 | INFO | train | epoch 121 | loss 1.546 | nll_loss 1.223 | ppl 2.33 | wps 24745.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11658 | lr 0.000292879 | gnorm 1.077 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30917
2022-03-04 19:09:51 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 19:09:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:11:39 | INFO | train_inner | epoch 122:     42 / 97 loss=1.539, nll_loss=1.216, ppl=2.32, wps=24777, ups=0.38, wpb=65495, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=1.065, loss_scale=32, train_wall=234, gb_free=21, wall=31025
2022-03-04 19:11:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:14:05 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 12.921 | nll_loss 12.731 | ppl 6797.6 | wps 45283.3 | wpb 510.9 | bsz 1 | num_updates 11754 | best_loss 7.571
2022-03-04 19:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11754 updates
2022-03-04 19:14:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:14:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:14:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 122 @ 11754 updates, score 12.921) (writing took 2.347039154730737 seconds)
2022-03-04 19:14:07 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 19:14:07 | INFO | train | epoch 122 | loss 1.533 | nll_loss 1.209 | ppl 2.31 | wps 24525.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11754 | lr 0.00029168 | gnorm 1.054 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31173
2022-03-04 19:14:07 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 19:14:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:16:05 | INFO | train_inner | epoch 123:     46 / 97 loss=1.529, nll_loss=1.205, ppl=2.31, wps=24583.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.068, loss_scale=16, train_wall=236, gb_free=21, wall=31291
2022-03-04 19:18:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:18:21 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 12.945 | nll_loss 12.754 | ppl 6908.67 | wps 45311.1 | wpb 510.9 | bsz 1 | num_updates 11851 | best_loss 7.571
2022-03-04 19:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11851 updates
2022-03-04 19:18:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:18:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:18:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 123 @ 11851 updates, score 12.945) (writing took 2.3368098568171263 seconds)
2022-03-04 19:18:23 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 19:18:23 | INFO | train | epoch 123 | loss 1.523 | nll_loss 1.199 | ppl 2.3 | wps 24824.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11851 | lr 0.000290484 | gnorm 1.067 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 31429
2022-03-04 19:18:23 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 19:18:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:18:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:20:31 | INFO | train_inner | epoch 124:     50 / 97 loss=1.517, nll_loss=1.193, ppl=2.29, wps=24612.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.06, loss_scale=16, train_wall=236, gb_free=21, wall=31557
2022-03-04 19:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:22:36 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 12.98 | nll_loss 12.791 | ppl 7087.78 | wps 44678.3 | wpb 510.9 | bsz 1 | num_updates 11947 | best_loss 7.571
2022-03-04 19:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11947 updates
2022-03-04 19:22:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:22:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:22:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 124 @ 11947 updates, score 12.98) (writing took 2.3573463391512632 seconds)
2022-03-04 19:22:39 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 19:22:39 | INFO | train | epoch 124 | loss 1.513 | nll_loss 1.188 | ppl 2.28 | wps 24569.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 11947 | lr 0.000289315 | gnorm 1.063 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31685
2022-03-04 19:22:39 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 19:22:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:24:55 | INFO | train_inner | epoch 125:     53 / 97 loss=1.508, nll_loss=1.183, ppl=2.27, wps=24851.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.058, loss_scale=32, train_wall=234, gb_free=21, wall=31821
2022-03-04 19:25:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:26:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:26:52 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 13.034 | nll_loss 12.846 | ppl 7360.09 | wps 44254.1 | wpb 510.9 | bsz 1 | num_updates 12043 | best_loss 7.571
2022-03-04 19:26:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12043 updates
2022-03-04 19:26:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:26:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:26:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 125 @ 12043 updates, score 13.034) (writing took 2.2872772300615907 seconds)
2022-03-04 19:26:55 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 19:26:55 | INFO | train | epoch 125 | loss 1.504 | nll_loss 1.179 | ppl 2.26 | wps 24571.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 12043 | lr 0.000288159 | gnorm 1.061 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31941
2022-03-04 19:26:55 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 19:26:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:29:21 | INFO | train_inner | epoch 126:     57 / 97 loss=1.499, nll_loss=1.174, ppl=2.26, wps=24603, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=1.065, loss_scale=16, train_wall=236, gb_free=21, wall=32087
2022-03-04 19:31:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:31:08 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 13.023 | nll_loss 12.832 | ppl 7293.83 | wps 44489.7 | wpb 510.9 | bsz 1 | num_updates 12140 | best_loss 7.571
2022-03-04 19:31:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12140 updates
2022-03-04 19:31:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:31:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 126 @ 12140 updates, score 13.023) (writing took 2.3359635742381215 seconds)
2022-03-04 19:31:11 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 19:31:11 | INFO | train | epoch 126 | loss 1.495 | nll_loss 1.169 | ppl 2.25 | wps 24814.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12140 | lr 0.000287006 | gnorm 1.056 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 32197
2022-03-04 19:31:11 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 19:31:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:31:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:33:47 | INFO | train_inner | epoch 127:     61 / 97 loss=1.49, nll_loss=1.164, ppl=2.24, wps=24568.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12200, lr=0.000286299, gnorm=1.062, loss_scale=16, train_wall=236, gb_free=21, wall=32354
2022-03-04 19:35:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:35:25 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 13.062 | nll_loss 12.872 | ppl 7498.11 | wps 44362.6 | wpb 510.9 | bsz 1 | num_updates 12236 | best_loss 7.571
2022-03-04 19:35:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12236 updates
2022-03-04 19:35:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:35:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:35:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 127 @ 12236 updates, score 13.062) (writing took 2.4295128183439374 seconds)
2022-03-04 19:35:27 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 19:35:27 | INFO | train | epoch 127 | loss 1.484 | nll_loss 1.158 | ppl 2.23 | wps 24525.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12236 | lr 0.000285878 | gnorm 1.063 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32453
2022-03-04 19:35:27 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 19:35:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:37:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:38:14 | INFO | train_inner | epoch 128:     65 / 97 loss=1.477, nll_loss=1.151, ppl=2.22, wps=24571.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=1.049, loss_scale=16, train_wall=236, gb_free=21, wall=32620
2022-03-04 19:39:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:39:41 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 13.08 | nll_loss 12.889 | ppl 7587.51 | wps 44422.8 | wpb 510.9 | bsz 1 | num_updates 12332 | best_loss 7.571
2022-03-04 19:39:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12332 updates
2022-03-04 19:39:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:39:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:39:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 128 @ 12332 updates, score 13.08) (writing took 2.364361925981939 seconds)
2022-03-04 19:39:43 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 19:39:43 | INFO | train | epoch 128 | loss 1.473 | nll_loss 1.146 | ppl 2.21 | wps 24512.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12332 | lr 0.000284763 | gnorm 1.044 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32710
2022-03-04 19:39:43 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 19:39:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:42:38 | INFO | train_inner | epoch 129:     68 / 97 loss=1.471, nll_loss=1.145, ppl=2.21, wps=24776, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=1.057, loss_scale=16, train_wall=234, gb_free=21, wall=32884
2022-03-04 19:43:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:43:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:43:58 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 13.087 | nll_loss 12.896 | ppl 7620.73 | wps 44996.8 | wpb 510.9 | bsz 1 | num_updates 12428 | best_loss 7.571
2022-03-04 19:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12428 updates
2022-03-04 19:43:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:44:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:44:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 129 @ 12428 updates, score 13.087) (writing took 2.357287425547838 seconds)
2022-03-04 19:44:00 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 19:44:00 | INFO | train | epoch 129 | loss 1.466 | nll_loss 1.14 | ppl 2.2 | wps 24505.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12428 | lr 0.000283661 | gnorm 1.053 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32966
2022-03-04 19:44:00 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 19:44:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:47:05 | INFO | train_inner | epoch 130:     72 / 97 loss=1.459, nll_loss=1.132, ppl=2.19, wps=24531.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=1.048, loss_scale=16, train_wall=237, gb_free=21, wall=33151
2022-03-04 19:48:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:48:14 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 13.154 | nll_loss 12.964 | ppl 7990.37 | wps 45227.7 | wpb 510.9 | bsz 1 | num_updates 12525 | best_loss 7.571
2022-03-04 19:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12525 updates
2022-03-04 19:48:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:48:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 130 @ 12525 updates, score 13.154) (writing took 2.2870707381516695 seconds)
2022-03-04 19:48:17 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 19:48:17 | INFO | train | epoch 130 | loss 1.457 | nll_loss 1.13 | ppl 2.19 | wps 24756 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12525 | lr 0.00028256 | gnorm 1.06 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33223
2022-03-04 19:48:17 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 19:48:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:49:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:51:32 | INFO | train_inner | epoch 131:     76 / 97 loss=1.451, nll_loss=1.124, ppl=2.18, wps=24539.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=1.065, loss_scale=16, train_wall=237, gb_free=21, wall=33418
2022-03-04 19:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:52:31 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 13.185 | nll_loss 12.997 | ppl 8177.39 | wps 44570.6 | wpb 510.9 | bsz 1 | num_updates 12621 | best_loss 7.571
2022-03-04 19:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12621 updates
2022-03-04 19:52:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:52:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:52:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 131 @ 12621 updates, score 13.185) (writing took 2.35425762552768 seconds)
2022-03-04 19:52:33 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 19:52:33 | INFO | train | epoch 131 | loss 1.447 | nll_loss 1.12 | ppl 2.17 | wps 24486.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12621 | lr 0.000281484 | gnorm 1.054 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33480
2022-03-04 19:52:33 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 19:52:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:55:57 | INFO | train_inner | epoch 132:     79 / 97 loss=1.439, nll_loss=1.112, ppl=2.16, wps=24769, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=1.033, loss_scale=32, train_wall=234, gb_free=21, wall=33683
2022-03-04 19:56:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:56:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:56:48 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 13.197 | nll_loss 13.008 | ppl 8237.65 | wps 44027.7 | wpb 510.9 | bsz 1 | num_updates 12717 | best_loss 7.571
2022-03-04 19:56:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12717 updates
2022-03-04 19:56:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:56:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 132 @ 12717 updates, score 13.197) (writing took 2.3634615456685424 seconds)
2022-03-04 19:56:50 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 19:56:50 | INFO | train | epoch 132 | loss 1.438 | nll_loss 1.111 | ppl 2.16 | wps 24493.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12717 | lr 0.000280419 | gnorm 1.04 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33736
2022-03-04 19:56:50 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 19:56:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:00:23 | INFO | train_inner | epoch 133:     83 / 97 loss=1.434, nll_loss=1.106, ppl=2.15, wps=24548.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=1.05, loss_scale=16, train_wall=236, gb_free=21, wall=33950
2022-03-04 20:00:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:01:04 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 13.179 | nll_loss 12.988 | ppl 8124.98 | wps 44827.8 | wpb 510.9 | bsz 1 | num_updates 12814 | best_loss 7.571
2022-03-04 20:01:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12814 updates
2022-03-04 20:01:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:01:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 133 @ 12814 updates, score 13.179) (writing took 2.3677736213430762 seconds)
2022-03-04 20:01:06 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 20:01:07 | INFO | train | epoch 133 | loss 1.43 | nll_loss 1.103 | ppl 2.15 | wps 24774.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12814 | lr 0.000279356 | gnorm 1.047 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33993
2022-03-04 20:01:07 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 20:01:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:02:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:04:50 | INFO | train_inner | epoch 134:     87 / 97 loss=1.423, nll_loss=1.095, ppl=2.14, wps=24568.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=1.046, loss_scale=16, train_wall=236, gb_free=21, wall=34216
2022-03-04 20:05:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:05:20 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 13.206 | nll_loss 13.017 | ppl 8291.25 | wps 45454.2 | wpb 510.9 | bsz 1 | num_updates 12910 | best_loss 7.571
2022-03-04 20:05:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12910 updates
2022-03-04 20:05:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:05:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 134 @ 12910 updates, score 13.206) (writing took 2.408078265376389 seconds)
2022-03-04 20:05:23 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 20:05:23 | INFO | train | epoch 134 | loss 1.422 | nll_loss 1.094 | ppl 2.13 | wps 24535.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12910 | lr 0.000278315 | gnorm 1.042 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34249
2022-03-04 20:05:23 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 20:05:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:08:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:09:16 | INFO | train_inner | epoch 135:     91 / 97 loss=1.418, nll_loss=1.09, ppl=2.13, wps=24605.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=1.054, loss_scale=16, train_wall=236, gb_free=21, wall=34482
2022-03-04 20:09:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:09:36 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 13.29 | nll_loss 13.103 | ppl 8800.74 | wps 45314 | wpb 510.9 | bsz 1 | num_updates 13006 | best_loss 7.571
2022-03-04 20:09:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13006 updates
2022-03-04 20:09:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:09:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 135 @ 13006 updates, score 13.29) (writing took 2.3113276241347194 seconds)
2022-03-04 20:09:39 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 20:09:39 | INFO | train | epoch 135 | loss 1.413 | nll_loss 1.085 | ppl 2.12 | wps 24571 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13006 | lr 0.000277286 | gnorm 1.053 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34505
2022-03-04 20:09:39 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 20:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:13:40 | INFO | train_inner | epoch 136:     94 / 97 loss=1.408, nll_loss=1.079, ppl=2.11, wps=24859.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13100, lr=0.000276289, gnorm=1.052, loss_scale=16, train_wall=234, gb_free=21, wall=34746
2022-03-04 20:13:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:13:52 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 13.252 | nll_loss 13.064 | ppl 8563.02 | wps 45291.8 | wpb 510.9 | bsz 1 | num_updates 13103 | best_loss 7.571
2022-03-04 20:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13103 updates
2022-03-04 20:13:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:13:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:13:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 136 @ 13103 updates, score 13.252) (writing took 2.375681729055941 seconds)
2022-03-04 20:13:54 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 20:13:54 | INFO | train | epoch 136 | loss 1.406 | nll_loss 1.078 | ppl 2.11 | wps 24834.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13103 | lr 0.000276258 | gnorm 1.053 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34761
2022-03-04 20:13:54 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 20:13:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:14:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:18:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:18:08 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 13.324 | nll_loss 13.137 | ppl 9007.25 | wps 44872.9 | wpb 510.9 | bsz 1 | num_updates 13199 | best_loss 7.571
2022-03-04 20:18:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13199 updates
2022-03-04 20:18:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:18:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:18:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 137 @ 13199 updates, score 13.324) (writing took 2.305939552374184 seconds)
2022-03-04 20:18:10 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 20:18:10 | INFO | train | epoch 137 | loss 1.399 | nll_loss 1.07 | ppl 2.1 | wps 24585 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13199 | lr 0.000275251 | gnorm 1.05 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35016
2022-03-04 20:18:10 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 20:18:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:18:13 | INFO | train_inner | epoch 138:      1 / 97 loss=1.4, nll_loss=1.071, ppl=2.1, wps=23957.1, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=13200, lr=0.000275241, gnorm=1.05, loss_scale=16, train_wall=236, gb_free=21, wall=35019
2022-03-04 20:20:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:22:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:22:24 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 13.301 | nll_loss 13.113 | ppl 8857.87 | wps 44476.7 | wpb 510.9 | bsz 1 | num_updates 13295 | best_loss 7.571
2022-03-04 20:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13295 updates
2022-03-04 20:22:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:22:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:22:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 138 @ 13295 updates, score 13.301) (writing took 2.354998835362494 seconds)
2022-03-04 20:22:26 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 20:22:26 | INFO | train | epoch 138 | loss 1.389 | nll_loss 1.06 | ppl 2.08 | wps 24583.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13295 | lr 0.000274256 | gnorm 1.043 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35272
2022-03-04 20:22:26 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 20:22:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:22:39 | INFO | train_inner | epoch 139:      5 / 97 loss=1.387, nll_loss=1.057, ppl=2.08, wps=24624.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13300, lr=0.000274204, gnorm=1.043, loss_scale=16, train_wall=236, gb_free=21, wall=35285
2022-03-04 20:25:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:26:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:26:39 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 13.344 | nll_loss 13.155 | ppl 9121.17 | wps 44808.1 | wpb 510.9 | bsz 1 | num_updates 13391 | best_loss 7.571
2022-03-04 20:26:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13391 updates
2022-03-04 20:26:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:26:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:26:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 139 @ 13391 updates, score 13.344) (writing took 2.345315490849316 seconds)
2022-03-04 20:26:42 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 20:26:42 | INFO | train | epoch 139 | loss 1.381 | nll_loss 1.052 | ppl 2.07 | wps 24589.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13391 | lr 0.000273271 | gnorm 1.037 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35528
2022-03-04 20:26:42 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 20:26:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:27:05 | INFO | train_inner | epoch 140:      9 / 97 loss=1.379, nll_loss=1.05, ppl=2.07, wps=24623.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13400, lr=0.000273179, gnorm=1.035, loss_scale=16, train_wall=236, gb_free=21, wall=35551
2022-03-04 20:30:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:30:55 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 13.351 | nll_loss 13.164 | ppl 9175.65 | wps 45662.9 | wpb 510.9 | bsz 1 | num_updates 13488 | best_loss 7.571
2022-03-04 20:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13488 updates
2022-03-04 20:30:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:30:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:30:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 140 @ 13488 updates, score 13.351) (writing took 2.3888584822416306 seconds)
2022-03-04 20:30:57 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 20:30:57 | INFO | train | epoch 140 | loss 1.374 | nll_loss 1.044 | ppl 2.06 | wps 24842.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13488 | lr 0.000272287 | gnorm 1.015 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35783
2022-03-04 20:30:57 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 20:30:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:31:28 | INFO | train_inner | epoch 141:     12 / 97 loss=1.37, nll_loss=1.04, ppl=2.06, wps=24861.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1.013, loss_scale=16, train_wall=234, gb_free=21, wall=35814
2022-03-04 20:32:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:35:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:35:11 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 13.412 | nll_loss 13.224 | ppl 9566.77 | wps 45093.4 | wpb 510.9 | bsz 1 | num_updates 13584 | best_loss 7.571
2022-03-04 20:35:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13584 updates
2022-03-04 20:35:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:35:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:35:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 141 @ 13584 updates, score 13.412) (writing took 2.2899358924478292 seconds)
2022-03-04 20:35:13 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 20:35:13 | INFO | train | epoch 141 | loss 1.369 | nll_loss 1.039 | ppl 2.05 | wps 24568.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13584 | lr 0.000271323 | gnorm 1.042 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36039
2022-03-04 20:35:13 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 20:35:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:35:54 | INFO | train_inner | epoch 142:     16 / 97 loss=1.367, nll_loss=1.037, ppl=2.05, wps=24612.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=1.041, loss_scale=16, train_wall=236, gb_free=21, wall=36080
2022-03-04 20:38:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:39:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:39:27 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 13.45 | nll_loss 13.263 | ppl 9830.65 | wps 44845 | wpb 510.9 | bsz 1 | num_updates 13680 | best_loss 7.571
2022-03-04 20:39:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13680 updates
2022-03-04 20:39:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:39:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:39:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 142 @ 13680 updates, score 13.45) (writing took 2.363468086346984 seconds)
2022-03-04 20:39:29 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 20:39:29 | INFO | train | epoch 142 | loss 1.361 | nll_loss 1.031 | ppl 2.04 | wps 24579.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13680 | lr 0.000270369 | gnorm 1.027 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36295
2022-03-04 20:39:29 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 20:39:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:40:20 | INFO | train_inner | epoch 143:     20 / 97 loss=1.358, nll_loss=1.027, ppl=2.04, wps=24613.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1.026, loss_scale=16, train_wall=236, gb_free=21, wall=36347
2022-03-04 20:43:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:43:43 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 13.47 | nll_loss 13.284 | ppl 9971.8 | wps 44999.9 | wpb 510.9 | bsz 1 | num_updates 13777 | best_loss 7.571
2022-03-04 20:43:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13777 updates
2022-03-04 20:43:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:43:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 143 @ 13777 updates, score 13.47) (writing took 3.9455212019383907 seconds)
2022-03-04 20:43:46 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 20:43:46 | INFO | train | epoch 143 | loss 1.354 | nll_loss 1.023 | ppl 2.03 | wps 24674.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13777 | lr 0.000269416 | gnorm 1.021 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36553
2022-03-04 20:43:46 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 20:43:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:44:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:44:48 | INFO | train_inner | epoch 144:     24 / 97 loss=1.352, nll_loss=1.021, ppl=2.03, wps=24470.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=1.018, loss_scale=16, train_wall=236, gb_free=21, wall=36614
2022-03-04 20:47:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:48:00 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 13.454 | nll_loss 13.266 | ppl 9847.77 | wps 44847.5 | wpb 510.9 | bsz 1 | num_updates 13873 | best_loss 7.571
2022-03-04 20:48:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13873 updates
2022-03-04 20:48:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:48:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:48:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 144 @ 13873 updates, score 13.454) (writing took 2.4535829881206155 seconds)
2022-03-04 20:48:02 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 20:48:02 | INFO | train | epoch 144 | loss 1.345 | nll_loss 1.014 | ppl 2.02 | wps 24571.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13873 | lr 0.000268482 | gnorm 1.012 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36808
2022-03-04 20:48:02 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 20:48:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:49:12 | INFO | train_inner | epoch 145:     27 / 97 loss=1.343, nll_loss=1.012, ppl=2.02, wps=24842.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=1.011, loss_scale=16, train_wall=234, gb_free=21, wall=36878
2022-03-04 20:50:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:52:16 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 13.516 | nll_loss 13.331 | ppl 10301.4 | wps 44564.5 | wpb 510.9 | bsz 1 | num_updates 13969 | best_loss 7.571
2022-03-04 20:52:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13969 updates
2022-03-04 20:52:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:52:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:52:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 145 @ 13969 updates, score 13.516) (writing took 2.3702215580269694 seconds)
2022-03-04 20:52:18 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 20:52:18 | INFO | train | epoch 145 | loss 1.338 | nll_loss 1.007 | ppl 2.01 | wps 24549.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13969 | lr 0.000267558 | gnorm 1.026 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37065
2022-03-04 20:52:18 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 20:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:53:38 | INFO | train_inner | epoch 146:     31 / 97 loss=1.335, nll_loss=1.004, ppl=2.01, wps=24586.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=1.031, loss_scale=16, train_wall=236, gb_free=21, wall=37144
2022-03-04 20:56:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:56:32 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 13.524 | nll_loss 13.338 | ppl 10355 | wps 44598.3 | wpb 510.9 | bsz 1 | num_updates 14066 | best_loss 7.571
2022-03-04 20:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14066 updates
2022-03-04 20:56:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:56:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:56:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 146 @ 14066 updates, score 13.524) (writing took 2.50158660300076 seconds)
2022-03-04 20:56:35 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 20:56:35 | INFO | train | epoch 146 | loss 1.333 | nll_loss 1.002 | ppl 2 | wps 24790.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14066 | lr 0.000266633 | gnorm 1.023 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 37321
2022-03-04 20:56:35 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 20:56:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:57:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:58:05 | INFO | train_inner | epoch 147:     35 / 97 loss=1.328, nll_loss=0.996, ppl=1.99, wps=24571.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=1.013, loss_scale=16, train_wall=236, gb_free=21, wall=37411
2022-03-04 21:00:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:00:48 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 13.501 | nll_loss 13.313 | ppl 10178.8 | wps 44669.1 | wpb 510.9 | bsz 1 | num_updates 14162 | best_loss 7.571
2022-03-04 21:00:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14162 updates
2022-03-04 21:00:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:00:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:00:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 147 @ 14162 updates, score 13.501) (writing took 2.438620294444263 seconds)
2022-03-04 21:00:51 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 21:00:51 | INFO | train | epoch 147 | loss 1.325 | nll_loss 0.994 | ppl 1.99 | wps 24557.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14162 | lr 0.000265728 | gnorm 1.014 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37577
2022-03-04 21:00:51 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 21:00:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:02:28 | INFO | train_inner | epoch 148:     38 / 97 loss=1.323, nll_loss=0.992, ppl=1.99, wps=24841.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=1.02, loss_scale=16, train_wall=234, gb_free=21, wall=37674
2022-03-04 21:03:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:05:04 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 13.513 | nll_loss 13.328 | ppl 10279.9 | wps 44776.8 | wpb 510.9 | bsz 1 | num_updates 14258 | best_loss 7.571
2022-03-04 21:05:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14258 updates
2022-03-04 21:05:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:05:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:05:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 148 @ 14258 updates, score 13.513) (writing took 2.451342625543475 seconds)
2022-03-04 21:05:07 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 21:05:07 | INFO | train | epoch 148 | loss 1.321 | nll_loss 0.989 | ppl 1.99 | wps 24569.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 14258 | lr 0.000264832 | gnorm 1.03 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37833
2022-03-04 21:05:07 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 21:05:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:06:54 | INFO | train_inner | epoch 149:     42 / 97 loss=1.317, nll_loss=0.985, ppl=1.98, wps=24612.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=1.027, loss_scale=16, train_wall=236, gb_free=21, wall=37940
2022-03-04 21:09:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:09:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:09:20 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 13.559 | nll_loss 13.372 | ppl 10604.8 | wps 45264.8 | wpb 510.9 | bsz 1 | num_updates 14354 | best_loss 7.571
2022-03-04 21:09:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14354 updates
2022-03-04 21:09:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:09:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:09:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 149 @ 14354 updates, score 13.559) (writing took 2.3335263039916754 seconds)
2022-03-04 21:09:22 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-04 21:09:22 | INFO | train | epoch 149 | loss 1.311 | nll_loss 0.979 | ppl 1.97 | wps 24575.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 14354 | lr 0.000263945 | gnorm 1.018 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38089
2022-03-04 21:09:22 | INFO | fairseq.trainer | begin training epoch 150
2022-03-04 21:09:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:11:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:11:23 | INFO | train_inner | epoch 150:     47 / 97 loss=1.309, nll_loss=0.977, ppl=1.97, wps=24354.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=1.028, loss_scale=8, train_wall=239, gb_free=21, wall=38209
2022-03-04 21:13:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:13:37 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 13.626 | nll_loss 13.44 | ppl 11114.7 | wps 44347.7 | wpb 510.9 | bsz 1 | num_updates 14450 | best_loss 7.571
2022-03-04 21:13:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14450 updates
2022-03-04 21:13:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:13:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:13:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 150 @ 14450 updates, score 13.626) (writing took 2.380065103061497 seconds)
2022-03-04 21:13:39 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-04 21:13:39 | INFO | train | epoch 150 | loss 1.307 | nll_loss 0.975 | ppl 1.97 | wps 24502.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14450 | lr 0.000263067 | gnorm 1.045 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 38345
2022-03-04 21:13:39 | INFO | fairseq.trainer | begin training epoch 151
2022-03-04 21:13:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:15:48 | INFO | train_inner | epoch 151:     50 / 97 loss=1.303, nll_loss=0.971, ppl=1.96, wps=24753.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=1.032, loss_scale=8, train_wall=234, gb_free=21, wall=38474
2022-03-04 21:17:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:17:53 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 13.591 | nll_loss 13.406 | ppl 10857.8 | wps 44315 | wpb 510.9 | bsz 1 | num_updates 14547 | best_loss 7.571
2022-03-04 21:17:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14547 updates
2022-03-04 21:17:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:17:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:17:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 151 @ 14547 updates, score 13.591) (writing took 2.387145234271884 seconds)
2022-03-04 21:17:56 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-04 21:17:56 | INFO | train | epoch 151 | loss 1.3 | nll_loss 0.968 | ppl 1.96 | wps 24742.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14547 | lr 0.000262188 | gnorm 1.016 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38602
2022-03-04 21:17:56 | INFO | fairseq.trainer | begin training epoch 152
2022-03-04 21:17:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:20:12 | INFO | train_inner | epoch 152:     53 / 97 loss=1.297, nll_loss=0.965, ppl=1.95, wps=24795.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14600, lr=0.000261712, gnorm=1.019, loss_scale=16, train_wall=234, gb_free=21, wall=38738
2022-03-04 21:22:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:22:09 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 13.623 | nll_loss 13.439 | ppl 11106.3 | wps 44433.7 | wpb 510.9 | bsz 1 | num_updates 14644 | best_loss 7.571
2022-03-04 21:22:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14644 updates
2022-03-04 21:22:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:22:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:22:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 152 @ 14644 updates, score 13.623) (writing took 2.4493564954027534 seconds)
2022-03-04 21:22:12 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-04 21:22:12 | INFO | train | epoch 152 | loss 1.293 | nll_loss 0.961 | ppl 1.95 | wps 24799.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14644 | lr 0.000261318 | gnorm 1.017 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38858
2022-03-04 21:22:12 | INFO | fairseq.trainer | begin training epoch 153
2022-03-04 21:22:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:22:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:24:38 | INFO | train_inner | epoch 153:     57 / 97 loss=1.291, nll_loss=0.959, ppl=1.94, wps=24608.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=1.013, loss_scale=16, train_wall=236, gb_free=21, wall=39004
2022-03-04 21:26:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:26:25 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 13.641 | nll_loss 13.453 | ppl 11215 | wps 45116.5 | wpb 510.9 | bsz 1 | num_updates 14740 | best_loss 7.571
2022-03-04 21:26:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14740 updates
2022-03-04 21:26:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:26:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 153 @ 14740 updates, score 13.641) (writing took 2.3673088401556015 seconds)
2022-03-04 21:26:28 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-04 21:26:28 | INFO | train | epoch 153 | loss 1.287 | nll_loss 0.954 | ppl 1.94 | wps 24571.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 14740 | lr 0.000260466 | gnorm 1.007 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39114
2022-03-04 21:26:28 | INFO | fairseq.trainer | begin training epoch 154
2022-03-04 21:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:28:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:29:04 | INFO | train_inner | epoch 154:     61 / 97 loss=1.284, nll_loss=0.951, ppl=1.93, wps=24596, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=1.006, loss_scale=16, train_wall=236, gb_free=21, wall=39270
2022-03-04 21:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:30:42 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 13.619 | nll_loss 13.433 | ppl 11061.5 | wps 44848.8 | wpb 510.9 | bsz 1 | num_updates 14836 | best_loss 7.571
2022-03-04 21:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14836 updates
2022-03-04 21:30:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:30:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:30:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 154 @ 14836 updates, score 13.619) (writing took 2.3510617977008224 seconds)
2022-03-04 21:30:44 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-04 21:30:44 | INFO | train | epoch 154 | loss 1.281 | nll_loss 0.949 | ppl 1.93 | wps 24537.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14836 | lr 0.000259622 | gnorm 0.997 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39370
2022-03-04 21:30:44 | INFO | fairseq.trainer | begin training epoch 155
2022-03-04 21:30:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:33:29 | INFO | train_inner | epoch 155:     64 / 97 loss=1.28, nll_loss=0.947, ppl=1.93, wps=24748.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=1.005, loss_scale=16, train_wall=235, gb_free=21, wall=39535
2022-03-04 21:34:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:34:59 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 13.664 | nll_loss 13.478 | ppl 11409.4 | wps 44556.4 | wpb 510.9 | bsz 1 | num_updates 14932 | best_loss 7.571
2022-03-04 21:34:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14932 updates
2022-03-04 21:34:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:35:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 155 @ 14932 updates, score 13.664) (writing took 2.361185571178794 seconds)
2022-03-04 21:35:01 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-04 21:35:01 | INFO | train | epoch 155 | loss 1.277 | nll_loss 0.944 | ppl 1.92 | wps 24469.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14932 | lr 0.000258786 | gnorm 1.013 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 39627
2022-03-04 21:35:01 | INFO | fairseq.trainer | begin training epoch 156
2022-03-04 21:35:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:37:56 | INFO | train_inner | epoch 156:     68 / 97 loss=1.272, nll_loss=0.938, ppl=1.92, wps=24560.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=1.015, loss_scale=16, train_wall=236, gb_free=21, wall=39802
2022-03-04 21:39:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:39:15 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 13.663 | nll_loss 13.477 | ppl 11405.7 | wps 44600.7 | wpb 510.9 | bsz 1 | num_updates 15029 | best_loss 7.571
2022-03-04 21:39:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15029 updates
2022-03-04 21:39:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:39:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:39:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 156 @ 15029 updates, score 13.663) (writing took 2.4070233227685094 seconds)
2022-03-04 21:39:17 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-04 21:39:17 | INFO | train | epoch 156 | loss 1.27 | nll_loss 0.937 | ppl 1.91 | wps 24796 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15029 | lr 0.00025795 | gnorm 1.016 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39883
2022-03-04 21:39:17 | INFO | fairseq.trainer | begin training epoch 157
2022-03-04 21:39:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:40:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:42:22 | INFO | train_inner | epoch 157:     72 / 97 loss=1.267, nll_loss=0.934, ppl=1.91, wps=24608.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=1.007, loss_scale=16, train_wall=236, gb_free=21, wall=40068
2022-03-04 21:43:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:43:31 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 13.692 | nll_loss 13.505 | ppl 11625.1 | wps 45231.6 | wpb 510.9 | bsz 1 | num_updates 15125 | best_loss 7.571
2022-03-04 21:43:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15125 updates
2022-03-04 21:43:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:43:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:43:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 157 @ 15125 updates, score 13.692) (writing took 2.328564580529928 seconds)
2022-03-04 21:43:33 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-04 21:43:33 | INFO | train | epoch 157 | loss 1.264 | nll_loss 0.931 | ppl 1.91 | wps 24586.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15125 | lr 0.00025713 | gnorm 1.003 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40139
2022-03-04 21:43:33 | INFO | fairseq.trainer | begin training epoch 158
2022-03-04 21:43:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:45:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:46:48 | INFO | train_inner | epoch 158:     76 / 97 loss=1.261, nll_loss=0.927, ppl=1.9, wps=24595, ups=0.38, wpb=65495, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=1.006, loss_scale=16, train_wall=236, gb_free=21, wall=40334
2022-03-04 21:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:47:47 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 13.716 | nll_loss 13.533 | ppl 11852.4 | wps 44766.7 | wpb 510.9 | bsz 1 | num_updates 15221 | best_loss 7.571
2022-03-04 21:47:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15221 updates
2022-03-04 21:47:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:47:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:47:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 158 @ 15221 updates, score 13.716) (writing took 2.4510345039889216 seconds)
2022-03-04 21:47:49 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-04 21:47:49 | INFO | train | epoch 158 | loss 1.259 | nll_loss 0.926 | ppl 1.9 | wps 24509.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15221 | lr 0.000256318 | gnorm 1.006 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40396
2022-03-04 21:47:49 | INFO | fairseq.trainer | begin training epoch 159
2022-03-04 21:47:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:51:13 | INFO | train_inner | epoch 159:     79 / 97 loss=1.257, nll_loss=0.924, ppl=1.9, wps=24761.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=1.002, loss_scale=16, train_wall=234, gb_free=21, wall=40599
2022-03-04 21:51:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:52:04 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 13.711 | nll_loss 13.527 | ppl 11804 | wps 44741.6 | wpb 510.9 | bsz 1 | num_updates 15318 | best_loss 7.571
2022-03-04 21:52:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15318 updates
2022-03-04 21:52:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:52:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 159 @ 15318 updates, score 13.711) (writing took 2.3275808254256845 seconds)
2022-03-04 21:52:06 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-04 21:52:06 | INFO | train | epoch 159 | loss 1.253 | nll_loss 0.92 | ppl 1.89 | wps 24768.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15318 | lr 0.000255505 | gnorm 0.995 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 40652
2022-03-04 21:52:06 | INFO | fairseq.trainer | begin training epoch 160
2022-03-04 21:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:52:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:55:39 | INFO | train_inner | epoch 160:     83 / 97 loss=1.249, nll_loss=0.915, ppl=1.89, wps=24601.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=0.994, loss_scale=16, train_wall=236, gb_free=21, wall=40865
2022-03-04 21:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:56:19 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 13.803 | nll_loss 13.619 | ppl 12582.8 | wps 45020 | wpb 510.9 | bsz 1 | num_updates 15414 | best_loss 7.571
2022-03-04 21:56:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15414 updates
2022-03-04 21:56:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:56:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:56:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 160 @ 15414 updates, score 13.803) (writing took 2.4430995965376496 seconds)
2022-03-04 21:56:22 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-04 21:56:22 | INFO | train | epoch 160 | loss 1.247 | nll_loss 0.914 | ppl 1.88 | wps 24554.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15414 | lr 0.000254708 | gnorm 0.999 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40908
2022-03-04 21:56:22 | INFO | fairseq.trainer | begin training epoch 161
2022-03-04 21:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:58:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:00:05 | INFO | train_inner | epoch 161:     87 / 97 loss=1.243, nll_loss=0.909, ppl=1.88, wps=24592.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=0.996, loss_scale=16, train_wall=236, gb_free=21, wall=41131
2022-03-04 22:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:00:36 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 13.759 | nll_loss 13.575 | ppl 12206.3 | wps 44985.6 | wpb 510.9 | bsz 1 | num_updates 15510 | best_loss 7.571
2022-03-04 22:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15510 updates
2022-03-04 22:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 161 @ 15510 updates, score 13.759) (writing took 2.4009996447712183 seconds)
2022-03-04 22:00:38 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-04 22:00:38 | INFO | train | epoch 161 | loss 1.243 | nll_loss 0.909 | ppl 1.88 | wps 24561.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15510 | lr 0.000253918 | gnorm 0.991 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41164
2022-03-04 22:00:38 | INFO | fairseq.trainer | begin training epoch 162
2022-03-04 22:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:03:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:04:31 | INFO | train_inner | epoch 162:     91 / 97 loss=1.241, nll_loss=0.907, ppl=1.87, wps=24610.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=0.995, loss_scale=16, train_wall=236, gb_free=21, wall=41397
2022-03-04 22:04:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:51 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 13.798 | nll_loss 13.614 | ppl 12541.1 | wps 44526.8 | wpb 510.9 | bsz 1 | num_updates 15606 | best_loss 7.571
2022-03-04 22:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15606 updates
2022-03-04 22:04:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 162 @ 15606 updates, score 13.798) (writing took 2.399743885733187 seconds)
2022-03-04 22:04:54 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-04 22:04:54 | INFO | train | epoch 162 | loss 1.238 | nll_loss 0.904 | ppl 1.87 | wps 24575.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15606 | lr 0.000253136 | gnorm 0.994 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41420
2022-03-04 22:04:54 | INFO | fairseq.trainer | begin training epoch 163
2022-03-04 22:04:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:06:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:08:57 | INFO | train_inner | epoch 163:     95 / 97 loss=1.235, nll_loss=0.9, ppl=1.87, wps=24611.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15700, lr=0.000252377, gnorm=1.003, loss_scale=8, train_wall=236, gb_free=21, wall=41663
2022-03-04 22:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:09:07 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 13.78 | nll_loss 13.598 | ppl 12401.8 | wps 45319.2 | wpb 510.9 | bsz 1 | num_updates 15702 | best_loss 7.571
2022-03-04 22:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15702 updates
2022-03-04 22:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 163 @ 15702 updates, score 13.78) (writing took 2.1371005177497864 seconds)
2022-03-04 22:09:09 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-04 22:09:09 | INFO | train | epoch 163 | loss 1.232 | nll_loss 0.898 | ppl 1.86 | wps 24599.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15702 | lr 0.000252361 | gnorm 1.003 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 41675
2022-03-04 22:09:09 | INFO | fairseq.trainer | begin training epoch 164
2022-03-04 22:09:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:13:23 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 13.774 | nll_loss 13.59 | ppl 12334.1 | wps 45301.3 | wpb 510.9 | bsz 1 | num_updates 15799 | best_loss 7.571
2022-03-04 22:13:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15799 updates
2022-03-04 22:13:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:13:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:13:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 164 @ 15799 updates, score 13.774) (writing took 2.3730406211689115 seconds)
2022-03-04 22:13:26 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-04 22:13:26 | INFO | train | epoch 164 | loss 1.228 | nll_loss 0.894 | ppl 1.86 | wps 24789 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15799 | lr 0.000251585 | gnorm 0.996 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41932
2022-03-04 22:13:26 | INFO | fairseq.trainer | begin training epoch 165
2022-03-04 22:13:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:13:28 | INFO | train_inner | epoch 165:      1 / 97 loss=1.228, nll_loss=0.894, ppl=1.86, wps=24153, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=15800, lr=0.000251577, gnorm=0.996, loss_scale=16, train_wall=234, gb_free=21, wall=41934
2022-03-04 22:17:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:17:39 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 13.797 | nll_loss 13.615 | ppl 12549.5 | wps 44715.4 | wpb 510.9 | bsz 1 | num_updates 15896 | best_loss 7.571
2022-03-04 22:17:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15896 updates
2022-03-04 22:17:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:17:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 165 @ 15896 updates, score 13.797) (writing took 2.1950110848993063 seconds)
2022-03-04 22:17:42 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-04 22:17:42 | INFO | train | epoch 165 | loss 1.221 | nll_loss 0.886 | ppl 1.85 | wps 24816.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15896 | lr 0.000250816 | gnorm 0.983 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42188
2022-03-04 22:17:42 | INFO | fairseq.trainer | begin training epoch 166
2022-03-04 22:17:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:17:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:17:55 | INFO | train_inner | epoch 166:      5 / 97 loss=1.22, nll_loss=0.886, ppl=1.85, wps=24602.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15900, lr=0.000250785, gnorm=0.985, loss_scale=16, train_wall=236, gb_free=21, wall=42201
2022-03-04 22:21:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:21:56 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 13.81 | nll_loss 13.627 | ppl 12654.6 | wps 44825.9 | wpb 510.9 | bsz 1 | num_updates 15992 | best_loss 7.571
2022-03-04 22:21:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15992 updates
2022-03-04 22:21:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:21:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:21:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 166 @ 15992 updates, score 13.81) (writing took 2.2972373897209764 seconds)
2022-03-04 22:21:58 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-04 22:21:58 | INFO | train | epoch 166 | loss 1.218 | nll_loss 0.883 | ppl 1.84 | wps 24519.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15992 | lr 0.000250063 | gnorm 0.998 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42444
2022-03-04 22:21:58 | INFO | fairseq.trainer | begin training epoch 167
2022-03-04 22:21:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:22:19 | INFO | train_inner | epoch 167:      8 / 97 loss=1.216, nll_loss=0.881, ppl=1.84, wps=24793.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16000, lr=0.00025, gnorm=0.994, loss_scale=16, train_wall=234, gb_free=21, wall=42465
2022-03-04 22:23:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:26:12 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 13.81 | nll_loss 13.627 | ppl 12647.1 | wps 44909.2 | wpb 510.9 | bsz 1 | num_updates 16088 | best_loss 7.571
2022-03-04 22:26:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16088 updates
2022-03-04 22:26:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:26:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:26:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 167 @ 16088 updates, score 13.81) (writing took 2.2766019254922867 seconds)
2022-03-04 22:26:14 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-04 22:26:14 | INFO | train | epoch 167 | loss 1.212 | nll_loss 0.877 | ppl 1.84 | wps 24548.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16088 | lr 0.000249315 | gnorm 0.985 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42700
2022-03-04 22:26:14 | INFO | fairseq.trainer | begin training epoch 168
2022-03-04 22:26:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:26:45 | INFO | train_inner | epoch 168:     12 / 97 loss=1.209, nll_loss=0.874, ppl=1.83, wps=24589.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=0.983, loss_scale=16, train_wall=236, gb_free=21, wall=42731
2022-03-04 22:29:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:30:28 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 13.819 | nll_loss 13.634 | ppl 12715.9 | wps 45411.4 | wpb 510.9 | bsz 1 | num_updates 16184 | best_loss 7.571
2022-03-04 22:30:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16184 updates
2022-03-04 22:30:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:30:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 168 @ 16184 updates, score 13.819) (writing took 2.440285594202578 seconds)
2022-03-04 22:30:30 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-04 22:30:30 | INFO | train | epoch 168 | loss 1.207 | nll_loss 0.872 | ppl 1.83 | wps 24552.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16184 | lr 0.000248575 | gnorm 0.978 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42956
2022-03-04 22:30:30 | INFO | fairseq.trainer | begin training epoch 169
2022-03-04 22:30:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:31:11 | INFO | train_inner | epoch 169:     16 / 97 loss=1.207, nll_loss=0.872, ppl=1.83, wps=24590.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=0.979, loss_scale=16, train_wall=236, gb_free=21, wall=42997
2022-03-04 22:34:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:34:44 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 13.857 | nll_loss 13.675 | ppl 13082.3 | wps 44761.3 | wpb 510.9 | bsz 1 | num_updates 16281 | best_loss 7.571
2022-03-04 22:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16281 updates
2022-03-04 22:34:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:34:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:34:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 169 @ 16281 updates, score 13.857) (writing took 2.3754571080207825 seconds)
2022-03-04 22:34:47 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-04 22:34:47 | INFO | train | epoch 169 | loss 1.203 | nll_loss 0.868 | ppl 1.83 | wps 24779.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16281 | lr 0.000247833 | gnorm 0.985 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43213
2022-03-04 22:34:47 | INFO | fairseq.trainer | begin training epoch 170
2022-03-04 22:34:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:35:35 | INFO | train_inner | epoch 170:     19 / 97 loss=1.2, nll_loss=0.865, ppl=1.82, wps=24795.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=0.982, loss_scale=32, train_wall=234, gb_free=21, wall=43262
2022-03-04 22:35:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:38:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:39:01 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 13.868 | nll_loss 13.686 | ppl 13176.4 | wps 44761.3 | wpb 510.9 | bsz 1 | num_updates 16377 | best_loss 7.571
2022-03-04 22:39:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16377 updates
2022-03-04 22:39:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:39:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:39:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 170 @ 16377 updates, score 13.868) (writing took 2.512620716355741 seconds)
2022-03-04 22:39:03 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-04 22:39:03 | INFO | train | epoch 170 | loss 1.199 | nll_loss 0.864 | ppl 1.82 | wps 24505.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16377 | lr 0.000247106 | gnorm 0.983 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43469
2022-03-04 22:39:03 | INFO | fairseq.trainer | begin training epoch 171
2022-03-04 22:39:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:40:02 | INFO | train_inner | epoch 171:     23 / 97 loss=1.197, nll_loss=0.862, ppl=1.82, wps=24538.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=0.982, loss_scale=16, train_wall=237, gb_free=21, wall=43528
2022-03-04 22:41:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:43:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:43:17 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 13.853 | nll_loss 13.67 | ppl 13031.4 | wps 45026.6 | wpb 510.9 | bsz 1 | num_updates 16473 | best_loss 7.571
2022-03-04 22:43:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16473 updates
2022-03-04 22:43:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:43:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 171 @ 16473 updates, score 13.853) (writing took 2.4201720813289285 seconds)
2022-03-04 22:43:20 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-04 22:43:20 | INFO | train | epoch 171 | loss 1.195 | nll_loss 0.86 | ppl 1.82 | wps 24498.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16473 | lr 0.000246385 | gnorm 0.997 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43726
2022-03-04 22:43:20 | INFO | fairseq.trainer | begin training epoch 172
2022-03-04 22:43:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:44:29 | INFO | train_inner | epoch 172:     27 / 97 loss=1.193, nll_loss=0.858, ppl=1.81, wps=24537.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=0.998, loss_scale=16, train_wall=237, gb_free=21, wall=43795
2022-03-04 22:47:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:47:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:47:34 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 13.932 | nll_loss 13.751 | ppl 13782.6 | wps 45429.7 | wpb 510.9 | bsz 1 | num_updates 16569 | best_loss 7.571
2022-03-04 22:47:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16569 updates
2022-03-04 22:47:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:47:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 172 @ 16569 updates, score 13.932) (writing took 2.465459832921624 seconds)
2022-03-04 22:47:37 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-04 22:47:37 | INFO | train | epoch 172 | loss 1.188 | nll_loss 0.853 | ppl 1.81 | wps 24486.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16569 | lr 0.00024567 | gnorm 0.978 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43983
2022-03-04 22:47:37 | INFO | fairseq.trainer | begin training epoch 173
2022-03-04 22:47:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:48:56 | INFO | train_inner | epoch 173:     31 / 97 loss=1.187, nll_loss=0.852, ppl=1.81, wps=24522.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=0.972, loss_scale=16, train_wall=237, gb_free=21, wall=44062
2022-03-04 22:51:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:51:51 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 13.909 | nll_loss 13.729 | ppl 13577.5 | wps 45128.5 | wpb 510.9 | bsz 1 | num_updates 16666 | best_loss 7.571
2022-03-04 22:51:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16666 updates
2022-03-04 22:51:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:51:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:51:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 173 @ 16666 updates, score 13.909) (writing took 2.325128300115466 seconds)
2022-03-04 22:51:53 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-04 22:51:53 | INFO | train | epoch 173 | loss 1.184 | nll_loss 0.848 | ppl 1.8 | wps 24739.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16666 | lr 0.000244954 | gnorm 0.966 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44239
2022-03-04 22:51:53 | INFO | fairseq.trainer | begin training epoch 174
2022-03-04 22:51:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:53:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:53:23 | INFO | train_inner | epoch 174:     35 / 97 loss=1.181, nll_loss=0.846, ppl=1.8, wps=24519.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=0.973, loss_scale=16, train_wall=237, gb_free=21, wall=44330
2022-03-04 22:56:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:56:08 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 13.914 | nll_loss 13.733 | ppl 13611.5 | wps 45235.1 | wpb 510.9 | bsz 1 | num_updates 16762 | best_loss 7.571
2022-03-04 22:56:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16762 updates
2022-03-04 22:56:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:56:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 174 @ 16762 updates, score 13.914) (writing took 2.3346221800893545 seconds)
2022-03-04 22:56:10 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-04 22:56:10 | INFO | train | epoch 174 | loss 1.18 | nll_loss 0.845 | ppl 1.8 | wps 24482.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16762 | lr 0.000244251 | gnorm 0.974 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44496
2022-03-04 22:56:10 | INFO | fairseq.trainer | begin training epoch 175
2022-03-04 22:56:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:57:48 | INFO | train_inner | epoch 175:     38 / 97 loss=1.178, nll_loss=0.843, ppl=1.79, wps=24767.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=0.976, loss_scale=16, train_wall=234, gb_free=21, wall=44594
2022-03-04 22:58:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:00:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:00:24 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 13.951 | nll_loss 13.769 | ppl 13961.8 | wps 45139.2 | wpb 510.9 | bsz 1 | num_updates 16858 | best_loss 7.571
2022-03-04 23:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16858 updates
2022-03-04 23:00:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 175 @ 16858 updates, score 13.951) (writing took 2.1438567293807864 seconds)
2022-03-04 23:00:27 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-04 23:00:27 | INFO | train | epoch 175 | loss 1.175 | nll_loss 0.839 | ppl 1.79 | wps 24519.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16858 | lr 0.000243555 | gnorm 0.974 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44753
2022-03-04 23:00:27 | INFO | fairseq.trainer | begin training epoch 176
2022-03-04 23:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:02:15 | INFO | train_inner | epoch 176:     42 / 97 loss=1.171, nll_loss=0.836, ppl=1.78, wps=24556.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=16900, lr=0.000243252, gnorm=0.968, loss_scale=16, train_wall=237, gb_free=21, wall=44861
2022-03-04 23:04:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:04:41 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 13.904 | nll_loss 13.722 | ppl 13512 | wps 45254.4 | wpb 510.9 | bsz 1 | num_updates 16955 | best_loss 7.571
2022-03-04 23:04:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16955 updates
2022-03-04 23:04:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:04:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:04:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 176 @ 16955 updates, score 13.904) (writing took 2.1973665775731206 seconds)
2022-03-04 23:04:43 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-04 23:04:43 | INFO | train | epoch 176 | loss 1.172 | nll_loss 0.837 | ppl 1.79 | wps 24762.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16955 | lr 0.000242857 | gnorm 0.976 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 45009
2022-03-04 23:04:43 | INFO | fairseq.trainer | begin training epoch 177
2022-03-04 23:04:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:04:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:06:42 | INFO | train_inner | epoch 177:     46 / 97 loss=1.171, nll_loss=0.835, ppl=1.78, wps=24537.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=0.967, loss_scale=16, train_wall=237, gb_free=21, wall=45128
2022-03-04 23:08:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:08:58 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 13.961 | nll_loss 13.78 | ppl 14063.2 | wps 44389.2 | wpb 510.9 | bsz 1 | num_updates 17051 | best_loss 7.571
2022-03-04 23:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17051 updates
2022-03-04 23:08:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:09:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 177 @ 17051 updates, score 13.961) (writing took 2.1474869390949607 seconds)
2022-03-04 23:09:00 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-04 23:09:00 | INFO | train | epoch 177 | loss 1.165 | nll_loss 0.83 | ppl 1.78 | wps 24496.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17051 | lr 0.000242173 | gnorm 0.959 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 45266
2022-03-04 23:09:00 | INFO | fairseq.trainer | begin training epoch 178
2022-03-04 23:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:10:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:11:08 | INFO | train_inner | epoch 178:     50 / 97 loss=1.163, nll_loss=0.827, ppl=1.77, wps=24539.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=0.972, loss_scale=16, train_wall=237, gb_free=21, wall=45395
2022-03-04 23:13:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:13:14 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 14.004 | nll_loss 13.823 | ppl 14492.3 | wps 44896.8 | wpb 510.9 | bsz 1 | num_updates 17147 | best_loss 7.571
2022-03-04 23:13:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17147 updates
2022-03-04 23:13:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:13:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:13:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 178 @ 17147 updates, score 14.004) (writing took 2.1775407576933503 seconds)
2022-03-04 23:13:16 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-04 23:13:16 | INFO | train | epoch 178 | loss 1.163 | nll_loss 0.828 | ppl 1.77 | wps 24533.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17147 | lr 0.000241494 | gnorm 0.976 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 45522
2022-03-04 23:13:16 | INFO | fairseq.trainer | begin training epoch 179
2022-03-04 23:13:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:15:32 | INFO | train_inner | epoch 179:     53 / 97 loss=1.163, nll_loss=0.828, ppl=1.77, wps=24847.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=0.976, loss_scale=16, train_wall=234, gb_free=21, wall=45658
2022-03-04 23:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:17:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:17:29 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 14.007 | nll_loss 13.828 | ppl 14538 | wps 45271 | wpb 510.9 | bsz 1 | num_updates 17243 | best_loss 7.571
2022-03-04 23:17:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17243 updates
2022-03-04 23:17:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:17:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:17:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 179 @ 17243 updates, score 14.007) (writing took 2.108103538863361 seconds)
2022-03-04 23:17:32 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-04 23:17:32 | INFO | train | epoch 179 | loss 1.158 | nll_loss 0.822 | ppl 1.77 | wps 24596.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 17243 | lr 0.000240821 | gnorm 0.971 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 45778
2022-03-04 23:17:32 | INFO | fairseq.trainer | begin training epoch 180
2022-03-04 23:17:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:19:58 | INFO | train_inner | epoch 180:     57 / 97 loss=1.155, nll_loss=0.819, ppl=1.76, wps=24642.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=0.97, loss_scale=16, train_wall=236, gb_free=21, wall=45924
2022-03-04 23:21:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:21:45 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 13.939 | nll_loss 13.757 | ppl 13846.9 | wps 44706.5 | wpb 510.9 | bsz 1 | num_updates 17340 | best_loss 7.571
2022-03-04 23:21:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17340 updates
2022-03-04 23:21:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:21:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:21:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 180 @ 17340 updates, score 13.939) (writing took 2.188493847846985 seconds)
2022-03-04 23:21:47 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-04 23:21:47 | INFO | train | epoch 180 | loss 1.154 | nll_loss 0.818 | ppl 1.76 | wps 24844 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17340 | lr 0.000240146 | gnorm 0.969 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46033
2022-03-04 23:21:47 | INFO | fairseq.trainer | begin training epoch 181
2022-03-04 23:21:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:22:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:24:24 | INFO | train_inner | epoch 181:     61 / 97 loss=1.15, nll_loss=0.814, ppl=1.76, wps=24596.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=0.966, loss_scale=16, train_wall=236, gb_free=21, wall=46190
2022-03-04 23:25:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:26:01 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 13.98 | nll_loss 13.799 | ppl 14256.8 | wps 45208 | wpb 510.9 | bsz 1 | num_updates 17436 | best_loss 7.571
2022-03-04 23:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17436 updates
2022-03-04 23:26:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:26:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:26:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 181 @ 17436 updates, score 13.98) (writing took 2.117006284184754 seconds)
2022-03-04 23:26:03 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-04 23:26:03 | INFO | train | epoch 181 | loss 1.15 | nll_loss 0.814 | ppl 1.76 | wps 24548.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17436 | lr 0.000239484 | gnorm 0.97 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46290
2022-03-04 23:26:03 | INFO | fairseq.trainer | begin training epoch 182
2022-03-04 23:26:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:27:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:28:51 | INFO | train_inner | epoch 182:     65 / 97 loss=1.149, nll_loss=0.813, ppl=1.76, wps=24557.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=0.974, loss_scale=16, train_wall=237, gb_free=21, wall=46457
2022-03-04 23:30:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:30:18 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 13.986 | nll_loss 13.805 | ppl 14314.9 | wps 44482.8 | wpb 510.9 | bsz 1 | num_updates 17532 | best_loss 7.571
2022-03-04 23:30:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17532 updates
2022-03-04 23:30:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:30:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:30:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 182 @ 17532 updates, score 13.986) (writing took 2.154412606731057 seconds)
2022-03-04 23:30:20 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-04 23:30:20 | INFO | train | epoch 182 | loss 1.146 | nll_loss 0.81 | ppl 1.75 | wps 24496 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17532 | lr 0.000238827 | gnorm 0.975 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46546
2022-03-04 23:30:20 | INFO | fairseq.trainer | begin training epoch 183
2022-03-04 23:30:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:33:15 | INFO | train_inner | epoch 183:     68 / 97 loss=1.143, nll_loss=0.807, ppl=1.75, wps=24772.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=0.964, loss_scale=16, train_wall=234, gb_free=21, wall=46721
2022-03-04 23:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:34:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:34:34 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 14.02 | nll_loss 13.84 | ppl 14664 | wps 44391.6 | wpb 510.9 | bsz 1 | num_updates 17628 | best_loss 7.571
2022-03-04 23:34:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17628 updates
2022-03-04 23:34:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:34:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:34:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 183 @ 17628 updates, score 14.02) (writing took 2.205197553150356 seconds)
2022-03-04 23:34:37 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-04 23:34:37 | INFO | train | epoch 183 | loss 1.141 | nll_loss 0.805 | ppl 1.75 | wps 24512.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17628 | lr 0.000238176 | gnorm 0.957 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46803
2022-03-04 23:34:37 | INFO | fairseq.trainer | begin training epoch 184
2022-03-04 23:34:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:37:41 | INFO | train_inner | epoch 184:     72 / 97 loss=1.14, nll_loss=0.804, ppl=1.75, wps=24600.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.962, loss_scale=16, train_wall=236, gb_free=21, wall=46987
2022-03-04 23:38:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:38:50 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 13.963 | nll_loss 13.782 | ppl 14086.9 | wps 45127 | wpb 510.9 | bsz 1 | num_updates 17725 | best_loss 7.571
2022-03-04 23:38:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17725 updates
2022-03-04 23:38:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:38:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 184 @ 17725 updates, score 13.963) (writing took 2.229582030326128 seconds)
2022-03-04 23:38:52 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-04 23:38:52 | INFO | train | epoch 184 | loss 1.138 | nll_loss 0.802 | ppl 1.74 | wps 24830.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17725 | lr 0.000237524 | gnorm 0.964 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47059
2022-03-04 23:38:52 | INFO | fairseq.trainer | begin training epoch 185
2022-03-04 23:38:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:39:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:42:07 | INFO | train_inner | epoch 185:     76 / 97 loss=1.137, nll_loss=0.8, ppl=1.74, wps=24627.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=0.96, loss_scale=16, train_wall=236, gb_free=21, wall=47253
2022-03-04 23:43:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:43:06 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 14.081 | nll_loss 13.902 | ppl 15306.1 | wps 44999.8 | wpb 510.9 | bsz 1 | num_updates 17821 | best_loss 7.571
2022-03-04 23:43:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17821 updates
2022-03-04 23:43:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:43:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:43:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 185 @ 17821 updates, score 14.081) (writing took 2.1384623646736145 seconds)
2022-03-04 23:43:08 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-04 23:43:08 | INFO | train | epoch 185 | loss 1.134 | nll_loss 0.798 | ppl 1.74 | wps 24602.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 17821 | lr 0.000236883 | gnorm 0.961 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47314
2022-03-04 23:43:08 | INFO | fairseq.trainer | begin training epoch 186
2022-03-04 23:43:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:45:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:46:33 | INFO | train_inner | epoch 186:     80 / 97 loss=1.132, nll_loss=0.796, ppl=1.74, wps=24638.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17900, lr=0.00023636, gnorm=0.955, loss_scale=16, train_wall=236, gb_free=21, wall=47519
2022-03-04 23:47:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:47:21 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 14.017 | nll_loss 13.837 | ppl 14634.8 | wps 44008.7 | wpb 510.9 | bsz 1 | num_updates 17917 | best_loss 7.571
2022-03-04 23:47:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17917 updates
2022-03-04 23:47:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:47:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:47:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 186 @ 17917 updates, score 14.017) (writing took 2.207586620002985 seconds)
2022-03-04 23:47:24 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-04 23:47:24 | INFO | train | epoch 186 | loss 1.132 | nll_loss 0.795 | ppl 1.74 | wps 24583.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 17917 | lr 0.000236248 | gnorm 0.956 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47570
2022-03-04 23:47:24 | INFO | fairseq.trainer | begin training epoch 187
2022-03-04 23:47:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:50:57 | INFO | train_inner | epoch 187:     83 / 97 loss=1.129, nll_loss=0.792, ppl=1.73, wps=24861.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=0.959, loss_scale=32, train_wall=234, gb_free=21, wall=47783
2022-03-04 23:51:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:51:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:51:37 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 14.049 | nll_loss 13.868 | ppl 14952.2 | wps 45194.4 | wpb 510.9 | bsz 1 | num_updates 18013 | best_loss 7.571
2022-03-04 23:51:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18013 updates
2022-03-04 23:51:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:51:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:51:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 187 @ 18013 updates, score 14.049) (writing took 2.190238829702139 seconds)
2022-03-04 23:51:39 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-04 23:51:39 | INFO | train | epoch 187 | loss 1.127 | nll_loss 0.791 | ppl 1.73 | wps 24594.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 18013 | lr 0.000235617 | gnorm 0.956 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47825
2022-03-04 23:51:39 | INFO | fairseq.trainer | begin training epoch 188
2022-03-04 23:51:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:55:23 | INFO | train_inner | epoch 188:     87 / 97 loss=1.125, nll_loss=0.789, ppl=1.73, wps=24583.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.963, loss_scale=16, train_wall=236, gb_free=21, wall=48049
2022-03-04 23:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:55:53 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 13.988 | nll_loss 13.806 | ppl 14323.7 | wps 45072.5 | wpb 510.9 | bsz 1 | num_updates 18110 | best_loss 7.571
2022-03-04 23:55:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18110 updates
2022-03-04 23:55:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:55:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 188 @ 18110 updates, score 13.988) (writing took 2.150394544005394 seconds)
2022-03-04 23:55:55 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-04 23:55:55 | INFO | train | epoch 188 | loss 1.124 | nll_loss 0.787 | ppl 1.73 | wps 24800.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18110 | lr 0.000234985 | gnorm 0.963 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 48082
2022-03-04 23:55:55 | INFO | fairseq.trainer | begin training epoch 189
2022-03-04 23:55:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:57:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:59:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:59:52 | INFO | train_inner | epoch 189:     92 / 97 loss=1.12, nll_loss=0.783, ppl=1.72, wps=24340.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.959, loss_scale=8, train_wall=239, gb_free=21, wall=48318
2022-03-05 00:00:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:00:10 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 14.045 | nll_loss 13.865 | ppl 14919.4 | wps 44824.4 | wpb 510.9 | bsz 1 | num_updates 18205 | best_loss 7.571
2022-03-05 00:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18205 updates
2022-03-05 00:00:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:00:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:00:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 189 @ 18205 updates, score 14.045) (writing took 2.136721381917596 seconds)
2022-03-05 00:00:12 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 00:00:12 | INFO | train | epoch 189 | loss 1.117 | nll_loss 0.781 | ppl 1.72 | wps 24276.2 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 18205 | lr 0.000234371 | gnorm 0.957 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 48338
2022-03-05 00:00:12 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 00:00:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:04:16 | INFO | train_inner | epoch 190:     95 / 97 loss=1.117, nll_loss=0.781, ppl=1.72, wps=24794.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18300, lr=0.000233762, gnorm=0.958, loss_scale=8, train_wall=234, gb_free=21, wall=48582
2022-03-05 00:04:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:04:26 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 14.09 | nll_loss 13.912 | ppl 15413.7 | wps 44381.2 | wpb 510.9 | bsz 1 | num_updates 18302 | best_loss 7.571
2022-03-05 00:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18302 updates
2022-03-05 00:04:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:04:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 190 @ 18302 updates, score 14.09) (writing took 2.2119945297017694 seconds)
2022-03-05 00:04:28 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 00:04:28 | INFO | train | epoch 190 | loss 1.116 | nll_loss 0.779 | ppl 1.72 | wps 24760.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18302 | lr 0.00023375 | gnorm 0.957 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 48594
2022-03-05 00:04:28 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 00:04:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:08:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:08:43 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 14.081 | nll_loss 13.902 | ppl 15308.8 | wps 44416 | wpb 510.9 | bsz 1 | num_updates 18399 | best_loss 7.571
2022-03-05 00:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18399 updates
2022-03-05 00:08:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:08:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 191 @ 18399 updates, score 14.081) (writing took 2.1817251294851303 seconds)
2022-03-05 00:08:45 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 00:08:45 | INFO | train | epoch 191 | loss 1.111 | nll_loss 0.775 | ppl 1.71 | wps 24758 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18399 | lr 0.000233133 | gnorm 0.958 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 48851
2022-03-05 00:08:45 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 00:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:08:48 | INFO | train_inner | epoch 192:      1 / 97 loss=1.112, nll_loss=0.775, ppl=1.71, wps=24110.6, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=18400, lr=0.000233126, gnorm=0.959, loss_scale=16, train_wall=234, gb_free=21, wall=48854
2022-03-05 00:11:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:12:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:12:59 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 14.166 | nll_loss 13.989 | ppl 16262.6 | wps 44869.1 | wpb 510.9 | bsz 1 | num_updates 18495 | best_loss 7.571
2022-03-05 00:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18495 updates
2022-03-05 00:12:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:13:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:13:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 192 @ 18495 updates, score 14.166) (writing took 2.2602335596457124 seconds)
2022-03-05 00:13:02 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 00:13:02 | INFO | train | epoch 192 | loss 1.108 | nll_loss 0.772 | ppl 1.71 | wps 24481.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18495 | lr 0.000232527 | gnorm 0.957 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 49108
2022-03-05 00:13:02 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 00:13:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:13:15 | INFO | train_inner | epoch 193:      5 / 97 loss=1.106, nll_loss=0.77, ppl=1.71, wps=24516.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18500, lr=0.000232495, gnorm=0.956, loss_scale=16, train_wall=237, gb_free=21, wall=49121
2022-03-05 00:16:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:17:16 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 14.04 | nll_loss 13.86 | ppl 14863.8 | wps 44840.9 | wpb 510.9 | bsz 1 | num_updates 18591 | best_loss 7.571
2022-03-05 00:17:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18591 updates
2022-03-05 00:17:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:17:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:17:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 193 @ 18591 updates, score 14.04) (writing took 2.1260592062026262 seconds)
2022-03-05 00:17:18 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 00:17:18 | INFO | train | epoch 193 | loss 1.105 | nll_loss 0.768 | ppl 1.7 | wps 24492.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18591 | lr 0.000231926 | gnorm 0.956 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 49365
2022-03-05 00:17:18 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 00:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:17:42 | INFO | train_inner | epoch 194:      9 / 97 loss=1.103, nll_loss=0.766, ppl=1.7, wps=24538.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18600, lr=0.000231869, gnorm=0.954, loss_scale=16, train_wall=237, gb_free=21, wall=49388
2022-03-05 00:21:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:21:33 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 14.099 | nll_loss 13.921 | ppl 15507.6 | wps 44449.8 | wpb 510.9 | bsz 1 | num_updates 18688 | best_loss 7.571
2022-03-05 00:21:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18688 updates
2022-03-05 00:21:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:21:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:21:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 194 @ 18688 updates, score 14.099) (writing took 2.156241001561284 seconds)
2022-03-05 00:21:35 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 00:21:35 | INFO | train | epoch 194 | loss 1.101 | nll_loss 0.764 | ppl 1.7 | wps 24777.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18688 | lr 0.000231323 | gnorm 0.949 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 49621
2022-03-05 00:21:35 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 00:21:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:22:06 | INFO | train_inner | epoch 195:     12 / 97 loss=1.1, nll_loss=0.763, ppl=1.7, wps=24802.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18700, lr=0.000231249, gnorm=0.95, loss_scale=16, train_wall=234, gb_free=21, wall=49652
2022-03-05 00:22:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:25:49 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 14.12 | nll_loss 13.941 | ppl 15730.2 | wps 44439.4 | wpb 510.9 | bsz 1 | num_updates 18784 | best_loss 7.571
2022-03-05 00:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18784 updates
2022-03-05 00:25:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:25:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 195 @ 18784 updates, score 14.12) (writing took 2.163257504813373 seconds)
2022-03-05 00:25:51 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 00:25:51 | INFO | train | epoch 195 | loss 1.096 | nll_loss 0.76 | ppl 1.69 | wps 24559.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 18784 | lr 0.000230731 | gnorm 0.95 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 49877
2022-03-05 00:25:51 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 00:25:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:26:32 | INFO | train_inner | epoch 196:     16 / 97 loss=1.096, nll_loss=0.759, ppl=1.69, wps=24590.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.951, loss_scale=16, train_wall=236, gb_free=21, wall=49918
2022-03-05 00:28:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:30:05 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 14.115 | nll_loss 13.936 | ppl 15672 | wps 44294.6 | wpb 510.9 | bsz 1 | num_updates 18880 | best_loss 7.571
2022-03-05 00:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18880 updates
2022-03-05 00:30:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 196 @ 18880 updates, score 14.115) (writing took 2.19908079598099 seconds)
2022-03-05 00:30:07 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 00:30:07 | INFO | train | epoch 196 | loss 1.094 | nll_loss 0.757 | ppl 1.69 | wps 24492.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18880 | lr 0.000230144 | gnorm 0.94 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50134
2022-03-05 00:30:08 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 00:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:30:59 | INFO | train_inner | epoch 197:     20 / 97 loss=1.092, nll_loss=0.755, ppl=1.69, wps=24517.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.936, loss_scale=16, train_wall=237, gb_free=21, wall=50185
2022-03-05 00:34:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:34:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:34:22 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 14.124 | nll_loss 13.944 | ppl 15763.5 | wps 44420.1 | wpb 510.9 | bsz 1 | num_updates 18976 | best_loss 7.571
2022-03-05 00:34:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18976 updates
2022-03-05 00:34:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:34:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:34:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 197 @ 18976 updates, score 14.124) (writing took 2.211715298704803 seconds)
2022-03-05 00:34:24 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 00:34:24 | INFO | train | epoch 197 | loss 1.091 | nll_loss 0.754 | ppl 1.69 | wps 24477.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18976 | lr 0.000229561 | gnorm 0.941 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 50390
2022-03-05 00:34:24 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 00:34:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:35:26 | INFO | train_inner | epoch 198:     24 / 97 loss=1.088, nll_loss=0.752, ppl=1.68, wps=24530, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.94, loss_scale=16, train_wall=237, gb_free=21, wall=50452
2022-03-05 00:38:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:38:38 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 14.15 | nll_loss 13.972 | ppl 16066.8 | wps 44217.3 | wpb 510.9 | bsz 1 | num_updates 19073 | best_loss 7.571
2022-03-05 00:38:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19073 updates
2022-03-05 00:38:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:38:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:38:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 198 @ 19073 updates, score 14.15) (writing took 2.2185088209807873 seconds)
2022-03-05 00:38:41 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 00:38:41 | INFO | train | epoch 198 | loss 1.088 | nll_loss 0.751 | ppl 1.68 | wps 24782.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19073 | lr 0.000228976 | gnorm 0.951 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50647
2022-03-05 00:38:41 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 00:38:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:39:50 | INFO | train_inner | epoch 199:     27 / 97 loss=1.086, nll_loss=0.75, ppl=1.68, wps=24813.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.947, loss_scale=32, train_wall=234, gb_free=21, wall=50716
2022-03-05 00:40:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:42:54 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 14.129 | nll_loss 13.95 | ppl 15829.4 | wps 44441.6 | wpb 510.9 | bsz 1 | num_updates 19169 | best_loss 7.571
2022-03-05 00:42:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19169 updates
2022-03-05 00:42:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:42:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:42:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 199 @ 19169 updates, score 14.129) (writing took 2.157067227177322 seconds)
2022-03-05 00:42:57 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 00:42:57 | INFO | train | epoch 199 | loss 1.083 | nll_loss 0.746 | ppl 1.68 | wps 24565 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19169 | lr 0.000228402 | gnorm 0.94 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50903
2022-03-05 00:42:57 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 00:42:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:44:16 | INFO | train_inner | epoch 200:     31 / 97 loss=1.081, nll_loss=0.744, ppl=1.67, wps=24613.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.94, loss_scale=16, train_wall=236, gb_free=21, wall=50982
2022-03-05 00:46:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:47:10 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 14.174 | nll_loss 13.997 | ppl 16345.8 | wps 44692.1 | wpb 510.9 | bsz 1 | num_updates 19265 | best_loss 7.571
2022-03-05 00:47:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19265 updates
2022-03-05 00:47:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:47:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:47:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 200 @ 19265 updates, score 14.174) (writing took 2.2096631256863475 seconds)
2022-03-05 00:47:12 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 00:47:12 | INFO | train | epoch 200 | loss 1.081 | nll_loss 0.744 | ppl 1.67 | wps 24583.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19265 | lr 0.000227832 | gnorm 0.937 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51159
2022-03-05 00:47:12 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 00:47:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:48:42 | INFO | train_inner | epoch 201:     35 / 97 loss=1.08, nll_loss=0.743, ppl=1.67, wps=24623.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.937, loss_scale=16, train_wall=236, gb_free=21, wall=51248
2022-03-05 00:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:51:26 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 14.16 | nll_loss 13.983 | ppl 16196.9 | wps 44879.4 | wpb 510.9 | bsz 1 | num_updates 19362 | best_loss 7.571
2022-03-05 00:51:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19362 updates
2022-03-05 00:51:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:51:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:51:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 201 @ 19362 updates, score 14.16) (writing took 2.135833923704922 seconds)
2022-03-05 00:51:28 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 00:51:28 | INFO | train | epoch 201 | loss 1.078 | nll_loss 0.741 | ppl 1.67 | wps 24857 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19362 | lr 0.000227261 | gnorm 0.938 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51414
2022-03-05 00:51:28 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 00:51:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:51:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:53:08 | INFO | train_inner | epoch 202:     39 / 97 loss=1.077, nll_loss=0.74, ppl=1.67, wps=24633.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.943, loss_scale=16, train_wall=236, gb_free=21, wall=51514
2022-03-05 00:55:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:55:41 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 14.176 | nll_loss 13.999 | ppl 16373 | wps 45105 | wpb 510.9 | bsz 1 | num_updates 19458 | best_loss 7.571
2022-03-05 00:55:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19458 updates
2022-03-05 00:55:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:55:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 202 @ 19458 updates, score 14.176) (writing took 2.22151720803231 seconds)
2022-03-05 00:55:44 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 00:55:44 | INFO | train | epoch 202 | loss 1.076 | nll_loss 0.74 | ppl 1.67 | wps 24589.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19458 | lr 0.0002267 | gnorm 0.947 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51670
2022-03-05 00:55:44 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 00:55:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:57:31 | INFO | train_inner | epoch 203:     42 / 97 loss=1.076, nll_loss=0.739, ppl=1.67, wps=24867, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.952, loss_scale=16, train_wall=234, gb_free=21, wall=51778
2022-03-05 00:57:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:59:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:59:57 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 14.154 | nll_loss 13.976 | ppl 16112.8 | wps 45438.3 | wpb 510.9 | bsz 1 | num_updates 19554 | best_loss 7.571
2022-03-05 00:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19554 updates
2022-03-05 00:59:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:59:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:59:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 203 @ 19554 updates, score 14.154) (writing took 2.1585115380585194 seconds)
2022-03-05 00:59:59 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 00:59:59 | INFO | train | epoch 203 | loss 1.072 | nll_loss 0.735 | ppl 1.66 | wps 24597.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19554 | lr 0.000226143 | gnorm 0.951 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51925
2022-03-05 00:59:59 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 00:59:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:01:57 | INFO | train_inner | epoch 204:     46 / 97 loss=1.069, nll_loss=0.733, ppl=1.66, wps=24633.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.948, loss_scale=16, train_wall=236, gb_free=21, wall=52043
2022-03-05 01:03:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:04:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:04:13 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 14.173 | nll_loss 13.996 | ppl 16337.8 | wps 45404.9 | wpb 510.9 | bsz 1 | num_updates 19650 | best_loss 7.571
2022-03-05 01:04:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19650 updates
2022-03-05 01:04:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:04:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:04:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 204 @ 19650 updates, score 14.173) (writing took 2.199963854625821 seconds)
2022-03-05 01:04:15 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 01:04:15 | INFO | train | epoch 204 | loss 1.068 | nll_loss 0.731 | ppl 1.66 | wps 24592.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19650 | lr 0.000225589 | gnorm 0.941 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52181
2022-03-05 01:04:15 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 01:04:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:06:23 | INFO | train_inner | epoch 205:     50 / 97 loss=1.067, nll_loss=0.731, ppl=1.66, wps=24630.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.934, loss_scale=16, train_wall=236, gb_free=21, wall=52309
2022-03-05 01:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:08:28 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 14.151 | nll_loss 13.972 | ppl 16074.1 | wps 45162.4 | wpb 510.9 | bsz 1 | num_updates 19747 | best_loss 7.571
2022-03-05 01:08:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19747 updates
2022-03-05 01:08:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:08:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:08:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 205 @ 19747 updates, score 14.151) (writing took 2.093086271546781 seconds)
2022-03-05 01:08:30 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 01:08:30 | INFO | train | epoch 205 | loss 1.065 | nll_loss 0.728 | ppl 1.66 | wps 24853.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19747 | lr 0.000225035 | gnorm 0.934 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52437
2022-03-05 01:08:31 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 01:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:09:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:10:49 | INFO | train_inner | epoch 206:     54 / 97 loss=1.063, nll_loss=0.726, ppl=1.65, wps=24632, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.935, loss_scale=16, train_wall=236, gb_free=21, wall=52575
2022-03-05 01:12:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:12:44 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 14.144 | nll_loss 13.967 | ppl 16009.6 | wps 44780.7 | wpb 510.9 | bsz 1 | num_updates 19843 | best_loss 7.571
2022-03-05 01:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19843 updates
2022-03-05 01:12:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:12:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 206 @ 19843 updates, score 14.144) (writing took 2.146853230893612 seconds)
2022-03-05 01:12:46 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 01:12:46 | INFO | train | epoch 206 | loss 1.063 | nll_loss 0.726 | ppl 1.65 | wps 24589.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19843 | lr 0.00022449 | gnorm 0.937 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52692
2022-03-05 01:12:46 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 01:12:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:14:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:15:15 | INFO | train_inner | epoch 207:     58 / 97 loss=1.061, nll_loss=0.724, ppl=1.65, wps=24627, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.926, loss_scale=16, train_wall=236, gb_free=21, wall=52841
2022-03-05 01:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:17:00 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 14.222 | nll_loss 14.045 | ppl 16906.5 | wps 44028.3 | wpb 510.9 | bsz 1 | num_updates 19939 | best_loss 7.571
2022-03-05 01:17:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19939 updates
2022-03-05 01:17:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:17:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 207 @ 19939 updates, score 14.222) (writing took 2.074573567137122 seconds)
2022-03-05 01:17:02 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 01:17:02 | INFO | train | epoch 207 | loss 1.058 | nll_loss 0.721 | ppl 1.65 | wps 24592.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19939 | lr 0.000223949 | gnorm 0.923 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52948
2022-03-05 01:17:02 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 01:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:19:38 | INFO | train_inner | epoch 208:     61 / 97 loss=1.058, nll_loss=0.722, ppl=1.65, wps=24879.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.937, loss_scale=16, train_wall=234, gb_free=21, wall=53104
2022-03-05 01:20:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:21:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:21:15 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 14.257 | nll_loss 14.08 | ppl 17323.9 | wps 44496.4 | wpb 510.9 | bsz 1 | num_updates 20035 | best_loss 7.571
2022-03-05 01:21:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20035 updates
2022-03-05 01:21:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:21:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:21:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 208 @ 20035 updates, score 14.257) (writing took 2.1683856435120106 seconds)
2022-03-05 01:21:17 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 01:21:17 | INFO | train | epoch 208 | loss 1.058 | nll_loss 0.721 | ppl 1.65 | wps 24593.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 20035 | lr 0.000223411 | gnorm 0.94 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53204
2022-03-05 01:21:17 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 01:21:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:24:04 | INFO | train_inner | epoch 209:     65 / 97 loss=1.055, nll_loss=0.718, ppl=1.64, wps=24608.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.928, loss_scale=16, train_wall=236, gb_free=21, wall=53370
2022-03-05 01:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:25:31 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 14.177 | nll_loss 14 | ppl 16386 | wps 44397.2 | wpb 510.9 | bsz 1 | num_updates 20132 | best_loss 7.571
2022-03-05 01:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20132 updates
2022-03-05 01:25:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:25:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:25:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 209 @ 20132 updates, score 14.177) (writing took 2.140038752928376 seconds)
2022-03-05 01:25:33 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 01:25:33 | INFO | train | epoch 209 | loss 1.053 | nll_loss 0.717 | ppl 1.64 | wps 24820 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20132 | lr 0.000222873 | gnorm 0.925 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53460
2022-03-05 01:25:33 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 01:25:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:26:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:28:31 | INFO | train_inner | epoch 210:     69 / 97 loss=1.051, nll_loss=0.715, ppl=1.64, wps=24581.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.927, loss_scale=16, train_wall=236, gb_free=21, wall=53637
2022-03-05 01:29:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:29:47 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 14.224 | nll_loss 14.046 | ppl 16918 | wps 44518.3 | wpb 510.9 | bsz 1 | num_updates 20228 | best_loss 7.571
2022-03-05 01:29:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20228 updates
2022-03-05 01:29:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:29:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 210 @ 20228 updates, score 14.224) (writing took 2.20423417724669 seconds)
2022-03-05 01:29:50 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 01:29:50 | INFO | train | epoch 210 | loss 1.049 | nll_loss 0.713 | ppl 1.64 | wps 24532.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20228 | lr 0.000222343 | gnorm 0.922 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53716
2022-03-05 01:29:50 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 01:29:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:31:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:32:57 | INFO | train_inner | epoch 211:     73 / 97 loss=1.049, nll_loss=0.712, ppl=1.64, wps=24565, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.916, loss_scale=16, train_wall=237, gb_free=21, wall=53904
2022-03-05 01:33:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:33:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:34:04 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 14.206 | nll_loss 14.028 | ppl 16710.5 | wps 44511.1 | wpb 510.9 | bsz 1 | num_updates 20323 | best_loss 7.571
2022-03-05 01:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20323 updates
2022-03-05 01:34:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 211 @ 20323 updates, score 14.206) (writing took 2.1509252544492483 seconds)
2022-03-05 01:34:06 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 01:34:06 | INFO | train | epoch 211 | loss 1.047 | nll_loss 0.71 | ppl 1.64 | wps 24283.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 20323 | lr 0.000221823 | gnorm 0.918 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 53972
2022-03-05 01:34:06 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 01:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:37:23 | INFO | train_inner | epoch 212:     77 / 97 loss=1.046, nll_loss=0.709, ppl=1.63, wps=24613.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.921, loss_scale=8, train_wall=236, gb_free=21, wall=54170
2022-03-05 01:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:38:20 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 14.259 | nll_loss 14.083 | ppl 17348.7 | wps 44893.4 | wpb 510.9 | bsz 1 | num_updates 20420 | best_loss 7.571
2022-03-05 01:38:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20420 updates
2022-03-05 01:38:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:38:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 212 @ 20420 updates, score 14.259) (writing took 2.131917211227119 seconds)
2022-03-05 01:38:22 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 01:38:22 | INFO | train | epoch 212 | loss 1.045 | nll_loss 0.709 | ppl 1.63 | wps 24835.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20420 | lr 0.000221295 | gnorm 0.917 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 54228
2022-03-05 01:38:22 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 01:38:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:41:47 | INFO | train_inner | epoch 213:     80 / 97 loss=1.044, nll_loss=0.707, ppl=1.63, wps=24854.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.92, loss_scale=16, train_wall=234, gb_free=21, wall=54433
2022-03-05 01:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:42:35 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 14.292 | nll_loss 14.117 | ppl 17764.7 | wps 45423.1 | wpb 510.9 | bsz 1 | num_updates 20517 | best_loss 7.571
2022-03-05 01:42:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20517 updates
2022-03-05 01:42:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 213 @ 20517 updates, score 14.292) (writing took 2.0856076953932643 seconds)
2022-03-05 01:42:37 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 01:42:37 | INFO | train | epoch 213 | loss 1.042 | nll_loss 0.705 | ppl 1.63 | wps 24849 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20517 | lr 0.000220772 | gnorm 0.924 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54483
2022-03-05 01:42:37 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 01:42:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:44:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:46:13 | INFO | train_inner | epoch 214:     84 / 97 loss=1.04, nll_loss=0.703, ppl=1.63, wps=24635.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.925, loss_scale=16, train_wall=236, gb_free=21, wall=54699
2022-03-05 01:46:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:46:51 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 14.25 | nll_loss 14.073 | ppl 17231.2 | wps 45020.4 | wpb 510.9 | bsz 1 | num_updates 20613 | best_loss 7.571
2022-03-05 01:46:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20613 updates
2022-03-05 01:46:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:46:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:46:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 214 @ 20613 updates, score 14.25) (writing took 2.2318202229216695 seconds)
2022-03-05 01:46:53 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 01:46:53 | INFO | train | epoch 214 | loss 1.039 | nll_loss 0.703 | ppl 1.63 | wps 24580.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 20613 | lr 0.000220257 | gnorm 0.921 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54739
2022-03-05 01:46:53 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 01:46:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:50:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:50:39 | INFO | train_inner | epoch 215:     88 / 97 loss=1.038, nll_loss=0.702, ppl=1.63, wps=24636.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.913, loss_scale=16, train_wall=236, gb_free=21, wall=54965
2022-03-05 01:51:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:51:07 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 14.249 | nll_loss 14.073 | ppl 17239.2 | wps 44438.7 | wpb 510.9 | bsz 1 | num_updates 20709 | best_loss 7.571
2022-03-05 01:51:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20709 updates
2022-03-05 01:51:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:51:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:51:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 215 @ 20709 updates, score 14.249) (writing took 2.136864336207509 seconds)
2022-03-05 01:51:09 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 01:51:09 | INFO | train | epoch 215 | loss 1.036 | nll_loss 0.699 | ppl 1.62 | wps 24604.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 20709 | lr 0.000219746 | gnorm 0.911 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54995
2022-03-05 01:51:09 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 01:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:55:02 | INFO | train_inner | epoch 216:     91 / 97 loss=1.034, nll_loss=0.698, ppl=1.62, wps=24856.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.922, loss_scale=16, train_wall=234, gb_free=21, wall=55228
2022-03-05 01:55:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:55:22 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 14.256 | nll_loss 14.081 | ppl 17333.1 | wps 44471.5 | wpb 510.9 | bsz 1 | num_updates 20806 | best_loss 7.571
2022-03-05 01:55:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20806 updates
2022-03-05 01:55:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:55:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:55:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 216 @ 20806 updates, score 14.256) (writing took 2.1245174473151565 seconds)
2022-03-05 01:55:24 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 01:55:24 | INFO | train | epoch 216 | loss 1.034 | nll_loss 0.697 | ppl 1.62 | wps 24835.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20806 | lr 0.000219233 | gnorm 0.921 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55251
2022-03-05 01:55:24 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 01:55:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:55:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:59:29 | INFO | train_inner | epoch 217:     95 / 97 loss=1.032, nll_loss=0.695, ppl=1.62, wps=24587.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20900, lr=0.000218739, gnorm=0.911, loss_scale=16, train_wall=237, gb_free=21, wall=55495
2022-03-05 01:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:59:38 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 14.216 | nll_loss 14.04 | ppl 16839.5 | wps 44859.9 | wpb 510.9 | bsz 1 | num_updates 20902 | best_loss 7.571
2022-03-05 01:59:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20902 updates
2022-03-05 01:59:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:59:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 217 @ 20902 updates, score 14.216) (writing took 2.095728605054319 seconds)
2022-03-05 01:59:41 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 01:59:41 | INFO | train | epoch 217 | loss 1.03 | nll_loss 0.693 | ppl 1.62 | wps 24548.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20902 | lr 0.000218729 | gnorm 0.911 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55507
2022-03-05 01:59:41 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 01:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:01:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:03:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:03:55 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 14.308 | nll_loss 14.133 | ppl 17966.3 | wps 45095.1 | wpb 510.9 | bsz 1 | num_updates 20998 | best_loss 7.571
2022-03-05 02:03:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 20998 updates
2022-03-05 02:03:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:03:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:03:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 218 @ 20998 updates, score 14.308) (writing took 2.1547526996582747 seconds)
2022-03-05 02:03:57 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 02:03:57 | INFO | train | epoch 218 | loss 1.029 | nll_loss 0.692 | ppl 1.62 | wps 24539.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20998 | lr 0.000218228 | gnorm 0.917 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55763
2022-03-05 02:03:57 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 02:03:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:02 | INFO | train_inner | epoch 219:      2 / 97 loss=1.029, nll_loss=0.692, ppl=1.62, wps=23931.1, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=21000, lr=0.000218218, gnorm=0.918, loss_scale=16, train_wall=237, gb_free=21, wall=55768
2022-03-05 02:07:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:08:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:08:11 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 14.253 | nll_loss 14.078 | ppl 17296.9 | wps 45161.5 | wpb 510.9 | bsz 1 | num_updates 21094 | best_loss 7.571
2022-03-05 02:08:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21094 updates
2022-03-05 02:08:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:08:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:08:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 219 @ 21094 updates, score 14.253) (writing took 2.2059942027553916 seconds)
2022-03-05 02:08:13 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 02:08:13 | INFO | train | epoch 219 | loss 1.026 | nll_loss 0.689 | ppl 1.61 | wps 24520.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21094 | lr 0.000217731 | gnorm 0.918 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56019
2022-03-05 02:08:13 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 02:08:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:08:29 | INFO | train_inner | epoch 220:      6 / 97 loss=1.024, nll_loss=0.687, ppl=1.61, wps=24557.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21100, lr=0.0002177, gnorm=0.918, loss_scale=16, train_wall=237, gb_free=21, wall=56035
2022-03-05 02:12:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:12:28 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 14.326 | nll_loss 14.153 | ppl 18214.2 | wps 44760.8 | wpb 510.9 | bsz 1 | num_updates 21191 | best_loss 7.571
2022-03-05 02:12:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21191 updates
2022-03-05 02:12:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:12:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 220 @ 21191 updates, score 14.326) (writing took 2.1376068014651537 seconds)
2022-03-05 02:12:30 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 02:12:30 | INFO | train | epoch 220 | loss 1.023 | nll_loss 0.686 | ppl 1.61 | wps 24762 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21191 | lr 0.000217232 | gnorm 0.926 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 56276
2022-03-05 02:12:30 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 02:12:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:12:53 | INFO | train_inner | epoch 221:      9 / 97 loss=1.022, nll_loss=0.685, ppl=1.61, wps=24783.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21200, lr=0.000217186, gnorm=0.921, loss_scale=16, train_wall=235, gb_free=21, wall=56299
2022-03-05 02:13:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:16:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:16:44 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 14.281 | nll_loss 14.107 | ppl 17643.9 | wps 44467.8 | wpb 510.9 | bsz 1 | num_updates 21287 | best_loss 7.571
2022-03-05 02:16:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21287 updates
2022-03-05 02:16:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:16:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:16:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 221 @ 21287 updates, score 14.281) (writing took 2.2394168758764863 seconds)
2022-03-05 02:16:46 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 02:16:46 | INFO | train | epoch 221 | loss 1.019 | nll_loss 0.682 | ppl 1.6 | wps 24498.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21287 | lr 0.000216742 | gnorm 0.912 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 56533
2022-03-05 02:16:46 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 02:16:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:17:20 | INFO | train_inner | epoch 222:     13 / 97 loss=1.018, nll_loss=0.682, ppl=1.6, wps=24538.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21300, lr=0.000216676, gnorm=0.913, loss_scale=16, train_wall=237, gb_free=21, wall=56566
2022-03-05 02:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:20:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:21:01 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 14.348 | nll_loss 14.175 | ppl 18500.1 | wps 44527.6 | wpb 510.9 | bsz 1 | num_updates 21383 | best_loss 7.571
2022-03-05 02:21:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21383 updates
2022-03-05 02:21:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:21:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 222 @ 21383 updates, score 14.348) (writing took 2.133659105747938 seconds)
2022-03-05 02:21:03 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 02:21:03 | INFO | train | epoch 222 | loss 1.018 | nll_loss 0.681 | ppl 1.6 | wps 24505.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21383 | lr 0.000216255 | gnorm 0.909 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 56789
2022-03-05 02:21:03 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 02:21:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:21:47 | INFO | train_inner | epoch 223:     17 / 97 loss=1.017, nll_loss=0.68, ppl=1.6, wps=24539.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.909, loss_scale=16, train_wall=237, gb_free=21, wall=56833
2022-03-05 02:22:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:25:17 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 14.325 | nll_loss 14.152 | ppl 18200.8 | wps 44576.7 | wpb 510.9 | bsz 1 | num_updates 21479 | best_loss 7.571
2022-03-05 02:25:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21479 updates
2022-03-05 02:25:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:25:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:25:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 223 @ 21479 updates, score 14.325) (writing took 2.2383299292996526 seconds)
2022-03-05 02:25:20 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 02:25:20 | INFO | train | epoch 223 | loss 1.015 | nll_loss 0.679 | ppl 1.6 | wps 24496.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21479 | lr 0.000215771 | gnorm 0.904 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 57046
2022-03-05 02:25:20 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 02:25:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:26:14 | INFO | train_inner | epoch 224:     21 / 97 loss=1.013, nll_loss=0.677, ppl=1.6, wps=24536.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.904, loss_scale=8, train_wall=237, gb_free=21, wall=57100
2022-03-05 02:29:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:29:34 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 14.324 | nll_loss 14.151 | ppl 18187.9 | wps 44372 | wpb 510.9 | bsz 1 | num_updates 21576 | best_loss 7.571
2022-03-05 02:29:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21576 updates
2022-03-05 02:29:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:29:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:29:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 224 @ 21576 updates, score 14.324) (writing took 2.0821302719414234 seconds)
2022-03-05 02:29:36 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 02:29:36 | INFO | train | epoch 224 | loss 1.014 | nll_loss 0.678 | ppl 1.6 | wps 24761.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21576 | lr 0.000215285 | gnorm 0.915 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 57302
2022-03-05 02:29:36 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 02:29:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:30:38 | INFO | train_inner | epoch 225:     24 / 97 loss=1.013, nll_loss=0.676, ppl=1.6, wps=24776.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.911, loss_scale=16, train_wall=235, gb_free=21, wall=57364
2022-03-05 02:30:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:33:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:33:51 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 14.332 | nll_loss 14.157 | ppl 18267.7 | wps 44602 | wpb 510.9 | bsz 1 | num_updates 21672 | best_loss 7.571
2022-03-05 02:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21672 updates
2022-03-05 02:33:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:33:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:33:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 225 @ 21672 updates, score 14.332) (writing took 2.161635278724134 seconds)
2022-03-05 02:33:53 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 02:33:53 | INFO | train | epoch 225 | loss 1.01 | nll_loss 0.673 | ppl 1.59 | wps 24497.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21672 | lr 0.000214808 | gnorm 0.905 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 57559
2022-03-05 02:33:53 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 02:33:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:35:05 | INFO | train_inner | epoch 226:     28 / 97 loss=1.01, nll_loss=0.673, ppl=1.59, wps=24531.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.909, loss_scale=8, train_wall=237, gb_free=21, wall=57631
2022-03-05 02:38:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:38:07 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 14.311 | nll_loss 14.134 | ppl 17980.2 | wps 44976.7 | wpb 510.9 | bsz 1 | num_updates 21769 | best_loss 7.571
2022-03-05 02:38:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21769 updates
2022-03-05 02:38:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:38:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:38:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 226 @ 21769 updates, score 14.311) (writing took 2.068341958336532 seconds)
2022-03-05 02:38:10 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 02:38:10 | INFO | train | epoch 226 | loss 1.008 | nll_loss 0.671 | ppl 1.59 | wps 24740.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21769 | lr 0.000214329 | gnorm 0.911 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 57816
2022-03-05 02:38:10 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 02:38:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:39:29 | INFO | train_inner | epoch 227:     31 / 97 loss=1.007, nll_loss=0.671, ppl=1.59, wps=24761.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.903, loss_scale=16, train_wall=235, gb_free=21, wall=57896
2022-03-05 02:42:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:42:24 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 14.325 | nll_loss 14.152 | ppl 18205.1 | wps 45202 | wpb 510.9 | bsz 1 | num_updates 21866 | best_loss 7.571
2022-03-05 02:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21866 updates
2022-03-05 02:42:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:42:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:42:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 227 @ 21866 updates, score 14.325) (writing took 2.1873379703611135 seconds)
2022-03-05 02:42:26 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 02:42:26 | INFO | train | epoch 227 | loss 1.006 | nll_loss 0.669 | ppl 1.59 | wps 24742 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21866 | lr 0.000213853 | gnorm 0.891 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 58072
2022-03-05 02:42:26 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 02:42:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:42:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:43:57 | INFO | train_inner | epoch 228:     35 / 97 loss=1.005, nll_loss=0.669, ppl=1.59, wps=24525.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.901, loss_scale=16, train_wall=237, gb_free=21, wall=58163
2022-03-05 02:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:46:41 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 14.307 | nll_loss 14.133 | ppl 17963.9 | wps 45166.9 | wpb 510.9 | bsz 1 | num_updates 21962 | best_loss 7.571
2022-03-05 02:46:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21962 updates
2022-03-05 02:46:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:46:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 228 @ 21962 updates, score 14.307) (writing took 2.1575860930606723 seconds)
2022-03-05 02:46:43 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 02:46:43 | INFO | train | epoch 228 | loss 1.003 | nll_loss 0.666 | ppl 1.59 | wps 24499.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21962 | lr 0.000213385 | gnorm 0.902 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 58329
2022-03-05 02:46:43 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 02:46:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:48:21 | INFO | train_inner | epoch 229:     38 / 97 loss=1.001, nll_loss=0.664, ppl=1.58, wps=24790.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.895, loss_scale=32, train_wall=234, gb_free=21, wall=58427
2022-03-05 02:48:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:50:57 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 14.365 | nll_loss 14.192 | ppl 18719.6 | wps 44324.1 | wpb 510.9 | bsz 1 | num_updates 22058 | best_loss 7.571
2022-03-05 02:50:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22058 updates
2022-03-05 02:50:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:50:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:50:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 229 @ 22058 updates, score 14.365) (writing took 2.2105073230341077 seconds)
2022-03-05 02:50:59 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 02:50:59 | INFO | train | epoch 229 | loss 1 | nll_loss 0.664 | ppl 1.58 | wps 24523.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22058 | lr 0.00021292 | gnorm 0.896 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58585
2022-03-05 02:50:59 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 02:50:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:52:47 | INFO | train_inner | epoch 230:     42 / 97 loss=0.998, nll_loss=0.661, ppl=1.58, wps=24569.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.898, loss_scale=16, train_wall=237, gb_free=21, wall=58693
2022-03-05 02:54:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:55:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:55:13 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 14.341 | nll_loss 14.169 | ppl 18423 | wps 44587.7 | wpb 510.9 | bsz 1 | num_updates 22154 | best_loss 7.571
2022-03-05 02:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22154 updates
2022-03-05 02:55:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:55:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:55:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 230 @ 22154 updates, score 14.341) (writing took 2.130313716828823 seconds)
2022-03-05 02:55:15 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 02:55:15 | INFO | train | epoch 230 | loss 0.997 | nll_loss 0.661 | ppl 1.58 | wps 24549.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22154 | lr 0.000212458 | gnorm 0.904 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58842
2022-03-05 02:55:15 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 02:55:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:56:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:57:16 | INFO | train_inner | epoch 231:     47 / 97 loss=0.997, nll_loss=0.66, ppl=1.58, wps=24351.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=22200, lr=0.000212238, gnorm=0.906, loss_scale=8, train_wall=239, gb_free=21, wall=58962
2022-03-05 02:59:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:59:29 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 14.356 | nll_loss 14.182 | ppl 18585.7 | wps 44831 | wpb 510.9 | bsz 1 | num_updates 22250 | best_loss 7.571
2022-03-05 02:59:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22250 updates
2022-03-05 02:59:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:59:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 231 @ 22250 updates, score 14.356) (writing took 2.1589276008307934 seconds)
2022-03-05 02:59:32 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 02:59:32 | INFO | train | epoch 231 | loss 0.994 | nll_loss 0.658 | ppl 1.58 | wps 24537.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22250 | lr 0.000212 | gnorm 0.899 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 59098
2022-03-05 02:59:32 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 02:59:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:01:40 | INFO | train_inner | epoch 232:     50 / 97 loss=0.994, nll_loss=0.657, ppl=1.58, wps=24815.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.896, loss_scale=8, train_wall=234, gb_free=21, wall=59226
2022-03-05 03:03:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:03:46 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 14.418 | nll_loss 14.245 | ppl 19414.6 | wps 45121.9 | wpb 510.9 | bsz 1 | num_updates 22347 | best_loss 7.571
2022-03-05 03:03:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22347 updates
2022-03-05 03:03:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:03:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:03:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 232 @ 22347 updates, score 14.418) (writing took 2.0943036694079638 seconds)
2022-03-05 03:03:48 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 03:03:48 | INFO | train | epoch 232 | loss 0.994 | nll_loss 0.658 | ppl 1.58 | wps 24804.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22347 | lr 0.000211539 | gnorm 0.902 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 59354
2022-03-05 03:03:48 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 03:03:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:06:04 | INFO | train_inner | epoch 233:     53 / 97 loss=0.994, nll_loss=0.657, ppl=1.58, wps=24803.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.907, loss_scale=16, train_wall=235, gb_free=21, wall=59490
2022-03-05 03:07:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:08:02 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 14.36 | nll_loss 14.187 | ppl 18646.7 | wps 45128.1 | wpb 510.9 | bsz 1 | num_updates 22444 | best_loss 7.571
2022-03-05 03:08:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22444 updates
2022-03-05 03:08:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:08:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:08:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 233 @ 22444 updates, score 14.36) (writing took 2.161910705268383 seconds)
2022-03-05 03:08:04 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 03:08:04 | INFO | train | epoch 233 | loss 0.992 | nll_loss 0.655 | ppl 1.57 | wps 24770.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22444 | lr 0.000211081 | gnorm 0.913 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 59610
2022-03-05 03:08:04 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 03:08:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:08:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:10:31 | INFO | train_inner | epoch 234:     57 / 97 loss=0.989, nll_loss=0.652, ppl=1.57, wps=24561, ups=0.38, wpb=65495, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.908, loss_scale=16, train_wall=237, gb_free=21, wall=59757
2022-03-05 03:12:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:12:19 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 14.355 | nll_loss 14.183 | ppl 18593.5 | wps 44970.9 | wpb 510.9 | bsz 1 | num_updates 22540 | best_loss 7.571
2022-03-05 03:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22540 updates
2022-03-05 03:12:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:12:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 234 @ 22540 updates, score 14.355) (writing took 2.159759793430567 seconds)
2022-03-05 03:12:21 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 03:12:21 | INFO | train | epoch 234 | loss 0.988 | nll_loss 0.651 | ppl 1.57 | wps 24516 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22540 | lr 0.000210631 | gnorm 0.895 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 59867
2022-03-05 03:12:21 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 03:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:13:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:14:58 | INFO | train_inner | epoch 235:     61 / 97 loss=0.986, nll_loss=0.65, ppl=1.57, wps=24536.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.893, loss_scale=16, train_wall=237, gb_free=21, wall=60024
2022-03-05 03:16:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:16:35 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 14.415 | nll_loss 14.244 | ppl 19401.2 | wps 44493.3 | wpb 510.9 | bsz 1 | num_updates 22636 | best_loss 7.571
2022-03-05 03:16:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22636 updates
2022-03-05 03:16:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:16:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:16:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 235 @ 22636 updates, score 14.415) (writing took 2.2231132481247187 seconds)
2022-03-05 03:16:37 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 03:16:37 | INFO | train | epoch 235 | loss 0.986 | nll_loss 0.649 | ppl 1.57 | wps 24490.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22636 | lr 0.000210184 | gnorm 0.893 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 60124
2022-03-05 03:16:37 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 03:16:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:19:22 | INFO | train_inner | epoch 236:     64 / 97 loss=0.986, nll_loss=0.649, ppl=1.57, wps=24765, ups=0.38, wpb=65495, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.891, loss_scale=16, train_wall=235, gb_free=21, wall=60288
2022-03-05 03:19:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:20:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:20:52 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 14.358 | nll_loss 14.186 | ppl 18635.8 | wps 44537.4 | wpb 510.9 | bsz 1 | num_updates 22732 | best_loss 7.571
2022-03-05 03:20:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22732 updates
2022-03-05 03:20:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:20:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 236 @ 22732 updates, score 14.358) (writing took 2.1757279234007 seconds)
2022-03-05 03:20:54 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 03:20:54 | INFO | train | epoch 236 | loss 0.983 | nll_loss 0.647 | ppl 1.57 | wps 24490.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22732 | lr 0.00020974 | gnorm 0.887 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 60380
2022-03-05 03:20:54 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 03:20:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:23:49 | INFO | train_inner | epoch 237:     68 / 97 loss=0.982, nll_loss=0.645, ppl=1.56, wps=24542.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.885, loss_scale=16, train_wall=237, gb_free=21, wall=60555
2022-03-05 03:25:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:25:08 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 14.399 | nll_loss 14.228 | ppl 19191 | wps 44337.3 | wpb 510.9 | bsz 1 | num_updates 22829 | best_loss 7.571
2022-03-05 03:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22829 updates
2022-03-05 03:25:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:25:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:25:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 237 @ 22829 updates, score 14.399) (writing took 2.2028259420767426 seconds)
2022-03-05 03:25:11 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 03:25:11 | INFO | train | epoch 237 | loss 0.983 | nll_loss 0.646 | ppl 1.56 | wps 24764.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22829 | lr 0.000209294 | gnorm 0.891 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 60637
2022-03-05 03:25:11 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 03:25:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:25:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:28:16 | INFO | train_inner | epoch 238:     72 / 97 loss=0.984, nll_loss=0.647, ppl=1.57, wps=24566.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.892, loss_scale=16, train_wall=237, gb_free=21, wall=60822
2022-03-05 03:29:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:29:25 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 14.46 | nll_loss 14.29 | ppl 20035.8 | wps 45073 | wpb 510.9 | bsz 1 | num_updates 22925 | best_loss 7.571
2022-03-05 03:29:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22925 updates
2022-03-05 03:29:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:29:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:29:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 238 @ 22925 updates, score 14.46) (writing took 2.0765230217948556 seconds)
2022-03-05 03:29:27 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 03:29:27 | INFO | train | epoch 238 | loss 0.979 | nll_loss 0.642 | ppl 1.56 | wps 24559.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22925 | lr 0.000208855 | gnorm 0.886 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 60893
2022-03-05 03:29:27 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 03:29:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:32:42 | INFO | train_inner | epoch 239:     76 / 97 loss=0.976, nll_loss=0.64, ppl=1.56, wps=24623.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.888, loss_scale=16, train_wall=236, gb_free=21, wall=61088
2022-03-05 03:33:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:33:40 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 14.449 | nll_loss 14.278 | ppl 19864.3 | wps 44760.2 | wpb 510.9 | bsz 1 | num_updates 23021 | best_loss 7.571
2022-03-05 03:33:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23021 updates
2022-03-05 03:33:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:33:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:33:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 239 @ 23021 updates, score 14.449) (writing took 2.211042856797576 seconds)
2022-03-05 03:33:42 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 03:33:42 | INFO | train | epoch 239 | loss 0.978 | nll_loss 0.642 | ppl 1.56 | wps 24574.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 23021 | lr 0.000208419 | gnorm 0.89 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61149
2022-03-05 03:33:43 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 03:33:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:37:05 | INFO | train_inner | epoch 240:     79 / 97 loss=0.977, nll_loss=0.64, ppl=1.56, wps=24848.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.893, loss_scale=16, train_wall=234, gb_free=21, wall=61351
2022-03-05 03:37:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:37:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:37:56 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 14.356 | nll_loss 14.184 | ppl 18613.7 | wps 44865.3 | wpb 510.9 | bsz 1 | num_updates 23117 | best_loss 7.571
2022-03-05 03:37:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23117 updates
2022-03-05 03:37:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:37:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:37:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 240 @ 23117 updates, score 14.356) (writing took 2.127310830168426 seconds)
2022-03-05 03:37:58 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 03:37:58 | INFO | train | epoch 240 | loss 0.975 | nll_loss 0.638 | ppl 1.56 | wps 24580.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 23117 | lr 0.000207986 | gnorm 0.894 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61404
2022-03-05 03:37:58 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 03:37:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:41:31 | INFO | train_inner | epoch 241:     83 / 97 loss=0.975, nll_loss=0.639, ppl=1.56, wps=24612.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.904, loss_scale=16, train_wall=236, gb_free=21, wall=61617
2022-03-05 03:42:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:42:12 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 14.386 | nll_loss 14.214 | ppl 19004.3 | wps 45071.5 | wpb 510.9 | bsz 1 | num_updates 23214 | best_loss 7.571
2022-03-05 03:42:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23214 updates
2022-03-05 03:42:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:42:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:42:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 241 @ 23214 updates, score 14.386) (writing took 2.177371760830283 seconds)
2022-03-05 03:42:14 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 03:42:14 | INFO | train | epoch 241 | loss 0.974 | nll_loss 0.637 | ppl 1.56 | wps 24820 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23214 | lr 0.000207551 | gnorm 0.905 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61660
2022-03-05 03:42:14 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 03:42:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:43:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:45:58 | INFO | train_inner | epoch 242:     87 / 97 loss=0.971, nll_loss=0.635, ppl=1.55, wps=24551.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.89, loss_scale=16, train_wall=237, gb_free=21, wall=61884
2022-03-05 03:46:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:46:29 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 14.456 | nll_loss 14.285 | ppl 19960.8 | wps 45067.3 | wpb 510.9 | bsz 1 | num_updates 23310 | best_loss 7.571
2022-03-05 03:46:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23310 updates
2022-03-05 03:46:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:46:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:46:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 242 @ 23310 updates, score 14.456) (writing took 2.1062129186466336 seconds)
2022-03-05 03:46:31 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 03:46:31 | INFO | train | epoch 242 | loss 0.97 | nll_loss 0.633 | ppl 1.55 | wps 24513.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23310 | lr 0.000207123 | gnorm 0.888 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 61917
2022-03-05 03:46:31 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 03:46:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:48:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:50:25 | INFO | train_inner | epoch 243:     91 / 97 loss=0.97, nll_loss=0.633, ppl=1.55, wps=24541.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.891, loss_scale=16, train_wall=237, gb_free=21, wall=62151
2022-03-05 03:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:50:45 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 14.41 | nll_loss 14.237 | ppl 19309.9 | wps 44990.5 | wpb 510.9 | bsz 1 | num_updates 23406 | best_loss 7.571
2022-03-05 03:50:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23406 updates
2022-03-05 03:50:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:50:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:50:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 243 @ 23406 updates, score 14.41) (writing took 2.1782548036426306 seconds)
2022-03-05 03:50:47 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 03:50:47 | INFO | train | epoch 243 | loss 0.969 | nll_loss 0.632 | ppl 1.55 | wps 24500.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23406 | lr 0.000206698 | gnorm 0.887 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 62173
2022-03-05 03:50:47 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 03:50:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:54:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:54:51 | INFO | train_inner | epoch 244:     95 / 97 loss=0.968, nll_loss=0.631, ppl=1.55, wps=24593.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23500, lr=0.000206284, gnorm=0.89, loss_scale=16, train_wall=236, gb_free=21, wall=62417
2022-03-05 03:54:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:55:01 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 14.424 | nll_loss 14.252 | ppl 19517.7 | wps 45089.9 | wpb 510.9 | bsz 1 | num_updates 23502 | best_loss 7.571
2022-03-05 03:55:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23502 updates
2022-03-05 03:55:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:55:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:55:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 244 @ 23502 updates, score 14.424) (writing took 2.171123712323606 seconds)
2022-03-05 03:55:03 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 03:55:03 | INFO | train | epoch 244 | loss 0.967 | nll_loss 0.631 | ppl 1.55 | wps 24557.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23502 | lr 0.000206275 | gnorm 0.89 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62429
2022-03-05 03:55:03 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 03:55:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:59:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:59:17 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 14.437 | nll_loss 14.266 | ppl 19707 | wps 45084 | wpb 510.9 | bsz 1 | num_updates 23599 | best_loss 7.571
2022-03-05 03:59:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23599 updates
2022-03-05 03:59:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:59:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:59:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 245 @ 23599 updates, score 14.437) (writing took 2.143746079877019 seconds)
2022-03-05 03:59:19 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 03:59:19 | INFO | train | epoch 245 | loss 0.965 | nll_loss 0.629 | ppl 1.55 | wps 24828.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23599 | lr 0.000205851 | gnorm 0.893 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62685
2022-03-05 03:59:19 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 03:59:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:59:22 | INFO | train_inner | epoch 246:      1 / 97 loss=0.966, nll_loss=0.629, ppl=1.55, wps=24187.6, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=23600, lr=0.000205847, gnorm=0.892, loss_scale=16, train_wall=234, gb_free=21, wall=62688
2022-03-05 04:00:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:03:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:03:33 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 14.455 | nll_loss 14.286 | ppl 19977.6 | wps 44810.5 | wpb 510.9 | bsz 1 | num_updates 23695 | best_loss 7.571
2022-03-05 04:03:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23695 updates
2022-03-05 04:03:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:03:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:03:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 246 @ 23695 updates, score 14.455) (writing took 2.1816389337182045 seconds)
2022-03-05 04:03:35 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 04:03:35 | INFO | train | epoch 246 | loss 0.963 | nll_loss 0.626 | ppl 1.54 | wps 24558.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23695 | lr 0.000205434 | gnorm 0.878 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62941
2022-03-05 04:03:35 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 04:03:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:03:48 | INFO | train_inner | epoch 247:      5 / 97 loss=0.961, nll_loss=0.625, ppl=1.54, wps=24598.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=0.878, loss_scale=16, train_wall=236, gb_free=21, wall=62954
2022-03-05 04:06:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:07:49 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 14.458 | nll_loss 14.288 | ppl 20007.6 | wps 44561.4 | wpb 510.9 | bsz 1 | num_updates 23791 | best_loss 7.571
2022-03-05 04:07:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23791 updates
2022-03-05 04:07:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:07:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:07:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 247 @ 23791 updates, score 14.458) (writing took 2.214966705068946 seconds)
2022-03-05 04:07:51 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 04:07:51 | INFO | train | epoch 247 | loss 0.96 | nll_loss 0.623 | ppl 1.54 | wps 24535.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23791 | lr 0.000205019 | gnorm 0.885 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 63198
2022-03-05 04:07:51 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 04:07:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:08:15 | INFO | train_inner | epoch 248:      9 / 97 loss=0.959, nll_loss=0.622, ppl=1.54, wps=24570.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23800, lr=0.00020498, gnorm=0.884, loss_scale=16, train_wall=237, gb_free=21, wall=63221
2022-03-05 04:12:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:12:06 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 14.454 | nll_loss 14.286 | ppl 19978.4 | wps 44489.4 | wpb 510.9 | bsz 1 | num_updates 23888 | best_loss 7.571
2022-03-05 04:12:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23888 updates
2022-03-05 04:12:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:12:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:12:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 248 @ 23888 updates, score 14.454) (writing took 2.1289479406550527 seconds)
2022-03-05 04:12:08 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 04:12:08 | INFO | train | epoch 248 | loss 0.959 | nll_loss 0.622 | ppl 1.54 | wps 24775.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23888 | lr 0.000204602 | gnorm 0.878 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 63454
2022-03-05 04:12:08 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 04:12:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:12:41 | INFO | train_inner | epoch 249:     13 / 97 loss=0.958, nll_loss=0.621, ppl=1.54, wps=24555.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=23900, lr=0.000204551, gnorm=0.876, loss_scale=16, train_wall=237, gb_free=21, wall=63487
2022-03-05 04:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:16:22 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 14.489 | nll_loss 14.319 | ppl 20439.6 | wps 44300.3 | wpb 510.9 | bsz 1 | num_updates 23984 | best_loss 7.571
2022-03-05 04:16:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23984 updates
2022-03-05 04:16:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:16:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:16:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 249 @ 23984 updates, score 14.489) (writing took 2.171571454964578 seconds)
2022-03-05 04:16:25 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 04:16:25 | INFO | train | epoch 249 | loss 0.957 | nll_loss 0.62 | ppl 1.54 | wps 24491.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23984 | lr 0.000204192 | gnorm 0.881 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 63711
2022-03-05 04:16:25 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 04:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:17:06 | INFO | train_inner | epoch 250:     16 / 97 loss=0.956, nll_loss=0.619, ppl=1.54, wps=24761.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.88, loss_scale=16, train_wall=235, gb_free=21, wall=63752
2022-03-05 04:18:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:20:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:20:39 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 14.468 | nll_loss 14.296 | ppl 20120.9 | wps 44687.5 | wpb 510.9 | bsz 1 | num_updates 24080 | best_loss 7.571
2022-03-05 04:20:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24080 updates
2022-03-05 04:20:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:20:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 250 @ 24080 updates, score 14.468) (writing took 2.1040737675502896 seconds)
2022-03-05 04:20:41 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 04:20:41 | INFO | train | epoch 250 | loss 0.954 | nll_loss 0.617 | ppl 1.53 | wps 24497 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24080 | lr 0.000203785 | gnorm 0.87 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 63967
2022-03-05 04:20:41 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 04:20:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:21:33 | INFO | train_inner | epoch 251:     20 / 97 loss=0.954, nll_loss=0.617, ppl=1.53, wps=24543.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.87, loss_scale=16, train_wall=237, gb_free=21, wall=64019
2022-03-05 04:24:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:24:56 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 14.493 | nll_loss 14.323 | ppl 20493.7 | wps 44598.4 | wpb 510.9 | bsz 1 | num_updates 24176 | best_loss 7.571
2022-03-05 04:24:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24176 updates
2022-03-05 04:24:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:24:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:24:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 251 @ 24176 updates, score 14.493) (writing took 2.1310140816494823 seconds)
2022-03-05 04:24:58 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 04:24:58 | INFO | train | epoch 251 | loss 0.953 | nll_loss 0.616 | ppl 1.53 | wps 24501.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24176 | lr 0.00020338 | gnorm 0.875 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 64224
2022-03-05 04:24:58 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 04:24:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:26:00 | INFO | train_inner | epoch 252:     24 / 97 loss=0.952, nll_loss=0.615, ppl=1.53, wps=24528.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.876, loss_scale=16, train_wall=237, gb_free=21, wall=64286
2022-03-05 04:29:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:29:12 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 14.438 | nll_loss 14.267 | ppl 19708.4 | wps 44780.8 | wpb 510.9 | bsz 1 | num_updates 24273 | best_loss 7.571
2022-03-05 04:29:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24273 updates
2022-03-05 04:29:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:29:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:29:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 252 @ 24273 updates, score 14.438) (writing took 2.1675388971343637 seconds)
2022-03-05 04:29:14 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 04:29:14 | INFO | train | epoch 252 | loss 0.951 | nll_loss 0.615 | ppl 1.53 | wps 24748.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24273 | lr 0.000202973 | gnorm 0.881 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 64481
2022-03-05 04:29:15 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 04:29:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:30:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:30:27 | INFO | train_inner | epoch 253:     28 / 97 loss=0.95, nll_loss=0.614, ppl=1.53, wps=24534.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.88, loss_scale=16, train_wall=237, gb_free=21, wall=64553
2022-03-05 04:33:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:33:29 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 14.485 | nll_loss 14.316 | ppl 20389.7 | wps 44861.8 | wpb 510.9 | bsz 1 | num_updates 24369 | best_loss 7.571
2022-03-05 04:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24369 updates
2022-03-05 04:33:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:33:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 253 @ 24369 updates, score 14.485) (writing took 2.1251824209466577 seconds)
2022-03-05 04:33:31 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 04:33:31 | INFO | train | epoch 253 | loss 0.948 | nll_loss 0.612 | ppl 1.53 | wps 24496.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24369 | lr 0.000202573 | gnorm 0.876 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 64737
2022-03-05 04:33:31 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 04:33:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:34:51 | INFO | train_inner | epoch 254:     31 / 97 loss=0.947, nll_loss=0.611, ppl=1.53, wps=24771.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.873, loss_scale=16, train_wall=235, gb_free=21, wall=64817
2022-03-05 04:36:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:37:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:37:46 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 14.462 | nll_loss 14.294 | ppl 20083.7 | wps 44724.5 | wpb 510.9 | bsz 1 | num_updates 24465 | best_loss 7.571
2022-03-05 04:37:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24465 updates
2022-03-05 04:37:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:37:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:37:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 254 @ 24465 updates, score 14.462) (writing took 2.2368166679516435 seconds)
2022-03-05 04:37:48 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 04:37:48 | INFO | train | epoch 254 | loss 0.946 | nll_loss 0.61 | ppl 1.53 | wps 24484.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24465 | lr 0.000202175 | gnorm 0.878 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 64994
2022-03-05 04:37:48 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 04:37:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:39:18 | INFO | train_inner | epoch 255:     35 / 97 loss=0.946, nll_loss=0.609, ppl=1.53, wps=24522.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.878, loss_scale=16, train_wall=237, gb_free=21, wall=65084
2022-03-05 04:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:41:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:42:03 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 14.516 | nll_loss 14.347 | ppl 20842.3 | wps 44672.7 | wpb 510.9 | bsz 1 | num_updates 24561 | best_loss 7.571
2022-03-05 04:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24561 updates
2022-03-05 04:42:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:42:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 255 @ 24561 updates, score 14.516) (writing took 2.14936135802418 seconds)
2022-03-05 04:42:05 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 04:42:05 | INFO | train | epoch 255 | loss 0.944 | nll_loss 0.608 | ppl 1.52 | wps 24483.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24561 | lr 0.000201779 | gnorm 0.873 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 65251
2022-03-05 04:42:05 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 04:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:43:45 | INFO | train_inner | epoch 256:     39 / 97 loss=0.944, nll_loss=0.607, ppl=1.52, wps=24519.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.872, loss_scale=16, train_wall=237, gb_free=21, wall=65351
2022-03-05 04:46:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:46:19 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 14.533 | nll_loss 14.364 | ppl 21085.2 | wps 44885.8 | wpb 510.9 | bsz 1 | num_updates 24658 | best_loss 7.571
2022-03-05 04:46:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24658 updates
2022-03-05 04:46:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:46:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:46:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 256 @ 24658 updates, score 14.533) (writing took 2.1231957050040364 seconds)
2022-03-05 04:46:21 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 04:46:21 | INFO | train | epoch 256 | loss 0.943 | nll_loss 0.607 | ppl 1.52 | wps 24751.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24658 | lr 0.000201382 | gnorm 0.873 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 65508
2022-03-05 04:46:21 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 04:46:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:47:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:48:12 | INFO | train_inner | epoch 257:     43 / 97 loss=0.94, nll_loss=0.604, ppl=1.52, wps=24542.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.87, loss_scale=16, train_wall=237, gb_free=21, wall=65618
2022-03-05 04:50:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:50:36 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 14.474 | nll_loss 14.303 | ppl 20219.7 | wps 44906.2 | wpb 510.9 | bsz 1 | num_updates 24754 | best_loss 7.571
2022-03-05 04:50:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24754 updates
2022-03-05 04:50:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:50:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:50:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 257 @ 24754 updates, score 14.474) (writing took 2.15871340688318 seconds)
2022-03-05 04:50:38 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 04:50:38 | INFO | train | epoch 257 | loss 0.941 | nll_loss 0.605 | ppl 1.52 | wps 24499.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24754 | lr 0.000200991 | gnorm 0.868 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 65764
2022-03-05 04:50:38 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 04:50:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:52:36 | INFO | train_inner | epoch 258:     46 / 97 loss=0.941, nll_loss=0.605, ppl=1.52, wps=24768.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.87, loss_scale=16, train_wall=235, gb_free=21, wall=65883
2022-03-05 04:53:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:54:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:54:53 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 14.521 | nll_loss 14.352 | ppl 20918.3 | wps 44668 | wpb 510.9 | bsz 1 | num_updates 24850 | best_loss 7.571
2022-03-05 04:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24850 updates
2022-03-05 04:54:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:54:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 258 @ 24850 updates, score 14.521) (writing took 2.0870087817311287 seconds)
2022-03-05 04:54:55 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 04:54:55 | INFO | train | epoch 258 | loss 0.939 | nll_loss 0.603 | ppl 1.52 | wps 24500.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24850 | lr 0.000200603 | gnorm 0.882 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 66021
2022-03-05 04:54:55 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 04:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:57:03 | INFO | train_inner | epoch 259:     50 / 97 loss=0.937, nll_loss=0.601, ppl=1.52, wps=24537.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.877, loss_scale=16, train_wall=237, gb_free=21, wall=66150
2022-03-05 04:59:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:59:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:59:09 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 14.543 | nll_loss 14.375 | ppl 21243 | wps 45000.2 | wpb 510.9 | bsz 1 | num_updates 24946 | best_loss 7.571
2022-03-05 04:59:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24946 updates
2022-03-05 04:59:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:59:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:59:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 259 @ 24946 updates, score 14.543) (writing took 2.1725930413231254 seconds)
2022-03-05 04:59:11 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 04:59:11 | INFO | train | epoch 259 | loss 0.937 | nll_loss 0.6 | ppl 1.52 | wps 24514.6 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 24946 | lr 0.000200216 | gnorm 0.867 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 66277
2022-03-05 04:59:11 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 04:59:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:01:30 | INFO | train_inner | epoch 260:     54 / 97 loss=0.937, nll_loss=0.601, ppl=1.52, wps=24553.3, ups=0.37, wpb=65531.7, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.87, loss_scale=16, train_wall=237, gb_free=21, wall=66416
2022-03-05 05:03:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:03:26 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 14.527 | nll_loss 14.358 | ppl 20997.4 | wps 44877.7 | wpb 510.9 | bsz 1 | num_updates 25043 | best_loss 7.571
2022-03-05 05:03:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25043 updates
2022-03-05 05:03:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:03:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:03:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 260 @ 25043 updates, score 14.527) (writing took 2.152943218126893 seconds)
2022-03-05 05:03:28 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 05:03:28 | INFO | train | epoch 260 | loss 0.935 | nll_loss 0.599 | ppl 1.51 | wps 24754.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25043 | lr 0.000199828 | gnorm 0.866 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 66534
2022-03-05 05:03:28 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 05:03:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:05:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:05:57 | INFO | train_inner | epoch 261:     58 / 97 loss=0.934, nll_loss=0.597, ppl=1.51, wps=24547.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.867, loss_scale=16, train_wall=237, gb_free=21, wall=66683
2022-03-05 05:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:07:42 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 14.532 | nll_loss 14.364 | ppl 21082.8 | wps 45184.9 | wpb 510.9 | bsz 1 | num_updates 25139 | best_loss 7.571
2022-03-05 05:07:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25139 updates
2022-03-05 05:07:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:07:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 261 @ 25139 updates, score 14.532) (writing took 2.1768973916769028 seconds)
2022-03-05 05:07:44 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 05:07:44 | INFO | train | epoch 261 | loss 0.933 | nll_loss 0.596 | ppl 1.51 | wps 24538.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25139 | lr 0.000199446 | gnorm 0.865 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66790
2022-03-05 05:07:44 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 05:07:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:10:21 | INFO | train_inner | epoch 262:     61 / 97 loss=0.933, nll_loss=0.597, ppl=1.51, wps=24854.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.876, loss_scale=16, train_wall=234, gb_free=21, wall=66947
2022-03-05 05:10:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:11:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:11:58 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 14.551 | nll_loss 14.383 | ppl 21370.8 | wps 44349.7 | wpb 510.9 | bsz 1 | num_updates 25235 | best_loss 7.571
2022-03-05 05:11:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25235 updates
2022-03-05 05:11:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:12:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 262 @ 25235 updates, score 14.551) (writing took 2.1893690787255764 seconds)
2022-03-05 05:12:00 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 05:12:00 | INFO | train | epoch 262 | loss 0.933 | nll_loss 0.597 | ppl 1.51 | wps 24578.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25235 | lr 0.000199067 | gnorm 0.881 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67046
2022-03-05 05:12:00 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 05:12:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:14:47 | INFO | train_inner | epoch 263:     65 / 97 loss=0.931, nll_loss=0.595, ppl=1.51, wps=24607.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.87, loss_scale=16, train_wall=236, gb_free=21, wall=67213
2022-03-05 05:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:16:14 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 14.547 | nll_loss 14.378 | ppl 21292.7 | wps 44575.1 | wpb 510.9 | bsz 1 | num_updates 25332 | best_loss 7.571
2022-03-05 05:16:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25332 updates
2022-03-05 05:16:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:16:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 263 @ 25332 updates, score 14.547) (writing took 2.2715582363307476 seconds)
2022-03-05 05:16:16 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 05:16:16 | INFO | train | epoch 263 | loss 0.93 | nll_loss 0.594 | ppl 1.51 | wps 24796.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25332 | lr 0.000198685 | gnorm 0.867 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67302
2022-03-05 05:16:16 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 05:16:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:16:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:19:13 | INFO | train_inner | epoch 264:     69 / 97 loss=0.929, nll_loss=0.592, ppl=1.51, wps=24564.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.861, loss_scale=16, train_wall=237, gb_free=21, wall=67480
2022-03-05 05:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:20:30 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 14.511 | nll_loss 14.341 | ppl 20751.2 | wps 45046.4 | wpb 510.9 | bsz 1 | num_updates 25428 | best_loss 7.571
2022-03-05 05:20:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25428 updates
2022-03-05 05:20:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:20:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:20:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 264 @ 25428 updates, score 14.511) (writing took 2.221594087779522 seconds)
2022-03-05 05:20:32 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 05:20:32 | INFO | train | epoch 264 | loss 0.927 | nll_loss 0.591 | ppl 1.51 | wps 24548.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25428 | lr 0.00019831 | gnorm 0.859 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67558
2022-03-05 05:20:32 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 05:20:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:22:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:23:39 | INFO | train_inner | epoch 265:     73 / 97 loss=0.928, nll_loss=0.592, ppl=1.51, wps=24627.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.865, loss_scale=16, train_wall=236, gb_free=21, wall=67745
2022-03-05 05:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:46 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 14.53 | nll_loss 14.36 | ppl 21024.5 | wps 45250.6 | wpb 510.9 | bsz 1 | num_updates 25524 | best_loss 7.571
2022-03-05 05:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25524 updates
2022-03-05 05:24:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:24:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:24:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 265 @ 25524 updates, score 14.53) (writing took 2.2800320498645306 seconds)
2022-03-05 05:24:48 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 05:24:48 | INFO | train | epoch 265 | loss 0.927 | nll_loss 0.591 | ppl 1.51 | wps 24591.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25524 | lr 0.000197936 | gnorm 0.866 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67814
2022-03-05 05:24:48 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 05:24:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:28:03 | INFO | train_inner | epoch 266:     76 / 97 loss=0.926, nll_loss=0.589, ppl=1.5, wps=24842.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.866, loss_scale=32, train_wall=234, gb_free=21, wall=68009
2022-03-05 05:28:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:28:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:29:02 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 14.589 | nll_loss 14.421 | ppl 21934.7 | wps 44496.6 | wpb 510.9 | bsz 1 | num_updates 25620 | best_loss 7.571
2022-03-05 05:29:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25620 updates
2022-03-05 05:29:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:29:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:29:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 266 @ 25620 updates, score 14.589) (writing took 2.3549492210149765 seconds)
2022-03-05 05:29:04 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 05:29:04 | INFO | train | epoch 266 | loss 0.925 | nll_loss 0.588 | ppl 1.5 | wps 24540.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25620 | lr 0.000197565 | gnorm 0.863 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68070
2022-03-05 05:29:04 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 05:29:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:32:30 | INFO | train_inner | epoch 267:     80 / 97 loss=0.923, nll_loss=0.587, ppl=1.5, wps=24538.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.85, loss_scale=16, train_wall=237, gb_free=21, wall=68276
2022-03-05 05:33:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:33:18 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 14.55 | nll_loss 14.384 | ppl 21378.1 | wps 44522.6 | wpb 510.9 | bsz 1 | num_updates 25717 | best_loss 7.571
2022-03-05 05:33:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25717 updates
2022-03-05 05:33:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:33:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:33:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 267 @ 25717 updates, score 14.55) (writing took 2.357181828469038 seconds)
2022-03-05 05:33:21 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 05:33:21 | INFO | train | epoch 267 | loss 0.924 | nll_loss 0.587 | ppl 1.5 | wps 24755.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25717 | lr 0.000197192 | gnorm 0.848 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68327
2022-03-05 05:33:21 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 05:33:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:34:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:36:56 | INFO | train_inner | epoch 268:     84 / 97 loss=0.923, nll_loss=0.587, ppl=1.5, wps=24601.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.848, loss_scale=16, train_wall=236, gb_free=21, wall=68542
2022-03-05 05:37:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:37:34 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 14.571 | nll_loss 14.404 | ppl 21675.4 | wps 44735.6 | wpb 510.9 | bsz 1 | num_updates 25813 | best_loss 7.571
2022-03-05 05:37:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25813 updates
2022-03-05 05:37:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:37:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:37:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 268 @ 25813 updates, score 14.571) (writing took 2.486288107931614 seconds)
2022-03-05 05:37:37 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 05:37:37 | INFO | train | epoch 268 | loss 0.921 | nll_loss 0.585 | ppl 1.5 | wps 24562.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25813 | lr 0.000196825 | gnorm 0.848 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68583
2022-03-05 05:37:37 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 05:37:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:40:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:41:22 | INFO | train_inner | epoch 269:     88 / 97 loss=0.922, nll_loss=0.586, ppl=1.5, wps=24600.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.858, loss_scale=16, train_wall=236, gb_free=21, wall=68808
2022-03-05 05:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:41:50 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 14.579 | nll_loss 14.412 | ppl 21796.2 | wps 45330.8 | wpb 510.9 | bsz 1 | num_updates 25909 | best_loss 7.571
2022-03-05 05:41:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25909 updates
2022-03-05 05:41:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:41:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:41:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 269 @ 25909 updates, score 14.579) (writing took 2.516002055257559 seconds)
2022-03-05 05:41:53 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 05:41:53 | INFO | train | epoch 269 | loss 0.92 | nll_loss 0.584 | ppl 1.5 | wps 24560.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25909 | lr 0.00019646 | gnorm 0.859 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68839
2022-03-05 05:41:53 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 05:41:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:45:46 | INFO | train_inner | epoch 270:     91 / 97 loss=0.919, nll_loss=0.583, ppl=1.5, wps=24813.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.869, loss_scale=16, train_wall=234, gb_free=21, wall=69072
2022-03-05 05:46:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:46:06 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 14.562 | nll_loss 14.393 | ppl 21517.7 | wps 44731.2 | wpb 510.9 | bsz 1 | num_updates 26006 | best_loss 7.571
2022-03-05 05:46:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26006 updates
2022-03-05 05:46:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:46:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:46:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 270 @ 26006 updates, score 14.562) (writing took 2.6111808279529214 seconds)
2022-03-05 05:46:09 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 05:46:09 | INFO | train | epoch 270 | loss 0.918 | nll_loss 0.582 | ppl 1.5 | wps 24770.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26006 | lr 0.000196094 | gnorm 0.867 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 69095
2022-03-05 05:46:09 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 05:46:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:46:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:50:13 | INFO | train_inner | epoch 271:     95 / 97 loss=0.917, nll_loss=0.581, ppl=1.5, wps=24514.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=26100, lr=0.00019574, gnorm=0.858, loss_scale=16, train_wall=237, gb_free=21, wall=69340
2022-03-05 05:50:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:50:23 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 14.629 | nll_loss 14.46 | ppl 22536 | wps 44425.1 | wpb 510.9 | bsz 1 | num_updates 26102 | best_loss 7.571
2022-03-05 05:50:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26102 updates
2022-03-05 05:50:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 271 @ 26102 updates, score 14.629) (writing took 2.5494944229722023 seconds)
2022-03-05 05:50:26 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 05:50:26 | INFO | train | epoch 271 | loss 0.916 | nll_loss 0.58 | ppl 1.49 | wps 24475.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26102 | lr 0.000195733 | gnorm 0.859 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69352
2022-03-05 05:50:26 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 05:50:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:52:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:54:40 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 14.557 | nll_loss 14.39 | ppl 21476.2 | wps 44489.7 | wpb 510.9 | bsz 1 | num_updates 26198 | best_loss 7.571
2022-03-05 05:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26198 updates
2022-03-05 05:54:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:54:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 272 @ 26198 updates, score 14.557) (writing took 2.6041260734200478 seconds)
2022-03-05 05:54:42 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 05:54:42 | INFO | train | epoch 272 | loss 0.916 | nll_loss 0.579 | ppl 1.49 | wps 24526.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26198 | lr 0.000195374 | gnorm 0.858 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69608
2022-03-05 05:54:42 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 05:54:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:54:47 | INFO | train_inner | epoch 273:      2 / 97 loss=0.916, nll_loss=0.58, ppl=1.49, wps=23880.2, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=26200, lr=0.000195366, gnorm=0.858, loss_scale=16, train_wall=236, gb_free=21, wall=69614
2022-03-05 05:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:58:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:58:56 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 14.6 | nll_loss 14.433 | ppl 22113.1 | wps 44985.6 | wpb 510.9 | bsz 1 | num_updates 26294 | best_loss 7.571
2022-03-05 05:58:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26294 updates
2022-03-05 05:58:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:58:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:58:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 273 @ 26294 updates, score 14.6) (writing took 2.8178597120568156 seconds)
2022-03-05 05:58:59 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 05:58:59 | INFO | train | epoch 273 | loss 0.914 | nll_loss 0.577 | ppl 1.49 | wps 24535.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26294 | lr 0.000195017 | gnorm 0.864 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 69865
2022-03-05 05:58:59 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 05:58:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:59:14 | INFO | train_inner | epoch 274:      6 / 97 loss=0.913, nll_loss=0.577, ppl=1.49, wps=24573.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26300, lr=0.000194994, gnorm=0.863, loss_scale=8, train_wall=236, gb_free=21, wall=69880
2022-03-05 06:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:03:12 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 14.572 | nll_loss 14.405 | ppl 21700.9 | wps 44961.8 | wpb 510.9 | bsz 1 | num_updates 26391 | best_loss 7.571
2022-03-05 06:03:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26391 updates
2022-03-05 06:03:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:03:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:03:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 274 @ 26391 updates, score 14.572) (writing took 2.6115268329158425 seconds)
2022-03-05 06:03:15 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 06:03:15 | INFO | train | epoch 274 | loss 0.912 | nll_loss 0.576 | ppl 1.49 | wps 24784.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26391 | lr 0.000194658 | gnorm 0.861 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70121
2022-03-05 06:03:15 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 06:03:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:03:38 | INFO | train_inner | epoch 275:      9 / 97 loss=0.911, nll_loss=0.575, ppl=1.49, wps=24803.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.86, loss_scale=16, train_wall=234, gb_free=21, wall=70144
2022-03-05 06:07:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:07:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:07:29 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 14.526 | nll_loss 14.358 | ppl 21001.4 | wps 44557.7 | wpb 510.9 | bsz 1 | num_updates 26487 | best_loss 7.571
2022-03-05 06:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26487 updates
2022-03-05 06:07:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 275 @ 26487 updates, score 14.526) (writing took 2.6427533673122525 seconds)
2022-03-05 06:07:32 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 06:07:32 | INFO | train | epoch 275 | loss 0.909 | nll_loss 0.573 | ppl 1.49 | wps 24494.5 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 26487 | lr 0.000194305 | gnorm 0.846 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70378
2022-03-05 06:07:32 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 06:07:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:08:05 | INFO | train_inner | epoch 276:     13 / 97 loss=0.908, nll_loss=0.572, ppl=1.49, wps=24533.8, ups=0.37, wpb=65533.9, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.846, loss_scale=16, train_wall=237, gb_free=21, wall=70411
2022-03-05 06:11:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:11:46 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 14.623 | nll_loss 14.458 | ppl 22509.3 | wps 44715.9 | wpb 510.9 | bsz 1 | num_updates 26584 | best_loss 7.571
2022-03-05 06:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26584 updates
2022-03-05 06:11:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:11:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:11:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 276 @ 26584 updates, score 14.623) (writing took 2.482212177477777 seconds)
2022-03-05 06:11:48 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 06:11:48 | INFO | train | epoch 276 | loss 0.909 | nll_loss 0.572 | ppl 1.49 | wps 24761.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26584 | lr 0.00019395 | gnorm 0.85 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70634
2022-03-05 06:11:48 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 06:11:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:12:29 | INFO | train_inner | epoch 277:     16 / 97 loss=0.908, nll_loss=0.572, ppl=1.49, wps=24789, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26600, lr=0.000193892, gnorm=0.849, loss_scale=16, train_wall=234, gb_free=21, wall=70676
2022-03-05 06:13:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:15:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:16:02 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 14.643 | nll_loss 14.476 | ppl 22793 | wps 45318.7 | wpb 510.9 | bsz 1 | num_updates 26680 | best_loss 7.571
2022-03-05 06:16:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26680 updates
2022-03-05 06:16:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:16:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:16:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 277 @ 26680 updates, score 14.643) (writing took 2.614233147352934 seconds)
2022-03-05 06:16:04 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 06:16:04 | INFO | train | epoch 277 | loss 0.907 | nll_loss 0.57 | ppl 1.48 | wps 24563.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26680 | lr 0.000193601 | gnorm 0.849 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70890
2022-03-05 06:16:04 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 06:16:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:16:55 | INFO | train_inner | epoch 278:     20 / 97 loss=0.906, nll_loss=0.569, ppl=1.48, wps=24618.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.849, loss_scale=16, train_wall=236, gb_free=21, wall=70942
2022-03-05 06:19:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:20:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:20:17 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 14.639 | nll_loss 14.471 | ppl 22716.5 | wps 45405.2 | wpb 510.9 | bsz 1 | num_updates 26776 | best_loss 7.571
2022-03-05 06:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26776 updates
2022-03-05 06:20:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:20:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:20:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 278 @ 26776 updates, score 14.639) (writing took 2.530644095502794 seconds)
2022-03-05 06:20:19 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 06:20:19 | INFO | train | epoch 278 | loss 0.905 | nll_loss 0.569 | ppl 1.48 | wps 24625.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26776 | lr 0.000193253 | gnorm 0.853 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 71146
2022-03-05 06:20:20 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 06:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:21:21 | INFO | train_inner | epoch 279:     24 / 97 loss=0.904, nll_loss=0.568, ppl=1.48, wps=24665.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.856, loss_scale=16, train_wall=236, gb_free=21, wall=71207
2022-03-05 06:24:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:24:32 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 14.589 | nll_loss 14.421 | ppl 21937.5 | wps 45323.9 | wpb 510.9 | bsz 1 | num_updates 26873 | best_loss 7.571
2022-03-05 06:24:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26873 updates
2022-03-05 06:24:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:24:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:24:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 279 @ 26873 updates, score 14.589) (writing took 2.644025039859116 seconds)
2022-03-05 06:24:35 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 06:24:35 | INFO | train | epoch 279 | loss 0.903 | nll_loss 0.567 | ppl 1.48 | wps 24872.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26873 | lr 0.000192904 | gnorm 0.849 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 71401
2022-03-05 06:24:35 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 06:24:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:24:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:25:47 | INFO | train_inner | epoch 280:     28 / 97 loss=0.903, nll_loss=0.566, ppl=1.48, wps=24653.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.843, loss_scale=16, train_wall=236, gb_free=21, wall=71473
2022-03-05 06:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:28:48 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 14.631 | nll_loss 14.464 | ppl 22598.6 | wps 45164.9 | wpb 510.9 | bsz 1 | num_updates 26969 | best_loss 7.571
2022-03-05 06:28:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26969 updates
2022-03-05 06:28:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:28:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 280 @ 26969 updates, score 14.631) (writing took 2.5388299580663443 seconds)
2022-03-05 06:28:50 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 06:28:50 | INFO | train | epoch 280 | loss 0.901 | nll_loss 0.565 | ppl 1.48 | wps 24621.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26969 | lr 0.000192561 | gnorm 0.844 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 71656
2022-03-05 06:28:50 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 06:28:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:30:10 | INFO | train_inner | epoch 281:     31 / 97 loss=0.902, nll_loss=0.566, ppl=1.48, wps=24903.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.846, loss_scale=16, train_wall=233, gb_free=21, wall=71736
2022-03-05 06:30:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:33:03 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 14.646 | nll_loss 14.48 | ppl 22848.5 | wps 45320 | wpb 510.9 | bsz 1 | num_updates 27065 | best_loss 7.571
2022-03-05 06:33:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27065 updates
2022-03-05 06:33:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:33:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:33:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 281 @ 27065 updates, score 14.646) (writing took 2.5414998019114137 seconds)
2022-03-05 06:33:05 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 06:33:05 | INFO | train | epoch 281 | loss 0.9 | nll_loss 0.564 | ppl 1.48 | wps 24632.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27065 | lr 0.000192219 | gnorm 0.86 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 71912
2022-03-05 06:33:06 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 06:33:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:34:35 | INFO | train_inner | epoch 282:     35 / 97 loss=0.898, nll_loss=0.562, ppl=1.48, wps=24668.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.857, loss_scale=16, train_wall=236, gb_free=21, wall=72001
2022-03-05 06:36:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:37:18 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 14.651 | nll_loss 14.484 | ppl 22918.4 | wps 45258.8 | wpb 510.9 | bsz 1 | num_updates 27161 | best_loss 7.571
2022-03-05 06:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27161 updates
2022-03-05 06:37:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:37:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:37:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 282 @ 27161 updates, score 14.651) (writing took 2.5443782852962613 seconds)
2022-03-05 06:37:21 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 06:37:21 | INFO | train | epoch 282 | loss 0.899 | nll_loss 0.563 | ppl 1.48 | wps 24624.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27161 | lr 0.000191879 | gnorm 0.848 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72167
2022-03-05 06:37:21 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 06:37:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:39:01 | INFO | train_inner | epoch 283:     39 / 97 loss=0.899, nll_loss=0.563, ppl=1.48, wps=24656.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.85, loss_scale=16, train_wall=236, gb_free=21, wall=72267
2022-03-05 06:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:41:34 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 14.625 | nll_loss 14.458 | ppl 22504 | wps 45162.1 | wpb 510.9 | bsz 1 | num_updates 27258 | best_loss 7.571
2022-03-05 06:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27258 updates
2022-03-05 06:41:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:41:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:41:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 283 @ 27258 updates, score 14.625) (writing took 2.595046754926443 seconds)
2022-03-05 06:41:36 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 06:41:36 | INFO | train | epoch 283 | loss 0.897 | nll_loss 0.561 | ppl 1.48 | wps 24872 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27258 | lr 0.000191537 | gnorm 0.843 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72422
2022-03-05 06:41:36 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 06:41:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:42:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:43:26 | INFO | train_inner | epoch 284:     43 / 97 loss=0.896, nll_loss=0.56, ppl=1.47, wps=24668.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.843, loss_scale=16, train_wall=236, gb_free=21, wall=72532
2022-03-05 06:45:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:45:49 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 14.575 | nll_loss 14.409 | ppl 21748.2 | wps 45248.6 | wpb 510.9 | bsz 1 | num_updates 27354 | best_loss 7.571
2022-03-05 06:45:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27354 updates
2022-03-05 06:45:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:45:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:45:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 284 @ 27354 updates, score 14.575) (writing took 2.688658880069852 seconds)
2022-03-05 06:45:52 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 06:45:52 | INFO | train | epoch 284 | loss 0.896 | nll_loss 0.56 | ppl 1.47 | wps 24625.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27354 | lr 0.000191201 | gnorm 0.844 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72678
2022-03-05 06:45:52 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 06:45:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:47:49 | INFO | train_inner | epoch 285:     46 / 97 loss=0.895, nll_loss=0.558, ppl=1.47, wps=24894.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.841, loss_scale=16, train_wall=233, gb_free=21, wall=72795
2022-03-05 06:48:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:49:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:50:04 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 14.615 | nll_loss 14.449 | ppl 22368.6 | wps 44998.1 | wpb 510.9 | bsz 1 | num_updates 27450 | best_loss 7.571
2022-03-05 06:50:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27450 updates
2022-03-05 06:50:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:50:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:50:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 285 @ 27450 updates, score 14.615) (writing took 2.598759445361793 seconds)
2022-03-05 06:50:07 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 06:50:07 | INFO | train | epoch 285 | loss 0.894 | nll_loss 0.558 | ppl 1.47 | wps 24615.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27450 | lr 0.000190866 | gnorm 0.845 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72933
2022-03-05 06:50:07 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 06:50:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:52:15 | INFO | train_inner | epoch 286:     50 / 97 loss=0.894, nll_loss=0.558, ppl=1.47, wps=24653.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27500, lr=0.000190693, gnorm=0.849, loss_scale=16, train_wall=236, gb_free=21, wall=73061
2022-03-05 06:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:54:20 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 14.635 | nll_loss 14.468 | ppl 22665.8 | wps 45196.3 | wpb 510.9 | bsz 1 | num_updates 27546 | best_loss 7.571
2022-03-05 06:54:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27546 updates
2022-03-05 06:54:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:54:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 286 @ 27546 updates, score 14.635) (writing took 2.6137750623747706 seconds)
2022-03-05 06:54:22 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 06:54:22 | INFO | train | epoch 286 | loss 0.893 | nll_loss 0.557 | ppl 1.47 | wps 24621 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27546 | lr 0.000190533 | gnorm 0.845 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73188
2022-03-05 06:54:22 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 06:54:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:56:40 | INFO | train_inner | epoch 287:     54 / 97 loss=0.891, nll_loss=0.555, ppl=1.47, wps=24659.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.837, loss_scale=16, train_wall=236, gb_free=21, wall=73327
2022-03-05 06:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:58:35 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 14.652 | nll_loss 14.486 | ppl 22953.2 | wps 45218.4 | wpb 510.9 | bsz 1 | num_updates 27643 | best_loss 7.571
2022-03-05 06:58:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27643 updates
2022-03-05 06:58:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:58:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:58:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 287 @ 27643 updates, score 14.652) (writing took 2.56368556804955 seconds)
2022-03-05 06:58:38 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 06:58:38 | INFO | train | epoch 287 | loss 0.891 | nll_loss 0.555 | ppl 1.47 | wps 24872.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27643 | lr 0.000190199 | gnorm 0.839 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73444
2022-03-05 06:58:38 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 06:58:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:59:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:01:06 | INFO | train_inner | epoch 288:     58 / 97 loss=0.892, nll_loss=0.556, ppl=1.47, wps=24657, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.85, loss_scale=16, train_wall=236, gb_free=21, wall=73592
2022-03-05 07:02:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:02:50 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 14.652 | nll_loss 14.487 | ppl 22967.3 | wps 45143.3 | wpb 510.9 | bsz 1 | num_updates 27739 | best_loss 7.571
2022-03-05 07:02:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27739 updates
2022-03-05 07:02:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:02:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:02:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 288 @ 27739 updates, score 14.652) (writing took 2.546630997210741 seconds)
2022-03-05 07:02:53 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 07:02:53 | INFO | train | epoch 288 | loss 0.89 | nll_loss 0.554 | ppl 1.47 | wps 24625 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27739 | lr 0.000189869 | gnorm 0.853 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73699
2022-03-05 07:02:53 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 07:02:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:05:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:05:32 | INFO | train_inner | epoch 289:     62 / 97 loss=0.888, nll_loss=0.552, ppl=1.47, wps=24667.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.842, loss_scale=16, train_wall=236, gb_free=21, wall=73858
2022-03-05 07:07:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:07:06 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 14.649 | nll_loss 14.485 | ppl 22926 | wps 45261.2 | wpb 510.9 | bsz 1 | num_updates 27835 | best_loss 7.571
2022-03-05 07:07:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27835 updates
2022-03-05 07:07:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 289 @ 27835 updates, score 14.649) (writing took 2.5873347837477922 seconds)
2022-03-05 07:07:08 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 07:07:08 | INFO | train | epoch 289 | loss 0.889 | nll_loss 0.553 | ppl 1.47 | wps 24624.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27835 | lr 0.000189542 | gnorm 0.838 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73954
2022-03-05 07:07:08 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 07:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:09:55 | INFO | train_inner | epoch 290:     65 / 97 loss=0.888, nll_loss=0.552, ppl=1.47, wps=24899.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.839, loss_scale=16, train_wall=233, gb_free=21, wall=74121
2022-03-05 07:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:11:21 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 14.656 | nll_loss 14.491 | ppl 23023 | wps 45140.9 | wpb 510.9 | bsz 1 | num_updates 27932 | best_loss 7.571
2022-03-05 07:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27932 updates
2022-03-05 07:11:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 290 @ 27932 updates, score 14.656) (writing took 2.61122131254524 seconds)
2022-03-05 07:11:24 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 07:11:24 | INFO | train | epoch 290 | loss 0.887 | nll_loss 0.551 | ppl 1.46 | wps 24869.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27932 | lr 0.000189212 | gnorm 0.838 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 74210
2022-03-05 07:11:24 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 07:11:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:14:20 | INFO | train_inner | epoch 291:     69 / 97 loss=0.886, nll_loss=0.55, ppl=1.46, wps=24655.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.843, loss_scale=16, train_wall=236, gb_free=21, wall=74386
2022-03-05 07:15:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:15:37 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 14.643 | nll_loss 14.478 | ppl 22814.9 | wps 45316.8 | wpb 510.9 | bsz 1 | num_updates 28028 | best_loss 7.571
2022-03-05 07:15:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28028 updates
2022-03-05 07:15:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:15:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:15:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 291 @ 28028 updates, score 14.643) (writing took 2.587679739110172 seconds)
2022-03-05 07:15:39 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 07:15:39 | INFO | train | epoch 291 | loss 0.886 | nll_loss 0.55 | ppl 1.46 | wps 24617.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28028 | lr 0.000188888 | gnorm 0.84 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 74465
2022-03-05 07:15:39 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 07:15:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:17:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:18:46 | INFO | train_inner | epoch 292:     73 / 97 loss=0.885, nll_loss=0.549, ppl=1.46, wps=24662.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.832, loss_scale=16, train_wall=236, gb_free=21, wall=74652
2022-03-05 07:19:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:19:52 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 14.625 | nll_loss 14.46 | ppl 22535.2 | wps 45402.5 | wpb 510.9 | bsz 1 | num_updates 28124 | best_loss 7.571
2022-03-05 07:19:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28124 updates
2022-03-05 07:19:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:19:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:19:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 292 @ 28124 updates, score 14.625) (writing took 2.5181489884853363 seconds)
2022-03-05 07:19:54 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 07:19:54 | INFO | train | epoch 292 | loss 0.884 | nll_loss 0.548 | ppl 1.46 | wps 24637.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28124 | lr 0.000188565 | gnorm 0.833 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 74721
2022-03-05 07:19:54 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 07:19:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:23:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:23:11 | INFO | train_inner | epoch 293:     77 / 97 loss=0.884, nll_loss=0.548, ppl=1.46, wps=24674.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.839, loss_scale=16, train_wall=236, gb_free=21, wall=74917
2022-03-05 07:24:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:24:07 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 14.682 | nll_loss 14.518 | ppl 23457.4 | wps 45054.7 | wpb 510.9 | bsz 1 | num_updates 28220 | best_loss 7.571
2022-03-05 07:24:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28220 updates
2022-03-05 07:24:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:24:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:24:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 293 @ 28220 updates, score 14.682) (writing took 2.3216821756213903 seconds)
2022-03-05 07:24:09 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 07:24:09 | INFO | train | epoch 293 | loss 0.883 | nll_loss 0.548 | ppl 1.46 | wps 24647 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28220 | lr 0.000188244 | gnorm 0.842 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 74976
2022-03-05 07:24:09 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 07:24:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:27:34 | INFO | train_inner | epoch 294:     80 / 97 loss=0.882, nll_loss=0.546, ppl=1.46, wps=24910.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.839, loss_scale=16, train_wall=233, gb_free=21, wall=75180
2022-03-05 07:28:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:28:22 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 14.669 | nll_loss 14.504 | ppl 23237.1 | wps 45225.3 | wpb 510.9 | bsz 1 | num_updates 28317 | best_loss 7.571
2022-03-05 07:28:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28317 updates
2022-03-05 07:28:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:28:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:28:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 294 @ 28317 updates, score 14.669) (writing took 2.566322226077318 seconds)
2022-03-05 07:28:25 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 07:28:25 | INFO | train | epoch 294 | loss 0.881 | nll_loss 0.545 | ppl 1.46 | wps 24869.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28317 | lr 0.000187921 | gnorm 0.836 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75231
2022-03-05 07:28:25 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 07:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:29:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:31:59 | INFO | train_inner | epoch 295:     84 / 97 loss=0.881, nll_loss=0.545, ppl=1.46, wps=24686.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.832, loss_scale=16, train_wall=235, gb_free=21, wall=75446
2022-03-05 07:32:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:32:37 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 14.676 | nll_loss 14.51 | ppl 23337.8 | wps 45259.2 | wpb 510.9 | bsz 1 | num_updates 28413 | best_loss 7.571
2022-03-05 07:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28413 updates
2022-03-05 07:32:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:32:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 295 @ 28413 updates, score 14.676) (writing took 2.2790418975055218 seconds)
2022-03-05 07:32:40 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 07:32:40 | INFO | train | epoch 295 | loss 0.88 | nll_loss 0.544 | ppl 1.46 | wps 24673.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28413 | lr 0.000187604 | gnorm 0.831 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75486
2022-03-05 07:32:40 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 07:32:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:34:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:36:25 | INFO | train_inner | epoch 296:     88 / 97 loss=0.88, nll_loss=0.544, ppl=1.46, wps=24696.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.837, loss_scale=16, train_wall=236, gb_free=21, wall=75711
2022-03-05 07:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:36:52 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 14.712 | nll_loss 14.547 | ppl 23936.7 | wps 45329.3 | wpb 510.9 | bsz 1 | num_updates 28509 | best_loss 7.571
2022-03-05 07:36:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28509 updates
2022-03-05 07:36:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:36:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:36:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 296 @ 28509 updates, score 14.712) (writing took 2.266470268368721 seconds)
2022-03-05 07:36:55 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 07:36:55 | INFO | train | epoch 296 | loss 0.878 | nll_loss 0.543 | ppl 1.46 | wps 24667.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28509 | lr 0.000187288 | gnorm 0.836 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75741
2022-03-05 07:36:55 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 07:36:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:40:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:40:50 | INFO | train_inner | epoch 297:     92 / 97 loss=0.877, nll_loss=0.541, ppl=1.46, wps=24688.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28600, lr=0.000186989, gnorm=0.83, loss_scale=16, train_wall=236, gb_free=21, wall=75976
2022-03-05 07:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:41:07 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 14.689 | nll_loss 14.525 | ppl 23569.1 | wps 45211.4 | wpb 510.9 | bsz 1 | num_updates 28605 | best_loss 7.571
2022-03-05 07:41:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28605 updates
2022-03-05 07:41:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:41:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 297 @ 28605 updates, score 14.689) (writing took 2.3731853999197483 seconds)
2022-03-05 07:41:10 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 07:41:10 | INFO | train | epoch 297 | loss 0.877 | nll_loss 0.541 | ppl 1.46 | wps 24633.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28605 | lr 0.000186973 | gnorm 0.832 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75996
2022-03-05 07:41:10 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 07:41:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:45:13 | INFO | train_inner | epoch 298:     95 / 97 loss=0.877, nll_loss=0.541, ppl=1.46, wps=24910.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.845, loss_scale=16, train_wall=233, gb_free=21, wall=76239
2022-03-05 07:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:45:23 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 14.691 | nll_loss 14.528 | ppl 23624.8 | wps 45167.8 | wpb 510.9 | bsz 1 | num_updates 28702 | best_loss 7.571
2022-03-05 07:45:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28702 updates
2022-03-05 07:45:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:45:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:45:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 298 @ 28702 updates, score 14.691) (writing took 2.3227740293368697 seconds)
2022-03-05 07:45:25 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 07:45:25 | INFO | train | epoch 298 | loss 0.876 | nll_loss 0.54 | ppl 1.45 | wps 24893 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28702 | lr 0.000186657 | gnorm 0.843 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76251
2022-03-05 07:45:25 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 07:45:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:46:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:49:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:49:38 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 14.721 | nll_loss 14.555 | ppl 24074.3 | wps 45232.6 | wpb 510.9 | bsz 1 | num_updates 28798 | best_loss 7.571
2022-03-05 07:49:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28798 updates
2022-03-05 07:49:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:49:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:49:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 299 @ 28798 updates, score 14.721) (writing took 2.4144959412515163 seconds)
2022-03-05 07:49:40 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 07:49:40 | INFO | train | epoch 299 | loss 0.873 | nll_loss 0.537 | ppl 1.45 | wps 24622.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28798 | lr 0.000186345 | gnorm 0.818 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76507
2022-03-05 07:49:40 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 07:49:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:49:46 | INFO | train_inner | epoch 300:      2 / 97 loss=0.873, nll_loss=0.538, ppl=1.45, wps=23999.2, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=28800, lr=0.000186339, gnorm=0.819, loss_scale=16, train_wall=236, gb_free=21, wall=76512
2022-03-05 07:52:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:53:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:53:53 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 14.726 | nll_loss 14.562 | ppl 24185.7 | wps 45310.3 | wpb 510.9 | bsz 1 | num_updates 28894 | best_loss 7.571
2022-03-05 07:53:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28894 updates
2022-03-05 07:53:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:53:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:53:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 300 @ 28894 updates, score 14.726) (writing took 2.518463815562427 seconds)
2022-03-05 07:53:55 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 07:53:55 | INFO | train | epoch 300 | loss 0.874 | nll_loss 0.538 | ppl 1.45 | wps 24654 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28894 | lr 0.000186036 | gnorm 0.838 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76762
2022-03-05 07:53:55 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 07:53:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:54:11 | INFO | train_inner | epoch 301:      6 / 97 loss=0.873, nll_loss=0.537, ppl=1.45, wps=24693.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.836, loss_scale=16, train_wall=235, gb_free=21, wall=76777
2022-03-05 07:57:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:58:08 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 14.727 | nll_loss 14.563 | ppl 24210.6 | wps 45261.2 | wpb 510.9 | bsz 1 | num_updates 28990 | best_loss 7.571
2022-03-05 07:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 28990 updates
2022-03-05 07:58:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:58:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 301 @ 28990 updates, score 14.727) (writing took 2.356534688733518 seconds)
2022-03-05 07:58:11 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 07:58:11 | INFO | train | epoch 301 | loss 0.872 | nll_loss 0.537 | ppl 1.45 | wps 24634.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28990 | lr 0.000185727 | gnorm 0.836 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77017
2022-03-05 07:58:11 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 07:58:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:58:36 | INFO | train_inner | epoch 302:     10 / 97 loss=0.871, nll_loss=0.536, ppl=1.45, wps=24672.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29000, lr=0.000185695, gnorm=0.836, loss_scale=16, train_wall=236, gb_free=21, wall=77042
2022-03-05 08:02:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:02:23 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 14.709 | nll_loss 14.546 | ppl 23916.5 | wps 45527.1 | wpb 510.9 | bsz 1 | num_updates 29087 | best_loss 7.571
2022-03-05 08:02:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29087 updates
2022-03-05 08:02:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:02:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:02:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 302 @ 29087 updates, score 14.709) (writing took 2.4538322994485497 seconds)
2022-03-05 08:02:26 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 08:02:26 | INFO | train | epoch 302 | loss 0.871 | nll_loss 0.535 | ppl 1.45 | wps 24896.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29087 | lr 0.000185417 | gnorm 0.825 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77272
2022-03-05 08:02:26 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 08:02:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:02:59 | INFO | train_inner | epoch 303:     13 / 97 loss=0.87, nll_loss=0.534, ppl=1.45, wps=24915.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29100, lr=0.000185376, gnorm=0.825, loss_scale=16, train_wall=233, gb_free=21, wall=77305
2022-03-05 08:03:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:06:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:06:39 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 14.722 | nll_loss 14.56 | ppl 24147.2 | wps 45268.8 | wpb 510.9 | bsz 1 | num_updates 29183 | best_loss 7.571
2022-03-05 08:06:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29183 updates
2022-03-05 08:06:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:06:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:06:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 303 @ 29183 updates, score 14.722) (writing took 2.342538522556424 seconds)
2022-03-05 08:06:41 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 08:06:41 | INFO | train | epoch 303 | loss 0.869 | nll_loss 0.533 | ppl 1.45 | wps 24641.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29183 | lr 0.000185112 | gnorm 0.832 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77527
2022-03-05 08:06:41 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 08:06:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:07:24 | INFO | train_inner | epoch 304:     17 / 97 loss=0.869, nll_loss=0.533, ppl=1.45, wps=24681.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29200, lr=0.000185058, gnorm=0.833, loss_scale=16, train_wall=236, gb_free=21, wall=77571
2022-03-05 08:10:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:10:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:10:54 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 14.687 | nll_loss 14.523 | ppl 23548 | wps 45274.9 | wpb 510.9 | bsz 1 | num_updates 29279 | best_loss 7.571
2022-03-05 08:10:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29279 updates
2022-03-05 08:10:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:10:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:10:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 304 @ 29279 updates, score 14.687) (writing took 2.562463065609336 seconds)
2022-03-05 08:10:56 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 08:10:56 | INFO | train | epoch 304 | loss 0.868 | nll_loss 0.533 | ppl 1.45 | wps 24615.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29279 | lr 0.000184808 | gnorm 0.831 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77782
2022-03-05 08:10:56 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 08:10:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:11:50 | INFO | train_inner | epoch 305:     21 / 97 loss=0.867, nll_loss=0.531, ppl=1.45, wps=24659.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29300, lr=0.000184742, gnorm=0.834, loss_scale=16, train_wall=236, gb_free=21, wall=77836
2022-03-05 08:15:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:15:09 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 14.711 | nll_loss 14.548 | ppl 23961.7 | wps 45675.8 | wpb 510.9 | bsz 1 | num_updates 29376 | best_loss 7.571
2022-03-05 08:15:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29376 updates
2022-03-05 08:15:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:15:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:15:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 305 @ 29376 updates, score 14.711) (writing took 2.495312985032797 seconds)
2022-03-05 08:15:12 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 08:15:12 | INFO | train | epoch 305 | loss 0.866 | nll_loss 0.53 | ppl 1.44 | wps 24894.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29376 | lr 0.000184503 | gnorm 0.824 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78038
2022-03-05 08:15:12 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 08:15:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:16:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:16:16 | INFO | train_inner | epoch 306:     25 / 97 loss=0.865, nll_loss=0.529, ppl=1.44, wps=24667.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.818, loss_scale=16, train_wall=236, gb_free=21, wall=78102
2022-03-05 08:19:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:19:25 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 14.683 | nll_loss 14.52 | ppl 23486.8 | wps 45089.9 | wpb 510.9 | bsz 1 | num_updates 29472 | best_loss 7.571
2022-03-05 08:19:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29472 updates
2022-03-05 08:19:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:19:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:19:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 306 @ 29472 updates, score 14.683) (writing took 2.3787154480814934 seconds)
2022-03-05 08:19:27 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 08:19:27 | INFO | train | epoch 306 | loss 0.865 | nll_loss 0.529 | ppl 1.44 | wps 24617.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29472 | lr 0.000184202 | gnorm 0.82 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 78293
2022-03-05 08:19:27 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 08:19:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:20:39 | INFO | train_inner | epoch 307:     28 / 97 loss=0.865, nll_loss=0.529, ppl=1.44, wps=24889.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.821, loss_scale=16, train_wall=234, gb_free=21, wall=78365
2022-03-05 08:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:23:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:23:40 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 14.706 | nll_loss 14.542 | ppl 23863.1 | wps 45025.7 | wpb 510.9 | bsz 1 | num_updates 29568 | best_loss 7.571
2022-03-05 08:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29568 updates
2022-03-05 08:23:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:23:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:23:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 307 @ 29568 updates, score 14.706) (writing took 2.5372429005801678 seconds)
2022-03-05 08:23:42 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 08:23:42 | INFO | train | epoch 307 | loss 0.864 | nll_loss 0.528 | ppl 1.44 | wps 24598.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29568 | lr 0.000183903 | gnorm 0.831 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 78549
2022-03-05 08:23:43 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 08:23:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:25:04 | INFO | train_inner | epoch 308:     32 / 97 loss=0.864, nll_loss=0.528, ppl=1.44, wps=24651, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.83, loss_scale=16, train_wall=236, gb_free=21, wall=78630
2022-03-05 08:27:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:27:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:27:55 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 14.71 | nll_loss 14.548 | ppl 23961.4 | wps 45230.7 | wpb 510.9 | bsz 1 | num_updates 29664 | best_loss 7.571
2022-03-05 08:27:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29664 updates
2022-03-05 08:27:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:27:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:27:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 308 @ 29664 updates, score 14.71) (writing took 2.51716878823936 seconds)
2022-03-05 08:27:58 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 08:27:58 | INFO | train | epoch 308 | loss 0.862 | nll_loss 0.526 | ppl 1.44 | wps 24638.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29664 | lr 0.000183605 | gnorm 0.827 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78804
2022-03-05 08:27:58 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 08:27:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:29:30 | INFO | train_inner | epoch 309:     36 / 97 loss=0.861, nll_loss=0.525, ppl=1.44, wps=24679.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.828, loss_scale=16, train_wall=235, gb_free=21, wall=78896
2022-03-05 08:32:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:32:10 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 14.717 | nll_loss 14.553 | ppl 24037.9 | wps 45367 | wpb 510.9 | bsz 1 | num_updates 29761 | best_loss 7.571
2022-03-05 08:32:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 29761 updates
2022-03-05 08:32:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:32:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:32:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 309 @ 29761 updates, score 14.717) (writing took 2.619546825066209 seconds)
2022-03-05 08:32:13 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-03-05 08:32:13 | INFO | train | epoch 309 | loss 0.861 | nll_loss 0.526 | ppl 1.44 | wps 24893.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29761 | lr 0.000183306 | gnorm 0.829 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 79059
2022-03-05 08:32:13 | INFO | fairseq.trainer | begin training epoch 310
2022-03-05 08:32:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:33:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:33:55 | INFO | train_inner | epoch 310:     40 / 97 loss=0.861, nll_loss=0.525, ppl=1.44, wps=24686.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29800, lr=0.000183186, gnorm=0.832, loss_scale=16, train_wall=235, gb_free=21, wall=79161
2022-03-05 08:36:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:36:25 | INFO | valid | epoch 310 | valid on 'valid' subset | loss 14.71 | nll_loss 14.546 | ppl 23918.7 | wps 45240 | wpb 510.9 | bsz 1 | num_updates 29857 | best_loss 7.571
2022-03-05 08:36:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 310 @ 29857 updates
2022-03-05 08:36:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:36:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:36:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 310 @ 29857 updates, score 14.71) (writing took 2.5318013625219464 seconds)
2022-03-05 08:36:28 | INFO | fairseq_cli.train | end of epoch 310 (average epoch stats below)
2022-03-05 08:36:28 | INFO | train | epoch 310 | loss 0.861 | nll_loss 0.525 | ppl 1.44 | wps 24656.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29857 | lr 0.000183011 | gnorm 0.827 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 79314
2022-03-05 08:36:28 | INFO | fairseq.trainer | begin training epoch 311
2022-03-05 08:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:38:18 | INFO | train_inner | epoch 311:     43 / 97 loss=0.86, nll_loss=0.524, ppl=1.44, wps=24929.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=29900, lr=0.000182879, gnorm=0.822, loss_scale=16, train_wall=233, gb_free=21, wall=79424
2022-03-05 08:39:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:40:40 | INFO | valid | epoch 311 | valid on 'valid' subset | loss 14.724 | nll_loss 14.562 | ppl 24185.2 | wps 45191.6 | wpb 510.9 | bsz 1 | num_updates 29953 | best_loss 7.571
2022-03-05 08:40:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 311 @ 29953 updates
2022-03-05 08:40:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:40:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:40:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 311 @ 29953 updates, score 14.724) (writing took 2.550126686692238 seconds)
2022-03-05 08:40:43 | INFO | fairseq_cli.train | end of epoch 311 (average epoch stats below)
2022-03-05 08:40:43 | INFO | train | epoch 311 | loss 0.859 | nll_loss 0.523 | ppl 1.44 | wps 24650.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29953 | lr 0.000182717 | gnorm 0.818 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 79569
2022-03-05 08:40:43 | INFO | fairseq.trainer | begin training epoch 312
2022-03-05 08:40:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:42:43 | INFO | train_inner | epoch 312:     47 / 97 loss=0.857, nll_loss=0.522, ppl=1.44, wps=24692, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=30000, lr=0.000182574, gnorm=0.816, loss_scale=16, train_wall=235, gb_free=21, wall=79689
2022-03-05 08:44:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:44:55 | INFO | valid | epoch 312 | valid on 'valid' subset | loss 14.758 | nll_loss 14.596 | ppl 24757.5 | wps 45268 | wpb 510.9 | bsz 1 | num_updates 30050 | best_loss 7.571
2022-03-05 08:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 312 @ 30050 updates
2022-03-05 08:44:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 312 @ 30050 updates, score 14.758) (writing took 2.5283599253743887 seconds)
2022-03-05 08:44:58 | INFO | fairseq_cli.train | end of epoch 312 (average epoch stats below)
2022-03-05 08:44:58 | INFO | train | epoch 312 | loss 0.857 | nll_loss 0.522 | ppl 1.44 | wps 24915.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30050 | lr 0.000182422 | gnorm 0.825 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 79824
2022-03-05 08:44:58 | INFO | fairseq.trainer | begin training epoch 313
2022-03-05 08:44:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:45:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:47:08 | INFO | train_inner | epoch 313:     51 / 97 loss=0.858, nll_loss=0.523, ppl=1.44, wps=24703.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=30100, lr=0.000182271, gnorm=0.829, loss_scale=16, train_wall=235, gb_free=21, wall=79954
2022-03-05 08:49:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:49:10 | INFO | valid | epoch 313 | valid on 'valid' subset | loss 14.743 | nll_loss 14.58 | ppl 24497.6 | wps 45258.6 | wpb 510.9 | bsz 1 | num_updates 30146 | best_loss 7.571
2022-03-05 08:49:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 313 @ 30146 updates
2022-03-05 08:49:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:49:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 313 @ 30146 updates, score 14.743) (writing took 2.4299374045804143 seconds)
2022-03-05 08:49:13 | INFO | fairseq_cli.train | end of epoch 313 (average epoch stats below)
2022-03-05 08:49:13 | INFO | train | epoch 313 | loss 0.857 | nll_loss 0.521 | ppl 1.44 | wps 24671.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 30146 | lr 0.000182132 | gnorm 0.82 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 80079
2022-03-05 08:49:13 | INFO | fairseq.trainer | begin training epoch 314
2022-03-05 08:49:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:51:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:51:33 | INFO | train_inner | epoch 314:     55 / 97 loss=0.856, nll_loss=0.521, ppl=1.43, wps=24697.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=30200, lr=0.000181969, gnorm=0.817, loss_scale=16, train_wall=235, gb_free=21, wall=80219
2022-03-05 08:52:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:53:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:53:25 | INFO | valid | epoch 314 | valid on 'valid' subset | loss 14.766 | nll_loss 14.603 | ppl 24889.7 | wps 45342.7 | wpb 510.9 | bsz 1 | num_updates 30241 | best_loss 7.571
2022-03-05 08:53:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 314 @ 30241 updates
2022-03-05 08:53:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:53:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:53:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 314 @ 30241 updates, score 14.766) (writing took 2.456445679999888 seconds)
2022-03-05 08:53:28 | INFO | fairseq_cli.train | end of epoch 314 (average epoch stats below)
2022-03-05 08:53:28 | INFO | train | epoch 314 | loss 0.854 | nll_loss 0.519 | ppl 1.43 | wps 24403.4 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 30241 | lr 0.000181845 | gnorm 0.816 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 80334
2022-03-05 08:53:28 | INFO | fairseq.trainer | begin training epoch 315
2022-03-05 08:53:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:55:58 | INFO | train_inner | epoch 315:     59 / 97 loss=0.853, nll_loss=0.517, ppl=1.43, wps=24701.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30300, lr=0.000181668, gnorm=0.812, loss_scale=8, train_wall=235, gb_free=21, wall=80485
2022-03-05 08:57:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:57:40 | INFO | valid | epoch 315 | valid on 'valid' subset | loss 14.749 | nll_loss 14.586 | ppl 24592.9 | wps 44830.3 | wpb 510.9 | bsz 1 | num_updates 30338 | best_loss 7.571
2022-03-05 08:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 315 @ 30338 updates
2022-03-05 08:57:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:57:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:57:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 315 @ 30338 updates, score 14.749) (writing took 2.4762553134933114 seconds)
2022-03-05 08:57:43 | INFO | fairseq_cli.train | end of epoch 315 (average epoch stats below)
2022-03-05 08:57:43 | INFO | train | epoch 315 | loss 0.855 | nll_loss 0.519 | ppl 1.43 | wps 24902.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30338 | lr 0.000181554 | gnorm 0.819 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 80589
2022-03-05 08:57:43 | INFO | fairseq.trainer | begin training epoch 316
2022-03-05 08:57:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:00:21 | INFO | train_inner | epoch 316:     62 / 97 loss=0.854, nll_loss=0.519, ppl=1.43, wps=24928.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30400, lr=0.000181369, gnorm=0.827, loss_scale=16, train_wall=233, gb_free=21, wall=80747
2022-03-05 09:01:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:01:55 | INFO | valid | epoch 316 | valid on 'valid' subset | loss 14.761 | nll_loss 14.599 | ppl 24819 | wps 45293 | wpb 510.9 | bsz 1 | num_updates 30435 | best_loss 7.571
2022-03-05 09:01:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 316 @ 30435 updates
2022-03-05 09:01:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:01:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:01:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 316 @ 30435 updates, score 14.761) (writing took 2.6121214581653476 seconds)
2022-03-05 09:01:58 | INFO | fairseq_cli.train | end of epoch 316 (average epoch stats below)
2022-03-05 09:01:58 | INFO | train | epoch 316 | loss 0.854 | nll_loss 0.518 | ppl 1.43 | wps 24900.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30435 | lr 0.000181265 | gnorm 0.824 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 80844
2022-03-05 09:01:58 | INFO | fairseq.trainer | begin training epoch 317
2022-03-05 09:01:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:03:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:04:47 | INFO | train_inner | epoch 317:     66 / 97 loss=0.854, nll_loss=0.518, ppl=1.43, wps=24679.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30500, lr=0.000181071, gnorm=0.825, loss_scale=16, train_wall=236, gb_free=21, wall=81013
2022-03-05 09:06:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:06:10 | INFO | valid | epoch 317 | valid on 'valid' subset | loss 14.759 | nll_loss 14.597 | ppl 24790.2 | wps 45256.5 | wpb 510.9 | bsz 1 | num_updates 30531 | best_loss 7.571
2022-03-05 09:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 317 @ 30531 updates
2022-03-05 09:06:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:06:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:06:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 317 @ 30531 updates, score 14.759) (writing took 2.4631643146276474 seconds)
2022-03-05 09:06:13 | INFO | fairseq_cli.train | end of epoch 317 (average epoch stats below)
2022-03-05 09:06:13 | INFO | train | epoch 317 | loss 0.851 | nll_loss 0.516 | ppl 1.43 | wps 24658.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 30531 | lr 0.00018098 | gnorm 0.824 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 81099
2022-03-05 09:06:13 | INFO | fairseq.trainer | begin training epoch 318
2022-03-05 09:06:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:09:09 | INFO | train_inner | epoch 318:     69 / 97 loss=0.851, nll_loss=0.516, ppl=1.43, wps=24938.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=30600, lr=0.000180775, gnorm=0.817, loss_scale=32, train_wall=233, gb_free=21, wall=81275
2022-03-05 09:09:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:10:25 | INFO | valid | epoch 318 | valid on 'valid' subset | loss 14.792 | nll_loss 14.631 | ppl 25367.3 | wps 45262.7 | wpb 510.9 | bsz 1 | num_updates 30627 | best_loss 7.571
2022-03-05 09:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 318 @ 30627 updates
2022-03-05 09:10:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:10:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 318 @ 30627 updates, score 14.792) (writing took 2.498284469358623 seconds)
2022-03-05 09:10:28 | INFO | fairseq_cli.train | end of epoch 318 (average epoch stats below)
2022-03-05 09:10:28 | INFO | train | epoch 318 | loss 0.85 | nll_loss 0.514 | ppl 1.43 | wps 24660.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 30627 | lr 0.000180696 | gnorm 0.813 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 81354
2022-03-05 09:10:28 | INFO | fairseq.trainer | begin training epoch 319
2022-03-05 09:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:13:34 | INFO | train_inner | epoch 319:     73 / 97 loss=0.848, nll_loss=0.513, ppl=1.43, wps=24697.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=30700, lr=0.000180481, gnorm=0.815, loss_scale=16, train_wall=235, gb_free=21, wall=81540
2022-03-05 09:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:14:40 | INFO | valid | epoch 319 | valid on 'valid' subset | loss 14.739 | nll_loss 14.576 | ppl 24420.6 | wps 45233.9 | wpb 510.9 | bsz 1 | num_updates 30724 | best_loss 7.571
2022-03-05 09:14:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 319 @ 30724 updates
2022-03-05 09:14:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:14:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 319 @ 30724 updates, score 14.739) (writing took 2.4435551026836038 seconds)
2022-03-05 09:14:43 | INFO | fairseq_cli.train | end of epoch 319 (average epoch stats below)
2022-03-05 09:14:43 | INFO | train | epoch 319 | loss 0.849 | nll_loss 0.514 | ppl 1.43 | wps 24908.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30724 | lr 0.00018041 | gnorm 0.817 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 81609
2022-03-05 09:14:43 | INFO | fairseq.trainer | begin training epoch 320
2022-03-05 09:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:15:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:16:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:18:02 | INFO | train_inner | epoch 320:     78 / 97 loss=0.849, nll_loss=0.514, ppl=1.43, wps=24462.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=30800, lr=0.000180187, gnorm=0.824, loss_scale=8, train_wall=238, gb_free=21, wall=81808
2022-03-05 09:18:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:18:55 | INFO | valid | epoch 320 | valid on 'valid' subset | loss 14.814 | nll_loss 14.651 | ppl 25727.5 | wps 45226.6 | wpb 510.9 | bsz 1 | num_updates 30819 | best_loss 7.571
2022-03-05 09:18:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 320 @ 30819 updates
2022-03-05 09:18:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 320 @ 30819 updates, score 14.814) (writing took 2.5108036771416664 seconds)
2022-03-05 09:18:58 | INFO | fairseq_cli.train | end of epoch 320 (average epoch stats below)
2022-03-05 09:18:58 | INFO | train | epoch 320 | loss 0.847 | nll_loss 0.512 | ppl 1.43 | wps 24403.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 30819 | lr 0.000180132 | gnorm 0.824 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 81864
2022-03-05 09:18:58 | INFO | fairseq.trainer | begin training epoch 321
2022-03-05 09:18:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:22:25 | INFO | train_inner | epoch 321:     81 / 97 loss=0.847, nll_loss=0.512, ppl=1.43, wps=24934.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30900, lr=0.000179896, gnorm=0.813, loss_scale=16, train_wall=233, gb_free=21, wall=82071
2022-03-05 09:23:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:23:10 | INFO | valid | epoch 321 | valid on 'valid' subset | loss 14.754 | nll_loss 14.591 | ppl 24684.4 | wps 45114.6 | wpb 510.9 | bsz 1 | num_updates 30916 | best_loss 7.571
2022-03-05 09:23:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 321 @ 30916 updates
2022-03-05 09:23:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:23:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:23:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 321 @ 30916 updates, score 14.754) (writing took 2.441033118404448 seconds)
2022-03-05 09:23:13 | INFO | fairseq_cli.train | end of epoch 321 (average epoch stats below)
2022-03-05 09:23:13 | INFO | train | epoch 321 | loss 0.847 | nll_loss 0.512 | ppl 1.43 | wps 24917.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30916 | lr 0.000179849 | gnorm 0.815 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 82119
2022-03-05 09:23:13 | INFO | fairseq.trainer | begin training epoch 322
2022-03-05 09:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:26:47 | INFO | train_inner | epoch 322:     84 / 97 loss=0.846, nll_loss=0.511, ppl=1.43, wps=24939.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31000, lr=0.000179605, gnorm=0.824, loss_scale=16, train_wall=233, gb_free=21, wall=82333
2022-03-05 09:27:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:25 | INFO | valid | epoch 322 | valid on 'valid' subset | loss 14.768 | nll_loss 14.606 | ppl 24942.5 | wps 45387.9 | wpb 510.9 | bsz 1 | num_updates 31013 | best_loss 7.571
2022-03-05 09:27:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 322 @ 31013 updates
2022-03-05 09:27:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:27:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:27:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 322 @ 31013 updates, score 14.768) (writing took 2.976527329534292 seconds)
2022-03-05 09:27:28 | INFO | fairseq_cli.train | end of epoch 322 (average epoch stats below)
2022-03-05 09:27:28 | INFO | train | epoch 322 | loss 0.846 | nll_loss 0.511 | ppl 1.42 | wps 24861.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31013 | lr 0.000179568 | gnorm 0.825 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 82374
2022-03-05 09:27:28 | INFO | fairseq.trainer | begin training epoch 323
2022-03-05 09:27:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:28:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:31:13 | INFO | train_inner | epoch 323:     88 / 97 loss=0.845, nll_loss=0.51, ppl=1.42, wps=24652.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31100, lr=0.000179316, gnorm=0.81, loss_scale=16, train_wall=235, gb_free=21, wall=82599
2022-03-05 09:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:31:41 | INFO | valid | epoch 323 | valid on 'valid' subset | loss 14.772 | nll_loss 14.611 | ppl 25021.3 | wps 45269.5 | wpb 510.9 | bsz 1 | num_updates 31109 | best_loss 7.571
2022-03-05 09:31:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 323 @ 31109 updates
2022-03-05 09:31:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:31:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:31:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 323 @ 31109 updates, score 14.772) (writing took 9.647061901167035 seconds)
2022-03-05 09:31:50 | INFO | fairseq_cli.train | end of epoch 323 (average epoch stats below)
2022-03-05 09:31:50 | INFO | train | epoch 323 | loss 0.844 | nll_loss 0.509 | ppl 1.42 | wps 23988.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31109 | lr 0.00017929 | gnorm 0.808 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 82636
2022-03-05 09:31:50 | INFO | fairseq.trainer | begin training epoch 324
2022-03-05 09:31:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:33:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:35:45 | INFO | train_inner | epoch 324:     92 / 97 loss=0.845, nll_loss=0.51, ppl=1.42, wps=24054.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=31200, lr=0.000179029, gnorm=0.81, loss_scale=16, train_wall=235, gb_free=21, wall=82871
2022-03-05 09:35:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:36:03 | INFO | valid | epoch 324 | valid on 'valid' subset | loss 14.784 | nll_loss 14.623 | ppl 25234.1 | wps 45781.4 | wpb 510.9 | bsz 1 | num_updates 31205 | best_loss 7.571
2022-03-05 09:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 324 @ 31205 updates
2022-03-05 09:36:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:36:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 324 @ 31205 updates, score 14.784) (writing took 2.486920164898038 seconds)
2022-03-05 09:36:05 | INFO | fairseq_cli.train | end of epoch 324 (average epoch stats below)
2022-03-05 09:36:05 | INFO | train | epoch 324 | loss 0.843 | nll_loss 0.508 | ppl 1.42 | wps 24668.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31205 | lr 0.000179014 | gnorm 0.807 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 82891
2022-03-05 09:36:05 | INFO | fairseq.trainer | begin training epoch 325
2022-03-05 09:36:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:40:10 | INFO | train_inner | epoch 325:     96 / 97 loss=0.842, nll_loss=0.507, ppl=1.42, wps=24708.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31300, lr=0.000178743, gnorm=0.806, loss_scale=16, train_wall=235, gb_free=21, wall=83136
2022-03-05 09:40:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:40:18 | INFO | valid | epoch 325 | valid on 'valid' subset | loss 14.764 | nll_loss 14.603 | ppl 24880.8 | wps 45230.7 | wpb 510.9 | bsz 1 | num_updates 31301 | best_loss 7.571
2022-03-05 09:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 325 @ 31301 updates
2022-03-05 09:40:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:40:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:40:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 325 @ 31301 updates, score 14.764) (writing took 2.5466089574620128 seconds)
2022-03-05 09:40:20 | INFO | fairseq_cli.train | end of epoch 325 (average epoch stats below)
2022-03-05 09:40:20 | INFO | train | epoch 325 | loss 0.842 | nll_loss 0.507 | ppl 1.42 | wps 24660.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31301 | lr 0.00017874 | gnorm 0.807 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 83146
2022-03-05 09:40:20 | INFO | fairseq.trainer | begin training epoch 326
2022-03-05 09:40:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:44:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:44:33 | INFO | valid | epoch 326 | valid on 'valid' subset | loss 14.817 | nll_loss 14.655 | ppl 25800.6 | wps 45170.9 | wpb 510.9 | bsz 1 | num_updates 31398 | best_loss 7.571
2022-03-05 09:44:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 326 @ 31398 updates
2022-03-05 09:44:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:44:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:44:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 326 @ 31398 updates, score 14.817) (writing took 2.4974973760545254 seconds)
2022-03-05 09:44:35 | INFO | fairseq_cli.train | end of epoch 326 (average epoch stats below)
2022-03-05 09:44:35 | INFO | train | epoch 326 | loss 0.842 | nll_loss 0.507 | ppl 1.42 | wps 24909.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31398 | lr 0.000178463 | gnorm 0.81 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 83401
2022-03-05 09:44:35 | INFO | fairseq.trainer | begin training epoch 327
2022-03-05 09:44:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:44:40 | INFO | train_inner | epoch 327:      2 / 97 loss=0.841, nll_loss=0.506, ppl=1.42, wps=24235, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=31400, lr=0.000178458, gnorm=0.81, loss_scale=16, train_wall=233, gb_free=21, wall=83407
2022-03-05 09:45:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:48:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:48:48 | INFO | valid | epoch 327 | valid on 'valid' subset | loss 14.757 | nll_loss 14.594 | ppl 24731.5 | wps 45186.9 | wpb 510.9 | bsz 1 | num_updates 31494 | best_loss 7.571
2022-03-05 09:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 327 @ 31494 updates
2022-03-05 09:48:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:48:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:48:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 327 @ 31494 updates, score 14.757) (writing took 2.471720661967993 seconds)
2022-03-05 09:48:50 | INFO | fairseq_cli.train | end of epoch 327 (average epoch stats below)
2022-03-05 09:48:50 | INFO | train | epoch 327 | loss 0.839 | nll_loss 0.504 | ppl 1.42 | wps 24645.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31494 | lr 0.000178191 | gnorm 0.806 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 83656
2022-03-05 09:48:50 | INFO | fairseq.trainer | begin training epoch 328
2022-03-05 09:48:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:49:06 | INFO | train_inner | epoch 328:      6 / 97 loss=0.839, nll_loss=0.504, ppl=1.42, wps=24686.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31500, lr=0.000178174, gnorm=0.806, loss_scale=16, train_wall=236, gb_free=21, wall=83672
2022-03-05 09:51:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:52:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:53:03 | INFO | valid | epoch 328 | valid on 'valid' subset | loss 14.784 | nll_loss 14.622 | ppl 25209.2 | wps 45410.8 | wpb 510.9 | bsz 1 | num_updates 31590 | best_loss 7.571
2022-03-05 09:53:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 328 @ 31590 updates
2022-03-05 09:53:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:53:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 328 @ 31590 updates, score 14.784) (writing took 2.457367394119501 seconds)
2022-03-05 09:53:05 | INFO | fairseq_cli.train | end of epoch 328 (average epoch stats below)
2022-03-05 09:53:05 | INFO | train | epoch 328 | loss 0.839 | nll_loss 0.504 | ppl 1.42 | wps 24660.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31590 | lr 0.00017792 | gnorm 0.807 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 83911
2022-03-05 09:53:05 | INFO | fairseq.trainer | begin training epoch 329
2022-03-05 09:53:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:53:31 | INFO | train_inner | epoch 329:     10 / 97 loss=0.838, nll_loss=0.503, ppl=1.42, wps=24699.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31600, lr=0.000177892, gnorm=0.806, loss_scale=16, train_wall=235, gb_free=21, wall=83937
2022-03-05 09:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:57:18 | INFO | valid | epoch 329 | valid on 'valid' subset | loss 14.724 | nll_loss 14.562 | ppl 24195.9 | wps 45232 | wpb 510.9 | bsz 1 | num_updates 31687 | best_loss 7.571
2022-03-05 09:57:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 329 @ 31687 updates
2022-03-05 09:57:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:57:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 329 @ 31687 updates, score 14.724) (writing took 2.9095307737588882 seconds)
2022-03-05 09:57:21 | INFO | fairseq_cli.train | end of epoch 329 (average epoch stats below)
2022-03-05 09:57:21 | INFO | train | epoch 329 | loss 0.837 | nll_loss 0.502 | ppl 1.42 | wps 24866.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31687 | lr 0.000177648 | gnorm 0.806 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 84167
2022-03-05 09:57:21 | INFO | fairseq.trainer | begin training epoch 330
2022-03-05 09:57:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:57:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:57:57 | INFO | train_inner | epoch 330:     14 / 97 loss=0.837, nll_loss=0.502, ppl=1.42, wps=24652.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31700, lr=0.000177611, gnorm=0.808, loss_scale=16, train_wall=235, gb_free=21, wall=84203
2022-03-05 10:01:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:01:33 | INFO | valid | epoch 330 | valid on 'valid' subset | loss 14.764 | nll_loss 14.602 | ppl 24872.4 | wps 45211.5 | wpb 510.9 | bsz 1 | num_updates 31783 | best_loss 7.571
2022-03-05 10:01:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 330 @ 31783 updates
2022-03-05 10:01:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:01:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:01:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 330 @ 31783 updates, score 14.764) (writing took 2.4756642505526543 seconds)
2022-03-05 10:01:36 | INFO | fairseq_cli.train | end of epoch 330 (average epoch stats below)
2022-03-05 10:01:36 | INFO | train | epoch 330 | loss 0.837 | nll_loss 0.502 | ppl 1.42 | wps 24657.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31783 | lr 0.000177379 | gnorm 0.817 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 84422
2022-03-05 10:01:36 | INFO | fairseq.trainer | begin training epoch 331
2022-03-05 10:01:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:02:19 | INFO | train_inner | epoch 331:     17 / 97 loss=0.836, nll_loss=0.501, ppl=1.42, wps=24933.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31800, lr=0.000177332, gnorm=0.814, loss_scale=16, train_wall=233, gb_free=21, wall=84465
2022-03-05 10:03:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:05:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:05:48 | INFO | valid | epoch 331 | valid on 'valid' subset | loss 14.765 | nll_loss 14.603 | ppl 24881.3 | wps 45151.5 | wpb 510.9 | bsz 1 | num_updates 31879 | best_loss 7.571
2022-03-05 10:05:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 331 @ 31879 updates
2022-03-05 10:05:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 331 @ 31879 updates, score 14.765) (writing took 2.658507871441543 seconds)
2022-03-05 10:05:51 | INFO | fairseq_cli.train | end of epoch 331 (average epoch stats below)
2022-03-05 10:05:51 | INFO | train | epoch 331 | loss 0.836 | nll_loss 0.501 | ppl 1.42 | wps 24633.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31879 | lr 0.000177112 | gnorm 0.806 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 84677
2022-03-05 10:05:51 | INFO | fairseq.trainer | begin training epoch 332
2022-03-05 10:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:06:45 | INFO | train_inner | epoch 332:     21 / 97 loss=0.835, nll_loss=0.5, ppl=1.41, wps=24674.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31900, lr=0.000177054, gnorm=0.803, loss_scale=16, train_wall=236, gb_free=21, wall=84731
2022-03-05 10:09:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:10:03 | INFO | valid | epoch 332 | valid on 'valid' subset | loss 14.791 | nll_loss 14.629 | ppl 25346.5 | wps 45196.2 | wpb 510.9 | bsz 1 | num_updates 31975 | best_loss 7.571
2022-03-05 10:10:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 332 @ 31975 updates
2022-03-05 10:10:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:10:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:10:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 332 @ 31975 updates, score 14.791) (writing took 2.4821518370881677 seconds)
2022-03-05 10:10:06 | INFO | fairseq_cli.train | end of epoch 332 (average epoch stats below)
2022-03-05 10:10:06 | INFO | train | epoch 332 | loss 0.835 | nll_loss 0.5 | ppl 1.41 | wps 24657 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31975 | lr 0.000176846 | gnorm 0.808 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 84932
2022-03-05 10:10:06 | INFO | fairseq.trainer | begin training epoch 333
2022-03-05 10:10:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:11:10 | INFO | train_inner | epoch 333:     25 / 97 loss=0.834, nll_loss=0.499, ppl=1.41, wps=24694.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=32000, lr=0.000176777, gnorm=0.812, loss_scale=16, train_wall=235, gb_free=21, wall=84996
2022-03-05 10:14:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:14:18 | INFO | valid | epoch 333 | valid on 'valid' subset | loss 14.779 | nll_loss 14.618 | ppl 25152.8 | wps 45216.1 | wpb 510.9 | bsz 1 | num_updates 32072 | best_loss 7.571
2022-03-05 10:14:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 333 @ 32072 updates
2022-03-05 10:14:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:14:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 333 @ 32072 updates, score 14.779) (writing took 2.5793981766328216 seconds)
2022-03-05 10:14:21 | INFO | fairseq_cli.train | end of epoch 333 (average epoch stats below)
2022-03-05 10:14:21 | INFO | train | epoch 333 | loss 0.833 | nll_loss 0.498 | ppl 1.41 | wps 24902.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 32072 | lr 0.000176578 | gnorm 0.816 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 85187
2022-03-05 10:14:21 | INFO | fairseq.trainer | begin training epoch 334
2022-03-05 10:14:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:15:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:15:35 | INFO | train_inner | epoch 334:     29 / 97 loss=0.833, nll_loss=0.498, ppl=1.41, wps=24678.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=32100, lr=0.000176501, gnorm=0.819, loss_scale=16, train_wall=236, gb_free=21, wall=85261
2022-03-05 10:17:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:18:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:18:34 | INFO | valid | epoch 334 | valid on 'valid' subset | loss 14.802 | nll_loss 14.641 | ppl 25541.5 | wps 45400.1 | wpb 510.9 | bsz 1 | num_updates 32167 | best_loss 7.571
2022-03-05 10:18:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 334 @ 32167 updates
2022-03-05 10:18:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:18:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:18:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 334 @ 32167 updates, score 14.802) (writing took 2.5081343045458198 seconds)
2022-03-05 10:18:36 | INFO | fairseq_cli.train | end of epoch 334 (average epoch stats below)
2022-03-05 10:18:36 | INFO | train | epoch 334 | loss 0.831 | nll_loss 0.497 | ppl 1.41 | wps 24393.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 32167 | lr 0.000176317 | gnorm 0.813 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 85442
2022-03-05 10:18:36 | INFO | fairseq.trainer | begin training epoch 335
2022-03-05 10:18:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:20:01 | INFO | train_inner | epoch 335:     33 / 97 loss=0.83, nll_loss=0.496, ppl=1.41, wps=24688.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=32200, lr=0.000176227, gnorm=0.806, loss_scale=8, train_wall=235, gb_free=21, wall=85527
2022-03-05 10:22:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:22:49 | INFO | valid | epoch 335 | valid on 'valid' subset | loss 14.769 | nll_loss 14.608 | ppl 24965.2 | wps 45276.1 | wpb 510.9 | bsz 1 | num_updates 32264 | best_loss 7.571
2022-03-05 10:22:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 335 @ 32264 updates
2022-03-05 10:22:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:22:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:22:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 335 @ 32264 updates, score 14.769) (writing took 2.441907168366015 seconds)
2022-03-05 10:22:51 | INFO | fairseq_cli.train | end of epoch 335 (average epoch stats below)
2022-03-05 10:22:51 | INFO | train | epoch 335 | loss 0.83 | nll_loss 0.495 | ppl 1.41 | wps 24905.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 32264 | lr 0.000176052 | gnorm 0.802 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 85697
2022-03-05 10:22:51 | INFO | fairseq.trainer | begin training epoch 336
2022-03-05 10:22:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:24:23 | INFO | train_inner | epoch 336:     36 / 97 loss=0.83, nll_loss=0.495, ppl=1.41, wps=24932.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=32300, lr=0.000175954, gnorm=0.8, loss_scale=16, train_wall=233, gb_free=21, wall=85789
2022-03-05 10:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:27:04 | INFO | valid | epoch 336 | valid on 'valid' subset | loss 14.756 | nll_loss 14.596 | ppl 24761.9 | wps 45236 | wpb 510.9 | bsz 1 | num_updates 32361 | best_loss 7.571
2022-03-05 10:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 336 @ 32361 updates
2022-03-05 10:27:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:27:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 336 @ 32361 updates, score 14.756) (writing took 2.5639369003474712 seconds)
2022-03-05 10:27:06 | INFO | fairseq_cli.train | end of epoch 336 (average epoch stats below)
2022-03-05 10:27:06 | INFO | train | epoch 336 | loss 0.831 | nll_loss 0.496 | ppl 1.41 | wps 24900.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 32361 | lr 0.000175788 | gnorm 0.8 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 85952
2022-03-05 10:27:06 | INFO | fairseq.trainer | begin training epoch 337
2022-03-05 10:27:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:28:46 | INFO | train_inner | epoch 337:     39 / 97 loss=0.829, nll_loss=0.495, ppl=1.41, wps=24927, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=32400, lr=0.000175682, gnorm=0.803, loss_scale=16, train_wall=233, gb_free=21, wall=86052
2022-03-05 10:29:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:31:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:31:19 | INFO | valid | epoch 337 | valid on 'valid' subset | loss 14.748 | nll_loss 14.587 | ppl 24612.8 | wps 45286.9 | wpb 510.9 | bsz 1 | num_updates 32457 | best_loss 7.571
2022-03-05 10:31:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 337 @ 32457 updates
2022-03-05 10:31:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:31:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:31:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 337 @ 32457 updates, score 14.748) (writing took 3.0049246354028583 seconds)
2022-03-05 10:31:22 | INFO | fairseq_cli.train | end of epoch 337 (average epoch stats below)
2022-03-05 10:31:22 | INFO | train | epoch 337 | loss 0.828 | nll_loss 0.493 | ppl 1.41 | wps 24614.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 32457 | lr 0.000175528 | gnorm 0.799 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 86208
2022-03-05 10:31:22 | INFO | fairseq.trainer | begin training epoch 338
2022-03-05 10:31:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:33:12 | INFO | train_inner | epoch 338:     43 / 97 loss=0.828, nll_loss=0.493, ppl=1.41, wps=24650.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=32500, lr=0.000175412, gnorm=0.796, loss_scale=16, train_wall=235, gb_free=21, wall=86318
2022-03-05 10:35:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:35:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:35:34 | INFO | valid | epoch 338 | valid on 'valid' subset | loss 14.825 | nll_loss 14.665 | ppl 25976.4 | wps 45373.2 | wpb 510.9 | bsz 1 | num_updates 32553 | best_loss 7.571
2022-03-05 10:35:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 338 @ 32553 updates
2022-03-05 10:35:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:35:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 338 @ 32553 updates, score 14.825) (writing took 2.5854719886556268 seconds)
2022-03-05 10:35:37 | INFO | fairseq_cli.train | end of epoch 338 (average epoch stats below)
2022-03-05 10:35:37 | INFO | train | epoch 338 | loss 0.828 | nll_loss 0.494 | ppl 1.41 | wps 24646.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 32553 | lr 0.000175269 | gnorm 0.799 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 86463
2022-03-05 10:35:37 | INFO | fairseq.trainer | begin training epoch 339
2022-03-05 10:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:37:37 | INFO | train_inner | epoch 339:     47 / 97 loss=0.828, nll_loss=0.494, ppl=1.41, wps=24682.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=32600, lr=0.000175142, gnorm=0.802, loss_scale=16, train_wall=235, gb_free=21, wall=86583
2022-03-05 10:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:39:49 | INFO | valid | epoch 339 | valid on 'valid' subset | loss 14.773 | nll_loss 14.611 | ppl 25020.8 | wps 45380.7 | wpb 510.9 | bsz 1 | num_updates 32650 | best_loss 7.571
2022-03-05 10:39:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 339 @ 32650 updates
2022-03-05 10:39:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:39:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 339 @ 32650 updates, score 14.773) (writing took 2.578800710849464 seconds)
2022-03-05 10:39:52 | INFO | fairseq_cli.train | end of epoch 339 (average epoch stats below)
2022-03-05 10:39:52 | INFO | train | epoch 339 | loss 0.828 | nll_loss 0.493 | ppl 1.41 | wps 24897.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 32650 | lr 0.000175008 | gnorm 0.804 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 86718
2022-03-05 10:39:52 | INFO | fairseq.trainer | begin training epoch 340
2022-03-05 10:39:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:42:02 | INFO | train_inner | epoch 340:     51 / 97 loss=0.827, nll_loss=0.492, ppl=1.41, wps=24692.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=32700, lr=0.000174874, gnorm=0.803, loss_scale=16, train_wall=235, gb_free=21, wall=86848
2022-03-05 10:44:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:44:04 | INFO | valid | epoch 340 | valid on 'valid' subset | loss 14.812 | nll_loss 14.651 | ppl 25728.7 | wps 45363 | wpb 510.9 | bsz 1 | num_updates 32746 | best_loss 7.571
2022-03-05 10:44:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 340 @ 32746 updates
2022-03-05 10:44:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:44:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:44:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 340 @ 32746 updates, score 14.812) (writing took 2.568941902369261 seconds)
2022-03-05 10:44:07 | INFO | fairseq_cli.train | end of epoch 340 (average epoch stats below)
2022-03-05 10:44:07 | INFO | train | epoch 340 | loss 0.825 | nll_loss 0.49 | ppl 1.4 | wps 24655.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 32746 | lr 0.000174751 | gnorm 0.8 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 86973
2022-03-05 10:44:07 | INFO | fairseq.trainer | begin training epoch 341
2022-03-05 10:44:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:46:25 | INFO | train_inner | epoch 341:     54 / 97 loss=0.825, nll_loss=0.49, ppl=1.4, wps=24927.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=32800, lr=0.000174608, gnorm=0.8, loss_scale=16, train_wall=233, gb_free=21, wall=87111
2022-03-05 10:47:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:48:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:48:19 | INFO | valid | epoch 341 | valid on 'valid' subset | loss 14.803 | nll_loss 14.643 | ppl 25577.1 | wps 45312.1 | wpb 510.9 | bsz 1 | num_updates 32842 | best_loss 7.571
2022-03-05 10:48:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 341 @ 32842 updates
2022-03-05 10:48:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:48:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:48:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 341 @ 32842 updates, score 14.803) (writing took 2.6298772981390357 seconds)
2022-03-05 10:48:22 | INFO | fairseq_cli.train | end of epoch 341 (average epoch stats below)
2022-03-05 10:48:22 | INFO | train | epoch 341 | loss 0.825 | nll_loss 0.49 | ppl 1.4 | wps 24638.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 32842 | lr 0.000174496 | gnorm 0.799 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 87228
2022-03-05 10:48:22 | INFO | fairseq.trainer | begin training epoch 342
2022-03-05 10:48:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:50:50 | INFO | train_inner | epoch 342:     58 / 97 loss=0.824, nll_loss=0.489, ppl=1.4, wps=24682.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=32900, lr=0.000174342, gnorm=0.797, loss_scale=16, train_wall=235, gb_free=21, wall=87376
2022-03-05 10:52:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:52:35 | INFO | valid | epoch 342 | valid on 'valid' subset | loss 14.823 | nll_loss 14.665 | ppl 25978.3 | wps 45349.9 | wpb 510.9 | bsz 1 | num_updates 32939 | best_loss 7.571
2022-03-05 10:52:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 342 @ 32939 updates
2022-03-05 10:52:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:52:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:52:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 342 @ 32939 updates, score 14.823) (writing took 2.570890663191676 seconds)
2022-03-05 10:52:37 | INFO | fairseq_cli.train | end of epoch 342 (average epoch stats below)
2022-03-05 10:52:37 | INFO | train | epoch 342 | loss 0.824 | nll_loss 0.489 | ppl 1.4 | wps 24907.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 32939 | lr 0.000174239 | gnorm 0.799 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 87483
2022-03-05 10:52:37 | INFO | fairseq.trainer | begin training epoch 343
2022-03-05 10:52:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:53:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:55:16 | INFO | train_inner | epoch 343:     62 / 97 loss=0.823, nll_loss=0.488, ppl=1.4, wps=24687.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=33000, lr=0.000174078, gnorm=0.796, loss_scale=16, train_wall=235, gb_free=21, wall=87642
2022-03-05 10:56:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:56:50 | INFO | valid | epoch 343 | valid on 'valid' subset | loss 14.763 | nll_loss 14.603 | ppl 24889.2 | wps 45366.6 | wpb 510.9 | bsz 1 | num_updates 33035 | best_loss 7.571
2022-03-05 10:56:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 343 @ 33035 updates
2022-03-05 10:56:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:56:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 343 @ 33035 updates, score 14.763) (writing took 2.573078477755189 seconds)
2022-03-05 10:56:52 | INFO | fairseq_cli.train | end of epoch 343 (average epoch stats below)
2022-03-05 10:56:52 | INFO | train | epoch 343 | loss 0.822 | nll_loss 0.488 | ppl 1.4 | wps 24654.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 33035 | lr 0.000173985 | gnorm 0.794 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 87738
2022-03-05 10:56:52 | INFO | fairseq.trainer | begin training epoch 344
2022-03-05 10:56:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:59:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:59:41 | INFO | train_inner | epoch 344:     66 / 97 loss=0.822, nll_loss=0.487, ppl=1.4, wps=24689.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=33100, lr=0.000173814, gnorm=0.799, loss_scale=16, train_wall=235, gb_free=21, wall=87907
2022-03-05 11:01:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:01:05 | INFO | valid | epoch 344 | valid on 'valid' subset | loss 14.808 | nll_loss 14.649 | ppl 25695.2 | wps 45258.5 | wpb 510.9 | bsz 1 | num_updates 33131 | best_loss 7.571
2022-03-05 11:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 344 @ 33131 updates
2022-03-05 11:01:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:01:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 344 @ 33131 updates, score 14.808) (writing took 2.5830233125016093 seconds)
2022-03-05 11:01:07 | INFO | fairseq_cli.train | end of epoch 344 (average epoch stats below)
2022-03-05 11:01:07 | INFO | train | epoch 344 | loss 0.821 | nll_loss 0.487 | ppl 1.4 | wps 24643 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 33131 | lr 0.000173733 | gnorm 0.803 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 87993
2022-03-05 11:01:07 | INFO | fairseq.trainer | begin training epoch 345
2022-03-05 11:01:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:04:04 | INFO | train_inner | epoch 345:     69 / 97 loss=0.821, nll_loss=0.487, ppl=1.4, wps=24925.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=33200, lr=0.000173553, gnorm=0.804, loss_scale=16, train_wall=233, gb_free=21, wall=88170
2022-03-05 11:04:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:05:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:05:20 | INFO | valid | epoch 345 | valid on 'valid' subset | loss 14.82 | nll_loss 14.66 | ppl 25884.1 | wps 45253.8 | wpb 510.9 | bsz 1 | num_updates 33227 | best_loss 7.571
2022-03-05 11:05:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 345 @ 33227 updates
2022-03-05 11:05:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:05:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:05:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 345 @ 33227 updates, score 14.82) (writing took 2.6180915581062436 seconds)
2022-03-05 11:05:22 | INFO | fairseq_cli.train | end of epoch 345 (average epoch stats below)
2022-03-05 11:05:22 | INFO | train | epoch 345 | loss 0.82 | nll_loss 0.486 | ppl 1.4 | wps 24649 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 33227 | lr 0.000173482 | gnorm 0.799 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 88249
2022-03-05 11:05:22 | INFO | fairseq.trainer | begin training epoch 346
2022-03-05 11:05:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:08:29 | INFO | train_inner | epoch 346:     73 / 97 loss=0.819, nll_loss=0.484, ppl=1.4, wps=24687.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=33300, lr=0.000173292, gnorm=0.794, loss_scale=16, train_wall=235, gb_free=21, wall=88435
2022-03-05 11:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:09:35 | INFO | valid | epoch 346 | valid on 'valid' subset | loss 14.889 | nll_loss 14.73 | ppl 27178.6 | wps 45318.4 | wpb 510.9 | bsz 1 | num_updates 33324 | best_loss 7.571
2022-03-05 11:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 346 @ 33324 updates
2022-03-05 11:09:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:09:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 346 @ 33324 updates, score 14.889) (writing took 2.5740226339548826 seconds)
2022-03-05 11:09:37 | INFO | fairseq_cli.train | end of epoch 346 (average epoch stats below)
2022-03-05 11:09:37 | INFO | train | epoch 346 | loss 0.818 | nll_loss 0.484 | ppl 1.4 | wps 24902.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 33324 | lr 0.000173229 | gnorm 0.793 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 88504
2022-03-05 11:09:37 | INFO | fairseq.trainer | begin training epoch 347
2022-03-05 11:09:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:10:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:12:54 | INFO | train_inner | epoch 347:     77 / 97 loss=0.82, nll_loss=0.486, ppl=1.4, wps=24682.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=33400, lr=0.000173032, gnorm=0.798, loss_scale=16, train_wall=236, gb_free=21, wall=88700
2022-03-05 11:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:13:50 | INFO | valid | epoch 347 | valid on 'valid' subset | loss 14.85 | nll_loss 14.691 | ppl 26441.6 | wps 45296.5 | wpb 510.9 | bsz 1 | num_updates 33420 | best_loss 7.571
2022-03-05 11:13:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 347 @ 33420 updates
2022-03-05 11:13:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:13:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 347 @ 33420 updates, score 14.85) (writing took 2.5573540600016713 seconds)
2022-03-05 11:13:53 | INFO | fairseq_cli.train | end of epoch 347 (average epoch stats below)
2022-03-05 11:13:53 | INFO | train | epoch 347 | loss 0.819 | nll_loss 0.485 | ppl 1.4 | wps 24649.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 33420 | lr 0.00017298 | gnorm 0.801 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 88759
2022-03-05 11:13:53 | INFO | fairseq.trainer | begin training epoch 348
2022-03-05 11:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:16:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:17:19 | INFO | train_inner | epoch 348:     81 / 97 loss=0.819, nll_loss=0.484, ppl=1.4, wps=24689.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=33500, lr=0.000172774, gnorm=0.8, loss_scale=16, train_wall=235, gb_free=21, wall=88966
2022-03-05 11:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:18:05 | INFO | valid | epoch 348 | valid on 'valid' subset | loss 14.813 | nll_loss 14.652 | ppl 25742.6 | wps 45234.1 | wpb 510.9 | bsz 1 | num_updates 33516 | best_loss 7.571
2022-03-05 11:18:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 348 @ 33516 updates
2022-03-05 11:18:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:18:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 348 @ 33516 updates, score 14.813) (writing took 2.5600988380610943 seconds)
2022-03-05 11:18:08 | INFO | fairseq_cli.train | end of epoch 348 (average epoch stats below)
2022-03-05 11:18:08 | INFO | train | epoch 348 | loss 0.818 | nll_loss 0.484 | ppl 1.4 | wps 24649.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 33516 | lr 0.000172732 | gnorm 0.8 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 89014
2022-03-05 11:18:08 | INFO | fairseq.trainer | begin training epoch 349
2022-03-05 11:18:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:21:42 | INFO | train_inner | epoch 349:     84 / 97 loss=0.818, nll_loss=0.484, ppl=1.4, wps=24920.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=33600, lr=0.000172516, gnorm=0.798, loss_scale=16, train_wall=233, gb_free=21, wall=89228
2022-03-05 11:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:22:20 | INFO | valid | epoch 349 | valid on 'valid' subset | loss 14.873 | nll_loss 14.715 | ppl 26894.5 | wps 45201.9 | wpb 510.9 | bsz 1 | num_updates 33613 | best_loss 7.571
2022-03-05 11:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 349 @ 33613 updates
2022-03-05 11:22:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:22:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:22:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 349 @ 33613 updates, score 14.873) (writing took 2.621323303319514 seconds)
2022-03-05 11:22:23 | INFO | fairseq_cli.train | end of epoch 349 (average epoch stats below)
2022-03-05 11:22:23 | INFO | train | epoch 349 | loss 0.817 | nll_loss 0.482 | ppl 1.4 | wps 24889.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 33613 | lr 0.000172483 | gnorm 0.797 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 89269
2022-03-05 11:22:23 | INFO | fairseq.trainer | begin training epoch 350
2022-03-05 11:22:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:22:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 328, in extract_features_scriptable
    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
KeyboardInterrupt
