Sender: LSF System <lsfadmin@eu-g3-061>
Subject: Job 208120484: <iwslt14_de_en_dropout_0.9> in cluster <euler> Done

Job <iwslt14_de_en_dropout_0.9> was submitted from host <eu-login-20> by user <andriusb> in cluster <euler> at Mon Mar 14 09:13:28 2022
Job was executed on host(s) <eu-g3-061>, in queue <gpuhe.4h>, as user <andriusb> in cluster <euler> at Mon Mar 14 09:14:03 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Mon Mar 14 09:14:03 2022
Terminated at Mon Mar 14 09:20:26 2022
Results reported at Mon Mar 14 09:20:26 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en --save-dir /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9 --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas "(0.9, 0.98)" --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.9 --weight-decay 0.0001 --criterion cross_entropy --max-tokens 32768 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --fp16 --no-epoch-checkpoints --patience 3 --seed 66575611 --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   378.18 sec.
    Max Memory :                                 4422 MB
    Average Memory :                             2870.71 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15578.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   383 sec.
    Turnaround time :                            418 sec.

The output (if any) follows:

2022-03-14 09:14:09 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 32768, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 32768, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.9, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=32768, max_tokens_valid=32768, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9', save_interval=1, save_interval_updates=0, scoring='bleu', seed=66575611, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-14 09:14:09 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2022-03-14 09:14:09 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2022-03-14 09:14:10 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2022-03-14 09:14:10 | INFO | fairseq_cli.train | task: TranslationTask
2022-03-14 09:14:10 | INFO | fairseq_cli.train | model: TransformerModel
2022-03-14 09:14:10 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-14 09:14:10 | INFO | fairseq_cli.train | num. shared model params: 39,469,056 (num. trained: 39,469,056)
2022-03-14 09:14:10 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-14 09:14:10 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-03-14 09:14:10 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-03-14 09:14:10 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2022-03-14 09:14:12 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-14 09:14:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-14 09:14:12 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-14 09:14:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-14 09:14:12 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-14 09:14:12 | INFO | fairseq_cli.train | max tokens per device = 32768 and max sentences per device = None
2022-03-14 09:14:12 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:14:12 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:14:12 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-14 09:14:12 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-03-14 09:14:12 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-03-14 09:14:12 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2022-03-14 09:14:13 | INFO | fairseq.trainer | begin training epoch 1
2022-03-14 09:14:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:14:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-14 09:14:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-14 09:14:45 | INFO | train_inner | epoch 001:    102 / 157 loss=12.925, ppl=7776.26, wps=80753.8, ups=3.2, wpb=25194.6, bsz=972.7, num_updates=100, lr=1.25e-05, gnorm=1.595, loss_scale=32, train_wall=32, gb_free=14.1, wall=33
2022-03-14 09:14:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-14 09:15:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:15:06 | INFO | fairseq.tasks.translation | example hypothesis: .........................
2022-03-14 09:15:06 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:15:10 | INFO | fairseq.tasks.translation | example hypothesis: ..............................
2022-03-14 09:15:10 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:15:15 | INFO | fairseq.tasks.translation | example hypothesis: .....
2022-03-14 09:15:15 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:15:19 | INFO | fairseq.tasks.translation | example hypothesis: ..........................................
2022-03-14 09:15:19 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:15:24 | INFO | fairseq.tasks.translation | example hypothesis: .................................................
2022-03-14 09:15:24 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:15:30 | INFO | fairseq.tasks.translation | example hypothesis: ...........................................................
2022-03-14 09:15:30 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:15:35 | INFO | fairseq.tasks.translation | example hypothesis: ........................................................................
2022-03-14 09:15:35 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:15:40 | INFO | fairseq.tasks.translation | example hypothesis: ..............................................................................................
2022-03-14 09:15:40 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:15:48 | INFO | fairseq.tasks.translation | example hypothesis: ........................................................................................................................................................................................
2022-03-14 09:15:48 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:15:50 | INFO | fairseq.tasks.translation | example hypothesis: ......................................................................................................................................................................................................................
2022-03-14 09:15:50 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:15:50 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.589 | ppl 6159.78 | bleu 0.02 | wps 3734.4 | wpb 17862.2 | bsz 728.3 | num_updates 154
2022-03-14 09:15:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 154 updates
2022-03-14 09:15:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_best.pt
2022-03-14 09:15:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_best.pt
2022-03-14 09:15:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_best.pt (epoch 1 @ 154 updates, score 0.02) (writing took 1.856719363015145 seconds)
2022-03-14 09:15:52 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-14 09:15:52 | INFO | train | epoch 001 | loss 12.547 | ppl 5983.96 | wps 39574.5 | ups 1.57 | wpb 25124 | bsz 982.4 | num_updates 154 | lr 1.925e-05 | gnorm 1.466 | loss_scale 16 | train_wall 49 | gb_free 22.4 | wall 100
2022-03-14 09:15:52 | INFO | fairseq.trainer | begin training epoch 2
2022-03-14 09:15:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:16:07 | INFO | train_inner | epoch 002:     46 / 157 loss=11.571, ppl=3042.37, wps=31248.4, ups=1.23, wpb=25324.3, bsz=1058.7, num_updates=200, lr=2.5e-05, gnorm=1.179, loss_scale=16, train_wall=30, gb_free=15.1, wall=114
2022-03-14 09:16:38 | INFO | train_inner | epoch 002:    146 / 157 loss=10.622, ppl=1575.98, wps=80841.1, ups=3.21, wpb=25173.8, bsz=981.7, num_updates=300, lr=3.75e-05, gnorm=1.001, loss_scale=16, train_wall=31, gb_free=14, wall=145
2022-03-14 09:16:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:16:44 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:44 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:16:47 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:47 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:16:49 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:49 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:16:51 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:51 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:16:54 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:54 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:16:56 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:56 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:16:58 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:58 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:16:59 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:16:59 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:17:01 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:17:01 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:17:01 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-14 09:17:01 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:17:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.86 | ppl 1858.09 | bleu 0 | wps 9751.7 | wpb 17862.2 | bsz 728.3 | num_updates 311 | best_bleu 0.02
2022-03-14 09:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 311 updates
2022-03-14 09:17:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:17:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt (epoch 2 @ 311 updates, score 0.0) (writing took 0.8435730568598956 seconds)
2022-03-14 09:17:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-14 09:17:02 | INFO | train | epoch 002 | loss 10.786 | ppl 1765.5 | wps 56518.9 | ups 2.25 | wpb 25153.6 | bsz 1020.6 | num_updates 311 | lr 3.8875e-05 | gnorm 1.041 | loss_scale 16 | train_wall 48 | gb_free 14.3 | wall 169
2022-03-14 09:17:02 | INFO | fairseq.trainer | begin training epoch 3
2022-03-14 09:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:17:29 | INFO | train_inner | epoch 003:     89 / 157 loss=9.995, ppl=1020.66, wps=47541.2, ups=1.93, wpb=24571.4, bsz=947.8, num_updates=400, lr=5e-05, gnorm=0.828, loss_scale=16, train_wall=30, gb_free=14, wall=197
2022-03-14 09:17:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:17:56 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:17:56 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:18:01 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:01 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:18:06 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:06 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:18:11 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:11 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:18:17 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:17 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:18:22 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:22 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:18:27 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:27 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:18:33 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:33 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:18:40 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:40 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:18:42 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:42 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:18:42 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.926 | ppl 1945.3 | bleu 0 | wps 3539.7 | wpb 17862.2 | bsz 728.3 | num_updates 468 | best_bleu 0.02
2022-03-14 09:18:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 468 updates
2022-03-14 09:18:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:18:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:18:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt (epoch 3 @ 468 updates, score 0.0) (writing took 0.9047737522050738 seconds)
2022-03-14 09:18:43 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-14 09:18:43 | INFO | train | epoch 003 | loss 9.892 | ppl 949.87 | wps 38950.6 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 468 | lr 5.85e-05 | gnorm 0.805 | loss_scale 16 | train_wall 47 | gb_free 14.1 | wall 271
2022-03-14 09:18:43 | INFO | fairseq.trainer | begin training epoch 4
2022-03-14 09:18:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:18:54 | INFO | train_inner | epoch 004:     32 / 157 loss=9.77, ppl=873.19, wps=30221, ups=1.19, wpb=25465, bsz=1109, num_updates=500, lr=6.25e-05, gnorm=0.776, loss_scale=16, train_wall=30, gb_free=15.1, wall=281
2022-03-14 09:19:25 | INFO | train_inner | epoch 004:    132 / 157 loss=9.728, ppl=847.99, wps=81756.1, ups=3.23, wpb=25326.6, bsz=1032.1, num_updates=600, lr=7.5e-05, gnorm=0.697, loss_scale=16, train_wall=31, gb_free=14, wall=312
2022-03-14 09:19:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:19:37 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:37 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:19:43 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:43 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:19:48 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:48 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:19:53 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:53 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:19:58 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:58 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:20:03 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:20:03 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:20:09 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:20:09 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:20:14 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:20:14 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:20:21 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:20:21 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:20:24 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:20:24 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:20:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.828 | ppl 1818.23 | bleu 0 | wps 3539.4 | wpb 17862.2 | bsz 728.3 | num_updates 625 | best_bleu 0.02
2022-03-14 09:20:24 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-14 09:20:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 625 updates
2022-03-14 09:20:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:20:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt
2022-03-14 09:20:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.9/checkpoint_last.pt (epoch 4 @ 625 updates, score 0.0) (writing took 0.9104568618349731 seconds)
2022-03-14 09:20:25 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-14 09:20:25 | INFO | train | epoch 004 | loss 9.731 | ppl 849.83 | wps 38916.6 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 625 | lr 7.8125e-05 | gnorm 0.697 | loss_scale 16 | train_wall 47 | gb_free 14.3 | wall 372
2022-03-14 09:20:25 | INFO | fairseq_cli.train | done training in 371.9 seconds
