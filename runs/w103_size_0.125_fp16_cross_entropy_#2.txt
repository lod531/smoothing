Sender: LSF System <lsfadmin@eu-g2-04>
Subject: Job 207129833: <w103_size_0.125_fp16_cross_entropy_#2> in cluster <euler> Exited

Job <w103_size_0.125_fp16_cross_entropy_#2> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:14:33 2022
Job was executed on host(s) <eu-g2-04>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:14:41 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:14:41 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   98059.70 sec.
    Max Memory :                                 8398 MB
    Average Memory :                             4425.82 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11602.00 MB
    Max Swap :                                   295 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   94240 sec.
    Turnaround time :                            94249 sec.

The output (if any) follows:

2022-03-04 09:14:56 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:14:57 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-04 09:15:00 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-04 09:15:00 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:15:00 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:15:00 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-04 09:15:00 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-04 09:15:00 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:15:00 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-04 09:15:10 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:15:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:15:10 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-03-04 09:15:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:15:10 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:15:10 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:15:10 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 09:15:10 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 09:15:10 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:15:11 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-04 09:15:11 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:15:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:15:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:15:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:15:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:15:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:21:41 | INFO | train_inner | epoch 001:    105 / 196 loss=16.407, ppl=86910, wps=18633.5, ups=0.28, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.626, loss_scale=4, train_wall=362, gb_free=7.2, wall=390
2022-03-04 09:27:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:27:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.004 | ppl 8216.05 | wps 33647.7 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-04 09:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-04 09:27:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:27:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.004) (writing took 9.00992127135396 seconds)
2022-03-04 09:27:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:27:16 | INFO | train | epoch 001 | loss 15.294 | ppl 40172.4 | wps 18196.8 | ups 0.28 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.629 | loss_scale 8 | train_wall 657 | gb_free 7.2 | wall 726
2022-03-04 09:27:16 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:27:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:27:48 | INFO | train_inner | epoch 002:      9 / 196 loss=13.976, ppl=16110.1, wps=17808.4, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.511, loss_scale=8, train_wall=325, gb_free=7.2, wall=758
2022-03-04 09:33:39 | INFO | train_inner | epoch 002:    109 / 196 loss=12.006, ppl=4113.19, wps=18657.7, ups=0.28, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.992, loss_scale=16, train_wall=325, gb_free=7.2, wall=1109
2022-03-04 09:38:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:38:51 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.272 | ppl 1236.46 | wps 33724.5 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.272
2022-03-04 09:38:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-04 09:38:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:38:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:08:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.272) (writing took 1795.8528443202376 seconds)
2022-03-04 10:08:47 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 10:08:47 | INFO | train | epoch 002 | loss 11.473 | ppl 2842.2 | wps 5148.6 | ups 0.08 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.836 | loss_scale 32 | train_wall 637 | gb_free 7.2 | wall 3217
2022-03-04 10:08:47 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 10:08:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:09:30 | INFO | train_inner | epoch 003:     13 / 196 loss=10.642, ppl=1597.92, wps=3038.3, ups=0.05, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.595, loss_scale=32, train_wall=322, gb_free=7.2, wall=3260
2022-03-04 10:15:03 | INFO | train_inner | epoch 003:    113 / 196 loss=10.045, ppl=1056.51, wps=19666.2, ups=0.3, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.53, loss_scale=32, train_wall=308, gb_free=7.2, wall=3593
2022-03-04 10:16:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:19:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:19:44 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.55 | ppl 749.84 | wps 38621.9 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.55
2022-03-04 10:19:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-04 10:19:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:19:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:19:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.55) (writing took 7.785077499225736 seconds)
2022-03-04 10:19:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 10:19:51 | INFO | train | epoch 003 | loss 9.934 | ppl 978.26 | wps 19215.7 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.545 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 3881
2022-03-04 10:19:51 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 10:19:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:20:51 | INFO | train_inner | epoch 004:     18 / 196 loss=9.707, ppl=835.73, wps=18807.8, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.59, loss_scale=32, train_wall=309, gb_free=7.2, wall=3941
2022-03-04 10:21:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:26:26 | INFO | train_inner | epoch 004:    119 / 196 loss=9.397, ppl=674.4, wps=19586.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.708, loss_scale=16, train_wall=309, gb_free=7.2, wall=4275
2022-03-04 10:30:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:30:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.069 | ppl 536.96 | wps 38217.8 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.069
2022-03-04 10:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-04 10:30:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.069) (writing took 7.677292482927442 seconds)
2022-03-04 10:30:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 10:30:53 | INFO | train | epoch 004 | loss 9.328 | ppl 642.56 | wps 19281.4 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.732 | loss_scale 32 | train_wall 600 | gb_free 7.2 | wall 4543
2022-03-04 10:30:53 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 10:30:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:32:09 | INFO | train_inner | epoch 005:     23 / 196 loss=9.145, ppl=565.98, wps=19009.3, ups=0.29, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.753, loss_scale=32, train_wall=306, gb_free=7.2, wall=4619
2022-03-04 10:35:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:37:44 | INFO | train_inner | epoch 005:    124 / 196 loss=8.903, ppl=478.58, wps=19598.8, ups=0.3, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.871, loss_scale=32, train_wall=309, gb_free=7.2, wall=4954
2022-03-04 10:41:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:41:49 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.677 | ppl 409.28 | wps 38639.9 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 8.677
2022-03-04 10:41:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-04 10:41:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:41:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 5 @ 972 updates, score 8.677) (writing took 7.942161101847887 seconds)
2022-03-04 10:41:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:41:57 | INFO | train | epoch 005 | loss 8.849 | ppl 461.01 | wps 19226.9 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.849 | loss_scale 32 | train_wall 601 | gb_free 7.2 | wall 5207
2022-03-04 10:41:57 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:43:30 | INFO | train_inner | epoch 006:     28 / 196 loss=8.685, ppl=411.45, wps=18879.4, ups=0.29, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.88, loss_scale=64, train_wall=308, gb_free=7.2, wall=5300
2022-03-04 10:44:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:49:05 | INFO | train_inner | epoch 006:    129 / 196 loss=8.477, ppl=356.42, wps=19541.9, ups=0.3, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.93, loss_scale=32, train_wall=310, gb_free=7.2, wall=5635
2022-03-04 10:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:52:52 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.35 | ppl 326.2 | wps 38090.7 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.35
2022-03-04 10:52:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-04 10:52:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:52:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.35) (writing took 8.211939616128802 seconds)
2022-03-04 10:53:00 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 10:53:00 | INFO | train | epoch 006 | loss 8.448 | ppl 349.27 | wps 19148.5 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.922 | loss_scale 16 | train_wall 600 | gb_free 7.2 | wall 5870
2022-03-04 10:53:00 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 10:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:54:53 | INFO | train_inner | epoch 007:     34 / 196 loss=8.302, ppl=315.54, wps=18804.3, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.936, loss_scale=16, train_wall=308, gb_free=7.2, wall=5983
2022-03-04 11:00:27 | INFO | train_inner | epoch 007:    134 / 196 loss=8.133, ppl=280.65, wps=19628.1, ups=0.3, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.944, loss_scale=32, train_wall=309, gb_free=7.2, wall=6317
2022-03-04 11:03:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:03:58 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.093 | ppl 273.1 | wps 39016.1 | wpb 510.9 | bsz 1 | num_updates 1362 | best_loss 8.093
2022-03-04 11:03:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1362 updates
2022-03-04 11:03:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:04:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:04:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 7 @ 1362 updates, score 8.093) (writing took 7.987914675846696 seconds)
2022-03-04 11:04:06 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 11:04:06 | INFO | train | epoch 007 | loss 8.116 | ppl 277.42 | wps 19273.7 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 1362 | lr 0.000170316 | gnorm 0.958 | loss_scale 32 | train_wall 603 | gb_free 7.2 | wall 6536
2022-03-04 11:04:06 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 11:04:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:06:15 | INFO | train_inner | epoch 008:     39 / 196 loss=7.979, ppl=252.33, wps=18788.2, ups=0.29, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.984, loss_scale=32, train_wall=309, gb_free=7.2, wall=6665
2022-03-04 11:11:49 | INFO | train_inner | epoch 008:    139 / 196 loss=7.825, ppl=226.77, wps=19633.7, ups=0.3, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.961, loss_scale=32, train_wall=309, gb_free=7.2, wall=6998
2022-03-04 11:12:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:15:05 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.85 | ppl 230.65 | wps 37013.8 | wpb 510.9 | bsz 1 | num_updates 1556 | best_loss 7.85
2022-03-04 11:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1556 updates
2022-03-04 11:15:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:15:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:15:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 8 @ 1556 updates, score 7.85) (writing took 8.165351560339332 seconds)
2022-03-04 11:15:13 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 11:15:13 | INFO | train | epoch 008 | loss 7.817 | ppl 225.48 | wps 19022.2 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 1556 | lr 0.000194561 | gnorm 0.977 | loss_scale 32 | train_wall 604 | gb_free 7.2 | wall 7203
2022-03-04 11:15:13 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 11:15:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:17:42 | INFO | train_inner | epoch 009:     44 / 196 loss=7.686, ppl=205.95, wps=18491, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.998, loss_scale=32, train_wall=314, gb_free=7.2, wall=7352
2022-03-04 11:20:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:23:23 | INFO | train_inner | epoch 009:    145 / 196 loss=7.54, ppl=186.08, wps=19249.7, ups=0.29, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.958, loss_scale=32, train_wall=315, gb_free=7.2, wall=7692
2022-03-04 11:26:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:26:20 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.652 | ppl 201.07 | wps 37285.2 | wpb 510.9 | bsz 1 | num_updates 1751 | best_loss 7.652
2022-03-04 11:26:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1751 updates
2022-03-04 11:26:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 9 @ 1751 updates, score 7.652) (writing took 8.072153393179178 seconds)
2022-03-04 11:26:28 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 11:26:28 | INFO | train | epoch 009 | loss 7.537 | ppl 185.72 | wps 18921.6 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 1751 | lr 0.000218931 | gnorm 0.979 | loss_scale 32 | train_wall 611 | gb_free 7.2 | wall 7878
2022-03-04 11:26:28 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 11:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:28:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:29:16 | INFO | train_inner | epoch 010:     50 / 196 loss=7.404, ppl=169.34, wps=18486.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.984, loss_scale=32, train_wall=314, gb_free=7.2, wall=8046
2022-03-04 11:34:56 | INFO | train_inner | epoch 010:    150 / 196 loss=7.27, ppl=154.29, wps=19294.9, ups=0.29, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.956, loss_scale=32, train_wall=314, gb_free=7.2, wall=8386
2022-03-04 11:35:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:37:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:37:37 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.468 | ppl 177.01 | wps 37637.3 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 7.468
2022-03-04 11:37:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-04 11:37:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:37:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 10 @ 1945 updates, score 7.468) (writing took 7.831052543595433 seconds)
2022-03-04 11:37:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 11:37:45 | INFO | train | epoch 010 | loss 7.274 | ppl 154.77 | wps 18754.5 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.955 | loss_scale 32 | train_wall 614 | gb_free 7.2 | wall 8555
2022-03-04 11:37:45 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 11:37:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:40:52 | INFO | train_inner | epoch 011:     55 / 196 loss=7.141, ppl=141.11, wps=18357.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.951, loss_scale=32, train_wall=317, gb_free=7.2, wall=8742
2022-03-04 11:43:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:46:34 | INFO | train_inner | epoch 011:    156 / 196 loss=7.028, ppl=130.53, wps=19124.5, ups=0.29, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.943, loss_scale=32, train_wall=317, gb_free=7.2, wall=9084
2022-03-04 11:48:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:48:56 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.343 | ppl 162.38 | wps 37030.4 | wpb 510.9 | bsz 1 | num_updates 2140 | best_loss 7.343
2022-03-04 11:48:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2140 updates
2022-03-04 11:48:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:49:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:49:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 11 @ 2140 updates, score 7.343) (writing took 7.203187372535467 seconds)
2022-03-04 11:49:03 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 11:49:03 | INFO | train | epoch 011 | loss 7.031 | ppl 130.78 | wps 18814.4 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 2140 | lr 0.000267547 | gnorm 0.944 | loss_scale 32 | train_wall 616 | gb_free 7.2 | wall 9233
2022-03-04 11:49:03 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 11:49:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:51:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:52:30 | INFO | train_inner | epoch 012:     61 / 196 loss=6.893, ppl=118.89, wps=18362.5, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.916, loss_scale=32, train_wall=317, gb_free=7.2, wall=9440
2022-03-04 11:58:13 | INFO | train_inner | epoch 012:    161 / 196 loss=6.799, ppl=111.33, wps=19146.3, ups=0.29, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.952, loss_scale=32, train_wall=317, gb_free=7.2, wall=9783
2022-03-04 11:58:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:00:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:00:18 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.193 | ppl 146.37 | wps 36187 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.193
2022-03-04 12:00:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-04 12:00:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:00:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.193) (writing took 8.617254015058279 seconds)
2022-03-04 12:00:27 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 12:00:27 | INFO | train | epoch 012 | loss 6.807 | ppl 111.93 | wps 18574.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.926 | loss_scale 32 | train_wall 619 | gb_free 7.2 | wall 9916
2022-03-04 12:00:27 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 12:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:04:12 | INFO | train_inner | epoch 013:     66 / 196 loss=6.668, ppl=101.67, wps=18184.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.895, loss_scale=32, train_wall=319, gb_free=7.2, wall=10142
2022-03-04 12:06:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:09:58 | INFO | train_inner | epoch 013:    167 / 196 loss=6.594, ppl=96.6, wps=18964.2, ups=0.29, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.911, loss_scale=32, train_wall=320, gb_free=7.2, wall=10488
2022-03-04 12:11:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:11:42 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.113 | ppl 138.41 | wps 36422.8 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 7.113
2022-03-04 12:11:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-04 12:11:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:11:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 13 @ 2529 updates, score 7.113) (writing took 7.440708439797163 seconds)
2022-03-04 12:11:50 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 12:11:50 | INFO | train | epoch 013 | loss 6.597 | ppl 96.83 | wps 18678.9 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.912 | loss_scale 32 | train_wall 620 | gb_free 7.2 | wall 10600
2022-03-04 12:11:50 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 12:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:14:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:15:56 | INFO | train_inner | epoch 014:     72 / 196 loss=6.463, ppl=88.19, wps=18227.7, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.922, loss_scale=32, train_wall=320, gb_free=7.2, wall=10846
2022-03-04 12:21:39 | INFO | train_inner | epoch 014:    172 / 196 loss=6.394, ppl=84.11, wps=19126.5, ups=0.29, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.868, loss_scale=32, train_wall=317, gb_free=7.2, wall=11189
2022-03-04 12:22:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:23:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:23:06 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.031 | ppl 130.74 | wps 36483.3 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 7.031
2022-03-04 12:23:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2723 updates
2022-03-04 12:23:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:23:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 14 @ 2723 updates, score 7.031) (writing took 7.393527491018176 seconds)
2022-03-04 12:23:14 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 12:23:14 | INFO | train | epoch 014 | loss 6.405 | ppl 84.75 | wps 18566.2 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 2723 | lr 0.000340407 | gnorm 0.888 | loss_scale 32 | train_wall 621 | gb_free 7.2 | wall 11284
2022-03-04 12:23:14 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 12:23:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:27:37 | INFO | train_inner | epoch 015:     77 / 196 loss=6.273, ppl=77.36, wps=18244, ups=0.28, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.889, loss_scale=32, train_wall=319, gb_free=7.2, wall=11547
2022-03-04 12:30:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:33:24 | INFO | train_inner | epoch 015:    178 / 196 loss=6.225, ppl=74.79, wps=18928.7, ups=0.29, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.894, loss_scale=32, train_wall=321, gb_free=7.2, wall=11893
2022-03-04 12:34:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:34:30 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.972 | ppl 125.52 | wps 36418.7 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 6.972
2022-03-04 12:34:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2918 updates
2022-03-04 12:34:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:34:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:34:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 15 @ 2918 updates, score 6.972) (writing took 7.45706033334136 seconds)
2022-03-04 12:34:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 12:34:38 | INFO | train | epoch 015 | loss 6.229 | ppl 75 | wps 18649.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 2918 | lr 0.000364777 | gnorm 0.891 | loss_scale 32 | train_wall 621 | gb_free 7.2 | wall 11968
2022-03-04 12:34:38 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 12:34:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:38:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:39:23 | INFO | train_inner | epoch 016:     83 / 196 loss=6.085, ppl=67.86, wps=18201, ups=0.28, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.869, loss_scale=32, train_wall=320, gb_free=7.2, wall=12253
2022-03-04 12:45:06 | INFO | train_inner | epoch 016:    183 / 196 loss=6.069, ppl=67.13, wps=19104.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.882, loss_scale=32, train_wall=318, gb_free=7.2, wall=12596
2022-03-04 12:45:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:45:55 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.942 | ppl 122.99 | wps 36416.3 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 6.942
2022-03-04 12:45:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3113 updates
2022-03-04 12:45:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:46:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 16 @ 3113 updates, score 6.942) (writing took 7.4424282517284155 seconds)
2022-03-04 12:46:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 12:46:03 | INFO | train | epoch 016 | loss 6.061 | ppl 66.79 | wps 18632.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 3113 | lr 0.000389147 | gnorm 0.874 | loss_scale 64 | train_wall 622 | gb_free 7.2 | wall 12653
2022-03-04 12:46:03 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 12:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:46:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:51:05 | INFO | train_inner | epoch 017:     88 / 196 loss=5.916, ppl=60.37, wps=18207.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.88, loss_scale=32, train_wall=320, gb_free=7.2, wall=12955
2022-03-04 12:53:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:56:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:56:54 | INFO | train_inner | epoch 017:    190 / 196 loss=5.914, ppl=60.3, wps=18754.5, ups=0.29, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.878, loss_scale=16, train_wall=324, gb_free=7.2, wall=13304
2022-03-04 12:57:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:57:20 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.94 | ppl 122.78 | wps 36138.7 | wpb 510.9 | bsz 1 | num_updates 3306 | best_loss 6.94
2022-03-04 12:57:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3306 updates
2022-03-04 12:57:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 12:57:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 17 @ 3306 updates, score 6.94) (writing took 7.467014521360397 seconds)
2022-03-04 12:57:27 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 12:57:27 | INFO | train | epoch 017 | loss 5.907 | ppl 60.01 | wps 18453.2 | ups 0.28 | wpb 65446.6 | bsz 127.8 | num_updates 3306 | lr 0.000413267 | gnorm 0.877 | loss_scale 16 | train_wall 621 | gb_free 7.2 | wall 13337
2022-03-04 12:57:27 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 12:57:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:02:50 | INFO | train_inner | epoch 018:     94 / 196 loss=5.757, ppl=54.07, wps=18370.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.857, loss_scale=16, train_wall=317, gb_free=7.2, wall=13660
2022-03-04 13:08:31 | INFO | train_inner | epoch 018:    194 / 196 loss=5.777, ppl=54.83, wps=19199.9, ups=0.29, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.851, loss_scale=32, train_wall=316, gb_free=7.2, wall=14001
2022-03-04 13:08:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:08:43 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.953 | ppl 123.89 | wps 37317.9 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 6.94
2022-03-04 13:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3502 updates
2022-03-04 13:08:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:08:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:08:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 18 @ 3502 updates, score 6.953) (writing took 4.384542342275381 seconds)
2022-03-04 13:08:48 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 13:08:48 | INFO | train | epoch 018 | loss 5.761 | ppl 54.22 | wps 18861.6 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 3502 | lr 0.000437762 | gnorm 0.857 | loss_scale 32 | train_wall 620 | gb_free 7.2 | wall 14017
2022-03-04 13:08:48 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 13:08:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:12:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:12:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:14:27 | INFO | train_inner | epoch 019:    100 / 196 loss=5.606, ppl=48.71, wps=18367.3, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.886, loss_scale=16, train_wall=320, gb_free=7.2, wall=14357
2022-03-04 13:19:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:19:58 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.934 | ppl 122.29 | wps 37378.1 | wpb 510.9 | bsz 1 | num_updates 3696 | best_loss 6.934
2022-03-04 13:19:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3696 updates
2022-03-04 13:19:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 13:20:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 13:20:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 19 @ 3696 updates, score 6.934) (writing took 7.545820811763406 seconds)
2022-03-04 13:20:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 13:20:06 | INFO | train | epoch 019 | loss 5.62 | ppl 49.18 | wps 18722.9 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 3696 | lr 0.000462008 | gnorm 0.863 | loss_scale 32 | train_wall 615 | gb_free 7.2 | wall 14696
2022-03-04 13:20:06 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 13:20:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:20:19 | INFO | train_inner | epoch 020:      4 / 196 loss=5.628, ppl=49.46, wps=18561.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.844, loss_scale=32, train_wall=314, gb_free=7.2, wall=14709
2022-03-04 13:22:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:26:02 | INFO | train_inner | epoch 020:    105 / 196 loss=5.461, ppl=44.05, wps=19129, ups=0.29, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.873, loss_scale=16, train_wall=317, gb_free=7.2, wall=15052
2022-03-04 13:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:31:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:31:16 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.01 | ppl 128.88 | wps 37512.2 | wpb 510.9 | bsz 1 | num_updates 3890 | best_loss 6.934
2022-03-04 13:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3890 updates
2022-03-04 13:31:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:31:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:31:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 20 @ 3890 updates, score 7.01) (writing took 4.400401450693607 seconds)
2022-03-04 13:31:21 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 13:31:21 | INFO | train | epoch 020 | loss 5.488 | ppl 44.88 | wps 18813.6 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 3890 | lr 0.000486253 | gnorm 0.856 | loss_scale 16 | train_wall 615 | gb_free 7.2 | wall 15370
2022-03-04 13:31:21 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 13:31:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:31:55 | INFO | train_inner | epoch 021:     10 / 196 loss=5.5, ppl=45.25, wps=18537.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.84, loss_scale=16, train_wall=317, gb_free=7.2, wall=15404
2022-03-04 13:37:32 | INFO | train_inner | epoch 021:    110 / 196 loss=5.34, ppl=40.5, wps=19429.1, ups=0.3, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.88, loss_scale=32, train_wall=312, gb_free=7.2, wall=15742
2022-03-04 13:38:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:42:32 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.01 | ppl 128.9 | wps 35583.4 | wpb 510.9 | bsz 1 | num_updates 4085 | best_loss 6.934
2022-03-04 13:42:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4085 updates
2022-03-04 13:42:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:42:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:42:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 21 @ 4085 updates, score 7.01) (writing took 6.099922254681587 seconds)
2022-03-04 13:42:38 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 13:42:38 | INFO | train | epoch 021 | loss 5.365 | ppl 41.21 | wps 18842.6 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 4085 | lr 0.000494771 | gnorm 0.873 | loss_scale 16 | train_wall 616 | gb_free 7.2 | wall 16048
2022-03-04 13:42:38 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 13:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:43:29 | INFO | train_inner | epoch 022:     15 / 196 loss=5.361, ppl=41.08, wps=18279.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.855, loss_scale=16, train_wall=320, gb_free=7.2, wall=16099
2022-03-04 13:49:14 | INFO | train_inner | epoch 022:    115 / 196 loss=5.212, ppl=37.07, wps=19033.2, ups=0.29, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.84, loss_scale=32, train_wall=319, gb_free=7.2, wall=16444
2022-03-04 13:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:53:58 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.134 | ppl 140.5 | wps 35720.1 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 6.934
2022-03-04 13:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-04 13:53:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:54:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 22 @ 4281 updates, score 7.134) (writing took 5.87740265391767 seconds)
2022-03-04 13:54:04 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 13:54:04 | INFO | train | epoch 022 | loss 5.227 | ppl 37.46 | wps 18703 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.829 | loss_scale 64 | train_wall 624 | gb_free 7.2 | wall 16734
2022-03-04 13:54:04 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 13:54:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:54:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:54:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:55:16 | INFO | train_inner | epoch 023:     21 / 196 loss=5.221, ppl=37.29, wps=18044.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.841, loss_scale=16, train_wall=324, gb_free=7.2, wall=16806
2022-03-04 14:01:00 | INFO | train_inner | epoch 023:    121 / 196 loss=5.076, ppl=33.74, wps=19049.9, ups=0.29, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.826, loss_scale=16, train_wall=318, gb_free=7.2, wall=17150
2022-03-04 14:05:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:05:23 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.121 | ppl 139.22 | wps 35645 | wpb 510.9 | bsz 1 | num_updates 4475 | best_loss 6.934
2022-03-04 14:05:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4475 updates
2022-03-04 14:05:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:05:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 23 @ 4475 updates, score 7.121) (writing took 5.966452231630683 seconds)
2022-03-04 14:05:29 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 14:05:29 | INFO | train | epoch 023 | loss 5.097 | ppl 34.22 | wps 18515.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 4475 | lr 0.000472719 | gnorm 0.835 | loss_scale 32 | train_wall 623 | gb_free 7.2 | wall 17419
2022-03-04 14:05:29 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 14:05:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:05:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:06:59 | INFO | train_inner | epoch 024:     26 / 196 loss=5.075, ppl=33.72, wps=18210, ups=0.28, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.813, loss_scale=16, train_wall=321, gb_free=7.2, wall=17509
2022-03-04 14:12:43 | INFO | train_inner | epoch 024:    126 / 196 loss=4.96, ppl=31.12, wps=19062.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.81, loss_scale=16, train_wall=318, gb_free=7.2, wall=17853
2022-03-04 14:16:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:16:49 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.185 | ppl 145.51 | wps 35714.9 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 6.934
2022-03-04 14:16:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-04 14:16:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:16:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 24 @ 4670 updates, score 7.185) (writing took 6.065389210358262 seconds)
2022-03-04 14:16:55 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 14:16:55 | INFO | train | epoch 024 | loss 4.976 | ppl 31.47 | wps 18620.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.807 | loss_scale 32 | train_wall 623 | gb_free 7.2 | wall 18105
2022-03-04 14:16:55 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 14:16:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:18:38 | INFO | train_inner | epoch 025:     30 / 196 loss=4.958, ppl=31.08, wps=18390.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.812, loss_scale=32, train_wall=318, gb_free=7.2, wall=18208
2022-03-04 14:20:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:24:26 | INFO | train_inner | epoch 025:    131 / 196 loss=4.849, ppl=28.81, wps=18862.3, ups=0.29, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.799, loss_scale=16, train_wall=322, gb_free=7.2, wall=18555
2022-03-04 14:28:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:28:14 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.182 | ppl 145.25 | wps 36626.2 | wpb 510.9 | bsz 1 | num_updates 4865 | best_loss 6.934
2022-03-04 14:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4865 updates
2022-03-04 14:28:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:28:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:28:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 25 @ 4865 updates, score 7.182) (writing took 5.866342205554247 seconds)
2022-03-04 14:28:20 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 14:28:20 | INFO | train | epoch 025 | loss 4.863 | ppl 29.11 | wps 18621.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 4865 | lr 0.000453376 | gnorm 0.812 | loss_scale 32 | train_wall 623 | gb_free 7.2 | wall 18790
2022-03-04 14:28:20 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 14:28:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:30:20 | INFO | train_inner | epoch 026:     35 / 196 loss=4.837, ppl=28.58, wps=18431.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.807, loss_scale=32, train_wall=317, gb_free=7.2, wall=18910
2022-03-04 14:35:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:36:06 | INFO | train_inner | epoch 026:    136 / 196 loss=4.751, ppl=26.92, wps=18944.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.822, loss_scale=32, train_wall=320, gb_free=7.2, wall=19256
2022-03-04 14:39:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:39:37 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.304 | ppl 158.05 | wps 35871.7 | wpb 510.9 | bsz 1 | num_updates 5060 | best_loss 6.934
2022-03-04 14:39:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5060 updates
2022-03-04 14:39:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:39:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:39:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 26 @ 5060 updates, score 7.304) (writing took 5.98514723405242 seconds)
2022-03-04 14:39:43 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 14:39:43 | INFO | train | epoch 026 | loss 4.755 | ppl 27.01 | wps 18692.6 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 5060 | lr 0.000444554 | gnorm 0.81 | loss_scale 32 | train_wall 621 | gb_free 7.2 | wall 19473
2022-03-04 14:39:43 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 14:39:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:42:01 | INFO | train_inner | epoch 027:     40 / 196 loss=4.713, ppl=26.23, wps=18443.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.793, loss_scale=32, train_wall=317, gb_free=7.2, wall=19610
2022-03-04 14:43:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:46:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:47:51 | INFO | train_inner | epoch 027:    142 / 196 loss=4.655, ppl=25.19, wps=18707.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.809, loss_scale=16, train_wall=324, gb_free=7.2, wall=19961
2022-03-04 14:50:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:51:01 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.374 | ppl 165.92 | wps 36901.4 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 6.934
2022-03-04 14:51:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5254 updates
2022-03-04 14:51:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:51:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:51:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 27 @ 5254 updates, score 7.374) (writing took 6.05521565862 seconds)
2022-03-04 14:51:07 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 14:51:07 | INFO | train | epoch 027 | loss 4.655 | ppl 25.19 | wps 18559.6 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 5254 | lr 0.00043627 | gnorm 0.807 | loss_scale 16 | train_wall 622 | gb_free 7.2 | wall 20157
2022-03-04 14:51:07 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 14:51:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:53:45 | INFO | train_inner | epoch 028:     46 / 196 loss=4.607, ppl=24.37, wps=18454.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.816, loss_scale=16, train_wall=317, gb_free=7.2, wall=20315
2022-03-04 14:59:29 | INFO | train_inner | epoch 028:    146 / 196 loss=4.556, ppl=23.52, wps=19067.2, ups=0.29, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.816, loss_scale=32, train_wall=318, gb_free=7.2, wall=20659
2022-03-04 15:01:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:02:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.411 | ppl 170.15 | wps 36124.5 | wpb 510.9 | bsz 1 | num_updates 5449 | best_loss 6.934
2022-03-04 15:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5449 updates
2022-03-04 15:02:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:02:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 28 @ 5449 updates, score 7.411) (writing took 5.747986156493425 seconds)
2022-03-04 15:02:32 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 15:02:32 | INFO | train | epoch 028 | loss 4.559 | ppl 23.58 | wps 18643.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 5449 | lr 0.000428392 | gnorm 0.807 | loss_scale 32 | train_wall 623 | gb_free 7.2 | wall 20841
2022-03-04 15:02:32 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 15:02:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:05:26 | INFO | train_inner | epoch 029:     51 / 196 loss=4.515, ppl=22.86, wps=18280.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.798, loss_scale=32, train_wall=320, gb_free=7.2, wall=21016
2022-03-04 15:09:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:11:13 | INFO | train_inner | epoch 029:    152 / 196 loss=4.473, ppl=22.2, wps=18884.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.816, loss_scale=32, train_wall=321, gb_free=7.2, wall=21363
2022-03-04 15:12:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:13:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:13:50 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.492 | ppl 179.96 | wps 36132.9 | wpb 510.9 | bsz 1 | num_updates 5643 | best_loss 6.934
2022-03-04 15:13:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5643 updates
2022-03-04 15:13:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 29 @ 5643 updates, score 7.492) (writing took 5.904926113784313 seconds)
2022-03-04 15:13:56 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 15:13:56 | INFO | train | epoch 029 | loss 4.469 | ppl 22.15 | wps 18557.8 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 5643 | lr 0.000420964 | gnorm 0.818 | loss_scale 16 | train_wall 622 | gb_free 7.2 | wall 21526
2022-03-04 15:13:56 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 15:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:17:11 | INFO | train_inner | epoch 030:     57 / 196 loss=4.415, ppl=21.33, wps=18297.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.82, loss_scale=16, train_wall=320, gb_free=7.2, wall=21721
2022-03-04 15:22:54 | INFO | train_inner | epoch 030:    157 / 196 loss=4.394, ppl=21.03, wps=19089.8, ups=0.29, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.816, loss_scale=32, train_wall=318, gb_free=7.2, wall=22064
2022-03-04 15:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:25:13 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.589 | ppl 192.48 | wps 35772.9 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 6.934
2022-03-04 15:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-04 15:25:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.589) (writing took 6.095376009121537 seconds)
2022-03-04 15:25:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 15:25:19 | INFO | train | epoch 030 | loss 4.384 | ppl 20.88 | wps 18769.2 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.812 | loss_scale 32 | train_wall 621 | gb_free 7.2 | wall 22209
2022-03-04 15:25:19 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 15:25:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:27:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:28:52 | INFO | train_inner | epoch 031:     62 / 196 loss=4.318, ppl=19.94, wps=18253.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.826, loss_scale=32, train_wall=320, gb_free=7.2, wall=22422
2022-03-04 15:29:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:34:35 | INFO | train_inner | epoch 031:    163 / 196 loss=4.322, ppl=20, wps=19093.6, ups=0.29, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.829, loss_scale=16, train_wall=318, gb_free=7.2, wall=22765
2022-03-04 15:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:36:37 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.621 | ppl 196.89 | wps 34123.1 | wpb 510.9 | bsz 1 | num_updates 6033 | best_loss 6.934
2022-03-04 15:36:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6033 updates
2022-03-04 15:36:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:36:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:36:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 31 @ 6033 updates, score 7.621) (writing took 5.40051425807178 seconds)
2022-03-04 15:36:42 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 15:36:42 | INFO | train | epoch 031 | loss 4.301 | ppl 19.71 | wps 18590.2 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 6033 | lr 0.00040713 | gnorm 0.828 | loss_scale 16 | train_wall 621 | gb_free 7.2 | wall 22892
2022-03-04 15:36:42 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 15:36:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:40:39 | INFO | train_inner | epoch 032:     67 / 196 loss=4.223, ppl=18.68, wps=17982.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.809, loss_scale=32, train_wall=325, gb_free=7.2, wall=23129
2022-03-04 15:44:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:46:35 | INFO | train_inner | epoch 032:    168 / 196 loss=4.248, ppl=19.01, wps=18375.7, ups=0.28, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.827, loss_scale=32, train_wall=330, gb_free=7.2, wall=23485
2022-03-04 15:47:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:48:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:48:20 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.705 | ppl 208.64 | wps 33941.1 | wpb 510.9 | bsz 1 | num_updates 6227 | best_loss 6.934
2022-03-04 15:48:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6227 updates
2022-03-04 15:48:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:48:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 32 @ 6227 updates, score 7.705) (writing took 5.294102780520916 seconds)
2022-03-04 15:48:25 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 15:48:25 | INFO | train | epoch 032 | loss 4.224 | ppl 18.68 | wps 18062.5 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 6227 | lr 0.000400738 | gnorm 0.822 | loss_scale 16 | train_wall 639 | gb_free 7.2 | wall 23595
2022-03-04 15:48:25 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 15:48:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:52:43 | INFO | train_inner | epoch 033:     73 / 196 loss=4.15, ppl=17.75, wps=17771.7, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.827, loss_scale=16, train_wall=329, gb_free=7.2, wall=23853
2022-03-04 15:58:35 | INFO | train_inner | epoch 033:    173 / 196 loss=4.175, ppl=18.07, wps=18625.5, ups=0.28, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.845, loss_scale=32, train_wall=326, gb_free=7.2, wall=24205
2022-03-04 15:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:00:01 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.778 | ppl 219.46 | wps 34376.7 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 6.934
2022-03-04 16:00:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6423 updates
2022-03-04 16:00:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:00:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:00:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 33 @ 6423 updates, score 7.778) (writing took 4.876722348853946 seconds)
2022-03-04 16:00:06 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 16:00:06 | INFO | train | epoch 033 | loss 4.149 | ppl 17.74 | wps 18294.6 | ups 0.28 | wpb 65448 | bsz 127.8 | num_updates 6423 | lr 0.000394576 | gnorm 0.828 | loss_scale 32 | train_wall 638 | gb_free 7.2 | wall 24296
2022-03-04 16:00:06 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 16:00:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:03:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:04:40 | INFO | train_inner | epoch 034:     78 / 196 loss=4.063, ppl=16.71, wps=17925.3, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.823, loss_scale=32, train_wall=327, gb_free=7.2, wall=24570
2022-03-04 16:10:29 | INFO | train_inner | epoch 034:    178 / 196 loss=4.109, ppl=17.25, wps=18765.3, ups=0.29, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.845, loss_scale=32, train_wall=323, gb_free=7.2, wall=24919
2022-03-04 16:11:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:11:37 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.834 | ppl 228.21 | wps 34527.6 | wpb 510.9 | bsz 1 | num_updates 6617 | best_loss 6.934
2022-03-04 16:11:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6617 updates
2022-03-04 16:11:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:11:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 34 @ 6617 updates, score 7.834) (writing took 4.819475265219808 seconds)
2022-03-04 16:11:42 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 16:11:42 | INFO | train | epoch 034 | loss 4.078 | ppl 16.89 | wps 18249.5 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 6617 | lr 0.000388749 | gnorm 0.837 | loss_scale 32 | train_wall 633 | gb_free 7.2 | wall 24992
2022-03-04 16:11:42 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 16:11:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:16:32 | INFO | train_inner | epoch 035:     83 / 196 loss=3.989, ppl=15.88, wps=18023.1, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.828, loss_scale=32, train_wall=325, gb_free=7.2, wall=25282
2022-03-04 16:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:22:27 | INFO | train_inner | epoch 035:    184 / 196 loss=4.052, ppl=16.58, wps=18462, ups=0.28, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.839, loss_scale=32, train_wall=329, gb_free=7.2, wall=25636
2022-03-04 16:23:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:23:14 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.86 | ppl 232.4 | wps 34324.6 | wpb 510.9 | bsz 1 | num_updates 6812 | best_loss 6.934
2022-03-04 16:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6812 updates
2022-03-04 16:23:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:23:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:23:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 35 @ 6812 updates, score 7.86) (writing took 4.855936419218779 seconds)
2022-03-04 16:23:19 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 16:23:19 | INFO | train | epoch 035 | loss 4.011 | ppl 16.12 | wps 18307.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 6812 | lr 0.000383145 | gnorm 0.835 | loss_scale 32 | train_wall 635 | gb_free 7.2 | wall 25689
2022-03-04 16:23:19 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 16:23:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:26:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:28:33 | INFO | train_inner | epoch 036:     89 / 196 loss=3.914, ppl=15.07, wps=17854.7, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.835, loss_scale=32, train_wall=328, gb_free=7.2, wall=26003
2022-03-04 16:34:24 | INFO | train_inner | epoch 036:    189 / 196 loss=3.991, ppl=15.9, wps=18636.7, ups=0.28, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.849, loss_scale=64, train_wall=325, gb_free=7.2, wall=26354
2022-03-04 16:34:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:34:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:34:55 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.923 | ppl 242.76 | wps 33647.3 | wpb 510.9 | bsz 1 | num_updates 7006 | best_loss 6.934
2022-03-04 16:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7006 updates
2022-03-04 16:34:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:34:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:35:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 36 @ 7006 updates, score 7.923) (writing took 4.889014180749655 seconds)
2022-03-04 16:35:00 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 16:35:00 | INFO | train | epoch 036 | loss 3.946 | ppl 15.41 | wps 18126.2 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 7006 | lr 0.000377803 | gnorm 0.844 | loss_scale 32 | train_wall 637 | gb_free 7.2 | wall 26390
2022-03-04 16:35:00 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 16:35:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:40:33 | INFO | train_inner | epoch 037:     94 / 196 loss=3.847, ppl=14.39, wps=17747.5, ups=0.27, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.839, loss_scale=32, train_wall=330, gb_free=7.2, wall=26723
2022-03-04 16:42:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:46:30 | INFO | train_inner | epoch 037:    195 / 196 loss=3.929, ppl=15.23, wps=18319.5, ups=0.28, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.849, loss_scale=32, train_wall=331, gb_free=7.2, wall=27080
2022-03-04 16:46:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:46:40 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.027 | ppl 260.81 | wps 33052.6 | wpb 510.9 | bsz 1 | num_updates 7201 | best_loss 6.934
2022-03-04 16:46:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7201 updates
2022-03-04 16:46:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:46:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:46:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 37 @ 7201 updates, score 8.027) (writing took 4.9120430164039135 seconds)
2022-03-04 16:46:45 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 16:46:45 | INFO | train | epoch 037 | loss 3.884 | ppl 14.77 | wps 18103.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 7201 | lr 0.000372652 | gnorm 0.844 | loss_scale 32 | train_wall 641 | gb_free 7.2 | wall 27094
2022-03-04 16:46:45 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 16:46:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:49:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:52:40 | INFO | train_inner | epoch 038:    100 / 196 loss=3.781, ppl=13.75, wps=17662.4, ups=0.27, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.855, loss_scale=16, train_wall=331, gb_free=7.2, wall=27450
2022-03-04 16:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:58:28 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.095 | ppl 273.34 | wps 32900.2 | wpb 510.9 | bsz 1 | num_updates 7396 | best_loss 6.934
2022-03-04 16:58:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7396 updates
2022-03-04 16:58:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 38 @ 7396 updates, score 8.095) (writing took 4.93441473133862 seconds)
2022-03-04 16:58:33 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 16:58:33 | INFO | train | epoch 038 | loss 3.826 | ppl 14.18 | wps 18022.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 7396 | lr 0.000367707 | gnorm 0.863 | loss_scale 32 | train_wall 644 | gb_free 7.2 | wall 27803
2022-03-04 16:58:33 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 16:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:58:47 | INFO | train_inner | epoch 039:      4 / 196 loss=3.864, ppl=14.56, wps=17839.7, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.871, loss_scale=32, train_wall=328, gb_free=7.2, wall=27817
2022-03-04 17:04:39 | INFO | train_inner | epoch 039:    104 / 196 loss=3.719, ppl=13.17, wps=18616.2, ups=0.28, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.846, loss_scale=32, train_wall=326, gb_free=7.2, wall=28169
2022-03-04 17:05:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:10:05 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.203 | ppl 294.76 | wps 34711.5 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 6.934
2022-03-04 17:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-04 17:10:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 39 @ 7591 updates, score 8.203) (writing took 4.774577580392361 seconds)
2022-03-04 17:10:10 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 17:10:10 | INFO | train | epoch 039 | loss 3.767 | ppl 13.62 | wps 18308.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.854 | loss_scale 32 | train_wall 635 | gb_free 7.2 | wall 28500
2022-03-04 17:10:10 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 17:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:10:41 | INFO | train_inner | epoch 040:      9 / 196 loss=3.804, ppl=13.96, wps=18036.3, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.861, loss_scale=32, train_wall=325, gb_free=7.2, wall=28531
2022-03-04 17:12:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:16:35 | INFO | train_inner | epoch 040:    110 / 196 loss=3.678, ppl=12.8, wps=18549.7, ups=0.28, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.85, loss_scale=32, train_wall=327, gb_free=7.2, wall=28884
2022-03-04 17:20:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:21:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:21:42 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.229 | ppl 300.04 | wps 34381.4 | wpb 510.9 | bsz 1 | num_updates 7785 | best_loss 6.934
2022-03-04 17:21:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7785 updates
2022-03-04 17:21:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:21:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:21:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 40 @ 7785 updates, score 8.229) (writing took 4.774745810776949 seconds)
2022-03-04 17:21:47 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 17:21:47 | INFO | train | epoch 040 | loss 3.713 | ppl 13.11 | wps 18210.2 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 7785 | lr 0.000358402 | gnorm 0.859 | loss_scale 32 | train_wall 635 | gb_free 7.2 | wall 29197
2022-03-04 17:21:47 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 17:21:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:22:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:43 | INFO | train_inner | epoch 041:     16 / 196 loss=3.739, ppl=13.35, wps=17723.4, ups=0.27, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.88, loss_scale=16, train_wall=331, gb_free=7.2, wall=29253
2022-03-04 17:28:35 | INFO | train_inner | epoch 041:    116 / 196 loss=3.628, ppl=12.36, wps=18623.9, ups=0.28, wpb=65532.4, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.855, loss_scale=16, train_wall=326, gb_free=7.2, wall=29605
2022-03-04 17:33:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:33:21 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.332 | ppl 322.13 | wps 34365.5 | wpb 510.9 | bsz 1 | num_updates 7980 | best_loss 6.934
2022-03-04 17:33:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7980 updates
2022-03-04 17:33:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:33:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:33:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 41 @ 7980 updates, score 8.332) (writing took 4.654891323298216 seconds)
2022-03-04 17:33:26 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 17:33:26 | INFO | train | epoch 041 | loss 3.661 | ppl 12.65 | wps 18265.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 7980 | lr 0.000353996 | gnorm 0.87 | loss_scale 32 | train_wall 636 | gb_free 7.2 | wall 29896
2022-03-04 17:33:26 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 17:33:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:34:36 | INFO | train_inner | epoch 042:     20 / 196 loss=3.679, ppl=12.81, wps=18116.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.882, loss_scale=32, train_wall=323, gb_free=7.2, wall=29966
2022-03-04 17:37:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:40:33 | INFO | train_inner | epoch 042:    121 / 196 loss=3.586, ppl=12.01, wps=18342.7, ups=0.28, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.869, loss_scale=32, train_wall=330, gb_free=7.2, wall=30323
2022-03-04 17:44:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:45:06 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.42 | ppl 342.46 | wps 32699.7 | wpb 510.9 | bsz 1 | num_updates 8175 | best_loss 6.934
2022-03-04 17:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8175 updates
2022-03-04 17:45:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:45:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:45:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 42 @ 8175 updates, score 8.42) (writing took 4.849944496527314 seconds)
2022-03-04 17:45:10 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 17:45:10 | INFO | train | epoch 042 | loss 3.611 | ppl 12.22 | wps 18110.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 8175 | lr 0.000349749 | gnorm 0.876 | loss_scale 32 | train_wall 641 | gb_free 7.2 | wall 30600
2022-03-04 17:45:10 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 17:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:46:43 | INFO | train_inner | epoch 043:     26 / 196 loss=3.619, ppl=12.29, wps=17663.2, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.882, loss_scale=32, train_wall=331, gb_free=7.2, wall=30693
2022-03-04 17:52:41 | INFO | train_inner | epoch 043:    126 / 196 loss=3.544, ppl=11.67, wps=18327.3, ups=0.28, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.875, loss_scale=32, train_wall=330, gb_free=7.2, wall=31051
2022-03-04 17:53:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:56:57 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.43 | ppl 345.01 | wps 32319.4 | wpb 510.9 | bsz 1 | num_updates 8368 | best_loss 6.934
2022-03-04 17:56:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8368 updates
2022-03-04 17:56:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:57:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:57:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 43 @ 8368 updates, score 8.43) (writing took 5.552073271945119 seconds)
2022-03-04 17:57:02 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 17:57:02 | INFO | train | epoch 043 | loss 3.56 | ppl 11.79 | wps 17745.3 | ups 0.27 | wpb 65446.6 | bsz 127.8 | num_updates 8368 | lr 0.000345692 | gnorm 0.882 | loss_scale 16 | train_wall 646 | gb_free 7.2 | wall 31312
2022-03-04 17:57:02 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 17:57:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:58:53 | INFO | train_inner | epoch 044:     32 / 196 loss=3.552, ppl=11.73, wps=17563, ups=0.27, wpb=65359.9, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.885, loss_scale=16, train_wall=333, gb_free=7.2, wall=31423
2022-03-04 18:04:32 | INFO | train_inner | epoch 044:    132 / 196 loss=3.504, ppl=11.35, wps=19364.2, ups=0.3, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.876, loss_scale=32, train_wall=313, gb_free=7.2, wall=31761
2022-03-04 18:08:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:08:14 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.546 | ppl 373.68 | wps 36782.1 | wpb 510.9 | bsz 1 | num_updates 8564 | best_loss 6.934
2022-03-04 18:08:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8564 updates
2022-03-04 18:08:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:08:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:08:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 44 @ 8564 updates, score 8.546) (writing took 4.728013014420867 seconds)
2022-03-04 18:08:19 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 18:08:19 | INFO | train | epoch 044 | loss 3.516 | ppl 11.44 | wps 18967.4 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 8564 | lr 0.000341713 | gnorm 0.885 | loss_scale 32 | train_wall 616 | gb_free 7.2 | wall 31988
2022-03-04 18:08:19 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 18:08:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:08:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:10:24 | INFO | train_inner | epoch 045:     37 / 196 loss=3.51, ppl=11.39, wps=18553.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.898, loss_scale=32, train_wall=316, gb_free=7.2, wall=32114
2022-03-04 18:13:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:16:06 | INFO | train_inner | epoch 045:    138 / 196 loss=3.462, ppl=11.02, wps=19145, ups=0.29, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.898, loss_scale=16, train_wall=316, gb_free=7.2, wall=32456
2022-03-04 18:19:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:19:28 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.644 | ppl 399.96 | wps 36880.7 | wpb 510.9 | bsz 1 | num_updates 8758 | best_loss 6.934
2022-03-04 18:19:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8758 updates
2022-03-04 18:19:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:19:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:19:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 45 @ 8758 updates, score 8.644) (writing took 4.18471946939826 seconds)
2022-03-04 18:19:32 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 18:19:32 | INFO | train | epoch 045 | loss 3.468 | ppl 11.07 | wps 18855.4 | ups 0.29 | wpb 65448.9 | bsz 127.8 | num_updates 8758 | lr 0.000337907 | gnorm 0.902 | loss_scale 16 | train_wall 613 | gb_free 7.2 | wall 32662
2022-03-04 18:19:32 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 18:19:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:21:54 | INFO | train_inner | epoch 046:     42 / 196 loss=3.458, ppl=10.99, wps=18799, ups=0.29, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.899, loss_scale=32, train_wall=312, gb_free=7.2, wall=32804
2022-03-04 18:27:33 | INFO | train_inner | epoch 046:    142 / 196 loss=3.42, ppl=10.7, wps=19352.5, ups=0.3, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.9, loss_scale=32, train_wall=313, gb_free=7.2, wall=33142
2022-03-04 18:28:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:30:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:30:40 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.643 | ppl 399.7 | wps 36896 | wpb 510.9 | bsz 1 | num_updates 8953 | best_loss 6.934
2022-03-04 18:30:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8953 updates
2022-03-04 18:30:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 46 @ 8953 updates, score 8.643) (writing took 5.957594549283385 seconds)
2022-03-04 18:30:46 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 18:30:46 | INFO | train | epoch 046 | loss 3.425 | ppl 10.74 | wps 18922 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 8953 | lr 0.000334207 | gnorm 0.891 | loss_scale 32 | train_wall 613 | gb_free 7.2 | wall 33336
2022-03-04 18:30:46 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 18:30:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:33:26 | INFO | train_inner | epoch 047:     47 / 196 loss=3.399, ppl=10.55, wps=18497.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.894, loss_scale=32, train_wall=316, gb_free=7.2, wall=33496
2022-03-04 18:35:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:39:08 | INFO | train_inner | epoch 047:    148 / 196 loss=3.388, ppl=10.47, wps=19179.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.894, loss_scale=32, train_wall=316, gb_free=7.2, wall=33838
2022-03-04 18:41:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:41:55 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.737 | ppl 426.53 | wps 37179.2 | wpb 510.9 | bsz 1 | num_updates 9148 | best_loss 6.934
2022-03-04 18:41:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9148 updates
2022-03-04 18:41:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:42:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 47 @ 9148 updates, score 8.737) (writing took 5.803955540060997 seconds)
2022-03-04 18:42:01 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 18:42:01 | INFO | train | epoch 047 | loss 3.384 | ppl 10.44 | wps 18913.3 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 9148 | lr 0.000330626 | gnorm 0.896 | loss_scale 32 | train_wall 613 | gb_free 7.2 | wall 34011
2022-03-04 18:42:01 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 18:42:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:43:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:45:01 | INFO | train_inner | epoch 048:     53 / 196 loss=3.351, ppl=10.21, wps=18499.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.89, loss_scale=32, train_wall=316, gb_free=7.2, wall=34191
2022-03-04 18:50:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:50:43 | INFO | train_inner | epoch 048:    154 / 196 loss=3.357, ppl=10.25, wps=19150.8, ups=0.29, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.908, loss_scale=32, train_wall=317, gb_free=7.2, wall=34533
2022-03-04 18:51:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:53:11 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.781 | ppl 439.83 | wps 36070.8 | wpb 510.9 | bsz 1 | num_updates 9341 | best_loss 6.934
2022-03-04 18:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9341 updates
2022-03-04 18:53:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:53:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:53:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 48 @ 9341 updates, score 8.781) (writing took 5.9765996634960175 seconds)
2022-03-04 18:53:17 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 18:53:17 | INFO | train | epoch 048 | loss 3.341 | ppl 10.14 | wps 18703.6 | ups 0.29 | wpb 65446.6 | bsz 127.8 | num_updates 9341 | lr 0.000327192 | gnorm 0.903 | loss_scale 16 | train_wall 613 | gb_free 7.2 | wall 34686
2022-03-04 18:53:17 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 18:53:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:56:35 | INFO | train_inner | epoch 049:     59 / 196 loss=3.304, ppl=9.88, wps=18554.1, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.919, loss_scale=16, train_wall=315, gb_free=7.2, wall=34885
2022-03-04 19:02:12 | INFO | train_inner | epoch 049:    159 / 196 loss=3.322, ppl=10, wps=19480.2, ups=0.3, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.895, loss_scale=32, train_wall=311, gb_free=7.2, wall=35222
2022-03-04 19:04:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:04:22 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.867 | ppl 466.89 | wps 36810.4 | wpb 510.9 | bsz 1 | num_updates 9537 | best_loss 6.934
2022-03-04 19:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9537 updates
2022-03-04 19:04:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:04:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:04:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 49 @ 9537 updates, score 8.867) (writing took 4.7934714406728745 seconds)
2022-03-04 19:04:27 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 19:04:27 | INFO | train | epoch 049 | loss 3.304 | ppl 9.88 | wps 19145.9 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 9537 | lr 0.000323813 | gnorm 0.905 | loss_scale 32 | train_wall 610 | gb_free 7.2 | wall 35356
2022-03-04 19:04:27 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 19:04:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:05:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:05:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:08:05 | INFO | train_inner | epoch 050:     65 / 196 loss=3.266, ppl=9.62, wps=18517.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.911, loss_scale=16, train_wall=316, gb_free=7.2, wall=35575
2022-03-04 19:13:44 | INFO | train_inner | epoch 050:    165 / 196 loss=3.286, ppl=9.76, wps=19346.6, ups=0.3, wpb=65532.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.913, loss_scale=32, train_wall=313, gb_free=7.2, wall=35913
2022-03-04 19:15:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:15:34 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.879 | ppl 470.84 | wps 36922 | wpb 510.9 | bsz 1 | num_updates 9731 | best_loss 6.934
2022-03-04 19:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9731 updates
2022-03-04 19:15:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:15:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:15:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 50 @ 9731 updates, score 8.879) (writing took 4.569205533713102 seconds)
2022-03-04 19:15:38 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 19:15:38 | INFO | train | epoch 050 | loss 3.266 | ppl 9.62 | wps 18906 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 9731 | lr 0.000320569 | gnorm 0.915 | loss_scale 32 | train_wall 611 | gb_free 7.2 | wall 36028
2022-03-04 19:15:38 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 19:15:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:15:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:19:35 | INFO | train_inner | epoch 051:     70 / 196 loss=3.209, ppl=9.25, wps=18579, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.926, loss_scale=16, train_wall=315, gb_free=7.2, wall=36265
2022-03-04 19:25:15 | INFO | train_inner | epoch 051:    170 / 196 loss=3.26, ppl=9.58, wps=19309.5, ups=0.29, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.934, loss_scale=32, train_wall=314, gb_free=7.2, wall=36605
2022-03-04 19:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:26:48 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.933 | ppl 488.83 | wps 36635.8 | wpb 510.9 | bsz 1 | num_updates 9926 | best_loss 6.934
2022-03-04 19:26:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9926 updates
2022-03-04 19:26:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:26:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 51 @ 9926 updates, score 8.933) (writing took 6.072732800617814 seconds)
2022-03-04 19:26:54 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 19:26:54 | INFO | train | epoch 051 | loss 3.229 | ppl 9.37 | wps 18876.5 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 9926 | lr 0.000317404 | gnorm 0.928 | loss_scale 32 | train_wall 614 | gb_free 7.2 | wall 36704
2022-03-04 19:26:54 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 19:26:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:30:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:31:08 | INFO | train_inner | epoch 052:     75 / 196 loss=3.18, ppl=9.06, wps=18482.1, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.918, loss_scale=32, train_wall=316, gb_free=7.2, wall=36958
2022-03-04 19:36:47 | INFO | train_inner | epoch 052:    175 / 196 loss=3.225, ppl=9.35, wps=19348.6, ups=0.3, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.937, loss_scale=32, train_wall=313, gb_free=7.2, wall=37297
2022-03-04 19:37:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:37:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:38:03 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.119 | ppl 555.85 | wps 36558.5 | wpb 510.9 | bsz 1 | num_updates 10120 | best_loss 6.934
2022-03-04 19:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10120 updates
2022-03-04 19:38:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:38:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:38:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 52 @ 10120 updates, score 9.119) (writing took 5.94360926002264 seconds)
2022-03-04 19:38:09 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 19:38:09 | INFO | train | epoch 052 | loss 3.193 | ppl 9.15 | wps 18806.6 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 10120 | lr 0.000314347 | gnorm 0.928 | loss_scale 32 | train_wall 613 | gb_free 7.2 | wall 37379
2022-03-04 19:38:09 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 19:38:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:42:41 | INFO | train_inner | epoch 053:     80 / 196 loss=3.132, ppl=8.77, wps=18493, ups=0.28, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.912, loss_scale=32, train_wall=316, gb_free=7.2, wall=37651
2022-03-04 19:43:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:48:22 | INFO | train_inner | epoch 053:    181 / 196 loss=3.198, ppl=9.18, wps=19177.6, ups=0.29, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.952, loss_scale=16, train_wall=316, gb_free=7.2, wall=37992
2022-03-04 19:49:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:49:18 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.058 | ppl 532.89 | wps 36777.5 | wpb 510.9 | bsz 1 | num_updates 10315 | best_loss 6.934
2022-03-04 19:49:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10315 updates
2022-03-04 19:49:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:49:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:49:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 53 @ 10315 updates, score 9.058) (writing took 5.983774892985821 seconds)
2022-03-04 19:49:24 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 19:49:24 | INFO | train | epoch 053 | loss 3.159 | ppl 8.93 | wps 18906.5 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 10315 | lr 0.000311362 | gnorm 0.935 | loss_scale 16 | train_wall 613 | gb_free 7.2 | wall 38054
2022-03-04 19:49:24 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 19:49:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:54:13 | INFO | train_inner | epoch 054:     85 / 196 loss=3.085, ppl=8.48, wps=18654.2, ups=0.29, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.927, loss_scale=32, train_wall=313, gb_free=7.2, wall=38343
2022-03-04 19:58:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:59:55 | INFO | train_inner | epoch 054:    186 / 196 loss=3.17, ppl=9, wps=19177.7, ups=0.29, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.922, loss_scale=32, train_wall=316, gb_free=7.2, wall=38684
2022-03-04 19:59:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:00:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:00:34 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.182 | ppl 580.74 | wps 36876.2 | wpb 510.9 | bsz 1 | num_updates 10509 | best_loss 6.934
2022-03-04 20:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10509 updates
2022-03-04 20:00:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 54 @ 10509 updates, score 9.182) (writing took 4.261135948821902 seconds)
2022-03-04 20:00:38 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 20:00:38 | INFO | train | epoch 054 | loss 3.124 | ppl 8.72 | wps 18850.3 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 10509 | lr 0.000308475 | gnorm 0.925 | loss_scale 16 | train_wall 614 | gb_free 7.2 | wall 38728
2022-03-04 20:00:38 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 20:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:05:46 | INFO | train_inner | epoch 055:     91 / 196 loss=3.053, ppl=8.3, wps=18596, ups=0.28, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.921, loss_scale=16, train_wall=316, gb_free=7.2, wall=39036
2022-03-04 20:11:25 | INFO | train_inner | epoch 055:    191 / 196 loss=3.143, ppl=8.83, wps=19358.1, ups=0.3, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.95, loss_scale=32, train_wall=313, gb_free=7.2, wall=39374
2022-03-04 20:11:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:11:47 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.2 | ppl 588.09 | wps 36906 | wpb 510.9 | bsz 1 | num_updates 10705 | best_loss 6.934
2022-03-04 20:11:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10705 updates
2022-03-04 20:11:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:11:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 55 @ 10705 updates, score 9.2) (writing took 4.3058703280985355 seconds)
2022-03-04 20:11:51 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 20:11:51 | INFO | train | epoch 055 | loss 3.095 | ppl 8.54 | wps 19059.5 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 10705 | lr 0.000305638 | gnorm 0.933 | loss_scale 32 | train_wall 613 | gb_free 7.2 | wall 39401
2022-03-04 20:11:51 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 20:11:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:14:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:17:16 | INFO | train_inner | epoch 056:     96 / 196 loss=3.018, ppl=8.1, wps=18616.9, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.922, loss_scale=32, train_wall=315, gb_free=7.2, wall=39726
2022-03-04 20:17:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:22:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:22:59 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.242 | ppl 605.63 | wps 36869.3 | wpb 510.9 | bsz 1 | num_updates 10899 | best_loss 6.934
2022-03-04 20:22:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10899 updates
2022-03-04 20:22:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:23:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:23:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 56 @ 10899 updates, score 9.242) (writing took 5.925712568685412 seconds)
2022-03-04 20:23:05 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 20:23:05 | INFO | train | epoch 056 | loss 3.06 | ppl 8.34 | wps 18842.6 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 10899 | lr 0.000302905 | gnorm 0.936 | loss_scale 16 | train_wall 613 | gb_free 7.2 | wall 40075
2022-03-04 20:23:05 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 20:23:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:23:08 | INFO | train_inner | epoch 057:      1 / 196 loss=3.106, ppl=8.61, wps=18536.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=10900, lr=0.000302891, gnorm=0.951, loss_scale=16, train_wall=315, gb_free=7.2, wall=40078
2022-03-04 20:27:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:28:50 | INFO | train_inner | epoch 057:    102 / 196 loss=2.986, ppl=7.92, wps=19155.9, ups=0.29, wpb=65532.4, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.939, loss_scale=16, train_wall=316, gb_free=7.2, wall=40420
2022-03-04 20:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:34:12 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.334 | ppl 645.45 | wps 37397.2 | wpb 510.9 | bsz 1 | num_updates 11094 | best_loss 6.934
2022-03-04 20:34:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11094 updates
2022-03-04 20:34:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:34:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 57 @ 11094 updates, score 9.334) (writing took 5.482595290988684 seconds)
2022-03-04 20:34:18 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 20:34:18 | INFO | train | epoch 057 | loss 3.031 | ppl 8.17 | wps 18957.6 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 11094 | lr 0.000300231 | gnorm 0.947 | loss_scale 16 | train_wall 612 | gb_free 7.2 | wall 40748
2022-03-04 20:34:18 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 20:34:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:34:38 | INFO | train_inner | epoch 058:      6 / 196 loss=3.07, ppl=8.4, wps=18794.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.954, loss_scale=32, train_wall=311, gb_free=7.2, wall=40768
2022-03-04 20:40:15 | INFO | train_inner | epoch 058:    106 / 196 loss=2.961, ppl=7.79, wps=19466.4, ups=0.3, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.931, loss_scale=32, train_wall=312, gb_free=7.2, wall=41105
2022-03-04 20:41:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:45:24 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.388 | ppl 669.9 | wps 36706 | wpb 510.9 | bsz 1 | num_updates 11289 | best_loss 6.934
2022-03-04 20:45:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11289 updates
2022-03-04 20:45:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:45:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:45:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 58 @ 11289 updates, score 9.388) (writing took 4.3162610568106174 seconds)
2022-03-04 20:45:29 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 20:45:29 | INFO | train | epoch 058 | loss 3 | ppl 8 | wps 19027.6 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 11289 | lr 0.000297627 | gnorm 0.943 | loss_scale 32 | train_wall 611 | gb_free 7.2 | wall 41419
2022-03-04 20:45:29 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 20:45:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:46:06 | INFO | train_inner | epoch 059:     11 / 196 loss=3.028, ppl=8.16, wps=18609.6, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.956, loss_scale=32, train_wall=315, gb_free=7.2, wall=41456
2022-03-04 20:49:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:51:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:51:52 | INFO | train_inner | epoch 059:    113 / 196 loss=2.937, ppl=7.66, wps=18944.4, ups=0.29, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.957, loss_scale=16, train_wall=320, gb_free=7.2, wall=41802
2022-03-04 20:56:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:56:40 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.39 | ppl 670.97 | wps 35785.5 | wpb 510.9 | bsz 1 | num_updates 11483 | best_loss 6.934
2022-03-04 20:56:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11483 updates
2022-03-04 20:56:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:56:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:56:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 59 @ 11483 updates, score 9.39) (writing took 4.248393444344401 seconds)
2022-03-04 20:56:45 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 20:56:45 | INFO | train | epoch 059 | loss 2.972 | ppl 7.84 | wps 18783.3 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 11483 | lr 0.000295102 | gnorm 0.961 | loss_scale 16 | train_wall 616 | gb_free 7.2 | wall 42095
2022-03-04 20:56:45 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 20:56:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:57:43 | INFO | train_inner | epoch 060:     17 / 196 loss=3, ppl=8, wps=18623.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.97, loss_scale=16, train_wall=315, gb_free=7.2, wall=42153
2022-03-04 20:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:03:31 | INFO | train_inner | epoch 060:    118 / 196 loss=2.916, ppl=7.55, wps=18817.5, ups=0.29, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.939, loss_scale=16, train_wall=322, gb_free=7.2, wall=42501
2022-03-04 21:08:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:08:08 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.497 | ppl 722.63 | wps 34585.4 | wpb 510.9 | bsz 1 | num_updates 11678 | best_loss 6.934
2022-03-04 21:08:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11678 updates
2022-03-04 21:08:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:08:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:08:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 60 @ 11678 updates, score 9.497) (writing took 5.959338586777449 seconds)
2022-03-04 21:08:14 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 21:08:14 | INFO | train | epoch 060 | loss 2.943 | ppl 7.69 | wps 18525.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 11678 | lr 0.000292628 | gnorm 0.957 | loss_scale 32 | train_wall 626 | gb_free 7.2 | wall 42783
2022-03-04 21:08:14 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 21:08:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:09:30 | INFO | train_inner | epoch 061:     22 / 196 loss=2.957, ppl=7.77, wps=18236.3, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.975, loss_scale=32, train_wall=320, gb_free=7.2, wall=42860
2022-03-04 21:10:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:15:21 | INFO | train_inner | epoch 061:    123 / 196 loss=2.894, ppl=7.43, wps=18629.2, ups=0.28, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.966, loss_scale=16, train_wall=325, gb_free=7.2, wall=43211
2022-03-04 21:19:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:19:41 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.52 | ppl 734.39 | wps 34684.4 | wpb 510.9 | bsz 1 | num_updates 11873 | best_loss 6.934
2022-03-04 21:19:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11873 updates
2022-03-04 21:19:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:19:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 61 @ 11873 updates, score 9.52) (writing took 5.277298469096422 seconds)
2022-03-04 21:19:46 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 21:19:46 | INFO | train | epoch 061 | loss 2.916 | ppl 7.55 | wps 18417.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 11873 | lr 0.000290215 | gnorm 0.967 | loss_scale 32 | train_wall 630 | gb_free 7.2 | wall 43476
2022-03-04 21:19:47 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 21:19:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:19:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:21:24 | INFO | train_inner | epoch 062:     28 / 196 loss=2.927, ppl=7.61, wps=18025.5, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.969, loss_scale=16, train_wall=324, gb_free=7.2, wall=43574
2022-03-04 21:27:12 | INFO | train_inner | epoch 062:    128 / 196 loss=2.869, ppl=7.3, wps=18818.1, ups=0.29, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.967, loss_scale=16, train_wall=322, gb_free=7.2, wall=43922
2022-03-04 21:28:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:31:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:31:14 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.624 | ppl 789.19 | wps 35010.7 | wpb 510.9 | bsz 1 | num_updates 12067 | best_loss 6.934
2022-03-04 21:31:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12067 updates
2022-03-04 21:31:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:31:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 62 @ 12067 updates, score 9.624) (writing took 4.822456104680896 seconds)
2022-03-04 21:31:19 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 21:31:19 | INFO | train | epoch 062 | loss 2.888 | ppl 7.4 | wps 18337.8 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 12067 | lr 0.000287873 | gnorm 0.976 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 44169
2022-03-04 21:31:19 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 21:31:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:33:14 | INFO | train_inner | epoch 063:     33 / 196 loss=2.897, ppl=7.45, wps=18083.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.99, loss_scale=16, train_wall=324, gb_free=7.2, wall=44284
2022-03-04 21:38:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:39:06 | INFO | train_inner | epoch 063:    134 / 196 loss=2.846, ppl=7.19, wps=18599.1, ups=0.28, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.963, loss_scale=16, train_wall=326, gb_free=7.2, wall=44636
2022-03-04 21:42:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:42:48 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.727 | ppl 847.75 | wps 35051.3 | wpb 510.9 | bsz 1 | num_updates 12262 | best_loss 6.934
2022-03-04 21:42:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12262 updates
2022-03-04 21:42:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:42:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 63 @ 12262 updates, score 9.727) (writing took 5.121246198192239 seconds)
2022-03-04 21:42:53 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 21:42:53 | INFO | train | epoch 063 | loss 2.862 | ppl 7.27 | wps 18387.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12262 | lr 0.000285574 | gnorm 0.969 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 44863
2022-03-04 21:42:53 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 21:42:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:45:05 | INFO | train_inner | epoch 064:     38 / 196 loss=2.866, ppl=7.29, wps=18198.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.951, loss_scale=16, train_wall=322, gb_free=7.2, wall=44995
2022-03-04 21:50:54 | INFO | train_inner | epoch 064:    138 / 196 loss=2.829, ppl=7.1, wps=18804.2, ups=0.29, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.966, loss_scale=32, train_wall=322, gb_free=7.2, wall=45344
2022-03-04 21:53:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:54:21 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.695 | ppl 828.64 | wps 34851.1 | wpb 510.9 | bsz 1 | num_updates 12457 | best_loss 6.934
2022-03-04 21:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12457 updates
2022-03-04 21:54:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:54:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:54:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 64 @ 12457 updates, score 9.695) (writing took 5.256511267274618 seconds)
2022-03-04 21:54:27 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 21:54:27 | INFO | train | epoch 064 | loss 2.839 | ppl 7.15 | wps 18394.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12457 | lr 0.00028333 | gnorm 0.966 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 45557
2022-03-04 21:54:27 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 21:54:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:56:57 | INFO | train_inner | epoch 065:     43 / 196 loss=2.831, ppl=7.11, wps=18005.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.993, loss_scale=16, train_wall=325, gb_free=7.2, wall=45707
2022-03-04 22:02:50 | INFO | train_inner | epoch 065:    143 / 196 loss=2.81, ppl=7.01, wps=18584.5, ups=0.28, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.974, loss_scale=32, train_wall=326, gb_free=7.2, wall=46059
2022-03-04 22:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:06:01 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.769 | ppl 872.76 | wps 34032.2 | wpb 510.9 | bsz 1 | num_updates 12653 | best_loss 6.934
2022-03-04 22:06:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12653 updates
2022-03-04 22:06:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:06:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 65 @ 12653 updates, score 9.769) (writing took 4.655350126326084 seconds)
2022-03-04 22:06:06 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 22:06:06 | INFO | train | epoch 065 | loss 2.813 | ppl 7.03 | wps 18347.9 | ups 0.28 | wpb 65448 | bsz 127.8 | num_updates 12653 | lr 0.000281127 | gnorm 0.988 | loss_scale 32 | train_wall 636 | gb_free 7.2 | wall 46256
2022-03-04 22:06:06 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 22:06:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:08:51 | INFO | train_inner | epoch 066:     47 / 196 loss=2.807, ppl=7, wps=18091.9, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.999, loss_scale=64, train_wall=324, gb_free=7.2, wall=46421
2022-03-04 22:08:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 22:11:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:14:48 | INFO | train_inner | epoch 066:    149 / 196 loss=2.791, ppl=6.92, wps=18323.4, ups=0.28, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.961, loss_scale=16, train_wall=331, gb_free=7.2, wall=46778
2022-03-04 22:17:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:17:40 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.83 | ppl 910.37 | wps 33768 | wpb 510.9 | bsz 1 | num_updates 12847 | best_loss 6.934
2022-03-04 22:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12847 updates
2022-03-04 22:17:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:17:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:17:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 66 @ 12847 updates, score 9.83) (writing took 4.627007853239775 seconds)
2022-03-04 22:17:44 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 22:17:44 | INFO | train | epoch 066 | loss 2.788 | ppl 6.91 | wps 18178.9 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 12847 | lr 0.000278997 | gnorm 0.974 | loss_scale 16 | train_wall 636 | gb_free 7.2 | wall 46954
2022-03-04 22:17:44 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 22:17:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:20:51 | INFO | train_inner | epoch 067:     53 / 196 loss=2.769, ppl=6.82, wps=18019.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.972, loss_scale=32, train_wall=325, gb_free=7.2, wall=47141
2022-03-04 22:26:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 22:26:47 | INFO | train_inner | epoch 067:    154 / 196 loss=2.775, ppl=6.84, wps=18427.2, ups=0.28, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.981, loss_scale=32, train_wall=329, gb_free=7.2, wall=47497
2022-03-04 22:27:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:29:17 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.896 | ppl 952.94 | wps 35850.6 | wpb 510.9 | bsz 1 | num_updates 13041 | best_loss 6.934
2022-03-04 22:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13041 updates
2022-03-04 22:29:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:29:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:29:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 67 @ 13041 updates, score 9.896) (writing took 5.429148927330971 seconds)
2022-03-04 22:29:23 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 22:29:23 | INFO | train | epoch 067 | loss 2.765 | ppl 6.8 | wps 18173.9 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 13041 | lr 0.000276914 | gnorm 0.978 | loss_scale 16 | train_wall 635 | gb_free 7.2 | wall 47653
2022-03-04 22:29:23 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 22:29:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:32:49 | INFO | train_inner | epoch 068:     59 / 196 loss=2.74, ppl=6.68, wps=18064.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.987, loss_scale=16, train_wall=324, gb_free=7.2, wall=47859
2022-03-04 22:36:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:38:40 | INFO | train_inner | epoch 068:    160 / 196 loss=2.757, ppl=6.76, wps=18642, ups=0.28, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.997, loss_scale=16, train_wall=325, gb_free=7.2, wall=48210
2022-03-04 22:40:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:40:50 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.918 | ppl 967.43 | wps 35337.4 | wpb 510.9 | bsz 1 | num_updates 13236 | best_loss 6.934
2022-03-04 22:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13236 updates
2022-03-04 22:40:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:40:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:40:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 68 @ 13236 updates, score 9.918) (writing took 5.254249582067132 seconds)
2022-03-04 22:40:56 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 22:40:56 | INFO | train | epoch 068 | loss 2.743 | ppl 6.69 | wps 18423.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13236 | lr 0.000274866 | gnorm 0.988 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 48346
2022-03-04 22:40:56 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 22:40:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:44:37 | INFO | train_inner | epoch 069:     64 / 196 loss=2.713, ppl=6.56, wps=18304.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=0.971, loss_scale=32, train_wall=320, gb_free=7.2, wall=48567
2022-03-04 22:50:24 | INFO | train_inner | epoch 069:    164 / 196 loss=2.739, ppl=6.68, wps=18898.3, ups=0.29, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.976, loss_scale=32, train_wall=321, gb_free=7.2, wall=48914
2022-03-04 22:51:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 22:52:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:52:21 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.94 | ppl 982.44 | wps 34828.8 | wpb 510.9 | bsz 1 | num_updates 13431 | best_loss 6.934
2022-03-04 22:52:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13431 updates
2022-03-04 22:52:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:52:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:52:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 69 @ 13431 updates, score 9.94) (writing took 4.292543463408947 seconds)
2022-03-04 22:52:25 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 22:52:25 | INFO | train | epoch 069 | loss 2.72 | ppl 6.59 | wps 18511.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13431 | lr 0.000272864 | gnorm 0.977 | loss_scale 32 | train_wall 628 | gb_free 7.2 | wall 49035
2022-03-04 22:52:25 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 22:52:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:55:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:56:29 | INFO | train_inner | epoch 070:     70 / 196 loss=2.688, ppl=6.44, wps=17901.8, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=1.005, loss_scale=16, train_wall=328, gb_free=7.2, wall=49279
2022-03-04 23:02:18 | INFO | train_inner | epoch 070:    170 / 196 loss=2.721, ppl=6.59, wps=18796, ups=0.29, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.98, loss_scale=16, train_wall=322, gb_free=7.2, wall=49628
2022-03-04 23:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:03:54 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.981 | ppl 1010.71 | wps 34510.2 | wpb 510.9 | bsz 1 | num_updates 13626 | best_loss 6.934
2022-03-04 23:03:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13626 updates
2022-03-04 23:03:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:04:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:04:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 70 @ 13626 updates, score 9.981) (writing took 5.692607643082738 seconds)
2022-03-04 23:04:00 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 23:04:00 | INFO | train | epoch 070 | loss 2.699 | ppl 6.49 | wps 18371.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13626 | lr 0.000270904 | gnorm 0.994 | loss_scale 32 | train_wall 631 | gb_free 7.2 | wall 49730
2022-03-04 23:04:00 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 23:04:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:06:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:08:21 | INFO | train_inner | epoch 071:     75 / 196 loss=2.66, ppl=6.32, wps=18013.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=1.001, loss_scale=16, train_wall=325, gb_free=7.2, wall=49991
2022-03-04 23:14:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:14:13 | INFO | train_inner | epoch 071:    176 / 196 loss=2.699, ppl=6.5, wps=18582.7, ups=0.28, wpb=65532.4, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.996, loss_scale=16, train_wall=326, gb_free=7.2, wall=50343
2022-03-04 23:15:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:15:29 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.021 | ppl 1039.31 | wps 34755.7 | wpb 510.9 | bsz 1 | num_updates 13820 | best_loss 6.934
2022-03-04 23:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13820 updates
2022-03-04 23:15:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:15:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 71 @ 13820 updates, score 10.021) (writing took 5.551142720505595 seconds)
2022-03-04 23:15:34 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 23:15:34 | INFO | train | epoch 071 | loss 2.676 | ppl 6.39 | wps 18282.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 13820 | lr 0.000268996 | gnorm 0.997 | loss_scale 16 | train_wall 632 | gb_free 7.2 | wall 50424
2022-03-04 23:15:34 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 23:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:20:13 | INFO | train_inner | epoch 072:     80 / 196 loss=2.631, ppl=6.19, wps=18161.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.994, loss_scale=16, train_wall=322, gb_free=7.2, wall=50703
2022-03-04 23:25:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:26:06 | INFO | train_inner | epoch 072:    181 / 196 loss=2.687, ppl=6.44, wps=18592.6, ups=0.28, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.992, loss_scale=16, train_wall=326, gb_free=7.2, wall=51056
2022-03-04 23:26:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:27:04 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.097 | ppl 1095.59 | wps 34797.9 | wpb 510.9 | bsz 1 | num_updates 14015 | best_loss 6.934
2022-03-04 23:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14015 updates
2022-03-04 23:27:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:27:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 72 @ 14015 updates, score 10.097) (writing took 5.5179867800325155 seconds)
2022-03-04 23:27:09 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 23:27:09 | INFO | train | epoch 072 | loss 2.656 | ppl 6.3 | wps 18365.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14015 | lr 0.000267118 | gnorm 0.993 | loss_scale 16 | train_wall 632 | gb_free 7.2 | wall 51119
2022-03-04 23:27:09 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 23:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:32:05 | INFO | train_inner | epoch 073:     85 / 196 loss=2.61, ppl=6.1, wps=18181.1, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=1.003, loss_scale=16, train_wall=321, gb_free=7.2, wall=51415
2022-03-04 23:36:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:37:57 | INFO | train_inner | epoch 073:    186 / 196 loss=2.671, ppl=6.37, wps=18627.4, ups=0.28, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.99, loss_scale=16, train_wall=326, gb_free=7.2, wall=51767
2022-03-04 23:38:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:38:37 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.149 | ppl 1135.73 | wps 34142.6 | wpb 510.9 | bsz 1 | num_updates 14210 | best_loss 6.934
2022-03-04 23:38:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14210 updates
2022-03-04 23:38:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:38:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:38:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 73 @ 14210 updates, score 10.149) (writing took 5.2180251479148865 seconds)
2022-03-04 23:38:43 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 23:38:43 | INFO | train | epoch 073 | loss 2.636 | ppl 6.22 | wps 18404.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14210 | lr 0.000265279 | gnorm 0.996 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 51812
2022-03-04 23:38:43 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 23:38:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:43:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:43:57 | INFO | train_inner | epoch 074:     91 / 196 loss=2.581, ppl=5.98, wps=18142.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.99, loss_scale=16, train_wall=323, gb_free=7.2, wall=52127
2022-03-04 23:49:43 | INFO | train_inner | epoch 074:    191 / 196 loss=2.661, ppl=6.32, wps=18971.6, ups=0.29, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=1.015, loss_scale=16, train_wall=320, gb_free=7.2, wall=52473
2022-03-04 23:49:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:50:06 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.198 | ppl 1174.63 | wps 34739.4 | wpb 510.9 | bsz 1 | num_updates 14405 | best_loss 6.934
2022-03-04 23:50:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14405 updates
2022-03-04 23:50:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:50:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 74 @ 14405 updates, score 10.198) (writing took 4.590149566531181 seconds)
2022-03-04 23:50:10 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 23:50:10 | INFO | train | epoch 074 | loss 2.617 | ppl 6.13 | wps 18555 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14405 | lr 0.000263477 | gnorm 1.004 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 52500
2022-03-04 23:50:10 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 23:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:54:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:55:45 | INFO | train_inner | epoch 075:     96 / 196 loss=2.556, ppl=5.88, wps=18053.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=1.002, loss_scale=16, train_wall=325, gb_free=7.2, wall=52835
2022-03-05 00:01:33 | INFO | train_inner | epoch 075:    196 / 196 loss=2.642, ppl=6.24, wps=18803.8, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=1, loss_scale=32, train_wall=322, gb_free=7.2, wall=53182
2022-03-05 00:01:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:01:39 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.182 | ppl 1162.02 | wps 34891.5 | wpb 510.9 | bsz 1 | num_updates 14600 | best_loss 6.934
2022-03-05 00:01:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14600 updates
2022-03-05 00:01:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:01:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 75 @ 14600 updates, score 10.182) (writing took 5.573175776749849 seconds)
2022-03-05 00:01:44 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-05 00:01:44 | INFO | train | epoch 075 | loss 2.597 | ppl 6.05 | wps 18386.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14600 | lr 0.000261712 | gnorm 1 | loss_scale 32 | train_wall 631 | gb_free 7.2 | wall 53194
2022-03-05 00:01:45 | INFO | fairseq.trainer | begin training epoch 76
2022-03-05 00:01:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:05:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:07:36 | INFO | train_inner | epoch 076:    101 / 196 loss=2.542, ppl=5.82, wps=18020.5, ups=0.27, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=1.011, loss_scale=16, train_wall=325, gb_free=7.2, wall=53546
2022-03-05 00:13:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:13:12 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.252 | ppl 1219.73 | wps 34303.1 | wpb 510.9 | bsz 1 | num_updates 14795 | best_loss 6.934
2022-03-05 00:13:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14795 updates
2022-03-05 00:13:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:13:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:13:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 76 @ 14795 updates, score 10.252) (writing took 4.526241682469845 seconds)
2022-03-05 00:13:17 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-05 00:13:17 | INFO | train | epoch 076 | loss 2.579 | ppl 5.97 | wps 18428.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14795 | lr 0.000259982 | gnorm 1.016 | loss_scale 32 | train_wall 630 | gb_free 7.2 | wall 53887
2022-03-05 00:13:17 | INFO | fairseq.trainer | begin training epoch 77
2022-03-05 00:13:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:13:34 | INFO | train_inner | epoch 077:      5 / 196 loss=2.608, ppl=6.1, wps=18245.3, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.023, loss_scale=32, train_wall=321, gb_free=7.2, wall=53904
2022-03-05 00:19:23 | INFO | train_inner | epoch 077:    105 / 196 loss=2.525, ppl=5.75, wps=18799.2, ups=0.29, wpb=65532.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=1.002, loss_scale=32, train_wall=322, gb_free=7.2, wall=54253
2022-03-05 00:20:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-05 00:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:24:44 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.29 | ppl 1251.89 | wps 36057.5 | wpb 510.9 | bsz 1 | num_updates 14990 | best_loss 6.934
2022-03-05 00:24:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14990 updates
2022-03-05 00:24:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 77 @ 14990 updates, score 10.29) (writing took 5.623881693929434 seconds)
2022-03-05 00:24:50 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-05 00:24:50 | INFO | train | epoch 077 | loss 2.56 | ppl 5.9 | wps 18425 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14990 | lr 0.000258285 | gnorm 1.014 | loss_scale 32 | train_wall 630 | gb_free 7.2 | wall 54580
2022-03-05 00:24:50 | INFO | fairseq.trainer | begin training epoch 78
2022-03-05 00:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:25:24 | INFO | train_inner | epoch 078:     10 / 196 loss=2.592, ppl=6.03, wps=18093.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.025, loss_scale=32, train_wall=323, gb_free=7.2, wall=54614
2022-03-05 00:27:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:31:14 | INFO | train_inner | epoch 078:    111 / 196 loss=2.503, ppl=5.67, wps=18732.2, ups=0.29, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.988, loss_scale=16, train_wall=324, gb_free=7.2, wall=54964
2022-03-05 00:36:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:36:15 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.313 | ppl 1272.18 | wps 34723.5 | wpb 510.9 | bsz 1 | num_updates 15185 | best_loss 6.934
2022-03-05 00:36:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15185 updates
2022-03-05 00:36:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:36:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:36:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 78 @ 15185 updates, score 10.313) (writing took 4.595256259664893 seconds)
2022-03-05 00:36:20 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-05 00:36:20 | INFO | train | epoch 078 | loss 2.542 | ppl 5.82 | wps 18489.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15185 | lr 0.000256621 | gnorm 1.006 | loss_scale 32 | train_wall 628 | gb_free 7.2 | wall 55270
2022-03-05 00:36:20 | INFO | fairseq.trainer | begin training epoch 79
2022-03-05 00:36:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:37:12 | INFO | train_inner | epoch 079:     15 / 196 loss=2.573, ppl=5.95, wps=18258.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.022, loss_scale=32, train_wall=321, gb_free=7.2, wall=55322
2022-03-05 00:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:43:04 | INFO | train_inner | epoch 079:    116 / 196 loss=2.493, ppl=5.63, wps=18635.9, ups=0.28, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.008, loss_scale=16, train_wall=325, gb_free=7.2, wall=55674
2022-03-05 00:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:47:48 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.41 | ppl 1360.83 | wps 34835.6 | wpb 510.9 | bsz 1 | num_updates 15380 | best_loss 6.934
2022-03-05 00:47:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15380 updates
2022-03-05 00:47:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:47:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:47:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 79 @ 15380 updates, score 10.41) (writing took 4.927655121311545 seconds)
2022-03-05 00:47:53 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-05 00:47:53 | INFO | train | epoch 079 | loss 2.524 | ppl 5.75 | wps 18410.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15380 | lr 0.000254989 | gnorm 1.013 | loss_scale 32 | train_wall 631 | gb_free 7.2 | wall 55963
2022-03-05 00:47:53 | INFO | fairseq.trainer | begin training epoch 80
2022-03-05 00:47:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:49:03 | INFO | train_inner | epoch 080:     20 / 196 loss=2.548, ppl=5.85, wps=18210.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.018, loss_scale=32, train_wall=322, gb_free=7.2, wall=56033
2022-03-05 00:52:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-05 00:54:54 | INFO | train_inner | epoch 080:    121 / 196 loss=2.478, ppl=5.57, wps=18639.6, ups=0.28, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=1.006, loss_scale=32, train_wall=325, gb_free=7.2, wall=56384
2022-03-05 00:58:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:59:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:59:21 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.401 | ppl 1352.49 | wps 34614.4 | wpb 510.9 | bsz 1 | num_updates 15574 | best_loss 6.934
2022-03-05 00:59:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15574 updates
2022-03-05 00:59:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:59:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:59:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 80 @ 15574 updates, score 10.401) (writing took 5.553703375160694 seconds)
2022-03-05 00:59:27 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-05 00:59:27 | INFO | train | epoch 080 | loss 2.506 | ppl 5.68 | wps 18303.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 15574 | lr 0.000253396 | gnorm 1.017 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 56657
2022-03-05 00:59:27 | INFO | fairseq.trainer | begin training epoch 81
2022-03-05 00:59:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:00:57 | INFO | train_inner | epoch 081:     26 / 196 loss=2.532, ppl=5.78, wps=18013.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.028, loss_scale=16, train_wall=325, gb_free=7.2, wall=56747
2022-03-05 01:06:46 | INFO | train_inner | epoch 081:    126 / 196 loss=2.471, ppl=5.54, wps=18782.5, ups=0.29, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=1.025, loss_scale=32, train_wall=323, gb_free=7.2, wall=57096
2022-03-05 01:07:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:10:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:10:55 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.499 | ppl 1446.83 | wps 34945.5 | wpb 510.9 | bsz 1 | num_updates 15769 | best_loss 6.934
2022-03-05 01:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15769 updates
2022-03-05 01:10:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 81 @ 15769 updates, score 10.499) (writing took 5.450560750439763 seconds)
2022-03-05 01:11:01 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-05 01:11:01 | INFO | train | epoch 081 | loss 2.489 | ppl 5.61 | wps 18390.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15769 | lr 0.000251824 | gnorm 1.025 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 57351
2022-03-05 01:11:01 | INFO | fairseq.trainer | begin training epoch 82
2022-03-05 01:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:12:49 | INFO | train_inner | epoch 082:     31 / 196 loss=2.496, ppl=5.64, wps=18024.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.028, loss_scale=16, train_wall=324, gb_free=7.2, wall=57459
2022-03-05 01:17:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:18:38 | INFO | train_inner | epoch 082:    132 / 196 loss=2.459, ppl=5.5, wps=18765.7, ups=0.29, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.005, loss_scale=16, train_wall=323, gb_free=7.2, wall=57808
2022-03-05 01:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:22:25 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.482 | ppl 1430.65 | wps 35251.4 | wpb 510.9 | bsz 1 | num_updates 15964 | best_loss 6.934
2022-03-05 01:22:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15964 updates
2022-03-05 01:22:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:22:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:22:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 82 @ 15964 updates, score 10.482) (writing took 5.527992827817798 seconds)
2022-03-05 01:22:30 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-05 01:22:30 | INFO | train | epoch 082 | loss 2.473 | ppl 5.55 | wps 18514.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15964 | lr 0.000250282 | gnorm 1.015 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 58040
2022-03-05 01:22:30 | INFO | fairseq.trainer | begin training epoch 83
2022-03-05 01:22:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:24:35 | INFO | train_inner | epoch 083:     36 / 196 loss=2.471, ppl=5.54, wps=18334.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.016, loss_scale=16, train_wall=319, gb_free=7.2, wall=58164
2022-03-05 01:29:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:30:27 | INFO | train_inner | epoch 083:    137 / 196 loss=2.454, ppl=5.48, wps=18596.6, ups=0.28, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.026, loss_scale=16, train_wall=326, gb_free=7.2, wall=58517
2022-03-05 01:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:33:58 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.633 | ppl 1588.22 | wps 34500.6 | wpb 510.9 | bsz 1 | num_updates 16159 | best_loss 6.934
2022-03-05 01:33:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16159 updates
2022-03-05 01:33:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:34:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 83 @ 16159 updates, score 10.633) (writing took 4.7049292009323835 seconds)
2022-03-05 01:34:03 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-05 01:34:03 | INFO | train | epoch 083 | loss 2.456 | ppl 5.49 | wps 18423.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16159 | lr 0.000248767 | gnorm 1.021 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 58733
2022-03-05 01:34:03 | INFO | fairseq.trainer | begin training epoch 84
2022-03-05 01:34:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:36:26 | INFO | train_inner | epoch 084:     41 / 196 loss=2.446, ppl=5.45, wps=18215.5, ups=0.28, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.021, loss_scale=16, train_wall=322, gb_free=7.2, wall=58876
2022-03-05 01:42:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:42:18 | INFO | train_inner | epoch 084:    142 / 196 loss=2.445, ppl=5.45, wps=18617.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.016, loss_scale=16, train_wall=326, gb_free=7.2, wall=59228
2022-03-05 01:45:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:45:32 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.573 | ppl 1523.5 | wps 34652.9 | wpb 510.9 | bsz 1 | num_updates 16354 | best_loss 6.934
2022-03-05 01:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16354 updates
2022-03-05 01:45:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:45:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:45:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 84 @ 16354 updates, score 10.573) (writing took 5.192177737131715 seconds)
2022-03-05 01:45:37 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-05 01:45:37 | INFO | train | epoch 084 | loss 2.44 | ppl 5.43 | wps 18387.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16354 | lr 0.000247279 | gnorm 1.022 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 59427
2022-03-05 01:45:37 | INFO | fairseq.trainer | begin training epoch 85
2022-03-05 01:45:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:48:17 | INFO | train_inner | epoch 085:     46 / 196 loss=2.435, ppl=5.41, wps=18189.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.027, loss_scale=16, train_wall=322, gb_free=7.2, wall=59587
2022-03-05 01:50:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:54:09 | INFO | train_inner | epoch 085:    147 / 196 loss=2.428, ppl=5.38, wps=18612.4, ups=0.28, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.03, loss_scale=16, train_wall=326, gb_free=7.2, wall=59939
2022-03-05 01:56:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:57:05 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.667 | ppl 1626.02 | wps 34355.2 | wpb 510.9 | bsz 1 | num_updates 16549 | best_loss 6.934
2022-03-05 01:57:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16549 updates
2022-03-05 01:57:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:57:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:57:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 85 @ 16549 updates, score 10.667) (writing took 5.247227404266596 seconds)
2022-03-05 01:57:11 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-05 01:57:11 | INFO | train | epoch 085 | loss 2.425 | ppl 5.37 | wps 18392.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16549 | lr 0.000245818 | gnorm 1.022 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 60121
2022-03-05 01:57:11 | INFO | fairseq.trainer | begin training epoch 86
2022-03-05 01:57:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:59:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:00:12 | INFO | train_inner | epoch 086:     52 / 196 loss=2.408, ppl=5.31, wps=18032.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.025, loss_scale=16, train_wall=324, gb_free=7.2, wall=60302
2022-03-05 02:06:00 | INFO | train_inner | epoch 086:    152 / 196 loss=2.416, ppl=5.34, wps=18808.6, ups=0.29, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.031, loss_scale=16, train_wall=322, gb_free=7.2, wall=60650
2022-03-05 02:08:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:08:38 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.653 | ppl 1610.13 | wps 34917.7 | wpb 510.9 | bsz 1 | num_updates 16744 | best_loss 6.934
2022-03-05 02:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16744 updates
2022-03-05 02:08:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:08:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:08:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 86 @ 16744 updates, score 10.653) (writing took 5.201276833191514 seconds)
2022-03-05 02:08:43 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-05 02:08:43 | INFO | train | epoch 086 | loss 2.409 | ppl 5.31 | wps 18427.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16744 | lr 0.000244383 | gnorm 1.033 | loss_scale 32 | train_wall 630 | gb_free 7.2 | wall 60813
2022-03-05 02:08:43 | INFO | fairseq.trainer | begin training epoch 87
2022-03-05 02:08:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:10:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:12:01 | INFO | train_inner | epoch 087:     57 / 196 loss=2.396, ppl=5.26, wps=18136.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.033, loss_scale=16, train_wall=323, gb_free=7.2, wall=61010
2022-03-05 02:17:47 | INFO | train_inner | epoch 087:    157 / 196 loss=2.4, ppl=5.28, wps=18934.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.032, loss_scale=16, train_wall=320, gb_free=7.2, wall=61357
2022-03-05 02:18:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:20:08 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.718 | ppl 1684.89 | wps 34735.6 | wpb 510.9 | bsz 1 | num_updates 16938 | best_loss 6.934
2022-03-05 02:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16938 updates
2022-03-05 02:20:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:20:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 87 @ 16938 updates, score 10.718) (writing took 4.490661343559623 seconds)
2022-03-05 02:20:12 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-05 02:20:12 | INFO | train | epoch 087 | loss 2.394 | ppl 5.26 | wps 18423.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 16938 | lr 0.000242979 | gnorm 1.039 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 61502
2022-03-05 02:20:13 | INFO | fairseq.trainer | begin training epoch 88
2022-03-05 02:20:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:23:49 | INFO | train_inner | epoch 088:     62 / 196 loss=2.376, ppl=5.19, wps=18051.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.048, loss_scale=16, train_wall=325, gb_free=7.2, wall=61719
2022-03-05 02:29:37 | INFO | train_inner | epoch 088:    162 / 196 loss=2.392, ppl=5.25, wps=18813.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.036, loss_scale=32, train_wall=322, gb_free=7.2, wall=62067
2022-03-05 02:30:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:31:41 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.781 | ppl 1759.27 | wps 34561.8 | wpb 510.9 | bsz 1 | num_updates 17133 | best_loss 6.934
2022-03-05 02:31:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17133 updates
2022-03-05 02:31:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:31:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 88 @ 17133 updates, score 10.781) (writing took 5.508683376014233 seconds)
2022-03-05 02:31:47 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-05 02:31:47 | INFO | train | epoch 088 | loss 2.38 | ppl 5.2 | wps 18381.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17133 | lr 0.000241592 | gnorm 1.039 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 62197
2022-03-05 02:31:47 | INFO | fairseq.trainer | begin training epoch 89
2022-03-05 02:31:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:35:41 | INFO | train_inner | epoch 089:     67 / 196 loss=2.354, ppl=5.11, wps=17984.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.027, loss_scale=16, train_wall=325, gb_free=7.2, wall=62430
2022-03-05 02:40:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:41:33 | INFO | train_inner | epoch 089:    168 / 196 loss=2.378, ppl=5.2, wps=18577, ups=0.28, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.046, loss_scale=16, train_wall=326, gb_free=7.2, wall=62783
2022-03-05 02:43:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:43:17 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.828 | ppl 1817.27 | wps 34407.3 | wpb 510.9 | bsz 1 | num_updates 17328 | best_loss 6.934
2022-03-05 02:43:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17328 updates
2022-03-05 02:43:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 89 @ 17328 updates, score 10.828) (writing took 5.180335611104965 seconds)
2022-03-05 02:43:22 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-05 02:43:22 | INFO | train | epoch 089 | loss 2.364 | ppl 5.15 | wps 18363.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17328 | lr 0.000240229 | gnorm 1.037 | loss_scale 16 | train_wall 632 | gb_free 7.2 | wall 62892
2022-03-05 02:43:22 | INFO | fairseq.trainer | begin training epoch 90
2022-03-05 02:43:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:47:33 | INFO | train_inner | epoch 090:     72 / 196 loss=2.341, ppl=5.07, wps=18185.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.03, loss_scale=16, train_wall=322, gb_free=7.2, wall=63143
2022-03-05 02:48:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:53:22 | INFO | train_inner | epoch 090:    173 / 196 loss=2.375, ppl=5.19, wps=18764.1, ups=0.29, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.035, loss_scale=16, train_wall=323, gb_free=7.2, wall=63492
2022-03-05 02:54:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:54:47 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.871 | ppl 1873.13 | wps 35993 | wpb 510.9 | bsz 1 | num_updates 17523 | best_loss 6.934
2022-03-05 02:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17523 updates
2022-03-05 02:54:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:54:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:54:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 90 @ 17523 updates, score 10.871) (writing took 5.353844629600644 seconds)
2022-03-05 02:54:52 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-05 02:54:52 | INFO | train | epoch 090 | loss 2.35 | ppl 5.1 | wps 18483 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17523 | lr 0.000238889 | gnorm 1.03 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 63582
2022-03-05 02:54:52 | INFO | fairseq.trainer | begin training epoch 91
2022-03-05 02:54:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:59:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:59:22 | INFO | train_inner | epoch 091:     78 / 196 loss=2.316, ppl=4.98, wps=18163.2, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.029, loss_scale=16, train_wall=322, gb_free=7.2, wall=63852
2022-03-05 03:05:10 | INFO | train_inner | epoch 091:    178 / 196 loss=2.362, ppl=5.14, wps=18832.2, ups=0.29, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.055, loss_scale=16, train_wall=322, gb_free=7.2, wall=64200
2022-03-05 03:06:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:06:18 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.901 | ppl 1912.31 | wps 34714.6 | wpb 510.9 | bsz 1 | num_updates 17718 | best_loss 6.934
2022-03-05 03:06:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17718 updates
2022-03-05 03:06:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:06:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 91 @ 17718 updates, score 10.901) (writing took 5.560947800055146 seconds)
2022-03-05 03:06:24 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-05 03:06:24 | INFO | train | epoch 091 | loss 2.337 | ppl 5.05 | wps 18457.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17718 | lr 0.000237571 | gnorm 1.04 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 64274
2022-03-05 03:06:24 | INFO | fairseq.trainer | begin training epoch 92
2022-03-05 03:06:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:07:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:11:13 | INFO | train_inner | epoch 092:     83 / 196 loss=2.295, ppl=4.91, wps=18014.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.029, loss_scale=16, train_wall=325, gb_free=7.2, wall=64563
2022-03-05 03:15:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:17:04 | INFO | train_inner | epoch 092:    184 / 196 loss=2.354, ppl=5.11, wps=18634.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.056, loss_scale=16, train_wall=325, gb_free=7.2, wall=64914
2022-03-05 03:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:17:52 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.903 | ppl 1914.57 | wps 35101.9 | wpb 510.9 | bsz 1 | num_updates 17912 | best_loss 6.934
2022-03-05 03:17:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17912 updates
2022-03-05 03:17:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:17:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 92 @ 17912 updates, score 10.903) (writing took 5.370739668607712 seconds)
2022-03-05 03:17:57 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-05 03:17:57 | INFO | train | epoch 092 | loss 2.322 | ppl 5 | wps 18311.6 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 17912 | lr 0.000236281 | gnorm 1.047 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 64967
2022-03-05 03:17:57 | INFO | fairseq.trainer | begin training epoch 93
2022-03-05 03:17:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:23:05 | INFO | train_inner | epoch 093:     88 / 196 loss=2.275, ppl=4.84, wps=18135.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.033, loss_scale=32, train_wall=322, gb_free=7.2, wall=65275
2022-03-05 03:25:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:29:01 | INFO | train_inner | epoch 093:    189 / 196 loss=2.346, ppl=5.08, wps=18407.5, ups=0.28, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.068, loss_scale=16, train_wall=329, gb_free=7.2, wall=65631
2022-03-05 03:29:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:29:31 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.896 | ppl 1905.75 | wps 33471.7 | wpb 510.9 | bsz 1 | num_updates 18107 | best_loss 6.934
2022-03-05 03:29:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18107 updates
2022-03-05 03:29:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:29:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:29:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 93 @ 18107 updates, score 10.896) (writing took 5.477048011496663 seconds)
2022-03-05 03:29:37 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-05 03:29:37 | INFO | train | epoch 093 | loss 2.308 | ppl 4.95 | wps 18241.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18107 | lr 0.000235005 | gnorm 1.05 | loss_scale 16 | train_wall 636 | gb_free 7.2 | wall 65667
2022-03-05 03:29:37 | INFO | fairseq.trainer | begin training epoch 94
2022-03-05 03:29:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:35:05 | INFO | train_inner | epoch 094:     93 / 196 loss=2.261, ppl=4.79, wps=17941.8, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.038, loss_scale=32, train_wall=326, gb_free=7.2, wall=65995
2022-03-05 03:38:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:41:02 | INFO | train_inner | epoch 094:    194 / 196 loss=2.335, ppl=5.05, wps=18393.4, ups=0.28, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.051, loss_scale=16, train_wall=329, gb_free=7.2, wall=66351
2022-03-05 03:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:41:14 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.952 | ppl 1981.67 | wps 34938.6 | wpb 510.9 | bsz 1 | num_updates 18302 | best_loss 6.934
2022-03-05 03:41:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18302 updates
2022-03-05 03:41:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:41:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:41:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 94 @ 18302 updates, score 10.952) (writing took 4.82408189214766 seconds)
2022-03-05 03:41:19 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-05 03:41:19 | INFO | train | epoch 094 | loss 2.295 | ppl 4.91 | wps 18176.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18302 | lr 0.00023375 | gnorm 1.042 | loss_scale 16 | train_wall 639 | gb_free 7.2 | wall 66369
2022-03-05 03:41:19 | INFO | fairseq.trainer | begin training epoch 95
2022-03-05 03:41:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:47:05 | INFO | train_inner | epoch 095:     98 / 196 loss=2.252, ppl=4.76, wps=18006.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=1.04, loss_scale=32, train_wall=325, gb_free=7.2, wall=66714
2022-03-05 03:49:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:52:52 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.97 | ppl 2006.06 | wps 35763 | wpb 510.9 | bsz 1 | num_updates 18497 | best_loss 6.934
2022-03-05 03:52:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18497 updates
2022-03-05 03:52:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:52:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 95 @ 18497 updates, score 10.97) (writing took 6.431720305234194 seconds)
2022-03-05 03:52:59 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-05 03:52:59 | INFO | train | epoch 095 | loss 2.283 | ppl 4.87 | wps 18236.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18497 | lr 0.000232514 | gnorm 1.044 | loss_scale 16 | train_wall 636 | gb_free 7.2 | wall 67069
2022-03-05 03:52:59 | INFO | fairseq.trainer | begin training epoch 96
2022-03-05 03:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:53:09 | INFO | train_inner | epoch 096:      3 / 196 loss=2.311, ppl=4.96, wps=17929, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.05, loss_scale=16, train_wall=326, gb_free=7.2, wall=67079
2022-03-05 03:58:55 | INFO | train_inner | epoch 096:    103 / 196 loss=2.239, ppl=4.72, wps=18947.2, ups=0.29, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.056, loss_scale=32, train_wall=320, gb_free=7.2, wall=67425
2022-03-05 04:04:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:04:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:04:22 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.106 | ppl 2203.75 | wps 34798.9 | wpb 510.9 | bsz 1 | num_updates 18692 | best_loss 6.934
2022-03-05 04:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18692 updates
2022-03-05 04:04:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:04:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 96 @ 18692 updates, score 11.106) (writing took 5.1494569424539804 seconds)
2022-03-05 04:04:27 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-05 04:04:27 | INFO | train | epoch 096 | loss 2.269 | ppl 4.82 | wps 18542.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18692 | lr 0.000231298 | gnorm 1.05 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 67757
2022-03-05 04:04:27 | INFO | fairseq.trainer | begin training epoch 97
2022-03-05 04:04:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:04:55 | INFO | train_inner | epoch 097:      8 / 196 loss=2.296, ppl=4.91, wps=18169.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.041, loss_scale=16, train_wall=322, gb_free=7.2, wall=67785
2022-03-05 04:10:44 | INFO | train_inner | epoch 097:    108 / 196 loss=2.228, ppl=4.69, wps=18754.4, ups=0.29, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.052, loss_scale=16, train_wall=323, gb_free=7.2, wall=68134
2022-03-05 04:13:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:15:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:15:57 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.068 | ppl 2147.56 | wps 34475.8 | wpb 510.9 | bsz 1 | num_updates 18887 | best_loss 6.934
2022-03-05 04:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18887 updates
2022-03-05 04:15:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:16:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:16:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 97 @ 18887 updates, score 11.068) (writing took 5.1791913118213415 seconds)
2022-03-05 04:16:02 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-05 04:16:02 | INFO | train | epoch 097 | loss 2.257 | ppl 4.78 | wps 18355.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18887 | lr 0.000230101 | gnorm 1.052 | loss_scale 16 | train_wall 632 | gb_free 7.2 | wall 68452
2022-03-05 04:16:02 | INFO | fairseq.trainer | begin training epoch 98
2022-03-05 04:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:16:48 | INFO | train_inner | epoch 098:     13 / 196 loss=2.279, ppl=4.85, wps=17981.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.049, loss_scale=16, train_wall=325, gb_free=7.2, wall=68498
2022-03-05 04:21:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:22:40 | INFO | train_inner | epoch 098:    114 / 196 loss=2.216, ppl=4.65, wps=18602.6, ups=0.28, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.042, loss_scale=16, train_wall=326, gb_free=7.2, wall=68850
2022-03-05 04:27:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:27:29 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.058 | ppl 2131.69 | wps 35106.3 | wpb 510.9 | bsz 1 | num_updates 19082 | best_loss 6.934
2022-03-05 04:27:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19082 updates
2022-03-05 04:27:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:27:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:27:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 98 @ 19082 updates, score 11.058) (writing took 5.908075775951147 seconds)
2022-03-05 04:27:35 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-05 04:27:35 | INFO | train | epoch 098 | loss 2.246 | ppl 4.74 | wps 18422.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19082 | lr 0.000228922 | gnorm 1.051 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 69145
2022-03-05 04:27:35 | INFO | fairseq.trainer | begin training epoch 99
2022-03-05 04:27:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:28:37 | INFO | train_inner | epoch 099:     18 / 196 loss=2.273, ppl=4.83, wps=18286.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.065, loss_scale=16, train_wall=319, gb_free=7.2, wall=69207
2022-03-05 04:31:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:34:27 | INFO | train_inner | epoch 099:    119 / 196 loss=2.208, ppl=4.62, wps=18744.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.06, loss_scale=16, train_wall=324, gb_free=7.2, wall=69557
2022-03-05 04:38:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:39:01 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 11.163 | ppl 2293.7 | wps 34832.8 | wpb 510.9 | bsz 1 | num_updates 19277 | best_loss 6.934
2022-03-05 04:39:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19277 updates
2022-03-05 04:39:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 99 @ 19277 updates, score 11.163) (writing took 5.180602198466659 seconds)
2022-03-05 04:39:06 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-05 04:39:06 | INFO | train | epoch 099 | loss 2.233 | ppl 4.7 | wps 18474.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19277 | lr 0.000227761 | gnorm 1.059 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 69836
2022-03-05 04:39:06 | INFO | fairseq.trainer | begin training epoch 100
2022-03-05 04:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:40:26 | INFO | train_inner | epoch 100:     23 / 196 loss=2.253, ppl=4.77, wps=18220.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.056, loss_scale=32, train_wall=321, gb_free=7.2, wall=69916
2022-03-05 04:41:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:46:18 | INFO | train_inner | epoch 100:    124 / 196 loss=2.202, ppl=4.6, wps=18623.5, ups=0.28, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.049, loss_scale=16, train_wall=325, gb_free=7.2, wall=70268
2022-03-05 04:49:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:50:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:50:34 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 11.092 | ppl 2183.29 | wps 34442.1 | wpb 510.9 | bsz 1 | num_updates 19471 | best_loss 6.934
2022-03-05 04:50:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19471 updates
2022-03-05 04:50:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 100 @ 19471 updates, score 11.092) (writing took 5.222691301256418 seconds)
2022-03-05 04:50:39 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-05 04:50:39 | INFO | train | epoch 100 | loss 2.22 | ppl 4.66 | wps 18302.2 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 19471 | lr 0.000226624 | gnorm 1.056 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 70529
2022-03-05 04:50:40 | INFO | fairseq.trainer | begin training epoch 101
2022-03-05 04:50:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:52:21 | INFO | train_inner | epoch 101:     29 / 196 loss=2.23, ppl=4.69, wps=18012.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.063, loss_scale=16, train_wall=325, gb_free=7.2, wall=70630
2022-03-05 04:57:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:58:12 | INFO | train_inner | epoch 101:    130 / 196 loss=2.198, ppl=4.59, wps=18623.6, ups=0.28, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.054, loss_scale=16, train_wall=326, gb_free=7.2, wall=70982
2022-03-05 05:02:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:02:08 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 11.174 | ppl 2311.2 | wps 34378.7 | wpb 510.9 | bsz 1 | num_updates 19666 | best_loss 6.934
2022-03-05 05:02:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19666 updates
2022-03-05 05:02:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 101 @ 19666 updates, score 11.174) (writing took 5.319521047174931 seconds)
2022-03-05 05:02:13 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-05 05:02:13 | INFO | train | epoch 101 | loss 2.209 | ppl 4.62 | wps 18403.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19666 | lr 0.000225498 | gnorm 1.057 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 71223
2022-03-05 05:02:13 | INFO | fairseq.trainer | begin training epoch 102
2022-03-05 05:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:04:12 | INFO | train_inner | epoch 102:     34 / 196 loss=2.207, ppl=4.62, wps=18204.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.058, loss_scale=16, train_wall=321, gb_free=7.2, wall=71341
2022-03-05 05:06:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:10:03 | INFO | train_inner | epoch 102:    135 / 196 loss=2.191, ppl=4.57, wps=18633, ups=0.28, wpb=65532.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.051, loss_scale=16, train_wall=325, gb_free=7.2, wall=71693
2022-03-05 05:13:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:13:41 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 11.212 | ppl 2372.18 | wps 35346.1 | wpb 510.9 | bsz 1 | num_updates 19861 | best_loss 6.934
2022-03-05 05:13:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19861 updates
2022-03-05 05:13:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:13:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:13:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 102 @ 19861 updates, score 11.212) (writing took 5.649471836164594 seconds)
2022-03-05 05:13:47 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-05 05:13:47 | INFO | train | epoch 102 | loss 2.197 | ppl 4.59 | wps 18398.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19861 | lr 0.000224388 | gnorm 1.057 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 71916
2022-03-05 05:13:47 | INFO | fairseq.trainer | begin training epoch 103
2022-03-05 05:13:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:14:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:16:06 | INFO | train_inner | epoch 103:     40 / 196 loss=2.206, ppl=4.62, wps=18026.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.065, loss_scale=16, train_wall=324, gb_free=7.2, wall=72056
2022-03-05 05:21:54 | INFO | train_inner | epoch 103:    140 / 196 loss=2.183, ppl=4.54, wps=18820.6, ups=0.29, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.055, loss_scale=32, train_wall=322, gb_free=7.2, wall=72404
2022-03-05 05:22:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:25:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:25:15 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 11.218 | ppl 2382.89 | wps 33672 | wpb 510.9 | bsz 1 | num_updates 20055 | best_loss 6.934
2022-03-05 05:25:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20055 updates
2022-03-05 05:25:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:25:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:25:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 103 @ 20055 updates, score 11.218) (writing took 5.437883500009775 seconds)
2022-03-05 05:25:20 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-05 05:25:20 | INFO | train | epoch 103 | loss 2.186 | ppl 4.55 | wps 18305.4 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 20055 | lr 0.0002233 | gnorm 1.06 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 72610
2022-03-05 05:25:20 | INFO | fairseq.trainer | begin training epoch 104
2022-03-05 05:25:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:27:57 | INFO | train_inner | epoch 104:     45 / 196 loss=2.175, ppl=4.51, wps=17998.6, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.066, loss_scale=16, train_wall=325, gb_free=7.2, wall=72767
2022-03-05 05:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:33:49 | INFO | train_inner | epoch 104:    146 / 196 loss=2.173, ppl=4.51, wps=18624.9, ups=0.28, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.065, loss_scale=16, train_wall=326, gb_free=7.2, wall=73119
2022-03-05 05:36:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:36:49 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 11.246 | ppl 2428.71 | wps 34846.5 | wpb 510.9 | bsz 1 | num_updates 20250 | best_loss 6.934
2022-03-05 05:36:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20250 updates
2022-03-05 05:36:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:36:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:36:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 104 @ 20250 updates, score 11.246) (writing took 5.1035521272569895 seconds)
2022-03-05 05:36:54 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-05 05:36:54 | INFO | train | epoch 104 | loss 2.175 | ppl 4.52 | wps 18396.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20250 | lr 0.000222222 | gnorm 1.071 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 73304
2022-03-05 05:36:54 | INFO | fairseq.trainer | begin training epoch 105
2022-03-05 05:36:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:39:48 | INFO | train_inner | epoch 105:     50 / 196 loss=2.169, ppl=4.5, wps=18233, ups=0.28, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.082, loss_scale=32, train_wall=321, gb_free=7.2, wall=73477
2022-03-05 05:40:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:45:37 | INFO | train_inner | epoch 105:    151 / 196 loss=2.176, ppl=4.52, wps=18760, ups=0.29, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.058, loss_scale=16, train_wall=323, gb_free=7.2, wall=73827
2022-03-05 05:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:48:18 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 11.304 | ppl 2528.23 | wps 35261.6 | wpb 510.9 | bsz 1 | num_updates 20445 | best_loss 6.934
2022-03-05 05:48:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20445 updates
2022-03-05 05:48:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:48:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:48:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 105 @ 20445 updates, score 11.304) (writing took 4.8167383428663015 seconds)
2022-03-05 05:48:23 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-05 05:48:23 | INFO | train | epoch 105 | loss 2.163 | ppl 4.48 | wps 18520.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20445 | lr 0.00022116 | gnorm 1.064 | loss_scale 32 | train_wall 627 | gb_free 7.2 | wall 73993
2022-03-05 05:48:23 | INFO | fairseq.trainer | begin training epoch 106
2022-03-05 05:48:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:48:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:51:37 | INFO | train_inner | epoch 106:     56 / 196 loss=2.144, ppl=4.42, wps=18140.7, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.055, loss_scale=16, train_wall=323, gb_free=7.2, wall=74187
2022-03-05 05:56:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:57:28 | INFO | train_inner | epoch 106:    157 / 196 loss=2.164, ppl=4.48, wps=18662.9, ups=0.28, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.073, loss_scale=16, train_wall=325, gb_free=7.2, wall=74538
2022-03-05 05:59:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:59:49 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 11.41 | ppl 2721.05 | wps 35477.4 | wpb 510.9 | bsz 1 | num_updates 20639 | best_loss 6.934
2022-03-05 05:59:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20639 updates
2022-03-05 05:59:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:59:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:59:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 106 @ 20639 updates, score 11.41) (writing took 5.9500634875148535 seconds)
2022-03-05 05:59:54 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-05 05:59:54 | INFO | train | epoch 106 | loss 2.153 | ppl 4.45 | wps 18363.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 20639 | lr 0.000220118 | gnorm 1.062 | loss_scale 16 | train_wall 629 | gb_free 7.2 | wall 74684
2022-03-05 05:59:55 | INFO | fairseq.trainer | begin training epoch 107
2022-03-05 05:59:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:03:26 | INFO | train_inner | epoch 107:     61 / 196 loss=2.134, ppl=4.39, wps=18274.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.06, loss_scale=16, train_wall=320, gb_free=7.2, wall=74896
2022-03-05 06:06:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:09:16 | INFO | train_inner | epoch 107:    162 / 196 loss=2.156, ppl=4.46, wps=18715.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.068, loss_scale=16, train_wall=324, gb_free=7.2, wall=75246
2022-03-05 06:11:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:11:20 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 11.393 | ppl 2688.57 | wps 34975.5 | wpb 510.9 | bsz 1 | num_updates 20834 | best_loss 6.934
2022-03-05 06:11:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20834 updates
2022-03-05 06:11:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:11:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:11:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 107 @ 20834 updates, score 11.393) (writing took 4.544638408347964 seconds)
2022-03-05 06:11:25 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-05 06:11:25 | INFO | train | epoch 107 | loss 2.142 | ppl 4.41 | wps 18490 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20834 | lr 0.000219086 | gnorm 1.064 | loss_scale 16 | train_wall 629 | gb_free 7.2 | wall 75375
2022-03-05 06:11:25 | INFO | fairseq.trainer | begin training epoch 108
2022-03-05 06:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:15:15 | INFO | train_inner | epoch 108:     66 / 196 loss=2.12, ppl=4.35, wps=18235.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.061, loss_scale=32, train_wall=321, gb_free=7.2, wall=75605
2022-03-05 06:17:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:21:07 | INFO | train_inner | epoch 108:    167 / 196 loss=2.152, ppl=4.44, wps=18626.9, ups=0.28, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.07, loss_scale=16, train_wall=325, gb_free=7.2, wall=75956
2022-03-05 06:22:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:22:53 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 11.458 | ppl 2813.39 | wps 34551.9 | wpb 510.9 | bsz 1 | num_updates 21029 | best_loss 6.934
2022-03-05 06:22:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21029 updates
2022-03-05 06:22:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:22:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:22:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 108 @ 21029 updates, score 11.458) (writing took 5.324115062132478 seconds)
2022-03-05 06:22:58 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-05 06:22:58 | INFO | train | epoch 108 | loss 2.132 | ppl 4.38 | wps 18396.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21029 | lr 0.000218067 | gnorm 1.068 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 76068
2022-03-05 06:22:58 | INFO | fairseq.trainer | begin training epoch 109
2022-03-05 06:22:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:26:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:27:10 | INFO | train_inner | epoch 109:     72 / 196 loss=2.099, ppl=4.28, wps=17995.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.069, loss_scale=16, train_wall=325, gb_free=7.2, wall=76320
2022-03-05 06:32:59 | INFO | train_inner | epoch 109:    172 / 196 loss=2.145, ppl=4.42, wps=18776.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.077, loss_scale=16, train_wall=323, gb_free=7.2, wall=76669
2022-03-05 06:34:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:34:28 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 11.404 | ppl 2709.12 | wps 34275.9 | wpb 510.9 | bsz 1 | num_updates 21224 | best_loss 6.934
2022-03-05 06:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21224 updates
2022-03-05 06:34:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:34:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:34:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 109 @ 21224 updates, score 11.404) (writing took 5.50159857980907 seconds)
2022-03-05 06:34:33 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-05 06:34:33 | INFO | train | epoch 109 | loss 2.121 | ppl 4.35 | wps 18361.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21224 | lr 0.000217063 | gnorm 1.07 | loss_scale 32 | train_wall 632 | gb_free 7.2 | wall 76763
2022-03-05 06:34:34 | INFO | fairseq.trainer | begin training epoch 110
2022-03-05 06:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:34:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:39:02 | INFO | train_inner | epoch 110:     77 / 196 loss=2.094, ppl=4.27, wps=18001.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.069, loss_scale=16, train_wall=325, gb_free=7.2, wall=77032
2022-03-05 06:42:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:44:53 | INFO | train_inner | epoch 110:    178 / 196 loss=2.135, ppl=4.39, wps=18657.5, ups=0.28, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.07, loss_scale=16, train_wall=325, gb_free=7.2, wall=77383
2022-03-05 06:45:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:46:01 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 11.428 | ppl 2755.46 | wps 33888.5 | wpb 510.9 | bsz 1 | num_updates 21418 | best_loss 6.934
2022-03-05 06:46:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21418 updates
2022-03-05 06:46:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:46:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 110 @ 21418 updates, score 11.428) (writing took 5.720998326316476 seconds)
2022-03-05 06:46:07 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-05 06:46:07 | INFO | train | epoch 110 | loss 2.11 | ppl 4.32 | wps 18305.7 | ups 0.28 | wpb 65448.9 | bsz 127.8 | num_updates 21418 | lr 0.000216078 | gnorm 1.069 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 77457
2022-03-05 06:46:07 | INFO | fairseq.trainer | begin training epoch 111
2022-03-05 06:46:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:50:53 | INFO | train_inner | epoch 111:     82 / 196 loss=2.075, ppl=4.21, wps=18186.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.07, loss_scale=32, train_wall=321, gb_free=7.2, wall=77742
2022-03-05 06:51:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:56:44 | INFO | train_inner | epoch 111:    183 / 196 loss=2.134, ppl=4.39, wps=18643.5, ups=0.28, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.069, loss_scale=16, train_wall=325, gb_free=7.2, wall=78094
2022-03-05 06:57:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:35 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 11.506 | ppl 2908.27 | wps 35407.5 | wpb 510.9 | bsz 1 | num_updates 21613 | best_loss 6.934
2022-03-05 06:57:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21613 updates
2022-03-05 06:57:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:57:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:57:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 111 @ 21613 updates, score 11.506) (writing took 5.42346740141511 seconds)
2022-03-05 06:57:40 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-05 06:57:40 | INFO | train | epoch 111 | loss 2.102 | ppl 4.29 | wps 18415.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21613 | lr 0.000215101 | gnorm 1.07 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 78150
2022-03-05 06:57:40 | INFO | fairseq.trainer | begin training epoch 112
2022-03-05 06:57:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:59:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:02:47 | INFO | train_inner | epoch 112:     88 / 196 loss=2.065, ppl=4.18, wps=18019.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.064, loss_scale=16, train_wall=325, gb_free=7.2, wall=78457
2022-03-05 07:08:35 | INFO | train_inner | epoch 112:    188 / 196 loss=2.121, ppl=4.35, wps=18807.1, ups=0.29, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.07, loss_scale=32, train_wall=322, gb_free=7.2, wall=78805
2022-03-05 07:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:09:09 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 11.494 | ppl 2884.27 | wps 34932.3 | wpb 510.9 | bsz 1 | num_updates 21808 | best_loss 6.934
2022-03-05 07:09:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21808 updates
2022-03-05 07:09:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:09:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 112 @ 21808 updates, score 11.494) (writing took 5.285813411697745 seconds)
2022-03-05 07:09:14 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-05 07:09:14 | INFO | train | epoch 112 | loss 2.091 | ppl 4.26 | wps 18398 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21808 | lr 0.000214137 | gnorm 1.066 | loss_scale 32 | train_wall 631 | gb_free 7.2 | wall 78844
2022-03-05 07:09:14 | INFO | fairseq.trainer | begin training epoch 113
2022-03-05 07:09:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:10:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:14:38 | INFO | train_inner | epoch 113:     93 / 196 loss=2.051, ppl=4.14, wps=18041, ups=0.28, wpb=65367, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.075, loss_scale=16, train_wall=324, gb_free=7.2, wall=79167
2022-03-05 07:17:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:20:29 | INFO | train_inner | epoch 113:    194 / 196 loss=2.118, ppl=4.34, wps=18667.5, ups=0.28, wpb=65532.4, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.095, loss_scale=16, train_wall=325, gb_free=7.2, wall=79519
2022-03-05 07:20:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:20:41 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 11.487 | ppl 2870.58 | wps 34801.5 | wpb 510.9 | bsz 1 | num_updates 22002 | best_loss 6.934
2022-03-05 07:20:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22002 updates
2022-03-05 07:20:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:20:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:20:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 113 @ 22002 updates, score 11.487) (writing took 5.373673187568784 seconds)
2022-03-05 07:20:46 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-05 07:20:46 | INFO | train | epoch 113 | loss 2.082 | ppl 4.23 | wps 18331.9 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 22002 | lr 0.000213191 | gnorm 1.087 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 79536
2022-03-05 07:20:46 | INFO | fairseq.trainer | begin training epoch 114
2022-03-05 07:20:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:26:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:26:30 | INFO | train_inner | epoch 114:     99 / 196 loss=2.031, ppl=4.09, wps=18106.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.066, loss_scale=16, train_wall=323, gb_free=7.2, wall=79880
2022-03-05 07:32:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:32:08 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 11.506 | ppl 2908.92 | wps 36489.2 | wpb 510.9 | bsz 1 | num_updates 22197 | best_loss 6.934
2022-03-05 07:32:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22197 updates
2022-03-05 07:32:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:32:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:32:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 114 @ 22197 updates, score 11.506) (writing took 4.646262552589178 seconds)
2022-03-05 07:32:13 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-05 07:32:13 | INFO | train | epoch 114 | loss 2.072 | ppl 4.21 | wps 18594.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 22197 | lr 0.000212253 | gnorm 1.073 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 80223
2022-03-05 07:32:13 | INFO | fairseq.trainer | begin training epoch 115
2022-03-05 07:32:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:32:23 | INFO | train_inner | epoch 115:      3 / 196 loss=2.113, ppl=4.33, wps=18490.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.08, loss_scale=16, train_wall=317, gb_free=7.2, wall=80233
2022-03-05 07:33:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:38:10 | INFO | train_inner | epoch 115:    104 / 196 loss=2.035, ppl=4.1, wps=18884, ups=0.29, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.059, loss_scale=16, train_wall=321, gb_free=7.2, wall=80580
2022-03-05 07:41:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:43:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:43:35 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 11.615 | ppl 3137.62 | wps 35053.6 | wpb 510.9 | bsz 1 | num_updates 22391 | best_loss 6.934
2022-03-05 07:43:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22391 updates
2022-03-05 07:43:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:43:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 115 @ 22391 updates, score 11.615) (writing took 4.384421411901712 seconds)
2022-03-05 07:43:39 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-05 07:43:39 | INFO | train | epoch 115 | loss 2.063 | ppl 4.18 | wps 18495.7 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 22391 | lr 0.000211331 | gnorm 1.073 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 80909
2022-03-05 07:43:39 | INFO | fairseq.trainer | begin training epoch 116
2022-03-05 07:43:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:44:11 | INFO | train_inner | epoch 116:      9 / 196 loss=2.082, ppl=4.23, wps=18130.7, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.087, loss_scale=16, train_wall=324, gb_free=7.2, wall=80941
2022-03-05 07:49:59 | INFO | train_inner | epoch 116:    109 / 196 loss=2.024, ppl=4.07, wps=18828.2, ups=0.29, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.065, loss_scale=32, train_wall=322, gb_free=7.2, wall=81289
2022-03-05 07:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:55:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:55:07 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 11.622 | ppl 3152.5 | wps 34888.9 | wpb 510.9 | bsz 1 | num_updates 22586 | best_loss 6.934
2022-03-05 07:55:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22586 updates
2022-03-05 07:55:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:55:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:55:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 116 @ 22586 updates, score 11.622) (writing took 5.179889898747206 seconds)
2022-03-05 07:55:13 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-05 07:55:13 | INFO | train | epoch 116 | loss 2.053 | ppl 4.15 | wps 18405.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 22586 | lr 0.000210417 | gnorm 1.07 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 81602
2022-03-05 07:55:13 | INFO | fairseq.trainer | begin training epoch 117
2022-03-05 07:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:56:02 | INFO | train_inner | epoch 117:     14 / 196 loss=2.077, ppl=4.22, wps=18020.1, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.073, loss_scale=16, train_wall=325, gb_free=7.2, wall=81651
2022-03-05 07:59:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:01:54 | INFO | train_inner | epoch 117:    115 / 196 loss=2.026, ppl=4.07, wps=18589.8, ups=0.28, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.074, loss_scale=16, train_wall=326, gb_free=7.2, wall=82004
2022-03-05 08:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:06:42 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 11.734 | ppl 3407.44 | wps 34994.7 | wpb 510.9 | bsz 1 | num_updates 22781 | best_loss 6.934
2022-03-05 08:06:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22781 updates
2022-03-05 08:06:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:06:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 117 @ 22781 updates, score 11.734) (writing took 5.336253331974149 seconds)
2022-03-05 08:06:47 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-05 08:06:47 | INFO | train | epoch 117 | loss 2.044 | ppl 4.12 | wps 18375.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 22781 | lr 0.000209514 | gnorm 1.076 | loss_scale 32 | train_wall 632 | gb_free 7.2 | wall 82297
2022-03-05 08:06:47 | INFO | fairseq.trainer | begin training epoch 118
2022-03-05 08:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:07:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:07:57 | INFO | train_inner | epoch 118:     20 / 196 loss=2.059, ppl=4.17, wps=18009.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.08, loss_scale=16, train_wall=325, gb_free=7.2, wall=82367
2022-03-05 08:13:46 | INFO | train_inner | epoch 118:    120 / 196 loss=2.018, ppl=4.05, wps=18778.6, ups=0.29, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.085, loss_scale=16, train_wall=323, gb_free=7.2, wall=82716
2022-03-05 08:16:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:18:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:18:16 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 11.733 | ppl 3404.47 | wps 34827.7 | wpb 510.9 | bsz 1 | num_updates 22975 | best_loss 6.934
2022-03-05 08:18:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22975 updates
2022-03-05 08:18:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:18:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:18:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 118 @ 22975 updates, score 11.733) (writing took 5.313179554417729 seconds)
2022-03-05 08:18:21 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-05 08:18:21 | INFO | train | epoch 118 | loss 2.037 | ppl 4.1 | wps 18285.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 22975 | lr 0.000208628 | gnorm 1.085 | loss_scale 16 | train_wall 632 | gb_free 7.2 | wall 82991
2022-03-05 08:18:22 | INFO | fairseq.trainer | begin training epoch 119
2022-03-05 08:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:19:49 | INFO | train_inner | epoch 119:     25 / 196 loss=2.051, ppl=4.14, wps=18023, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.088, loss_scale=16, train_wall=325, gb_free=7.2, wall=83079
2022-03-05 08:24:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:25:40 | INFO | train_inner | epoch 119:    126 / 196 loss=2.014, ppl=4.04, wps=18649.6, ups=0.28, wpb=65532.4, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.065, loss_scale=16, train_wall=325, gb_free=7.2, wall=83430
2022-03-05 08:29:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:29:49 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 11.722 | ppl 3379.19 | wps 34673 | wpb 510.9 | bsz 1 | num_updates 23170 | best_loss 6.934
2022-03-05 08:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23170 updates
2022-03-05 08:29:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:29:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:29:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 119 @ 23170 updates, score 11.722) (writing took 5.311756204813719 seconds)
2022-03-05 08:29:54 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-05 08:29:54 | INFO | train | epoch 119 | loss 2.027 | ppl 4.08 | wps 18429.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 23170 | lr 0.000207748 | gnorm 1.079 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 83684
2022-03-05 08:29:54 | INFO | fairseq.trainer | begin training epoch 120
2022-03-05 08:29:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:31:38 | INFO | train_inner | epoch 120:     30 / 196 loss=2.035, ppl=4.1, wps=18237.7, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.086, loss_scale=16, train_wall=321, gb_free=7.2, wall=83788
2022-03-05 08:33:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:37:30 | INFO | train_inner | epoch 120:    131 / 196 loss=2.009, ppl=4.03, wps=18657.5, ups=0.28, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.071, loss_scale=16, train_wall=325, gb_free=7.2, wall=84140
2022-03-05 08:41:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:41:21 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 11.757 | ppl 3462.19 | wps 35078.4 | wpb 510.9 | bsz 1 | num_updates 23365 | best_loss 6.934
2022-03-05 08:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23365 updates
2022-03-05 08:41:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:41:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:41:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 120 @ 23365 updates, score 11.757) (writing took 5.3412037789821625 seconds)
2022-03-05 08:41:26 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-05 08:41:26 | INFO | train | epoch 120 | loss 2.018 | ppl 4.05 | wps 18437.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 23365 | lr 0.000206879 | gnorm 1.079 | loss_scale 32 | train_wall 630 | gb_free 7.2 | wall 84376
2022-03-05 08:41:26 | INFO | fairseq.trainer | begin training epoch 121
2022-03-05 08:41:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:41:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:43:31 | INFO | train_inner | epoch 121:     36 / 196 loss=2.021, ppl=4.06, wps=18070.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.096, loss_scale=16, train_wall=324, gb_free=7.2, wall=84501
2022-03-05 08:49:21 | INFO | train_inner | epoch 121:    136 / 196 loss=2.001, ppl=4, wps=18748.2, ups=0.29, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.079, loss_scale=32, train_wall=323, gb_free=7.2, wall=84851
2022-03-05 08:49:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:52:58 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 11.716 | ppl 3364.85 | wps 32638.8 | wpb 510.9 | bsz 1 | num_updates 23559 | best_loss 6.934
2022-03-05 08:52:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23559 updates
2022-03-05 08:52:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:53:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 121 @ 23559 updates, score 11.716) (writing took 5.866295892745256 seconds)
2022-03-05 08:53:04 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-05 08:53:04 | INFO | train | epoch 121 | loss 2.009 | ppl 4.02 | wps 18187.6 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 23559 | lr 0.000206026 | gnorm 1.082 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 85074
2022-03-05 08:53:04 | INFO | fairseq.trainer | begin training epoch 122
2022-03-05 08:53:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:55:29 | INFO | train_inner | epoch 122:     41 / 196 loss=2.009, ppl=4.02, wps=17774.8, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.08, loss_scale=16, train_wall=328, gb_free=7.2, wall=85219
2022-03-05 08:59:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:01:23 | INFO | train_inner | epoch 122:    142 / 196 loss=2, ppl=4, wps=18487.7, ups=0.28, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.071, loss_scale=16, train_wall=328, gb_free=7.2, wall=85573
2022-03-05 09:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:04:38 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 11.759 | ppl 3465.17 | wps 33519.4 | wpb 510.9 | bsz 1 | num_updates 23754 | best_loss 6.934
2022-03-05 09:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23754 updates
2022-03-05 09:04:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:04:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 122 @ 23754 updates, score 11.759) (writing took 6.271457515656948 seconds)
2022-03-05 09:04:45 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-05 09:04:45 | INFO | train | epoch 122 | loss 2.001 | ppl 4 | wps 18225.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 23754 | lr 0.000205178 | gnorm 1.075 | loss_scale 16 | train_wall 636 | gb_free 7.2 | wall 85774
2022-03-05 09:04:45 | INFO | fairseq.trainer | begin training epoch 123
2022-03-05 09:04:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:07:26 | INFO | train_inner | epoch 123:     46 / 196 loss=2.004, ppl=4.01, wps=18027.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.08, loss_scale=32, train_wall=324, gb_free=7.2, wall=85936
2022-03-05 09:07:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:13:20 | INFO | train_inner | epoch 123:    147 / 196 loss=1.99, ppl=3.97, wps=18520.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.084, loss_scale=16, train_wall=327, gb_free=7.2, wall=86290
2022-03-05 09:15:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:16:15 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 11.821 | ppl 3618.85 | wps 35933.2 | wpb 510.9 | bsz 1 | num_updates 23948 | best_loss 6.934
2022-03-05 09:16:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23948 updates
2022-03-05 09:16:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:16:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:16:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 123 @ 23948 updates, score 11.821) (writing took 4.4002427607774734 seconds)
2022-03-05 09:16:20 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-05 09:16:20 | INFO | train | epoch 123 | loss 1.993 | ppl 3.98 | wps 18264.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 23948 | lr 0.000204346 | gnorm 1.09 | loss_scale 16 | train_wall 633 | gb_free 7.2 | wall 86470
2022-03-05 09:16:20 | INFO | fairseq.trainer | begin training epoch 124
2022-03-05 09:16:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:19:19 | INFO | train_inner | epoch 124:     52 / 196 loss=1.988, ppl=3.97, wps=18193.5, ups=0.28, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.084, loss_scale=16, train_wall=323, gb_free=7.2, wall=86649
2022-03-05 09:23:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:25:10 | INFO | train_inner | epoch 124:    153 / 196 loss=1.989, ppl=3.97, wps=18682.5, ups=0.29, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.096, loss_scale=16, train_wall=325, gb_free=7.2, wall=87000
2022-03-05 09:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:45 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 11.851 | ppl 3694.11 | wps 34098.5 | wpb 510.9 | bsz 1 | num_updates 24143 | best_loss 6.934
2022-03-05 09:27:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24143 updates
2022-03-05 09:27:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:27:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 124 @ 24143 updates, score 11.851) (writing took 4.385494517162442 seconds)
2022-03-05 09:27:49 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-05 09:27:49 | INFO | train | epoch 124 | loss 1.984 | ppl 3.95 | wps 18509.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 24143 | lr 0.000203519 | gnorm 1.09 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 87159
2022-03-05 09:27:49 | INFO | fairseq.trainer | begin training epoch 125
2022-03-05 09:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:31:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:31:11 | INFO | train_inner | epoch 125:     58 / 196 loss=1.973, ppl=3.92, wps=18095.1, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.094, loss_scale=16, train_wall=324, gb_free=7.2, wall=87361
2022-03-05 09:36:59 | INFO | train_inner | epoch 125:    158 / 196 loss=1.98, ppl=3.95, wps=18848.3, ups=0.29, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.088, loss_scale=16, train_wall=322, gb_free=7.2, wall=87708
2022-03-05 09:39:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:39:16 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 11.785 | ppl 3530.02 | wps 35025.1 | wpb 510.9 | bsz 1 | num_updates 24338 | best_loss 6.934
2022-03-05 09:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24338 updates
2022-03-05 09:39:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:39:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:39:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 125 @ 24338 updates, score 11.785) (writing took 5.572446273639798 seconds)
2022-03-05 09:39:22 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-05 09:39:22 | INFO | train | epoch 125 | loss 1.975 | ppl 3.93 | wps 18425.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 24338 | lr 0.000202702 | gnorm 1.089 | loss_scale 32 | train_wall 630 | gb_free 7.2 | wall 87852
2022-03-05 09:39:22 | INFO | fairseq.trainer | begin training epoch 126
2022-03-05 09:39:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:41:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:43:01 | INFO | train_inner | epoch 126:     63 / 196 loss=1.959, ppl=3.89, wps=18038, ups=0.28, wpb=65367, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.083, loss_scale=16, train_wall=324, gb_free=7.2, wall=88071
2022-03-05 09:48:49 | INFO | train_inner | epoch 126:    163 / 196 loss=1.983, ppl=3.95, wps=18846.3, ups=0.29, wpb=65532.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.094, loss_scale=32, train_wall=322, gb_free=7.2, wall=88419
2022-03-05 09:49:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:50:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:50:49 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 11.875 | ppl 3755.32 | wps 35486 | wpb 510.9 | bsz 1 | num_updates 24532 | best_loss 6.934
2022-03-05 09:50:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24532 updates
2022-03-05 09:50:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:50:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 126 @ 24532 updates, score 11.875) (writing took 4.728415444493294 seconds)
2022-03-05 09:50:54 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-05 09:50:54 | INFO | train | epoch 126 | loss 1.966 | ppl 3.91 | wps 18353.4 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 24532 | lr 0.000201899 | gnorm 1.089 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 88543
2022-03-05 09:50:54 | INFO | fairseq.trainer | begin training epoch 127
2022-03-05 09:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:54:50 | INFO | train_inner | epoch 127:     68 / 196 loss=1.944, ppl=3.85, wps=18080.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.09, loss_scale=16, train_wall=324, gb_free=7.2, wall=88780
2022-03-05 09:57:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:00:42 | INFO | train_inner | epoch 127:    169 / 196 loss=1.975, ppl=3.93, wps=18647, ups=0.28, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.086, loss_scale=16, train_wall=325, gb_free=7.2, wall=89132
2022-03-05 10:02:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:02:21 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 11.888 | ppl 3790.88 | wps 34684.8 | wpb 510.9 | bsz 1 | num_updates 24727 | best_loss 6.934
2022-03-05 10:02:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24727 updates
2022-03-05 10:02:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:02:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:02:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 127 @ 24727 updates, score 11.888) (writing took 4.714602695778012 seconds)
2022-03-05 10:02:26 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-05 10:02:26 | INFO | train | epoch 127 | loss 1.959 | ppl 3.89 | wps 18434.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 24727 | lr 0.000201101 | gnorm 1.09 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 89236
2022-03-05 10:02:26 | INFO | fairseq.trainer | begin training epoch 128
2022-03-05 10:02:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:05:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:06:44 | INFO | train_inner | epoch 128:     74 / 196 loss=1.935, ppl=3.82, wps=18064.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.091, loss_scale=16, train_wall=325, gb_free=7.2, wall=89493
2022-03-05 10:12:31 | INFO | train_inner | epoch 128:    174 / 196 loss=1.973, ppl=3.92, wps=18852, ups=0.29, wpb=65532.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.097, loss_scale=16, train_wall=322, gb_free=7.2, wall=89841
2022-03-05 10:13:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:13:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:13:53 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 11.91 | ppl 3849.33 | wps 34598.8 | wpb 510.9 | bsz 1 | num_updates 24921 | best_loss 6.934
2022-03-05 10:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24921 updates
2022-03-05 10:13:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 128 @ 24921 updates, score 11.91) (writing took 5.568707957863808 seconds)
2022-03-05 10:13:59 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-05 10:13:59 | INFO | train | epoch 128 | loss 1.951 | ppl 3.87 | wps 18321 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 24921 | lr 0.000200317 | gnorm 1.089 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 89929
2022-03-05 10:13:59 | INFO | fairseq.trainer | begin training epoch 129
2022-03-05 10:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:18:34 | INFO | train_inner | epoch 129:     79 / 196 loss=1.924, ppl=3.79, wps=18012.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.081, loss_scale=16, train_wall=325, gb_free=7.2, wall=90204
2022-03-05 10:21:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:24:25 | INFO | train_inner | epoch 129:    180 / 196 loss=1.967, ppl=3.91, wps=18694.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.106, loss_scale=16, train_wall=324, gb_free=7.2, wall=90554
2022-03-05 10:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:25:25 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 11.867 | ppl 3736.43 | wps 35408.4 | wpb 510.9 | bsz 1 | num_updates 25116 | best_loss 6.934
2022-03-05 10:25:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25116 updates
2022-03-05 10:25:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:25:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 129 @ 25116 updates, score 11.867) (writing took 5.17590544000268 seconds)
2022-03-05 10:25:30 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-05 10:25:30 | INFO | train | epoch 129 | loss 1.944 | ppl 3.85 | wps 18459.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 25116 | lr 0.000199538 | gnorm 1.096 | loss_scale 16 | train_wall 629 | gb_free 7.2 | wall 90620
2022-03-05 10:25:30 | INFO | fairseq.trainer | begin training epoch 130
2022-03-05 10:25:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:30:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:30:24 | INFO | train_inner | epoch 130:     85 / 196 loss=1.914, ppl=3.77, wps=18183.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.089, loss_scale=16, train_wall=322, gb_free=7.2, wall=90914
2022-03-05 10:36:10 | INFO | train_inner | epoch 130:    185 / 196 loss=1.963, ppl=3.9, wps=18946.9, ups=0.29, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.102, loss_scale=16, train_wall=320, gb_free=7.2, wall=91260
2022-03-05 10:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:36:54 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 11.991 | ppl 4071.76 | wps 34882.2 | wpb 510.9 | bsz 1 | num_updates 25311 | best_loss 6.934
2022-03-05 10:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25311 updates
2022-03-05 10:36:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:36:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 130 @ 25311 updates, score 11.991) (writing took 4.624208757653832 seconds)
2022-03-05 10:36:58 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-05 10:36:58 | INFO | train | epoch 130 | loss 1.936 | ppl 3.83 | wps 18547.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 25311 | lr 0.000198767 | gnorm 1.093 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 91308
2022-03-05 10:36:58 | INFO | fairseq.trainer | begin training epoch 131
2022-03-05 10:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:38:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:42:12 | INFO | train_inner | epoch 131:     90 / 196 loss=1.905, ppl=3.75, wps=18051.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.085, loss_scale=16, train_wall=325, gb_free=7.2, wall=91622
2022-03-05 10:45:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:48:02 | INFO | train_inner | epoch 131:    191 / 196 loss=1.957, ppl=3.88, wps=18739.1, ups=0.29, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.109, loss_scale=16, train_wall=324, gb_free=7.2, wall=91972
2022-03-05 10:48:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:48:25 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 11.904 | ppl 3832.39 | wps 34623.2 | wpb 510.9 | bsz 1 | num_updates 25505 | best_loss 6.934
2022-03-05 10:48:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25505 updates
2022-03-05 10:48:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:48:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:48:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 131 @ 25505 updates, score 11.904) (writing took 6.129750872030854 seconds)
2022-03-05 10:48:31 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-05 10:48:31 | INFO | train | epoch 131 | loss 1.928 | ppl 3.81 | wps 18337.6 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 25505 | lr 0.00019801 | gnorm 1.098 | loss_scale 16 | train_wall 629 | gb_free 7.2 | wall 92001
2022-03-05 10:48:31 | INFO | fairseq.trainer | begin training epoch 132
2022-03-05 10:48:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:54:00 | INFO | train_inner | epoch 132:     95 / 196 loss=1.889, ppl=3.7, wps=18259.9, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.079, loss_scale=32, train_wall=320, gb_free=7.2, wall=92330
2022-03-05 10:55:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:59:49 | INFO | train_inner | epoch 132:    196 / 196 loss=1.954, ppl=3.87, wps=18698.7, ups=0.29, wpb=65367, bsz=127.7, num_updates=25700, lr=0.000197257, gnorm=1.078, loss_scale=16, train_wall=323, gb_free=7.2, wall=92679
2022-03-05 10:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:59:56 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 11.966 | ppl 3999.99 | wps 34723.9 | wpb 510.9 | bsz 1 | num_updates 25700 | best_loss 6.934
2022-03-05 10:59:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25700 updates
2022-03-05 10:59:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:00:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:00:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 132 @ 25700 updates, score 11.966) (writing took 4.429856512695551 seconds)
2022-03-05 11:00:00 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-05 11:00:00 | INFO | train | epoch 132 | loss 1.921 | ppl 3.79 | wps 18512.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 25700 | lr 0.000197257 | gnorm 1.078 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 92690
2022-03-05 11:00:00 | INFO | fairseq.trainer | begin training epoch 133
2022-03-05 11:00:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:03:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:05:51 | INFO | train_inner | epoch 133:    101 / 196 loss=1.881, ppl=3.68, wps=18107.5, ups=0.28, wpb=65532.4, bsz=128, num_updates=25800, lr=0.000196875, gnorm=1.088, loss_scale=16, train_wall=325, gb_free=7.2, wall=93041
2022-03-05 11:11:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:11:27 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 12.052 | ppl 4245.41 | wps 34018.6 | wpb 510.9 | bsz 1 | num_updates 25895 | best_loss 6.934
2022-03-05 11:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25895 updates
2022-03-05 11:11:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:11:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:11:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 133 @ 25895 updates, score 12.052) (writing took 5.553248001262546 seconds)
2022-03-05 11:11:33 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-05 11:11:33 | INFO | train | epoch 133 | loss 1.913 | ppl 3.77 | wps 18429.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 25895 | lr 0.000196513 | gnorm 1.087 | loss_scale 32 | train_wall 630 | gb_free 7.2 | wall 93383
2022-03-05 11:11:33 | INFO | fairseq.trainer | begin training epoch 134
2022-03-05 11:11:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:11:50 | INFO | train_inner | epoch 134:      5 / 196 loss=1.941, ppl=3.84, wps=18212.5, ups=0.28, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.082, loss_scale=32, train_wall=321, gb_free=7.2, wall=93400
2022-03-05 11:12:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:17:41 | INFO | train_inner | epoch 134:    106 / 196 loss=1.878, ppl=3.68, wps=18658.2, ups=0.28, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.096, loss_scale=16, train_wall=325, gb_free=7.2, wall=93751
2022-03-05 11:20:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:22:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:23:00 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 12.004 | ppl 4107.63 | wps 33993.8 | wpb 510.9 | bsz 1 | num_updates 26089 | best_loss 6.934
2022-03-05 11:23:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26089 updates
2022-03-05 11:23:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:23:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 134 @ 26089 updates, score 12.004) (writing took 5.932038825005293 seconds)
2022-03-05 11:23:06 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-05 11:23:06 | INFO | train | epoch 134 | loss 1.905 | ppl 3.75 | wps 18312 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 26089 | lr 0.000195781 | gnorm 1.092 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 94076
2022-03-05 11:23:06 | INFO | fairseq.trainer | begin training epoch 135
2022-03-05 11:23:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:23:44 | INFO | train_inner | epoch 135:     11 / 196 loss=1.928, ppl=3.8, wps=18017.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.093, loss_scale=16, train_wall=324, gb_free=7.2, wall=94114
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/cross_entropy.py", line 35, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4217, in multi_head_attention_forward
    v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
