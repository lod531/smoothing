Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 207014247: <w103_fp16_size_0.25_jelinek_0.02_0.08_0.9_#2> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.02_0.08_0.9_#2> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 10:48:23 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 10:48:39 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 10:48:39 2022
Terminated at Fri Mar  4 13:22:40 2022
Results reported at Fri Mar  4 13:22:40 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.02, 0.08, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321672 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   95588.38 sec.
    Max Memory :                                 8745 MB
    Average Memory :                             3355.99 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11255.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   95641 sec.
    Turnaround time :                            95657 sec.

The output (if any) follows:

2022-03-03 10:48:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321672, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321672, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02, 0.08, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 10:48:50 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 10:48:54 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 687/450337 [00:00<01:05, 6861.01it/s]  0%|          | 1374/450337 [00:00<01:11, 6238.00it/s]  0%|          | 2002/450337 [00:00<01:14, 5987.75it/s]  1%|          | 2641/450337 [00:00<01:12, 6137.20it/s]  1%|          | 3316/450337 [00:00<01:10, 6342.98it/s]  1%|          | 3998/450337 [00:00<01:08, 6495.42it/s]  1%|          | 4781/450337 [00:00<01:04, 6923.44it/s]  1%|          | 5514/450337 [00:00<01:03, 7041.70it/s]  1%|▏         | 6220/450337 [00:00<01:03, 6964.96it/s]  2%|▏         | 6918/450337 [00:01<01:09, 6387.57it/s]  2%|▏         | 7591/450337 [00:01<01:08, 6480.16it/s]  2%|▏         | 8247/450337 [00:01<01:09, 6348.06it/s]  2%|▏         | 8887/450337 [00:01<01:09, 6335.77it/s]  2%|▏         | 9525/450337 [00:01<01:10, 6277.12it/s]  2%|▏         | 10244/450337 [00:01<01:07, 6537.57it/s]  2%|▏         | 10901/450337 [00:01<01:09, 6307.50it/s]  3%|▎         | 11541/450337 [00:01<01:09, 6333.60it/s]  3%|▎         | 12203/450337 [00:01<01:08, 6416.68it/s]  3%|▎         | 12847/450337 [00:01<01:08, 6348.65it/s]  3%|▎         | 13556/450337 [00:02<01:06, 6564.37it/s]  3%|▎         | 14214/450337 [00:02<01:07, 6420.91it/s]  3%|▎         | 14893/450337 [00:02<01:06, 6528.40it/s]  3%|▎         | 15548/450337 [00:02<01:07, 6442.86it/s]  4%|▎         | 16194/450337 [00:02<01:09, 6267.84it/s]  4%|▎         | 16823/450337 [00:02<01:09, 6231.36it/s]  4%|▍         | 17500/450337 [00:02<01:07, 6386.81it/s]  4%|▍         | 18140/450337 [00:02<01:08, 6289.83it/s]  4%|▍         | 18803/450337 [00:02<01:07, 6386.27it/s]  4%|▍         | 19599/450337 [00:03<01:02, 6849.27it/s]  5%|▍         | 20286/450337 [00:03<01:06, 6430.88it/s]  5%|▍         | 20955/450337 [00:03<01:06, 6501.15it/s]  5%|▍         | 21610/450337 [00:03<01:09, 6208.16it/s]  5%|▍         | 22312/450337 [00:03<01:06, 6432.82it/s]  5%|▌         | 23001/450337 [00:03<01:05, 6553.20it/s]  5%|▌         | 23717/450337 [00:03<01:03, 6725.84it/s]  5%|▌         | 24515/450337 [00:03<01:00, 7086.03it/s]  6%|▌         | 25255/450337 [00:03<00:59, 7176.47it/s]  6%|▌         | 25975/450337 [00:03<01:01, 6867.22it/s]  6%|▌         | 26666/450337 [00:04<01:05, 6510.96it/s]  6%|▌         | 27323/450337 [00:04<01:07, 6248.37it/s]  6%|▌         | 27953/450337 [00:04<01:08, 6161.78it/s]  6%|▋         | 28736/450337 [00:04<01:03, 6618.33it/s]  7%|▋         | 29404/450337 [00:04<01:06, 6291.44it/s]  7%|▋         | 30106/450337 [00:04<01:04, 6491.92it/s]  7%|▋         | 30761/450337 [00:04<01:08, 6155.29it/s]  7%|▋         | 31390/450337 [00:04<01:07, 6188.44it/s]  7%|▋         | 32014/450337 [00:04<01:08, 6119.28it/s]  7%|▋         | 32630/450337 [00:05<01:08, 6113.93it/s]  7%|▋         | 33244/450337 [00:05<01:09, 5984.74it/s]  8%|▊         | 33861/450337 [00:05<01:08, 6037.21it/s]  8%|▊         | 34547/450337 [00:05<01:06, 6274.76it/s]  8%|▊         | 35177/450337 [00:05<01:06, 6199.82it/s]  8%|▊         | 35819/450337 [00:05<01:06, 6262.63it/s]  8%|▊         | 36473/450337 [00:05<01:05, 6339.32it/s]  8%|▊         | 37108/450337 [00:05<01:07, 6103.78it/s]  8%|▊         | 37721/450337 [00:05<01:08, 6008.64it/s]  9%|▊         | 38325/450337 [00:06<01:08, 6016.79it/s]  9%|▊         | 38982/450337 [00:06<01:06, 6175.96it/s]  9%|▉         | 39601/450337 [00:06<01:06, 6132.15it/s]  9%|▉         | 40285/450337 [00:06<01:04, 6332.98it/s]  9%|▉         | 40983/450337 [00:06<01:02, 6524.22it/s]  9%|▉         | 41637/450337 [00:06<01:04, 6351.06it/s]  9%|▉         | 42274/450337 [00:06<01:07, 6067.49it/s] 10%|▉         | 42884/450337 [00:06<01:07, 6036.08it/s] 10%|▉         | 43495/450337 [00:06<01:07, 6050.24it/s] 10%|▉         | 44162/450337 [00:06<01:05, 6227.57it/s] 10%|▉         | 44863/450337 [00:07<01:02, 6457.26it/s] 10%|█         | 45525/450337 [00:07<01:02, 6502.55it/s] 10%|█         | 46270/450337 [00:07<00:59, 6780.30it/s] 10%|█         | 46950/450337 [00:07<01:01, 6541.81it/s] 11%|█         | 47954/450337 [00:07<00:53, 7553.06it/s] 11%|█         | 48715/450337 [00:07<00:54, 7419.71it/s] 11%|█         | 49477/450337 [00:07<00:53, 7476.67it/s] 11%|█         | 50228/450337 [00:07<00:57, 6927.99it/s] 11%|█▏        | 50931/450337 [00:07<01:00, 6653.16it/s] 11%|█▏        | 51604/450337 [00:07<01:00, 6632.71it/s] 12%|█▏        | 52453/450337 [00:08<00:55, 7147.20it/s] 12%|█▏        | 53175/450337 [00:08<00:57, 6874.53it/s] 12%|█▏        | 53869/450337 [00:08<00:58, 6728.12it/s] 12%|█▏        | 54547/450337 [00:08<01:02, 6318.52it/s] 12%|█▏        | 55204/450337 [00:08<01:01, 6387.31it/s] 12%|█▏        | 55871/450337 [00:08<01:01, 6461.55it/s] 13%|█▎        | 56557/450337 [00:08<00:59, 6575.59it/s] 13%|█▎        | 57218/450337 [00:08<01:03, 6157.71it/s] 13%|█▎        | 57847/450337 [00:08<01:03, 6190.46it/s] 13%|█▎        | 58620/450337 [00:09<00:59, 6628.28it/s] 13%|█▎        | 59289/450337 [00:09<01:01, 6358.96it/s] 13%|█▎        | 59931/450337 [00:09<01:01, 6300.67it/s] 13%|█▎        | 60585/450337 [00:09<01:01, 6362.91it/s] 14%|█▎        | 61411/450337 [00:09<00:56, 6906.73it/s] 14%|█▍        | 62106/450337 [00:09<00:58, 6609.62it/s] 14%|█▍        | 62772/450337 [00:09<00:58, 6581.21it/s] 14%|█▍        | 63434/450337 [00:09<00:59, 6502.84it/s] 14%|█▍        | 64087/450337 [00:09<01:00, 6409.41it/s] 14%|█▍        | 64736/450337 [00:10<01:00, 6417.49it/s] 15%|█▍        | 65379/450337 [00:10<01:01, 6307.24it/s] 15%|█▍        | 66034/450337 [00:10<01:00, 6374.17it/s] 15%|█▍        | 66803/450337 [00:10<00:56, 6757.14it/s] 15%|█▍        | 67481/450337 [00:10<01:00, 6320.67it/s] 15%|█▌        | 68120/450337 [00:10<01:03, 6062.19it/s] 15%|█▌        | 68921/450337 [00:10<00:57, 6597.58it/s] 15%|█▌        | 69589/450337 [00:10<00:58, 6531.13it/s] 16%|█▌        | 70251/450337 [00:10<00:58, 6547.81it/s] 16%|█▌        | 70910/450337 [00:10<00:59, 6375.86it/s] 16%|█▌        | 71585/450337 [00:11<00:58, 6479.90it/s] 16%|█▌        | 72240/450337 [00:11<00:58, 6499.40it/s] 16%|█▌        | 73028/450337 [00:11<00:54, 6901.78it/s] 16%|█▋        | 73758/450337 [00:11<00:53, 7014.75it/s] 17%|█▋        | 74589/450337 [00:11<00:50, 7393.50it/s] 17%|█▋        | 75330/450337 [00:11<00:53, 6999.10it/s] 17%|█▋        | 76036/450337 [00:11<00:54, 6897.34it/s] 17%|█▋        | 76730/450337 [00:11<00:54, 6892.92it/s] 17%|█▋        | 77422/450337 [00:11<00:54, 6780.96it/s] 17%|█▋        | 78102/450337 [00:12<00:55, 6662.81it/s] 17%|█▋        | 78770/450337 [00:12<00:59, 6223.27it/s] 18%|█▊        | 79399/450337 [00:12<01:02, 5950.03it/s] 18%|█▊        | 80001/450337 [00:12<01:02, 5968.37it/s] 18%|█▊        | 80602/450337 [00:12<01:02, 5925.33it/s] 18%|█▊        | 81372/450337 [00:12<00:57, 6430.51it/s] 18%|█▊        | 82020/450337 [00:12<00:58, 6287.26it/s] 18%|█▊        | 82725/450337 [00:12<00:56, 6506.33it/s] 19%|█▊        | 83413/450337 [00:12<00:55, 6612.94it/s] 19%|█▊        | 84084/450337 [00:12<00:55, 6639.56it/s] 19%|█▉        | 84776/450337 [00:13<00:54, 6719.23it/s] 19%|█▉        | 85450/450337 [00:13<00:56, 6419.02it/s] 19%|█▉        | 86096/450337 [00:13<00:56, 6418.08it/s] 19%|█▉        | 86786/450337 [00:13<00:55, 6558.24it/s] 19%|█▉        | 87514/450337 [00:13<00:53, 6769.48it/s] 20%|█▉        | 88223/450337 [00:13<00:52, 6863.42it/s] 20%|█▉        | 88911/450337 [00:13<00:57, 6266.32it/s] 20%|█▉        | 89549/450337 [00:13<00:58, 6195.39it/s] 20%|██        | 90176/450337 [00:13<00:59, 6046.87it/s] 20%|██        | 90786/450337 [00:14<01:00, 5948.99it/s] 20%|██        | 91540/450337 [00:14<00:56, 6394.05it/s] 20%|██        | 92202/450337 [00:14<00:55, 6454.62it/s] 21%|██        | 92875/450337 [00:14<00:54, 6534.65it/s] 21%|██        | 93532/450337 [00:14<00:55, 6406.14it/s] 21%|██        | 94175/450337 [00:14<00:58, 6092.17it/s] 21%|██        | 94920/450337 [00:14<00:54, 6472.39it/s] 21%|██        | 95573/450337 [00:14<00:56, 6292.26it/s] 21%|██▏       | 96280/450337 [00:14<00:54, 6511.99it/s] 22%|██▏       | 97102/450337 [00:14<00:50, 7002.60it/s] 22%|██▏       | 97807/450337 [00:15<00:53, 6598.75it/s] 22%|██▏       | 98475/450337 [00:15<00:53, 6612.17it/s] 22%|██▏       | 99142/450337 [00:15<00:54, 6485.80it/s] 22%|██▏       | 99795/450337 [00:15<00:55, 6320.26it/s] 22%|██▏       | 100430/450337 [00:15<00:57, 6095.80it/s] 22%|██▏       | 101124/450337 [00:15<00:55, 6329.34it/s] 23%|██▎       | 101761/450337 [00:15<00:55, 6298.65it/s] 23%|██▎       | 102406/450337 [00:15<00:54, 6341.52it/s] 23%|██▎       | 103064/450337 [00:15<00:54, 6410.27it/s] 23%|██▎       | 103707/450337 [00:16<00:54, 6309.18it/s] 23%|██▎       | 104340/450337 [00:16<00:55, 6252.58it/s] 23%|██▎       | 104994/450337 [00:16<00:54, 6335.58it/s] 23%|██▎       | 105657/450337 [00:16<00:53, 6413.82it/s] 24%|██▎       | 106300/450337 [00:16<00:54, 6328.68it/s] 24%|██▎       | 106934/450337 [00:16<00:54, 6262.39it/s] 24%|██▍       | 107561/450337 [00:16<00:54, 6261.09it/s] 24%|██▍       | 108188/450337 [00:16<00:55, 6126.97it/s] 24%|██▍       | 108802/450337 [00:16<00:57, 5933.45it/s] 24%|██▍       | 109440/450337 [00:16<00:56, 6059.71it/s] 24%|██▍       | 110082/450337 [00:17<00:55, 6162.14it/s] 25%|██▍       | 110797/450337 [00:17<00:52, 6451.86it/s] 25%|██▍       | 111491/450337 [00:17<00:51, 6594.77it/s] 25%|██▍       | 112152/450337 [00:17<00:53, 6375.59it/s] 25%|██▌       | 112829/450337 [00:17<00:52, 6489.16it/s] 25%|██▌       | 113480/450337 [00:17<00:53, 6253.50it/s] 25%|██▌       | 114124/450337 [00:17<00:53, 6306.64it/s] 25%|██▌       | 114757/450337 [00:17<00:53, 6253.37it/s] 26%|██▌       | 115384/450337 [00:17<00:53, 6213.97it/s] 26%|██▌       | 116007/450337 [00:18<00:56, 5967.11it/s] 26%|██▌       | 116617/450337 [00:18<00:55, 6002.47it/s] 26%|██▌       | 117220/450337 [00:18<00:56, 5939.23it/s] 26%|██▌       | 117830/450337 [00:18<00:55, 5982.34it/s] 26%|██▋       | 118552/450337 [00:18<00:52, 6341.76it/s] 26%|██▋       | 119188/450337 [00:18<00:53, 6151.09it/s] 27%|██▋       | 119806/450337 [00:18<00:53, 6155.01it/s] 27%|██▋       | 120613/450337 [00:18<00:49, 6713.58it/s] 27%|██▋       | 121287/450337 [00:18<00:50, 6550.03it/s] 27%|██▋       | 121945/450337 [00:18<00:52, 6234.82it/s] 27%|██▋       | 122626/450337 [00:19<00:51, 6393.91it/s] 27%|██▋       | 123270/450337 [00:19<00:52, 6220.94it/s] 28%|██▊       | 123914/450337 [00:19<00:51, 6281.66it/s] 28%|██▊       | 124545/450337 [00:19<00:51, 6266.34it/s] 28%|██▊       | 125177/450337 [00:19<00:51, 6276.63it/s] 28%|██▊       | 125857/450337 [00:19<00:50, 6427.98it/s] 28%|██▊       | 126550/450337 [00:19<00:49, 6574.64it/s] 28%|██▊       | 127214/450337 [00:19<00:49, 6589.81it/s] 28%|██▊       | 127874/450337 [00:19<00:49, 6502.39it/s] 29%|██▊       | 128525/450337 [00:19<00:50, 6339.58it/s] 29%|██▊       | 129203/450337 [00:20<00:49, 6463.83it/s] 29%|██▉       | 129851/450337 [00:20<00:51, 6215.08it/s] 29%|██▉       | 130501/450337 [00:20<00:50, 6294.17it/s] 29%|██▉       | 131153/450337 [00:20<00:50, 6359.75it/s] 29%|██▉       | 131791/450337 [00:20<00:55, 5760.82it/s] 29%|██▉       | 132567/450337 [00:20<00:50, 6290.91it/s] 30%|██▉       | 133250/450337 [00:20<00:49, 6440.20it/s] 30%|██▉       | 133904/450337 [00:20<00:50, 6306.57it/s] 30%|██▉       | 134648/450337 [00:20<00:47, 6626.45it/s] 30%|███       | 135429/450337 [00:21<00:45, 6965.94it/s] 30%|███       | 136132/450337 [00:21<00:45, 6862.33it/s] 30%|███       | 136870/450337 [00:21<00:44, 7008.11it/s] 31%|███       | 137600/450337 [00:21<00:44, 7088.39it/s] 31%|███       | 138312/450337 [00:21<00:46, 6712.31it/s] 31%|███       | 139004/450337 [00:21<00:45, 6771.40it/s] 31%|███       | 139686/450337 [00:21<00:46, 6633.84it/s] 31%|███       | 140353/450337 [00:21<00:48, 6415.38it/s] 31%|███▏      | 141030/450337 [00:21<00:47, 6515.95it/s] 31%|███▏      | 141685/450337 [00:22<00:48, 6399.24it/s] 32%|███▏      | 142327/450337 [00:22<00:49, 6207.32it/s] 32%|███▏      | 142950/450337 [00:22<00:50, 6128.05it/s] 32%|███▏      | 143739/450337 [00:22<00:46, 6632.79it/s] 32%|███▏      | 144406/450337 [00:22<00:46, 6549.86it/s] 32%|███▏      | 145174/450337 [00:22<00:44, 6874.63it/s] 32%|███▏      | 145865/450337 [00:22<00:44, 6807.06it/s] 33%|███▎      | 146548/450337 [00:22<00:45, 6623.40it/s] 33%|███▎      | 147213/450337 [00:22<00:47, 6436.17it/s] 33%|███▎      | 147859/450337 [00:22<00:49, 6066.19it/s] 33%|███▎      | 148516/450337 [00:23<00:48, 6201.01it/s] 33%|███▎      | 149191/450337 [00:23<00:47, 6355.75it/s] 33%|███▎      | 149831/450337 [00:23<00:48, 6201.80it/s] 33%|███▎      | 150455/450337 [00:23<00:49, 6118.21it/s] 34%|███▎      | 151069/450337 [00:23<00:48, 6116.86it/s] 34%|███▎      | 151705/450337 [00:23<00:48, 6177.41it/s] 34%|███▍      | 152324/450337 [00:23<00:50, 5907.49it/s] 34%|███▍      | 153031/450337 [00:23<00:47, 6236.46it/s] 34%|███▍      | 153659/450337 [00:23<00:47, 6230.75it/s] 34%|███▍      | 154285/450337 [00:24<00:47, 6231.45it/s] 34%|███▍      | 154924/450337 [00:24<00:47, 6278.05it/s] 35%|███▍      | 155586/450337 [00:24<00:46, 6377.38it/s] 35%|███▍      | 156284/450337 [00:24<00:44, 6556.24it/s] 35%|███▍      | 156941/450337 [00:24<00:45, 6409.28it/s] 35%|███▌      | 157662/450337 [00:24<00:44, 6642.59it/s] 35%|███▌      | 158407/450337 [00:24<00:42, 6873.39it/s] 35%|███▌      | 159096/450337 [00:24<00:45, 6340.13it/s] 35%|███▌      | 159793/450337 [00:24<00:44, 6513.54it/s] 36%|███▌      | 160452/450337 [00:24<00:44, 6485.04it/s] 36%|███▌      | 161167/450337 [00:25<00:43, 6672.78it/s] 36%|███▌      | 161839/450337 [00:25<00:45, 6276.71it/s] 36%|███▌      | 162555/450337 [00:25<00:44, 6524.33it/s] 36%|███▌      | 163236/450337 [00:25<00:43, 6606.13it/s] 36%|███▋      | 163902/450337 [00:25<00:45, 6295.22it/s] 37%|███▋      | 164538/450337 [00:25<00:45, 6282.74it/s] 37%|███▋      | 165264/450337 [00:25<00:43, 6559.19it/s] 37%|███▋      | 165987/450337 [00:25<00:42, 6749.20it/s] 37%|███▋      | 166666/450337 [00:25<00:42, 6732.34it/s] 37%|███▋      | 167342/450337 [00:25<00:42, 6719.38it/s] 37%|███▋      | 168016/450337 [00:26<00:44, 6323.06it/s] 37%|███▋      | 168706/450337 [00:26<00:43, 6484.54it/s] 38%|███▊      | 169360/450337 [00:26<00:43, 6441.27it/s] 38%|███▊      | 170008/450337 [00:26<00:44, 6239.17it/s] 38%|███▊      | 170636/450337 [00:26<00:46, 6052.75it/s] 38%|███▊      | 171245/450337 [00:26<00:46, 6009.19it/s] 38%|███▊      | 171892/450337 [00:26<00:45, 6140.67it/s] 38%|███▊      | 172508/450337 [00:26<00:45, 6127.50it/s] 38%|███▊      | 173123/450337 [00:26<00:45, 6128.42it/s] 39%|███▊      | 173781/450337 [00:27<00:44, 6256.46it/s] 39%|███▊      | 174408/450337 [00:27<00:45, 6098.70it/s] 39%|███▉      | 175133/450337 [00:27<00:42, 6432.91it/s] 39%|███▉      | 175779/450337 [00:27<00:44, 6188.32it/s] 39%|███▉      | 176419/450337 [00:27<00:43, 6246.62it/s] 39%|███▉      | 177046/450337 [00:27<00:45, 6068.42it/s] 39%|███▉      | 177656/450337 [00:27<00:49, 5558.11it/s] 40%|███▉      | 178313/450337 [00:27<00:46, 5832.95it/s] 40%|███▉      | 178973/450337 [00:27<00:44, 6046.53it/s] 40%|███▉      | 179620/450337 [00:28<00:43, 6157.95it/s] 40%|████      | 180242/450337 [00:28<00:44, 6037.76it/s] 40%|████      | 180879/450337 [00:28<00:43, 6127.01it/s] 40%|████      | 181496/450337 [00:28<00:44, 5988.22it/s] 40%|████      | 182126/450337 [00:28<00:44, 6072.39it/s] 41%|████      | 182736/450337 [00:28<00:44, 6053.34it/s] 41%|████      | 183361/450337 [00:28<00:43, 6107.39it/s] 41%|████      | 183973/450337 [00:28<00:46, 5736.48it/s] 41%|████      | 184562/450337 [00:28<00:46, 5777.31it/s] 41%|████      | 185208/450337 [00:28<00:44, 5965.94it/s] 41%|████▏     | 185808/450337 [00:29<00:45, 5817.13it/s] 41%|████▏     | 186393/450337 [00:29<00:45, 5751.97it/s] 42%|████▏     | 187097/450337 [00:29<00:43, 6121.67it/s] 42%|████▏     | 187732/450337 [00:29<00:42, 6185.70it/s] 42%|████▏     | 188353/450337 [00:29<00:42, 6181.22it/s] 42%|████▏     | 189023/450337 [00:29<00:41, 6333.49it/s] 42%|████▏     | 189681/450337 [00:29<00:40, 6406.01it/s] 42%|████▏     | 190323/450337 [00:29<00:40, 6379.45it/s] 42%|████▏     | 190989/450337 [00:29<00:40, 6456.95it/s] 43%|████▎     | 191636/450337 [00:29<00:40, 6389.36it/s] 43%|████▎     | 192315/450337 [00:30<00:39, 6503.88it/s] 43%|████▎     | 192966/450337 [00:30<00:39, 6464.87it/s] 43%|████▎     | 193627/450337 [00:30<00:39, 6507.81it/s] 43%|████▎     | 194279/450337 [00:30<00:39, 6469.93it/s] 43%|████▎     | 194927/450337 [00:30<00:39, 6472.48it/s] 43%|████▎     | 195575/450337 [00:30<00:39, 6390.93it/s] 44%|████▎     | 196215/450337 [00:30<00:40, 6233.72it/s] 44%|████▎     | 196847/450337 [00:30<00:40, 6258.31it/s] 44%|████▍     | 197822/450337 [00:30<00:34, 7282.85it/s] 44%|████▍     | 198553/450337 [00:31<00:37, 6692.42it/s] 44%|████▍     | 199233/450337 [00:31<00:37, 6617.50it/s] 44%|████▍     | 199902/450337 [00:31<00:38, 6494.94it/s] 45%|████▍     | 200557/450337 [00:31<00:40, 6159.86it/s] 45%|████▍     | 201224/450337 [00:31<00:39, 6297.02it/s] 45%|████▍     | 201859/450337 [00:31<00:39, 6239.93it/s] 45%|████▍     | 202487/450337 [00:31<00:39, 6245.69it/s] 45%|████▌     | 203159/450337 [00:31<00:38, 6378.53it/s] 45%|████▌     | 203904/450337 [00:31<00:36, 6688.31it/s] 45%|████▌     | 204576/450337 [00:31<00:38, 6332.92it/s] 46%|████▌     | 205215/450337 [00:32<00:39, 6242.18it/s] 46%|████▌     | 205843/450337 [00:32<00:39, 6182.06it/s] 46%|████▌     | 206528/450337 [00:32<00:38, 6369.29it/s] 46%|████▌     | 207168/450337 [00:32<00:39, 6149.61it/s] 46%|████▌     | 207795/450337 [00:32<00:39, 6177.86it/s] 46%|████▋     | 208415/450337 [00:32<00:39, 6105.89it/s] 46%|████▋     | 209028/450337 [00:32<00:41, 5816.06it/s] 47%|████▋     | 209732/450337 [00:32<00:39, 6162.02it/s] 47%|████▋     | 210446/450337 [00:32<00:37, 6439.79it/s] 47%|████▋     | 211095/450337 [00:33<00:37, 6428.00it/s] 47%|████▋     | 211741/450337 [00:33<00:37, 6319.63it/s] 47%|████▋     | 212400/450337 [00:33<00:37, 6392.90it/s] 47%|████▋     | 213147/450337 [00:33<00:35, 6707.13it/s] 47%|████▋     | 213820/450337 [00:33<00:38, 6141.32it/s] 48%|████▊     | 214445/450337 [00:33<00:38, 6125.55it/s] 48%|████▊     | 215065/450337 [00:33<00:38, 6042.18it/s] 48%|████▊     | 215675/450337 [00:33<00:39, 5934.35it/s] 48%|████▊     | 216340/450337 [00:33<00:38, 6128.92it/s] 48%|████▊     | 216957/450337 [00:33<00:38, 6108.51it/s] 48%|████▊     | 217572/450337 [00:34<00:38, 6112.71it/s] 48%|████▊     | 218280/450337 [00:34<00:36, 6395.66it/s] 49%|████▊     | 218922/450337 [00:34<00:36, 6378.83it/s] 49%|████▉     | 219574/450337 [00:34<00:35, 6413.20it/s] 49%|████▉     | 220217/450337 [00:34<00:36, 6369.47it/s] 49%|████▉     | 220855/450337 [00:34<00:36, 6227.76it/s] 49%|████▉     | 221545/450337 [00:34<00:35, 6419.93it/s] 49%|████▉     | 222189/450337 [00:34<00:36, 6171.72it/s] 49%|████▉     | 222828/450337 [00:34<00:36, 6231.40it/s] 50%|████▉     | 223454/450337 [00:35<00:39, 5816.63it/s] 50%|████▉     | 224108/450337 [00:35<00:37, 6018.20it/s] 50%|████▉     | 224833/450337 [00:35<00:35, 6367.10it/s] 50%|█████     | 225523/450337 [00:35<00:34, 6515.23it/s] 50%|█████     | 226317/450337 [00:35<00:32, 6931.71it/s] 50%|█████     | 227015/450337 [00:35<00:34, 6483.78it/s] 51%|█████     | 227672/450337 [00:35<00:34, 6381.55it/s] 51%|█████     | 228316/450337 [00:35<00:35, 6306.87it/s] 51%|█████     | 228951/450337 [00:35<00:36, 6127.55it/s] 51%|█████     | 229567/450337 [00:35<00:36, 6001.27it/s] 51%|█████     | 230179/450337 [00:36<00:36, 6026.81it/s] 51%|█████▏    | 230808/450337 [00:36<00:35, 6101.19it/s] 51%|█████▏    | 231420/450337 [00:36<00:35, 6089.95it/s] 52%|█████▏    | 232030/450337 [00:36<00:36, 6015.02it/s] 52%|█████▏    | 232740/450337 [00:36<00:34, 6328.72it/s] 52%|█████▏    | 233480/450337 [00:36<00:32, 6640.13it/s] 52%|█████▏    | 234173/450337 [00:36<00:32, 6724.44it/s] 52%|█████▏    | 234853/450337 [00:36<00:31, 6746.54it/s] 52%|█████▏    | 235550/450337 [00:36<00:31, 6801.39it/s] 52%|█████▏    | 236231/450337 [00:37<00:32, 6587.15it/s] 53%|█████▎    | 236892/450337 [00:37<00:33, 6304.55it/s] 53%|█████▎    | 237558/450337 [00:37<00:33, 6405.70it/s] 53%|█████▎    | 238202/450337 [00:37<00:34, 6194.43it/s] 53%|█████▎    | 238965/450337 [00:37<00:32, 6603.56it/s] 53%|█████▎    | 239630/450337 [00:37<00:32, 6474.22it/s] 53%|█████▎    | 240281/450337 [00:37<00:33, 6316.52it/s] 54%|█████▎    | 241152/450337 [00:37<00:29, 7000.93it/s] 54%|█████▎    | 241858/450337 [00:37<00:30, 6877.01it/s] 54%|█████▍    | 242550/450337 [00:37<00:31, 6684.85it/s] 54%|█████▍    | 243222/450337 [00:38<00:31, 6665.11it/s] 54%|█████▍    | 243891/450337 [00:38<00:31, 6494.04it/s] 54%|█████▍    | 244575/450337 [00:38<00:31, 6591.76it/s] 54%|█████▍    | 245237/450337 [00:38<00:32, 6386.83it/s] 55%|█████▍    | 245954/450337 [00:38<00:30, 6606.78it/s] 55%|█████▍    | 246759/450337 [00:38<00:28, 7022.33it/s] 55%|█████▍    | 247465/450337 [00:38<00:29, 6875.18it/s] 55%|█████▌    | 248156/450337 [00:38<00:30, 6687.15it/s] 55%|█████▌    | 248851/450337 [00:38<00:29, 6762.59it/s] 55%|█████▌    | 249608/450337 [00:39<00:28, 6992.48it/s] 56%|█████▌    | 250310/450337 [00:39<00:29, 6674.66it/s] 56%|█████▌    | 250982/450337 [00:39<00:29, 6650.98it/s] 56%|█████▌    | 251650/450337 [00:39<00:30, 6606.62it/s] 56%|█████▌    | 252313/450337 [00:39<00:30, 6581.51it/s] 56%|█████▌    | 252973/450337 [00:39<00:30, 6431.79it/s] 56%|█████▋    | 253643/450337 [00:39<00:30, 6505.16it/s] 56%|█████▋    | 254306/450337 [00:39<00:29, 6535.92it/s] 57%|█████▋    | 254961/450337 [00:39<00:30, 6435.87it/s] 57%|█████▋    | 255606/450337 [00:39<00:30, 6371.08it/s] 57%|█████▋    | 256348/450337 [00:40<00:29, 6677.20it/s] 57%|█████▋    | 257021/450337 [00:40<00:28, 6689.51it/s] 57%|█████▋    | 257742/450337 [00:40<00:28, 6835.72it/s] 57%|█████▋    | 258427/450337 [00:40<00:31, 6109.28it/s] 58%|█████▊    | 259053/450337 [00:40<00:31, 6106.42it/s] 58%|█████▊    | 259674/450337 [00:40<00:31, 6115.20it/s] 58%|█████▊    | 260357/450337 [00:40<00:30, 6317.02it/s] 58%|█████▊    | 261108/450337 [00:40<00:28, 6661.06it/s] 58%|█████▊    | 261780/450337 [00:40<00:28, 6510.01it/s] 58%|█████▊    | 262436/450337 [00:41<00:30, 6217.99it/s] 58%|█████▊    | 263063/450337 [00:41<00:30, 6152.45it/s] 59%|█████▊    | 263708/450337 [00:41<00:29, 6233.03it/s] 59%|█████▊    | 264335/450337 [00:41<00:31, 5964.84it/s] 59%|█████▉    | 265032/450337 [00:41<00:29, 6247.60it/s] 59%|█████▉    | 265662/450337 [00:41<00:30, 6150.85it/s] 59%|█████▉    | 266281/450337 [00:41<00:30, 6049.99it/s] 59%|█████▉    | 266889/450337 [00:41<00:30, 5998.12it/s] 59%|█████▉    | 267530/450337 [00:41<00:29, 6108.28it/s] 60%|█████▉    | 268143/450337 [00:41<00:29, 6100.74it/s] 60%|█████▉    | 268795/450337 [00:42<00:29, 6222.47it/s] 60%|█████▉    | 269480/450337 [00:42<00:28, 6407.47it/s] 60%|█████▉    | 270124/450337 [00:42<00:28, 6411.87it/s] 60%|██████    | 270766/450337 [00:42<00:28, 6378.26it/s] 60%|██████    | 271405/450337 [00:42<00:28, 6280.23it/s] 60%|██████    | 272125/450337 [00:42<00:27, 6543.19it/s] 61%|██████    | 272960/450337 [00:42<00:25, 7072.11it/s] 61%|██████    | 273669/450337 [00:42<00:25, 7036.93it/s] 61%|██████    | 274374/450337 [00:42<00:25, 6873.19it/s] 61%|██████    | 275063/450337 [00:43<00:27, 6399.33it/s] 61%|██████    | 275710/450337 [00:43<00:28, 6181.07it/s] 61%|██████▏   | 276334/450337 [00:43<00:28, 6005.18it/s] 62%|██████▏   | 276964/450337 [00:43<00:28, 6086.58it/s] 62%|██████▏   | 277576/450337 [00:43<00:28, 6057.08it/s] 62%|██████▏   | 278251/450337 [00:43<00:27, 6254.44it/s] 62%|██████▏   | 278969/450337 [00:43<00:26, 6523.34it/s] 62%|██████▏   | 279624/450337 [00:43<00:26, 6390.23it/s] 62%|██████▏   | 280336/450337 [00:43<00:25, 6598.80it/s] 62%|██████▏   | 280998/450337 [00:43<00:27, 6096.90it/s] 63%|██████▎   | 281681/450337 [00:44<00:26, 6298.96it/s] 63%|██████▎   | 282319/450337 [00:44<00:27, 6105.82it/s] 63%|██████▎   | 282969/450337 [00:44<00:26, 6213.20it/s] 63%|██████▎   | 283685/450337 [00:44<00:25, 6483.15it/s] 63%|██████▎   | 284338/450337 [00:44<00:25, 6449.24it/s] 63%|██████▎   | 284986/450337 [00:44<00:26, 6306.43it/s] 63%|██████▎   | 285627/450337 [00:44<00:25, 6335.23it/s] 64%|██████▎   | 286278/450337 [00:44<00:25, 6383.78it/s] 64%|██████▎   | 286918/450337 [00:44<00:25, 6324.40it/s] 64%|██████▍   | 287552/450337 [00:45<00:26, 6180.01it/s] 64%|██████▍   | 288245/450337 [00:45<00:25, 6397.61it/s] 64%|██████▍   | 288948/450337 [00:45<00:24, 6581.91it/s] 64%|██████▍   | 289608/450337 [00:45<00:25, 6182.56it/s] 64%|██████▍   | 290232/450337 [00:45<00:26, 5996.49it/s] 65%|██████▍   | 290920/450337 [00:45<00:25, 6243.57it/s] 65%|██████▍   | 291566/450337 [00:45<00:25, 6303.92it/s] 65%|██████▍   | 292200/450337 [00:45<00:25, 6170.39it/s] 65%|██████▌   | 292849/450337 [00:45<00:25, 6246.97it/s] 65%|██████▌   | 293489/450337 [00:45<00:24, 6291.31it/s] 65%|██████▌   | 294123/450337 [00:46<00:24, 6301.57it/s] 65%|██████▌   | 294796/450337 [00:46<00:24, 6421.41it/s] 66%|██████▌   | 295484/450337 [00:46<00:23, 6556.51it/s] 66%|██████▌   | 296163/450337 [00:46<00:23, 6623.75it/s] 66%|██████▌   | 296827/450337 [00:46<00:23, 6462.54it/s] 66%|██████▌   | 297475/450337 [00:46<00:23, 6380.25it/s] 66%|██████▌   | 298175/450337 [00:46<00:23, 6561.27it/s] 66%|██████▋   | 298833/450337 [00:46<00:23, 6464.95it/s] 67%|██████▋   | 299532/450337 [00:46<00:22, 6618.51it/s] 67%|██████▋   | 300195/450337 [00:46<00:23, 6526.37it/s] 67%|██████▋   | 300849/450337 [00:47<00:22, 6526.73it/s] 67%|██████▋   | 301503/450337 [00:47<00:23, 6250.25it/s] 67%|██████▋   | 302131/450337 [00:47<00:24, 6085.84it/s] 67%|██████▋   | 302843/450337 [00:47<00:23, 6380.70it/s] 67%|██████▋   | 303518/450337 [00:47<00:22, 6484.65it/s] 68%|██████▊   | 304195/450337 [00:47<00:22, 6564.59it/s] 68%|██████▊   | 304854/450337 [00:47<00:22, 6435.49it/s] 68%|██████▊   | 305500/450337 [00:47<00:23, 6293.16it/s] 68%|██████▊   | 306131/450337 [00:47<00:23, 6255.56it/s] 68%|██████▊   | 306758/450337 [00:48<00:23, 6091.64it/s] 68%|██████▊   | 307475/450337 [00:48<00:22, 6398.14it/s] 68%|██████▊   | 308117/450337 [00:48<00:23, 6074.95it/s] 69%|██████▊   | 308729/450337 [00:48<00:23, 5909.05it/s] 69%|██████▉   | 309702/450337 [00:48<00:20, 6985.89it/s] 69%|██████▉   | 310410/450337 [00:48<00:20, 6785.05it/s] 69%|██████▉   | 311096/450337 [00:48<00:21, 6551.41it/s] 69%|██████▉   | 311757/450337 [00:48<00:22, 6263.36it/s] 69%|██████▉   | 312402/450337 [00:48<00:21, 6307.10it/s] 70%|██████▉   | 313081/450337 [00:49<00:21, 6442.53it/s] 70%|██████▉   | 313801/450337 [00:49<00:20, 6656.95it/s] 70%|██████▉   | 314505/450337 [00:49<00:20, 6762.43it/s] 70%|██████▉   | 315184/450337 [00:49<00:21, 6227.05it/s] 70%|███████   | 315844/450337 [00:49<00:21, 6327.48it/s] 70%|███████   | 316485/450337 [00:49<00:22, 6072.19it/s] 70%|███████   | 317143/450337 [00:49<00:21, 6214.10it/s] 71%|███████   | 317770/450337 [00:49<00:22, 5897.75it/s] 71%|███████   | 318436/450337 [00:49<00:21, 6104.77it/s] 71%|███████   | 319093/450337 [00:49<00:21, 6226.87it/s] 71%|███████   | 319735/450337 [00:50<00:20, 6277.11it/s] 71%|███████   | 320367/450337 [00:50<00:20, 6218.34it/s] 71%|███████▏  | 321013/450337 [00:50<00:20, 6283.51it/s] 71%|███████▏  | 321688/450337 [00:50<00:20, 6417.87it/s] 72%|███████▏  | 322332/450337 [00:50<00:20, 6127.51it/s] 72%|███████▏  | 322949/450337 [00:50<00:20, 6084.98it/s] 72%|███████▏  | 323632/450337 [00:50<00:20, 6300.06it/s] 72%|███████▏  | 324282/450337 [00:50<00:19, 6348.82it/s] 72%|███████▏  | 324941/450337 [00:50<00:19, 6415.31it/s] 72%|███████▏  | 325584/450337 [00:51<00:20, 5961.76it/s] 72%|███████▏  | 326241/450337 [00:51<00:20, 6131.57it/s] 73%|███████▎  | 326977/450337 [00:51<00:19, 6475.14it/s] 73%|███████▎  | 327696/450337 [00:51<00:18, 6675.98it/s] 73%|███████▎  | 328492/450337 [00:51<00:17, 7051.73it/s] 73%|███████▎  | 329202/450337 [00:51<00:17, 6757.56it/s] 73%|███████▎  | 329883/450337 [00:51<00:18, 6564.67it/s] 73%|███████▎  | 330544/450337 [00:51<00:19, 6074.95it/s] 74%|███████▎  | 331231/450337 [00:51<00:18, 6284.85it/s] 74%|███████▎  | 331868/450337 [00:51<00:19, 6165.35it/s] 74%|███████▍  | 332566/450337 [00:52<00:18, 6386.15it/s] 74%|███████▍  | 333210/450337 [00:52<00:18, 6235.70it/s] 74%|███████▍  | 333896/450337 [00:52<00:18, 6413.34it/s] 74%|███████▍  | 334542/450337 [00:52<00:18, 6327.53it/s] 74%|███████▍  | 335201/450337 [00:52<00:17, 6399.61it/s] 75%|███████▍  | 335861/450337 [00:52<00:17, 6454.84it/s] 75%|███████▍  | 336523/450337 [00:52<00:17, 6503.17it/s] 75%|███████▍  | 337199/450337 [00:52<00:17, 6576.64it/s] 75%|███████▌  | 337904/450337 [00:52<00:16, 6716.10it/s] 75%|███████▌  | 338577/450337 [00:53<00:16, 6639.71it/s] 75%|███████▌  | 339242/450337 [00:53<00:16, 6609.91it/s] 75%|███████▌  | 339904/450337 [00:53<00:17, 6307.92it/s] 76%|███████▌  | 340538/450337 [00:53<00:18, 6082.71it/s] 76%|███████▌  | 341224/450337 [00:53<00:17, 6297.88it/s] 76%|███████▌  | 341858/450337 [00:53<00:17, 6206.67it/s] 76%|███████▌  | 342577/450337 [00:53<00:16, 6488.07it/s] 76%|███████▌  | 343229/450337 [00:53<00:16, 6396.77it/s] 76%|███████▋  | 343973/450337 [00:53<00:15, 6688.01it/s] 77%|███████▋  | 344645/450337 [00:53<00:16, 6327.60it/s] 77%|███████▋  | 345347/450337 [00:54<00:16, 6520.09it/s] 77%|███████▋  | 346004/450337 [00:54<00:17, 6001.04it/s] 77%|███████▋  | 346698/450337 [00:54<00:16, 6254.54it/s] 77%|███████▋  | 347345/450337 [00:54<00:16, 6307.76it/s] 77%|███████▋  | 347990/450337 [00:54<00:16, 6340.10it/s] 77%|███████▋  | 348630/450337 [00:54<00:16, 6298.52it/s] 78%|███████▊  | 349264/450337 [00:54<00:16, 6282.49it/s] 78%|███████▊  | 349961/450337 [00:54<00:15, 6473.31it/s] 78%|███████▊  | 350739/450337 [00:54<00:14, 6857.44it/s] 78%|███████▊  | 351427/450337 [00:55<00:14, 6678.78it/s] 78%|███████▊  | 352098/450337 [00:55<00:15, 6370.75it/s] 78%|███████▊  | 352740/450337 [00:55<00:16, 5926.18it/s] 78%|███████▊  | 353427/450337 [00:55<00:15, 6179.96it/s] 79%|███████▊  | 354053/450337 [00:55<00:16, 5948.37it/s] 79%|███████▉  | 354821/450337 [00:55<00:14, 6426.66it/s] 79%|███████▉  | 355472/450337 [00:55<00:15, 6132.91it/s] 79%|███████▉  | 356142/450337 [00:55<00:14, 6283.38it/s] 79%|███████▉  | 356777/450337 [00:55<00:14, 6249.67it/s] 79%|███████▉  | 357407/450337 [00:56<00:15, 5874.66it/s] 80%|███████▉  | 358089/450337 [00:56<00:15, 6136.91it/s] 80%|███████▉  | 358720/450337 [00:56<00:14, 6184.51it/s] 80%|███████▉  | 359378/450337 [00:56<00:14, 6294.20it/s] 80%|███████▉  | 360012/450337 [00:56<00:14, 6256.08it/s] 80%|████████  | 360737/450337 [00:56<00:13, 6544.43it/s] 80%|████████  | 361395/450337 [00:56<00:13, 6440.56it/s] 80%|████████  | 362203/450337 [00:56<00:12, 6915.85it/s] 81%|████████  | 362898/450337 [00:56<00:13, 6709.49it/s] 81%|████████  | 363572/450337 [00:56<00:12, 6689.45it/s] 81%|████████  | 364272/450337 [00:57<00:12, 6779.86it/s] 81%|████████  | 364952/450337 [00:57<00:12, 6784.57it/s] 81%|████████  | 365651/450337 [00:57<00:12, 6844.76it/s] 81%|████████▏ | 366337/450337 [00:57<00:12, 6681.63it/s] 81%|████████▏ | 367007/450337 [00:57<00:12, 6512.12it/s] 82%|████████▏ | 367660/450337 [00:57<00:12, 6456.71it/s] 82%|████████▏ | 368307/450337 [00:57<00:13, 6091.70it/s] 82%|████████▏ | 369016/450337 [00:57<00:12, 6367.38it/s] 82%|████████▏ | 369673/450337 [00:57<00:12, 6418.04it/s] 82%|████████▏ | 370319/450337 [00:57<00:12, 6388.20it/s] 82%|████████▏ | 370961/450337 [00:58<00:12, 6301.30it/s] 83%|████████▎ | 371593/450337 [00:58<00:12, 6280.01it/s] 83%|████████▎ | 372223/450337 [00:58<00:12, 6071.01it/s] 83%|████████▎ | 372896/450337 [00:58<00:12, 6255.08it/s] 83%|████████▎ | 373544/450337 [00:58<00:12, 6315.54it/s] 83%|████████▎ | 374229/450337 [00:58<00:11, 6470.86it/s] 83%|████████▎ | 374916/450337 [00:58<00:11, 6584.70it/s] 83%|████████▎ | 375576/450337 [00:58<00:12, 6060.47it/s] 84%|████████▎ | 376211/450337 [00:58<00:12, 6139.50it/s] 84%|████████▎ | 376832/450337 [00:59<00:12, 5875.95it/s] 84%|████████▍ | 377447/450337 [00:59<00:12, 5949.66it/s] 84%|████████▍ | 378159/450337 [00:59<00:11, 6275.86it/s] 84%|████████▍ | 378812/450337 [00:59<00:11, 6345.68it/s] 84%|████████▍ | 379496/450337 [00:59<00:10, 6489.87it/s] 84%|████████▍ | 380234/450337 [00:59<00:10, 6747.73it/s] 85%|████████▍ | 380944/450337 [00:59<00:10, 6849.95it/s] 85%|████████▍ | 381631/450337 [00:59<00:10, 6663.75it/s] 85%|████████▍ | 382300/450337 [00:59<00:10, 6250.82it/s] 85%|████████▌ | 382932/450337 [00:59<00:11, 6091.04it/s] 85%|████████▌ | 383558/450337 [01:00<00:10, 6135.82it/s] 85%|████████▌ | 384266/450337 [01:00<00:10, 6403.62it/s] 85%|████████▌ | 384956/450337 [01:00<00:09, 6547.42it/s] 86%|████████▌ | 385614/450337 [01:00<00:10, 6457.76it/s] 86%|████████▌ | 386263/450337 [01:00<00:10, 6163.94it/s] 86%|████████▌ | 386975/450337 [01:00<00:09, 6428.38it/s] 86%|████████▌ | 387788/450337 [01:00<00:09, 6919.31it/s] 86%|████████▋ | 388485/450337 [01:00<00:09, 6502.61it/s] 86%|████████▋ | 389144/450337 [01:00<00:09, 6480.44it/s] 87%|████████▋ | 389849/450337 [01:01<00:09, 6638.18it/s] 87%|████████▋ | 390518/450337 [01:01<00:09, 6564.14it/s] 87%|████████▋ | 391178/450337 [01:01<00:09, 6523.89it/s] 87%|████████▋ | 391833/450337 [01:01<00:09, 6108.46it/s] 87%|████████▋ | 392504/450337 [01:01<00:09, 6272.69it/s] 87%|████████▋ | 393137/450337 [01:01<00:09, 6176.62it/s] 87%|████████▋ | 393759/450337 [01:01<00:09, 6069.35it/s] 88%|████████▊ | 394432/450337 [01:01<00:08, 6257.91it/s] 88%|████████▊ | 395061/450337 [01:01<00:08, 6150.50it/s] 88%|████████▊ | 395752/450337 [01:01<00:08, 6366.36it/s] 88%|████████▊ | 396391/450337 [01:02<00:08, 6284.97it/s] 88%|████████▊ | 397022/450337 [01:02<00:08, 6118.81it/s] 88%|████████▊ | 397636/450337 [01:02<00:08, 6031.00it/s] 88%|████████▊ | 398343/450337 [01:02<00:08, 6329.66it/s] 89%|████████▊ | 399072/450337 [01:02<00:07, 6609.87it/s] 89%|████████▉ | 399820/450337 [01:02<00:07, 6864.97it/s] 89%|████████▉ | 400509/450337 [01:02<00:07, 6675.10it/s] 89%|████████▉ | 401179/450337 [01:02<00:07, 6485.66it/s] 89%|████████▉ | 401830/450337 [01:02<00:07, 6209.39it/s] 89%|████████▉ | 402455/450337 [01:03<00:08, 5802.34it/s] 90%|████████▉ | 403075/450337 [01:03<00:07, 5910.62it/s] 90%|████████▉ | 403759/450337 [01:03<00:07, 6164.10it/s] 90%|████████▉ | 404381/450337 [01:03<00:07, 6025.03it/s] 90%|████████▉ | 405043/450337 [01:03<00:07, 6190.78it/s] 90%|█████████ | 405666/450337 [01:03<00:07, 6069.35it/s] 90%|█████████ | 406316/450337 [01:03<00:07, 6191.31it/s] 90%|█████████ | 406938/450337 [01:03<00:07, 6084.64it/s] 90%|█████████ | 407549/450337 [01:03<00:07, 5999.19it/s] 91%|█████████ | 408160/450337 [01:04<00:06, 6028.97it/s] 91%|█████████ | 408877/450337 [01:04<00:06, 6359.31it/s] 91%|█████████ | 409515/450337 [01:04<00:06, 6336.75it/s] 91%|█████████ | 410150/450337 [01:04<00:06, 6094.66it/s] 91%|█████████ | 410833/450337 [01:04<00:06, 6303.93it/s] 91%|█████████▏| 411466/450337 [01:04<00:06, 6163.84it/s] 92%|█████████▏| 412085/450337 [01:04<00:06, 6115.05it/s] 92%|█████████▏| 412698/450337 [01:04<00:06, 5595.06it/s] 92%|█████████▏| 413292/450337 [01:04<00:06, 5684.84it/s] 92%|█████████▏| 413883/450337 [01:04<00:06, 5746.87it/s] 92%|█████████▏| 414463/450337 [01:05<00:06, 5598.67it/s] 92%|█████████▏| 415049/450337 [01:05<00:06, 5671.87it/s] 92%|█████████▏| 415697/450337 [01:05<00:05, 5901.44it/s] 92%|█████████▏| 416291/450337 [01:05<00:06, 5589.38it/s] 93%|█████████▎| 416950/450337 [01:05<00:05, 5870.86it/s] 93%|█████████▎| 417569/450337 [01:05<00:05, 5960.53it/s] 93%|█████████▎| 418261/450337 [01:05<00:05, 6236.18it/s] 93%|█████████▎| 418889/450337 [01:05<00:05, 6195.92it/s] 93%|█████████▎| 419528/450337 [01:05<00:04, 6248.53it/s] 93%|█████████▎| 420165/450337 [01:06<00:04, 6283.42it/s] 93%|█████████▎| 420808/450337 [01:06<00:04, 6324.60it/s] 94%|█████████▎| 421442/450337 [01:06<00:04, 5942.55it/s] 94%|█████████▎| 422063/450337 [01:06<00:04, 6018.79it/s] 94%|█████████▍| 422795/450337 [01:06<00:04, 6393.41it/s] 94%|█████████▍| 423439/450337 [01:06<00:04, 6283.85it/s] 94%|█████████▍| 424071/450337 [01:06<00:04, 6208.58it/s] 94%|█████████▍| 424695/450337 [01:06<00:04, 6133.98it/s] 94%|█████████▍| 425310/450337 [01:06<00:04, 6083.96it/s] 95%|█████████▍| 426003/450337 [01:06<00:03, 6328.00it/s] 95%|█████████▍| 426782/450337 [01:07<00:03, 6756.17it/s] 95%|█████████▍| 427460/450337 [01:07<00:03, 6545.70it/s] 95%|█████████▌| 428118/450337 [01:07<00:03, 6352.30it/s] 95%|█████████▌| 428851/450337 [01:07<00:03, 6624.17it/s] 95%|█████████▌| 429538/450337 [01:07<00:03, 6690.74it/s] 96%|█████████▌| 430210/450337 [01:07<00:03, 6685.63it/s] 96%|█████████▌| 430881/450337 [01:07<00:03, 6348.82it/s] 96%|█████████▌| 431521/450337 [01:07<00:03, 6098.31it/s] 96%|█████████▌| 432136/450337 [01:07<00:02, 6079.71it/s] 96%|█████████▌| 433064/450337 [01:08<00:02, 6991.52it/s] 96%|█████████▋| 433770/450337 [01:08<00:02, 6640.97it/s] 96%|█████████▋| 434442/450337 [01:08<00:02, 6320.01it/s] 97%|█████████▋| 435082/450337 [01:08<00:02, 6174.36it/s] 97%|█████████▋| 435802/450337 [01:08<00:02, 6454.99it/s] 97%|█████████▋| 436454/450337 [01:08<00:02, 6201.91it/s] 97%|█████████▋| 437089/450337 [01:08<00:02, 6243.22it/s] 97%|█████████▋| 437764/450337 [01:08<00:01, 6385.82it/s] 97%|█████████▋| 438406/450337 [01:08<00:01, 6241.68it/s] 97%|█████████▋| 439033/450337 [01:08<00:01, 6130.30it/s] 98%|█████████▊| 439648/450337 [01:09<00:01, 5973.23it/s] 98%|█████████▊| 440247/450337 [01:09<00:01, 5929.79it/s] 98%|█████████▊| 440907/450337 [01:09<00:01, 6120.97it/s] 98%|█████████▊| 441549/450337 [01:09<00:01, 6206.61it/s] 98%|█████████▊| 442233/450337 [01:09<00:01, 6385.42it/s] 98%|█████████▊| 442873/450337 [01:09<00:01, 6270.47it/s] 98%|█████████▊| 443572/450337 [01:09<00:01, 6480.45it/s] 99%|█████████▊| 444240/450337 [01:09<00:00, 6532.23it/s] 99%|█████████▉| 444895/450337 [01:09<00:00, 5960.01it/s] 99%|█████████▉| 445530/450337 [01:10<00:00, 6064.92it/s] 99%|█████████▉| 446145/450337 [01:10<00:00, 6040.70it/s] 99%|█████████▉| 446755/450337 [01:10<00:00, 6033.37it/s] 99%|█████████▉| 447451/450337 [01:10<00:00, 6294.25it/s] 99%|█████████▉| 448084/450337 [01:10<00:00, 6251.14it/s]100%|█████████▉| 448791/450337 [01:10<00:00, 6490.31it/s]100%|█████████▉| 449443/450337 [01:10<00:00, 6256.50it/s]100%|█████████▉| 450073/450337 [01:10<00:00, 6268.71it/s]100%|██████████| 450337/450337 [01:10<00:00, 6361.80it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1911/450337 [00:00<00:23, 19097.63it/s]  1%|          | 3978/450337 [00:00<00:22, 20014.94it/s]  1%|▏         | 6229/450337 [00:00<00:21, 21146.56it/s]  2%|▏         | 8344/450337 [00:00<00:21, 20307.53it/s]  2%|▏         | 10380/450337 [00:00<00:21, 20214.08it/s]  3%|▎         | 12405/450337 [00:00<00:21, 20109.65it/s]  3%|▎         | 14463/450337 [00:00<00:21, 20260.41it/s]  4%|▎         | 16491/450337 [00:00<00:21, 20131.80it/s]  4%|▍         | 18506/450337 [00:00<00:21, 20131.96it/s]  5%|▍         | 20686/450337 [00:01<00:20, 20639.16it/s]  5%|▌         | 22751/450337 [00:01<00:20, 20514.35it/s]  6%|▌         | 25037/450337 [00:01<00:20, 21221.08it/s]  6%|▌         | 27161/450337 [00:01<00:20, 20417.14it/s]  6%|▋         | 29264/450337 [00:01<00:20, 20592.61it/s]  7%|▋         | 31329/450337 [00:01<00:20, 20058.40it/s]  7%|▋         | 33341/450337 [00:01<00:21, 19738.58it/s]  8%|▊         | 35418/450337 [00:01<00:20, 20037.14it/s]  8%|▊         | 37426/450337 [00:01<00:20, 19664.77it/s]  9%|▊         | 39397/450337 [00:01<00:20, 19676.16it/s]  9%|▉         | 41500/450337 [00:02<00:20, 20072.93it/s] 10%|▉         | 43510/450337 [00:02<00:20, 19603.49it/s] 10%|█         | 45611/450337 [00:02<00:20, 20012.25it/s] 11%|█         | 48044/450337 [00:02<00:18, 21281.98it/s] 11%|█         | 50192/450337 [00:02<00:18, 21320.91it/s] 12%|█▏        | 52433/450337 [00:02<00:18, 21643.68it/s] 12%|█▏        | 54601/450337 [00:02<00:18, 20857.44it/s] 13%|█▎        | 56701/450337 [00:02<00:18, 20888.49it/s] 13%|█▎        | 58796/450337 [00:02<00:18, 20810.27it/s] 14%|█▎        | 60881/450337 [00:02<00:18, 20688.79it/s] 14%|█▍        | 62953/450337 [00:03<00:18, 20673.54it/s] 14%|█▍        | 65023/450337 [00:03<00:18, 20515.21it/s] 15%|█▍        | 67084/450337 [00:03<00:18, 20538.42it/s] 15%|█▌        | 69139/450337 [00:03<00:18, 20540.48it/s] 16%|█▌        | 71246/450337 [00:03<00:18, 20696.63it/s] 16%|█▋        | 73456/450337 [00:03<00:17, 21111.25it/s] 17%|█▋        | 75625/450337 [00:03<00:17, 21277.79it/s] 17%|█▋        | 77754/450337 [00:03<00:17, 21274.97it/s] 18%|█▊        | 79882/450337 [00:03<00:18, 20321.71it/s] 18%|█▊        | 81959/450337 [00:03<00:18, 20451.52it/s] 19%|█▊        | 84097/450337 [00:04<00:17, 20722.92it/s] 19%|█▉        | 86175/450337 [00:04<00:17, 20613.49it/s] 20%|█▉        | 88372/450337 [00:04<00:17, 21007.69it/s] 20%|██        | 90477/450337 [00:04<00:17, 20273.74it/s] 21%|██        | 92622/450337 [00:04<00:17, 20608.97it/s] 21%|██        | 94690/450337 [00:04<00:17, 20586.28it/s] 22%|██▏       | 96915/450337 [00:04<00:16, 21073.01it/s] 22%|██▏       | 99027/450337 [00:04<00:16, 20717.45it/s] 22%|██▏       | 101103/450337 [00:04<00:17, 20337.73it/s] 23%|██▎       | 103171/450337 [00:05<00:16, 20433.38it/s] 23%|██▎       | 105218/450337 [00:05<00:17, 20154.75it/s] 24%|██▍       | 107236/450337 [00:05<00:17, 20066.11it/s] 24%|██▍       | 109245/450337 [00:05<00:17, 19858.02it/s] 25%|██▍       | 111398/450337 [00:05<00:16, 20341.76it/s] 25%|██▌       | 113434/450337 [00:05<00:16, 20301.99it/s] 26%|██▌       | 115466/450337 [00:05<00:16, 20097.32it/s] 26%|██▌       | 117477/450337 [00:05<00:16, 19690.17it/s] 27%|██▋       | 119505/450337 [00:05<00:16, 19860.03it/s] 27%|██▋       | 121592/450337 [00:05<00:16, 20154.28it/s] 27%|██▋       | 123610/450337 [00:06<00:16, 20087.57it/s] 28%|██▊       | 125655/450337 [00:06<00:16, 20189.81it/s] 28%|██▊       | 127738/450337 [00:06<00:15, 20379.08it/s] 29%|██▉       | 129777/450337 [00:06<00:15, 20340.81it/s] 29%|██▉       | 131812/450337 [00:06<00:16, 19698.29it/s] 30%|██▉       | 134006/450337 [00:06<00:15, 20347.92it/s] 30%|███       | 136261/450337 [00:06<00:14, 20995.79it/s] 31%|███       | 138486/450337 [00:06<00:14, 21361.37it/s] 31%|███       | 140626/450337 [00:06<00:14, 21023.19it/s] 32%|███▏      | 142732/450337 [00:06<00:15, 20506.34it/s] 32%|███▏      | 144951/450337 [00:07<00:14, 20989.91it/s] 33%|███▎      | 147055/450337 [00:07<00:14, 20697.01it/s] 33%|███▎      | 149129/450337 [00:07<00:14, 20369.67it/s] 34%|███▎      | 151169/450337 [00:07<00:15, 19844.13it/s] 34%|███▍      | 153327/450337 [00:07<00:14, 20339.72it/s] 34%|███▍      | 155366/450337 [00:07<00:14, 20069.87it/s] 35%|███▍      | 157527/450337 [00:07<00:14, 20511.25it/s] 35%|███▌      | 159582/450337 [00:07<00:14, 20367.23it/s] 36%|███▌      | 161686/450337 [00:07<00:14, 20563.36it/s] 36%|███▋      | 163749/450337 [00:08<00:13, 20582.81it/s] 37%|███▋      | 165948/450337 [00:08<00:13, 20999.34it/s] 37%|███▋      | 168050/450337 [00:08<00:13, 20696.40it/s] 38%|███▊      | 170122/450337 [00:08<00:13, 20474.40it/s] 38%|███▊      | 172171/450337 [00:08<00:13, 20219.41it/s] 39%|███▊      | 174195/450337 [00:08<00:13, 20223.48it/s] 39%|███▉      | 176219/450337 [00:08<00:13, 19863.16it/s] 40%|███▉      | 178207/450337 [00:08<00:14, 19241.41it/s] 40%|████      | 180174/450337 [00:08<00:13, 19361.50it/s] 40%|████      | 182130/450337 [00:08<00:13, 19416.53it/s] 41%|████      | 184075/450337 [00:09<00:13, 19072.52it/s] 41%|████▏     | 186008/450337 [00:09<00:13, 19145.17it/s] 42%|████▏     | 188019/450337 [00:09<00:13, 19424.90it/s] 42%|████▏     | 190136/450337 [00:09<00:13, 19941.16it/s] 43%|████▎     | 192242/450337 [00:09<00:12, 20265.55it/s] 43%|████▎     | 194293/450337 [00:09<00:12, 20335.11it/s] 44%|████▎     | 196341/450337 [00:09<00:12, 20376.95it/s] 44%|████▍     | 198564/450337 [00:09<00:12, 20928.79it/s] 45%|████▍     | 200658/450337 [00:09<00:12, 20304.70it/s] 45%|████▌     | 202693/450337 [00:09<00:12, 20294.34it/s] 45%|████▌     | 204764/450337 [00:10<00:12, 20408.62it/s] 46%|████▌     | 206808/450337 [00:10<00:12, 20228.18it/s] 46%|████▋     | 208833/450337 [00:10<00:12, 19738.23it/s] 47%|████▋     | 210940/450337 [00:10<00:11, 20123.68it/s] 47%|████▋     | 213082/450337 [00:10<00:11, 20497.64it/s] 48%|████▊     | 215135/450337 [00:10<00:11, 19770.08it/s] 48%|████▊     | 217119/450337 [00:10<00:11, 19493.38it/s] 49%|████▊     | 219280/450337 [00:10<00:11, 20103.46it/s] 49%|████▉     | 221296/450337 [00:10<00:11, 20080.35it/s] 50%|████▉     | 223308/450337 [00:10<00:11, 19475.41it/s] 50%|█████     | 225438/450337 [00:11<00:11, 20000.12it/s] 51%|█████     | 227523/450337 [00:11<00:11, 20247.80it/s] 51%|█████     | 229553/450337 [00:11<00:11, 19756.90it/s] 51%|█████▏    | 231534/450337 [00:11<00:11, 19705.08it/s] 52%|█████▏    | 233725/450337 [00:11<00:10, 20350.70it/s] 52%|█████▏    | 235911/450337 [00:11<00:10, 20795.82it/s] 53%|█████▎    | 237995/450337 [00:11<00:10, 19995.45it/s] 53%|█████▎    | 240095/450337 [00:11<00:10, 20283.80it/s] 54%|█████▍    | 242384/450337 [00:11<00:09, 21041.58it/s] 54%|█████▍    | 244495/450337 [00:12<00:09, 20839.28it/s] 55%|█████▍    | 246713/450337 [00:12<00:09, 21232.71it/s] 55%|█████▌    | 248841/450337 [00:12<00:09, 21082.45it/s] 56%|█████▌    | 250993/450337 [00:12<00:09, 21208.30it/s] 56%|█████▌    | 253117/450337 [00:12<00:09, 20907.26it/s] 57%|█████▋    | 255211/450337 [00:12<00:09, 20824.29it/s] 57%|█████▋    | 257367/450337 [00:12<00:09, 21039.15it/s] 58%|█████▊    | 259473/450337 [00:12<00:09, 20299.89it/s] 58%|█████▊    | 261676/450337 [00:12<00:09, 20801.25it/s] 59%|█████▊    | 263762/450337 [00:12<00:09, 20311.08it/s] 59%|█████▉    | 265799/450337 [00:13<00:09, 19991.13it/s] 59%|█████▉    | 267803/450337 [00:13<00:09, 19737.30it/s] 60%|█████▉    | 269871/450337 [00:13<00:09, 20002.10it/s] 60%|██████    | 271946/450337 [00:13<00:08, 20214.97it/s] 61%|██████    | 274211/450337 [00:13<00:08, 20917.08it/s] 61%|██████▏   | 276306/450337 [00:13<00:08, 20138.77it/s] 62%|██████▏   | 278328/450337 [00:13<00:08, 20154.22it/s] 62%|██████▏   | 280450/450337 [00:13<00:08, 20461.73it/s] 63%|██████▎   | 282501/450337 [00:13<00:08, 19899.91it/s] 63%|██████▎   | 284597/450337 [00:13<00:08, 20204.72it/s] 64%|██████▎   | 286623/450337 [00:14<00:08, 20119.25it/s] 64%|██████▍   | 288732/450337 [00:14<00:07, 20400.29it/s] 65%|██████▍   | 290775/450337 [00:14<00:07, 20174.15it/s] 65%|██████▌   | 292795/450337 [00:14<00:07, 20111.80it/s] 65%|██████▌   | 294808/450337 [00:14<00:07, 19980.79it/s] 66%|██████▌   | 296808/450337 [00:14<00:07, 19888.68it/s] 66%|██████▋   | 298798/450337 [00:14<00:07, 19884.87it/s] 67%|██████▋   | 300854/450337 [00:14<00:07, 20075.88it/s] 67%|██████▋   | 302863/450337 [00:14<00:07, 19874.86it/s] 68%|██████▊   | 304940/450337 [00:14<00:07, 20134.80it/s] 68%|██████▊   | 306955/450337 [00:15<00:07, 19756.25it/s] 69%|██████▊   | 308933/450337 [00:15<00:07, 19708.55it/s] 69%|██████▉   | 311145/450337 [00:15<00:06, 20420.28it/s] 70%|██████▉   | 313246/450337 [00:15<00:06, 20594.96it/s] 70%|███████   | 315308/450337 [00:15<00:06, 20460.77it/s] 70%|███████   | 317356/450337 [00:15<00:06, 20223.89it/s] 71%|███████   | 319380/450337 [00:15<00:06, 19964.29it/s] 71%|███████▏  | 321430/450337 [00:15<00:06, 20120.97it/s] 72%|███████▏  | 323444/450337 [00:15<00:06, 19718.63it/s] 72%|███████▏  | 325419/450337 [00:16<00:06, 19518.09it/s] 73%|███████▎  | 327637/450337 [00:16<00:06, 20296.69it/s] 73%|███████▎  | 329740/450337 [00:16<00:05, 20511.79it/s] 74%|███████▎  | 331794/450337 [00:16<00:05, 20075.67it/s] 74%|███████▍  | 333807/450337 [00:16<00:05, 20091.33it/s] 75%|███████▍  | 335853/450337 [00:16<00:05, 20199.22it/s] 75%|███████▌  | 337979/450337 [00:16<00:05, 20505.08it/s] 76%|███████▌  | 340032/450337 [00:16<00:05, 19976.77it/s] 76%|███████▌  | 342112/450337 [00:16<00:05, 20214.10it/s] 76%|███████▋  | 344222/450337 [00:16<00:05, 20470.42it/s] 77%|███████▋  | 346272/450337 [00:17<00:05, 19950.13it/s] 77%|███████▋  | 348282/450337 [00:17<00:05, 19991.42it/s] 78%|███████▊  | 350533/450337 [00:17<00:04, 20731.88it/s] 78%|███████▊  | 352610/450337 [00:17<00:05, 19525.78it/s] 79%|███████▊  | 354615/450337 [00:17<00:04, 19672.45it/s] 79%|███████▉  | 356595/450337 [00:17<00:04, 19353.59it/s] 80%|███████▉  | 358539/450337 [00:17<00:04, 18999.55it/s] 80%|████████  | 360521/450337 [00:17<00:04, 19235.15it/s] 81%|████████  | 362651/450337 [00:17<00:04, 19836.34it/s] 81%|████████  | 364683/450337 [00:17<00:04, 19977.86it/s] 81%|████████▏ | 366693/450337 [00:18<00:04, 20007.15it/s] 82%|████████▏ | 368697/450337 [00:18<00:04, 19640.81it/s] 82%|████████▏ | 370665/450337 [00:18<00:04, 19565.20it/s] 83%|████████▎ | 372624/450337 [00:18<00:04, 19372.68it/s] 83%|████████▎ | 374649/450337 [00:18<00:03, 19628.04it/s] 84%|████████▎ | 376614/450337 [00:18<00:03, 19061.92it/s] 84%|████████▍ | 378536/450337 [00:18<00:03, 19106.94it/s] 85%|████████▍ | 380617/450337 [00:18<00:03, 19603.48it/s] 85%|████████▍ | 382581/450337 [00:18<00:03, 19337.44it/s] 85%|████████▌ | 384566/450337 [00:19<00:03, 19487.62it/s] 86%|████████▌ | 386517/450337 [00:19<00:03, 19101.78it/s] 86%|████████▋ | 388605/450337 [00:19<00:03, 19621.51it/s] 87%|████████▋ | 390584/450337 [00:19<00:03, 19670.56it/s] 87%|████████▋ | 392554/450337 [00:19<00:03, 19116.01it/s] 88%|████████▊ | 394471/450337 [00:19<00:02, 19007.15it/s] 88%|████████▊ | 396421/450337 [00:19<00:02, 19143.60it/s] 88%|████████▊ | 398406/450337 [00:19<00:02, 19347.56it/s] 89%|████████▉ | 400589/450337 [00:19<00:02, 20079.42it/s] 89%|████████▉ | 402600/450337 [00:19<00:02, 19294.77it/s] 90%|████████▉ | 404651/450337 [00:20<00:02, 19646.22it/s] 90%|█████████ | 406623/450337 [00:20<00:02, 19271.16it/s] 91%|█████████ | 408653/450337 [00:20<00:02, 19569.15it/s] 91%|█████████ | 410656/450337 [00:20<00:02, 19699.60it/s] 92%|█████████▏| 412630/450337 [00:20<00:01, 19219.54it/s] 92%|█████████▏| 414557/450337 [00:20<00:01, 18898.18it/s] 92%|█████████▏| 416451/450337 [00:20<00:01, 18875.46it/s] 93%|█████████▎| 418422/450337 [00:20<00:01, 19117.56it/s] 93%|█████████▎| 420448/450337 [00:20<00:01, 19448.68it/s] 94%|█████████▍| 422396/450337 [00:20<00:01, 19158.98it/s] 94%|█████████▍| 424409/450337 [00:21<00:01, 19444.51it/s] 95%|█████████▍| 426506/450337 [00:21<00:01, 19892.43it/s] 95%|█████████▌| 428670/450337 [00:21<00:01, 20410.98it/s] 96%|█████████▌| 430714/450337 [00:21<00:00, 20293.62it/s] 96%|█████████▌| 432786/450337 [00:21<00:00, 20419.62it/s] 97%|█████████▋| 434830/450337 [00:21<00:00, 19894.29it/s] 97%|█████████▋| 436846/450337 [00:21<00:00, 19968.85it/s] 97%|█████████▋| 438846/450337 [00:21<00:00, 19873.66it/s] 98%|█████████▊| 440836/450337 [00:21<00:00, 19596.60it/s] 98%|█████████▊| 442855/450337 [00:21<00:00, 19766.89it/s] 99%|█████████▉| 444834/450337 [00:22<00:00, 19673.58it/s] 99%|█████████▉| 446803/450337 [00:22<00:00, 19536.52it/s]100%|█████████▉| 448915/450337 [00:22<00:00, 20001.88it/s]100%|██████████| 450337/450337 [00:22<00:00, 20132.63it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 19.54it/s]2022-03-03 10:50:38 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 10:50:38 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 10:50:38 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 10:50:38 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 10:50:38 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 10:50:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 10:50:38 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 10:50:39 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 10:50:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:50:39 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 10:50:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:50:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 10:50:39 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 10:50:39 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_last.pt
2022-03-03 10:50:39 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_last.pt
2022-03-03 10:50:39 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 10:50:39 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 10:50:39 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 10:50:39 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 10:50:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 10:50:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 10:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 10:51:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 10:51:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 10:58:41 | INFO | train_inner | epoch 001:    105 / 393 loss=17.053, ppl=135955, wps=14689.7, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.502, loss_scale=4, train_wall=477, gb_free=10.1, wall=482
2022-03-03 11:06:07 | INFO | train_inner | epoch 001:    205 / 393 loss=14.583, ppl=24538.4, wps=14688.6, ups=0.22, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.597, loss_scale=4, train_wall=441, gb_free=10.1, wall=929
2022-03-03 11:13:33 | INFO | train_inner | epoch 001:    305 / 393 loss=12.505, ppl=5812.85, wps=14694.7, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.087, loss_scale=4, train_wall=441, gb_free=10.1, wall=1375
2022-03-03 11:20:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:20:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.688 | ppl 1650.1 | wps 34109.8 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 11:20:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 11:20:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 11:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 11:20:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.688) (writing took 4.434308798983693 seconds)
2022-03-03 11:20:14 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 11:20:14 | INFO | train | epoch 001 | loss 13.894 | ppl 15224.7 | wps 14601.5 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.732 | loss_scale 4 | train_wall 1745 | gb_free 10.1 | wall 1776
2022-03-03 11:20:14 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 11:20:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:21:08 | INFO | train_inner | epoch 002:     12 / 393 loss=11.038, ppl=2103.12, wps=14346.4, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.595, loss_scale=4, train_wall=439, gb_free=10.1, wall=1829
2022-03-03 11:28:34 | INFO | train_inner | epoch 002:    112 / 393 loss=10.467, ppl=1415.61, wps=14705.5, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.451, loss_scale=4, train_wall=441, gb_free=10.1, wall=2275
2022-03-03 11:35:59 | INFO | train_inner | epoch 002:    212 / 393 loss=10.184, ppl=1163.23, wps=14699.4, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.522, loss_scale=8, train_wall=441, gb_free=10.1, wall=2721
2022-03-03 11:43:25 | INFO | train_inner | epoch 002:    312 / 393 loss=9.958, ppl=994.31, wps=14702, ups=0.22, wpb=65535.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.548, loss_scale=8, train_wall=441, gb_free=10.1, wall=3167
2022-03-03 11:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:49:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.641 | ppl 798.23 | wps 34052.8 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.641
2022-03-03 11:49:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 11:49:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 11:49:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 11:49:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.641) (writing took 4.77233593352139 seconds)
2022-03-03 11:49:36 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 11:49:36 | INFO | train | epoch 002 | loss 10.128 | ppl 1118.89 | wps 14607.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.533 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 3537
2022-03-03 11:49:36 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 11:49:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:51:00 | INFO | train_inner | epoch 003:     19 / 393 loss=9.742, ppl=856.48, wps=14333.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.64, loss_scale=8, train_wall=439, gb_free=10.1, wall=3622
2022-03-03 11:58:26 | INFO | train_inner | epoch 003:    119 / 393 loss=9.538, ppl=743.37, wps=14697.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.741, loss_scale=8, train_wall=441, gb_free=10.1, wall=4068
2022-03-03 12:05:52 | INFO | train_inner | epoch 003:    219 / 393 loss=9.374, ppl=663.51, wps=14694.6, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.761, loss_scale=8, train_wall=441, gb_free=10.1, wall=4514
2022-03-03 12:13:18 | INFO | train_inner | epoch 003:    319 / 393 loss=9.223, ppl=597.76, wps=14696.5, ups=0.22, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.784, loss_scale=16, train_wall=441, gb_free=10.1, wall=4960
2022-03-03 12:18:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:18:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.993 | ppl 509.39 | wps 33960.8 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 8.993
2022-03-03 12:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 12:18:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 12:18:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 12:18:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 3 @ 1174 updates, score 8.993) (writing took 4.583301839418709 seconds)
2022-03-03 12:18:57 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 12:18:57 | INFO | train | epoch 003 | loss 9.339 | ppl 647.52 | wps 14601.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.765 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 5299
2022-03-03 12:18:57 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 12:18:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:20:53 | INFO | train_inner | epoch 004:     26 / 393 loss=9.073, ppl=538.62, wps=14331.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.807, loss_scale=16, train_wall=439, gb_free=10.1, wall=5415
2022-03-03 12:28:19 | INFO | train_inner | epoch 004:    126 / 393 loss=8.923, ppl=485.36, wps=14695.7, ups=0.22, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.77, loss_scale=16, train_wall=441, gb_free=10.1, wall=5861
2022-03-03 12:35:45 | INFO | train_inner | epoch 004:    226 / 393 loss=8.816, ppl=450.68, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.825, loss_scale=16, train_wall=441, gb_free=10.1, wall=6307
2022-03-03 12:43:11 | INFO | train_inner | epoch 004:    326 / 393 loss=8.704, ppl=416.89, wps=14692.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.825, loss_scale=16, train_wall=441, gb_free=10.1, wall=6753
2022-03-03 12:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:48:15 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.532 | ppl 370.08 | wps 34084.1 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.532
2022-03-03 12:48:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 12:48:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 12:48:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 12:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.532) (writing took 4.559752562083304 seconds)
2022-03-03 12:48:19 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 12:48:19 | INFO | train | epoch 004 | loss 8.792 | ppl 443.34 | wps 14602.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.81 | loss_scale 32 | train_wall 1731 | gb_free 10.1 | wall 7061
2022-03-03 12:48:19 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 12:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:50:46 | INFO | train_inner | epoch 005:     33 / 393 loss=8.581, ppl=382.91, wps=14335.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.815, loss_scale=32, train_wall=439, gb_free=10.1, wall=7208
2022-03-03 12:58:12 | INFO | train_inner | epoch 005:    133 / 393 loss=8.457, ppl=351.48, wps=14693.6, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.809, loss_scale=32, train_wall=441, gb_free=10.1, wall=7654
2022-03-03 13:05:38 | INFO | train_inner | epoch 005:    233 / 393 loss=8.373, ppl=331.63, wps=14694.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.813, loss_scale=32, train_wall=441, gb_free=10.1, wall=8100
2022-03-03 13:13:05 | INFO | train_inner | epoch 005:    333 / 393 loss=8.291, ppl=313.2, wps=14691.5, ups=0.22, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.801, loss_scale=32, train_wall=441, gb_free=10.1, wall=8546
2022-03-03 13:17:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:17:37 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.178 | ppl 289.65 | wps 34132.8 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 8.178
2022-03-03 13:17:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-03 13:17:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 13:17:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 13:17:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 5 @ 1960 updates, score 8.178) (writing took 4.799783135764301 seconds)
2022-03-03 13:17:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 13:17:41 | INFO | train | epoch 005 | loss 8.362 | ppl 329.12 | wps 14598.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.803 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 8823
2022-03-03 13:17:41 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 13:17:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:20:40 | INFO | train_inner | epoch 006:     40 / 393 loss=8.173, ppl=288.62, wps=14327, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.788, loss_scale=32, train_wall=439, gb_free=10.1, wall=9001
2022-03-03 13:24:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 13:28:10 | INFO | train_inner | epoch 006:    141 / 393 loss=8.078, ppl=270.23, wps=14549.6, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.77, loss_scale=32, train_wall=445, gb_free=10.1, wall=9452
2022-03-03 13:35:36 | INFO | train_inner | epoch 006:    241 / 393 loss=8.004, ppl=256.77, wps=14690.9, ups=0.22, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.745, loss_scale=32, train_wall=441, gb_free=10.1, wall=9898
2022-03-03 13:43:02 | INFO | train_inner | epoch 006:    341 / 393 loss=7.947, ppl=246.69, wps=14695.1, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.761, loss_scale=32, train_wall=441, gb_free=10.1, wall=10344
2022-03-03 13:46:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:46:59 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.909 | ppl 240.35 | wps 34133.5 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 7.909
2022-03-03 13:46:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 13:46:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 13:47:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 13:47:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 6 @ 2352 updates, score 7.909) (writing took 4.768597791902721 seconds)
2022-03-03 13:47:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 13:47:03 | INFO | train | epoch 006 | loss 8.005 | ppl 256.8 | wps 14563.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.762 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 10585
2022-03-03 13:47:04 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 13:47:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:50:38 | INFO | train_inner | epoch 007:     48 / 393 loss=7.84, ppl=229.09, wps=14328.4, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.739, loss_scale=32, train_wall=439, gb_free=10.1, wall=10799
2022-03-03 13:58:04 | INFO | train_inner | epoch 007:    148 / 393 loss=7.748, ppl=214.91, wps=14688.3, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.724, loss_scale=32, train_wall=441, gb_free=10.1, wall=11245
2022-03-03 14:02:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 14:05:34 | INFO | train_inner | epoch 007:    249 / 393 loss=7.702, ppl=208.2, wps=14549.5, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.731, loss_scale=32, train_wall=445, gb_free=10.1, wall=11696
2022-03-03 14:13:00 | INFO | train_inner | epoch 007:    349 / 393 loss=7.657, ppl=201.77, wps=14693.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.709, loss_scale=32, train_wall=441, gb_free=10.1, wall=12142
2022-03-03 14:16:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:16:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.687 | ppl 206.12 | wps 33924.8 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 7.687
2022-03-03 14:16:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-03 14:16:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 14:16:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 14:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 7 @ 2744 updates, score 7.687) (writing took 4.470217881724238 seconds)
2022-03-03 14:16:26 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-03 14:16:26 | INFO | train | epoch 007 | loss 7.701 | ppl 208.05 | wps 14562.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.723 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 12347
2022-03-03 14:16:26 | INFO | fairseq.trainer | begin training epoch 8
2022-03-03 14:16:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:20:36 | INFO | train_inner | epoch 008:     56 / 393 loss=7.536, ppl=185.61, wps=14334.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.706, loss_scale=32, train_wall=439, gb_free=10.1, wall=12597
2022-03-03 14:22:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:28:06 | INFO | train_inner | epoch 008:    157 / 393 loss=7.468, ppl=177.08, wps=14551.7, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.708, loss_scale=16, train_wall=445, gb_free=10.1, wall=13047
2022-03-03 14:35:32 | INFO | train_inner | epoch 008:    257 / 393 loss=7.45, ppl=174.89, wps=14696.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.705, loss_scale=16, train_wall=441, gb_free=10.1, wall=13493
2022-03-03 14:42:58 | INFO | train_inner | epoch 008:    357 / 393 loss=7.418, ppl=171, wps=14698.2, ups=0.22, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.67, loss_scale=16, train_wall=441, gb_free=10.1, wall=13939
2022-03-03 14:45:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:45:43 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.515 | ppl 182.97 | wps 34081.7 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 7.515
2022-03-03 14:45:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-03 14:45:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 14:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 14:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 8 @ 3136 updates, score 7.515) (writing took 4.762716709636152 seconds)
2022-03-03 14:45:47 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-03 14:45:47 | INFO | train | epoch 008 | loss 7.446 | ppl 174.38 | wps 14565 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.696 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 14109
2022-03-03 14:45:47 | INFO | fairseq.trainer | begin training epoch 9
2022-03-03 14:45:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:50:33 | INFO | train_inner | epoch 009:     64 / 393 loss=7.307, ppl=158.39, wps=14333.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.691, loss_scale=16, train_wall=439, gb_free=10.1, wall=14394
2022-03-03 14:57:59 | INFO | train_inner | epoch 009:    164 / 393 loss=7.247, ppl=151.88, wps=14697.1, ups=0.22, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.678, loss_scale=16, train_wall=441, gb_free=10.1, wall=14840
2022-03-03 15:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:05:29 | INFO | train_inner | epoch 009:    265 / 393 loss=7.233, ppl=150.47, wps=14545.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.663, loss_scale=16, train_wall=446, gb_free=10.1, wall=15291
2022-03-03 15:12:55 | INFO | train_inner | epoch 009:    365 / 393 loss=7.215, ppl=148.59, wps=14701.1, ups=0.22, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.654, loss_scale=16, train_wall=441, gb_free=10.1, wall=15737
2022-03-03 15:14:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:15:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.391 | ppl 167.81 | wps 34207.1 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 7.391
2022-03-03 15:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-03 15:15:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 15:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 15:15:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 9 @ 3528 updates, score 7.391) (writing took 4.772150571458042 seconds)
2022-03-03 15:15:09 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-03 15:15:09 | INFO | train | epoch 009 | loss 7.234 | ppl 150.49 | wps 14564.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.668 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 15871
2022-03-03 15:15:09 | INFO | fairseq.trainer | begin training epoch 10
2022-03-03 15:15:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:20:31 | INFO | train_inner | epoch 010:     72 / 393 loss=7.098, ppl=136.97, wps=14326.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.665, loss_scale=16, train_wall=439, gb_free=10.1, wall=16192
2022-03-03 15:27:57 | INFO | train_inner | epoch 010:    172 / 393 loss=7.061, ppl=133.57, wps=14689.5, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.647, loss_scale=16, train_wall=441, gb_free=10.1, wall=16638
2022-03-03 15:35:23 | INFO | train_inner | epoch 010:    272 / 393 loss=7.053, ppl=132.8, wps=14693.2, ups=0.22, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.668, loss_scale=16, train_wall=441, gb_free=10.1, wall=17084
2022-03-03 15:42:49 | INFO | train_inner | epoch 010:    372 / 393 loss=7.049, ppl=132.4, wps=14691.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.615, loss_scale=16, train_wall=441, gb_free=10.1, wall=17530
2022-03-03 15:43:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:44:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:44:27 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.285 | ppl 155.95 | wps 34059.4 | wpb 2034.1 | bsz 4 | num_updates 3920 | best_loss 7.285
2022-03-03 15:44:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3920 updates
2022-03-03 15:44:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 15:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 15:44:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 10 @ 3920 updates, score 7.285) (writing took 4.564300650730729 seconds)
2022-03-03 15:44:32 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-03 15:44:32 | INFO | train | epoch 010 | loss 7.055 | ppl 132.97 | wps 14562.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3920 | lr 0.000490002 | gnorm 0.649 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 17633
2022-03-03 15:44:32 | INFO | fairseq.trainer | begin training epoch 11
2022-03-03 15:44:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:50:28 | INFO | train_inner | epoch 011:     80 / 393 loss=6.925, ppl=121.54, wps=14200.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.659, loss_scale=16, train_wall=444, gb_free=10.1, wall=17990
2022-03-03 15:57:54 | INFO | train_inner | epoch 011:    180 / 393 loss=6.89, ppl=118.61, wps=14697.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.625, loss_scale=16, train_wall=441, gb_free=10.1, wall=18436
2022-03-03 16:05:20 | INFO | train_inner | epoch 011:    280 / 393 loss=6.896, ppl=119.11, wps=14696.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.585, loss_scale=16, train_wall=441, gb_free=10.1, wall=18882
2022-03-03 16:12:46 | INFO | train_inner | epoch 011:    380 / 393 loss=6.904, ppl=119.76, wps=14691.1, ups=0.22, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.61, loss_scale=16, train_wall=441, gb_free=10.1, wall=19328
2022-03-03 16:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:13:49 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.205 | ppl 147.52 | wps 34114.3 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 7.205
2022-03-03 16:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-03 16:13:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 16:13:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 16:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 11 @ 4313 updates, score 7.205) (writing took 4.572636241093278 seconds)
2022-03-03 16:13:53 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-03 16:13:53 | INFO | train | epoch 011 | loss 6.896 | ppl 119.12 | wps 14603.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.617 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 19395
2022-03-03 16:13:53 | INFO | fairseq.trainer | begin training epoch 12
2022-03-03 16:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:20:21 | INFO | train_inner | epoch 012:     87 / 393 loss=6.735, ppl=106.5, wps=14335.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.587, loss_scale=16, train_wall=439, gb_free=10.1, wall=19783
2022-03-03 16:22:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:27:52 | INFO | train_inner | epoch 012:    188 / 393 loss=6.737, ppl=106.67, wps=14557.4, ups=0.22, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.587, loss_scale=16, train_wall=445, gb_free=10.1, wall=20233
2022-03-03 16:35:17 | INFO | train_inner | epoch 012:    288 / 393 loss=6.762, ppl=108.52, wps=14696.4, ups=0.22, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.569, loss_scale=16, train_wall=441, gb_free=10.1, wall=20679
2022-03-03 16:42:43 | INFO | train_inner | epoch 012:    388 / 393 loss=6.766, ppl=108.83, wps=14696.6, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.566, loss_scale=16, train_wall=441, gb_free=10.1, wall=21125
2022-03-03 16:43:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:43:10 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.138 | ppl 140.89 | wps 34059.1 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.138
2022-03-03 16:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-03 16:43:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 16:43:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 16:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.138) (writing took 4.553839774802327 seconds)
2022-03-03 16:43:15 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-03 16:43:15 | INFO | train | epoch 012 | loss 6.746 | ppl 107.32 | wps 14567.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.574 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 21156
2022-03-03 16:43:15 | INFO | fairseq.trainer | begin training epoch 13
2022-03-03 16:43:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:47:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 16:50:23 | INFO | train_inner | epoch 013:     96 / 393 loss=6.595, ppl=96.65, wps=14201.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.568, loss_scale=8, train_wall=443, gb_free=10.1, wall=21584
2022-03-03 16:57:49 | INFO | train_inner | epoch 013:    196 / 393 loss=6.619, ppl=98.29, wps=14702.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.564, loss_scale=8, train_wall=441, gb_free=10.1, wall=22030
2022-03-03 17:05:14 | INFO | train_inner | epoch 013:    296 / 393 loss=6.632, ppl=99.16, wps=14703.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.567, loss_scale=8, train_wall=441, gb_free=10.1, wall=22476
2022-03-03 17:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:12:31 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.086 | ppl 135.9 | wps 34031.8 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 7.086
2022-03-03 17:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-03 17:12:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 17:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 17:12:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 13 @ 5097 updates, score 7.086) (writing took 4.745896052569151 seconds)
2022-03-03 17:12:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-03 17:12:36 | INFO | train | epoch 013 | loss 6.621 | ppl 98.42 | wps 14571.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.566 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 22917
2022-03-03 17:12:36 | INFO | fairseq.trainer | begin training epoch 14
2022-03-03 17:12:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:12:49 | INFO | train_inner | epoch 014:      3 / 393 loss=6.64, ppl=99.76, wps=14341.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.559, loss_scale=8, train_wall=439, gb_free=10.1, wall=22931
2022-03-03 17:20:15 | INFO | train_inner | epoch 014:    103 / 393 loss=6.485, ppl=89.59, wps=14703.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.56, loss_scale=8, train_wall=441, gb_free=10.1, wall=23376
2022-03-03 17:27:41 | INFO | train_inner | epoch 014:    203 / 393 loss=6.508, ppl=91, wps=14699.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.556, loss_scale=16, train_wall=441, gb_free=10.1, wall=23822
2022-03-03 17:31:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 17:35:11 | INFO | train_inner | epoch 014:    304 / 393 loss=6.526, ppl=92.13, wps=14556.7, ups=0.22, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.548, loss_scale=8, train_wall=445, gb_free=10.1, wall=24272
2022-03-03 17:41:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:41:52 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.065 | ppl 133.9 | wps 34051.6 | wpb 2034.1 | bsz 4 | num_updates 5489 | best_loss 7.065
2022-03-03 17:41:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5489 updates
2022-03-03 17:41:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 17:41:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 17:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 14 @ 5489 updates, score 7.065) (writing took 4.62662547826767 seconds)
2022-03-03 17:41:57 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-03 17:41:57 | INFO | train | epoch 014 | loss 6.515 | ppl 91.44 | wps 14572.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5489 | lr 0.000426828 | gnorm 0.554 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 24678
2022-03-03 17:41:57 | INFO | fairseq.trainer | begin training epoch 15
2022-03-03 17:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:42:46 | INFO | train_inner | epoch 015:     11 / 393 loss=6.526, ppl=92.13, wps=14343.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.546, loss_scale=8, train_wall=439, gb_free=10.1, wall=24727
2022-03-03 17:50:12 | INFO | train_inner | epoch 015:    111 / 393 loss=6.385, ppl=83.59, wps=14702.1, ups=0.22, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.553, loss_scale=8, train_wall=441, gb_free=10.1, wall=25173
2022-03-03 17:57:37 | INFO | train_inner | epoch 015:    211 / 393 loss=6.418, ppl=85.49, wps=14707.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.559, loss_scale=8, train_wall=441, gb_free=10.1, wall=25619
2022-03-03 18:05:03 | INFO | train_inner | epoch 015:    311 / 393 loss=6.439, ppl=86.78, wps=14705.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.556, loss_scale=8, train_wall=441, gb_free=10.1, wall=26064
2022-03-03 18:11:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:11:13 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.057 | ppl 133.19 | wps 34073.5 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 7.057
2022-03-03 18:11:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-03 18:11:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 18:11:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 18:11:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 15 @ 5882 updates, score 7.057) (writing took 4.728602767921984 seconds)
2022-03-03 18:11:18 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-03 18:11:18 | INFO | train | epoch 015 | loss 6.422 | ppl 85.74 | wps 14609.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.553 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 26439
2022-03-03 18:11:18 | INFO | fairseq.trainer | begin training epoch 16
2022-03-03 18:11:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:12:38 | INFO | train_inner | epoch 016:     18 / 393 loss=6.425, ppl=85.91, wps=14338, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.549, loss_scale=16, train_wall=439, gb_free=10.1, wall=26519
2022-03-03 18:14:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 18:20:08 | INFO | train_inner | epoch 016:    119 / 393 loss=6.301, ppl=78.86, wps=14555, ups=0.22, wpb=65530.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.572, loss_scale=8, train_wall=445, gb_free=10.1, wall=26970
2022-03-03 18:27:34 | INFO | train_inner | epoch 016:    219 / 393 loss=6.336, ppl=80.79, wps=14699.4, ups=0.22, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.544, loss_scale=8, train_wall=441, gb_free=10.1, wall=27415
2022-03-03 18:35:00 | INFO | train_inner | epoch 016:    319 / 393 loss=6.361, ppl=82.18, wps=14693.3, ups=0.22, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.577, loss_scale=8, train_wall=441, gb_free=10.1, wall=27861
2022-03-03 18:40:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:40:35 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.054 | ppl 132.85 | wps 33989.9 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 7.054
2022-03-03 18:40:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-03 18:40:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 18:40:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 18:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 16 @ 6274 updates, score 7.054) (writing took 4.442295926623046 seconds)
2022-03-03 18:40:39 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-03 18:40:39 | INFO | train | epoch 016 | loss 6.34 | ppl 81.01 | wps 14568.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.565 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 28200
2022-03-03 18:40:39 | INFO | fairseq.trainer | begin training epoch 17
2022-03-03 18:40:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:42:35 | INFO | train_inner | epoch 017:     26 / 393 loss=6.334, ppl=80.7, wps=14341.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.57, loss_scale=8, train_wall=439, gb_free=10.1, wall=28316
2022-03-03 18:50:01 | INFO | train_inner | epoch 017:    126 / 393 loss=6.233, ppl=75.22, wps=14698, ups=0.22, wpb=65530.9, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.56, loss_scale=8, train_wall=441, gb_free=10.1, wall=28762
2022-03-03 18:53:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 18:57:31 | INFO | train_inner | epoch 017:    227 / 393 loss=6.258, ppl=76.51, wps=14557.6, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.537, loss_scale=8, train_wall=445, gb_free=10.1, wall=29212
2022-03-03 19:04:57 | INFO | train_inner | epoch 017:    327 / 393 loss=6.294, ppl=78.48, wps=14700.2, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.553, loss_scale=8, train_wall=441, gb_free=10.1, wall=29658
2022-03-03 19:09:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:09:56 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.033 | ppl 130.98 | wps 34006.9 | wpb 2034.1 | bsz 4 | num_updates 6666 | best_loss 7.033
2022-03-03 19:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6666 updates
2022-03-03 19:09:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 19:10:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 19:10:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 17 @ 6666 updates, score 7.033) (writing took 4.5570850893855095 seconds)
2022-03-03 19:10:00 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-03 19:10:00 | INFO | train | epoch 017 | loss 6.267 | ppl 77.01 | wps 14570.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6666 | lr 0.000387318 | gnorm 0.552 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 29962
2022-03-03 19:10:00 | INFO | fairseq.trainer | begin training epoch 18
2022-03-03 19:10:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:12:32 | INFO | train_inner | epoch 018:     34 / 393 loss=6.259, ppl=76.6, wps=14337.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.582, loss_scale=8, train_wall=439, gb_free=10.1, wall=30113
2022-03-03 19:19:58 | INFO | train_inner | epoch 018:    134 / 393 loss=6.164, ppl=71.69, wps=14694.2, ups=0.22, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.536, loss_scale=8, train_wall=441, gb_free=10.1, wall=30559
2022-03-03 19:27:24 | INFO | train_inner | epoch 018:    234 / 393 loss=6.2, ppl=73.52, wps=14700.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.566, loss_scale=8, train_wall=441, gb_free=10.1, wall=31005
2022-03-03 19:32:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 19:34:54 | INFO | train_inner | epoch 018:    335 / 393 loss=6.236, ppl=75.35, wps=14556, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.564, loss_scale=8, train_wall=445, gb_free=10.1, wall=31455
2022-03-03 19:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:39:17 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.049 | ppl 132.45 | wps 34167.5 | wpb 2034.1 | bsz 4 | num_updates 7058 | best_loss 7.033
2022-03-03 19:39:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7058 updates
2022-03-03 19:39:17 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-03 19:39:17 | INFO | train | epoch 018 | loss 6.201 | ppl 73.58 | wps 14606.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7058 | lr 0.000376408 | gnorm 0.56 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 31718
2022-03-03 19:39:17 | INFO | fairseq.trainer | begin training epoch 19
2022-03-03 19:39:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:42:24 | INFO | train_inner | epoch 019:     42 / 393 loss=6.174, ppl=72.2, wps=14486.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.548, loss_scale=8, train_wall=439, gb_free=10.1, wall=31906
2022-03-03 19:49:50 | INFO | train_inner | epoch 019:    142 / 393 loss=6.101, ppl=68.63, wps=14699.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.564, loss_scale=8, train_wall=441, gb_free=10.1, wall=32352
2022-03-03 19:57:16 | INFO | train_inner | epoch 019:    242 / 393 loss=6.151, ppl=71.06, wps=14703.5, ups=0.22, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.568, loss_scale=8, train_wall=441, gb_free=10.1, wall=32797
2022-03-03 20:04:42 | INFO | train_inner | epoch 019:    342 / 393 loss=6.177, ppl=72.37, wps=14700.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.572, loss_scale=8, train_wall=441, gb_free=10.1, wall=33243
2022-03-03 20:08:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:08:34 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.032 | ppl 130.85 | wps 34004.1 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 7.032
2022-03-03 20:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-03 20:08:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 20:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt
2022-03-03 20:08:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#2/checkpoint_best.pt (epoch 19 @ 7451 updates, score 7.032) (writing took 4.52777439262718 seconds)
2022-03-03 20:08:38 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-03 20:08:38 | INFO | train | epoch 019 | loss 6.142 | ppl 70.6 | wps 14608.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.565 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 33479
2022-03-03 20:08:38 | INFO | fairseq.trainer | begin training epoch 20
2022-03-03 20:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:12:17 | INFO | train_inner | epoch 020:     49 / 393 loss=6.099, ppl=68.57, wps=14343.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.553, loss_scale=16, train_wall=439, gb_free=10.1, wall=33698
2022-03-03 20:12:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:19:47 | INFO | train_inner | epoch 020:    150 / 393 loss=6.057, ppl=66.56, wps=14558.5, ups=0.22, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.571, loss_scale=8, train_wall=445, gb_free=10.1, wall=34148
2022-03-03 20:27:12 | INFO | train_inner | epoch 020:    250 / 393 loss=6.089, ppl=68.08, wps=14704.6, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.592, loss_scale=8, train_wall=441, gb_free=10.1, wall=34594
2022-03-03 20:34:38 | INFO | train_inner | epoch 020:    350 / 393 loss=6.126, ppl=69.85, wps=14705.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.559, loss_scale=8, train_wall=441, gb_free=10.1, wall=35039
2022-03-03 20:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:37:54 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.045 | ppl 132.04 | wps 33964.8 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 7.032
2022-03-03 20:37:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-03 20:37:54 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-03 20:37:54 | INFO | train | epoch 020 | loss 6.086 | ppl 67.91 | wps 14610.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.567 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 35236
2022-03-03 20:37:54 | INFO | fairseq.trainer | begin training epoch 21
2022-03-03 20:37:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:42:09 | INFO | train_inner | epoch 021:     57 / 393 loss=6.037, ppl=65.65, wps=14481.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.553, loss_scale=8, train_wall=439, gb_free=10.1, wall=35490
2022-03-03 20:49:34 | INFO | train_inner | epoch 021:    157 / 393 loss=6.007, ppl=64.31, wps=14702.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.575, loss_scale=8, train_wall=441, gb_free=10.1, wall=35936
2022-03-03 20:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:57:05 | INFO | train_inner | epoch 021:    258 / 393 loss=6.034, ppl=65.53, wps=14554.7, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.564, loss_scale=8, train_wall=445, gb_free=10.1, wall=36386
2022-03-03 21:04:31 | INFO | train_inner | epoch 021:    358 / 393 loss=6.083, ppl=67.79, wps=14697.7, ups=0.22, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.546, loss_scale=8, train_wall=441, gb_free=10.1, wall=36832
2022-03-03 21:07:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:07:11 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.064 | ppl 133.79 | wps 34131.6 | wpb 2034.1 | bsz 4 | num_updates 8235 | best_loss 7.032
2022-03-03 21:07:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8235 updates
2022-03-03 21:07:11 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-03 21:07:11 | INFO | train | epoch 021 | loss 6.035 | ppl 65.56 | wps 14608.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8235 | lr 0.000348472 | gnorm 0.564 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 36992
2022-03-03 21:07:11 | INFO | fairseq.trainer | begin training epoch 22
2022-03-03 21:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:12:01 | INFO | train_inner | epoch 022:     65 / 393 loss=5.986, ppl=63.39, wps=14485.4, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.58, loss_scale=8, train_wall=439, gb_free=10.1, wall=37282
2022-03-03 21:19:27 | INFO | train_inner | epoch 022:    165 / 393 loss=5.965, ppl=62.45, wps=14698.2, ups=0.22, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.597, loss_scale=8, train_wall=441, gb_free=10.1, wall=37728
2022-03-03 21:24:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 21:26:57 | INFO | train_inner | epoch 022:    266 / 393 loss=5.996, ppl=63.84, wps=14554, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.578, loss_scale=4, train_wall=445, gb_free=10.1, wall=38179
2022-03-03 21:34:23 | INFO | train_inner | epoch 022:    366 / 393 loss=6.028, ppl=65.24, wps=14702.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.579, loss_scale=4, train_wall=441, gb_free=10.1, wall=38624
2022-03-03 21:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:36:28 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.063 | ppl 133.73 | wps 34028.2 | wpb 2034.1 | bsz 4 | num_updates 8627 | best_loss 7.032
2022-03-03 21:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8627 updates
2022-03-03 21:36:28 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-03 21:36:28 | INFO | train | epoch 022 | loss 5.987 | ppl 63.44 | wps 14607.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8627 | lr 0.000340463 | gnorm 0.578 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 38749
2022-03-03 21:36:28 | INFO | fairseq.trainer | begin training epoch 23
2022-03-03 21:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:41:53 | INFO | train_inner | epoch 023:     73 / 393 loss=5.923, ppl=60.66, wps=14490.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.587, loss_scale=4, train_wall=439, gb_free=10.1, wall=39075
2022-03-03 21:49:19 | INFO | train_inner | epoch 023:    173 / 393 loss=5.914, ppl=60.31, wps=14703, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.575, loss_scale=4, train_wall=441, gb_free=10.1, wall=39520
2022-03-03 21:56:45 | INFO | train_inner | epoch 023:    273 / 393 loss=5.958, ppl=62.18, wps=14704.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.604, loss_scale=4, train_wall=441, gb_free=10.1, wall=39966
2022-03-03 22:04:10 | INFO | train_inner | epoch 023:    373 / 393 loss=5.993, ppl=63.7, wps=14702.1, ups=0.22, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=40412
2022-03-03 22:05:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:05:44 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.08 | ppl 135.28 | wps 34082.3 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 7.032
2022-03-03 22:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-03 22:05:44 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-03 22:05:44 | INFO | train | epoch 023 | loss 5.944 | ppl 61.56 | wps 14648.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.595 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 40505
2022-03-03 22:05:44 | INFO | fairseq.trainer | begin training epoch 24
2022-03-03 22:05:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:11:41 | INFO | train_inner | epoch 024:     80 / 393 loss=5.873, ppl=58.62, wps=14487.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.571, loss_scale=8, train_wall=439, gb_free=10.1, wall=40862
2022-03-03 22:19:06 | INFO | train_inner | epoch 024:    180 / 393 loss=5.88, ppl=58.91, wps=14701.5, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.6, loss_scale=8, train_wall=441, gb_free=10.1, wall=41308
2022-03-03 22:26:32 | INFO | train_inner | epoch 024:    280 / 393 loss=5.911, ppl=60.18, wps=14703.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.569, loss_scale=8, train_wall=441, gb_free=10.1, wall=41754
2022-03-03 22:33:58 | INFO | train_inner | epoch 024:    380 / 393 loss=5.953, ppl=61.94, wps=14704.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.585, loss_scale=8, train_wall=441, gb_free=10.1, wall=42199
2022-03-03 22:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:35:00 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.085 | ppl 135.8 | wps 33961.8 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 7.032
2022-03-03 22:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-03 22:35:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-03 22:35:00 | INFO | train | epoch 024 | loss 5.902 | ppl 59.8 | wps 14647.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.58 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 42262
2022-03-03 22:35:00 | INFO | fairseq.trainer | begin training epoch 25
2022-03-03 22:35:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:41:28 | INFO | train_inner | epoch 025:     87 / 393 loss=5.821, ppl=56.55, wps=14489.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.583, loss_scale=16, train_wall=439, gb_free=10.1, wall=42650
2022-03-03 22:41:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:48:58 | INFO | train_inner | epoch 025:    188 / 393 loss=5.838, ppl=57.22, wps=14558.9, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.586, loss_scale=8, train_wall=445, gb_free=10.1, wall=43100
2022-03-03 22:50:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 22:56:28 | INFO | train_inner | epoch 025:    289 / 393 loss=5.884, ppl=59.05, wps=14561.4, ups=0.22, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.598, loss_scale=4, train_wall=445, gb_free=10.1, wall=43550
2022-03-03 23:03:54 | INFO | train_inner | epoch 025:    389 / 393 loss=5.922, ppl=60.64, wps=14709.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.604, loss_scale=4, train_wall=441, gb_free=10.1, wall=43995
2022-03-03 23:04:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:04:16 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.11 | ppl 138.16 | wps 34054.8 | wpb 2034.1 | bsz 4 | num_updates 9804 | best_loss 7.032
2022-03-03 23:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9804 updates
2022-03-03 23:04:16 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-03 23:04:16 | INFO | train | epoch 025 | loss 5.863 | ppl 58.2 | wps 14577 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 9804 | lr 0.000319373 | gnorm 0.595 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 44018
2022-03-03 23:04:16 | INFO | fairseq.trainer | begin training epoch 26
2022-03-03 23:04:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:11:24 | INFO | train_inner | epoch 026:     96 / 393 loss=5.769, ppl=54.52, wps=14498.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.602, loss_scale=4, train_wall=439, gb_free=10.1, wall=44445
2022-03-03 23:18:50 | INFO | train_inner | epoch 026:    196 / 393 loss=5.807, ppl=56, wps=14705.3, ups=0.22, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.597, loss_scale=4, train_wall=441, gb_free=10.1, wall=44891
2022-03-03 23:26:15 | INFO | train_inner | epoch 026:    296 / 393 loss=5.848, ppl=57.62, wps=14707.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.613, loss_scale=4, train_wall=441, gb_free=10.1, wall=45336
2022-03-03 23:33:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:33:32 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.108 | ppl 137.96 | wps 33970.8 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 7.032
2022-03-03 23:33:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-03 23:33:32 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-03 23:33:32 | INFO | train | epoch 026 | loss 5.826 | ppl 56.73 | wps 14650 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.597 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 45774
2022-03-03 23:33:32 | INFO | fairseq.trainer | begin training epoch 27
2022-03-03 23:33:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:33:46 | INFO | train_inner | epoch 027:      3 / 393 loss=5.878, ppl=58.8, wps=14481.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.576, loss_scale=8, train_wall=439, gb_free=10.1, wall=45787
2022-03-03 23:41:12 | INFO | train_inner | epoch 027:    103 / 393 loss=5.732, ppl=53.16, wps=14695.9, ups=0.22, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=46233
2022-03-03 23:48:38 | INFO | train_inner | epoch 027:    203 / 393 loss=5.777, ppl=54.82, wps=14694.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.579, loss_scale=8, train_wall=441, gb_free=10.1, wall=46679
2022-03-03 23:50:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 23:56:08 | INFO | train_inner | epoch 027:    304 / 393 loss=5.821, ppl=56.55, wps=14559.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.64, loss_scale=4, train_wall=445, gb_free=10.1, wall=47129
2022-03-04 00:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:02:49 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.123 | ppl 139.41 | wps 34031.4 | wpb 2034.1 | bsz 4 | num_updates 10589 | best_loss 7.032
2022-03-04 00:02:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10589 updates
2022-03-04 00:02:49 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 00:02:49 | INFO | train | epoch 027 | loss 5.792 | ppl 55.39 | wps 14608.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10589 | lr 0.000307307 | gnorm 0.603 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 47530
2022-03-04 00:02:49 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 00:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:03:38 | INFO | train_inner | epoch 028:     11 / 393 loss=5.826, ppl=56.74, wps=14495.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.609, loss_scale=4, train_wall=439, gb_free=10.1, wall=47579
2022-03-04 00:11:04 | INFO | train_inner | epoch 028:    111 / 393 loss=5.699, ppl=51.96, wps=14703, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.6, loss_scale=4, train_wall=441, gb_free=10.1, wall=48025
2022-03-04 00:18:29 | INFO | train_inner | epoch 028:    211 / 393 loss=5.737, ppl=53.34, wps=14711.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.605, loss_scale=4, train_wall=441, gb_free=10.1, wall=48470
2022-03-04 00:25:55 | INFO | train_inner | epoch 028:    311 / 393 loss=5.796, ppl=55.57, wps=14703.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.603, loss_scale=4, train_wall=441, gb_free=10.1, wall=48916
2022-03-04 00:30:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 00:31:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:32:05 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.15 | ppl 141.98 | wps 34114.3 | wpb 2034.1 | bsz 4 | num_updates 10981 | best_loss 7.032
2022-03-04 00:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10981 updates
2022-03-04 00:32:05 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 00:32:05 | INFO | train | epoch 028 | loss 5.759 | ppl 54.14 | wps 14613.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10981 | lr 0.000301772 | gnorm 0.611 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 49286
2022-03-04 00:32:05 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 00:32:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:33:29 | INFO | train_inner | epoch 029:     19 / 393 loss=5.791, ppl=55.36, wps=14349.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.629, loss_scale=4, train_wall=443, gb_free=10.1, wall=49371
2022-03-04 00:40:55 | INFO | train_inner | epoch 029:    119 / 393 loss=5.681, ppl=51.3, wps=14705.2, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.602, loss_scale=4, train_wall=441, gb_free=10.1, wall=49816
2022-03-04 00:48:21 | INFO | train_inner | epoch 029:    219 / 393 loss=5.72, ppl=52.72, wps=14701.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.604, loss_scale=4, train_wall=441, gb_free=10.1, wall=50262
2022-03-04 00:55:47 | INFO | train_inner | epoch 029:    319 / 393 loss=5.754, ppl=53.97, wps=14704.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.643, loss_scale=4, train_wall=441, gb_free=10.1, wall=50708
2022-03-04 01:01:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:01:21 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.154 | ppl 142.41 | wps 34061.2 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 7.032
2022-03-04 01:01:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-04 01:01:21 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 01:01:21 | INFO | train | epoch 029 | loss 5.727 | ppl 52.98 | wps 14650.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.612 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 51042
2022-03-04 01:01:21 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 01:01:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:03:17 | INFO | train_inner | epoch 030:     26 / 393 loss=5.74, ppl=53.45, wps=14491.8, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.591, loss_scale=4, train_wall=439, gb_free=10.1, wall=51158
2022-03-04 01:10:42 | INFO | train_inner | epoch 030:    126 / 393 loss=5.65, ppl=50.22, wps=14702.6, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.599, loss_scale=8, train_wall=441, gb_free=10.1, wall=51604
2022-03-04 01:18:08 | INFO | train_inner | epoch 030:    226 / 393 loss=5.695, ppl=51.81, wps=14697.2, ups=0.22, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.601, loss_scale=8, train_wall=441, gb_free=10.1, wall=52050
2022-03-04 01:19:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 01:25:39 | INFO | train_inner | epoch 030:    327 / 393 loss=5.728, ppl=53.02, wps=14556.5, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.642, loss_scale=4, train_wall=445, gb_free=10.1, wall=52500
2022-03-04 01:30:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:30:37 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.17 | ppl 144.04 | wps 33896.3 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 7.032
2022-03-04 01:30:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-04 01:30:37 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 01:30:37 | INFO | train | epoch 030 | loss 5.698 | ppl 51.9 | wps 14608.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.614 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 52799
2022-03-04 01:30:37 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 01:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:33:09 | INFO | train_inner | epoch 031:     34 / 393 loss=5.701, ppl=52.01, wps=14490.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.621, loss_scale=4, train_wall=439, gb_free=10.1, wall=52950
2022-03-04 01:40:35 | INFO | train_inner | epoch 031:    134 / 393 loss=5.625, ppl=49.34, wps=14699.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.597, loss_scale=4, train_wall=441, gb_free=10.1, wall=53396
2022-03-04 01:48:00 | INFO | train_inner | epoch 031:    234 / 393 loss=5.671, ppl=50.96, wps=14703.5, ups=0.22, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.618, loss_scale=4, train_wall=441, gb_free=10.1, wall=53842
2022-03-04 01:55:26 | INFO | train_inner | epoch 031:    334 / 393 loss=5.704, ppl=52.12, wps=14705.4, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.628, loss_scale=4, train_wall=441, gb_free=10.1, wall=54287
2022-03-04 01:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:59:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.184 | ppl 145.38 | wps 33923.7 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 7.032
2022-03-04 01:59:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-04 01:59:54 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 01:59:54 | INFO | train | epoch 031 | loss 5.669 | ppl 50.89 | wps 14648.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.612 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 54555
2022-03-04 01:59:54 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 01:59:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:02:57 | INFO | train_inner | epoch 032:     41 / 393 loss=5.669, ppl=50.87, wps=14484.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.626, loss_scale=8, train_wall=439, gb_free=10.1, wall=54738
2022-03-04 02:10:22 | INFO | train_inner | epoch 032:    141 / 393 loss=5.593, ppl=48.26, wps=14697.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=55184
2022-03-04 02:11:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 02:17:53 | INFO | train_inner | epoch 032:    242 / 393 loss=5.642, ppl=49.93, wps=14558.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.637, loss_scale=4, train_wall=445, gb_free=10.1, wall=55634
2022-03-04 02:25:19 | INFO | train_inner | epoch 032:    342 / 393 loss=5.685, ppl=51.45, wps=14677.7, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.597, loss_scale=4, train_wall=442, gb_free=10.1, wall=56080
2022-03-04 02:29:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:29:11 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.202 | ppl 147.21 | wps 33979.8 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 7.032
2022-03-04 02:29:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-04 02:29:11 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 02:29:11 | INFO | train | epoch 032 | loss 5.643 | ppl 49.95 | wps 14602.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.63 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 56312
2022-03-04 02:29:11 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 02:29:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:32:49 | INFO | train_inner | epoch 033:     49 / 393 loss=5.629, ppl=49.47, wps=14490.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.645, loss_scale=4, train_wall=439, gb_free=10.1, wall=56531
2022-03-04 02:40:15 | INFO | train_inner | epoch 033:    149 / 393 loss=5.573, ppl=47.59, wps=14702.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.64, loss_scale=4, train_wall=441, gb_free=10.1, wall=56976
2022-03-04 02:47:41 | INFO | train_inner | epoch 033:    249 / 393 loss=5.618, ppl=49.12, wps=14705, ups=0.22, wpb=65535.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.629, loss_scale=4, train_wall=441, gb_free=10.1, wall=57422
2022-03-04 02:55:06 | INFO | train_inner | epoch 033:    349 / 393 loss=5.665, ppl=50.73, wps=14704, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.634, loss_scale=8, train_wall=441, gb_free=10.1, wall=57868
2022-03-04 02:57:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 02:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:58:27 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.222 | ppl 149.33 | wps 34025.5 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.032
2022-03-04 02:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-04 02:58:27 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 02:58:27 | INFO | train | epoch 033 | loss 5.617 | ppl 49.07 | wps 14612.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.627 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 58068
2022-03-04 02:58:27 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 02:58:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:02:41 | INFO | train_inner | epoch 034:     57 / 393 loss=5.588, ppl=48.12, wps=14349.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.612, loss_scale=4, train_wall=443, gb_free=10.1, wall=58323
2022-03-04 03:10:07 | INFO | train_inner | epoch 034:    157 / 393 loss=5.549, ppl=46.83, wps=14702.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.62, loss_scale=4, train_wall=441, gb_free=10.1, wall=58768
2022-03-04 03:17:32 | INFO | train_inner | epoch 034:    257 / 393 loss=5.604, ppl=48.63, wps=14712.2, ups=0.22, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.681, loss_scale=4, train_wall=441, gb_free=10.1, wall=59214
2022-03-04 03:24:58 | INFO | train_inner | epoch 034:    357 / 393 loss=5.644, ppl=50, wps=14704.3, ups=0.22, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.614, loss_scale=4, train_wall=441, gb_free=10.1, wall=59659
2022-03-04 03:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:27:43 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.237 | ppl 150.84 | wps 34095.3 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 7.032
2022-03-04 03:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-04 03:27:43 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 03:27:43 | INFO | train | epoch 034 | loss 5.592 | ppl 48.25 | wps 14651.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.634 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 59824
2022-03-04 03:27:43 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 03:27:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:32:28 | INFO | train_inner | epoch 035:     64 / 393 loss=5.557, ppl=47.06, wps=14493.4, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.632, loss_scale=4, train_wall=439, gb_free=10.1, wall=60110
2022-03-04 03:37:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 03:39:58 | INFO | train_inner | epoch 035:    165 / 393 loss=5.539, ppl=46.49, wps=14559.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.652, loss_scale=4, train_wall=445, gb_free=10.1, wall=60560
2022-03-04 03:47:24 | INFO | train_inner | epoch 035:    265 / 393 loss=5.579, ppl=47.8, wps=14703.7, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.679, loss_scale=4, train_wall=441, gb_free=10.1, wall=61005
2022-03-04 03:54:50 | INFO | train_inner | epoch 035:    365 / 393 loss=5.614, ppl=48.96, wps=14698.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.636, loss_scale=4, train_wall=441, gb_free=10.1, wall=61451
2022-03-04 03:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:56:59 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.237 | ppl 150.89 | wps 34054.3 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 7.032
2022-03-04 03:56:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-04 03:56:59 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 03:56:59 | INFO | train | epoch 035 | loss 5.569 | ppl 47.47 | wps 14611.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.65 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 61580
2022-03-04 03:56:59 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 03:56:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:02:20 | INFO | train_inner | epoch 036:     72 / 393 loss=5.521, ppl=45.93, wps=14498.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.632, loss_scale=4, train_wall=439, gb_free=10.1, wall=61901
2022-03-04 04:09:46 | INFO | train_inner | epoch 036:    172 / 393 loss=5.517, ppl=45.78, wps=14702.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.658, loss_scale=4, train_wall=441, gb_free=10.1, wall=62347
2022-03-04 04:16:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 04:17:16 | INFO | train_inner | epoch 036:    273 / 393 loss=5.559, ppl=47.14, wps=14560.2, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.646, loss_scale=4, train_wall=445, gb_free=10.1, wall=62797
2022-03-04 04:24:41 | INFO | train_inner | epoch 036:    373 / 393 loss=5.602, ppl=48.57, wps=14706.8, ups=0.22, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.676, loss_scale=4, train_wall=441, gb_free=10.1, wall=63243
2022-03-04 04:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:26:15 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.254 | ppl 152.65 | wps 33968.8 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.032
2022-03-04 04:26:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-04 04:26:15 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 04:26:15 | INFO | train | epoch 036 | loss 5.547 | ppl 46.75 | wps 14613.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.654 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 63336
2022-03-04 04:26:15 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 04:26:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:32:12 | INFO | train_inner | epoch 037:     80 / 393 loss=5.492, ppl=45, wps=14487.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.638, loss_scale=4, train_wall=439, gb_free=10.1, wall=63693
2022-03-04 04:39:37 | INFO | train_inner | epoch 037:    180 / 393 loss=5.494, ppl=45.08, wps=14706.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.628, loss_scale=4, train_wall=441, gb_free=10.1, wall=64139
2022-03-04 04:47:03 | INFO | train_inner | epoch 037:    280 / 393 loss=5.545, ppl=46.69, wps=14706.5, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.661, loss_scale=4, train_wall=441, gb_free=10.1, wall=64584
2022-03-04 04:54:28 | INFO | train_inner | epoch 037:    380 / 393 loss=5.587, ppl=48.05, wps=14711.3, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.665, loss_scale=4, train_wall=441, gb_free=10.1, wall=65030
2022-03-04 04:55:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:55:31 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.274 | ppl 154.74 | wps 33989.2 | wpb 2034.1 | bsz 4 | num_updates 14513 | best_loss 7.032
2022-03-04 04:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14513 updates
2022-03-04 04:55:31 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 04:55:31 | INFO | train | epoch 037 | loss 5.526 | ppl 46.08 | wps 14651.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14513 | lr 0.000262495 | gnorm 0.645 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 65092
2022-03-04 04:55:31 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 04:55:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:57:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 05:02:03 | INFO | train_inner | epoch 038:     88 / 393 loss=5.451, ppl=43.75, wps=14347, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.647, loss_scale=4, train_wall=443, gb_free=10.1, wall=65485
2022-03-04 05:09:29 | INFO | train_inner | epoch 038:    188 / 393 loss=5.477, ppl=44.54, wps=14699.1, ups=0.22, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.639, loss_scale=4, train_wall=441, gb_free=10.1, wall=65930
2022-03-04 05:16:55 | INFO | train_inner | epoch 038:    288 / 393 loss=5.53, ppl=46.22, wps=14701.9, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.657, loss_scale=4, train_wall=441, gb_free=10.1, wall=66376
2022-03-04 05:24:21 | INFO | train_inner | epoch 038:    388 / 393 loss=5.566, ppl=47.38, wps=14701.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.663, loss_scale=4, train_wall=441, gb_free=10.1, wall=66822
2022-03-04 05:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:24:47 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.289 | ppl 156.34 | wps 34010.9 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 7.032
2022-03-04 05:24:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-04 05:24:47 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 05:24:47 | INFO | train | epoch 038 | loss 5.505 | ppl 45.41 | wps 14609.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.652 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 66849
2022-03-04 05:24:47 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 05:24:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:31:51 | INFO | train_inner | epoch 039:     95 / 393 loss=5.433, ppl=43.21, wps=14492.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.661, loss_scale=4, train_wall=439, gb_free=10.1, wall=67272
2022-03-04 05:39:16 | INFO | train_inner | epoch 039:    195 / 393 loss=5.469, ppl=44.29, wps=14704.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.65, loss_scale=8, train_wall=441, gb_free=10.1, wall=67718
2022-03-04 05:40:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 05:46:47 | INFO | train_inner | epoch 039:    296 / 393 loss=5.508, ppl=45.51, wps=14556.2, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.674, loss_scale=4, train_wall=445, gb_free=10.1, wall=68168
2022-03-04 05:53:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:54:04 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.288 | ppl 156.33 | wps 34059.4 | wpb 2034.1 | bsz 4 | num_updates 15297 | best_loss 7.032
2022-03-04 05:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15297 updates
2022-03-04 05:54:04 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 05:54:04 | INFO | train | epoch 039 | loss 5.486 | ppl 44.82 | wps 14611.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15297 | lr 0.00025568 | gnorm 0.661 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 68605
2022-03-04 05:54:04 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 05:54:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:54:17 | INFO | train_inner | epoch 040:      3 / 393 loss=5.537, ppl=46.42, wps=14489.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.657, loss_scale=4, train_wall=439, gb_free=10.1, wall=68618
2022-03-04 06:01:43 | INFO | train_inner | epoch 040:    103 / 393 loss=5.405, ppl=42.36, wps=14704.3, ups=0.22, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.655, loss_scale=4, train_wall=441, gb_free=10.1, wall=69064
2022-03-04 06:09:08 | INFO | train_inner | epoch 040:    203 / 393 loss=5.459, ppl=43.97, wps=14704.9, ups=0.22, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.666, loss_scale=4, train_wall=441, gb_free=10.1, wall=69510
2022-03-04 06:16:34 | INFO | train_inner | epoch 040:    303 / 393 loss=5.488, ppl=44.87, wps=14701.6, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.675, loss_scale=4, train_wall=441, gb_free=10.1, wall=69956
2022-03-04 06:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:23:20 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.316 | ppl 159.31 | wps 34167 | wpb 2034.1 | bsz 4 | num_updates 15690 | best_loss 7.032
2022-03-04 06:23:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15690 updates
2022-03-04 06:23:20 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 06:23:20 | INFO | train | epoch 040 | loss 5.467 | ppl 44.24 | wps 14647.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15690 | lr 0.000252458 | gnorm 0.659 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 70361
2022-03-04 06:23:20 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 06:23:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:24:05 | INFO | train_inner | epoch 041:     10 / 393 loss=5.511, ppl=45.59, wps=14485.8, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.643, loss_scale=8, train_wall=439, gb_free=10.1, wall=70406
2022-03-04 06:26:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 06:31:35 | INFO | train_inner | epoch 041:    111 / 393 loss=5.389, ppl=41.91, wps=14558.1, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.676, loss_scale=4, train_wall=445, gb_free=10.1, wall=70856
2022-03-04 06:39:00 | INFO | train_inner | epoch 041:    211 / 393 loss=5.438, ppl=43.35, wps=14703.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.662, loss_scale=4, train_wall=441, gb_free=10.1, wall=71302
2022-03-04 06:46:26 | INFO | train_inner | epoch 041:    311 / 393 loss=5.478, ppl=44.58, wps=14700.3, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.667, loss_scale=4, train_wall=441, gb_free=10.1, wall=71748
2022-03-04 06:52:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:52:36 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.332 | ppl 161.08 | wps 34072.5 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 7.032
2022-03-04 06:52:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-04 06:52:36 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 06:52:36 | INFO | train | epoch 041 | loss 5.449 | ppl 43.67 | wps 14609.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.671 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 72118
2022-03-04 06:52:36 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 06:52:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:53:57 | INFO | train_inner | epoch 042:     18 / 393 loss=5.478, ppl=44.58, wps=14485.3, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.669, loss_scale=4, train_wall=439, gb_free=10.1, wall=72198
2022-03-04 07:01:22 | INFO | train_inner | epoch 042:    118 / 393 loss=5.377, ppl=41.56, wps=14701.4, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.674, loss_scale=4, train_wall=441, gb_free=10.1, wall=72644
2022-03-04 07:08:48 | INFO | train_inner | epoch 042:    218 / 393 loss=5.421, ppl=42.86, wps=14695.1, ups=0.22, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.673, loss_scale=8, train_wall=441, gb_free=10.1, wall=73090
2022-03-04 07:11:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 07:16:19 | INFO | train_inner | epoch 042:    319 / 393 loss=5.468, ppl=44.25, wps=14555.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.68, loss_scale=4, train_wall=445, gb_free=10.1, wall=73540
2022-03-04 07:21:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:21:53 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.333 | ppl 161.19 | wps 34135 | wpb 2034.1 | bsz 4 | num_updates 16474 | best_loss 7.032
2022-03-04 07:21:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16474 updates
2022-03-04 07:21:53 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 07:21:53 | INFO | train | epoch 042 | loss 5.432 | ppl 43.16 | wps 14609.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.671 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 73874
2022-03-04 07:21:53 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 07:21:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:23:49 | INFO | train_inner | epoch 043:     26 / 393 loss=5.45, ppl=43.7, wps=14496.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.652, loss_scale=4, train_wall=439, gb_free=10.1, wall=73990
2022-03-04 07:31:14 | INFO | train_inner | epoch 043:    126 / 393 loss=5.366, ppl=41.24, wps=14703.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.695, loss_scale=4, train_wall=441, gb_free=10.1, wall=74436
2022-03-04 07:38:40 | INFO | train_inner | epoch 043:    226 / 393 loss=5.408, ppl=42.47, wps=14700.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.684, loss_scale=4, train_wall=441, gb_free=10.1, wall=74882
2022-03-04 07:46:06 | INFO | train_inner | epoch 043:    326 / 393 loss=5.447, ppl=43.61, wps=14704.6, ups=0.22, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.666, loss_scale=4, train_wall=441, gb_free=10.1, wall=75327
2022-03-04 07:51:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:51:09 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.346 | ppl 162.65 | wps 33953.6 | wpb 2034.1 | bsz 4 | num_updates 16867 | best_loss 7.032
2022-03-04 07:51:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16867 updates
2022-03-04 07:51:09 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 07:51:09 | INFO | train | epoch 043 | loss 5.415 | ppl 42.66 | wps 14648.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16867 | lr 0.00024349 | gnorm 0.676 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 75630
2022-03-04 07:51:09 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 07:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:53:36 | INFO | train_inner | epoch 044:     33 / 393 loss=5.429, ppl=43.07, wps=14489.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.671, loss_scale=8, train_wall=439, gb_free=10.1, wall=75778
2022-03-04 07:57:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 08:01:07 | INFO | train_inner | epoch 044:    134 / 393 loss=5.351, ppl=40.82, wps=14553, ups=0.22, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.685, loss_scale=4, train_wall=445, gb_free=10.1, wall=76228
2022-03-04 08:08:32 | INFO | train_inner | epoch 044:    234 / 393 loss=5.399, ppl=42.21, wps=14702.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.665, loss_scale=4, train_wall=441, gb_free=10.1, wall=76674
2022-03-04 08:15:58 | INFO | train_inner | epoch 044:    334 / 393 loss=5.434, ppl=43.22, wps=14703.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.695, loss_scale=4, train_wall=441, gb_free=10.1, wall=77119
2022-03-04 08:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:20:25 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.354 | ppl 163.65 | wps 34125.9 | wpb 2034.1 | bsz 4 | num_updates 17259 | best_loss 7.032
2022-03-04 08:20:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17259 updates
2022-03-04 08:20:25 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 08:20:25 | INFO | train | epoch 044 | loss 5.399 | ppl 42.2 | wps 14610.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17259 | lr 0.000240709 | gnorm 0.681 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 77387
2022-03-04 08:20:25 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 08:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:23:28 | INFO | train_inner | epoch 045:     41 / 393 loss=5.404, ppl=42.33, wps=14494.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.671, loss_scale=4, train_wall=439, gb_free=10.1, wall=77570
2022-03-04 08:30:54 | INFO | train_inner | epoch 045:    141 / 393 loss=5.342, ppl=40.56, wps=14698.9, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.676, loss_scale=4, train_wall=441, gb_free=10.1, wall=78015
2022-03-04 08:38:20 | INFO | train_inner | epoch 045:    241 / 393 loss=5.379, ppl=41.62, wps=14703.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.669, loss_scale=8, train_wall=441, gb_free=10.1, wall=78461
2022-03-04 08:42:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 08:45:50 | INFO | train_inner | epoch 045:    342 / 393 loss=5.428, ppl=43.04, wps=14555.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.692, loss_scale=4, train_wall=445, gb_free=10.1, wall=78911
2022-03-04 08:49:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:49:42 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.378 | ppl 166.39 | wps 34104.3 | wpb 2034.1 | bsz 4 | num_updates 17651 | best_loss 7.032
2022-03-04 08:49:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17651 updates
2022-03-04 08:49:42 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 08:49:42 | INFO | train | epoch 045 | loss 5.384 | ppl 41.76 | wps 14609.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17651 | lr 0.000238021 | gnorm 0.676 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 79143
2022-03-04 08:49:42 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 08:49:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:53:20 | INFO | train_inner | epoch 046:     49 / 393 loss=5.376, ppl=41.53, wps=14490.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.681, loss_scale=4, train_wall=439, gb_free=10.1, wall=79362
2022-03-04 09:00:46 | INFO | train_inner | epoch 046:    149 / 393 loss=5.327, ppl=40.14, wps=14704.8, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.667, loss_scale=4, train_wall=441, gb_free=10.1, wall=79807
2022-03-04 09:08:12 | INFO | train_inner | epoch 046:    249 / 393 loss=5.368, ppl=41.31, wps=14701.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.682, loss_scale=4, train_wall=441, gb_free=10.1, wall=80253
2022-03-04 09:15:37 | INFO | train_inner | epoch 046:    349 / 393 loss=5.414, ppl=42.62, wps=14705.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.695, loss_scale=4, train_wall=441, gb_free=10.1, wall=80699
2022-03-04 09:18:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:18:58 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.379 | ppl 166.45 | wps 34050.1 | wpb 2034.1 | bsz 4 | num_updates 18044 | best_loss 7.032
2022-03-04 09:18:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18044 updates
2022-03-04 09:18:58 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 09:18:58 | INFO | train | epoch 046 | loss 5.369 | ppl 41.33 | wps 14649.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18044 | lr 0.000235415 | gnorm 0.682 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 80899
2022-03-04 09:18:58 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 09:18:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:23:08 | INFO | train_inner | epoch 047:     56 / 393 loss=5.353, ppl=40.88, wps=14492.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.691, loss_scale=8, train_wall=439, gb_free=10.1, wall=81149
2022-03-04 09:28:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:30:38 | INFO | train_inner | epoch 047:    157 / 393 loss=5.32, ppl=39.94, wps=14558.4, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.683, loss_scale=4, train_wall=445, gb_free=10.1, wall=81599
2022-03-04 09:38:03 | INFO | train_inner | epoch 047:    257 / 393 loss=5.359, ppl=41.03, wps=14706.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.699, loss_scale=4, train_wall=441, gb_free=10.1, wall=82045
2022-03-04 09:45:29 | INFO | train_inner | epoch 047:    357 / 393 loss=5.401, ppl=42.24, wps=14700.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.71, loss_scale=4, train_wall=441, gb_free=10.1, wall=82490
2022-03-04 09:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:48:14 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.388 | ppl 167.5 | wps 34161.6 | wpb 2034.1 | bsz 4 | num_updates 18436 | best_loss 7.032
2022-03-04 09:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18436 updates
2022-03-04 09:48:14 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 09:48:14 | INFO | train | epoch 047 | loss 5.355 | ppl 40.92 | wps 14612.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18436 | lr 0.000232898 | gnorm 0.695 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 82655
2022-03-04 09:48:14 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 09:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:52:59 | INFO | train_inner | epoch 048:     64 / 393 loss=5.319, ppl=39.93, wps=14488.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.664, loss_scale=4, train_wall=439, gb_free=10.1, wall=82941
2022-03-04 10:00:25 | INFO | train_inner | epoch 048:    164 / 393 loss=5.311, ppl=39.7, wps=14704.8, ups=0.22, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.708, loss_scale=4, train_wall=441, gb_free=10.1, wall=83387
2022-03-04 10:07:51 | INFO | train_inner | epoch 048:    264 / 393 loss=5.355, ppl=40.94, wps=14705.3, ups=0.22, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.66, loss_scale=8, train_wall=441, gb_free=10.1, wall=83832
2022-03-04 10:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 10:15:21 | INFO | train_inner | epoch 048:    365 / 393 loss=5.39, ppl=41.92, wps=14560.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.707, loss_scale=4, train_wall=445, gb_free=10.1, wall=84282
2022-03-04 10:17:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:17:30 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.411 | ppl 170.15 | wps 33994.6 | wpb 2034.1 | bsz 4 | num_updates 18828 | best_loss 7.032
2022-03-04 10:17:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18828 updates
2022-03-04 10:17:30 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 10:17:30 | INFO | train | epoch 048 | loss 5.341 | ppl 40.54 | wps 14612.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18828 | lr 0.000230461 | gnorm 0.686 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 84412
2022-03-04 10:17:30 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 10:17:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:22:51 | INFO | train_inner | epoch 049:     72 / 393 loss=5.305, ppl=39.52, wps=14490.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.707, loss_scale=4, train_wall=439, gb_free=10.1, wall=84733
2022-03-04 10:30:17 | INFO | train_inner | epoch 049:    172 / 393 loss=5.291, ppl=39.15, wps=14705.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.692, loss_scale=4, train_wall=441, gb_free=10.1, wall=85178
2022-03-04 10:37:42 | INFO | train_inner | epoch 049:    272 / 393 loss=5.341, ppl=40.53, wps=14703.9, ups=0.22, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.728, loss_scale=4, train_wall=441, gb_free=10.1, wall=85624
2022-03-04 10:45:08 | INFO | train_inner | epoch 049:    372 / 393 loss=5.386, ppl=41.81, wps=14704.6, ups=0.22, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.684, loss_scale=4, train_wall=441, gb_free=10.1, wall=86070
2022-03-04 10:46:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:46:46 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.415 | ppl 170.69 | wps 34167.5 | wpb 2034.1 | bsz 4 | num_updates 19221 | best_loss 7.032
2022-03-04 10:46:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19221 updates
2022-03-04 10:46:46 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 10:46:46 | INFO | train | epoch 049 | loss 5.328 | ppl 40.17 | wps 14650.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 19221 | lr 0.000228093 | gnorm 0.706 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 86168
2022-03-04 10:46:46 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 10:46:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:52:38 | INFO | train_inner | epoch 050:     79 / 393 loss=5.277, ppl=38.78, wps=14492.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.714, loss_scale=8, train_wall=439, gb_free=10.1, wall=86520
2022-03-04 10:57:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:00:09 | INFO | train_inner | epoch 050:    180 / 393 loss=5.287, ppl=39.04, wps=14555.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.72, loss_scale=4, train_wall=445, gb_free=10.1, wall=86970
2022-03-04 11:07:34 | INFO | train_inner | epoch 050:    280 / 393 loss=5.331, ppl=40.26, wps=14705, ups=0.22, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.679, loss_scale=4, train_wall=441, gb_free=10.1, wall=87416
2022-03-04 11:15:00 | INFO | train_inner | epoch 050:    380 / 393 loss=5.369, ppl=41.33, wps=14704.5, ups=0.22, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.713, loss_scale=4, train_wall=441, gb_free=10.1, wall=87861
2022-03-04 11:15:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:16:02 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.427 | ppl 172.08 | wps 34110.9 | wpb 2034.1 | bsz 4 | num_updates 19613 | best_loss 7.032
2022-03-04 11:16:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19613 updates
2022-03-04 11:16:02 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 11:16:02 | INFO | train | epoch 050 | loss 5.314 | ppl 39.79 | wps 14611.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19613 | lr 0.000225802 | gnorm 0.707 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 87924
2022-03-04 11:16:02 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 11:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:22:30 | INFO | train_inner | epoch 051:     87 / 393 loss=5.265, ppl=38.46, wps=14493.3, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.725, loss_scale=4, train_wall=439, gb_free=10.1, wall=88312
2022-03-04 11:29:56 | INFO | train_inner | epoch 051:    187 / 393 loss=5.273, ppl=38.66, wps=14702, ups=0.22, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.696, loss_scale=4, train_wall=441, gb_free=10.1, wall=88757
2022-03-04 11:37:22 | INFO | train_inner | epoch 051:    287 / 393 loss=5.324, ppl=40.06, wps=14703, ups=0.22, wpb=65530.9, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.698, loss_scale=8, train_wall=441, gb_free=10.1, wall=89203
2022-03-04 11:38:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:44:52 | INFO | train_inner | epoch 051:    388 / 393 loss=5.358, ppl=41.01, wps=14561.6, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.723, loss_scale=4, train_wall=445, gb_free=10.1, wall=89653
2022-03-04 11:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:45:18 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.426 | ppl 171.93 | wps 33939.1 | wpb 2034.1 | bsz 4 | num_updates 20005 | best_loss 7.032
2022-03-04 11:45:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20005 updates
2022-03-04 11:45:18 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 11:45:18 | INFO | train | epoch 051 | loss 5.303 | ppl 39.48 | wps 14612 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20005 | lr 0.000223579 | gnorm 0.709 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 89680
2022-03-04 11:45:18 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 11:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:52:22 | INFO | train_inner | epoch 052:     95 / 393 loss=5.229, ppl=37.5, wps=14489.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.709, loss_scale=4, train_wall=439, gb_free=10.1, wall=90103
2022-03-04 11:59:48 | INFO | train_inner | epoch 052:    195 / 393 loss=5.273, ppl=38.65, wps=14705.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.69, loss_scale=4, train_wall=441, gb_free=10.1, wall=90549
2022-03-04 12:07:13 | INFO | train_inner | epoch 052:    295 / 393 loss=5.314, ppl=39.78, wps=14703.8, ups=0.22, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.705, loss_scale=4, train_wall=441, gb_free=10.1, wall=90995
2022-03-04 12:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:14:34 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.43 | ppl 172.5 | wps 34102.9 | wpb 2034.1 | bsz 4 | num_updates 20398 | best_loss 7.032
2022-03-04 12:14:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20398 updates
2022-03-04 12:14:34 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 12:14:34 | INFO | train | epoch 052 | loss 5.291 | ppl 39.15 | wps 14650.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20398 | lr 0.000221415 | gnorm 0.704 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 91436
2022-03-04 12:14:34 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 12:14:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:14:43 | INFO | train_inner | epoch 053:      2 / 393 loss=5.35, ppl=40.78, wps=14494.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.716, loss_scale=4, train_wall=439, gb_free=10.1, wall=91445
2022-03-04 12:17:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 12:22:14 | INFO | train_inner | epoch 053:    103 / 393 loss=5.213, ppl=37.1, wps=14560.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.714, loss_scale=4, train_wall=445, gb_free=10.1, wall=91895
2022-03-04 12:29:39 | INFO | train_inner | epoch 053:    203 / 393 loss=5.268, ppl=38.53, wps=14703.5, ups=0.22, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.696, loss_scale=4, train_wall=441, gb_free=10.1, wall=92341
2022-03-04 12:37:05 | INFO | train_inner | epoch 053:    303 / 393 loss=5.298, ppl=39.35, wps=14703.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.684, loss_scale=4, train_wall=441, gb_free=10.1, wall=92786
2022-03-04 12:43:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:43:50 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.462 | ppl 176.34 | wps 33970.1 | wpb 2034.1 | bsz 4 | num_updates 20790 | best_loss 7.032
2022-03-04 12:43:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20790 updates
2022-03-04 12:43:50 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 12:43:50 | INFO | train | epoch 053 | loss 5.279 | ppl 38.82 | wps 14612.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20790 | lr 0.000219317 | gnorm 0.706 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 93192
2022-03-04 12:43:51 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 12:43:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:44:35 | INFO | train_inner | epoch 054:     10 / 393 loss=5.327, ppl=40.13, wps=14493.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.733, loss_scale=4, train_wall=439, gb_free=10.1, wall=93237
2022-03-04 12:52:01 | INFO | train_inner | epoch 054:    110 / 393 loss=5.207, ppl=36.94, wps=14705.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.7, loss_scale=4, train_wall=441, gb_free=10.1, wall=93682
2022-03-04 12:58:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 12:59:31 | INFO | train_inner | epoch 054:    211 / 393 loss=5.256, ppl=38.22, wps=14555, ups=0.22, wpb=65535.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.714, loss_scale=4, train_wall=445, gb_free=10.1, wall=94132
2022-03-04 13:06:57 | INFO | train_inner | epoch 054:    311 / 393 loss=5.297, ppl=39.32, wps=14709.1, ups=0.22, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.714, loss_scale=4, train_wall=441, gb_free=10.1, wall=94578
2022-03-04 13:13:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:13:07 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.451 | ppl 174.92 | wps 34048.5 | wpb 2034.1 | bsz 4 | num_updates 21182 | best_loss 7.032
2022-03-04 13:13:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21182 updates
2022-03-04 13:13:07 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 13:13:07 | INFO | train | epoch 054 | loss 5.268 | ppl 38.52 | wps 14612.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21182 | lr 0.000217278 | gnorm 0.712 | loss_scale 4 | train_wall 1730 | gb_free 10.1 | wall 94948
2022-03-04 13:13:07 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 13:13:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:14:27 | INFO | train_inner | epoch 055:     18 / 393 loss=5.303, ppl=39.47, wps=14489.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.724, loss_scale=4, train_wall=439, gb_free=10.1, wall=95028
2022-03-04 13:21:53 | INFO | train_inner | epoch 055:    118 / 393 loss=5.207, ppl=36.94, wps=14703.3, ups=0.22, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.706, loss_scale=4, train_wall=441, gb_free=10.1, wall=95474
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
