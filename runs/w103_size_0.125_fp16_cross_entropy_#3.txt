Sender: LSF System <lsfadmin@eu-g2-03>
Subject: Job 207129918: <w103_size_0.125_fp16_cross_entropy_#3> in cluster <euler> Exited

Job <w103_size_0.125_fp16_cross_entropy_#3> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:14:44 2022
Job was executed on host(s) <eu-g2-03>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:15:13 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:15:13 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575613 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   97349.47 sec.
    Max Memory :                                 7490 MB
    Average Memory :                             4225.61 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               12510.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   94208 sec.
    Turnaround time :                            94238 sec.

The output (if any) follows:

2022-03-04 09:15:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575613, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:15:28 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-04 09:15:31 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-04 09:15:31 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:15:31 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:15:31 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-04 09:15:31 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-04 09:15:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:15:31 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-04 09:15:40 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:15:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:15:40 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-03-04 09:15:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:15:40 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:15:40 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:15:40 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 09:15:40 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 09:15:40 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:15:41 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-04 09:15:41 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:15:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:15:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:16:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:16:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:17:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:21:47 | INFO | train_inner | epoch 001:    105 / 196 loss=16.444, ppl=89165.3, wps=19543.4, ups=0.3, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.634, loss_scale=4, train_wall=340, gb_free=7.2, wall=367
2022-03-04 09:26:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:26:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.08 | ppl 8657.75 | wps 37401 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-04 09:26:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-04 09:26:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:26:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:52:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.08) (writing took 1563.9951469060034 seconds)
2022-03-04 09:52:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:52:59 | INFO | train | epoch 001 | loss 15.311 | ppl 40652.8 | wps 5643.2 | ups 0.09 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.62 | loss_scale 8 | train_wall 619 | gb_free 7.2 | wall 2238
2022-03-04 09:52:59 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:53:28 | INFO | train_inner | epoch 002:      9 / 196 loss=13.972, ppl=16074.2, wps=3437.6, ups=0.05, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.485, loss_scale=8, train_wall=307, gb_free=7.2, wall=2268
2022-03-04 09:59:01 | INFO | train_inner | epoch 002:    109 / 196 loss=11.982, ppl=4046.02, wps=19721.7, ups=0.3, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.027, loss_scale=16, train_wall=307, gb_free=7.2, wall=2600
2022-03-04 10:03:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:03:55 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.296 | ppl 1257.4 | wps 38148.7 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.296
2022-03-04 10:03:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-04 10:03:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:04:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:30:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.296) (writing took 1608.6551496181637 seconds)
2022-03-04 10:30:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 10:30:44 | INFO | train | epoch 002 | loss 11.445 | ppl 2788.22 | wps 5662.5 | ups 0.09 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.86 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 4504
2022-03-04 10:30:44 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 10:30:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:31:27 | INFO | train_inner | epoch 003:     13 / 196 loss=10.606, ppl=1559.1, wps=3358.1, ups=0.05, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.607, loss_scale=16, train_wall=307, gb_free=7.2, wall=4547
2022-03-04 10:37:00 | INFO | train_inner | epoch 003:    113 / 196 loss=10.031, ppl=1046.49, wps=19686.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.533, loss_scale=32, train_wall=308, gb_free=7.2, wall=4880
2022-03-04 10:38:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:41:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:41:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.584 | ppl 767.29 | wps 38227.3 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.584
2022-03-04 10:41:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-04 10:41:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:41:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:41:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.584) (writing took 7.507391985505819 seconds)
2022-03-04 10:41:49 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 10:41:49 | INFO | train | epoch 003 | loss 9.916 | ppl 966.04 | wps 19191.4 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.571 | loss_scale 32 | train_wall 603 | gb_free 7.2 | wall 5169
2022-03-04 10:41:49 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 10:41:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:42:49 | INFO | train_inner | epoch 004:     18 / 196 loss=9.687, ppl=824.42, wps=18726.1, ups=0.29, wpb=65367, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.624, loss_scale=32, train_wall=311, gb_free=7.2, wall=5229
2022-03-04 10:46:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:48:26 | INFO | train_inner | epoch 004:    119 / 196 loss=9.39, ppl=671.12, wps=19464.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.67, loss_scale=32, train_wall=311, gb_free=7.2, wall=5565
2022-03-04 10:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:52:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:52:47 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.106 | ppl 551.08 | wps 37822 | wpb 510.9 | bsz 1 | num_updates 776 | best_loss 9.106
2022-03-04 10:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 776 updates
2022-03-04 10:52:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:52:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:19:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 4 @ 776 updates, score 9.106) (writing took 1588.8641405440867 seconds)
2022-03-04 11:19:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 11:19:16 | INFO | train | epoch 004 | loss 9.327 | ppl 642.19 | wps 5649.8 | ups 0.09 | wpb 65447.1 | bsz 127.8 | num_updates 776 | lr 9.70806e-05 | gnorm 0.709 | loss_scale 16 | train_wall 604 | gb_free 7.2 | wall 7416
2022-03-04 11:19:16 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 11:19:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:20:36 | INFO | train_inner | epoch 005:     24 / 196 loss=9.156, ppl=570.31, wps=3386.2, ups=0.05, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.822, loss_scale=16, train_wall=310, gb_free=7.2, wall=7496
2022-03-04 11:26:10 | INFO | train_inner | epoch 005:    124 / 196 loss=8.898, ppl=477.14, wps=19631.9, ups=0.3, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.839, loss_scale=32, train_wall=309, gb_free=7.2, wall=7830
2022-03-04 11:30:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:30:16 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.69 | ppl 412.93 | wps 37787 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 8.69
2022-03-04 11:30:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-04 11:30:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:30:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 5 @ 972 updates, score 8.69) (writing took 1347.2507820408791 seconds)
2022-03-04 11:52:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 11:52:43 | INFO | train | epoch 005 | loss 8.856 | ppl 463.33 | wps 6392.7 | ups 0.1 | wpb 65448 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.899 | loss_scale 32 | train_wall 604 | gb_free 7.2 | wall 9422
2022-03-04 11:52:43 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 11:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:54:19 | INFO | train_inner | epoch 006:     29 / 196 loss=8.693, ppl=413.94, wps=3870.3, ups=0.06, wpb=65363.4, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.947, loss_scale=32, train_wall=310, gb_free=7.2, wall=9519
2022-03-04 11:59:51 | INFO | train_inner | epoch 006:    129 / 196 loss=8.486, ppl=358.48, wps=19727.5, ups=0.3, wpb=65532.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.972, loss_scale=32, train_wall=307, gb_free=7.2, wall=9851
2022-03-04 12:01:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:03:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:03:39 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.378 | ppl 332.62 | wps 38227.3 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.378
2022-03-04 12:03:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-04 12:03:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 12:03:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 12:03:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.378) (writing took 12.040232909843326 seconds)
2022-03-04 12:03:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 12:03:51 | INFO | train | epoch 006 | loss 8.46 | ppl 352.08 | wps 19012.2 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.958 | loss_scale 32 | train_wall 601 | gb_free 7.2 | wall 10090
2022-03-04 12:03:51 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 12:03:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:05:44 | INFO | train_inner | epoch 007:     34 / 196 loss=8.32, ppl=319.66, wps=18512.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.992, loss_scale=32, train_wall=310, gb_free=7.2, wall=10204
2022-03-04 12:08:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:11:21 | INFO | train_inner | epoch 007:    135 / 196 loss=8.15, ppl=284.11, wps=19472.5, ups=0.3, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.971, loss_scale=32, train_wall=311, gb_free=7.2, wall=10540
2022-03-04 12:14:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:14:49 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.108 | ppl 275.84 | wps 38168.2 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.108
2022-03-04 12:14:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-04 12:14:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 12:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 12:14:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 7 @ 1361 updates, score 8.108) (writing took 7.54140130802989 seconds)
2022-03-04 12:14:56 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 12:14:56 | INFO | train | epoch 007 | loss 8.132 | ppl 280.52 | wps 19178.3 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.99 | loss_scale 32 | train_wall 603 | gb_free 7.2 | wall 10756
2022-03-04 12:14:56 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 12:14:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:15:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:17:09 | INFO | train_inner | epoch 008:     40 / 196 loss=7.997, ppl=255.4, wps=18752.5, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.985, loss_scale=32, train_wall=310, gb_free=7.2, wall=10889
2022-03-04 12:22:42 | INFO | train_inner | epoch 008:    140 / 196 loss=7.843, ppl=229.57, wps=19717.2, ups=0.3, wpb=65532.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=1.03, loss_scale=32, train_wall=307, gb_free=7.2, wall=11221
2022-03-04 12:23:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:25:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:25:53 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.874 | ppl 234.55 | wps 38198.8 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 7.874
2022-03-04 12:25:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-04 12:25:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 12:25:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 12:26:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 8 @ 1555 updates, score 7.874) (writing took 10.9856247138232 seconds)
2022-03-04 12:26:04 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 12:26:04 | INFO | train | epoch 008 | loss 7.836 | ppl 228.43 | wps 19013.1 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 1.011 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 11424
2022-03-04 12:26:04 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 12:26:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:28:34 | INFO | train_inner | epoch 009:     45 / 196 loss=7.702, ppl=208.27, wps=18558.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=1.022, loss_scale=32, train_wall=310, gb_free=7.2, wall=11574
2022-03-04 12:30:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:34:10 | INFO | train_inner | epoch 009:    146 / 196 loss=7.565, ppl=189.36, wps=19513.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.99, loss_scale=32, train_wall=311, gb_free=7.2, wall=11909
2022-03-04 12:36:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:37:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.665 | ppl 202.9 | wps 38502.4 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 7.665
2022-03-04 12:37:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1750 updates
2022-03-04 12:37:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 12:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 13:04:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 9 @ 1750 updates, score 7.665) (writing took 1647.5803584344685 seconds)
2022-03-04 13:04:28 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 13:04:28 | INFO | train | epoch 009 | loss 7.561 | ppl 188.88 | wps 5538.4 | ups 0.08 | wpb 65447.5 | bsz 127.8 | num_updates 1750 | lr 0.000218806 | gnorm 0.993 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 13728
2022-03-04 13:04:28 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 13:04:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:05:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:07:18 | INFO | train_inner | epoch 010:     51 / 196 loss=7.424, ppl=171.77, wps=3287.2, ups=0.05, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.981, loss_scale=32, train_wall=310, gb_free=7.2, wall=13898
2022-03-04 13:12:50 | INFO | train_inner | epoch 010:    151 / 196 loss=7.307, ppl=158.37, wps=19749.2, ups=0.3, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.996, loss_scale=64, train_wall=307, gb_free=7.2, wall=14230
2022-03-04 13:13:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:15:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:15:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.527 | ppl 184.5 | wps 38524 | wpb 510.9 | bsz 1 | num_updates 1944 | best_loss 7.527
2022-03-04 13:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1944 updates
2022-03-04 13:15:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 13:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 13:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 10 @ 1944 updates, score 7.527) (writing took 1577.3891075253487 seconds)
2022-03-04 13:41:42 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 13:41:42 | INFO | train | epoch 010 | loss 7.305 | ppl 158.16 | wps 5685.1 | ups 0.09 | wpb 65447.1 | bsz 127.8 | num_updates 1944 | lr 0.000243051 | gnorm 0.995 | loss_scale 32 | train_wall 601 | gb_free 7.2 | wall 15961
2022-03-04 13:41:42 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 13:41:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:44:48 | INFO | train_inner | epoch 011:     56 / 196 loss=7.166, ppl=143.65, wps=3407.8, ups=0.05, wpb=65359.9, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.975, loss_scale=32, train_wall=310, gb_free=7.2, wall=16148
2022-03-04 13:47:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:50:24 | INFO | train_inner | epoch 011:    157 / 196 loss=7.064, ppl=133.82, wps=19499.9, ups=0.3, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.966, loss_scale=32, train_wall=311, gb_free=7.2, wall=16484
2022-03-04 13:52:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:52:39 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.36 | ppl 164.29 | wps 38199.4 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 7.36
2022-03-04 13:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2139 updates
2022-03-04 13:52:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 13:52:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 13:52:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 11 @ 2139 updates, score 7.36) (writing took 7.547296388074756 seconds)
2022-03-04 13:52:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 13:52:46 | INFO | train | epoch 011 | loss 7.066 | ppl 134.02 | wps 19201.8 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 2139 | lr 0.000267422 | gnorm 0.965 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 16626
2022-03-04 13:52:46 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 13:52:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:55:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:56:12 | INFO | train_inner | epoch 012:     62 / 196 loss=6.92, ppl=121.12, wps=18774.8, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.951, loss_scale=32, train_wall=310, gb_free=7.2, wall=16832
2022-03-04 13:58:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:01:48 | INFO | train_inner | epoch 012:    163 / 196 loss=6.844, ppl=114.86, wps=19505.3, ups=0.3, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.942, loss_scale=16, train_wall=311, gb_free=7.2, wall=17168
2022-03-04 14:03:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:03:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.245 | ppl 151.72 | wps 37955.2 | wpb 510.9 | bsz 1 | num_updates 2333 | best_loss 7.245
2022-03-04 14:03:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2333 updates
2022-03-04 14:03:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 14:03:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 14:30:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 12 @ 2333 updates, score 7.245) (writing took 1621.4194790963084 seconds)
2022-03-04 14:30:45 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 14:30:45 | INFO | train | epoch 012 | loss 6.844 | ppl 114.86 | wps 5572.1 | ups 0.09 | wpb 65447.1 | bsz 127.8 | num_updates 2333 | lr 0.000291667 | gnorm 0.966 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 18904
2022-03-04 14:30:45 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 14:30:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:34:28 | INFO | train_inner | epoch 013:     67 / 196 loss=6.701, ppl=104.02, wps=3334.5, ups=0.05, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.944, loss_scale=32, train_wall=308, gb_free=7.2, wall=19128
2022-03-04 14:40:02 | INFO | train_inner | epoch 013:    167 / 196 loss=6.628, ppl=98.93, wps=19622, ups=0.3, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.927, loss_scale=32, train_wall=309, gb_free=7.2, wall=19462
2022-03-04 14:40:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:41:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.158 | ppl 142.84 | wps 37891.6 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 7.158
2022-03-04 14:41:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2528 updates
2022-03-04 14:41:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 14:41:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 15:07:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 13 @ 2528 updates, score 7.158) (writing took 1515.5522621218115 seconds)
2022-03-04 15:07:00 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 15:07:00 | INFO | train | epoch 013 | loss 6.633 | ppl 99.23 | wps 5867.6 | ups 0.09 | wpb 65447.5 | bsz 127.8 | num_updates 2528 | lr 0.000316037 | gnorm 0.919 | loss_scale 32 | train_wall 604 | gb_free 7.2 | wall 21080
2022-03-04 15:07:00 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 15:07:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:10:59 | INFO | train_inner | epoch 014:     72 / 196 loss=6.499, ppl=90.42, wps=3521.3, ups=0.05, wpb=65367, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.916, loss_scale=32, train_wall=310, gb_free=7.2, wall=21318
2022-03-04 15:12:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:13:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:16:37 | INFO | train_inner | epoch 014:    174 / 196 loss=6.432, ppl=86.34, wps=19375.2, ups=0.3, wpb=65532.4, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.891, loss_scale=16, train_wall=313, gb_free=7.2, wall=21657
2022-03-04 15:17:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:17:55 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.083 | ppl 135.61 | wps 38415.1 | wpb 510.9 | bsz 1 | num_updates 2722 | best_loss 7.083
2022-03-04 15:17:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2722 updates
2022-03-04 15:17:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 15:17:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 15:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 14 @ 2722 updates, score 7.083) (writing took 1663.8673903122544 seconds)
2022-03-04 15:45:39 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 15:45:39 | INFO | train | epoch 014 | loss 6.44 | ppl 86.79 | wps 5475.1 | ups 0.08 | wpb 65447.1 | bsz 127.8 | num_updates 2722 | lr 0.000340282 | gnorm 0.91 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 23399
2022-03-04 15:45:39 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 15:45:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:49:58 | INFO | train_inner | epoch 015:     78 / 196 loss=6.292, ppl=78.36, wps=3267.4, ups=0.05, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.927, loss_scale=32, train_wall=306, gb_free=7.2, wall=23657
2022-03-04 15:53:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:55:33 | INFO | train_inner | epoch 015:    179 / 196 loss=6.263, ppl=76.82, wps=19532.1, ups=0.3, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.921, loss_scale=16, train_wall=310, gb_free=7.2, wall=23993
2022-03-04 15:56:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:56:35 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.017 | ppl 129.54 | wps 37797.3 | wpb 510.9 | bsz 1 | num_updates 2917 | best_loss 7.017
2022-03-04 15:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2917 updates
2022-03-04 15:56:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 15:56:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 16:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 15 @ 2917 updates, score 7.017) (writing took 1607.5471673496068 seconds)
2022-03-04 16:23:22 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 16:23:22 | INFO | train | epoch 015 | loss 6.261 | ppl 76.68 | wps 5638.6 | ups 0.09 | wpb 65447.5 | bsz 127.8 | num_updates 2917 | lr 0.000364652 | gnorm 0.901 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 25662
2022-03-04 16:23:22 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 16:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:27:58 | INFO | train_inner | epoch 016:     83 / 196 loss=6.117, ppl=69.41, wps=3361.4, ups=0.05, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.883, loss_scale=32, train_wall=306, gb_free=7.2, wall=25937
2022-03-04 16:33:30 | INFO | train_inner | epoch 016:    183 / 196 loss=6.103, ppl=68.73, wps=19734.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.886, loss_scale=32, train_wall=307, gb_free=7.2, wall=26269
2022-03-04 16:33:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:34:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:34:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.037 | ppl 131.3 | wps 38588.5 | wpb 510.9 | bsz 1 | num_updates 3112 | best_loss 7.017
2022-03-04 16:34:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3112 updates
2022-03-04 16:34:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:34:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:34:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 16 @ 3112 updates, score 7.037) (writing took 4.347353793680668 seconds)
2022-03-04 16:34:22 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 16:34:22 | INFO | train | epoch 016 | loss 6.095 | ppl 68.36 | wps 19338 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3112 | lr 0.000389022 | gnorm 0.902 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 26322
2022-03-04 16:34:22 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 16:34:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:39:14 | INFO | train_inner | epoch 017:     88 / 196 loss=5.94, ppl=61.41, wps=18965, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.903, loss_scale=16, train_wall=309, gb_free=7.2, wall=26614
2022-03-04 16:44:47 | INFO | train_inner | epoch 017:    188 / 196 loss=5.951, ppl=61.88, wps=19720.7, ups=0.3, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.883, loss_scale=32, train_wall=307, gb_free=7.2, wall=26946
2022-03-04 16:45:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:45:18 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.084 | ppl 135.65 | wps 38320.5 | wpb 510.9 | bsz 1 | num_updates 3308 | best_loss 7.017
2022-03-04 16:45:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3308 updates
2022-03-04 16:45:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:45:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:45:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 17 @ 3308 updates, score 7.084) (writing took 4.309579949826002 seconds)
2022-03-04 16:45:23 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 16:45:23 | INFO | train | epoch 017 | loss 5.938 | ppl 61.33 | wps 19420.4 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 3308 | lr 0.000413517 | gnorm 0.884 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 26982
2022-03-04 16:45:23 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 16:45:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:48:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:49:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:50:35 | INFO | train_inner | epoch 018:     94 / 196 loss=5.79, ppl=55.32, wps=18774.9, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.9, loss_scale=16, train_wall=313, gb_free=7.2, wall=27295
2022-03-04 16:56:07 | INFO | train_inner | epoch 018:    194 / 196 loss=5.806, ppl=55.95, wps=19754.8, ups=0.3, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.852, loss_scale=16, train_wall=307, gb_free=7.2, wall=27626
2022-03-04 16:56:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:56:18 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.028 | ppl 130.47 | wps 38261.7 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 7.017
2022-03-04 16:56:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3502 updates
2022-03-04 16:56:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:56:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:56:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 18 @ 3502 updates, score 7.028) (writing took 4.20450596511364 seconds)
2022-03-04 16:56:22 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 16:56:22 | INFO | train | epoch 018 | loss 5.79 | ppl 55.33 | wps 19245.2 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 3502 | lr 0.000437762 | gnorm 0.876 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 27642
2022-03-04 16:56:22 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 16:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:59:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:01:51 | INFO | train_inner | epoch 019:     99 / 196 loss=5.628, ppl=49.47, wps=18971, ups=0.29, wpb=65367, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.888, loss_scale=16, train_wall=309, gb_free=7.2, wall=27971
2022-03-04 17:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:07:18 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.065 | ppl 133.91 | wps 37954.9 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 7.017
2022-03-04 17:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3697 updates
2022-03-04 17:07:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:07:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:07:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 19 @ 3697 updates, score 7.065) (writing took 4.232036652043462 seconds)
2022-03-04 17:07:23 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 17:07:23 | INFO | train | epoch 019 | loss 5.652 | ppl 50.28 | wps 19330.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3697 | lr 0.000462133 | gnorm 0.884 | loss_scale 32 | train_wall 601 | gb_free 7.2 | wall 28302
2022-03-04 17:07:23 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 17:07:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:07:33 | INFO | train_inner | epoch 020:      3 / 196 loss=5.671, ppl=50.95, wps=19139.8, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.881, loss_scale=32, train_wall=307, gb_free=7.2, wall=28312
2022-03-04 17:09:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:13:08 | INFO | train_inner | epoch 020:    104 / 196 loss=5.498, ppl=45.19, wps=19563.6, ups=0.3, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.883, loss_scale=16, train_wall=310, gb_free=7.2, wall=28647
2022-03-04 17:17:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:18:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:18:18 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.157 | ppl 142.73 | wps 38602.3 | wpb 510.9 | bsz 1 | num_updates 3891 | best_loss 7.017
2022-03-04 17:18:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3891 updates
2022-03-04 17:18:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 20 @ 3891 updates, score 7.157) (writing took 4.402146594598889 seconds)
2022-03-04 17:18:22 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 17:18:22 | INFO | train | epoch 020 | loss 5.52 | ppl 45.89 | wps 19250.1 | ups 0.29 | wpb 65448.9 | bsz 127.8 | num_updates 3891 | lr 0.000486378 | gnorm 0.883 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 28962
2022-03-04 17:18:22 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 17:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:18:52 | INFO | train_inner | epoch 021:      9 / 196 loss=5.528, ppl=46.15, wps=18980.9, ups=0.29, wpb=65367, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.877, loss_scale=16, train_wall=309, gb_free=7.2, wall=28992
2022-03-04 17:24:25 | INFO | train_inner | epoch 021:    109 / 196 loss=5.362, ppl=41.14, wps=19699.2, ups=0.3, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.856, loss_scale=16, train_wall=308, gb_free=7.2, wall=29324
2022-03-04 17:29:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:29:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.112 | ppl 138.33 | wps 38180.2 | wpb 510.9 | bsz 1 | num_updates 4087 | best_loss 7.017
2022-03-04 17:29:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4087 updates
2022-03-04 17:29:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:29:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:29:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 21 @ 4087 updates, score 7.112) (writing took 4.250500287860632 seconds)
2022-03-04 17:29:23 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 17:29:23 | INFO | train | epoch 021 | loss 5.39 | ppl 41.93 | wps 19409.7 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 4087 | lr 0.00049465 | gnorm 0.858 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 29623
2022-03-04 17:29:23 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 17:29:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:30:07 | INFO | train_inner | epoch 022:     13 / 196 loss=5.396, ppl=42.11, wps=19125.7, ups=0.29, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.874, loss_scale=32, train_wall=307, gb_free=7.2, wall=29666
2022-03-04 17:32:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:34:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:35:46 | INFO | train_inner | epoch 022:    115 / 196 loss=5.235, ppl=37.66, wps=19308.3, ups=0.29, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.852, loss_scale=16, train_wall=314, gb_free=7.2, wall=30006
2022-03-04 17:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:40:20 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.2 | ppl 147.07 | wps 38098.7 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 7.017
2022-03-04 17:40:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-04 17:40:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:40:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:40:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 22 @ 4281 updates, score 7.2) (writing took 4.228739243000746 seconds)
2022-03-04 17:40:24 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 17:40:24 | INFO | train | epoch 022 | loss 5.255 | ppl 38.2 | wps 19199.2 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.859 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 30284
2022-03-04 17:40:24 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 17:40:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:41:28 | INFO | train_inner | epoch 023:     19 / 196 loss=5.246, ppl=37.96, wps=19131.8, ups=0.29, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.841, loss_scale=16, train_wall=307, gb_free=7.2, wall=30347
2022-03-04 17:42:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:47:04 | INFO | train_inner | epoch 023:    120 / 196 loss=5.104, ppl=34.4, wps=19513.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.822, loss_scale=16, train_wall=311, gb_free=7.2, wall=30683
2022-03-04 17:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:51:21 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.218 | ppl 148.87 | wps 38626.7 | wpb 510.9 | bsz 1 | num_updates 4476 | best_loss 7.017
2022-03-04 17:51:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4476 updates
2022-03-04 17:51:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:51:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:51:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 23 @ 4476 updates, score 7.218) (writing took 4.18818480707705 seconds)
2022-03-04 17:51:25 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 17:51:25 | INFO | train | epoch 023 | loss 5.124 | ppl 34.86 | wps 19315 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4476 | lr 0.000472667 | gnorm 0.816 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 30945
2022-03-04 17:51:25 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 17:51:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:52:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:52:48 | INFO | train_inner | epoch 024:     25 / 196 loss=5.11, ppl=34.53, wps=18954.4, ups=0.29, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.829, loss_scale=16, train_wall=310, gb_free=7.2, wall=31028
2022-03-04 17:58:21 | INFO | train_inner | epoch 024:    125 / 196 loss=4.984, ppl=31.66, wps=19696.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.834, loss_scale=16, train_wall=308, gb_free=7.2, wall=31361
2022-03-04 18:00:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:02:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:02:22 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.311 | ppl 158.8 | wps 37997.8 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 7.017
2022-03-04 18:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-04 18:02:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:02:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:02:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 24 @ 4670 updates, score 7.311) (writing took 4.261166233569384 seconds)
2022-03-04 18:02:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 18:02:27 | INFO | train | epoch 024 | loss 5.002 | ppl 32.04 | wps 19199.6 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.824 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 31606
2022-03-04 18:02:27 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 18:02:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:04:07 | INFO | train_inner | epoch 025:     30 / 196 loss=4.981, ppl=31.59, wps=18923.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.798, loss_scale=16, train_wall=310, gb_free=7.2, wall=31706
2022-03-04 18:09:39 | INFO | train_inner | epoch 025:    130 / 196 loss=4.877, ppl=29.39, wps=19716.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.829, loss_scale=32, train_wall=307, gb_free=7.2, wall=32039
2022-03-04 18:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:13:23 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.362 | ppl 164.45 | wps 38124.5 | wpb 510.9 | bsz 1 | num_updates 4866 | best_loss 7.017
2022-03-04 18:13:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4866 updates
2022-03-04 18:13:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:13:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 25 @ 4866 updates, score 7.362) (writing took 4.276553967967629 seconds)
2022-03-04 18:13:28 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 18:13:28 | INFO | train | epoch 025 | loss 4.889 | ppl 29.63 | wps 19399.2 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 4866 | lr 0.000453329 | gnorm 0.822 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 32267
2022-03-04 18:13:28 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 18:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:15:21 | INFO | train_inner | epoch 026:     34 / 196 loss=4.86, ppl=29.05, wps=19110, ups=0.29, wpb=65367, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.809, loss_scale=64, train_wall=307, gb_free=7.2, wall=32381
2022-03-04 18:15:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:20:57 | INFO | train_inner | epoch 026:    135 / 196 loss=4.771, ppl=27.31, wps=19491.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.834, loss_scale=32, train_wall=311, gb_free=7.2, wall=32717
2022-03-04 18:22:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:24:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:24:25 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.411 | ppl 170.19 | wps 37968.2 | wpb 510.9 | bsz 1 | num_updates 5060 | best_loss 7.017
2022-03-04 18:24:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5060 updates
2022-03-04 18:24:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:24:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 26 @ 5060 updates, score 7.411) (writing took 4.2091094981879 seconds)
2022-03-04 18:24:30 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 18:24:30 | INFO | train | epoch 026 | loss 4.778 | ppl 27.44 | wps 19186.4 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 5060 | lr 0.000444554 | gnorm 0.806 | loss_scale 32 | train_wall 603 | gb_free 7.2 | wall 32929
2022-03-04 18:24:30 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 18:24:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:26:43 | INFO | train_inner | epoch 027:     40 / 196 loss=4.738, ppl=26.68, wps=18903.9, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.8, loss_scale=32, train_wall=311, gb_free=7.2, wall=33063
2022-03-04 18:29:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:32:19 | INFO | train_inner | epoch 027:    141 / 196 loss=4.672, ppl=25.5, wps=19485.2, ups=0.3, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.813, loss_scale=32, train_wall=311, gb_free=7.2, wall=33399
2022-03-04 18:35:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:35:27 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.479 | ppl 178.38 | wps 38785.6 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 7.017
2022-03-04 18:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5255 updates
2022-03-04 18:35:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 27 @ 5255 updates, score 7.479) (writing took 4.488253090530634 seconds)
2022-03-04 18:35:31 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 18:35:31 | INFO | train | epoch 027 | loss 4.679 | ppl 25.61 | wps 19279.4 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 5255 | lr 0.000436228 | gnorm 0.831 | loss_scale 32 | train_wall 603 | gb_free 7.2 | wall 33591
2022-03-04 18:35:32 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 18:35:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:37:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:38:05 | INFO | train_inner | epoch 028:     46 / 196 loss=4.636, ppl=24.87, wps=18923.9, ups=0.29, wpb=65367, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.834, loss_scale=32, train_wall=310, gb_free=7.2, wall=33744
2022-03-04 18:43:36 | INFO | train_inner | epoch 028:    146 / 196 loss=4.584, ppl=23.99, wps=19749.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.829, loss_scale=32, train_wall=307, gb_free=7.2, wall=34076
2022-03-04 18:44:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:45:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:46:27 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.55 | ppl 187.4 | wps 38276 | wpb 510.9 | bsz 1 | num_updates 5448 | best_loss 7.017
2022-03-04 18:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5448 updates
2022-03-04 18:46:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:46:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:46:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 28 @ 5448 updates, score 7.55) (writing took 4.40856827609241 seconds)
2022-03-04 18:46:32 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 18:46:32 | INFO | train | epoch 028 | loss 4.579 | ppl 23.9 | wps 19128.7 | ups 0.29 | wpb 65446.6 | bsz 127.8 | num_updates 5448 | lr 0.000428432 | gnorm 0.815 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 34251
2022-03-04 18:46:32 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 18:46:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:49:25 | INFO | train_inner | epoch 029:     52 / 196 loss=4.522, ppl=22.98, wps=18780.4, ups=0.29, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.818, loss_scale=16, train_wall=312, gb_free=7.2, wall=34424
2022-03-04 18:54:57 | INFO | train_inner | epoch 029:    152 / 196 loss=4.503, ppl=22.67, wps=19732.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.793, loss_scale=32, train_wall=307, gb_free=7.2, wall=34756
2022-03-04 18:56:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:57:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:57:27 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.609 | ppl 195.28 | wps 38283.6 | wpb 510.9 | bsz 1 | num_updates 5643 | best_loss 7.017
2022-03-04 18:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5643 updates
2022-03-04 18:57:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:57:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 29 @ 5643 updates, score 7.609) (writing took 4.412083920091391 seconds)
2022-03-04 18:57:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 18:57:32 | INFO | train | epoch 029 | loss 4.49 | ppl 22.47 | wps 19334.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5643 | lr 0.000420964 | gnorm 0.822 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 34912
2022-03-04 18:57:32 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 18:57:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:00:41 | INFO | train_inner | epoch 030:     57 / 196 loss=4.426, ppl=21.5, wps=18978.1, ups=0.29, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.854, loss_scale=16, train_wall=309, gb_free=7.2, wall=35101
2022-03-04 19:06:13 | INFO | train_inner | epoch 030:    157 / 196 loss=4.422, ppl=21.43, wps=19761.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.821, loss_scale=32, train_wall=307, gb_free=7.2, wall=35432
2022-03-04 19:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:08:27 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.681 | ppl 205.27 | wps 38379 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 7.017
2022-03-04 19:08:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-04 19:08:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:08:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.681) (writing took 4.402733670547605 seconds)
2022-03-04 19:08:31 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 19:08:31 | INFO | train | epoch 030 | loss 4.404 | ppl 21.18 | wps 19453.5 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.826 | loss_scale 32 | train_wall 600 | gb_free 7.2 | wall 35571
2022-03-04 19:08:31 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 19:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:08:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:11:57 | INFO | train_inner | epoch 031:     62 / 196 loss=4.334, ppl=20.16, wps=18970.7, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.817, loss_scale=16, train_wall=309, gb_free=7.2, wall=35777
2022-03-04 19:16:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:17:33 | INFO | train_inner | epoch 031:    163 / 196 loss=4.339, ppl=20.24, wps=19530.4, ups=0.3, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.841, loss_scale=16, train_wall=310, gb_free=7.2, wall=36112
2022-03-04 19:19:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:19:27 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.732 | ppl 212.56 | wps 38043.6 | wpb 510.9 | bsz 1 | num_updates 6033 | best_loss 7.017
2022-03-04 19:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6033 updates
2022-03-04 19:19:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:19:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:19:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 31 @ 6033 updates, score 7.732) (writing took 4.329931505024433 seconds)
2022-03-04 19:19:32 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 19:19:32 | INFO | train | epoch 031 | loss 4.32 | ppl 19.97 | wps 19223.9 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 6033 | lr 0.00040713 | gnorm 0.826 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 36231
2022-03-04 19:19:32 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 19:19:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:23:14 | INFO | train_inner | epoch 032:     67 / 196 loss=4.246, ppl=18.97, wps=19140, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.826, loss_scale=16, train_wall=306, gb_free=7.2, wall=36454
2022-03-04 19:28:46 | INFO | train_inner | epoch 032:    167 / 196 loss=4.265, ppl=19.23, wps=19730.5, ups=0.3, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.828, loss_scale=32, train_wall=307, gb_free=7.2, wall=36786
2022-03-04 19:30:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:30:28 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.789 | ppl 221.11 | wps 38160.8 | wpb 510.9 | bsz 1 | num_updates 6229 | best_loss 7.017
2022-03-04 19:30:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6229 updates
2022-03-04 19:30:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:30:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:30:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 32 @ 6229 updates, score 7.789) (writing took 4.282927680760622 seconds)
2022-03-04 19:30:32 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 19:30:32 | INFO | train | epoch 032 | loss 4.243 | ppl 18.93 | wps 19433.6 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 6229 | lr 0.000400674 | gnorm 0.834 | loss_scale 32 | train_wall 601 | gb_free 7.2 | wall 36892
2022-03-04 19:30:32 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 19:30:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:31:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:34:31 | INFO | train_inner | epoch 033:     72 / 196 loss=4.169, ppl=17.99, wps=18976.8, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.83, loss_scale=32, train_wall=309, gb_free=7.2, wall=37131
2022-03-04 19:38:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:40:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:40:10 | INFO | train_inner | epoch 033:    174 / 196 loss=4.194, ppl=18.3, wps=19353, ups=0.3, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.865, loss_scale=16, train_wall=313, gb_free=7.2, wall=37469
2022-03-04 19:41:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:41:27 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.898 | ppl 238.46 | wps 38269.9 | wpb 510.9 | bsz 1 | num_updates 6422 | best_loss 7.017
2022-03-04 19:41:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6422 updates
2022-03-04 19:41:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:41:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 33 @ 6422 updates, score 7.898) (writing took 4.32614853605628 seconds)
2022-03-04 19:41:32 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 19:41:32 | INFO | train | epoch 033 | loss 4.166 | ppl 17.96 | wps 19140.3 | ups 0.29 | wpb 65446.6 | bsz 127.8 | num_updates 6422 | lr 0.000394607 | gnorm 0.841 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 37551
2022-03-04 19:41:32 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 19:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:45:51 | INFO | train_inner | epoch 034:     78 / 196 loss=4.085, ppl=16.98, wps=19150.4, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.83, loss_scale=16, train_wall=306, gb_free=7.2, wall=37811
2022-03-04 19:51:23 | INFO | train_inner | epoch 034:    178 / 196 loss=4.127, ppl=17.47, wps=19721.3, ups=0.3, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.843, loss_scale=32, train_wall=307, gb_free=7.2, wall=38143
2022-03-04 19:52:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:52:28 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.937 | ppl 245.08 | wps 38304.7 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 7.017
2022-03-04 19:52:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-04 19:52:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:52:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:52:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 34 @ 6618 updates, score 7.937) (writing took 4.2036619540303946 seconds)
2022-03-04 19:52:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 19:52:32 | INFO | train | epoch 034 | loss 4.097 | ppl 17.11 | wps 19430.5 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.844 | loss_scale 32 | train_wall 601 | gb_free 7.2 | wall 38212
2022-03-04 19:52:32 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 19:52:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:54:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:57:08 | INFO | train_inner | epoch 035:     83 / 196 loss=4.006, ppl=16.07, wps=18955.1, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.856, loss_scale=32, train_wall=310, gb_free=7.2, wall=38488
2022-03-04 20:01:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:02:44 | INFO | train_inner | epoch 035:    184 / 196 loss=4.063, ppl=16.71, wps=19529.3, ups=0.3, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.869, loss_scale=32, train_wall=310, gb_free=7.2, wall=38823
2022-03-04 20:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:03:28 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.077 | ppl 270.06 | wps 38254.7 | wpb 510.9 | bsz 1 | num_updates 6812 | best_loss 7.017
2022-03-04 20:03:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6812 updates
2022-03-04 20:03:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:03:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:03:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 35 @ 6812 updates, score 8.077) (writing took 4.319366153329611 seconds)
2022-03-04 20:03:33 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 20:03:33 | INFO | train | epoch 035 | loss 4.027 | ppl 16.3 | wps 19217.3 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 6812 | lr 0.000383145 | gnorm 0.859 | loss_scale 32 | train_wall 602 | gb_free 7.2 | wall 38872
2022-03-04 20:03:33 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 20:03:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:08:25 | INFO | train_inner | epoch 036:     88 / 196 loss=3.933, ppl=15.28, wps=19144.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.847, loss_scale=32, train_wall=306, gb_free=7.2, wall=39165
2022-03-04 20:08:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:14:00 | INFO | train_inner | epoch 036:    189 / 196 loss=4.003, ppl=16.03, wps=19541.8, ups=0.3, wpb=65532.4, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.867, loss_scale=32, train_wall=310, gb_free=7.2, wall=39500
2022-03-04 20:14:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:14:29 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.107 | ppl 275.73 | wps 38445.3 | wpb 510.9 | bsz 1 | num_updates 7007 | best_loss 7.017
2022-03-04 20:14:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7007 updates
2022-03-04 20:14:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:14:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:14:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 36 @ 7007 updates, score 8.107) (writing took 4.240925835445523 seconds)
2022-03-04 20:14:33 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 20:14:33 | INFO | train | epoch 036 | loss 3.962 | ppl 15.58 | wps 19334.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7007 | lr 0.000377776 | gnorm 0.86 | loss_scale 32 | train_wall 601 | gb_free 7.2 | wall 39532
2022-03-04 20:14:33 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 20:14:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:16:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:19:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:19:48 | INFO | train_inner | epoch 037:     95 / 196 loss=3.867, ppl=14.59, wps=18790.9, ups=0.29, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.866, loss_scale=16, train_wall=312, gb_free=7.2, wall=39848
2022-03-04 20:25:20 | INFO | train_inner | epoch 037:    195 / 196 loss=3.94, ppl=15.34, wps=19728.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.892, loss_scale=16, train_wall=307, gb_free=7.2, wall=40180
2022-03-04 20:25:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:25:29 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.183 | ppl 290.69 | wps 38049.2 | wpb 510.9 | bsz 1 | num_updates 7201 | best_loss 7.017
2022-03-04 20:25:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7201 updates
2022-03-04 20:25:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:25:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:25:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 37 @ 7201 updates, score 8.183) (writing took 4.189403364434838 seconds)
2022-03-04 20:25:33 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 20:25:33 | INFO | train | epoch 037 | loss 3.899 | ppl 14.92 | wps 19234.1 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 7201 | lr 0.000372652 | gnorm 0.879 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 40193
2022-03-04 20:25:33 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 20:25:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:31:01 | INFO | train_inner | epoch 038:     99 / 196 loss=3.785, ppl=13.79, wps=19173.6, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.873, loss_scale=32, train_wall=306, gb_free=7.2, wall=40521
2022-03-04 20:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:36:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:36:28 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.289 | ppl 312.73 | wps 38177.1 | wpb 510.9 | bsz 1 | num_updates 7396 | best_loss 7.017
2022-03-04 20:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7396 updates
2022-03-04 20:36:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:36:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:36:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 38 @ 7396 updates, score 8.289) (writing took 4.279280669987202 seconds)
2022-03-04 20:36:32 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 20:36:32 | INFO | train | epoch 038 | loss 3.837 | ppl 14.29 | wps 19354.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7396 | lr 0.000367707 | gnorm 0.876 | loss_scale 16 | train_wall 600 | gb_free 7.2 | wall 40852
2022-03-04 20:36:32 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 20:36:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:36:46 | INFO | train_inner | epoch 039:      4 / 196 loss=3.883, ppl=14.76, wps=18986.2, ups=0.29, wpb=65367, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.876, loss_scale=16, train_wall=309, gb_free=7.2, wall=40865
2022-03-04 20:42:17 | INFO | train_inner | epoch 039:    104 / 196 loss=3.736, ppl=13.33, wps=19750.6, ups=0.3, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.869, loss_scale=32, train_wall=307, gb_free=7.2, wall=41197
2022-03-04 20:44:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:47:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:47:28 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.347 | ppl 325.51 | wps 37592.8 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 7.017
2022-03-04 20:47:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-04 20:47:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:47:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:47:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 39 @ 7591 updates, score 8.347) (writing took 4.205335671082139 seconds)
2022-03-04 20:47:32 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 20:47:32 | INFO | train | epoch 039 | loss 3.78 | ppl 13.74 | wps 19344.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.877 | loss_scale 16 | train_wall 601 | gb_free 7.2 | wall 41512
2022-03-04 20:47:32 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 20:47:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:48:02 | INFO | train_inner | epoch 040:      9 / 196 loss=3.813, ppl=14.05, wps=18971, ups=0.29, wpb=65367, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.89, loss_scale=16, train_wall=309, gb_free=7.2, wall=41542
2022-03-04 20:53:35 | INFO | train_inner | epoch 040:    109 / 196 loss=3.694, ppl=12.94, wps=19689.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.869, loss_scale=32, train_wall=308, gb_free=7.2, wall=41874
2022-03-04 20:54:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:58:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:58:29 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.452 | ppl 350.22 | wps 38521.3 | wpb 510.9 | bsz 1 | num_updates 7786 | best_loss 7.017
2022-03-04 20:58:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7786 updates
2022-03-04 20:58:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 40 @ 7786 updates, score 8.452) (writing took 4.243403671309352 seconds)
2022-03-04 20:58:33 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 20:58:33 | INFO | train | epoch 040 | loss 3.726 | ppl 13.23 | wps 19311.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7786 | lr 0.000358379 | gnorm 0.877 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 42172
2022-03-04 20:58:33 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 20:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:59:20 | INFO | train_inner | epoch 041:     14 / 196 loss=3.746, ppl=13.42, wps=18958.3, ups=0.29, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.882, loss_scale=16, train_wall=310, gb_free=7.2, wall=42219
2022-03-04 21:04:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:04:55 | INFO | train_inner | epoch 041:    115 / 196 loss=3.644, ppl=12.5, wps=19526.8, ups=0.3, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.891, loss_scale=16, train_wall=310, gb_free=7.2, wall=42555
2022-03-04 21:09:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:09:30 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.488 | ppl 358.96 | wps 35856.6 | wpb 510.9 | bsz 1 | num_updates 7981 | best_loss 7.017
2022-03-04 21:09:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7981 updates
2022-03-04 21:09:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:09:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 41 @ 7981 updates, score 8.488) (writing took 4.457396902143955 seconds)
2022-03-04 21:09:35 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 21:09:35 | INFO | train | epoch 041 | loss 3.673 | ppl 12.75 | wps 19278.6 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 7981 | lr 0.000353974 | gnorm 0.898 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 42834
2022-03-04 21:09:35 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 21:09:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:10:38 | INFO | train_inner | epoch 042:     19 / 196 loss=3.689, ppl=12.9, wps=19078.1, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.909, loss_scale=16, train_wall=307, gb_free=7.2, wall=42897
2022-03-04 21:12:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:16:12 | INFO | train_inner | epoch 042:    120 / 196 loss=3.596, ppl=12.09, wps=19586.2, ups=0.3, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.909, loss_scale=16, train_wall=309, gb_free=7.2, wall=43232
2022-03-04 21:20:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:20:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:20:32 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.616 | ppl 392.22 | wps 37055.3 | wpb 510.9 | bsz 1 | num_updates 8175 | best_loss 7.017
2022-03-04 21:20:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8175 updates
2022-03-04 21:20:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:20:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:20:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 42 @ 8175 updates, score 8.616) (writing took 4.360691208392382 seconds)
2022-03-04 21:20:37 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 21:20:37 | INFO | train | epoch 042 | loss 3.62 | ppl 12.3 | wps 19183.1 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 8175 | lr 0.000349749 | gnorm 0.908 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 43496
2022-03-04 21:20:37 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 21:20:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:22:01 | INFO | train_inner | epoch 043:     25 / 196 loss=3.624, ppl=12.33, wps=18761.8, ups=0.29, wpb=65359.9, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.903, loss_scale=16, train_wall=313, gb_free=7.2, wall=43580
2022-03-04 21:27:37 | INFO | train_inner | epoch 043:    125 / 196 loss=3.554, ppl=11.74, wps=19519.2, ups=0.3, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.915, loss_scale=32, train_wall=311, gb_free=7.2, wall=43916
2022-03-04 21:29:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:31:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:31:37 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.691 | ppl 413.18 | wps 38525.1 | wpb 510.9 | bsz 1 | num_updates 8370 | best_loss 7.017
2022-03-04 21:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8370 updates
2022-03-04 21:31:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:31:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 43 @ 8370 updates, score 8.691) (writing took 4.2785561215132475 seconds)
2022-03-04 21:31:41 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 21:31:41 | INFO | train | epoch 043 | loss 3.572 | ppl 11.89 | wps 19214 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 8370 | lr 0.000345651 | gnorm 0.908 | loss_scale 16 | train_wall 605 | gb_free 7.2 | wall 44161
2022-03-04 21:31:41 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 21:31:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:33:20 | INFO | train_inner | epoch 044:     30 / 196 loss=3.572, ppl=11.89, wps=19009.1, ups=0.29, wpb=65367, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.905, loss_scale=16, train_wall=309, gb_free=7.2, wall=44260
2022-03-04 21:36:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:38:55 | INFO | train_inner | epoch 044:    131 / 196 loss=3.508, ppl=11.38, wps=19571.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.908, loss_scale=16, train_wall=310, gb_free=7.2, wall=44595
2022-03-04 21:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:42:38 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.748 | ppl 429.93 | wps 37232.3 | wpb 510.9 | bsz 1 | num_updates 8565 | best_loss 7.017
2022-03-04 21:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8565 updates
2022-03-04 21:42:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:42:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:42:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 44 @ 8565 updates, score 8.748) (writing took 4.320476042106748 seconds)
2022-03-04 21:42:42 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 21:42:42 | INFO | train | epoch 044 | loss 3.524 | ppl 11.5 | wps 19304.5 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 8565 | lr 0.000341693 | gnorm 0.914 | loss_scale 16 | train_wall 602 | gb_free 7.2 | wall 44822
2022-03-04 21:42:42 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 21:42:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:44:39 | INFO | train_inner | epoch 045:     35 / 196 loss=3.523, ppl=11.49, wps=19017.4, ups=0.29, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.912, loss_scale=32, train_wall=308, gb_free=7.2, wall=44939
2022-03-04 21:46:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:50:17 | INFO | train_inner | epoch 045:    136 / 196 loss=3.469, ppl=11.08, wps=19413.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.923, loss_scale=16, train_wall=312, gb_free=7.2, wall=45276
2022-03-04 21:53:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:53:42 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.753 | ppl 431.32 | wps 37707.7 | wpb 510.9 | bsz 1 | num_updates 8760 | best_loss 7.017
2022-03-04 21:53:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8760 updates
2022-03-04 21:53:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:53:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:53:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 45 @ 8760 updates, score 8.753) (writing took 4.292878417298198 seconds)
2022-03-04 21:53:46 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 21:53:46 | INFO | train | epoch 045 | loss 3.478 | ppl 11.14 | wps 19217.7 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 8760 | lr 0.000337869 | gnorm 0.912 | loss_scale 32 | train_wall 605 | gb_free 7.2 | wall 45486
2022-03-04 21:53:46 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 21:53:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:55:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:56:03 | INFO | train_inner | epoch 046:     41 / 196 loss=3.466, ppl=11.05, wps=18854.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.907, loss_scale=16, train_wall=311, gb_free=7.2, wall=45623
2022-03-04 22:01:36 | INFO | train_inner | epoch 046:    141 / 196 loss=3.431, ppl=10.78, wps=19665.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.935, loss_scale=16, train_wall=308, gb_free=7.2, wall=45956
2022-03-04 22:02:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:45 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.782 | ppl 440.1 | wps 38312.8 | wpb 510.9 | bsz 1 | num_updates 8954 | best_loss 7.017
2022-03-04 22:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8954 updates
2022-03-04 22:04:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:04:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 46 @ 8954 updates, score 8.782) (writing took 4.530520044267178 seconds)
2022-03-04 22:04:49 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 22:04:49 | INFO | train | epoch 046 | loss 3.434 | ppl 10.81 | wps 19151.6 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 8954 | lr 0.000334188 | gnorm 0.927 | loss_scale 16 | train_wall 604 | gb_free 7.2 | wall 46149
2022-03-04 22:04:49 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 22:04:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:07:22 | INFO | train_inner | epoch 047:     46 / 196 loss=3.416, ppl=10.67, wps=18895.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.92, loss_scale=16, train_wall=310, gb_free=7.2, wall=46302
2022-03-04 22:09:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:12:59 | INFO | train_inner | epoch 047:    147 / 196 loss=3.394, ppl=10.51, wps=19466.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.949, loss_scale=16, train_wall=311, gb_free=7.2, wall=46639
2022-03-04 22:15:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:15:47 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.971 | ppl 501.87 | wps 37946.2 | wpb 510.9 | bsz 1 | num_updates 9149 | best_loss 7.017
2022-03-04 22:15:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9149 updates
2022-03-04 22:15:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:15:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:15:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 47 @ 9149 updates, score 8.971) (writing took 4.57876250334084 seconds)
2022-03-04 22:15:52 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 22:15:52 | INFO | train | epoch 047 | loss 3.393 | ppl 10.5 | wps 19255.1 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 9149 | lr 0.000330608 | gnorm 0.936 | loss_scale 16 | train_wall 603 | gb_free 7.2 | wall 46812
2022-03-04 22:15:52 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 22:15:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:17:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:18:45 | INFO | train_inner | epoch 048:     52 / 196 loss=3.37, ppl=10.34, wps=18884.1, ups=0.29, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.937, loss_scale=16, train_wall=310, gb_free=7.2, wall=46985
2022-03-04 22:24:18 | INFO | train_inner | epoch 048:    152 / 196 loss=3.351, ppl=10.2, wps=19670.2, ups=0.3, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.93, loss_scale=16, train_wall=308, gb_free=7.2, wall=47318
2022-03-04 22:24:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:26:50 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.005 | ppl 513.93 | wps 37575.4 | wpb 510.9 | bsz 1 | num_updates 9343 | best_loss 7.017
2022-03-04 22:26:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9343 updates
2022-03-04 22:26:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:26:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 48 @ 9343 updates, score 9.005) (writing took 4.545255282893777 seconds)
2022-03-04 22:26:54 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 22:26:54 | INFO | train | epoch 048 | loss 3.349 | ppl 10.19 | wps 19166.3 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 9343 | lr 0.000327157 | gnorm 0.942 | loss_scale 16 | train_wall 603 | gb_free 7.2 | wall 47474
2022-03-04 22:26:54 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 22:26:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:30:04 | INFO | train_inner | epoch 049:     57 / 196 loss=3.314, ppl=9.94, wps=18902.1, ups=0.29, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.949, loss_scale=16, train_wall=310, gb_free=7.2, wall=47664
2022-03-04 22:32:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:35:41 | INFO | train_inner | epoch 049:    158 / 196 loss=3.328, ppl=10.04, wps=19472.8, ups=0.3, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.935, loss_scale=16, train_wall=311, gb_free=7.2, wall=48000
2022-03-04 22:37:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:37:52 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.109 | ppl 552.12 | wps 37725.9 | wpb 510.9 | bsz 1 | num_updates 9538 | best_loss 7.017
2022-03-04 22:37:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9538 updates
2022-03-04 22:37:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:37:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:37:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 49 @ 9538 updates, score 9.109) (writing took 4.546651856973767 seconds)
2022-03-04 22:37:57 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 22:37:57 | INFO | train | epoch 049 | loss 3.31 | ppl 9.92 | wps 19259.9 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 9538 | lr 0.000323796 | gnorm 0.94 | loss_scale 16 | train_wall 603 | gb_free 7.2 | wall 48137
2022-03-04 22:37:57 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 22:37:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:39:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:41:27 | INFO | train_inner | epoch 050:     63 / 196 loss=3.272, ppl=9.66, wps=18870.8, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.953, loss_scale=16, train_wall=311, gb_free=7.2, wall=48347
2022-03-04 22:46:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:47:04 | INFO | train_inner | epoch 050:    164 / 196 loss=3.291, ppl=9.79, wps=19457.2, ups=0.3, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.958, loss_scale=16, train_wall=312, gb_free=7.2, wall=48684
2022-03-04 22:48:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:48:56 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.226 | ppl 598.98 | wps 38295.1 | wpb 510.9 | bsz 1 | num_updates 9732 | best_loss 7.017
2022-03-04 22:48:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9732 updates
2022-03-04 22:48:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:49:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:49:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 50 @ 9732 updates, score 9.226) (writing took 4.5062734335660934 seconds)
2022-03-04 22:49:00 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 22:49:00 | INFO | train | epoch 050 | loss 3.271 | ppl 9.65 | wps 19148.5 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 9732 | lr 0.000320552 | gnorm 0.958 | loss_scale 16 | train_wall 604 | gb_free 7.2 | wall 48800
2022-03-04 22:49:00 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 22:49:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:52:47 | INFO | train_inner | epoch 051:     68 / 196 loss=3.215, ppl=9.28, wps=19074.9, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.949, loss_scale=16, train_wall=307, gb_free=7.2, wall=49026
2022-03-04 22:54:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:58:25 | INFO | train_inner | epoch 051:    169 / 196 loss=3.263, ppl=9.6, wps=19346.2, ups=0.3, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.953, loss_scale=16, train_wall=313, gb_free=7.2, wall=49365
2022-03-04 22:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:00:04 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.236 | ppl 602.88 | wps 35673.8 | wpb 510.9 | bsz 1 | num_updates 9927 | best_loss 7.017
2022-03-04 23:00:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9927 updates
2022-03-04 23:00:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:00:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 51 @ 9927 updates, score 9.236) (writing took 4.448252135887742 seconds)
2022-03-04 23:00:09 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 23:00:09 | INFO | train | epoch 051 | loss 3.235 | ppl 9.42 | wps 19087.7 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 9927 | lr 0.000317388 | gnorm 0.95 | loss_scale 16 | train_wall 608 | gb_free 7.2 | wall 49468
2022-03-04 23:00:09 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 23:00:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:02:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:04:27 | INFO | train_inner | epoch 052:     74 / 196 loss=3.186, ppl=9.1, wps=18070, ups=0.28, wpb=65367, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.967, loss_scale=16, train_wall=325, gb_free=7.2, wall=49727
2022-03-04 23:09:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:10:20 | INFO | train_inner | epoch 052:    175 / 196 loss=3.229, ppl=9.38, wps=18574, ups=0.28, wpb=65532.4, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.967, loss_scale=16, train_wall=326, gb_free=7.2, wall=50080
2022-03-04 23:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:11:39 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.252 | ppl 609.92 | wps 33982.2 | wpb 510.9 | bsz 1 | num_updates 10121 | best_loss 7.017
2022-03-04 23:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10121 updates
2022-03-04 23:11:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:11:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 52 @ 10121 updates, score 9.252) (writing took 5.111757839098573 seconds)
2022-03-04 23:11:44 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 23:11:44 | INFO | train | epoch 052 | loss 3.198 | ppl 9.17 | wps 18268.8 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 10121 | lr 0.000314332 | gnorm 0.972 | loss_scale 16 | train_wall 632 | gb_free 7.2 | wall 50163
2022-03-04 23:11:44 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 23:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:16:21 | INFO | train_inner | epoch 053:     79 / 196 loss=3.142, ppl=8.83, wps=18121, ups=0.28, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.955, loss_scale=16, train_wall=323, gb_free=7.2, wall=50440
2022-03-04 23:17:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:22:14 | INFO | train_inner | epoch 053:    180 / 196 loss=3.193, ppl=9.14, wps=18519.6, ups=0.28, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.975, loss_scale=16, train_wall=327, gb_free=7.2, wall=50794
2022-03-04 23:23:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:23:16 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.435 | ppl 691.94 | wps 33538 | wpb 510.9 | bsz 1 | num_updates 10316 | best_loss 7.017
2022-03-04 23:23:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10316 updates
2022-03-04 23:23:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:23:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 53 @ 10316 updates, score 9.435) (writing took 4.888284232467413 seconds)
2022-03-04 23:23:21 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 23:23:21 | INFO | train | epoch 053 | loss 3.163 | ppl 8.96 | wps 18309.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 10316 | lr 0.000311347 | gnorm 0.956 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 50860
2022-03-04 23:23:21 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 23:23:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:25:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:28:17 | INFO | train_inner | epoch 054:     85 / 196 loss=3.103, ppl=8.59, wps=18028, ups=0.28, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.972, loss_scale=16, train_wall=325, gb_free=7.2, wall=51157
2022-03-04 23:32:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:34:08 | INFO | train_inner | epoch 054:    186 / 196 loss=3.165, ppl=8.97, wps=18646.9, ups=0.28, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.975, loss_scale=16, train_wall=325, gb_free=7.2, wall=51508
2022-03-04 23:34:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:34:49 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.421 | ppl 685.64 | wps 34932.7 | wpb 510.9 | bsz 1 | num_updates 10510 | best_loss 7.017
2022-03-04 23:34:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10510 updates
2022-03-04 23:34:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:34:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 54 @ 10510 updates, score 9.421) (writing took 4.7250504568219185 seconds)
2022-03-04 23:34:53 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 23:34:53 | INFO | train | epoch 054 | loss 3.129 | ppl 8.75 | wps 18327.7 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 10510 | lr 0.00030846 | gnorm 0.978 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 51553
2022-03-04 23:34:53 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 23:34:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:40:06 | INFO | train_inner | epoch 055:     90 / 196 loss=3.064, ppl=8.36, wps=18265.3, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.967, loss_scale=16, train_wall=321, gb_free=7.2, wall=51866
2022-03-04 23:40:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:46:01 | INFO | train_inner | epoch 055:    191 / 196 loss=3.14, ppl=8.81, wps=18463, ups=0.28, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.976, loss_scale=16, train_wall=328, gb_free=7.2, wall=52221
2022-03-04 23:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:46:25 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.481 | ppl 714.67 | wps 33469.6 | wpb 510.9 | bsz 1 | num_updates 10705 | best_loss 7.017
2022-03-04 23:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10705 updates
2022-03-04 23:46:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:46:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:46:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 55 @ 10705 updates, score 9.481) (writing took 5.068334840238094 seconds)
2022-03-04 23:46:30 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 23:46:30 | INFO | train | epoch 055 | loss 3.097 | ppl 8.56 | wps 18328.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 10705 | lr 0.000305638 | gnorm 0.972 | loss_scale 16 | train_wall 633 | gb_free 7.2 | wall 52249
2022-03-04 23:46:30 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 23:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:48:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:52:08 | INFO | train_inner | epoch 056:     96 / 196 loss=3.024, ppl=8.13, wps=17838.2, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.973, loss_scale=16, train_wall=328, gb_free=7.2, wall=52587
2022-03-04 23:55:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:57:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:58:03 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.537 | ppl 742.9 | wps 34368.5 | wpb 510.9 | bsz 1 | num_updates 10899 | best_loss 7.017
2022-03-04 23:58:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10899 updates
2022-03-04 23:58:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:58:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:58:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 56 @ 10899 updates, score 9.537) (writing took 4.754362482577562 seconds)
2022-03-04 23:58:08 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 23:58:08 | INFO | train | epoch 056 | loss 3.065 | ppl 8.37 | wps 18176.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 10899 | lr 0.000302905 | gnorm 0.977 | loss_scale 16 | train_wall 636 | gb_free 7.2 | wall 52948
2022-03-04 23:58:08 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 23:58:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:58:12 | INFO | train_inner | epoch 057:      1 / 196 loss=3.109, ppl=8.63, wps=17949.3, ups=0.27, wpb=65367, bsz=127.7, num_updates=10900, lr=0.000302891, gnorm=0.984, loss_scale=16, train_wall=327, gb_free=7.2, wall=52952
2022-03-05 00:03:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:04:07 | INFO | train_inner | epoch 057:    102 / 196 loss=2.988, ppl=7.93, wps=18438.7, ups=0.28, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.999, loss_scale=16, train_wall=329, gb_free=7.2, wall=53307
2022-03-05 00:09:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:09:45 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.581 | ppl 765.83 | wps 33034.6 | wpb 510.9 | bsz 1 | num_updates 11094 | best_loss 7.017
2022-03-05 00:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11094 updates
2022-03-05 00:09:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:09:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 57 @ 11094 updates, score 9.581) (writing took 5.188855212181807 seconds)
2022-03-05 00:09:50 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-05 00:09:50 | INFO | train | epoch 057 | loss 3.034 | ppl 8.19 | wps 18188.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 11094 | lr 0.000300231 | gnorm 0.994 | loss_scale 16 | train_wall 638 | gb_free 7.2 | wall 53650
2022-03-05 00:09:50 | INFO | fairseq.trainer | begin training epoch 58
2022-03-05 00:09:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:10:11 | INFO | train_inner | epoch 058:      6 / 196 loss=3.075, ppl=8.43, wps=17958.1, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.987, loss_scale=16, train_wall=326, gb_free=7.2, wall=53671
2022-03-05 00:11:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:16:09 | INFO | train_inner | epoch 058:    107 / 196 loss=2.969, ppl=7.83, wps=18299, ups=0.28, wpb=65532.4, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.993, loss_scale=16, train_wall=331, gb_free=7.2, wall=54029
2022-03-05 00:19:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:21:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:21:29 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.63 | ppl 792.38 | wps 33596.7 | wpb 510.9 | bsz 1 | num_updates 11288 | best_loss 7.017
2022-03-05 00:21:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11288 updates
2022-03-05 00:21:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 58 @ 11288 updates, score 9.63) (writing took 5.136975236237049 seconds)
2022-03-05 00:21:34 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-05 00:21:34 | INFO | train | epoch 058 | loss 3.003 | ppl 8.02 | wps 18023.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 11288 | lr 0.00029764 | gnorm 0.992 | loss_scale 16 | train_wall 640 | gb_free 7.2 | wall 54354
2022-03-05 00:21:34 | INFO | fairseq.trainer | begin training epoch 59
2022-03-05 00:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:22:17 | INFO | train_inner | epoch 059:     12 / 196 loss=3.029, ppl=8.16, wps=17799, ups=0.27, wpb=65367, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.994, loss_scale=16, train_wall=329, gb_free=7.2, wall=54396
2022-03-05 00:27:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:28:13 | INFO | train_inner | epoch 059:    113 / 196 loss=2.942, ppl=7.68, wps=18399.3, ups=0.28, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.996, loss_scale=16, train_wall=329, gb_free=7.2, wall=54753
2022-03-05 00:33:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:33:12 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.716 | ppl 840.94 | wps 33565.3 | wpb 510.9 | bsz 1 | num_updates 11483 | best_loss 7.017
2022-03-05 00:33:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11483 updates
2022-03-05 00:33:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:33:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:33:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 59 @ 11483 updates, score 9.716) (writing took 4.773462077602744 seconds)
2022-03-05 00:33:17 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-05 00:33:17 | INFO | train | epoch 059 | loss 2.976 | ppl 7.87 | wps 18175.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 11483 | lr 0.000295102 | gnorm 1.002 | loss_scale 16 | train_wall 639 | gb_free 7.2 | wall 55056
2022-03-05 00:33:17 | INFO | fairseq.trainer | begin training epoch 60
2022-03-05 00:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:34:17 | INFO | train_inner | epoch 060:     17 / 196 loss=3.002, ppl=8.01, wps=17961.9, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=1.014, loss_scale=16, train_wall=326, gb_free=7.2, wall=55116
2022-03-05 00:34:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:40:15 | INFO | train_inner | epoch 060:    118 / 196 loss=2.919, ppl=7.56, wps=18279.1, ups=0.28, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.98, loss_scale=16, train_wall=331, gb_free=7.2, wall=55475
2022-03-05 00:42:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:44:59 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.73 | ppl 849.27 | wps 32768.5 | wpb 510.9 | bsz 1 | num_updates 11677 | best_loss 7.017
2022-03-05 00:44:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11677 updates
2022-03-05 00:44:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 60 @ 11677 updates, score 9.73) (writing took 5.118052251636982 seconds)
2022-03-05 00:45:04 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-05 00:45:04 | INFO | train | epoch 060 | loss 2.946 | ppl 7.71 | wps 17946.8 | ups 0.27 | wpb 65447.1 | bsz 127.8 | num_updates 11677 | lr 0.00029264 | gnorm 0.997 | loss_scale 16 | train_wall 643 | gb_free 7.2 | wall 55764
2022-03-05 00:45:04 | INFO | fairseq.trainer | begin training epoch 61
2022-03-05 00:45:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:46:26 | INFO | train_inner | epoch 061:     23 / 196 loss=2.963, ppl=7.8, wps=17638.2, ups=0.27, wpb=65359.9, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=1.008, loss_scale=16, train_wall=332, gb_free=7.2, wall=55846
2022-03-05 00:50:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:50:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:52:28 | INFO | train_inner | epoch 061:    125 / 196 loss=2.898, ppl=7.46, wps=18115.8, ups=0.28, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=1.002, loss_scale=8, train_wall=335, gb_free=7.2, wall=56207
2022-03-05 00:56:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:56:45 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.848 | ppl 921.67 | wps 33446.5 | wpb 510.9 | bsz 1 | num_updates 11871 | best_loss 7.017
2022-03-05 00:56:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11871 updates
2022-03-05 00:56:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:56:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 61 @ 11871 updates, score 9.848) (writing took 4.9312104638665915 seconds)
2022-03-05 00:56:50 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-05 00:56:50 | INFO | train | epoch 061 | loss 2.92 | ppl 7.57 | wps 17992.6 | ups 0.27 | wpb 65447.1 | bsz 127.8 | num_updates 11871 | lr 0.000290239 | gnorm 1.007 | loss_scale 8 | train_wall 642 | gb_free 7.2 | wall 56469
2022-03-05 00:56:50 | INFO | fairseq.trainer | begin training epoch 62
2022-03-05 00:56:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:58:32 | INFO | train_inner | epoch 062:     29 / 196 loss=2.926, ppl=7.6, wps=17918.3, ups=0.27, wpb=65367, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=1.009, loss_scale=8, train_wall=327, gb_free=7.2, wall=56572
2022-03-05 01:04:27 | INFO | train_inner | epoch 062:    129 / 196 loss=2.882, ppl=7.37, wps=18495.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=12000, lr=0.000288675, gnorm=1.007, loss_scale=16, train_wall=328, gb_free=7.2, wall=56926
2022-03-05 01:06:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:08:31 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.826 | ppl 907.78 | wps 32655.8 | wpb 510.9 | bsz 1 | num_updates 12066 | best_loss 7.017
2022-03-05 01:08:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12066 updates
2022-03-05 01:08:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:08:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:08:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 62 @ 12066 updates, score 9.826) (writing took 4.98560425825417 seconds)
2022-03-05 01:08:36 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-05 01:08:36 | INFO | train | epoch 062 | loss 2.893 | ppl 7.43 | wps 18072.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12066 | lr 0.000287885 | gnorm 1.006 | loss_scale 16 | train_wall 642 | gb_free 7.2 | wall 57176
2022-03-05 01:08:36 | INFO | fairseq.trainer | begin training epoch 63
2022-03-05 01:08:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:10:37 | INFO | train_inner | epoch 063:     34 / 196 loss=2.894, ppl=7.43, wps=17649.5, ups=0.27, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=1.007, loss_scale=16, train_wall=331, gb_free=7.2, wall=57297
2022-03-05 01:14:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:16:37 | INFO | train_inner | epoch 063:    135 / 196 loss=2.857, ppl=7.25, wps=18232, ups=0.28, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=1.013, loss_scale=16, train_wall=332, gb_free=7.2, wall=57656
2022-03-05 01:20:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:20:14 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.92 | ppl 968.72 | wps 35613.2 | wpb 510.9 | bsz 1 | num_updates 12261 | best_loss 7.017
2022-03-05 01:20:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12261 updates
2022-03-05 01:20:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:20:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:20:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 63 @ 12261 updates, score 9.92) (writing took 4.83467261493206 seconds)
2022-03-05 01:20:19 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-05 01:20:19 | INFO | train | epoch 063 | loss 2.866 | ppl 7.29 | wps 18145.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12261 | lr 0.000285586 | gnorm 1.016 | loss_scale 16 | train_wall 640 | gb_free 7.2 | wall 57879
2022-03-05 01:20:19 | INFO | fairseq.trainer | begin training epoch 64
2022-03-05 01:20:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:21:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:22:38 | INFO | train_inner | epoch 064:     40 / 196 loss=2.862, ppl=7.27, wps=18088.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=1.034, loss_scale=16, train_wall=324, gb_free=7.2, wall=58018
2022-03-05 01:28:24 | INFO | train_inner | epoch 064:    140 / 196 loss=2.828, ppl=7.1, wps=18912.3, ups=0.29, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=1.019, loss_scale=16, train_wall=321, gb_free=7.2, wall=58364
2022-03-05 01:29:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:31:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:31:44 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10 | ppl 1024.11 | wps 35023.5 | wpb 510.9 | bsz 1 | num_updates 12455 | best_loss 7.017
2022-03-05 01:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12455 updates
2022-03-05 01:31:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 64 @ 12455 updates, score 10.0) (writing took 4.818923816084862 seconds)
2022-03-05 01:31:49 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-05 01:31:49 | INFO | train | epoch 064 | loss 2.84 | ppl 7.16 | wps 18417.9 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 12455 | lr 0.000283353 | gnorm 1.025 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 58568
2022-03-05 01:31:49 | INFO | fairseq.trainer | begin training epoch 65
2022-03-05 01:31:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:25 | INFO | train_inner | epoch 065:     45 / 196 loss=2.835, ppl=7.14, wps=18127.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=1.027, loss_scale=16, train_wall=323, gb_free=7.2, wall=58725
2022-03-05 01:36:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:40:18 | INFO | train_inner | epoch 065:    146 / 196 loss=2.816, ppl=7.04, wps=18552.2, ups=0.28, wpb=65532.4, bsz=128, num_updates=12600, lr=0.000281718, gnorm=1.038, loss_scale=16, train_wall=327, gb_free=7.2, wall=59078
2022-03-05 01:43:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:43:18 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 10.037 | ppl 1050.58 | wps 35117.1 | wpb 510.9 | bsz 1 | num_updates 12650 | best_loss 7.017
2022-03-05 01:43:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12650 updates
2022-03-05 01:43:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:43:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:43:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 65 @ 12650 updates, score 10.037) (writing took 4.704769015312195 seconds)
2022-03-05 01:43:23 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-05 01:43:23 | INFO | train | epoch 065 | loss 2.816 | ppl 7.04 | wps 18384.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12650 | lr 0.000281161 | gnorm 1.032 | loss_scale 16 | train_wall 632 | gb_free 7.2 | wall 59262
2022-03-05 01:43:23 | INFO | fairseq.trainer | begin training epoch 66
2022-03-05 01:43:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:44:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:46:20 | INFO | train_inner | epoch 066:     51 / 196 loss=2.8, ppl=6.97, wps=18052.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=1.023, loss_scale=16, train_wall=325, gb_free=7.2, wall=59440
2022-03-05 01:52:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:52:13 | INFO | train_inner | epoch 066:    152 / 196 loss=2.801, ppl=6.97, wps=18562.8, ups=0.28, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=1.019, loss_scale=16, train_wall=326, gb_free=7.2, wall=59793
2022-03-05 01:54:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:54:53 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 10.099 | ppl 1096.46 | wps 35035.5 | wpb 510.9 | bsz 1 | num_updates 12844 | best_loss 7.017
2022-03-05 01:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12844 updates
2022-03-05 01:54:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:54:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:54:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 66 @ 12844 updates, score 10.099) (writing took 4.926343036815524 seconds)
2022-03-05 01:54:58 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-05 01:54:58 | INFO | train | epoch 066 | loss 2.79 | ppl 6.92 | wps 18252 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 12844 | lr 0.000279029 | gnorm 1.019 | loss_scale 16 | train_wall 633 | gb_free 7.2 | wall 59958
2022-03-05 01:54:58 | INFO | fairseq.trainer | begin training epoch 67
2022-03-05 01:54:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:58:15 | INFO | train_inner | epoch 067:     56 / 196 loss=2.765, ppl=6.8, wps=18067.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=1.004, loss_scale=16, train_wall=324, gb_free=7.2, wall=60155
2022-03-05 01:59:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:04:12 | INFO | train_inner | epoch 067:    157 / 196 loss=2.78, ppl=6.87, wps=18345.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=1.049, loss_scale=16, train_wall=330, gb_free=7.2, wall=60512
2022-03-05 02:06:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:06:35 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.217 | ppl 1190.43 | wps 33820.2 | wpb 510.9 | bsz 1 | num_updates 13039 | best_loss 7.017
2022-03-05 02:06:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13039 updates
2022-03-05 02:06:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:06:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:06:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 67 @ 13039 updates, score 10.217) (writing took 4.9877525717020035 seconds)
2022-03-05 02:06:40 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-05 02:06:40 | INFO | train | epoch 067 | loss 2.769 | ppl 6.82 | wps 18182.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13039 | lr 0.000276935 | gnorm 1.035 | loss_scale 16 | train_wall 638 | gb_free 7.2 | wall 60660
2022-03-05 02:06:40 | INFO | fairseq.trainer | begin training epoch 68
2022-03-05 02:06:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:07:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:10:19 | INFO | train_inner | epoch 068:     62 / 196 loss=2.744, ppl=6.7, wps=17842.7, ups=0.27, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=1.034, loss_scale=16, train_wall=328, gb_free=7.2, wall=60878
2022-03-05 02:15:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:16:13 | INFO | train_inner | epoch 068:    163 / 196 loss=2.757, ppl=6.76, wps=18504.1, ups=0.28, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=1.017, loss_scale=16, train_wall=328, gb_free=7.2, wall=61233
2022-03-05 02:18:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:18:14 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.245 | ppl 1213.12 | wps 35168 | wpb 510.9 | bsz 1 | num_updates 13233 | best_loss 7.017
2022-03-05 02:18:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13233 updates
2022-03-05 02:18:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:18:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:18:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 68 @ 13233 updates, score 10.245) (writing took 4.850667333230376 seconds)
2022-03-05 02:18:19 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-05 02:18:19 | INFO | train | epoch 068 | loss 2.744 | ppl 6.7 | wps 18185.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 13233 | lr 0.000274898 | gnorm 1.024 | loss_scale 16 | train_wall 635 | gb_free 7.2 | wall 61358
2022-03-05 02:18:19 | INFO | fairseq.trainer | begin training epoch 69
2022-03-05 02:18:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:22:13 | INFO | train_inner | epoch 069:     67 / 196 loss=2.718, ppl=6.58, wps=18138.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=1.029, loss_scale=16, train_wall=323, gb_free=7.2, wall=61593
2022-03-05 02:22:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:28:07 | INFO | train_inner | epoch 069:    168 / 196 loss=2.745, ppl=6.7, wps=18515.6, ups=0.28, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=1.027, loss_scale=16, train_wall=327, gb_free=7.2, wall=61947
2022-03-05 02:29:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:29:51 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.251 | ppl 1218.96 | wps 33994 | wpb 510.9 | bsz 1 | num_updates 13428 | best_loss 7.017
2022-03-05 02:29:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13428 updates
2022-03-05 02:29:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:29:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:29:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 69 @ 13428 updates, score 10.251) (writing took 4.864447919651866 seconds)
2022-03-05 02:29:56 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-05 02:29:56 | INFO | train | epoch 069 | loss 2.724 | ppl 6.6 | wps 18307.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13428 | lr 0.000272894 | gnorm 1.029 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 62055
2022-03-05 02:29:56 | INFO | fairseq.trainer | begin training epoch 70
2022-03-05 02:29:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:30:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:34:12 | INFO | train_inner | epoch 070:     73 / 196 loss=2.689, ppl=6.45, wps=17919.7, ups=0.27, wpb=65367, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=1.041, loss_scale=16, train_wall=327, gb_free=7.2, wall=62312
2022-03-05 02:38:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:39:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:40:11 | INFO | train_inner | epoch 070:    175 / 196 loss=2.723, ppl=6.6, wps=18239.9, ups=0.28, wpb=65532.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=1.053, loss_scale=8, train_wall=332, gb_free=7.2, wall=62671
2022-03-05 02:41:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:41:31 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.233 | ppl 1203.15 | wps 33516.4 | wpb 510.9 | bsz 1 | num_updates 13621 | best_loss 7.017
2022-03-05 02:41:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13621 updates
2022-03-05 02:41:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:41:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:41:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 70 @ 13621 updates, score 10.233) (writing took 4.885526970028877 seconds)
2022-03-05 02:41:36 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-05 02:41:36 | INFO | train | epoch 070 | loss 2.699 | ppl 6.5 | wps 18038.1 | ups 0.28 | wpb 65446.6 | bsz 127.8 | num_updates 13621 | lr 0.000270954 | gnorm 1.048 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 62756
2022-03-05 02:41:36 | INFO | fairseq.trainer | begin training epoch 71
2022-03-05 02:41:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:46:13 | INFO | train_inner | epoch 071:     79 / 196 loss=2.651, ppl=6.28, wps=18080.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=1.04, loss_scale=8, train_wall=324, gb_free=7.2, wall=63032
2022-03-05 02:52:02 | INFO | train_inner | epoch 071:    179 / 196 loss=2.715, ppl=6.57, wps=18743.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=13800, lr=0.000269191, gnorm=1.033, loss_scale=16, train_wall=323, gb_free=7.2, wall=63382
2022-03-05 02:53:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:53:07 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.366 | ppl 1319.9 | wps 34286.7 | wpb 510.9 | bsz 1 | num_updates 13817 | best_loss 7.017
2022-03-05 02:53:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13817 updates
2022-03-05 02:53:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:53:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 71 @ 13817 updates, score 10.366) (writing took 4.778200415894389 seconds)
2022-03-05 02:53:12 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-05 02:53:12 | INFO | train | epoch 071 | loss 2.68 | ppl 6.41 | wps 18419.6 | ups 0.28 | wpb 65448 | bsz 127.8 | num_updates 13817 | lr 0.000269025 | gnorm 1.034 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 63452
2022-03-05 02:53:12 | INFO | fairseq.trainer | begin training epoch 72
2022-03-05 02:53:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:54:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:58:07 | INFO | train_inner | epoch 072:     84 / 196 loss=2.628, ppl=6.18, wps=17940.2, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=1.04, loss_scale=16, train_wall=326, gb_free=7.2, wall=63746
2022-03-05 03:02:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:04:01 | INFO | train_inner | epoch 072:    185 / 196 loss=2.691, ppl=6.46, wps=18485.6, ups=0.28, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=1.045, loss_scale=16, train_wall=328, gb_free=7.2, wall=64101
2022-03-05 03:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:04:46 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.441 | ppl 1390.17 | wps 33302.6 | wpb 510.9 | bsz 1 | num_updates 14011 | best_loss 7.017
2022-03-05 03:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14011 updates
2022-03-05 03:04:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 72 @ 14011 updates, score 10.441) (writing took 5.146696258336306 seconds)
2022-03-05 03:04:51 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-05 03:04:51 | INFO | train | epoch 072 | loss 2.658 | ppl 6.31 | wps 18173.8 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 14011 | lr 0.000267156 | gnorm 1.042 | loss_scale 16 | train_wall 635 | gb_free 7.2 | wall 64151
2022-03-05 03:04:51 | INFO | fairseq.trainer | begin training epoch 73
2022-03-05 03:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:05:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:10:08 | INFO | train_inner | epoch 073:     90 / 196 loss=2.604, ppl=6.08, wps=17802.8, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=1.033, loss_scale=8, train_wall=328, gb_free=7.2, wall=64468
2022-03-05 03:16:01 | INFO | train_inner | epoch 073:    190 / 196 loss=2.68, ppl=6.41, wps=18599.2, ups=0.28, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=1.058, loss_scale=16, train_wall=326, gb_free=7.2, wall=64820
2022-03-05 03:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:16:27 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.428 | ppl 1377.97 | wps 34692.4 | wpb 510.9 | bsz 1 | num_updates 14206 | best_loss 7.017
2022-03-05 03:16:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14206 updates
2022-03-05 03:16:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:16:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 73 @ 14206 updates, score 10.428) (writing took 4.809770820662379 seconds)
2022-03-05 03:16:32 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-05 03:16:32 | INFO | train | epoch 073 | loss 2.638 | ppl 6.22 | wps 18204.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14206 | lr 0.000265316 | gnorm 1.045 | loss_scale 16 | train_wall 638 | gb_free 7.2 | wall 64852
2022-03-05 03:16:32 | INFO | fairseq.trainer | begin training epoch 74
2022-03-05 03:16:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:21:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:22:07 | INFO | train_inner | epoch 074:     96 / 196 loss=2.579, ppl=5.98, wps=17827.2, ups=0.27, wpb=65367, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=1.042, loss_scale=8, train_wall=329, gb_free=7.2, wall=65187
2022-03-05 03:27:57 | INFO | train_inner | epoch 074:    196 / 196 loss=2.66, ppl=6.32, wps=18714.1, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=14400, lr=0.000263523, gnorm=1.065, loss_scale=8, train_wall=323, gb_free=7.2, wall=65536
2022-03-05 03:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:28:03 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.457 | ppl 1405.87 | wps 34019.6 | wpb 510.9 | bsz 1 | num_updates 14400 | best_loss 7.017
2022-03-05 03:28:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14400 updates
2022-03-05 03:28:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:28:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 74 @ 14400 updates, score 10.457) (writing took 4.998489769175649 seconds)
2022-03-05 03:28:08 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-05 03:28:08 | INFO | train | epoch 074 | loss 2.618 | ppl 6.14 | wps 18235 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 14400 | lr 0.000263523 | gnorm 1.053 | loss_scale 8 | train_wall 633 | gb_free 7.2 | wall 65548
2022-03-05 03:28:08 | INFO | fairseq.trainer | begin training epoch 75
2022-03-05 03:28:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:34:03 | INFO | train_inner | epoch 075:    101 / 196 loss=2.557, ppl=5.88, wps=17899, ups=0.27, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=1.05, loss_scale=8, train_wall=328, gb_free=7.2, wall=65903
2022-03-05 03:39:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:39:43 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.516 | ppl 1464.27 | wps 33334 | wpb 510.9 | bsz 1 | num_updates 14595 | best_loss 7.017
2022-03-05 03:39:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14595 updates
2022-03-05 03:39:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:39:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:39:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 75 @ 14595 updates, score 10.516) (writing took 5.022939519956708 seconds)
2022-03-05 03:39:48 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-05 03:39:48 | INFO | train | epoch 075 | loss 2.599 | ppl 6.06 | wps 18229.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14595 | lr 0.000261757 | gnorm 1.065 | loss_scale 16 | train_wall 637 | gb_free 7.2 | wall 66248
2022-03-05 03:39:48 | INFO | fairseq.trainer | begin training epoch 76
2022-03-05 03:39:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:40:06 | INFO | train_inner | epoch 076:      5 / 196 loss=2.634, ppl=6.21, wps=17996.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=1.081, loss_scale=16, train_wall=325, gb_free=7.2, wall=66266
2022-03-05 03:42:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:45:59 | INFO | train_inner | epoch 076:    106 / 196 loss=2.543, ppl=5.83, wps=18563.5, ups=0.28, wpb=65532.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=1.07, loss_scale=8, train_wall=326, gb_free=7.2, wall=66619
2022-03-05 03:51:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:51:19 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.593 | ppl 1544.28 | wps 34539.2 | wpb 510.9 | bsz 1 | num_updates 14790 | best_loss 7.017
2022-03-05 03:51:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14790 updates
2022-03-05 03:51:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:51:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 76 @ 14790 updates, score 10.593) (writing took 4.920511357486248 seconds)
2022-03-05 03:51:24 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-05 03:51:24 | INFO | train | epoch 076 | loss 2.578 | ppl 5.97 | wps 18348.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14790 | lr 0.000260025 | gnorm 1.067 | loss_scale 16 | train_wall 633 | gb_free 7.2 | wall 66944
2022-03-05 03:51:24 | INFO | fairseq.trainer | begin training epoch 77
2022-03-05 03:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:51:59 | INFO | train_inner | epoch 077:     10 / 196 loss=2.609, ppl=6.1, wps=18171.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.066, loss_scale=16, train_wall=322, gb_free=7.2, wall=66979
2022-03-05 03:55:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:57:53 | INFO | train_inner | epoch 077:    111 / 196 loss=2.526, ppl=5.76, wps=18523.4, ups=0.28, wpb=65532.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=1.051, loss_scale=8, train_wall=327, gb_free=7.2, wall=67332
2022-03-05 04:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:02:56 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.566 | ppl 1516.41 | wps 33769.3 | wpb 510.9 | bsz 1 | num_updates 14985 | best_loss 7.017
2022-03-05 04:02:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14985 updates
2022-03-05 04:02:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:03:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:03:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 77 @ 14985 updates, score 10.566) (writing took 4.87179602868855 seconds)
2022-03-05 04:03:01 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-05 04:03:01 | INFO | train | epoch 077 | loss 2.561 | ppl 5.9 | wps 18301.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14985 | lr 0.000258328 | gnorm 1.071 | loss_scale 8 | train_wall 634 | gb_free 7.2 | wall 67641
2022-03-05 04:03:01 | INFO | fairseq.trainer | begin training epoch 78
2022-03-05 04:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:03:54 | INFO | train_inner | epoch 078:     15 / 196 loss=2.59, ppl=6.02, wps=18083.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.088, loss_scale=16, train_wall=324, gb_free=7.2, wall=67694
2022-03-05 04:06:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:09:50 | INFO | train_inner | epoch 078:    116 / 196 loss=2.511, ppl=5.7, wps=18404.5, ups=0.28, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=1.047, loss_scale=8, train_wall=329, gb_free=7.2, wall=68050
2022-03-05 04:14:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:14:36 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.608 | ppl 1560.19 | wps 34805.9 | wpb 510.9 | bsz 1 | num_updates 15180 | best_loss 7.017
2022-03-05 04:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15180 updates
2022-03-05 04:14:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 78 @ 15180 updates, score 10.608) (writing took 5.072828887030482 seconds)
2022-03-05 04:14:41 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-05 04:14:41 | INFO | train | epoch 078 | loss 2.541 | ppl 5.82 | wps 18233.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15180 | lr 0.000256664 | gnorm 1.069 | loss_scale 16 | train_wall 637 | gb_free 7.2 | wall 68341
2022-03-05 04:14:41 | INFO | fairseq.trainer | begin training epoch 79
2022-03-05 04:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:15:51 | INFO | train_inner | epoch 079:     20 / 196 loss=2.562, ppl=5.9, wps=18115.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.095, loss_scale=16, train_wall=323, gb_free=7.2, wall=68411
2022-03-05 04:21:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:21:44 | INFO | train_inner | epoch 079:    121 / 196 loss=2.499, ppl=5.65, wps=18554.8, ups=0.28, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.055, loss_scale=16, train_wall=327, gb_free=7.2, wall=68764
2022-03-05 04:26:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:26:13 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.731 | ppl 1699.52 | wps 34002.2 | wpb 510.9 | bsz 1 | num_updates 15375 | best_loss 7.017
2022-03-05 04:26:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15375 updates
2022-03-05 04:26:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:26:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:26:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 79 @ 15375 updates, score 10.731) (writing took 4.739261087030172 seconds)
2022-03-05 04:26:17 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-05 04:26:17 | INFO | train | epoch 079 | loss 2.524 | ppl 5.75 | wps 18327.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15375 | lr 0.000255031 | gnorm 1.067 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 69037
2022-03-05 04:26:17 | INFO | fairseq.trainer | begin training epoch 80
2022-03-05 04:26:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:27:45 | INFO | train_inner | epoch 080:     25 / 196 loss=2.539, ppl=5.81, wps=18095.9, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.063, loss_scale=16, train_wall=324, gb_free=7.2, wall=69125
2022-03-05 04:29:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:33:40 | INFO | train_inner | epoch 080:    126 / 196 loss=2.495, ppl=5.64, wps=18506.2, ups=0.28, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=1.071, loss_scale=16, train_wall=328, gb_free=7.2, wall=69479
2022-03-05 04:36:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:37:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:37:52 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.76 | ppl 1734.61 | wps 33656.2 | wpb 510.9 | bsz 1 | num_updates 15569 | best_loss 7.017
2022-03-05 04:37:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15569 updates
2022-03-05 04:37:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:37:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:37:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 80 @ 15569 updates, score 10.76) (writing took 5.042083989828825 seconds)
2022-03-05 04:37:57 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-05 04:37:57 | INFO | train | epoch 080 | loss 2.506 | ppl 5.68 | wps 18161.6 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 15569 | lr 0.000253437 | gnorm 1.07 | loss_scale 16 | train_wall 636 | gb_free 7.2 | wall 69736
2022-03-05 04:37:57 | INFO | fairseq.trainer | begin training epoch 81
2022-03-05 04:37:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:39:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:39:49 | INFO | train_inner | epoch 081:     32 / 196 loss=2.512, ppl=5.7, wps=17681.4, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.073, loss_scale=8, train_wall=331, gb_free=7.2, wall=69849
2022-03-05 04:45:40 | INFO | train_inner | epoch 081:    132 / 196 loss=2.476, ppl=5.56, wps=18667.1, ups=0.28, wpb=65532.4, bsz=128, num_updates=15700, lr=0.000252377, gnorm=1.067, loss_scale=8, train_wall=325, gb_free=7.2, wall=70200
2022-03-05 04:49:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:49:29 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.72 | ppl 1687.1 | wps 34856.6 | wpb 510.9 | bsz 1 | num_updates 15764 | best_loss 7.017
2022-03-05 04:49:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15764 updates
2022-03-05 04:49:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:49:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:49:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 81 @ 15764 updates, score 10.72) (writing took 4.9340038653463125 seconds)
2022-03-05 04:49:34 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-05 04:49:34 | INFO | train | epoch 081 | loss 2.49 | ppl 5.62 | wps 18302.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15764 | lr 0.000251864 | gnorm 1.069 | loss_scale 16 | train_wall 635 | gb_free 7.2 | wall 70433
2022-03-05 04:49:34 | INFO | fairseq.trainer | begin training epoch 82
2022-03-05 04:49:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:51:39 | INFO | train_inner | epoch 082:     36 / 196 loss=2.498, ppl=5.65, wps=18208.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.089, loss_scale=16, train_wall=322, gb_free=7.2, wall=70559
2022-03-05 04:52:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:57:33 | INFO | train_inner | epoch 082:    137 / 196 loss=2.462, ppl=5.51, wps=18541.7, ups=0.28, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.101, loss_scale=8, train_wall=327, gb_free=7.2, wall=70912
2022-03-05 05:00:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:01:05 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.768 | ppl 1744.13 | wps 33984.8 | wpb 510.9 | bsz 1 | num_updates 15959 | best_loss 7.017
2022-03-05 05:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15959 updates
2022-03-05 05:01:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:01:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:01:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 82 @ 15959 updates, score 10.768) (writing took 4.786398388445377 seconds)
2022-03-05 05:01:10 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-05 05:01:10 | INFO | train | epoch 082 | loss 2.472 | ppl 5.55 | wps 18328.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15959 | lr 0.000250321 | gnorm 1.099 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 71130
2022-03-05 05:01:10 | INFO | fairseq.trainer | begin training epoch 83
2022-03-05 05:01:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:03:34 | INFO | train_inner | epoch 083:     41 / 196 loss=2.465, ppl=5.52, wps=18089.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.084, loss_scale=16, train_wall=324, gb_free=7.2, wall=71274
2022-03-05 05:07:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:09:30 | INFO | train_inner | epoch 083:    142 / 196 loss=2.45, ppl=5.47, wps=18424.7, ups=0.28, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.078, loss_scale=16, train_wall=329, gb_free=7.2, wall=71629
2022-03-05 05:12:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:12:46 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.861 | ppl 1859.41 | wps 33629.9 | wpb 510.9 | bsz 1 | num_updates 16154 | best_loss 7.017
2022-03-05 05:12:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16154 updates
2022-03-05 05:12:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:12:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:12:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 83 @ 16154 updates, score 10.861) (writing took 5.290069222450256 seconds)
2022-03-05 05:12:51 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-05 05:12:51 | INFO | train | epoch 083 | loss 2.456 | ppl 5.49 | wps 18204.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16154 | lr 0.000248805 | gnorm 1.076 | loss_scale 16 | train_wall 637 | gb_free 7.2 | wall 71831
2022-03-05 05:12:51 | INFO | fairseq.trainer | begin training epoch 84
2022-03-05 05:12:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:15:34 | INFO | train_inner | epoch 084:     46 / 196 loss=2.454, ppl=5.48, wps=17950.4, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.071, loss_scale=16, train_wall=326, gb_free=7.2, wall=71994
2022-03-05 05:15:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:15:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:21:33 | INFO | train_inner | epoch 084:    148 / 196 loss=2.44, ppl=5.43, wps=18252, ups=0.28, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.078, loss_scale=8, train_wall=332, gb_free=7.2, wall=72353
2022-03-05 05:23:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:24:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:27 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.904 | ppl 1916.82 | wps 34099.1 | wpb 510.9 | bsz 1 | num_updates 16347 | best_loss 7.017
2022-03-05 05:24:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16347 updates
2022-03-05 05:24:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:24:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:24:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 84 @ 16347 updates, score 10.904) (writing took 4.729738123714924 seconds)
2022-03-05 05:24:31 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-05 05:24:31 | INFO | train | epoch 084 | loss 2.44 | ppl 5.43 | wps 18039.4 | ups 0.28 | wpb 65446.6 | bsz 127.8 | num_updates 16347 | lr 0.000247332 | gnorm 1.074 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 72531
2022-03-05 05:24:31 | INFO | fairseq.trainer | begin training epoch 85
2022-03-05 05:24:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:27:37 | INFO | train_inner | epoch 085:     53 / 196 loss=2.427, ppl=5.38, wps=17954, ups=0.27, wpb=65367, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.073, loss_scale=8, train_wall=326, gb_free=7.2, wall=72717
2022-03-05 05:33:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:33:31 | INFO | train_inner | epoch 085:    154 / 196 loss=2.436, ppl=5.41, wps=18503.7, ups=0.28, wpb=65532.4, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.084, loss_scale=8, train_wall=328, gb_free=7.2, wall=73071
2022-03-05 05:35:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:36:05 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.927 | ppl 1946.43 | wps 33685.2 | wpb 510.9 | bsz 1 | num_updates 16542 | best_loss 7.017
2022-03-05 05:36:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16542 updates
2022-03-05 05:36:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:36:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 85 @ 16542 updates, score 10.927) (writing took 4.83546063490212 seconds)
2022-03-05 05:36:09 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-05 05:36:09 | INFO | train | epoch 085 | loss 2.424 | ppl 5.37 | wps 18284.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16542 | lr 0.00024587 | gnorm 1.085 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 73229
2022-03-05 05:36:09 | INFO | fairseq.trainer | begin training epoch 86
2022-03-05 05:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:39:34 | INFO | train_inner | epoch 086:     58 / 196 loss=2.398, ppl=5.27, wps=18004, ups=0.28, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.094, loss_scale=8, train_wall=325, gb_free=7.2, wall=73434
2022-03-05 05:44:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:45:30 | INFO | train_inner | epoch 086:    159 / 196 loss=2.427, ppl=5.38, wps=18408.6, ups=0.28, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.095, loss_scale=8, train_wall=329, gb_free=7.2, wall=73790
2022-03-05 05:47:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:47:44 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.998 | ppl 2044.88 | wps 37094.1 | wpb 510.9 | bsz 1 | num_updates 16737 | best_loss 7.017
2022-03-05 05:47:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16737 updates
2022-03-05 05:47:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:47:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:47:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 86 @ 16737 updates, score 10.998) (writing took 5.02043785341084 seconds)
2022-03-05 05:47:49 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-05 05:47:49 | INFO | train | epoch 086 | loss 2.409 | ppl 5.31 | wps 18243.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16737 | lr 0.000244434 | gnorm 1.096 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 73929
2022-03-05 05:47:49 | INFO | fairseq.trainer | begin training epoch 87
2022-03-05 05:47:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:51:28 | INFO | train_inner | epoch 087:     63 / 196 loss=2.386, ppl=5.23, wps=18275.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.093, loss_scale=8, train_wall=321, gb_free=7.2, wall=74148
2022-03-05 05:57:19 | INFO | train_inner | epoch 087:    163 / 196 loss=2.408, ppl=5.31, wps=18681.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.083, loss_scale=16, train_wall=324, gb_free=7.2, wall=74498
2022-03-05 05:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:59:20 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.021 | ppl 2077.34 | wps 34517.9 | wpb 510.9 | bsz 1 | num_updates 16933 | best_loss 7.017
2022-03-05 05:59:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16933 updates
2022-03-05 05:59:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:59:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:59:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 87 @ 16933 updates, score 11.021) (writing took 4.903539868071675 seconds)
2022-03-05 05:59:25 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-05 05:59:25 | INFO | train | epoch 087 | loss 2.394 | ppl 5.26 | wps 18436.8 | ups 0.28 | wpb 65448 | bsz 127.8 | num_updates 16933 | lr 0.000243015 | gnorm 1.091 | loss_scale 16 | train_wall 633 | gb_free 7.2 | wall 74624
2022-03-05 05:59:25 | INFO | fairseq.trainer | begin training epoch 88
2022-03-05 05:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:59:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:00:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:03:26 | INFO | train_inner | epoch 088:     69 / 196 loss=2.368, ppl=5.16, wps=17780, ups=0.27, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.091, loss_scale=8, train_wall=330, gb_free=7.2, wall=74866
2022-03-05 06:08:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:09:22 | INFO | train_inner | epoch 088:    170 / 196 loss=2.395, ppl=5.26, wps=18451.5, ups=0.28, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.105, loss_scale=8, train_wall=328, gb_free=7.2, wall=75221
2022-03-05 06:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:10:59 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.052 | ppl 2122.86 | wps 33660 | wpb 510.9 | bsz 1 | num_updates 17126 | best_loss 7.017
2022-03-05 06:10:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17126 updates
2022-03-05 06:10:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:11:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 88 @ 17126 updates, score 11.052) (writing took 5.025038328021765 seconds)
2022-03-05 06:11:04 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-05 06:11:04 | INFO | train | epoch 088 | loss 2.377 | ppl 5.19 | wps 18066.6 | ups 0.28 | wpb 65446.6 | bsz 127.8 | num_updates 17126 | lr 0.000241642 | gnorm 1.09 | loss_scale 8 | train_wall 636 | gb_free 7.2 | wall 75324
2022-03-05 06:11:04 | INFO | fairseq.trainer | begin training epoch 89
2022-03-05 06:11:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:15:25 | INFO | train_inner | epoch 089:     74 / 196 loss=2.346, ppl=5.09, wps=17998.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.076, loss_scale=8, train_wall=325, gb_free=7.2, wall=75584
2022-03-05 06:19:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:21:18 | INFO | train_inner | epoch 089:    175 / 196 loss=2.39, ppl=5.24, wps=18562.6, ups=0.28, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.109, loss_scale=8, train_wall=327, gb_free=7.2, wall=75937
2022-03-05 06:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:22:37 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.037 | ppl 2101.41 | wps 35582.9 | wpb 510.9 | bsz 1 | num_updates 17321 | best_loss 7.017
2022-03-05 06:22:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17321 updates
2022-03-05 06:22:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:22:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 89 @ 17321 updates, score 11.037) (writing took 4.988522795960307 seconds)
2022-03-05 06:22:42 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-05 06:22:42 | INFO | train | epoch 089 | loss 2.365 | ppl 5.15 | wps 18293 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17321 | lr 0.000240278 | gnorm 1.097 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 76021
2022-03-05 06:22:42 | INFO | fairseq.trainer | begin training epoch 90
2022-03-05 06:22:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:27:19 | INFO | train_inner | epoch 090:     79 / 196 loss=2.325, ppl=5.01, wps=18102.9, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.101, loss_scale=16, train_wall=323, gb_free=7.2, wall=76298
2022-03-05 06:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:33:13 | INFO | train_inner | epoch 090:    180 / 196 loss=2.382, ppl=5.21, wps=18517.6, ups=0.28, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.111, loss_scale=8, train_wall=327, gb_free=7.2, wall=76652
2022-03-05 06:34:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:34:14 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.138 | ppl 2254.03 | wps 34183.5 | wpb 510.9 | bsz 1 | num_updates 17516 | best_loss 7.017
2022-03-05 06:34:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17516 updates
2022-03-05 06:34:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:34:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:34:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 90 @ 17516 updates, score 11.138) (writing took 4.849067384377122 seconds)
2022-03-05 06:34:19 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-05 06:34:19 | INFO | train | epoch 090 | loss 2.349 | ppl 5.09 | wps 18289.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17516 | lr 0.000238937 | gnorm 1.103 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 76719
2022-03-05 06:34:19 | INFO | fairseq.trainer | begin training epoch 91
2022-03-05 06:34:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:39:15 | INFO | train_inner | epoch 091:     84 / 196 loss=2.309, ppl=4.95, wps=18062.7, ups=0.28, wpb=65367, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.085, loss_scale=16, train_wall=324, gb_free=7.2, wall=77014
2022-03-05 06:42:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:45:11 | INFO | train_inner | epoch 091:    185 / 196 loss=2.362, ppl=5.14, wps=18404, ups=0.28, wpb=65532.4, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.114, loss_scale=8, train_wall=329, gb_free=7.2, wall=77370
2022-03-05 06:45:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:45:55 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.132 | ppl 2243.58 | wps 33589.9 | wpb 510.9 | bsz 1 | num_updates 17711 | best_loss 7.017
2022-03-05 06:45:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17711 updates
2022-03-05 06:45:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:46:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:46:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 91 @ 17711 updates, score 11.132) (writing took 4.924815483391285 seconds)
2022-03-05 06:46:00 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-05 06:46:00 | INFO | train | epoch 091 | loss 2.335 | ppl 5.05 | wps 18211.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17711 | lr 0.000237618 | gnorm 1.101 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 77420
2022-03-05 06:46:00 | INFO | fairseq.trainer | begin training epoch 92
2022-03-05 06:46:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:51:11 | INFO | train_inner | epoch 092:     89 / 196 loss=2.294, ppl=4.9, wps=18141.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.092, loss_scale=16, train_wall=323, gb_free=7.2, wall=77731
2022-03-05 06:54:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:57:04 | INFO | train_inner | epoch 092:    190 / 196 loss=2.356, ppl=5.12, wps=18574.7, ups=0.28, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.095, loss_scale=8, train_wall=326, gb_free=7.2, wall=78083
2022-03-05 06:57:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:30 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.204 | ppl 2358.51 | wps 34302.4 | wpb 510.9 | bsz 1 | num_updates 17906 | best_loss 7.017
2022-03-05 06:57:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17906 updates
2022-03-05 06:57:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:57:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 92 @ 17906 updates, score 11.204) (writing took 4.798640480265021 seconds)
2022-03-05 06:57:35 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-05 06:57:35 | INFO | train | epoch 092 | loss 2.322 | ppl 5 | wps 18359 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17906 | lr 0.00023632 | gnorm 1.092 | loss_scale 8 | train_wall 632 | gb_free 7.2 | wall 78115
2022-03-05 06:57:35 | INFO | fairseq.trainer | begin training epoch 93
2022-03-05 06:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:03:05 | INFO | train_inner | epoch 093:     94 / 196 loss=2.27, ppl=4.82, wps=18120, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.092, loss_scale=16, train_wall=323, gb_free=7.2, wall=78444
2022-03-05 07:03:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:08:59 | INFO | train_inner | epoch 093:    195 / 196 loss=2.35, ppl=5.1, wps=18498.5, ups=0.28, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.111, loss_scale=8, train_wall=328, gb_free=7.2, wall=78798
2022-03-05 07:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:09:08 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.176 | ppl 2314.05 | wps 33556.6 | wpb 510.9 | bsz 1 | num_updates 18101 | best_loss 7.017
2022-03-05 07:09:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18101 updates
2022-03-05 07:09:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:09:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 93 @ 18101 updates, score 11.176) (writing took 4.852370141074061 seconds)
2022-03-05 07:09:13 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-05 07:09:13 | INFO | train | epoch 093 | loss 2.308 | ppl 4.95 | wps 18293.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18101 | lr 0.000235044 | gnorm 1.102 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 78813
2022-03-05 07:09:13 | INFO | fairseq.trainer | begin training epoch 94
2022-03-05 07:09:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:15:02 | INFO | train_inner | epoch 094:     99 / 196 loss=2.259, ppl=4.79, wps=17976, ups=0.28, wpb=65367, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.104, loss_scale=16, train_wall=326, gb_free=7.2, wall=79162
2022-03-05 07:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:20:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:20:49 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.293 | ppl 2508.72 | wps 34446.8 | wpb 510.9 | bsz 1 | num_updates 18296 | best_loss 7.017
2022-03-05 07:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18296 updates
2022-03-05 07:20:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:20:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:20:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 94 @ 18296 updates, score 11.293) (writing took 4.7716979421675205 seconds)
2022-03-05 07:20:54 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-05 07:20:54 | INFO | train | epoch 094 | loss 2.295 | ppl 4.91 | wps 18212.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18296 | lr 0.000233788 | gnorm 1.104 | loss_scale 16 | train_wall 638 | gb_free 7.2 | wall 79513
2022-03-05 07:20:54 | INFO | fairseq.trainer | begin training epoch 95
2022-03-05 07:20:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:21:08 | INFO | train_inner | epoch 095:      4 / 196 loss=2.331, ppl=5.03, wps=17899.1, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=18300, lr=0.000233762, gnorm=1.104, loss_scale=16, train_wall=327, gb_free=7.2, wall=79527
2022-03-05 07:26:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:27:00 | INFO | train_inner | epoch 095:    105 / 196 loss=2.247, ppl=4.75, wps=18597.2, ups=0.28, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=1.095, loss_scale=16, train_wall=326, gb_free=7.2, wall=79880
2022-03-05 07:29:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:32:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:32:24 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.322 | ppl 2559.38 | wps 34302.7 | wpb 510.9 | bsz 1 | num_updates 18490 | best_loss 7.017
2022-03-05 07:32:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18490 updates
2022-03-05 07:32:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:32:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:32:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 95 @ 18490 updates, score 11.322) (writing took 4.748967472463846 seconds)
2022-03-05 07:32:29 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-05 07:32:29 | INFO | train | epoch 095 | loss 2.282 | ppl 4.86 | wps 18262.8 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 18490 | lr 0.000232558 | gnorm 1.104 | loss_scale 8 | train_wall 633 | gb_free 7.2 | wall 80208
2022-03-05 07:32:29 | INFO | fairseq.trainer | begin training epoch 96
2022-03-05 07:32:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:33:04 | INFO | train_inner | epoch 096:     10 / 196 loss=2.313, ppl=4.97, wps=17962, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.11, loss_scale=8, train_wall=326, gb_free=7.2, wall=80244
2022-03-05 07:38:54 | INFO | train_inner | epoch 096:    110 / 196 loss=2.236, ppl=4.71, wps=18707.6, ups=0.29, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.095, loss_scale=16, train_wall=324, gb_free=7.2, wall=80594
2022-03-05 07:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:43:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:44:04 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.292 | ppl 2507.78 | wps 33469.9 | wpb 510.9 | bsz 1 | num_updates 18685 | best_loss 7.017
2022-03-05 07:44:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18685 updates
2022-03-05 07:44:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:44:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:44:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 96 @ 18685 updates, score 11.292) (writing took 5.360858244821429 seconds)
2022-03-05 07:44:09 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-05 07:44:09 | INFO | train | epoch 096 | loss 2.268 | ppl 4.82 | wps 18216.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18685 | lr 0.000231341 | gnorm 1.106 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 80909
2022-03-05 07:44:09 | INFO | fairseq.trainer | begin training epoch 97
2022-03-05 07:44:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:45:02 | INFO | train_inner | epoch 097:     15 / 196 loss=2.29, ppl=4.89, wps=17753.1, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.125, loss_scale=8, train_wall=329, gb_free=7.2, wall=80962
2022-03-05 07:50:55 | INFO | train_inner | epoch 097:    115 / 196 loss=2.23, ppl=4.69, wps=18591.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.094, loss_scale=16, train_wall=326, gb_free=7.2, wall=81315
2022-03-05 07:53:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:55:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:55:44 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.36 | ppl 2628.94 | wps 34419 | wpb 510.9 | bsz 1 | num_updates 18880 | best_loss 7.017
2022-03-05 07:55:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18880 updates
2022-03-05 07:55:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:55:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:55:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 97 @ 18880 updates, score 11.36) (writing took 4.806923370808363 seconds)
2022-03-05 07:55:48 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-05 07:55:48 | INFO | train | epoch 097 | loss 2.255 | ppl 4.77 | wps 18257.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18880 | lr 0.000230144 | gnorm 1.113 | loss_scale 8 | train_wall 636 | gb_free 7.2 | wall 81608
2022-03-05 07:55:48 | INFO | fairseq.trainer | begin training epoch 98
2022-03-05 07:55:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:56:59 | INFO | train_inner | epoch 098:     20 / 196 loss=2.274, ppl=4.84, wps=17966.4, ups=0.27, wpb=65367, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.125, loss_scale=8, train_wall=326, gb_free=7.2, wall=81678
2022-03-05 08:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:02:54 | INFO | train_inner | epoch 098:    121 / 196 loss=2.219, ppl=4.66, wps=18470.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.111, loss_scale=8, train_wall=328, gb_free=7.2, wall=82033
2022-03-05 08:07:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:07:22 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.344 | ppl 2600.32 | wps 34046.5 | wpb 510.9 | bsz 1 | num_updates 19075 | best_loss 7.017
2022-03-05 08:07:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19075 updates
2022-03-05 08:07:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:07:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:07:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 98 @ 19075 updates, score 11.344) (writing took 4.782915875315666 seconds)
2022-03-05 08:07:27 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-05 08:07:27 | INFO | train | epoch 098 | loss 2.243 | ppl 4.73 | wps 18273.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19075 | lr 0.000228964 | gnorm 1.106 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 82306
2022-03-05 08:07:27 | INFO | fairseq.trainer | begin training epoch 99
2022-03-05 08:07:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:08:54 | INFO | train_inner | epoch 099:     25 / 196 loss=2.264, ppl=4.8, wps=18111.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.11, loss_scale=8, train_wall=323, gb_free=7.2, wall=82394
2022-03-05 08:14:47 | INFO | train_inner | epoch 099:    125 / 196 loss=2.214, ppl=4.64, wps=18608.2, ups=0.28, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.099, loss_scale=16, train_wall=326, gb_free=7.2, wall=82746
2022-03-05 08:16:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:18:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:18:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:19:03 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 11.394 | ppl 2691.66 | wps 33794.1 | wpb 510.9 | bsz 1 | num_updates 19269 | best_loss 7.017
2022-03-05 08:19:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19269 updates
2022-03-05 08:19:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:19:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:19:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 99 @ 19269 updates, score 11.394) (writing took 5.224390722811222 seconds)
2022-03-05 08:19:08 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-05 08:19:08 | INFO | train | epoch 099 | loss 2.231 | ppl 4.69 | wps 18107.4 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 19269 | lr 0.000227809 | gnorm 1.109 | loss_scale 8 | train_wall 638 | gb_free 7.2 | wall 83008
2022-03-05 08:19:08 | INFO | fairseq.trainer | begin training epoch 100
2022-03-05 08:19:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:20:57 | INFO | train_inner | epoch 100:     31 / 196 loss=2.244, ppl=4.74, wps=17650.5, ups=0.27, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.108, loss_scale=8, train_wall=332, gb_free=7.2, wall=83117
2022-03-05 08:26:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:26:50 | INFO | train_inner | epoch 100:    132 / 196 loss=2.201, ppl=4.6, wps=18554.9, ups=0.28, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.101, loss_scale=8, train_wall=327, gb_free=7.2, wall=83470
2022-03-05 08:30:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:30:41 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 11.433 | ppl 2764.96 | wps 34218.5 | wpb 510.9 | bsz 1 | num_updates 19464 | best_loss 7.017
2022-03-05 08:30:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19464 updates
2022-03-05 08:30:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:30:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 100 @ 19464 updates, score 11.433) (writing took 4.848321612924337 seconds)
2022-03-05 08:30:46 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-05 08:30:46 | INFO | train | epoch 100 | loss 2.22 | ppl 4.66 | wps 18296.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19464 | lr 0.000226665 | gnorm 1.108 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 83705
2022-03-05 08:30:46 | INFO | fairseq.trainer | begin training epoch 101
2022-03-05 08:30:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:32:52 | INFO | train_inner | epoch 101:     36 / 196 loss=2.234, ppl=4.7, wps=18059.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.128, loss_scale=8, train_wall=324, gb_free=7.2, wall=83832
2022-03-05 08:37:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:38:47 | INFO | train_inner | epoch 101:    137 / 196 loss=2.196, ppl=4.58, wps=18466, ups=0.28, wpb=65532.4, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.136, loss_scale=8, train_wall=328, gb_free=7.2, wall=84187
2022-03-05 08:42:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:42:20 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 11.423 | ppl 2746.6 | wps 33286.5 | wpb 510.9 | bsz 1 | num_updates 19659 | best_loss 7.017
2022-03-05 08:42:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19659 updates
2022-03-05 08:42:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:42:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:42:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 101 @ 19659 updates, score 11.423) (writing took 4.797110490500927 seconds)
2022-03-05 08:42:25 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-05 08:42:25 | INFO | train | epoch 101 | loss 2.208 | ppl 4.62 | wps 18256.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19659 | lr 0.000225538 | gnorm 1.133 | loss_scale 8 | train_wall 636 | gb_free 7.2 | wall 84404
2022-03-05 08:42:25 | INFO | fairseq.trainer | begin training epoch 102
2022-03-05 08:42:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:44:49 | INFO | train_inner | epoch 102:     41 / 196 loss=2.218, ppl=4.65, wps=18041.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.124, loss_scale=8, train_wall=324, gb_free=7.2, wall=84549
2022-03-05 08:50:42 | INFO | train_inner | epoch 102:    141 / 196 loss=2.189, ppl=4.56, wps=18576.4, ups=0.28, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.121, loss_scale=16, train_wall=326, gb_free=7.2, wall=84902
2022-03-05 08:52:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:54:00 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 11.528 | ppl 2953.04 | wps 34353.9 | wpb 510.9 | bsz 1 | num_updates 19854 | best_loss 7.017
2022-03-05 08:54:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19854 updates
2022-03-05 08:54:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:54:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:54:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 102 @ 19854 updates, score 11.528) (writing took 4.983836757019162 seconds)
2022-03-05 08:54:05 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-05 08:54:05 | INFO | train | epoch 102 | loss 2.196 | ppl 4.58 | wps 18224.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19854 | lr 0.000224427 | gnorm 1.122 | loss_scale 16 | train_wall 637 | gb_free 7.2 | wall 85105
2022-03-05 08:54:05 | INFO | fairseq.trainer | begin training epoch 103
2022-03-05 08:54:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:54:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:56:49 | INFO | train_inner | epoch 103:     47 / 196 loss=2.185, ppl=4.55, wps=17816.2, ups=0.27, wpb=65359.9, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.127, loss_scale=8, train_wall=329, gb_free=7.2, wall=85269
2022-03-05 09:02:40 | INFO | train_inner | epoch 103:    147 / 196 loss=2.185, ppl=4.55, wps=18665.9, ups=0.28, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.128, loss_scale=16, train_wall=325, gb_free=7.2, wall=85620
2022-03-05 09:03:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:05:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:05:38 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 11.589 | ppl 3079.77 | wps 34263.3 | wpb 510.9 | bsz 1 | num_updates 20048 | best_loss 7.017
2022-03-05 09:05:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20048 updates
2022-03-05 09:05:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:05:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:05:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 103 @ 20048 updates, score 11.589) (writing took 4.880036320537329 seconds)
2022-03-05 09:05:42 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-05 09:05:42 | INFO | train | epoch 103 | loss 2.183 | ppl 4.54 | wps 18200.7 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 20048 | lr 0.000223339 | gnorm 1.129 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 85802
2022-03-05 09:05:42 | INFO | fairseq.trainer | begin training epoch 104
2022-03-05 09:05:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:08:45 | INFO | train_inner | epoch 104:     52 / 196 loss=2.176, ppl=4.52, wps=17901.8, ups=0.27, wpb=65367, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.133, loss_scale=8, train_wall=327, gb_free=7.2, wall=85985
2022-03-05 09:12:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:14:41 | INFO | train_inner | epoch 104:    153 / 196 loss=2.175, ppl=4.52, wps=18445.2, ups=0.28, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.119, loss_scale=8, train_wall=329, gb_free=7.2, wall=86340
2022-03-05 09:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:17:18 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 11.498 | ppl 2891.94 | wps 33939.8 | wpb 510.9 | bsz 1 | num_updates 20243 | best_loss 7.017
2022-03-05 09:17:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20243 updates
2022-03-05 09:17:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:17:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:17:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 104 @ 20243 updates, score 11.498) (writing took 5.193115148693323 seconds)
2022-03-05 09:17:23 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-05 09:17:23 | INFO | train | epoch 104 | loss 2.172 | ppl 4.51 | wps 18213.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20243 | lr 0.000222261 | gnorm 1.125 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 86503
2022-03-05 09:17:23 | INFO | fairseq.trainer | begin training epoch 105
2022-03-05 09:17:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:20:44 | INFO | train_inner | epoch 105:     57 / 196 loss=2.162, ppl=4.47, wps=17967.9, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.129, loss_scale=16, train_wall=325, gb_free=7.2, wall=86704
2022-03-05 09:24:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:26:37 | INFO | train_inner | epoch 105:    158 / 196 loss=2.175, ppl=4.52, wps=18563.7, ups=0.28, wpb=65532.4, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.129, loss_scale=8, train_wall=327, gb_free=7.2, wall=87057
2022-03-05 09:28:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:28:55 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 11.563 | ppl 3024.99 | wps 34718.3 | wpb 510.9 | bsz 1 | num_updates 20438 | best_loss 7.017
2022-03-05 09:28:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20438 updates
2022-03-05 09:28:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:29:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 105 @ 20438 updates, score 11.563) (writing took 4.829442957416177 seconds)
2022-03-05 09:29:00 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-05 09:29:00 | INFO | train | epoch 105 | loss 2.163 | ppl 4.48 | wps 18312.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20438 | lr 0.000221198 | gnorm 1.128 | loss_scale 8 | train_wall 634 | gb_free 7.2 | wall 87200
2022-03-05 09:29:00 | INFO | fairseq.trainer | begin training epoch 106
2022-03-05 09:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:32:37 | INFO | train_inner | epoch 106:     62 / 196 loss=2.138, ppl=4.4, wps=18162.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.119, loss_scale=16, train_wall=322, gb_free=7.2, wall=87417
2022-03-05 09:36:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:38:32 | INFO | train_inner | epoch 106:    163 / 196 loss=2.172, ppl=4.51, wps=18494.5, ups=0.28, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.129, loss_scale=8, train_wall=328, gb_free=7.2, wall=87771
2022-03-05 09:40:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:40:33 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 11.605 | ppl 3115.66 | wps 33711.4 | wpb 510.9 | bsz 1 | num_updates 20633 | best_loss 7.017
2022-03-05 09:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20633 updates
2022-03-05 09:40:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:40:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:40:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 106 @ 20633 updates, score 11.605) (writing took 4.957359874621034 seconds)
2022-03-05 09:40:38 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-05 09:40:38 | INFO | train | epoch 106 | loss 2.153 | ppl 4.45 | wps 18290.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20633 | lr 0.00022015 | gnorm 1.126 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 87897
2022-03-05 09:40:38 | INFO | fairseq.trainer | begin training epoch 107
2022-03-05 09:40:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:44:33 | INFO | train_inner | epoch 107:     67 / 196 loss=2.125, ppl=4.36, wps=18105.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.123, loss_scale=16, train_wall=323, gb_free=7.2, wall=88132
2022-03-05 09:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:50:29 | INFO | train_inner | epoch 107:    168 / 196 loss=2.162, ppl=4.48, wps=18376.1, ups=0.28, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.126, loss_scale=8, train_wall=330, gb_free=7.2, wall=88489
2022-03-05 09:52:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:52:14 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 11.656 | ppl 3227.75 | wps 33315 | wpb 510.9 | bsz 1 | num_updates 20828 | best_loss 7.017
2022-03-05 09:52:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20828 updates
2022-03-05 09:52:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:52:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:52:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 107 @ 20828 updates, score 11.656) (writing took 4.953092273324728 seconds)
2022-03-05 09:52:19 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-05 09:52:19 | INFO | train | epoch 107 | loss 2.141 | ppl 4.41 | wps 18207.5 | ups 0.28 | wpb 65449.4 | bsz 127.8 | num_updates 20828 | lr 0.000219117 | gnorm 1.123 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 88598
2022-03-05 09:52:19 | INFO | fairseq.trainer | begin training epoch 108
2022-03-05 09:52:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:54:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:56:37 | INFO | train_inner | epoch 108:     73 / 196 loss=2.112, ppl=4.32, wps=17797.2, ups=0.27, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.122, loss_scale=8, train_wall=329, gb_free=7.2, wall=88856
2022-03-05 10:01:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:02:31 | INFO | train_inner | epoch 108:    174 / 196 loss=2.152, ppl=4.44, wps=18496.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.133, loss_scale=8, train_wall=328, gb_free=7.2, wall=89210
2022-03-05 10:03:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:03:54 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 11.64 | ppl 3191.95 | wps 34030.5 | wpb 510.9 | bsz 1 | num_updates 21022 | best_loss 7.017
2022-03-05 10:03:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21022 updates
2022-03-05 10:03:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:03:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:03:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 108 @ 21022 updates, score 11.64) (writing took 4.75127917714417 seconds)
2022-03-05 10:03:58 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-05 10:03:58 | INFO | train | epoch 108 | loss 2.13 | ppl 4.38 | wps 18149.4 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 21022 | lr 0.000218104 | gnorm 1.129 | loss_scale 8 | train_wall 637 | gb_free 7.2 | wall 89298
2022-03-05 10:03:58 | INFO | fairseq.trainer | begin training epoch 109
2022-03-05 10:03:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:08:32 | INFO | train_inner | epoch 109:     78 / 196 loss=2.102, ppl=4.29, wps=18095, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.131, loss_scale=8, train_wall=324, gb_free=7.2, wall=89572
2022-03-05 10:14:23 | INFO | train_inner | epoch 109:    178 / 196 loss=2.148, ppl=4.43, wps=18698.3, ups=0.29, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.126, loss_scale=16, train_wall=324, gb_free=7.2, wall=89922
2022-03-05 10:15:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:15:32 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 11.686 | ppl 3293.8 | wps 33539.9 | wpb 510.9 | bsz 1 | num_updates 21218 | best_loss 7.017
2022-03-05 10:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21218 updates
2022-03-05 10:15:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:15:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:15:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 109 @ 21218 updates, score 11.686) (writing took 5.167681701481342 seconds)
2022-03-05 10:15:37 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-05 10:15:37 | INFO | train | epoch 109 | loss 2.12 | ppl 4.35 | wps 18368 | ups 0.28 | wpb 65448 | bsz 127.8 | num_updates 21218 | lr 0.000217094 | gnorm 1.128 | loss_scale 16 | train_wall 635 | gb_free 7.2 | wall 89996
2022-03-05 10:15:37 | INFO | fairseq.trainer | begin training epoch 110
2022-03-05 10:15:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:17:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:20:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:20:33 | INFO | train_inner | epoch 110:     84 / 196 loss=2.085, ppl=4.24, wps=17647.4, ups=0.27, wpb=65363.4, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.124, loss_scale=8, train_wall=332, gb_free=7.2, wall=90293
2022-03-05 10:26:24 | INFO | train_inner | epoch 110:    184 / 196 loss=2.139, ppl=4.41, wps=18686.9, ups=0.29, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.14, loss_scale=8, train_wall=324, gb_free=7.2, wall=90643
2022-03-05 10:27:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:27:11 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 11.772 | ppl 3497.91 | wps 36019.7 | wpb 510.9 | bsz 1 | num_updates 21412 | best_loss 7.017
2022-03-05 10:27:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21412 updates
2022-03-05 10:27:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:27:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 110 @ 21412 updates, score 11.772) (writing took 5.088920595124364 seconds)
2022-03-05 10:27:16 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-05 10:27:16 | INFO | train | epoch 110 | loss 2.109 | ppl 4.32 | wps 18161.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 21412 | lr 0.000216108 | gnorm 1.132 | loss_scale 8 | train_wall 636 | gb_free 7.2 | wall 90695
2022-03-05 10:27:16 | INFO | fairseq.trainer | begin training epoch 111
2022-03-05 10:27:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:31:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:32:27 | INFO | train_inner | epoch 111:     89 / 196 loss=2.066, ppl=4.19, wps=17975.3, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.131, loss_scale=8, train_wall=326, gb_free=7.2, wall=91007
2022-03-05 10:38:18 | INFO | train_inner | epoch 111:    189 / 196 loss=2.135, ppl=4.39, wps=18681.1, ups=0.29, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.139, loss_scale=8, train_wall=325, gb_free=7.2, wall=91358
2022-03-05 10:38:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:38:48 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 11.854 | ppl 3702.28 | wps 34242.6 | wpb 510.9 | bsz 1 | num_updates 21607 | best_loss 7.017
2022-03-05 10:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21607 updates
2022-03-05 10:38:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:38:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:38:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 111 @ 21607 updates, score 11.854) (writing took 4.8273369539529085 seconds)
2022-03-05 10:38:53 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-05 10:38:53 | INFO | train | epoch 111 | loss 2.1 | ppl 4.29 | wps 18305.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21607 | lr 0.000215131 | gnorm 1.134 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 91393
2022-03-05 10:38:53 | INFO | fairseq.trainer | begin training epoch 112
2022-03-05 10:38:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:40:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:44:22 | INFO | train_inner | epoch 112:     94 / 196 loss=2.057, ppl=4.16, wps=17939.2, ups=0.27, wpb=65367, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.121, loss_scale=8, train_wall=326, gb_free=7.2, wall=91722
2022-03-05 10:48:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:50:17 | INFO | train_inner | epoch 112:    195 / 196 loss=2.129, ppl=4.37, wps=18464.7, ups=0.28, wpb=65532.4, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.146, loss_scale=8, train_wall=328, gb_free=7.2, wall=92077
2022-03-05 10:50:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:50:27 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 11.796 | ppl 3557.09 | wps 33421.1 | wpb 510.9 | bsz 1 | num_updates 21801 | best_loss 7.017
2022-03-05 10:50:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21801 updates
2022-03-05 10:50:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:50:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:50:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 112 @ 21801 updates, score 11.796) (writing took 4.927057892084122 seconds)
2022-03-05 10:50:32 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-05 10:50:32 | INFO | train | epoch 112 | loss 2.091 | ppl 4.26 | wps 18177.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 21801 | lr 0.000214172 | gnorm 1.134 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 92091
2022-03-05 10:50:32 | INFO | fairseq.trainer | begin training epoch 113
2022-03-05 10:50:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:56:20 | INFO | train_inner | epoch 113:     99 / 196 loss=2.052, ppl=4.15, wps=18044.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.127, loss_scale=16, train_wall=324, gb_free=7.2, wall=92439
2022-03-05 10:59:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:01:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:02:04 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 11.827 | ppl 3633.96 | wps 33684.5 | wpb 510.9 | bsz 1 | num_updates 21996 | best_loss 7.017
2022-03-05 11:02:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 21996 updates
2022-03-05 11:02:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:02:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:02:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 113 @ 21996 updates, score 11.827) (writing took 4.8906448520720005 seconds)
2022-03-05 11:02:09 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-05 11:02:09 | INFO | train | epoch 113 | loss 2.078 | ppl 4.22 | wps 18309 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21996 | lr 0.00021322 | gnorm 1.138 | loss_scale 8 | train_wall 634 | gb_free 7.2 | wall 92788
2022-03-05 11:02:09 | INFO | fairseq.trainer | begin training epoch 114
2022-03-05 11:02:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:02:23 | INFO | train_inner | epoch 114:      4 / 196 loss=2.101, ppl=4.29, wps=18003.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22000, lr=0.000213201, gnorm=1.149, loss_scale=8, train_wall=325, gb_free=7.2, wall=92802
2022-03-05 11:07:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:08:17 | INFO | train_inner | epoch 114:    105 / 196 loss=2.044, ppl=4.12, wps=18480.1, ups=0.28, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=1.141, loss_scale=8, train_wall=328, gb_free=7.2, wall=93157
2022-03-05 11:13:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:13:42 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 11.887 | ppl 3786.71 | wps 33603.2 | wpb 510.9 | bsz 1 | num_updates 22191 | best_loss 7.017
2022-03-05 11:13:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22191 updates
2022-03-05 11:13:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:13:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:13:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 114 @ 22191 updates, score 11.887) (writing took 5.12160274758935 seconds)
2022-03-05 11:13:48 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-05 11:13:48 | INFO | train | epoch 114 | loss 2.071 | ppl 4.2 | wps 18257.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 22191 | lr 0.000212281 | gnorm 1.14 | loss_scale 8 | train_wall 635 | gb_free 7.2 | wall 93487
2022-03-05 11:13:48 | INFO | fairseq.trainer | begin training epoch 115
2022-03-05 11:13:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:14:19 | INFO | train_inner | epoch 115:      9 / 196 loss=2.091, ppl=4.26, wps=18059.3, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.139, loss_scale=8, train_wall=324, gb_free=7.2, wall=93519
2022-03-05 11:16:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:20:14 | INFO | train_inner | epoch 115:    110 / 196 loss=2.034, ppl=4.1, wps=18459, ups=0.28, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.124, loss_scale=8, train_wall=328, gb_free=7.2, wall=93874
2022-03-05 11:25:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 342, in train
    valid_losses, should_stop = validate_and_save(
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 429, in validate_and_save
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 497, in validate
    if cfg.dataset.max_valid_steps is not None and i > cfg.dataset.max_valid_steps:
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 295, in __getattr__
    try:
KeyboardInterrupt
