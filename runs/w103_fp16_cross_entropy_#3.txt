Sender: LSF System <lsfadmin@eu-g3-075>
Subject: Job 204473338: <w103_fp16_cross_entropy_#3> in cluster <euler> Exited

Job <w103_fp16_cross_entropy_#3> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Feb 10 20:26:59 2022
Job was executed on host(s) <eu-g3-075>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Feb 10 20:40:22 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Feb 10 20:40:22 2022
Terminated at Fri Feb 11 15:15:27 2022
Results reported at Fri Feb 11 15:15:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 6658483 --fp16 --max-update 5000000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   66689.41 sec.
    Max Memory :                                 18544 MB
    Average Memory :                             3855.08 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               1456.00 MB
    Max Swap :                                   46 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   66905 sec.
    Turnaround time :                            67708 sec.

The output (if any) follows:

2022-02-10 20:40:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6658483, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6658483, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-10 20:40:31 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-10 20:40:37 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-10 20:40:37 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-10 20:40:37 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-10 20:40:37 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-10 20:40:37 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-10 20:40:37 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-10 20:40:37 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-10 20:40:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-10 20:40:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-10 20:40:46 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-10 20:40:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-10 20:40:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-10 20:40:46 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-10 20:40:46 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint_last.pt
2022-02-10 20:40:46 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint_last.pt
2022-02-10 20:40:46 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-10 20:40:48 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-10 20:40:48 | INFO | fairseq.trainer | begin training epoch 1
2022-02-10 20:40:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-10 20:41:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-10 20:41:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 20:41:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-10 20:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-10 20:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-10 20:49:32 | INFO | train_inner | epoch 001:    105 / 1576 loss=18.408, ppl=347784, wps=13743.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.933, loss_scale=4, train_wall=510, gb_free=10, wall=526
2022-02-10 20:57:24 | INFO | train_inner | epoch 001:    205 / 1576 loss=15.843, ppl=58778.4, wps=13890.1, ups=0.21, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.749, loss_scale=4, train_wall=461, gb_free=10, wall=998
2022-02-10 21:05:15 | INFO | train_inner | epoch 001:    305 / 1576 loss=13.345, ppl=10406.6, wps=13896.4, ups=0.21, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.329, loss_scale=4, train_wall=461, gb_free=10, wall=1470
2022-02-10 21:13:07 | INFO | train_inner | epoch 001:    405 / 1576 loss=11.36, ppl=2628.56, wps=13900.4, ups=0.21, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.788, loss_scale=8, train_wall=461, gb_free=10, wall=1941
2022-02-10 21:20:58 | INFO | train_inner | epoch 001:    505 / 1576 loss=10.549, ppl=1498.6, wps=13917.6, ups=0.21, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.532, loss_scale=8, train_wall=460, gb_free=10, wall=2412
2022-02-10 21:28:48 | INFO | train_inner | epoch 001:    605 / 1576 loss=10.274, ppl=1238.4, wps=13925.8, ups=0.21, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.474, loss_scale=16, train_wall=460, gb_free=10, wall=2883
2022-02-10 21:36:39 | INFO | train_inner | epoch 001:    705 / 1576 loss=10.053, ppl=1062.13, wps=13913.2, ups=0.21, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.458, loss_scale=16, train_wall=461, gb_free=10, wall=3354
2022-02-10 21:44:30 | INFO | train_inner | epoch 001:    805 / 1576 loss=9.833, ppl=912.04, wps=13920.1, ups=0.21, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.472, loss_scale=16, train_wall=460, gb_free=10, wall=3824
2022-02-10 21:52:21 | INFO | train_inner | epoch 001:    905 / 1576 loss=9.615, ppl=784.14, wps=13906.6, ups=0.21, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.498, loss_scale=32, train_wall=461, gb_free=10, wall=4296
2022-02-10 22:00:13 | INFO | train_inner | epoch 001:   1005 / 1576 loss=9.42, ppl=684.93, wps=13902.5, ups=0.21, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.519, loss_scale=32, train_wall=461, gb_free=10, wall=4767
2022-02-10 22:08:04 | INFO | train_inner | epoch 001:   1105 / 1576 loss=9.23, ppl=600.38, wps=13918, ups=0.21, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.55, loss_scale=64, train_wall=460, gb_free=10, wall=5238
2022-02-10 22:09:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 22:16:00 | INFO | train_inner | epoch 001:   1206 / 1576 loss=9.053, ppl=531.25, wps=13771.6, ups=0.21, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.573, loss_scale=32, train_wall=465, gb_free=10, wall=5714
2022-02-10 22:23:51 | INFO | train_inner | epoch 001:   1306 / 1576 loss=8.908, ppl=480.43, wps=13908.6, ups=0.21, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.564, loss_scale=32, train_wall=461, gb_free=10, wall=6185
2022-02-10 22:31:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 22:31:47 | INFO | train_inner | epoch 001:   1407 / 1576 loss=8.77, ppl=436.4, wps=13774.3, ups=0.21, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.574, loss_scale=32, train_wall=465, gb_free=10, wall=6661
2022-02-10 22:39:37 | INFO | train_inner | epoch 001:   1507 / 1576 loss=8.643, ppl=399.83, wps=13918.9, ups=0.21, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.58, loss_scale=32, train_wall=460, gb_free=10, wall=7132
2022-02-10 22:44:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-10 22:45:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.265 | ppl 307.63 | wps 37538 | wpb 1021.8 | bsz 2 | num_updates 1569
2022-02-10 22:45:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1569 updates
2022-02-10 22:45:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint1.pt
2022-02-10 22:45:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint1.pt
2022-02-10 22:45:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint1.pt (epoch 1 @ 1569 updates, score 8.265) (writing took 29.602498669177294 seconds)
2022-02-10 22:45:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-10 22:45:34 | INFO | train | epoch 001 | loss 10.786 | ppl 1765.17 | wps 13815.5 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 1569 | lr 0.000196186 | gnorm 0.828 | loss_scale 32 | train_wall 7282 | gb_free 10 | wall 7488
2022-02-10 22:45:34 | INFO | fairseq.trainer | begin training epoch 2
2022-02-10 22:45:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-10 22:48:00 | INFO | train_inner | epoch 002:     31 / 1576 loss=8.524, ppl=368.14, wps=12935.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=0.583, loss_scale=32, train_wall=456, gb_free=10, wall=7634
2022-02-10 22:52:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 22:55:55 | INFO | train_inner | epoch 002:    132 / 1576 loss=8.408, ppl=339.74, wps=13779.6, ups=0.21, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.564, loss_scale=32, train_wall=465, gb_free=10, wall=8109
2022-02-10 23:03:46 | INFO | train_inner | epoch 002:    232 / 1576 loss=8.305, ppl=316.32, wps=13919.4, ups=0.21, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.58, loss_scale=32, train_wall=460, gb_free=10, wall=8580
2022-02-10 23:11:37 | INFO | train_inner | epoch 002:    332 / 1576 loss=8.208, ppl=295.67, wps=13923.8, ups=0.21, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.575, loss_scale=32, train_wall=460, gb_free=10, wall=9051
2022-02-10 23:17:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 23:19:32 | INFO | train_inner | epoch 002:    433 / 1576 loss=8.12, ppl=278.23, wps=13784.5, ups=0.21, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.562, loss_scale=32, train_wall=465, gb_free=10, wall=9526
2022-02-10 23:27:23 | INFO | train_inner | epoch 002:    533 / 1576 loss=8.011, ppl=257.97, wps=13926.7, ups=0.21, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.546, loss_scale=32, train_wall=460, gb_free=10, wall=9997
2022-02-10 23:35:13 | INFO | train_inner | epoch 002:    633 / 1576 loss=7.949, ppl=247.12, wps=13927.8, ups=0.21, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.538, loss_scale=32, train_wall=460, gb_free=10, wall=10467
2022-02-10 23:38:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 23:43:09 | INFO | train_inner | epoch 002:    734 / 1576 loss=7.851, ppl=230.85, wps=13774.4, ups=0.21, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.541, loss_scale=32, train_wall=465, gb_free=10, wall=10943
2022-02-10 23:51:00 | INFO | train_inner | epoch 002:    834 / 1576 loss=7.785, ppl=220.51, wps=13914.4, ups=0.21, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.541, loss_scale=32, train_wall=461, gb_free=10, wall=11414
2022-02-10 23:58:51 | INFO | train_inner | epoch 002:    934 / 1576 loss=7.708, ppl=209.02, wps=13928.3, ups=0.21, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.531, loss_scale=64, train_wall=460, gb_free=10, wall=11885
2022-02-11 00:01:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 00:06:46 | INFO | train_inner | epoch 002:   1035 / 1576 loss=7.637, ppl=199.04, wps=13783.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.525, loss_scale=32, train_wall=465, gb_free=10, wall=12360
2022-02-11 00:14:37 | INFO | train_inner | epoch 002:   1135 / 1576 loss=7.573, ppl=190.46, wps=13916.9, ups=0.21, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.527, loss_scale=32, train_wall=460, gb_free=10, wall=12831
2022-02-11 00:21:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 00:22:32 | INFO | train_inner | epoch 002:   1236 / 1576 loss=7.512, ppl=182.59, wps=13790, ups=0.21, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.523, loss_scale=32, train_wall=465, gb_free=10, wall=13306
2022-02-11 00:30:23 | INFO | train_inner | epoch 002:   1336 / 1576 loss=7.441, ppl=173.83, wps=13923.1, ups=0.21, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.515, loss_scale=32, train_wall=460, gb_free=10, wall=13777
2022-02-11 00:38:14 | INFO | train_inner | epoch 002:   1436 / 1576 loss=7.391, ppl=167.85, wps=13911.8, ups=0.21, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.514, loss_scale=32, train_wall=461, gb_free=10, wall=14248
2022-02-11 00:42:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 00:46:10 | INFO | train_inner | epoch 002:   1537 / 1576 loss=7.338, ppl=161.76, wps=13777, ups=0.21, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.514, loss_scale=32, train_wall=465, gb_free=10, wall=14724
2022-02-11 00:49:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 00:49:15 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.074 | ppl 134.77 | wps 37423.1 | wpb 1021.8 | bsz 2 | num_updates 3139 | best_loss 7.074
2022-02-11 00:49:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3139 updates
2022-02-11 00:49:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint2.pt
2022-02-11 00:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint2.pt
2022-02-11 00:49:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint2.pt (epoch 2 @ 3139 updates, score 7.074) (writing took 30.63845192361623 seconds)
2022-02-11 00:49:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-11 00:49:46 | INFO | train | epoch 002 | loss 7.816 | ppl 225.31 | wps 13799 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 3139 | lr 0.000392397 | gnorm 0.54 | loss_scale 32 | train_wall 7250 | gb_free 10 | wall 14940
2022-02-11 00:49:46 | INFO | fairseq.trainer | begin training epoch 3
2022-02-11 00:49:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 00:54:33 | INFO | train_inner | epoch 003:     61 / 1576 loss=7.255, ppl=152.75, wps=12902.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.523, loss_scale=32, train_wall=456, gb_free=10, wall=15227
2022-02-11 01:02:24 | INFO | train_inner | epoch 003:    161 / 1576 loss=7.193, ppl=146.37, wps=13912.1, ups=0.21, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.499, loss_scale=32, train_wall=460, gb_free=10, wall=15698
2022-02-11 01:03:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 01:06:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 01:10:27 | INFO | train_inner | epoch 003:    263 / 1576 loss=7.159, ppl=142.87, wps=13589.6, ups=0.21, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.508, loss_scale=16, train_wall=471, gb_free=10, wall=16181
2022-02-11 01:18:20 | INFO | train_inner | epoch 003:    363 / 1576 loss=7.12, ppl=139.06, wps=13851.3, ups=0.21, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.519, loss_scale=16, train_wall=463, gb_free=10, wall=16654
2022-02-11 01:26:10 | INFO | train_inner | epoch 003:    463 / 1576 loss=7.068, ppl=134.17, wps=13920, ups=0.21, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.498, loss_scale=16, train_wall=460, gb_free=10, wall=17125
2022-02-11 01:31:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 01:34:06 | INFO | train_inner | epoch 003:    564 / 1576 loss=7.032, ppl=130.88, wps=13782.7, ups=0.21, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.501, loss_scale=16, train_wall=465, gb_free=10, wall=17600
2022-02-11 01:41:57 | INFO | train_inner | epoch 003:    664 / 1576 loss=7.018, ppl=129.57, wps=13918.4, ups=0.21, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.503, loss_scale=16, train_wall=460, gb_free=10, wall=18071
2022-02-11 01:49:48 | INFO | train_inner | epoch 003:    764 / 1576 loss=6.952, ppl=123.82, wps=13921.7, ups=0.21, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.518, loss_scale=16, train_wall=460, gb_free=10, wall=18542
2022-02-11 01:56:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 01:57:43 | INFO | train_inner | epoch 003:    865 / 1576 loss=6.925, ppl=121.56, wps=13780.2, ups=0.21, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.497, loss_scale=16, train_wall=465, gb_free=10, wall=19017
2022-02-11 02:05:34 | INFO | train_inner | epoch 003:    965 / 1576 loss=6.9, ppl=119.39, wps=13925, ups=0.21, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.497, loss_scale=16, train_wall=460, gb_free=10, wall=19488
2022-02-11 02:13:24 | INFO | train_inner | epoch 003:   1065 / 1576 loss=6.875, ppl=117.39, wps=13922.8, ups=0.21, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.489, loss_scale=16, train_wall=460, gb_free=10, wall=19959
2022-02-11 02:21:15 | INFO | train_inner | epoch 003:   1165 / 1576 loss=6.839, ppl=114.5, wps=13926, ups=0.21, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.485, loss_scale=32, train_wall=460, gb_free=10, wall=20429
2022-02-11 02:22:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 02:29:10 | INFO | train_inner | epoch 003:   1266 / 1576 loss=6.795, ppl=111.08, wps=13790.2, ups=0.21, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.48, loss_scale=16, train_wall=465, gb_free=10, wall=20904
2022-02-11 02:37:01 | INFO | train_inner | epoch 003:   1366 / 1576 loss=6.78, ppl=109.87, wps=13924, ups=0.21, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.489, loss_scale=16, train_wall=460, gb_free=10, wall=21375
2022-02-11 02:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 02:44:57 | INFO | train_inner | epoch 003:   1467 / 1576 loss=6.75, ppl=107.62, wps=13778.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.483, loss_scale=16, train_wall=465, gb_free=10, wall=21851
2022-02-11 02:52:47 | INFO | train_inner | epoch 003:   1567 / 1576 loss=6.723, ppl=105.67, wps=13921.1, ups=0.21, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.482, loss_scale=16, train_wall=460, gb_free=10, wall=22322
2022-02-11 02:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 02:53:32 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.474 | ppl 88.88 | wps 37531.6 | wpb 1021.8 | bsz 2 | num_updates 4709 | best_loss 6.474
2022-02-11 02:53:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4709 updates
2022-02-11 02:53:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint3.pt
2022-02-11 02:53:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint3.pt
2022-02-11 02:54:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint3.pt (epoch 3 @ 4709 updates, score 6.474) (writing took 29.540647702291608 seconds)
2022-02-11 02:54:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-11 02:54:01 | INFO | train | epoch 003 | loss 6.952 | ppl 123.82 | wps 13793.9 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 4709 | lr 0.000460825 | gnorm 0.498 | loss_scale 16 | train_wall 7253 | gb_free 10 | wall 22395
2022-02-11 02:54:01 | INFO | fairseq.trainer | begin training epoch 4
2022-02-11 02:54:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 03:01:09 | INFO | train_inner | epoch 004:     91 / 1576 loss=6.649, ppl=100.34, wps=12947.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.485, loss_scale=16, train_wall=456, gb_free=10, wall=22823
2022-02-11 03:06:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 03:09:04 | INFO | train_inner | epoch 004:    192 / 1576 loss=6.62, ppl=98.33, wps=13793.1, ups=0.21, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.476, loss_scale=16, train_wall=464, gb_free=10, wall=23298
2022-02-11 03:16:55 | INFO | train_inner | epoch 004:    292 / 1576 loss=6.606, ppl=97.43, wps=13922.1, ups=0.21, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.472, loss_scale=16, train_wall=460, gb_free=10, wall=23769
2022-02-11 03:24:45 | INFO | train_inner | epoch 004:    392 / 1576 loss=6.594, ppl=96.57, wps=13940.2, ups=0.21, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.472, loss_scale=16, train_wall=460, gb_free=10, wall=24239
2022-02-11 03:28:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 03:32:40 | INFO | train_inner | epoch 004:    493 / 1576 loss=6.582, ppl=95.79, wps=13788.8, ups=0.21, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.477, loss_scale=16, train_wall=465, gb_free=10, wall=24715
2022-02-11 03:40:31 | INFO | train_inner | epoch 004:    593 / 1576 loss=6.565, ppl=94.65, wps=13929.4, ups=0.21, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.483, loss_scale=16, train_wall=460, gb_free=10, wall=25185
2022-02-11 03:48:22 | INFO | train_inner | epoch 004:    693 / 1576 loss=6.56, ppl=94.38, wps=13920.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.474, loss_scale=32, train_wall=460, gb_free=10, wall=25656
2022-02-11 03:54:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 03:56:17 | INFO | train_inner | epoch 004:    794 / 1576 loss=6.541, ppl=93.13, wps=13784.8, ups=0.21, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.476, loss_scale=16, train_wall=465, gb_free=10, wall=26131
2022-02-11 04:04:08 | INFO | train_inner | epoch 004:    894 / 1576 loss=6.521, ppl=91.82, wps=13923.1, ups=0.21, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.481, loss_scale=16, train_wall=460, gb_free=10, wall=26602
2022-02-11 04:11:58 | INFO | train_inner | epoch 004:    994 / 1576 loss=6.499, ppl=90.46, wps=13922.7, ups=0.21, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.464, loss_scale=16, train_wall=460, gb_free=10, wall=27073
2022-02-11 04:18:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 04:19:54 | INFO | train_inner | epoch 004:   1095 / 1576 loss=6.498, ppl=90.4, wps=13785, ups=0.21, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.484, loss_scale=16, train_wall=465, gb_free=10, wall=27548
2022-02-11 04:27:45 | INFO | train_inner | epoch 004:   1195 / 1576 loss=6.488, ppl=89.77, wps=13925.9, ups=0.21, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.478, loss_scale=16, train_wall=460, gb_free=10, wall=28019
2022-02-11 04:35:35 | INFO | train_inner | epoch 004:   1295 / 1576 loss=6.482, ppl=89.39, wps=13938.6, ups=0.21, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.472, loss_scale=16, train_wall=460, gb_free=10, wall=28489
2022-02-11 04:39:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 04:43:30 | INFO | train_inner | epoch 004:   1396 / 1576 loss=6.462, ppl=88.16, wps=13786.5, ups=0.21, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.485, loss_scale=16, train_wall=465, gb_free=10, wall=28964
2022-02-11 04:51:21 | INFO | train_inner | epoch 004:   1496 / 1576 loss=6.452, ppl=87.56, wps=13927, ups=0.21, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.483, loss_scale=16, train_wall=460, gb_free=10, wall=29435
2022-02-11 04:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 04:57:40 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.198 | ppl 73.44 | wps 37274 | wpb 1021.8 | bsz 2 | num_updates 6280 | best_loss 6.198
2022-02-11 04:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6280 updates
2022-02-11 04:57:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint4.pt
2022-02-11 04:57:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint4.pt
2022-02-11 04:58:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint4.pt (epoch 4 @ 6280 updates, score 6.198) (writing took 28.976890760473907 seconds)
2022-02-11 04:58:09 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-11 04:58:09 | INFO | train | epoch 004 | loss 6.535 | ppl 92.74 | wps 13816.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 6280 | lr 0.000399043 | gnorm 0.478 | loss_scale 16 | train_wall 7247 | gb_free 10 | wall 29843
2022-02-11 04:58:09 | INFO | fairseq.trainer | begin training epoch 5
2022-02-11 04:58:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 04:59:43 | INFO | train_inner | epoch 005:     20 / 1576 loss=6.429, ppl=86.17, wps=12938.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.496, loss_scale=16, train_wall=457, gb_free=10, wall=29937
2022-02-11 05:00:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 05:07:38 | INFO | train_inner | epoch 005:    121 / 1576 loss=6.369, ppl=82.65, wps=13784.5, ups=0.21, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.477, loss_scale=16, train_wall=465, gb_free=10, wall=30412
2022-02-11 05:15:29 | INFO | train_inner | epoch 005:    221 / 1576 loss=6.359, ppl=82.08, wps=13919.5, ups=0.21, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.478, loss_scale=16, train_wall=460, gb_free=10, wall=30883
2022-02-11 05:20:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 05:23:27 | INFO | train_inner | epoch 005:    322 / 1576 loss=6.357, ppl=81.99, wps=13715.3, ups=0.21, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.483, loss_scale=16, train_wall=467, gb_free=10, wall=31361
2022-02-11 05:31:20 | INFO | train_inner | epoch 005:    422 / 1576 loss=6.349, ppl=81.49, wps=13847.1, ups=0.21, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.487, loss_scale=16, train_wall=462, gb_free=10, wall=31834
2022-02-11 05:39:13 | INFO | train_inner | epoch 005:    522 / 1576 loss=6.347, ppl=81.38, wps=13847.5, ups=0.21, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.497, loss_scale=16, train_wall=462, gb_free=10, wall=32307
2022-02-11 05:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 05:47:11 | INFO | train_inner | epoch 005:    623 / 1576 loss=6.343, ppl=81.19, wps=13709.5, ups=0.21, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.489, loss_scale=16, train_wall=467, gb_free=10, wall=32786
2022-02-11 05:51:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 05:55:09 | INFO | train_inner | epoch 005:    724 / 1576 loss=6.339, ppl=80.93, wps=13712.9, ups=0.21, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.49, loss_scale=8, train_wall=467, gb_free=10, wall=33263
2022-02-11 06:03:02 | INFO | train_inner | epoch 005:    824 / 1576 loss=6.329, ppl=80.37, wps=13852, ups=0.21, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.5, loss_scale=8, train_wall=462, gb_free=10, wall=33737
2022-02-11 06:10:56 | INFO | train_inner | epoch 005:    924 / 1576 loss=6.325, ppl=80.2, wps=13850.2, ups=0.21, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.498, loss_scale=8, train_wall=462, gb_free=10, wall=34210
2022-02-11 06:18:49 | INFO | train_inner | epoch 005:   1024 / 1576 loss=6.334, ppl=80.7, wps=13844.3, ups=0.21, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.492, loss_scale=16, train_wall=463, gb_free=10, wall=34683
2022-02-11 06:26:42 | INFO | train_inner | epoch 005:   1124 / 1576 loss=6.303, ppl=78.97, wps=13844.4, ups=0.21, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.507, loss_scale=16, train_wall=463, gb_free=10, wall=35156
2022-02-11 06:31:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 06:34:40 | INFO | train_inner | epoch 005:   1225 / 1576 loss=6.311, ppl=79.39, wps=13714.7, ups=0.21, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.496, loss_scale=16, train_wall=467, gb_free=10, wall=35634
2022-02-11 06:42:33 | INFO | train_inner | epoch 005:   1325 / 1576 loss=6.298, ppl=78.67, wps=13854.1, ups=0.21, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.504, loss_scale=16, train_wall=462, gb_free=10, wall=36107
2022-02-11 06:50:26 | INFO | train_inner | epoch 005:   1425 / 1576 loss=6.3, ppl=78.78, wps=13864.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.515, loss_scale=16, train_wall=462, gb_free=10, wall=36580
2022-02-11 06:52:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 06:58:24 | INFO | train_inner | epoch 005:   1526 / 1576 loss=6.291, ppl=78.28, wps=13713.4, ups=0.21, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.508, loss_scale=16, train_wall=467, gb_free=10, wall=37058
2022-02-11 07:02:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 07:02:22 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.066 | ppl 67 | wps 36770 | wpb 1021.8 | bsz 2 | num_updates 7850 | best_loss 6.066
2022-02-11 07:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7850 updates
2022-02-11 07:02:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint5.pt
2022-02-11 07:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint5.pt
2022-02-11 07:02:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint5.pt (epoch 5 @ 7850 updates, score 6.066) (writing took 28.94279547035694 seconds)
2022-02-11 07:02:51 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-11 07:02:51 | INFO | train | epoch 005 | loss 6.33 | ppl 80.45 | wps 13743.1 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 7850 | lr 0.000356915 | gnorm 0.496 | loss_scale 16 | train_wall 7278 | gb_free 10 | wall 37325
2022-02-11 07:02:51 | INFO | fairseq.trainer | begin training epoch 6
2022-02-11 07:02:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 07:06:48 | INFO | train_inner | epoch 006:     50 / 1576 loss=6.256, ppl=76.42, wps=12894.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.517, loss_scale=16, train_wall=458, gb_free=10, wall=37562
2022-02-11 07:13:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 07:14:45 | INFO | train_inner | epoch 006:    151 / 1576 loss=6.217, ppl=74.4, wps=13731.7, ups=0.21, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.517, loss_scale=16, train_wall=466, gb_free=10, wall=38039
2022-02-11 07:22:38 | INFO | train_inner | epoch 006:    251 / 1576 loss=6.22, ppl=74.55, wps=13862.1, ups=0.21, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.513, loss_scale=16, train_wall=462, gb_free=10, wall=38512
2022-02-11 07:26:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 07:30:36 | INFO | train_inner | epoch 006:    352 / 1576 loss=6.214, ppl=74.23, wps=13708.2, ups=0.21, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.518, loss_scale=8, train_wall=467, gb_free=10, wall=38990
2022-02-11 07:38:29 | INFO | train_inner | epoch 006:    452 / 1576 loss=6.214, ppl=74.25, wps=13858.1, ups=0.21, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.512, loss_scale=8, train_wall=462, gb_free=10, wall=39463
2022-02-11 07:46:21 | INFO | train_inner | epoch 006:    552 / 1576 loss=6.213, ppl=74.2, wps=13864.4, ups=0.21, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.53, loss_scale=16, train_wall=462, gb_free=10, wall=39935
2022-02-11 07:54:14 | INFO | train_inner | epoch 006:    652 / 1576 loss=6.213, ppl=74.2, wps=13857.6, ups=0.21, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.532, loss_scale=16, train_wall=462, gb_free=10, wall=40408
2022-02-11 08:01:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 08:02:12 | INFO | train_inner | epoch 006:    753 / 1576 loss=6.206, ppl=73.84, wps=13705.3, ups=0.21, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.534, loss_scale=8, train_wall=467, gb_free=10, wall=40887
2022-02-11 08:10:05 | INFO | train_inner | epoch 006:    853 / 1576 loss=6.22, ppl=74.56, wps=13855.1, ups=0.21, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.536, loss_scale=8, train_wall=462, gb_free=10, wall=41360
2022-02-11 08:17:58 | INFO | train_inner | epoch 006:    953 / 1576 loss=6.219, ppl=74.49, wps=13859.7, ups=0.21, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.531, loss_scale=8, train_wall=462, gb_free=10, wall=41832
2022-02-11 08:24:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 08:25:56 | INFO | train_inner | epoch 006:   1054 / 1576 loss=6.206, ppl=73.8, wps=13729.8, ups=0.21, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.535, loss_scale=8, train_wall=466, gb_free=10, wall=42310
2022-02-11 08:33:49 | INFO | train_inner | epoch 006:   1154 / 1576 loss=6.197, ppl=73.34, wps=13854, ups=0.21, wpb=65532.3, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.533, loss_scale=8, train_wall=462, gb_free=10, wall=42783
2022-02-11 08:41:41 | INFO | train_inner | epoch 006:   1254 / 1576 loss=6.194, ppl=73.21, wps=13863.9, ups=0.21, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.547, loss_scale=8, train_wall=462, gb_free=10, wall=43255
2022-02-11 08:45:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 08:49:39 | INFO | train_inner | epoch 006:   1355 / 1576 loss=6.2, ppl=73.52, wps=13725.1, ups=0.21, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.541, loss_scale=8, train_wall=467, gb_free=10, wall=43733
2022-02-11 08:57:32 | INFO | train_inner | epoch 006:   1455 / 1576 loss=6.198, ppl=73.43, wps=13858.1, ups=0.21, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.551, loss_scale=8, train_wall=462, gb_free=10, wall=44206
2022-02-11 09:05:24 | INFO | train_inner | epoch 006:   1555 / 1576 loss=6.188, ppl=72.91, wps=13864.9, ups=0.21, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.547, loss_scale=8, train_wall=462, gb_free=10, wall=44679
2022-02-11 09:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 09:07:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.962 | ppl 62.32 | wps 36645.2 | wpb 1021.8 | bsz 2 | num_updates 9421 | best_loss 5.962
2022-02-11 09:07:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9421 updates
2022-02-11 09:07:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint6.pt
2022-02-11 09:07:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint6.pt
2022-02-11 09:07:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint6.pt (epoch 6 @ 9421 updates, score 5.962) (writing took 31.677148425020278 seconds)
2022-02-11 09:07:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-11 09:07:37 | INFO | train | epoch 006 | loss 6.208 | ppl 73.92 | wps 13745.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 9421 | lr 0.0003258 | gnorm 0.532 | loss_scale 16 | train_wall 7277 | gb_free 10 | wall 44811
2022-02-11 09:07:37 | INFO | fairseq.trainer | begin training epoch 7
2022-02-11 09:07:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 09:09:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 09:13:55 | INFO | train_inner | epoch 007:     80 / 1576 loss=6.125, ppl=69.77, wps=12721.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.56, loss_scale=8, train_wall=462, gb_free=10, wall=45189
2022-02-11 09:21:48 | INFO | train_inner | epoch 007:    180 / 1576 loss=6.119, ppl=69.5, wps=13861.3, ups=0.21, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.56, loss_scale=8, train_wall=462, gb_free=10, wall=45662
2022-02-11 09:29:41 | INFO | train_inner | epoch 007:    280 / 1576 loss=6.124, ppl=69.76, wps=13859.6, ups=0.21, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.574, loss_scale=8, train_wall=462, gb_free=10, wall=46135
2022-02-11 09:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 09:37:38 | INFO | train_inner | epoch 007:    381 / 1576 loss=6.126, ppl=69.82, wps=13720.3, ups=0.21, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.567, loss_scale=8, train_wall=467, gb_free=10, wall=46613
2022-02-11 09:45:31 | INFO | train_inner | epoch 007:    481 / 1576 loss=6.127, ppl=69.89, wps=13859.6, ups=0.21, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.572, loss_scale=8, train_wall=462, gb_free=10, wall=47085
2022-02-11 09:50:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 09:53:29 | INFO | train_inner | epoch 007:    582 / 1576 loss=6.127, ppl=69.91, wps=13722.5, ups=0.21, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.574, loss_scale=8, train_wall=467, gb_free=10, wall=47563
2022-02-11 10:01:21 | INFO | train_inner | epoch 007:    682 / 1576 loss=6.126, ppl=69.86, wps=13866.6, ups=0.21, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.572, loss_scale=8, train_wall=462, gb_free=10, wall=48036
2022-02-11 10:09:14 | INFO | train_inner | epoch 007:    782 / 1576 loss=6.126, ppl=69.86, wps=13855.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.579, loss_scale=8, train_wall=462, gb_free=10, wall=48509
2022-02-11 10:10:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 10:17:12 | INFO | train_inner | epoch 007:    883 / 1576 loss=6.13, ppl=70.06, wps=13719.7, ups=0.21, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.596, loss_scale=8, train_wall=467, gb_free=10, wall=48986
2022-02-11 10:25:05 | INFO | train_inner | epoch 007:    983 / 1576 loss=6.121, ppl=69.58, wps=13852.1, ups=0.21, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.599, loss_scale=8, train_wall=462, gb_free=10, wall=49459
2022-02-11 10:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 10:33:03 | INFO | train_inner | epoch 007:   1084 / 1576 loss=6.126, ppl=69.82, wps=13719.8, ups=0.21, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.6, loss_scale=8, train_wall=467, gb_free=10, wall=49937
2022-02-11 10:40:56 | INFO | train_inner | epoch 007:   1184 / 1576 loss=6.119, ppl=69.48, wps=13851.7, ups=0.21, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.594, loss_scale=8, train_wall=462, gb_free=10, wall=50410
2022-02-11 10:48:49 | INFO | train_inner | epoch 007:   1284 / 1576 loss=6.122, ppl=69.67, wps=13855.7, ups=0.21, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.593, loss_scale=8, train_wall=462, gb_free=10, wall=50883
2022-02-11 10:53:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 10:56:47 | INFO | train_inner | epoch 007:   1385 / 1576 loss=6.126, ppl=69.83, wps=13723.1, ups=0.21, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.602, loss_scale=8, train_wall=467, gb_free=10, wall=51361
2022-02-11 11:04:39 | INFO | train_inner | epoch 007:   1485 / 1576 loss=6.11, ppl=69.08, wps=13858.2, ups=0.21, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.608, loss_scale=8, train_wall=462, gb_free=10, wall=51834
2022-02-11 11:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 11:11:52 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.875 | ppl 58.7 | wps 36568.8 | wpb 1021.8 | bsz 2 | num_updates 10991 | best_loss 5.875
2022-02-11 11:11:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10991 updates
2022-02-11 11:11:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint7.pt
2022-02-11 11:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint7.pt
2022-02-11 11:12:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint7.pt (epoch 7 @ 10991 updates, score 5.875) (writing took 32.59312223084271 seconds)
2022-02-11 11:12:24 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-11 11:12:24 | INFO | train | epoch 007 | loss 6.123 | ppl 69.69 | wps 13734.9 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 10991 | lr 0.000301635 | gnorm 0.585 | loss_scale 8 | train_wall 7277 | gb_free 10 | wall 52298
2022-02-11 11:12:24 | INFO | fairseq.trainer | begin training epoch 8
2022-02-11 11:12:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 11:13:07 | INFO | train_inner | epoch 008:      9 / 1576 loss=6.116, ppl=69.34, wps=12802.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.623, loss_scale=8, train_wall=458, gb_free=10, wall=52341
2022-02-11 11:13:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 11:21:04 | INFO | train_inner | epoch 008:    110 / 1576 loss=6.042, ppl=65.88, wps=13727.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.617, loss_scale=8, train_wall=466, gb_free=10, wall=52818
2022-02-11 11:28:57 | INFO | train_inner | epoch 008:    210 / 1576 loss=6.045, ppl=66.02, wps=13857.9, ups=0.21, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.638, loss_scale=8, train_wall=462, gb_free=10, wall=53291
2022-02-11 11:34:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 11:36:55 | INFO | train_inner | epoch 008:    311 / 1576 loss=6.052, ppl=66.33, wps=13714.7, ups=0.21, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.627, loss_scale=8, train_wall=467, gb_free=10, wall=53769
2022-02-11 11:44:49 | INFO | train_inner | epoch 008:    411 / 1576 loss=6.07, ppl=67.18, wps=13841.1, ups=0.21, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.631, loss_scale=8, train_wall=462, gb_free=10, wall=54243
2022-02-11 11:52:41 | INFO | train_inner | epoch 008:    511 / 1576 loss=6.061, ppl=66.75, wps=13873.4, ups=0.21, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.644, loss_scale=8, train_wall=462, gb_free=10, wall=54715
2022-02-11 11:54:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 12:00:38 | INFO | train_inner | epoch 008:    612 / 1576 loss=6.068, ppl=67.09, wps=13726.2, ups=0.21, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.655, loss_scale=8, train_wall=467, gb_free=10, wall=55193
2022-02-11 12:07:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 12:08:35 | INFO | train_inner | epoch 008:    713 / 1576 loss=6.055, ppl=66.48, wps=13740.6, ups=0.21, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.644, loss_scale=4, train_wall=466, gb_free=10, wall=55669
2022-02-11 12:16:27 | INFO | train_inner | epoch 008:    813 / 1576 loss=6.067, ppl=67.04, wps=13886.4, ups=0.21, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.64, loss_scale=4, train_wall=461, gb_free=10, wall=56141
2022-02-11 12:24:19 | INFO | train_inner | epoch 008:    913 / 1576 loss=6.063, ppl=66.84, wps=13879.9, ups=0.21, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.661, loss_scale=4, train_wall=461, gb_free=10, wall=56614
2022-02-11 12:32:12 | INFO | train_inner | epoch 008:   1013 / 1576 loss=6.061, ppl=66.76, wps=13872.3, ups=0.21, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.661, loss_scale=8, train_wall=462, gb_free=10, wall=57086
2022-02-11 12:33:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 12:40:09 | INFO | train_inner | epoch 008:   1114 / 1576 loss=6.069, ppl=67.14, wps=13745.6, ups=0.21, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.656, loss_scale=4, train_wall=466, gb_free=10, wall=57563
2022-02-11 12:48:01 | INFO | train_inner | epoch 008:   1214 / 1576 loss=6.069, ppl=67.12, wps=13869.7, ups=0.21, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.664, loss_scale=4, train_wall=462, gb_free=10, wall=58035
2022-02-11 12:55:54 | INFO | train_inner | epoch 008:   1314 / 1576 loss=6.054, ppl=66.42, wps=13867.8, ups=0.21, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.677, loss_scale=8, train_wall=462, gb_free=10, wall=58508
2022-02-11 12:58:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 13:03:51 | INFO | train_inner | epoch 008:   1415 / 1576 loss=6.07, ppl=67.17, wps=13734.6, ups=0.21, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.675, loss_scale=4, train_wall=466, gb_free=10, wall=58985
2022-02-11 13:11:43 | INFO | train_inner | epoch 008:   1515 / 1576 loss=6.063, ppl=66.84, wps=13886.5, ups=0.21, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.693, loss_scale=4, train_wall=461, gb_free=10, wall=59457
2022-02-11 13:16:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 13:16:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.835 | ppl 57.08 | wps 36562.4 | wpb 1021.8 | bsz 2 | num_updates 12561 | best_loss 5.835
2022-02-11 13:16:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12561 updates
2022-02-11 13:16:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint8.pt
2022-02-11 13:16:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint8.pt
2022-02-11 13:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint8.pt (epoch 8 @ 12561 updates, score 5.835) (writing took 29.847451996989548 seconds)
2022-02-11 13:17:02 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-11 13:17:02 | INFO | train | epoch 008 | loss 6.061 | ppl 66.76 | wps 13752.1 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 12561 | lr 0.000282155 | gnorm 0.653 | loss_scale 4 | train_wall 7271 | gb_free 10 | wall 59776
2022-02-11 13:17:02 | INFO | fairseq.trainer | begin training epoch 9
2022-02-11 13:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 13:19:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 13:20:10 | INFO | train_inner | epoch 009:     40 / 1576 loss=6.044, ppl=65.97, wps=12804.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.691, loss_scale=4, train_wall=461, gb_free=10, wall=59964
2022-02-11 13:28:03 | INFO | train_inner | epoch 009:    140 / 1576 loss=5.985, ppl=63.33, wps=13864.5, ups=0.21, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.706, loss_scale=4, train_wall=462, gb_free=10, wall=60437
2022-02-11 13:35:55 | INFO | train_inner | epoch 009:    240 / 1576 loss=5.999, ppl=63.94, wps=13878.2, ups=0.21, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.687, loss_scale=4, train_wall=461, gb_free=10, wall=60909
2022-02-11 13:43:47 | INFO | train_inner | epoch 009:    340 / 1576 loss=6.019, ppl=64.87, wps=13899.4, ups=0.21, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.715, loss_scale=8, train_wall=461, gb_free=10, wall=61381
2022-02-11 13:51:39 | INFO | train_inner | epoch 009:    440 / 1576 loss=6.016, ppl=64.73, wps=13861.9, ups=0.21, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.71, loss_scale=8, train_wall=462, gb_free=10, wall=61853
2022-02-11 13:55:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 13:59:36 | INFO | train_inner | epoch 009:    541 / 1576 loss=6.01, ppl=64.44, wps=13737.4, ups=0.21, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.7, loss_scale=4, train_wall=466, gb_free=10, wall=62331
2022-02-11 14:07:29 | INFO | train_inner | epoch 009:    641 / 1576 loss=6.021, ppl=64.92, wps=13871.8, ups=0.21, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.731, loss_scale=4, train_wall=462, gb_free=10, wall=62803
2022-02-11 14:15:21 | INFO | train_inner | epoch 009:    741 / 1576 loss=6.015, ppl=64.68, wps=13868.3, ups=0.21, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.721, loss_scale=4, train_wall=462, gb_free=10, wall=63276
2022-02-11 14:19:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 14:23:19 | INFO | train_inner | epoch 009:    842 / 1576 loss=6.01, ppl=64.45, wps=13725.5, ups=0.21, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.714, loss_scale=4, train_wall=467, gb_free=10, wall=63753
2022-02-11 14:31:12 | INFO | train_inner | epoch 009:    942 / 1576 loss=6.013, ppl=64.57, wps=13863.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.747, loss_scale=4, train_wall=462, gb_free=10, wall=64226
2022-02-11 14:39:04 | INFO | train_inner | epoch 009:   1042 / 1576 loss=6.024, ppl=65.07, wps=13881.4, ups=0.21, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.74, loss_scale=4, train_wall=461, gb_free=10, wall=64698
2022-02-11 14:39:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 14:47:01 | INFO | train_inner | epoch 009:   1143 / 1576 loss=6.016, ppl=64.72, wps=13739.3, ups=0.21, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.771, loss_scale=4, train_wall=466, gb_free=10, wall=65175
2022-02-11 14:54:53 | INFO | train_inner | epoch 009:   1243 / 1576 loss=6.02, ppl=64.88, wps=13875.4, ups=0.21, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.751, loss_scale=4, train_wall=461, gb_free=10, wall=65647
2022-02-11 15:00:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 15:02:47 | INFO | train_inner | epoch 009:   1344 / 1576 loss=6.018, ppl=64.8, wps=13822.6, ups=0.21, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.767, loss_scale=4, train_wall=464, gb_free=10, wall=66121
2022-02-11 15:10:37 | INFO | train_inner | epoch 009:   1444 / 1576 loss=6.02, ppl=64.88, wps=13956, ups=0.21, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.774, loss_scale=4, train_wall=459, gb_free=10, wall=66591
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 204616372: <w103_fp16_cross_entropy_#3> in cluster <euler> Done

Job <w103_fp16_cross_entropy_#3> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 16:42:31 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 17:39:14 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 17:39:14 2022
Terminated at Mon Feb 14 11:57:57 2022
Results reported at Mon Feb 14 11:57:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 6658483 --fp16 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   238626.64 sec.
    Max Memory :                                 17930 MB
    Average Memory :                             3146.36 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               2070.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   238723 sec.
    Turnaround time :                            242126 sec.

The output (if any) follows:

2022-02-11 17:39:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6658483, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6658483, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 17:39:21 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 17:39:26 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 17:39:26 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 17:39:26 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 17:39:26 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-11 17:39:26 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 17:39:26 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 17:39:26 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 17:39:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 17:39:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:30 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 17:39:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 17:39:30 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 17:39:30 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint_last.pt
2022-02-11 17:39:30 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint_last.pt
2022-02-11 17:39:30 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 17:39:31 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 17:39:31 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 17:39:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 17:39:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 17:39:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 17:40:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 17:40:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 17:40:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 17:48:34 | INFO | train_inner | epoch 001:    105 / 1576 loss=18.087, ppl=278380, wps=13804.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.637, loss_scale=4, train_wall=529, gb_free=10, wall=544
2022-02-11 17:56:30 | INFO | train_inner | epoch 001:    205 / 1576 loss=15.423, ppl=43918.6, wps=13781.3, ups=0.21, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.764, loss_scale=4, train_wall=465, gb_free=10, wall=1019
2022-02-11 18:04:25 | INFO | train_inner | epoch 001:    305 / 1576 loss=12.988, ppl=8121.98, wps=13786.5, ups=0.21, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.237, loss_scale=8, train_wall=465, gb_free=10, wall=1495
2022-02-11 18:12:20 | INFO | train_inner | epoch 001:    405 / 1576 loss=11.074, ppl=2156.41, wps=13788.7, ups=0.21, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.702, loss_scale=8, train_wall=465, gb_free=10, wall=1970
2022-02-11 18:20:15 | INFO | train_inner | epoch 001:    505 / 1576 loss=10.23, ppl=1201.33, wps=13796, ups=0.21, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.524, loss_scale=8, train_wall=465, gb_free=10, wall=2445
2022-02-11 18:28:10 | INFO | train_inner | epoch 001:    605 / 1576 loss=9.838, ppl=915.45, wps=13792.3, ups=0.21, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.556, loss_scale=16, train_wall=465, gb_free=10, wall=2920
2022-02-11 18:36:05 | INFO | train_inner | epoch 001:    705 / 1576 loss=9.543, ppl=745.96, wps=13799.8, ups=0.21, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.671, loss_scale=16, train_wall=464, gb_free=10, wall=3395
2022-02-11 18:44:00 | INFO | train_inner | epoch 001:    805 / 1576 loss=9.296, ppl=628.66, wps=13803.5, ups=0.21, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.716, loss_scale=32, train_wall=464, gb_free=10, wall=3870
2022-02-11 18:51:55 | INFO | train_inner | epoch 001:    905 / 1576 loss=9.055, ppl=531.85, wps=13797.2, ups=0.21, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.868, loss_scale=32, train_wall=464, gb_free=10, wall=4345
2022-02-11 18:59:50 | INFO | train_inner | epoch 001:   1005 / 1576 loss=8.851, ppl=461.88, wps=13802.2, ups=0.21, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.885, loss_scale=32, train_wall=464, gb_free=10, wall=4820
2022-02-11 19:01:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:07:49 | INFO | train_inner | epoch 001:   1106 / 1576 loss=8.666, ppl=406.23, wps=13671.5, ups=0.21, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.894, loss_scale=32, train_wall=469, gb_free=10, wall=5299
2022-02-11 19:15:44 | INFO | train_inner | epoch 001:   1206 / 1576 loss=8.499, ppl=361.88, wps=13806.8, ups=0.21, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.941, loss_scale=32, train_wall=464, gb_free=10, wall=5774
2022-02-11 19:22:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:23:43 | INFO | train_inner | epoch 001:   1307 / 1576 loss=8.359, ppl=328.25, wps=13669.9, ups=0.21, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.954, loss_scale=32, train_wall=469, gb_free=10, wall=6253
2022-02-11 19:31:38 | INFO | train_inner | epoch 001:   1407 / 1576 loss=8.216, ppl=297.4, wps=13803.2, ups=0.21, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.945, loss_scale=32, train_wall=464, gb_free=10, wall=6728
2022-02-11 19:39:33 | INFO | train_inner | epoch 001:   1507 / 1576 loss=8.083, ppl=271.11, wps=13808.5, ups=0.21, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.968, loss_scale=32, train_wall=464, gb_free=10, wall=7203
2022-02-11 19:42:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:44:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 19:45:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.771 | ppl 218.41 | wps 37152.2 | wpb 1021.8 | bsz 2 | num_updates 1568
2022-02-11 19:45:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1568 updates
2022-02-11 19:45:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint1.pt
2022-02-11 19:45:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint1.pt
2022-02-11 19:45:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint1.pt (epoch 1 @ 1568 updates, score 7.771) (writing took 29.94689169805497 seconds)
2022-02-11 19:45:32 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-11 19:45:32 | INFO | train | epoch 001 | loss 10.31 | ppl 1269.44 | wps 13707 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 1568 | lr 0.000196061 | gnorm 1.08 | loss_scale 32 | train_wall 7357 | gb_free 10 | wall 7562
2022-02-11 19:45:32 | INFO | fairseq.trainer | begin training epoch 2
2022-02-11 19:45:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 19:48:04 | INFO | train_inner | epoch 002:     32 / 1576 loss=7.949, ppl=247.14, wps=12716.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=0.966, loss_scale=32, train_wall=464, gb_free=10, wall=7713
2022-02-11 19:55:58 | INFO | train_inner | epoch 002:    132 / 1576 loss=7.82, ppl=226.01, wps=13809.9, ups=0.21, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.956, loss_scale=32, train_wall=464, gb_free=10, wall=8188
2022-02-11 20:03:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:03:58 | INFO | train_inner | epoch 002:    233 / 1576 loss=7.707, ppl=208.9, wps=13673.3, ups=0.21, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.926, loss_scale=32, train_wall=469, gb_free=10, wall=8667
2022-02-11 20:11:52 | INFO | train_inner | epoch 002:    333 / 1576 loss=7.605, ppl=194.66, wps=13809.9, ups=0.21, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.925, loss_scale=32, train_wall=464, gb_free=10, wall=9142
2022-02-11 20:19:47 | INFO | train_inner | epoch 002:    433 / 1576 loss=7.515, ppl=182.95, wps=13813.4, ups=0.21, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.891, loss_scale=32, train_wall=464, gb_free=10, wall=9616
2022-02-11 20:25:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:27:46 | INFO | train_inner | epoch 002:    534 / 1576 loss=7.404, ppl=169.37, wps=13678.2, ups=0.21, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.89, loss_scale=32, train_wall=468, gb_free=10, wall=10095
2022-02-11 20:32:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 20:35:45 | INFO | train_inner | epoch 002:    635 / 1576 loss=7.343, ppl=162.34, wps=13678, ups=0.21, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.885, loss_scale=16, train_wall=468, gb_free=10, wall=10575
2022-02-11 20:43:39 | INFO | train_inner | epoch 002:    735 / 1576 loss=7.243, ppl=151.43, wps=13818.6, ups=0.21, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.843, loss_scale=16, train_wall=464, gb_free=10, wall=11049
2022-02-11 20:51:34 | INFO | train_inner | epoch 002:    835 / 1576 loss=7.177, ppl=144.69, wps=13806.3, ups=0.21, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.858, loss_scale=16, train_wall=464, gb_free=10, wall=11523
2022-02-11 20:59:28 | INFO | train_inner | epoch 002:    935 / 1576 loss=7.104, ppl=137.56, wps=13811.2, ups=0.21, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.843, loss_scale=32, train_wall=464, gb_free=10, wall=11998
2022-02-11 21:06:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 21:07:27 | INFO | train_inner | epoch 002:   1036 / 1576 loss=7.031, ppl=130.76, wps=13677.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.833, loss_scale=16, train_wall=469, gb_free=10, wall=12477
2022-02-11 21:15:22 | INFO | train_inner | epoch 002:   1136 / 1576 loss=6.968, ppl=125.17, wps=13814.2, ups=0.21, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.797, loss_scale=16, train_wall=464, gb_free=10, wall=12952
2022-02-11 21:23:16 | INFO | train_inner | epoch 002:   1236 / 1576 loss=6.907, ppl=119.98, wps=13809.3, ups=0.21, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.812, loss_scale=16, train_wall=464, gb_free=10, wall=13426
2022-02-11 21:30:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 21:31:15 | INFO | train_inner | epoch 002:   1337 / 1576 loss=6.836, ppl=114.21, wps=13682.2, ups=0.21, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.805, loss_scale=16, train_wall=468, gb_free=10, wall=13905
2022-02-11 21:39:10 | INFO | train_inner | epoch 002:   1437 / 1576 loss=6.783, ppl=110.11, wps=13821, ups=0.21, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.771, loss_scale=16, train_wall=464, gb_free=10, wall=14379
2022-02-11 21:47:04 | INFO | train_inner | epoch 002:   1537 / 1576 loss=6.731, ppl=106.2, wps=13815.7, ups=0.21, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.768, loss_scale=16, train_wall=464, gb_free=10, wall=14854
2022-02-11 21:50:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 21:50:11 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.528 | ppl 92.26 | wps 37298.4 | wpb 1021.8 | bsz 2 | num_updates 3139 | best_loss 6.528
2022-02-11 21:50:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3139 updates
2022-02-11 21:50:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint2.pt
2022-02-11 21:50:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint2.pt
2022-02-11 21:50:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint2.pt (epoch 2 @ 3139 updates, score 6.528) (writing took 30.179465698078275 seconds)
2022-02-11 21:50:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-11 21:50:41 | INFO | train | epoch 002 | loss 7.212 | ppl 148.27 | wps 13703.6 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 3139 | lr 0.000392397 | gnorm 0.854 | loss_scale 16 | train_wall 7306 | gb_free 10 | wall 15071
2022-02-11 21:50:41 | INFO | fairseq.trainer | begin training epoch 3
2022-02-11 21:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 21:55:30 | INFO | train_inner | epoch 003:     61 / 1576 loss=6.631, ppl=99.12, wps=12835.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.792, loss_scale=32, train_wall=459, gb_free=10, wall=15360
2022-02-11 21:57:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:03:29 | INFO | train_inner | epoch 003:    162 / 1576 loss=6.562, ppl=94.52, wps=13679.9, ups=0.21, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.759, loss_scale=16, train_wall=468, gb_free=10, wall=15839
2022-02-11 22:11:23 | INFO | train_inner | epoch 003:    262 / 1576 loss=6.525, ppl=92.08, wps=13822.3, ups=0.21, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.755, loss_scale=16, train_wall=464, gb_free=10, wall=16313
2022-02-11 22:19:18 | INFO | train_inner | epoch 003:    362 / 1576 loss=6.486, ppl=89.61, wps=13812.5, ups=0.21, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.75, loss_scale=32, train_wall=464, gb_free=10, wall=16787
2022-02-11 22:22:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:27:17 | INFO | train_inner | epoch 003:    463 / 1576 loss=6.437, ppl=86.64, wps=13674.7, ups=0.21, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.732, loss_scale=16, train_wall=469, gb_free=10, wall=17267
2022-02-11 22:35:11 | INFO | train_inner | epoch 003:    563 / 1576 loss=6.405, ppl=84.76, wps=13818.5, ups=0.21, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.74, loss_scale=16, train_wall=464, gb_free=10, wall=17741
2022-02-11 22:43:06 | INFO | train_inner | epoch 003:    663 / 1576 loss=6.39, ppl=83.88, wps=13810.4, ups=0.21, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.749, loss_scale=32, train_wall=464, gb_free=10, wall=18215
2022-02-11 22:44:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:51:05 | INFO | train_inner | epoch 003:    764 / 1576 loss=6.323, ppl=80.07, wps=13683.7, ups=0.21, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.701, loss_scale=16, train_wall=468, gb_free=10, wall=18694
2022-02-11 22:58:59 | INFO | train_inner | epoch 003:    864 / 1576 loss=6.299, ppl=78.72, wps=13821.3, ups=0.21, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.721, loss_scale=16, train_wall=464, gb_free=10, wall=19169
2022-02-11 23:06:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:06:58 | INFO | train_inner | epoch 003:    965 / 1576 loss=6.27, ppl=77.15, wps=13684.6, ups=0.21, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.69, loss_scale=16, train_wall=468, gb_free=10, wall=19647
2022-02-11 23:14:52 | INFO | train_inner | epoch 003:   1065 / 1576 loss=6.25, ppl=76.09, wps=13817.1, ups=0.21, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.677, loss_scale=16, train_wall=464, gb_free=10, wall=20122
2022-02-11 23:22:46 | INFO | train_inner | epoch 003:   1165 / 1576 loss=6.211, ppl=74.07, wps=13819.9, ups=0.21, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.665, loss_scale=16, train_wall=464, gb_free=10, wall=20596
2022-02-11 23:28:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:30:45 | INFO | train_inner | epoch 003:   1266 / 1576 loss=6.167, ppl=71.85, wps=13688.4, ups=0.21, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.641, loss_scale=16, train_wall=468, gb_free=10, wall=21075
2022-02-11 23:38:39 | INFO | train_inner | epoch 003:   1366 / 1576 loss=6.148, ppl=70.91, wps=13815.4, ups=0.21, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.654, loss_scale=16, train_wall=464, gb_free=10, wall=21549
2022-02-11 23:46:34 | INFO | train_inner | epoch 003:   1466 / 1576 loss=6.119, ppl=69.48, wps=13821.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.612, loss_scale=16, train_wall=464, gb_free=10, wall=22023
2022-02-11 23:49:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:54:33 | INFO | train_inner | epoch 003:   1567 / 1576 loss=6.092, ppl=68.2, wps=13682.8, ups=0.21, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.62, loss_scale=16, train_wall=468, gb_free=10, wall=22502
2022-02-11 23:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 23:55:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.925 | ppl 60.75 | wps 37134.5 | wpb 1021.8 | bsz 2 | num_updates 4709 | best_loss 5.925
2022-02-11 23:55:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4709 updates
2022-02-11 23:55:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint3.pt
2022-02-11 23:55:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint3.pt
2022-02-11 23:55:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint3.pt (epoch 3 @ 4709 updates, score 5.925) (writing took 32.15065150056034 seconds)
2022-02-11 23:55:49 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-11 23:55:49 | INFO | train | epoch 003 | loss 6.322 | ppl 80 | wps 13696.3 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 4709 | lr 0.000460825 | gnorm 0.7 | loss_scale 16 | train_wall 7303 | gb_free 10 | wall 22579
2022-02-11 23:55:49 | INFO | fairseq.trainer | begin training epoch 4
2022-02-11 23:55:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 00:03:00 | INFO | train_inner | epoch 004:     91 / 1576 loss=5.971, ppl=62.73, wps=12789.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.614, loss_scale=16, train_wall=459, gb_free=10, wall=23010
2022-02-12 00:10:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:10:59 | INFO | train_inner | epoch 004:    192 / 1576 loss=5.939, ppl=61.35, wps=13685.6, ups=0.21, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.603, loss_scale=16, train_wall=468, gb_free=10, wall=23489
2022-02-12 00:18:53 | INFO | train_inner | epoch 004:    292 / 1576 loss=5.933, ppl=61.08, wps=13824.9, ups=0.21, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.613, loss_scale=16, train_wall=463, gb_free=10, wall=23963
2022-02-12 00:26:48 | INFO | train_inner | epoch 004:    392 / 1576 loss=5.916, ppl=60.4, wps=13818.6, ups=0.21, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.589, loss_scale=16, train_wall=464, gb_free=10, wall=24437
2022-02-12 00:31:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:34:46 | INFO | train_inner | epoch 004:    493 / 1576 loss=5.909, ppl=60.08, wps=13688, ups=0.21, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.593, loss_scale=16, train_wall=468, gb_free=10, wall=24916
2022-02-12 00:42:40 | INFO | train_inner | epoch 004:    593 / 1576 loss=5.893, ppl=59.44, wps=13824.7, ups=0.21, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.583, loss_scale=16, train_wall=463, gb_free=10, wall=25390
2022-02-12 00:50:35 | INFO | train_inner | epoch 004:    693 / 1576 loss=5.89, ppl=59.32, wps=13824.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.585, loss_scale=16, train_wall=463, gb_free=10, wall=25864
2022-02-12 00:52:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:58:33 | INFO | train_inner | epoch 004:    794 / 1576 loss=5.872, ppl=58.56, wps=13685.7, ups=0.21, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.59, loss_scale=16, train_wall=468, gb_free=10, wall=26343
2022-02-12 01:06:27 | INFO | train_inner | epoch 004:    894 / 1576 loss=5.851, ppl=57.71, wps=13824.2, ups=0.21, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.607, loss_scale=16, train_wall=463, gb_free=10, wall=26817
2022-02-12 01:14:22 | INFO | train_inner | epoch 004:    994 / 1576 loss=5.831, ppl=56.94, wps=13824.7, ups=0.21, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.556, loss_scale=32, train_wall=464, gb_free=10, wall=27291
2022-02-12 01:15:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:22:20 | INFO | train_inner | epoch 004:   1095 / 1576 loss=5.832, ppl=56.97, wps=13692.3, ups=0.21, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.581, loss_scale=16, train_wall=468, gb_free=10, wall=27770
2022-02-12 01:30:14 | INFO | train_inner | epoch 004:   1195 / 1576 loss=5.82, ppl=56.5, wps=13822.1, ups=0.21, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.572, loss_scale=16, train_wall=464, gb_free=10, wall=28244
2022-02-12 01:35:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:38:13 | INFO | train_inner | epoch 004:   1296 / 1576 loss=5.817, ppl=56.39, wps=13689.3, ups=0.21, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.587, loss_scale=16, train_wall=468, gb_free=10, wall=28723
2022-02-12 01:46:07 | INFO | train_inner | epoch 004:   1396 / 1576 loss=5.793, ppl=55.44, wps=13823.7, ups=0.21, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.566, loss_scale=16, train_wall=464, gb_free=10, wall=29197
2022-02-12 01:54:01 | INFO | train_inner | epoch 004:   1496 / 1576 loss=5.787, ppl=55.21, wps=13825.5, ups=0.21, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.585, loss_scale=16, train_wall=463, gb_free=10, wall=29671
2022-02-12 01:57:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:00:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 02:00:23 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.658 | ppl 50.5 | wps 37174 | wpb 1021.8 | bsz 2 | num_updates 6279 | best_loss 5.658
2022-02-12 02:00:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6279 updates
2022-02-12 02:00:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint4.pt
2022-02-12 02:00:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint4.pt
2022-02-12 02:00:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint4.pt (epoch 4 @ 6279 updates, score 5.658) (writing took 29.128852194175124 seconds)
2022-02-12 02:00:52 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-12 02:00:52 | INFO | train | epoch 004 | loss 5.864 | ppl 58.25 | wps 13706.4 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 6279 | lr 0.000399075 | gnorm 0.588 | loss_scale 16 | train_wall 7301 | gb_free 10 | wall 30081
2022-02-12 02:00:52 | INFO | fairseq.trainer | begin training epoch 5
2022-02-12 02:00:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 02:02:31 | INFO | train_inner | epoch 005:     21 / 1576 loss=5.748, ppl=53.75, wps=12738.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.566, loss_scale=16, train_wall=464, gb_free=10, wall=30181
2022-02-12 02:10:25 | INFO | train_inner | epoch 005:    121 / 1576 loss=5.642, ppl=49.95, wps=13824, ups=0.21, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.557, loss_scale=16, train_wall=463, gb_free=10, wall=30655
2022-02-12 02:18:19 | INFO | train_inner | epoch 005:    221 / 1576 loss=5.639, ppl=49.85, wps=13824.8, ups=0.21, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.555, loss_scale=32, train_wall=463, gb_free=10, wall=31129
2022-02-12 02:19:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:26:18 | INFO | train_inner | epoch 005:    322 / 1576 loss=5.642, ppl=49.95, wps=13689.1, ups=0.21, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.562, loss_scale=16, train_wall=468, gb_free=10, wall=31608
2022-02-12 02:34:12 | INFO | train_inner | epoch 005:    422 / 1576 loss=5.635, ppl=49.69, wps=13822.2, ups=0.21, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.552, loss_scale=16, train_wall=464, gb_free=10, wall=32082
2022-02-12 02:41:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:42:11 | INFO | train_inner | epoch 005:    523 / 1576 loss=5.637, ppl=49.76, wps=13680.5, ups=0.21, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.547, loss_scale=16, train_wall=468, gb_free=10, wall=32561
2022-02-12 02:50:06 | INFO | train_inner | epoch 005:    623 / 1576 loss=5.636, ppl=49.73, wps=13810, ups=0.21, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.581, loss_scale=16, train_wall=464, gb_free=10, wall=33035
2022-02-12 02:58:00 | INFO | train_inner | epoch 005:    723 / 1576 loss=5.63, ppl=49.53, wps=13817.1, ups=0.21, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.538, loss_scale=16, train_wall=464, gb_free=10, wall=33510
2022-02-12 03:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:03:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 03:06:03 | INFO | train_inner | epoch 005:    825 / 1576 loss=5.622, ppl=49.25, wps=13559, ups=0.21, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.565, loss_scale=8, train_wall=473, gb_free=10, wall=33993
2022-02-12 03:13:57 | INFO | train_inner | epoch 005:    925 / 1576 loss=5.62, ppl=49.19, wps=13834.5, ups=0.21, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.544, loss_scale=8, train_wall=463, gb_free=10, wall=34467
2022-02-12 03:21:51 | INFO | train_inner | epoch 005:   1025 / 1576 loss=5.63, ppl=49.53, wps=13829.1, ups=0.21, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.579, loss_scale=8, train_wall=463, gb_free=10, wall=34941
2022-02-12 03:29:45 | INFO | train_inner | epoch 005:   1125 / 1576 loss=5.601, ppl=48.54, wps=13818.8, ups=0.21, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.563, loss_scale=16, train_wall=464, gb_free=10, wall=35415
2022-02-12 03:37:40 | INFO | train_inner | epoch 005:   1225 / 1576 loss=5.607, ppl=48.75, wps=13814.7, ups=0.21, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.547, loss_scale=16, train_wall=464, gb_free=10, wall=35889
2022-02-12 03:44:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:45:38 | INFO | train_inner | epoch 005:   1326 / 1576 loss=5.596, ppl=48.37, wps=13688.8, ups=0.21, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.559, loss_scale=16, train_wall=468, gb_free=10, wall=36368
2022-02-12 03:53:32 | INFO | train_inner | epoch 005:   1426 / 1576 loss=5.598, ppl=48.45, wps=13825.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.526, loss_scale=16, train_wall=463, gb_free=10, wall=36842
2022-02-12 04:01:26 | INFO | train_inner | epoch 005:   1526 / 1576 loss=5.59, ppl=48.17, wps=13823, ups=0.21, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.583, loss_scale=16, train_wall=464, gb_free=10, wall=37316
2022-02-12 04:05:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 04:05:26 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.499 | ppl 45.21 | wps 37177.4 | wpb 1021.8 | bsz 2 | num_updates 7850 | best_loss 5.499
2022-02-12 04:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7850 updates
2022-02-12 04:05:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint5.pt
2022-02-12 04:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint5.pt
2022-02-12 04:05:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint5.pt (epoch 5 @ 7850 updates, score 5.499) (writing took 28.974768690764904 seconds)
2022-02-12 04:05:55 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-12 04:05:55 | INFO | train | epoch 005 | loss 5.622 | ppl 49.23 | wps 13714.5 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 7850 | lr 0.000356915 | gnorm 0.556 | loss_scale 32 | train_wall 7301 | gb_free 10 | wall 37584
2022-02-12 04:05:55 | INFO | fairseq.trainer | begin training epoch 6
2022-02-12 04:05:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 04:06:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:09:56 | INFO | train_inner | epoch 006:     51 / 1576 loss=5.527, ppl=46.1, wps=12744.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.558, loss_scale=16, train_wall=464, gb_free=10, wall=37826
2022-02-12 04:17:51 | INFO | train_inner | epoch 006:    151 / 1576 loss=5.459, ppl=44, wps=13817.2, ups=0.21, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.531, loss_scale=16, train_wall=464, gb_free=10, wall=38300
2022-02-12 04:25:45 | INFO | train_inner | epoch 006:    251 / 1576 loss=5.469, ppl=44.3, wps=13819, ups=0.21, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.591, loss_scale=16, train_wall=464, gb_free=10, wall=38774
2022-02-12 04:27:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:33:44 | INFO | train_inner | epoch 006:    352 / 1576 loss=5.467, ppl=44.23, wps=13679.5, ups=0.21, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.539, loss_scale=16, train_wall=468, gb_free=10, wall=39254
2022-02-12 04:41:38 | INFO | train_inner | epoch 006:    452 / 1576 loss=5.471, ppl=44.36, wps=13821.2, ups=0.21, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.532, loss_scale=16, train_wall=464, gb_free=10, wall=39728
2022-02-12 04:48:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:49:37 | INFO | train_inner | epoch 006:    553 / 1576 loss=5.472, ppl=44.37, wps=13686.4, ups=0.21, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.572, loss_scale=16, train_wall=468, gb_free=10, wall=40207
2022-02-12 04:57:31 | INFO | train_inner | epoch 006:    653 / 1576 loss=5.476, ppl=44.52, wps=13817.2, ups=0.21, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.538, loss_scale=16, train_wall=464, gb_free=10, wall=40681
2022-02-12 05:05:25 | INFO | train_inner | epoch 006:    753 / 1576 loss=5.468, ppl=44.26, wps=13822.9, ups=0.21, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.563, loss_scale=16, train_wall=464, gb_free=10, wall=41155
2022-02-12 05:09:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:09:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 05:13:29 | INFO | train_inner | epoch 006:    855 / 1576 loss=5.484, ppl=44.76, wps=13558.5, ups=0.21, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.562, loss_scale=8, train_wall=473, gb_free=10, wall=41638
2022-02-12 05:21:23 | INFO | train_inner | epoch 006:    955 / 1576 loss=5.485, ppl=44.79, wps=13825.3, ups=0.21, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.57, loss_scale=8, train_wall=463, gb_free=10, wall=42112
2022-02-12 05:29:17 | INFO | train_inner | epoch 006:   1055 / 1576 loss=5.473, ppl=44.41, wps=13831.1, ups=0.21, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.526, loss_scale=8, train_wall=463, gb_free=10, wall=42586
2022-02-12 05:37:11 | INFO | train_inner | epoch 006:   1155 / 1576 loss=5.466, ppl=44.21, wps=13817.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.544, loss_scale=16, train_wall=464, gb_free=10, wall=43060
2022-02-12 05:45:05 | INFO | train_inner | epoch 006:   1255 / 1576 loss=5.464, ppl=44.15, wps=13818.6, ups=0.21, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.556, loss_scale=16, train_wall=464, gb_free=10, wall=43535
2022-02-12 05:50:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:53:04 | INFO | train_inner | epoch 006:   1356 / 1576 loss=5.47, ppl=44.31, wps=13679.2, ups=0.21, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.542, loss_scale=16, train_wall=468, gb_free=10, wall=44014
2022-02-12 06:00:58 | INFO | train_inner | epoch 006:   1456 / 1576 loss=5.472, ppl=44.38, wps=13818.5, ups=0.21, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.539, loss_scale=16, train_wall=464, gb_free=10, wall=44488
2022-02-12 06:08:53 | INFO | train_inner | epoch 006:   1556 / 1576 loss=5.463, ppl=44.1, wps=13815, ups=0.21, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.549, loss_scale=16, train_wall=464, gb_free=10, wall=44962
2022-02-12 06:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 06:10:29 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.402 | ppl 42.28 | wps 37160.3 | wpb 1021.8 | bsz 2 | num_updates 9420 | best_loss 5.402
2022-02-12 06:10:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9420 updates
2022-02-12 06:10:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint6.pt
2022-02-12 06:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint6.pt
2022-02-12 06:10:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint6.pt (epoch 6 @ 9420 updates, score 5.402) (writing took 28.95017486438155 seconds)
2022-02-12 06:10:58 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-12 06:10:58 | INFO | train | epoch 006 | loss 5.47 | ppl 44.32 | wps 13704.4 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 9420 | lr 0.000325818 | gnorm 0.552 | loss_scale 32 | train_wall 7302 | gb_free 10 | wall 45088
2022-02-12 06:10:58 | INFO | fairseq.trainer | begin training epoch 7
2022-02-12 06:10:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 06:16:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 06:17:22 | INFO | train_inner | epoch 007:     81 / 1576 loss=5.348, ppl=40.73, wps=12745.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.535, loss_scale=16, train_wall=464, gb_free=10, wall=45472
2022-02-12 06:25:17 | INFO | train_inner | epoch 007:    181 / 1576 loss=5.343, ppl=40.6, wps=13817, ups=0.21, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.559, loss_scale=16, train_wall=464, gb_free=10, wall=45947
2022-02-12 06:33:11 | INFO | train_inner | epoch 007:    281 / 1576 loss=5.351, ppl=40.8, wps=13811.5, ups=0.21, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.524, loss_scale=16, train_wall=464, gb_free=10, wall=46421
2022-02-12 06:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 06:41:10 | INFO | train_inner | epoch 007:    382 / 1576 loss=5.357, ppl=41, wps=13682, ups=0.21, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.535, loss_scale=16, train_wall=468, gb_free=10, wall=46900
2022-02-12 06:49:05 | INFO | train_inner | epoch 007:    482 / 1576 loss=5.362, ppl=41.14, wps=13813.8, ups=0.21, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.551, loss_scale=16, train_wall=464, gb_free=10, wall=47374
2022-02-12 06:56:59 | INFO | train_inner | epoch 007:    582 / 1576 loss=5.362, ppl=41.12, wps=13822.6, ups=0.21, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.538, loss_scale=16, train_wall=464, gb_free=10, wall=47849
2022-02-12 06:57:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:04:58 | INFO | train_inner | epoch 007:    683 / 1576 loss=5.368, ppl=41.3, wps=13684.4, ups=0.21, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.572, loss_scale=16, train_wall=468, gb_free=10, wall=48327
2022-02-12 07:12:52 | INFO | train_inner | epoch 007:    783 / 1576 loss=5.368, ppl=41.29, wps=13806.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.56, loss_scale=16, train_wall=464, gb_free=10, wall=48802
2022-02-12 07:18:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:20:51 | INFO | train_inner | epoch 007:    884 / 1576 loss=5.377, ppl=41.55, wps=13684.8, ups=0.21, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.587, loss_scale=16, train_wall=468, gb_free=10, wall=49281
2022-02-12 07:28:46 | INFO | train_inner | epoch 007:    984 / 1576 loss=5.366, ppl=41.24, wps=13819.1, ups=0.21, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.573, loss_scale=16, train_wall=464, gb_free=10, wall=49755
2022-02-12 07:36:40 | INFO | train_inner | epoch 007:   1084 / 1576 loss=5.37, ppl=41.35, wps=13821.6, ups=0.21, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.54, loss_scale=16, train_wall=464, gb_free=10, wall=50229
2022-02-12 07:38:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:44:39 | INFO | train_inner | epoch 007:   1185 / 1576 loss=5.368, ppl=41.28, wps=13679.5, ups=0.21, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.543, loss_scale=16, train_wall=468, gb_free=10, wall=50708
2022-02-12 07:52:33 | INFO | train_inner | epoch 007:   1285 / 1576 loss=5.374, ppl=41.48, wps=13818, ups=0.21, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.553, loss_scale=16, train_wall=464, gb_free=10, wall=51183
2022-02-12 08:00:27 | INFO | train_inner | epoch 007:   1385 / 1576 loss=5.374, ppl=41.46, wps=13820.1, ups=0.21, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.535, loss_scale=32, train_wall=464, gb_free=10, wall=51657
2022-02-12 08:01:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:08:26 | INFO | train_inner | epoch 007:   1486 / 1576 loss=5.365, ppl=41.21, wps=13684.5, ups=0.21, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.554, loss_scale=16, train_wall=468, gb_free=10, wall=52136
2022-02-12 08:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 08:15:35 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.346 | ppl 40.67 | wps 37121.5 | wpb 1021.8 | bsz 2 | num_updates 10990 | best_loss 5.346
2022-02-12 08:15:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10990 updates
2022-02-12 08:15:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint7.pt
2022-02-12 08:15:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint7.pt
2022-02-12 08:16:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint7.pt (epoch 7 @ 10990 updates, score 5.346) (writing took 28.57461259048432 seconds)
2022-02-12 08:16:04 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-12 08:16:04 | INFO | train | epoch 007 | loss 5.363 | ppl 41.17 | wps 13701.3 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 10990 | lr 0.000301648 | gnorm 0.55 | loss_scale 16 | train_wall 7304 | gb_free 10 | wall 52593
2022-02-12 08:16:04 | INFO | fairseq.trainer | begin training epoch 8
2022-02-12 08:16:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 08:16:51 | INFO | train_inner | epoch 008:     10 / 1576 loss=5.365, ppl=41.22, wps=12864.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.557, loss_scale=16, train_wall=460, gb_free=10, wall=52641
2022-02-12 08:21:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:24:50 | INFO | train_inner | epoch 008:    111 / 1576 loss=5.24, ppl=37.8, wps=13693, ups=0.21, wpb=65532.3, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.553, loss_scale=16, train_wall=468, gb_free=10, wall=53119
2022-02-12 08:32:44 | INFO | train_inner | epoch 008:    211 / 1576 loss=5.253, ppl=38.12, wps=13817.2, ups=0.21, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.549, loss_scale=16, train_wall=464, gb_free=10, wall=53594
2022-02-12 08:40:38 | INFO | train_inner | epoch 008:    311 / 1576 loss=5.262, ppl=38.37, wps=13818.2, ups=0.21, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.536, loss_scale=16, train_wall=464, gb_free=10, wall=54068
2022-02-12 08:42:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:48:37 | INFO | train_inner | epoch 008:    412 / 1576 loss=5.28, ppl=38.85, wps=13685.1, ups=0.21, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.572, loss_scale=16, train_wall=468, gb_free=10, wall=54547
2022-02-12 08:56:31 | INFO | train_inner | epoch 008:    512 / 1576 loss=5.279, ppl=38.84, wps=13822, ups=0.21, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.551, loss_scale=16, train_wall=464, gb_free=10, wall=55021
2022-02-12 09:02:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 09:04:30 | INFO | train_inner | epoch 008:    613 / 1576 loss=5.287, ppl=39.04, wps=13688.6, ups=0.21, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.578, loss_scale=16, train_wall=468, gb_free=10, wall=55500
2022-02-12 09:12:24 | INFO | train_inner | epoch 008:    713 / 1576 loss=5.277, ppl=38.79, wps=13820.5, ups=0.21, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.524, loss_scale=16, train_wall=464, gb_free=10, wall=55974
2022-02-12 09:17:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 09:20:23 | INFO | train_inner | epoch 008:    814 / 1576 loss=5.288, ppl=39.07, wps=13690.2, ups=0.21, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.554, loss_scale=8, train_wall=468, gb_free=10, wall=56453
2022-02-12 09:28:17 | INFO | train_inner | epoch 008:    914 / 1576 loss=5.288, ppl=39.08, wps=13827.6, ups=0.21, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.536, loss_scale=8, train_wall=463, gb_free=10, wall=56927
2022-02-12 09:36:11 | INFO | train_inner | epoch 008:   1014 / 1576 loss=5.287, ppl=39.05, wps=13825.7, ups=0.21, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.541, loss_scale=8, train_wall=463, gb_free=10, wall=57401
2022-02-12 09:44:05 | INFO | train_inner | epoch 008:   1114 / 1576 loss=5.298, ppl=39.34, wps=13821.3, ups=0.21, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.573, loss_scale=16, train_wall=464, gb_free=10, wall=57875
2022-02-12 09:51:59 | INFO | train_inner | epoch 008:   1214 / 1576 loss=5.297, ppl=39.32, wps=13819.4, ups=0.21, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.55, loss_scale=16, train_wall=464, gb_free=10, wall=58349
2022-02-12 09:59:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 09:59:58 | INFO | train_inner | epoch 008:   1315 / 1576 loss=5.285, ppl=39, wps=13685.6, ups=0.21, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.551, loss_scale=16, train_wall=468, gb_free=10, wall=58828
2022-02-12 10:07:52 | INFO | train_inner | epoch 008:   1415 / 1576 loss=5.307, ppl=39.58, wps=13819.6, ups=0.21, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.555, loss_scale=16, train_wall=464, gb_free=10, wall=59302
2022-02-12 10:15:47 | INFO | train_inner | epoch 008:   1515 / 1576 loss=5.295, ppl=39.26, wps=13812.3, ups=0.21, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.53, loss_scale=16, train_wall=464, gb_free=10, wall=59777
2022-02-12 10:17:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 10:20:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 10:20:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.297 | ppl 39.32 | wps 37126.5 | wpb 1021.8 | bsz 2 | num_updates 12560 | best_loss 5.297
2022-02-12 10:20:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12560 updates
2022-02-12 10:20:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint8.pt
2022-02-12 10:20:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint8.pt
2022-02-12 10:21:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint8.pt (epoch 8 @ 12560 updates, score 5.297) (writing took 28.12783065997064 seconds)
2022-02-12 10:21:06 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-12 10:21:06 | INFO | train | epoch 008 | loss 5.282 | ppl 38.92 | wps 13706.8 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 12560 | lr 0.000282166 | gnorm 0.551 | loss_scale 8 | train_wall 7301 | gb_free 10 | wall 60096
2022-02-12 10:21:06 | INFO | fairseq.trainer | begin training epoch 9
2022-02-12 10:21:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 10:24:16 | INFO | train_inner | epoch 009:     40 / 1576 loss=5.255, ppl=38.19, wps=12774.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.553, loss_scale=8, train_wall=464, gb_free=10, wall=60285
2022-02-12 10:32:10 | INFO | train_inner | epoch 009:    140 / 1576 loss=5.168, ppl=35.96, wps=13824.7, ups=0.21, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.545, loss_scale=8, train_wall=463, gb_free=10, wall=60759
2022-02-12 10:40:04 | INFO | train_inner | epoch 009:    240 / 1576 loss=5.186, ppl=36.41, wps=13822.4, ups=0.21, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.549, loss_scale=16, train_wall=464, gb_free=10, wall=61233
2022-02-12 10:47:58 | INFO | train_inner | epoch 009:    340 / 1576 loss=5.212, ppl=37.05, wps=13811.1, ups=0.21, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.544, loss_scale=16, train_wall=464, gb_free=10, wall=61708
2022-02-12 10:55:53 | INFO | train_inner | epoch 009:    440 / 1576 loss=5.214, ppl=37.12, wps=13812.2, ups=0.21, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.552, loss_scale=16, train_wall=464, gb_free=10, wall=62182
2022-02-12 10:58:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 11:03:52 | INFO | train_inner | epoch 009:    541 / 1576 loss=5.212, ppl=37.07, wps=13684.6, ups=0.21, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.541, loss_scale=16, train_wall=468, gb_free=10, wall=62661
2022-02-12 11:11:46 | INFO | train_inner | epoch 009:    641 / 1576 loss=5.224, ppl=37.38, wps=13812, ups=0.21, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.543, loss_scale=16, train_wall=464, gb_free=10, wall=63136
2022-02-12 11:18:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 11:19:45 | INFO | train_inner | epoch 009:    742 / 1576 loss=5.217, ppl=37.21, wps=13682.7, ups=0.21, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.573, loss_scale=16, train_wall=468, gb_free=10, wall=63615
2022-02-12 11:25:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 11:27:44 | INFO | train_inner | epoch 009:    843 / 1576 loss=5.219, ppl=37.25, wps=13690.7, ups=0.21, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.547, loss_scale=8, train_wall=468, gb_free=10, wall=64093
2022-02-12 11:35:38 | INFO | train_inner | epoch 009:    943 / 1576 loss=5.219, ppl=37.24, wps=13824.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.542, loss_scale=8, train_wall=463, gb_free=10, wall=64567
2022-02-12 11:43:32 | INFO | train_inner | epoch 009:   1043 / 1576 loss=5.234, ppl=37.62, wps=13827.1, ups=0.21, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.545, loss_scale=8, train_wall=463, gb_free=10, wall=65041
2022-02-12 11:51:26 | INFO | train_inner | epoch 009:   1143 / 1576 loss=5.229, ppl=37.51, wps=13821.2, ups=0.21, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.557, loss_scale=16, train_wall=464, gb_free=10, wall=65516
2022-02-12 11:59:20 | INFO | train_inner | epoch 009:   1243 / 1576 loss=5.235, ppl=37.66, wps=13820.4, ups=0.21, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.558, loss_scale=16, train_wall=464, gb_free=10, wall=65990
2022-02-12 12:06:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 12:07:19 | INFO | train_inner | epoch 009:   1344 / 1576 loss=5.235, ppl=37.66, wps=13683, ups=0.21, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.547, loss_scale=16, train_wall=468, gb_free=10, wall=66469
2022-02-12 12:15:13 | INFO | train_inner | epoch 009:   1444 / 1576 loss=5.239, ppl=37.75, wps=13816.5, ups=0.21, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.555, loss_scale=16, train_wall=464, gb_free=10, wall=66943
2022-02-12 12:23:08 | INFO | train_inner | epoch 009:   1544 / 1576 loss=5.242, ppl=37.83, wps=13815.1, ups=0.21, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.55, loss_scale=16, train_wall=464, gb_free=10, wall=67417
2022-02-12 12:25:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 12:25:41 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.253 | ppl 38.13 | wps 37249.2 | wpb 1021.8 | bsz 2 | num_updates 14132 | best_loss 5.253
2022-02-12 12:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14132 updates
2022-02-12 12:25:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint9.pt
2022-02-12 12:25:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint9.pt
2022-02-12 12:26:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint9.pt (epoch 9 @ 14132 updates, score 5.253) (writing took 28.276602504774928 seconds)
2022-02-12 12:26:10 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-12 12:26:10 | INFO | train | epoch 009 | loss 5.219 | ppl 37.23 | wps 13722.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 14132 | lr 0.00026601 | gnorm 0.551 | loss_scale 16 | train_wall 7302 | gb_free 10 | wall 67599
2022-02-12 12:26:10 | INFO | fairseq.trainer | begin training epoch 10
2022-02-12 12:26:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 12:28:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 12:31:37 | INFO | train_inner | epoch 010:     69 / 1576 loss=5.162, ppl=35.79, wps=12768, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=14200, lr=0.000265372, gnorm=0.566, loss_scale=16, train_wall=464, gb_free=10, wall=67926
2022-02-12 12:39:31 | INFO | train_inner | epoch 010:    169 / 1576 loss=5.127, ppl=34.93, wps=13821.3, ups=0.21, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.555, loss_scale=16, train_wall=464, gb_free=10, wall=68400
2022-02-12 12:47:25 | INFO | train_inner | epoch 010:    269 / 1576 loss=5.134, ppl=35.12, wps=13816.3, ups=0.21, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.573, loss_scale=16, train_wall=464, gb_free=10, wall=68875
2022-02-12 12:49:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 12:55:24 | INFO | train_inner | epoch 010:    370 / 1576 loss=5.149, ppl=35.48, wps=13685.6, ups=0.21, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.565, loss_scale=16, train_wall=468, gb_free=10, wall=69354
2022-02-12 12:56:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:03:23 | INFO | train_inner | epoch 010:    471 / 1576 loss=5.156, ppl=35.64, wps=13692.8, ups=0.21, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.577, loss_scale=8, train_wall=468, gb_free=10, wall=69832
2022-02-12 13:11:17 | INFO | train_inner | epoch 010:    571 / 1576 loss=5.157, ppl=35.68, wps=13825.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.575, loss_scale=8, train_wall=463, gb_free=10, wall=70306
2022-02-12 13:19:11 | INFO | train_inner | epoch 010:    671 / 1576 loss=5.178, ppl=36.2, wps=13824.1, ups=0.21, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.533, loss_scale=16, train_wall=463, gb_free=10, wall=70780
2022-02-12 13:27:05 | INFO | train_inner | epoch 010:    771 / 1576 loss=5.163, ppl=35.83, wps=13818.9, ups=0.21, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.587, loss_scale=16, train_wall=464, gb_free=10, wall=71255
2022-02-12 13:34:59 | INFO | train_inner | epoch 010:    871 / 1576 loss=5.168, ppl=35.94, wps=13817.5, ups=0.21, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.556, loss_scale=16, train_wall=464, gb_free=10, wall=71729
2022-02-12 13:37:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 13:42:58 | INFO | train_inner | epoch 010:    972 / 1576 loss=5.178, ppl=36.2, wps=13678.6, ups=0.21, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.563, loss_scale=16, train_wall=468, gb_free=10, wall=72208
2022-02-12 13:50:53 | INFO | train_inner | epoch 010:   1072 / 1576 loss=5.182, ppl=36.31, wps=13812.8, ups=0.21, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.548, loss_scale=16, train_wall=464, gb_free=10, wall=72682
2022-02-12 13:57:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 13:58:52 | INFO | train_inner | epoch 010:   1173 / 1576 loss=5.179, ppl=36.23, wps=13684.2, ups=0.21, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.543, loss_scale=16, train_wall=468, gb_free=10, wall=73161
2022-02-12 14:06:46 | INFO | train_inner | epoch 010:   1273 / 1576 loss=5.176, ppl=36.15, wps=13823.8, ups=0.21, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.561, loss_scale=16, train_wall=464, gb_free=10, wall=73635
2022-02-12 14:14:40 | INFO | train_inner | epoch 010:   1373 / 1576 loss=5.186, ppl=36.4, wps=13822.4, ups=0.21, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.54, loss_scale=16, train_wall=464, gb_free=10, wall=74110
2022-02-12 14:18:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 14:22:39 | INFO | train_inner | epoch 010:   1474 / 1576 loss=5.205, ppl=36.89, wps=13676.2, ups=0.21, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.592, loss_scale=16, train_wall=469, gb_free=10, wall=74589
2022-02-12 14:30:33 | INFO | train_inner | epoch 010:   1574 / 1576 loss=5.193, ppl=36.58, wps=13816.6, ups=0.21, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.547, loss_scale=16, train_wall=464, gb_free=10, wall=75063
2022-02-12 14:30:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 14:30:45 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.227 | ppl 37.44 | wps 37079.1 | wpb 1021.8 | bsz 2 | num_updates 15702 | best_loss 5.227
2022-02-12 14:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15702 updates
2022-02-12 14:30:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint10.pt
2022-02-12 14:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint10.pt
2022-02-12 14:31:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint10.pt (epoch 10 @ 15702 updates, score 5.227) (writing took 28.343133055604994 seconds)
2022-02-12 14:31:13 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-12 14:31:13 | INFO | train | epoch 010 | loss 5.167 | ppl 35.92 | wps 13705 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 15702 | lr 0.000252361 | gnorm 0.56 | loss_scale 16 | train_wall 7302 | gb_free 10 | wall 75103
2022-02-12 14:31:13 | INFO | fairseq.trainer | begin training epoch 11
2022-02-12 14:31:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 14:38:58 | INFO | train_inner | epoch 011:     98 / 1576 loss=5.057, ppl=33.28, wps=12884, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=15800, lr=0.000251577, gnorm=0.6, loss_scale=32, train_wall=459, gb_free=10, wall=75567
2022-02-12 14:39:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 14:46:56 | INFO | train_inner | epoch 011:    199 / 1576 loss=5.078, ppl=33.78, wps=13687.1, ups=0.21, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.546, loss_scale=16, train_wall=468, gb_free=10, wall=76046
2022-02-12 14:54:51 | INFO | train_inner | epoch 011:    299 / 1576 loss=5.103, ppl=34.36, wps=13819, ups=0.21, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.54, loss_scale=16, train_wall=464, gb_free=10, wall=76520
2022-02-12 14:59:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 15:02:50 | INFO | train_inner | epoch 011:    400 / 1576 loss=5.101, ppl=34.31, wps=13680, ups=0.21, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.574, loss_scale=16, train_wall=468, gb_free=10, wall=76999
2022-02-12 15:10:44 | INFO | train_inner | epoch 011:    500 / 1576 loss=5.106, ppl=34.45, wps=13821, ups=0.21, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.559, loss_scale=16, train_wall=464, gb_free=10, wall=77474
2022-02-12 15:10:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:18:43 | INFO | train_inner | epoch 011:    601 / 1576 loss=5.108, ppl=34.49, wps=13692.9, ups=0.21, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.591, loss_scale=8, train_wall=468, gb_free=10, wall=77952
2022-02-12 15:26:37 | INFO | train_inner | epoch 011:    701 / 1576 loss=5.119, ppl=34.75, wps=13823.7, ups=0.21, wpb=65532.3, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.576, loss_scale=8, train_wall=463, gb_free=10, wall=78426
2022-02-12 15:34:31 | INFO | train_inner | epoch 011:    801 / 1576 loss=5.134, ppl=35.11, wps=13824, ups=0.21, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.564, loss_scale=16, train_wall=463, gb_free=10, wall=78900
2022-02-12 15:42:25 | INFO | train_inner | epoch 011:    901 / 1576 loss=5.133, ppl=35.09, wps=13823.2, ups=0.21, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.586, loss_scale=16, train_wall=464, gb_free=10, wall=79374
2022-02-12 15:48:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:50:24 | INFO | train_inner | epoch 011:   1002 / 1576 loss=5.13, ppl=35.03, wps=13679.6, ups=0.21, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.579, loss_scale=8, train_wall=468, gb_free=10, wall=79854
2022-02-12 15:58:18 | INFO | train_inner | epoch 011:   1102 / 1576 loss=5.133, ppl=35.08, wps=13831.4, ups=0.21, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.568, loss_scale=8, train_wall=463, gb_free=10, wall=80327
2022-02-12 16:06:12 | INFO | train_inner | epoch 011:   1202 / 1576 loss=5.148, ppl=35.46, wps=13829.9, ups=0.21, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.594, loss_scale=8, train_wall=463, gb_free=10, wall=80801
2022-02-12 16:09:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 16:14:10 | INFO | train_inner | epoch 011:   1303 / 1576 loss=5.149, ppl=35.49, wps=13696.5, ups=0.21, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.602, loss_scale=8, train_wall=468, gb_free=10, wall=81280
2022-02-12 16:22:04 | INFO | train_inner | epoch 011:   1403 / 1576 loss=5.158, ppl=35.71, wps=13839.9, ups=0.21, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.561, loss_scale=8, train_wall=463, gb_free=10, wall=81753
2022-02-12 16:29:57 | INFO | train_inner | epoch 011:   1503 / 1576 loss=5.166, ppl=35.91, wps=13832, ups=0.21, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.584, loss_scale=8, train_wall=463, gb_free=10, wall=82227
2022-02-12 16:35:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 16:35:45 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.202 | ppl 36.81 | wps 37052.2 | wpb 1021.8 | bsz 2 | num_updates 17273 | best_loss 5.202
2022-02-12 16:35:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17273 updates
2022-02-12 16:35:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint11.pt
2022-02-12 16:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint11.pt
2022-02-12 16:36:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint11.pt (epoch 11 @ 17273 updates, score 5.202) (writing took 28.28596945758909 seconds)
2022-02-12 16:36:14 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-12 16:36:14 | INFO | train | epoch 011 | loss 5.123 | ppl 34.85 | wps 13718.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 17273 | lr 0.000240611 | gnorm 0.574 | loss_scale 16 | train_wall 7299 | gb_free 10 | wall 82603
2022-02-12 16:36:14 | INFO | fairseq.trainer | begin training epoch 12
2022-02-12 16:36:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 16:38:22 | INFO | train_inner | epoch 012:     27 / 1576 loss=5.127, ppl=34.94, wps=12883.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=17300, lr=0.000240424, gnorm=0.575, loss_scale=16, train_wall=459, gb_free=10, wall=82731
2022-02-12 16:46:16 | INFO | train_inner | epoch 012:    127 / 1576 loss=5.032, ppl=32.73, wps=13817.4, ups=0.21, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.577, loss_scale=16, train_wall=464, gb_free=10, wall=83206
2022-02-12 16:51:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 16:54:15 | INFO | train_inner | epoch 012:    228 / 1576 loss=5.048, ppl=33.08, wps=13683.3, ups=0.21, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.542, loss_scale=16, train_wall=468, gb_free=10, wall=83685
2022-02-12 17:02:09 | INFO | train_inner | epoch 012:    328 / 1576 loss=5.061, ppl=33.39, wps=13816.2, ups=0.21, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.559, loss_scale=16, train_wall=464, gb_free=10, wall=84159
2022-02-12 17:07:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 17:10:08 | INFO | train_inner | epoch 012:    429 / 1576 loss=5.07, ppl=33.6, wps=13685.8, ups=0.21, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.591, loss_scale=8, train_wall=468, gb_free=10, wall=84638
2022-02-12 17:18:02 | INFO | train_inner | epoch 012:    529 / 1576 loss=5.057, ppl=33.3, wps=13829.4, ups=0.21, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.573, loss_scale=8, train_wall=463, gb_free=10, wall=85112
2022-02-12 17:25:56 | INFO | train_inner | epoch 012:    629 / 1576 loss=5.086, ppl=33.96, wps=13825.9, ups=0.21, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.586, loss_scale=8, train_wall=463, gb_free=10, wall=85586
2022-02-12 17:33:50 | INFO | train_inner | epoch 012:    729 / 1576 loss=5.083, ppl=33.89, wps=13815.7, ups=0.21, wpb=65532.3, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.558, loss_scale=16, train_wall=464, gb_free=10, wall=86060
2022-02-12 17:41:45 | INFO | train_inner | epoch 012:    829 / 1576 loss=5.092, ppl=34.11, wps=13812.5, ups=0.21, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.576, loss_scale=16, train_wall=464, gb_free=10, wall=86534
2022-02-12 17:47:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 17:49:44 | INFO | train_inner | epoch 012:    930 / 1576 loss=5.097, ppl=34.22, wps=13682, ups=0.21, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.573, loss_scale=16, train_wall=468, gb_free=10, wall=87013
2022-02-12 17:53:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 17:57:43 | INFO | train_inner | epoch 012:   1031 / 1576 loss=5.105, ppl=34.42, wps=13689.3, ups=0.21, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.581, loss_scale=8, train_wall=468, gb_free=10, wall=87492
2022-02-12 18:05:36 | INFO | train_inner | epoch 012:   1131 / 1576 loss=5.105, ppl=34.42, wps=13828.7, ups=0.21, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.582, loss_scale=8, train_wall=463, gb_free=10, wall=87966
2022-02-12 18:13:31 | INFO | train_inner | epoch 012:   1231 / 1576 loss=5.102, ppl=34.34, wps=13819.3, ups=0.21, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.566, loss_scale=8, train_wall=464, gb_free=10, wall=88440
2022-02-12 18:21:25 | INFO | train_inner | epoch 012:   1331 / 1576 loss=5.124, ppl=34.86, wps=13821.1, ups=0.21, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.569, loss_scale=16, train_wall=464, gb_free=10, wall=88915
2022-02-12 18:22:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 18:29:23 | INFO | train_inner | epoch 012:   1432 / 1576 loss=5.114, ppl=34.63, wps=13691.6, ups=0.21, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.578, loss_scale=8, train_wall=468, gb_free=10, wall=89393
2022-02-12 18:37:18 | INFO | train_inner | epoch 012:   1532 / 1576 loss=5.121, ppl=34.8, wps=13823.3, ups=0.21, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.589, loss_scale=8, train_wall=464, gb_free=10, wall=89867
2022-02-12 18:40:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 18:40:48 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.183 | ppl 36.34 | wps 37104.3 | wpb 1021.8 | bsz 2 | num_updates 18844 | best_loss 5.183
2022-02-12 18:40:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18844 updates
2022-02-12 18:40:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint12.pt
2022-02-12 18:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint12.pt
2022-02-12 18:41:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint12.pt (epoch 12 @ 18844 updates, score 5.183) (writing took 28.340003478340805 seconds)
2022-02-12 18:41:16 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-12 18:41:16 | INFO | train | epoch 012 | loss 5.087 | ppl 33.99 | wps 13715.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 18844 | lr 0.000230363 | gnorm 0.575 | loss_scale 8 | train_wall 7301 | gb_free 10 | wall 90106
2022-02-12 18:41:16 | INFO | fairseq.trainer | begin training epoch 13
2022-02-12 18:41:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 18:45:42 | INFO | train_inner | epoch 013:     56 / 1576 loss=5.055, ppl=33.24, wps=12887.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=18900, lr=0.000230022, gnorm=0.583, loss_scale=16, train_wall=459, gb_free=10, wall=90371
2022-02-12 18:53:36 | INFO | train_inner | epoch 013:    156 / 1576 loss=5.002, ppl=32.05, wps=13817.5, ups=0.21, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.578, loss_scale=16, train_wall=464, gb_free=10, wall=90846
2022-02-12 18:57:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:01:35 | INFO | train_inner | epoch 013:    257 / 1576 loss=5.011, ppl=32.25, wps=13682.9, ups=0.21, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.574, loss_scale=8, train_wall=468, gb_free=10, wall=91325
2022-02-12 19:09:29 | INFO | train_inner | epoch 013:    357 / 1576 loss=5.028, ppl=32.63, wps=13823.4, ups=0.21, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.576, loss_scale=8, train_wall=464, gb_free=10, wall=91799
2022-02-12 19:17:23 | INFO | train_inner | epoch 013:    457 / 1576 loss=5.032, ppl=32.71, wps=13827.2, ups=0.21, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.573, loss_scale=8, train_wall=463, gb_free=10, wall=92273
2022-02-12 19:20:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:25:22 | INFO | train_inner | epoch 013:    558 / 1576 loss=5.044, ppl=33, wps=13692.4, ups=0.21, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.581, loss_scale=8, train_wall=468, gb_free=10, wall=92751
2022-02-12 19:33:16 | INFO | train_inner | epoch 013:    658 / 1576 loss=5.061, ppl=33.38, wps=13829.5, ups=0.21, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.564, loss_scale=8, train_wall=463, gb_free=10, wall=93225
2022-02-12 19:41:09 | INFO | train_inner | epoch 013:    758 / 1576 loss=5.062, ppl=33.4, wps=13828.6, ups=0.21, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.567, loss_scale=16, train_wall=463, gb_free=10, wall=93699
2022-02-12 19:49:04 | INFO | train_inner | epoch 013:    858 / 1576 loss=5.055, ppl=33.25, wps=13822.4, ups=0.21, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.57, loss_scale=16, train_wall=464, gb_free=10, wall=94173
2022-02-12 19:56:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:57:03 | INFO | train_inner | epoch 013:    959 / 1576 loss=5.06, ppl=33.36, wps=13679.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.582, loss_scale=8, train_wall=468, gb_free=10, wall=94652
2022-02-12 20:04:56 | INFO | train_inner | epoch 013:   1059 / 1576 loss=5.068, ppl=33.55, wps=13840.6, ups=0.21, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.577, loss_scale=8, train_wall=463, gb_free=10, wall=95126
2022-02-12 20:12:50 | INFO | train_inner | epoch 013:   1159 / 1576 loss=5.075, ppl=33.7, wps=13830.4, ups=0.21, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.589, loss_scale=8, train_wall=463, gb_free=10, wall=95600
2022-02-12 20:20:44 | INFO | train_inner | epoch 013:   1259 / 1576 loss=5.088, ppl=34, wps=13827.3, ups=0.21, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.579, loss_scale=16, train_wall=463, gb_free=10, wall=96074
2022-02-12 20:28:38 | INFO | train_inner | epoch 013:   1359 / 1576 loss=5.089, ppl=34.04, wps=13822.1, ups=0.21, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.587, loss_scale=16, train_wall=464, gb_free=10, wall=96548
2022-02-12 20:32:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 20:36:37 | INFO | train_inner | epoch 013:   1460 / 1576 loss=5.095, ppl=34.17, wps=13688.1, ups=0.21, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.565, loss_scale=8, train_wall=468, gb_free=10, wall=97027
2022-02-12 20:44:31 | INFO | train_inner | epoch 013:   1560 / 1576 loss=5.09, ppl=34.07, wps=13826, ups=0.21, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.566, loss_scale=8, train_wall=463, gb_free=10, wall=97501
2022-02-12 20:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 20:45:48 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.169 | ppl 35.98 | wps 37195.3 | wpb 1021.8 | bsz 2 | num_updates 20416 | best_loss 5.169
2022-02-12 20:45:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20416 updates
2022-02-12 20:45:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint13.pt
2022-02-12 20:45:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint13.pt
2022-02-12 20:46:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint13.pt (epoch 13 @ 20416 updates, score 5.169) (writing took 28.84313949663192 seconds)
2022-02-12 20:46:17 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-12 20:46:17 | INFO | train | epoch 013 | loss 5.056 | ppl 33.26 | wps 13726.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 20416 | lr 0.000221317 | gnorm 0.575 | loss_scale 8 | train_wall 7299 | gb_free 10 | wall 97607
2022-02-12 20:46:17 | INFO | fairseq.trainer | begin training epoch 14
2022-02-12 20:46:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 20:52:55 | INFO | train_inner | epoch 014:     84 / 1576 loss=4.992, ppl=31.82, wps=12882.7, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=20500, lr=0.000220863, gnorm=0.588, loss_scale=8, train_wall=459, gb_free=10, wall=98005
2022-02-12 21:00:49 | INFO | train_inner | epoch 014:    184 / 1576 loss=4.989, ppl=31.77, wps=13817.4, ups=0.21, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.601, loss_scale=16, train_wall=464, gb_free=10, wall=98479
2022-02-12 21:08:44 | INFO | train_inner | epoch 014:    284 / 1576 loss=4.995, ppl=31.9, wps=13818.3, ups=0.21, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.589, loss_scale=16, train_wall=464, gb_free=10, wall=98953
2022-02-12 21:14:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 21:16:43 | INFO | train_inner | epoch 014:    385 / 1576 loss=5.001, ppl=32.03, wps=13681.3, ups=0.21, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.585, loss_scale=16, train_wall=468, gb_free=10, wall=99432
2022-02-12 21:24:37 | INFO | train_inner | epoch 014:    485 / 1576 loss=5.002, ppl=32.04, wps=13818.1, ups=0.21, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.592, loss_scale=16, train_wall=464, gb_free=10, wall=99907
2022-02-12 21:32:31 | INFO | train_inner | epoch 014:    585 / 1576 loss=5.025, ppl=32.55, wps=13824.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.571, loss_scale=16, train_wall=463, gb_free=10, wall=100381
2022-02-12 21:34:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 21:40:30 | INFO | train_inner | epoch 014:    686 / 1576 loss=5.023, ppl=32.5, wps=13687.5, ups=0.21, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.568, loss_scale=16, train_wall=468, gb_free=10, wall=100860
2022-02-12 21:48:24 | INFO | train_inner | epoch 014:    786 / 1576 loss=5.029, ppl=32.66, wps=13817.8, ups=0.21, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.581, loss_scale=16, train_wall=464, gb_free=10, wall=101334
2022-02-12 21:51:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 21:56:23 | INFO | train_inner | epoch 014:    887 / 1576 loss=5.034, ppl=32.76, wps=13689, ups=0.21, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.589, loss_scale=8, train_wall=468, gb_free=10, wall=101813
2022-02-12 22:04:17 | INFO | train_inner | epoch 014:    987 / 1576 loss=5.03, ppl=32.68, wps=13830.2, ups=0.21, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.585, loss_scale=8, train_wall=463, gb_free=10, wall=102286
2022-02-12 22:12:11 | INFO | train_inner | epoch 014:   1087 / 1576 loss=5.049, ppl=33.1, wps=13828.4, ups=0.21, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.589, loss_scale=16, train_wall=463, gb_free=10, wall=102760
2022-02-12 22:20:05 | INFO | train_inner | epoch 014:   1187 / 1576 loss=5.049, ppl=33.1, wps=13824.1, ups=0.21, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.6, loss_scale=16, train_wall=464, gb_free=10, wall=103234
2022-02-12 22:21:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 22:28:03 | INFO | train_inner | epoch 014:   1288 / 1576 loss=5.058, ppl=33.32, wps=13692.3, ups=0.21, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.583, loss_scale=8, train_wall=468, gb_free=10, wall=103713
2022-02-12 22:35:57 | INFO | train_inner | epoch 014:   1388 / 1576 loss=5.05, ppl=33.12, wps=13832.9, ups=0.21, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.576, loss_scale=8, train_wall=463, gb_free=10, wall=104187
2022-02-12 22:43:51 | INFO | train_inner | epoch 014:   1488 / 1576 loss=5.064, ppl=33.46, wps=13821.5, ups=0.21, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.607, loss_scale=16, train_wall=464, gb_free=10, wall=104661
2022-02-12 22:50:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 22:50:51 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.167 | ppl 35.93 | wps 37137.8 | wpb 1021.8 | bsz 2 | num_updates 21988 | best_loss 5.167
2022-02-12 22:50:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21988 updates
2022-02-12 22:50:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint14.pt
2022-02-12 22:51:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint14.pt
2022-02-12 22:51:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint14.pt (epoch 14 @ 21988 updates, score 5.167) (writing took 30.11933350749314 seconds)
2022-02-12 22:51:21 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-12 22:51:21 | INFO | train | epoch 014 | loss 5.028 | ppl 32.62 | wps 13722.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 21988 | lr 0.000213259 | gnorm 0.586 | loss_scale 16 | train_wall 7301 | gb_free 10 | wall 105111
2022-02-12 22:51:21 | INFO | fairseq.trainer | begin training epoch 15
2022-02-12 22:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 22:52:18 | INFO | train_inner | epoch 015:     12 / 1576 loss=5.051, ppl=33.15, wps=12829.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=22000, lr=0.000213201, gnorm=0.588, loss_scale=16, train_wall=460, gb_free=10, wall=105167
2022-02-12 22:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 23:00:16 | INFO | train_inner | epoch 015:    113 / 1576 loss=4.944, ppl=30.77, wps=13703.8, ups=0.21, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.593, loss_scale=8, train_wall=467, gb_free=10, wall=105646
2022-02-12 23:08:10 | INFO | train_inner | epoch 015:    213 / 1576 loss=4.966, ppl=31.25, wps=13822.8, ups=0.21, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.59, loss_scale=8, train_wall=463, gb_free=10, wall=106120
2022-02-12 23:15:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 23:16:08 | INFO | train_inner | epoch 015:    314 / 1576 loss=4.968, ppl=31.29, wps=13697.8, ups=0.21, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.597, loss_scale=8, train_wall=468, gb_free=10, wall=106598
2022-02-12 23:24:02 | INFO | train_inner | epoch 015:    414 / 1576 loss=4.967, ppl=31.28, wps=13830.4, ups=0.21, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.568, loss_scale=8, train_wall=463, gb_free=10, wall=107072
2022-02-12 23:31:56 | INFO | train_inner | epoch 015:    514 / 1576 loss=4.989, ppl=31.76, wps=13832.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.582, loss_scale=8, train_wall=463, gb_free=10, wall=107546
2022-02-12 23:36:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 23:39:55 | INFO | train_inner | epoch 015:    615 / 1576 loss=4.996, ppl=31.9, wps=13690.9, ups=0.21, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.586, loss_scale=8, train_wall=468, gb_free=10, wall=108024
2022-02-12 23:47:49 | INFO | train_inner | epoch 015:    715 / 1576 loss=5.002, ppl=32.04, wps=13829.7, ups=0.21, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.618, loss_scale=8, train_wall=463, gb_free=10, wall=108498
2022-02-12 23:55:43 | INFO | train_inner | epoch 015:    815 / 1576 loss=5.006, ppl=32.12, wps=13827.9, ups=0.21, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.574, loss_scale=8, train_wall=463, gb_free=10, wall=108972
2022-02-13 00:01:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:03:41 | INFO | train_inner | epoch 015:    916 / 1576 loss=5.01, ppl=32.21, wps=13693.9, ups=0.21, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.588, loss_scale=8, train_wall=468, gb_free=10, wall=109451
2022-02-13 00:11:35 | INFO | train_inner | epoch 015:   1016 / 1576 loss=5.031, ppl=32.71, wps=13831.5, ups=0.21, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.593, loss_scale=8, train_wall=463, gb_free=10, wall=109925
2022-02-13 00:19:29 | INFO | train_inner | epoch 015:   1116 / 1576 loss=5.031, ppl=32.69, wps=13836.2, ups=0.21, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.573, loss_scale=8, train_wall=463, gb_free=10, wall=110398
2022-02-13 00:24:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:27:27 | INFO | train_inner | epoch 015:   1217 / 1576 loss=5.028, ppl=32.64, wps=13694.1, ups=0.21, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.591, loss_scale=8, train_wall=468, gb_free=10, wall=110877
2022-02-13 00:35:21 | INFO | train_inner | epoch 015:   1317 / 1576 loss=5.036, ppl=32.81, wps=13832, ups=0.21, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.581, loss_scale=8, train_wall=463, gb_free=10, wall=111351
2022-02-13 00:43:15 | INFO | train_inner | epoch 015:   1417 / 1576 loss=5.029, ppl=32.65, wps=13831.6, ups=0.21, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.598, loss_scale=8, train_wall=463, gb_free=10, wall=111824
2022-02-13 00:51:09 | INFO | train_inner | epoch 015:   1517 / 1576 loss=5.045, ppl=33.02, wps=13817.7, ups=0.21, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.586, loss_scale=16, train_wall=464, gb_free=10, wall=112299
2022-02-13 00:55:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 00:55:51 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.157 | ppl 35.67 | wps 37080.1 | wpb 1021.8 | bsz 2 | num_updates 23559 | best_loss 5.157
2022-02-13 00:55:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23559 updates
2022-02-13 00:55:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint15.pt
2022-02-13 00:55:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint15.pt
2022-02-13 00:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint15.pt (epoch 15 @ 23559 updates, score 5.157) (writing took 28.539675056003034 seconds)
2022-02-13 00:56:19 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-13 00:56:19 | INFO | train | epoch 015 | loss 5.004 | ppl 32.08 | wps 13722.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 23559 | lr 0.000206026 | gnorm 0.589 | loss_scale 16 | train_wall 7297 | gb_free 10 | wall 112609
2022-02-13 00:56:19 | INFO | fairseq.trainer | begin training epoch 16
2022-02-13 00:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 00:59:33 | INFO | train_inner | epoch 016:     41 / 1576 loss=4.986, ppl=31.7, wps=12882.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=23600, lr=0.000205847, gnorm=0.609, loss_scale=16, train_wall=459, gb_free=10, wall=112803
2022-02-13 01:05:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 01:07:32 | INFO | train_inner | epoch 016:    142 / 1576 loss=4.928, ppl=30.43, wps=13678.5, ups=0.21, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.584, loss_scale=16, train_wall=468, gb_free=10, wall=113282
2022-02-13 01:11:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 01:15:31 | INFO | train_inner | epoch 016:    243 / 1576 loss=4.942, ppl=30.73, wps=13683, ups=0.21, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.619, loss_scale=8, train_wall=468, gb_free=10, wall=113761
2022-02-13 01:23:25 | INFO | train_inner | epoch 016:    343 / 1576 loss=4.948, ppl=30.88, wps=13832.2, ups=0.21, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.568, loss_scale=8, train_wall=463, gb_free=10, wall=114235
2022-02-13 01:31:19 | INFO | train_inner | epoch 016:    443 / 1576 loss=4.96, ppl=31.13, wps=13826.2, ups=0.21, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.602, loss_scale=8, train_wall=463, gb_free=10, wall=114709
2022-02-13 01:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 01:39:18 | INFO | train_inner | epoch 016:    544 / 1576 loss=4.969, ppl=31.32, wps=13690.6, ups=0.21, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.624, loss_scale=8, train_wall=468, gb_free=10, wall=115188
2022-02-13 01:47:12 | INFO | train_inner | epoch 016:    644 / 1576 loss=4.983, ppl=31.63, wps=13824.8, ups=0.21, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.595, loss_scale=8, train_wall=463, gb_free=10, wall=115662
2022-02-13 01:55:06 | INFO | train_inner | epoch 016:    744 / 1576 loss=4.977, ppl=31.5, wps=13823.9, ups=0.21, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.578, loss_scale=16, train_wall=463, gb_free=10, wall=116136
2022-02-13 01:57:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:03:05 | INFO | train_inner | epoch 016:    845 / 1576 loss=4.985, ppl=31.66, wps=13684.1, ups=0.21, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.604, loss_scale=8, train_wall=468, gb_free=10, wall=116615
2022-02-13 02:10:59 | INFO | train_inner | epoch 016:    945 / 1576 loss=4.995, ppl=31.88, wps=13823.3, ups=0.21, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.602, loss_scale=8, train_wall=463, gb_free=10, wall=117089
2022-02-13 02:18:53 | INFO | train_inner | epoch 016:   1045 / 1576 loss=5.004, ppl=32.09, wps=13824.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.585, loss_scale=16, train_wall=463, gb_free=10, wall=117563
2022-02-13 02:19:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:26:52 | INFO | train_inner | epoch 016:   1146 / 1576 loss=5.013, ppl=32.29, wps=13693, ups=0.21, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.635, loss_scale=8, train_wall=468, gb_free=10, wall=118041
2022-02-13 02:34:46 | INFO | train_inner | epoch 016:   1246 / 1576 loss=4.998, ppl=31.95, wps=13827.2, ups=0.21, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.59, loss_scale=8, train_wall=463, gb_free=10, wall=118515
2022-02-13 02:42:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:42:44 | INFO | train_inner | epoch 016:   1347 / 1576 loss=5.008, ppl=32.17, wps=13688, ups=0.21, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.587, loss_scale=8, train_wall=468, gb_free=10, wall=118994
2022-02-13 02:50:39 | INFO | train_inner | epoch 016:   1447 / 1576 loss=5.019, ppl=32.41, wps=13816.7, ups=0.21, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.582, loss_scale=8, train_wall=464, gb_free=10, wall=119468
2022-02-13 02:58:33 | INFO | train_inner | epoch 016:   1547 / 1576 loss=5.019, ppl=32.41, wps=13825.9, ups=0.21, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.603, loss_scale=8, train_wall=463, gb_free=10, wall=119942
2022-02-13 03:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 03:00:52 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.14 | ppl 35.25 | wps 37101.3 | wpb 1021.8 | bsz 2 | num_updates 25129 | best_loss 5.14
2022-02-13 03:00:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 25129 updates
2022-02-13 03:00:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint16.pt
2022-02-13 03:01:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint16.pt
2022-02-13 03:01:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint16.pt (epoch 16 @ 25129 updates, score 5.14) (writing took 28.402607971802354 seconds)
2022-02-13 03:01:20 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-13 03:01:20 | INFO | train | epoch 016 | loss 4.982 | ppl 31.61 | wps 13708.9 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 25129 | lr 0.000199486 | gnorm 0.597 | loss_scale 8 | train_wall 7299 | gb_free 10 | wall 120110
2022-02-13 03:01:21 | INFO | fairseq.trainer | begin training epoch 17
2022-02-13 03:01:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 03:06:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:07:01 | INFO | train_inner | epoch 017:     72 / 1576 loss=4.944, ppl=30.78, wps=12770.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=25200, lr=0.000199205, gnorm=0.586, loss_scale=8, train_wall=463, gb_free=10, wall=120451
2022-02-13 03:14:55 | INFO | train_inner | epoch 017:    172 / 1576 loss=4.912, ppl=30.11, wps=13834.1, ups=0.21, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.623, loss_scale=8, train_wall=463, gb_free=10, wall=120925
2022-02-13 03:22:49 | INFO | train_inner | epoch 017:    272 / 1576 loss=4.928, ppl=30.44, wps=13843.7, ups=0.21, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.607, loss_scale=8, train_wall=463, gb_free=10, wall=121398
2022-02-13 03:29:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:30:47 | INFO | train_inner | epoch 017:    373 / 1576 loss=4.926, ppl=30.41, wps=13696.3, ups=0.21, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.628, loss_scale=8, train_wall=468, gb_free=10, wall=121877
2022-02-13 03:38:41 | INFO | train_inner | epoch 017:    473 / 1576 loss=4.947, ppl=30.85, wps=13836.7, ups=0.21, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.577, loss_scale=8, train_wall=463, gb_free=10, wall=122350
2022-02-13 03:46:34 | INFO | train_inner | epoch 017:    573 / 1576 loss=4.95, ppl=30.92, wps=13833.9, ups=0.21, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.596, loss_scale=8, train_wall=463, gb_free=10, wall=122824
2022-02-13 03:49:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:54:33 | INFO | train_inner | epoch 017:    674 / 1576 loss=4.96, ppl=31.12, wps=13698.1, ups=0.21, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.606, loss_scale=8, train_wall=468, gb_free=10, wall=123303
2022-02-13 04:02:27 | INFO | train_inner | epoch 017:    774 / 1576 loss=4.965, ppl=31.24, wps=13834.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.589, loss_scale=8, train_wall=463, gb_free=10, wall=123776
2022-02-13 04:10:20 | INFO | train_inner | epoch 017:    874 / 1576 loss=4.963, ppl=31.18, wps=13843.7, ups=0.21, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.603, loss_scale=16, train_wall=463, gb_free=10, wall=124250
2022-02-13 04:14:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:18:18 | INFO | train_inner | epoch 017:    975 / 1576 loss=4.983, ppl=31.61, wps=13701.7, ups=0.21, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.598, loss_scale=8, train_wall=468, gb_free=10, wall=124728
2022-02-13 04:26:12 | INFO | train_inner | epoch 017:   1075 / 1576 loss=4.988, ppl=31.73, wps=13841.2, ups=0.21, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.61, loss_scale=8, train_wall=463, gb_free=10, wall=125201
2022-02-13 04:34:05 | INFO | train_inner | epoch 017:   1175 / 1576 loss=4.988, ppl=31.74, wps=13842.2, ups=0.21, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.609, loss_scale=8, train_wall=463, gb_free=10, wall=125675
2022-02-13 04:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:42:04 | INFO | train_inner | epoch 017:   1276 / 1576 loss=4.992, ppl=31.82, wps=13702, ups=0.21, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.586, loss_scale=8, train_wall=468, gb_free=10, wall=126153
2022-02-13 04:49:57 | INFO | train_inner | epoch 017:   1376 / 1576 loss=4.989, ppl=31.75, wps=13843.3, ups=0.21, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.598, loss_scale=8, train_wall=463, gb_free=10, wall=126627
2022-02-13 04:57:51 | INFO | train_inner | epoch 017:   1476 / 1576 loss=5.003, ppl=32.07, wps=13828.9, ups=0.21, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.616, loss_scale=8, train_wall=463, gb_free=10, wall=127101
2022-02-13 05:05:41 | INFO | train_inner | epoch 017:   1576 / 1576 loss=5.008, ppl=32.18, wps=13827.5, ups=0.21, wpb=64962.6, bsz=126.9, num_updates=26700, lr=0.000193528, gnorm=0.635, loss_scale=16, train_wall=459, gb_free=10, wall=127570
2022-02-13 05:05:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 05:05:47 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.128 | ppl 34.96 | wps 37135.7 | wpb 1021.8 | bsz 2 | num_updates 26700 | best_loss 5.128
2022-02-13 05:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26700 updates
2022-02-13 05:05:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint17.pt
2022-02-13 05:05:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint17.pt
2022-02-13 05:06:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint17.pt (epoch 17 @ 26700 updates, score 5.128) (writing took 29.112389339134097 seconds)
2022-02-13 05:06:16 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-13 05:06:16 | INFO | train | epoch 017 | loss 4.964 | ppl 31.21 | wps 13728.7 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 26700 | lr 0.000193528 | gnorm 0.605 | loss_scale 16 | train_wall 7293 | gb_free 10 | wall 127605
2022-02-13 05:06:16 | INFO | fairseq.trainer | begin training epoch 18
2022-02-13 05:06:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 05:14:09 | INFO | train_inner | epoch 018:    100 / 1576 loss=4.881, ppl=29.47, wps=12881.3, ups=0.2, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.595, loss_scale=16, train_wall=463, gb_free=10, wall=128079
2022-02-13 05:20:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 05:20:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:22:13 | INFO | train_inner | epoch 018:    202 / 1576 loss=4.901, ppl=29.88, wps=13555.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.585, loss_scale=8, train_wall=473, gb_free=10, wall=128563
2022-02-13 05:30:07 | INFO | train_inner | epoch 018:    302 / 1576 loss=4.908, ppl=30.02, wps=13834.2, ups=0.21, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.631, loss_scale=8, train_wall=463, gb_free=10, wall=129036
2022-02-13 05:38:00 | INFO | train_inner | epoch 018:    402 / 1576 loss=4.922, ppl=30.33, wps=13831.5, ups=0.21, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.593, loss_scale=8, train_wall=463, gb_free=10, wall=129510
2022-02-13 05:45:54 | INFO | train_inner | epoch 018:    502 / 1576 loss=4.934, ppl=30.58, wps=13825.9, ups=0.21, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.6, loss_scale=16, train_wall=463, gb_free=10, wall=129984
2022-02-13 05:48:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:53:53 | INFO | train_inner | epoch 018:    603 / 1576 loss=4.938, ppl=30.66, wps=13698, ups=0.21, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.61, loss_scale=8, train_wall=468, gb_free=10, wall=130463
2022-02-13 06:01:46 | INFO | train_inner | epoch 018:    703 / 1576 loss=4.948, ppl=30.88, wps=13841, ups=0.21, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.605, loss_scale=8, train_wall=463, gb_free=10, wall=130936
2022-02-13 06:09:40 | INFO | train_inner | epoch 018:    803 / 1576 loss=4.945, ppl=30.8, wps=13833.2, ups=0.21, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.602, loss_scale=16, train_wall=463, gb_free=10, wall=131410
2022-02-13 06:17:34 | INFO | train_inner | epoch 018:    903 / 1576 loss=4.96, ppl=31.13, wps=13821.7, ups=0.21, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.597, loss_scale=16, train_wall=464, gb_free=10, wall=131884
2022-02-13 06:21:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 06:25:33 | INFO | train_inner | epoch 018:   1004 / 1576 loss=4.961, ppl=31.15, wps=13696.1, ups=0.21, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.616, loss_scale=8, train_wall=468, gb_free=10, wall=132362
2022-02-13 06:33:27 | INFO | train_inner | epoch 018:   1104 / 1576 loss=4.957, ppl=31.07, wps=13831.4, ups=0.21, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.601, loss_scale=8, train_wall=463, gb_free=10, wall=132836
2022-02-13 06:41:20 | INFO | train_inner | epoch 018:   1204 / 1576 loss=4.975, ppl=31.46, wps=13838.4, ups=0.21, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.599, loss_scale=8, train_wall=463, gb_free=10, wall=133310
2022-02-13 06:49:14 | INFO | train_inner | epoch 018:   1304 / 1576 loss=4.984, ppl=31.64, wps=13829.2, ups=0.21, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.616, loss_scale=16, train_wall=463, gb_free=10, wall=133784
2022-02-13 06:57:08 | INFO | train_inner | epoch 018:   1404 / 1576 loss=4.984, ppl=31.66, wps=13828, ups=0.21, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.602, loss_scale=16, train_wall=463, gb_free=10, wall=134258
2022-02-13 07:01:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 07:05:07 | INFO | train_inner | epoch 018:   1505 / 1576 loss=4.984, ppl=31.64, wps=13680.2, ups=0.21, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.608, loss_scale=16, train_wall=468, gb_free=10, wall=134737
2022-02-13 07:08:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:10:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 07:10:45 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.124 | ppl 34.86 | wps 37266.1 | wpb 1021.8 | bsz 2 | num_updates 28270 | best_loss 5.124
2022-02-13 07:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 28270 updates
2022-02-13 07:10:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint18.pt
2022-02-13 07:10:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint18.pt
2022-02-13 07:11:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint18.pt (epoch 18 @ 28270 updates, score 5.124) (writing took 29.16628777142614 seconds)
2022-02-13 07:11:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-13 07:11:14 | INFO | train | epoch 018 | loss 4.948 | ppl 30.86 | wps 13713.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 28270 | lr 0.000188078 | gnorm 0.605 | loss_scale 8 | train_wall 7297 | gb_free 10 | wall 135104
2022-02-13 07:11:14 | INFO | fairseq.trainer | begin training epoch 19
2022-02-13 07:11:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 07:13:36 | INFO | train_inner | epoch 019:     30 / 1576 loss=4.953, ppl=30.97, wps=12756.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=28300, lr=0.000187978, gnorm=0.614, loss_scale=8, train_wall=463, gb_free=10, wall=135246
2022-02-13 07:21:30 | INFO | train_inner | epoch 019:    130 / 1576 loss=4.872, ppl=29.28, wps=13838.9, ups=0.21, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.613, loss_scale=8, train_wall=463, gb_free=10, wall=135720
2022-02-13 07:29:23 | INFO | train_inner | epoch 019:    230 / 1576 loss=4.886, ppl=29.57, wps=13842.4, ups=0.21, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.609, loss_scale=8, train_wall=463, gb_free=10, wall=136193
2022-02-13 07:37:17 | INFO | train_inner | epoch 019:    330 / 1576 loss=4.899, ppl=29.83, wps=13829.7, ups=0.21, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.608, loss_scale=16, train_wall=463, gb_free=10, wall=136667
2022-02-13 07:38:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:45:16 | INFO | train_inner | epoch 019:    431 / 1576 loss=4.902, ppl=29.9, wps=13697.6, ups=0.21, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.633, loss_scale=8, train_wall=468, gb_free=10, wall=137145
2022-02-13 07:53:09 | INFO | train_inner | epoch 019:    531 / 1576 loss=4.93, ppl=30.49, wps=13842, ups=0.21, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.604, loss_scale=8, train_wall=463, gb_free=10, wall=137619
2022-02-13 08:01:03 | INFO | train_inner | epoch 019:    631 / 1576 loss=4.933, ppl=30.55, wps=13831.4, ups=0.21, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.609, loss_scale=16, train_wall=463, gb_free=10, wall=138093
2022-02-13 08:08:57 | INFO | train_inner | epoch 019:    731 / 1576 loss=4.927, ppl=30.42, wps=13832.1, ups=0.21, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.628, loss_scale=16, train_wall=463, gb_free=10, wall=138566
2022-02-13 08:16:50 | INFO | train_inner | epoch 019:    831 / 1576 loss=4.93, ppl=30.49, wps=13836, ups=0.21, wpb=65536, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.587, loss_scale=16, train_wall=463, gb_free=10, wall=139040
2022-02-13 08:19:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 08:19:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:24:54 | INFO | train_inner | epoch 019:    933 / 1576 loss=4.945, ppl=30.8, wps=13563.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.622, loss_scale=8, train_wall=472, gb_free=10, wall=139523
2022-02-13 08:32:47 | INFO | train_inner | epoch 019:   1033 / 1576 loss=4.945, ppl=30.79, wps=13841.1, ups=0.21, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.598, loss_scale=8, train_wall=463, gb_free=10, wall=139997
2022-02-13 08:40:41 | INFO | train_inner | epoch 019:   1133 / 1576 loss=4.958, ppl=31.09, wps=13840.2, ups=0.21, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.618, loss_scale=16, train_wall=463, gb_free=10, wall=140470
2022-02-13 08:41:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:48:39 | INFO | train_inner | epoch 019:   1234 / 1576 loss=4.949, ppl=30.89, wps=13706.8, ups=0.21, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.635, loss_scale=8, train_wall=467, gb_free=10, wall=140948
2022-02-13 08:56:32 | INFO | train_inner | epoch 019:   1334 / 1576 loss=4.962, ppl=31.17, wps=13841, ups=0.21, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.62, loss_scale=8, train_wall=463, gb_free=10, wall=141422
2022-02-13 09:04:26 | INFO | train_inner | epoch 019:   1434 / 1576 loss=4.975, ppl=31.45, wps=13834.8, ups=0.21, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.605, loss_scale=16, train_wall=463, gb_free=10, wall=141896
2022-02-13 09:05:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 09:12:24 | INFO | train_inner | epoch 019:   1535 / 1576 loss=4.977, ppl=31.5, wps=13709.1, ups=0.21, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.623, loss_scale=8, train_wall=467, gb_free=10, wall=142374
2022-02-13 09:15:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 09:15:40 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.112 | ppl 34.57 | wps 37148.9 | wpb 1021.8 | bsz 2 | num_updates 29841 | best_loss 5.112
2022-02-13 09:15:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 29841 updates
2022-02-13 09:15:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint19.pt
2022-02-13 09:15:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint19.pt
2022-02-13 09:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint19.pt (epoch 19 @ 29841 updates, score 5.112) (writing took 29.37517079245299 seconds)
2022-02-13 09:16:09 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-13 09:16:09 | INFO | train | epoch 019 | loss 4.932 | ppl 30.54 | wps 13729.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 29841 | lr 0.00018306 | gnorm 0.615 | loss_scale 8 | train_wall 7293 | gb_free 10 | wall 142599
2022-02-13 09:16:09 | INFO | fairseq.trainer | begin training epoch 20
2022-02-13 09:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 09:20:48 | INFO | train_inner | epoch 020:     59 / 1576 loss=4.901, ppl=29.87, wps=12875.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=29900, lr=0.000182879, gnorm=0.631, loss_scale=8, train_wall=459, gb_free=10, wall=142878
2022-02-13 09:28:42 | INFO | train_inner | epoch 020:    159 / 1576 loss=4.856, ppl=28.95, wps=13839.6, ups=0.21, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.604, loss_scale=16, train_wall=463, gb_free=10, wall=143352
2022-02-13 09:36:35 | INFO | train_inner | epoch 020:    259 / 1576 loss=4.88, ppl=29.44, wps=13840.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.604, loss_scale=16, train_wall=463, gb_free=10, wall=143825
2022-02-13 09:39:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 09:44:34 | INFO | train_inner | epoch 020:    360 / 1576 loss=4.887, ppl=29.59, wps=13707.9, ups=0.21, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.644, loss_scale=8, train_wall=467, gb_free=10, wall=144303
2022-02-13 09:52:27 | INFO | train_inner | epoch 020:    460 / 1576 loss=4.899, ppl=29.83, wps=13843.7, ups=0.21, wpb=65536, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.633, loss_scale=8, train_wall=463, gb_free=10, wall=144777
2022-02-13 10:00:20 | INFO | train_inner | epoch 020:    560 / 1576 loss=4.907, ppl=30.01, wps=13844.2, ups=0.21, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.609, loss_scale=16, train_wall=463, gb_free=10, wall=145250
2022-02-13 10:08:14 | INFO | train_inner | epoch 020:    660 / 1576 loss=4.907, ppl=30, wps=13838.5, ups=0.21, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.599, loss_scale=16, train_wall=463, gb_free=10, wall=145724
2022-02-13 10:10:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:16:12 | INFO | train_inner | epoch 020:    761 / 1576 loss=4.914, ppl=30.16, wps=13711.9, ups=0.21, wpb=65536, bsz=128, num_updates=30600, lr=0.000180775, gnorm=0.663, loss_scale=8, train_wall=467, gb_free=10, wall=146202
2022-02-13 10:24:05 | INFO | train_inner | epoch 020:    861 / 1576 loss=4.929, ppl=30.46, wps=13846.2, ups=0.21, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.618, loss_scale=8, train_wall=463, gb_free=10, wall=146675
2022-02-13 10:30:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:32:03 | INFO | train_inner | epoch 020:    962 / 1576 loss=4.936, ppl=30.61, wps=13707.1, ups=0.21, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.628, loss_scale=8, train_wall=467, gb_free=10, wall=147153
2022-02-13 10:39:57 | INFO | train_inner | epoch 020:   1062 / 1576 loss=4.936, ppl=30.62, wps=13843.6, ups=0.21, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.626, loss_scale=8, train_wall=463, gb_free=10, wall=147626
2022-02-13 10:47:50 | INFO | train_inner | epoch 020:   1162 / 1576 loss=4.944, ppl=30.79, wps=13844.4, ups=0.21, wpb=65536, bsz=128, num_updates=31000, lr=0.000179605, gnorm=0.615, loss_scale=8, train_wall=463, gb_free=10, wall=148100
2022-02-13 10:55:43 | INFO | train_inner | epoch 020:   1262 / 1576 loss=4.968, ppl=31.29, wps=13845.9, ups=0.21, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.618, loss_scale=16, train_wall=463, gb_free=10, wall=148573
2022-02-13 10:58:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:03:42 | INFO | train_inner | epoch 020:   1363 / 1576 loss=4.954, ppl=30.99, wps=13705.7, ups=0.21, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.636, loss_scale=8, train_wall=468, gb_free=10, wall=149051
2022-02-13 11:11:35 | INFO | train_inner | epoch 020:   1463 / 1576 loss=4.955, ppl=31.03, wps=13836.8, ups=0.21, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.618, loss_scale=8, train_wall=463, gb_free=10, wall=149525
2022-02-13 11:19:28 | INFO | train_inner | epoch 020:   1563 / 1576 loss=4.959, ppl=31.1, wps=13851.1, ups=0.21, wpb=65536, bsz=128, num_updates=31400, lr=0.000178458, gnorm=0.608, loss_scale=16, train_wall=463, gb_free=10, wall=149998
2022-02-13 11:20:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 11:20:32 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.108 | ppl 34.48 | wps 37116.8 | wpb 1021.8 | bsz 2 | num_updates 31413 | best_loss 5.108
2022-02-13 11:20:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 31413 updates
2022-02-13 11:20:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint20.pt
2022-02-13 11:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint20.pt
2022-02-13 11:21:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint20.pt (epoch 20 @ 31413 updates, score 5.108) (writing took 29.263186931610107 seconds)
2022-02-13 11:21:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-13 11:21:01 | INFO | train | epoch 020 | loss 4.92 | ppl 30.26 | wps 13743.9 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 31413 | lr 0.000178421 | gnorm 0.622 | loss_scale 16 | train_wall 7290 | gb_free 10 | wall 150091
2022-02-13 11:21:01 | INFO | fairseq.trainer | begin training epoch 21
2022-02-13 11:21:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 11:21:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:27:57 | INFO | train_inner | epoch 021:     88 / 1576 loss=4.851, ppl=28.85, wps=12769.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=31500, lr=0.000178174, gnorm=0.653, loss_scale=8, train_wall=463, gb_free=10, wall=150507
2022-02-13 11:35:50 | INFO | train_inner | epoch 021:    188 / 1576 loss=4.86, ppl=29.03, wps=13851, ups=0.21, wpb=65536, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.611, loss_scale=8, train_wall=463, gb_free=10, wall=150980
2022-02-13 11:43:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:43:48 | INFO | train_inner | epoch 021:    289 / 1576 loss=4.87, ppl=29.23, wps=13713.7, ups=0.21, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.626, loss_scale=8, train_wall=467, gb_free=10, wall=151458
2022-02-13 11:51:41 | INFO | train_inner | epoch 021:    389 / 1576 loss=4.881, ppl=29.47, wps=13850.8, ups=0.21, wpb=65536, bsz=128, num_updates=31800, lr=0.000177332, gnorm=0.61, loss_scale=8, train_wall=463, gb_free=10, wall=151931
2022-02-13 11:59:34 | INFO | train_inner | epoch 021:    489 / 1576 loss=4.896, ppl=29.77, wps=13855.1, ups=0.21, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.625, loss_scale=8, train_wall=462, gb_free=10, wall=152404
2022-02-13 12:07:27 | INFO | train_inner | epoch 021:    589 / 1576 loss=4.903, ppl=29.92, wps=13852.5, ups=0.21, wpb=65536, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.631, loss_scale=16, train_wall=463, gb_free=10, wall=152877
2022-02-13 12:15:21 | INFO | train_inner | epoch 021:    689 / 1576 loss=4.891, ppl=29.68, wps=13843.4, ups=0.21, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.621, loss_scale=16, train_wall=463, gb_free=10, wall=153351
2022-02-13 12:23:14 | INFO | train_inner | epoch 021:    789 / 1576 loss=4.897, ppl=29.8, wps=13850.8, ups=0.21, wpb=65536, bsz=128, num_updates=32200, lr=0.000176227, gnorm=0.622, loss_scale=16, train_wall=463, gb_free=10, wall=153824
2022-02-13 12:23:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 12:26:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:31:17 | INFO | train_inner | epoch 021:    891 / 1576 loss=4.91, ppl=30.07, wps=13581.9, ups=0.21, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.606, loss_scale=8, train_wall=472, gb_free=10, wall=154306
2022-02-13 12:39:10 | INFO | train_inner | epoch 021:    991 / 1576 loss=4.921, ppl=30.29, wps=13853.2, ups=0.21, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.615, loss_scale=8, train_wall=463, gb_free=10, wall=154779
2022-02-13 12:47:03 | INFO | train_inner | epoch 021:   1091 / 1576 loss=4.926, ppl=30.39, wps=13854.3, ups=0.21, wpb=65536, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.65, loss_scale=16, train_wall=463, gb_free=10, wall=155252
2022-02-13 12:51:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:55:00 | INFO | train_inner | epoch 021:   1192 / 1576 loss=4.925, ppl=30.38, wps=13715.5, ups=0.21, wpb=65536, bsz=128, num_updates=32600, lr=0.000175142, gnorm=0.63, loss_scale=8, train_wall=467, gb_free=10, wall=155730
2022-02-13 13:02:54 | INFO | train_inner | epoch 021:   1292 / 1576 loss=4.938, ppl=30.66, wps=13852, ups=0.21, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.635, loss_scale=8, train_wall=463, gb_free=10, wall=156203
2022-02-13 13:10:47 | INFO | train_inner | epoch 021:   1392 / 1576 loss=4.962, ppl=31.17, wps=13853.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.64, loss_scale=8, train_wall=463, gb_free=10, wall=156676
2022-02-13 13:18:40 | INFO | train_inner | epoch 021:   1492 / 1576 loss=4.948, ppl=30.86, wps=13852.2, ups=0.21, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.624, loss_scale=16, train_wall=463, gb_free=10, wall=157149
2022-02-13 13:19:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:25:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 13:25:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.1 | ppl 34.3 | wps 37165.4 | wpb 1021.8 | bsz 2 | num_updates 32983 | best_loss 5.1
2022-02-13 13:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 32983 updates
2022-02-13 13:25:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint21.pt
2022-02-13 13:25:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint21.pt
2022-02-13 13:25:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint21.pt (epoch 21 @ 32983 updates, score 5.1) (writing took 28.41929613891989 seconds)
2022-02-13 13:25:47 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-13 13:25:47 | INFO | train | epoch 021 | loss 4.907 | ppl 30.01 | wps 13736.4 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 32983 | lr 0.000174123 | gnorm 0.626 | loss_scale 8 | train_wall 7286 | gb_free 10 | wall 157577
2022-02-13 13:25:47 | INFO | fairseq.trainer | begin training epoch 22
2022-02-13 13:25:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 13:27:08 | INFO | train_inner | epoch 022:     17 / 1576 loss=4.932, ppl=30.52, wps=12792.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=33000, lr=0.000174078, gnorm=0.625, loss_scale=8, train_wall=463, gb_free=10, wall=157657
2022-02-13 13:35:00 | INFO | train_inner | epoch 022:    117 / 1576 loss=4.842, ppl=28.67, wps=13865.4, ups=0.21, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.635, loss_scale=8, train_wall=462, gb_free=10, wall=158130
2022-02-13 13:42:53 | INFO | train_inner | epoch 022:    217 / 1576 loss=4.849, ppl=28.81, wps=13858.9, ups=0.21, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.622, loss_scale=16, train_wall=462, gb_free=10, wall=158603
2022-02-13 13:46:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:50:51 | INFO | train_inner | epoch 022:    318 / 1576 loss=4.847, ppl=28.78, wps=13716.9, ups=0.21, wpb=65536, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.636, loss_scale=8, train_wall=467, gb_free=10, wall=159081
2022-02-13 13:58:44 | INFO | train_inner | epoch 022:    418 / 1576 loss=4.87, ppl=29.25, wps=13859.8, ups=0.21, wpb=65536, bsz=128, num_updates=33400, lr=0.000173032, gnorm=0.62, loss_scale=8, train_wall=462, gb_free=10, wall=159553
2022-02-13 14:06:36 | INFO | train_inner | epoch 022:    518 / 1576 loss=4.875, ppl=29.34, wps=13861.1, ups=0.21, wpb=65536, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.637, loss_scale=16, train_wall=462, gb_free=10, wall=160026
2022-02-13 14:08:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 14:14:34 | INFO | train_inner | epoch 022:    619 / 1576 loss=4.885, ppl=29.54, wps=13720.4, ups=0.21, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.666, loss_scale=8, train_wall=467, gb_free=10, wall=160504
2022-02-13 14:22:27 | INFO | train_inner | epoch 022:    719 / 1576 loss=4.898, ppl=29.81, wps=13859.8, ups=0.21, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.622, loss_scale=8, train_wall=462, gb_free=10, wall=160977
2022-02-13 14:30:20 | INFO | train_inner | epoch 022:    819 / 1576 loss=4.899, ppl=29.83, wps=13856.3, ups=0.21, wpb=65536, bsz=128, num_updates=33800, lr=0.000172005, gnorm=0.649, loss_scale=16, train_wall=462, gb_free=10, wall=161450
2022-02-13 14:32:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 14:38:18 | INFO | train_inner | epoch 022:    920 / 1576 loss=4.909, ppl=30.05, wps=13716.9, ups=0.21, wpb=65536, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.629, loss_scale=8, train_wall=467, gb_free=10, wall=161927
2022-02-13 14:46:11 | INFO | train_inner | epoch 022:   1020 / 1576 loss=4.917, ppl=30.2, wps=13857.6, ups=0.21, wpb=65536, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.626, loss_scale=8, train_wall=462, gb_free=10, wall=162400
2022-02-13 14:54:04 | INFO | train_inner | epoch 022:   1120 / 1576 loss=4.919, ppl=30.25, wps=13859.4, ups=0.21, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=0.625, loss_scale=16, train_wall=462, gb_free=10, wall=162873
2022-02-13 14:58:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:02:01 | INFO | train_inner | epoch 022:   1221 / 1576 loss=4.922, ppl=30.32, wps=13713.8, ups=0.21, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=0.643, loss_scale=8, train_wall=467, gb_free=10, wall=163351
2022-02-13 15:09:54 | INFO | train_inner | epoch 022:   1321 / 1576 loss=4.927, ppl=30.41, wps=13860.8, ups=0.21, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.635, loss_scale=8, train_wall=462, gb_free=10, wall=163824
2022-02-13 15:17:47 | INFO | train_inner | epoch 022:   1421 / 1576 loss=4.941, ppl=30.71, wps=13849.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.631, loss_scale=8, train_wall=463, gb_free=10, wall=164297
2022-02-13 15:25:40 | INFO | train_inner | epoch 022:   1521 / 1576 loss=4.94, ppl=30.69, wps=13855.2, ups=0.21, wpb=65536, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.648, loss_scale=16, train_wall=462, gb_free=10, wall=164770
2022-02-13 15:29:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 15:30:02 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.103 | ppl 34.36 | wps 37179.3 | wpb 1021.8 | bsz 2 | num_updates 34555 | best_loss 5.1
2022-02-13 15:30:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 34555 updates
2022-02-13 15:30:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint22.pt
2022-02-13 15:30:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint22.pt
2022-02-13 15:30:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint22.pt (epoch 22 @ 34555 updates, score 5.103) (writing took 19.48479754384607 seconds)
2022-02-13 15:30:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-13 15:30:22 | INFO | train | epoch 022 | loss 4.896 | ppl 29.78 | wps 13775.1 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 34555 | lr 0.000170116 | gnorm 0.635 | loss_scale 16 | train_wall 7283 | gb_free 10 | wall 165052
2022-02-13 15:30:22 | INFO | fairseq.trainer | begin training epoch 23
2022-02-13 15:30:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 15:30:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:33:59 | INFO | train_inner | epoch 023:     46 / 1576 loss=4.885, ppl=29.56, wps=13021.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=34600, lr=0.000170005, gnorm=0.626, loss_scale=8, train_wall=463, gb_free=10, wall=165269
2022-02-13 15:41:52 | INFO | train_inner | epoch 023:    146 / 1576 loss=4.824, ppl=28.33, wps=13849.2, ups=0.21, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.62, loss_scale=8, train_wall=463, gb_free=10, wall=165742
2022-02-13 15:49:46 | INFO | train_inner | epoch 023:    246 / 1576 loss=4.837, ppl=28.59, wps=13850.8, ups=0.21, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.641, loss_scale=8, train_wall=463, gb_free=10, wall=166215
2022-02-13 15:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:57:44 | INFO | train_inner | epoch 023:    347 / 1576 loss=4.844, ppl=28.71, wps=13711.2, ups=0.21, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.63, loss_scale=8, train_wall=467, gb_free=10, wall=166693
2022-02-13 16:05:37 | INFO | train_inner | epoch 023:    447 / 1576 loss=4.85, ppl=28.83, wps=13853.5, ups=0.21, wpb=65536, bsz=128, num_updates=35000, lr=0.000169031, gnorm=0.616, loss_scale=8, train_wall=463, gb_free=10, wall=167166
2022-02-13 16:13:30 | INFO | train_inner | epoch 023:    547 / 1576 loss=4.866, ppl=29.16, wps=13855.3, ups=0.21, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.618, loss_scale=8, train_wall=463, gb_free=10, wall=167639
2022-02-13 16:19:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:21:28 | INFO | train_inner | epoch 023:    648 / 1576 loss=4.887, ppl=29.59, wps=13707.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.647, loss_scale=8, train_wall=467, gb_free=10, wall=168117
2022-02-13 16:29:21 | INFO | train_inner | epoch 023:    748 / 1576 loss=4.877, ppl=29.38, wps=13857.5, ups=0.21, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.618, loss_scale=8, train_wall=462, gb_free=10, wall=168590
2022-02-13 16:37:14 | INFO | train_inner | epoch 023:    848 / 1576 loss=4.895, ppl=29.76, wps=13855.5, ups=0.21, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=0.669, loss_scale=8, train_wall=462, gb_free=10, wall=169063
2022-02-13 16:40:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:45:11 | INFO | train_inner | epoch 023:    949 / 1576 loss=4.891, ppl=29.66, wps=13725.6, ups=0.21, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.641, loss_scale=8, train_wall=467, gb_free=10, wall=169541
2022-02-13 16:53:04 | INFO | train_inner | epoch 023:   1049 / 1576 loss=4.912, ppl=30.11, wps=13855.8, ups=0.21, wpb=65536, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.627, loss_scale=8, train_wall=463, gb_free=10, wall=170014
2022-02-13 17:00:57 | INFO | train_inner | epoch 023:   1149 / 1576 loss=4.916, ppl=30.18, wps=13850.3, ups=0.21, wpb=65536, bsz=128, num_updates=35700, lr=0.000167365, gnorm=0.617, loss_scale=16, train_wall=463, gb_free=10, wall=170487
2022-02-13 17:02:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:08:55 | INFO | train_inner | epoch 023:   1250 / 1576 loss=4.916, ppl=30.2, wps=13713.5, ups=0.21, wpb=65536, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.661, loss_scale=8, train_wall=467, gb_free=10, wall=170965
2022-02-13 17:16:48 | INFO | train_inner | epoch 023:   1350 / 1576 loss=4.924, ppl=30.37, wps=13854.1, ups=0.21, wpb=65536, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.64, loss_scale=8, train_wall=463, gb_free=10, wall=171438
2022-02-13 17:24:42 | INFO | train_inner | epoch 023:   1450 / 1576 loss=4.927, ppl=30.43, wps=13846.3, ups=0.21, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.68, loss_scale=16, train_wall=463, gb_free=10, wall=171911
2022-02-13 17:31:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:32:39 | INFO | train_inner | epoch 023:   1551 / 1576 loss=4.937, ppl=30.64, wps=13715.4, ups=0.21, wpb=65536, bsz=128, num_updates=36100, lr=0.000166436, gnorm=0.641, loss_scale=8, train_wall=467, gb_free=10, wall=172389
2022-02-13 17:34:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 17:34:39 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.097 | ppl 34.24 | wps 37363.7 | wpb 1021.8 | bsz 2 | num_updates 36125 | best_loss 5.097
2022-02-13 17:34:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 36125 updates
2022-02-13 17:34:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint23.pt
2022-02-13 17:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint23.pt
2022-02-13 17:35:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint23.pt (epoch 23 @ 36125 updates, score 5.097) (writing took 29.131446290761232 seconds)
2022-02-13 17:35:09 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-13 17:35:09 | INFO | train | epoch 023 | loss 4.886 | ppl 29.57 | wps 13735.5 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 36125 | lr 0.000166378 | gnorm 0.638 | loss_scale 8 | train_wall 7286 | gb_free 10 | wall 172538
2022-02-13 17:35:09 | INFO | fairseq.trainer | begin training epoch 24
2022-02-13 17:35:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 17:41:03 | INFO | train_inner | epoch 024:     75 / 1576 loss=4.828, ppl=28.41, wps=12900.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=36200, lr=0.000166206, gnorm=0.633, loss_scale=8, train_wall=458, gb_free=10, wall=172893
2022-02-13 17:48:56 | INFO | train_inner | epoch 024:    175 / 1576 loss=4.817, ppl=28.18, wps=13860.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.646, loss_scale=8, train_wall=462, gb_free=10, wall=173365
2022-02-13 17:56:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:56:54 | INFO | train_inner | epoch 024:    276 / 1576 loss=4.828, ppl=28.4, wps=13708, ups=0.21, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.643, loss_scale=8, train_wall=467, gb_free=10, wall=173844
2022-02-13 18:04:47 | INFO | train_inner | epoch 024:    376 / 1576 loss=4.853, ppl=28.89, wps=13849.9, ups=0.21, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=0.666, loss_scale=8, train_wall=463, gb_free=10, wall=174317
2022-02-13 18:12:40 | INFO | train_inner | epoch 024:    476 / 1576 loss=4.853, ppl=28.9, wps=13849, ups=0.21, wpb=65536, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.627, loss_scale=8, train_wall=463, gb_free=10, wall=174790
2022-02-13 18:20:33 | INFO | train_inner | epoch 024:    576 / 1576 loss=4.868, ppl=29.21, wps=13854.1, ups=0.21, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.636, loss_scale=16, train_wall=463, gb_free=10, wall=175263
2022-02-13 18:26:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:28:31 | INFO | train_inner | epoch 024:    677 / 1576 loss=4.876, ppl=29.37, wps=13714.4, ups=0.21, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.639, loss_scale=8, train_wall=467, gb_free=10, wall=175741
2022-02-13 18:36:24 | INFO | train_inner | epoch 024:    777 / 1576 loss=4.872, ppl=29.28, wps=13854.7, ups=0.21, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=0.629, loss_scale=8, train_wall=463, gb_free=10, wall=176214
2022-02-13 18:44:17 | INFO | train_inner | epoch 024:    877 / 1576 loss=4.883, ppl=29.52, wps=13853.5, ups=0.21, wpb=65536, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.64, loss_scale=8, train_wall=463, gb_free=10, wall=176687
2022-02-13 18:48:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:52:15 | INFO | train_inner | epoch 024:    978 / 1576 loss=4.888, ppl=29.62, wps=13711.7, ups=0.21, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.665, loss_scale=8, train_wall=467, gb_free=10, wall=177165
2022-02-13 19:00:08 | INFO | train_inner | epoch 024:   1078 / 1576 loss=4.9, ppl=29.86, wps=13848.8, ups=0.21, wpb=65536, bsz=128, num_updates=37200, lr=0.000163956, gnorm=0.627, loss_scale=8, train_wall=463, gb_free=10, wall=177638
2022-02-13 19:08:01 | INFO | train_inner | epoch 024:   1178 / 1576 loss=4.901, ppl=29.87, wps=13853.1, ups=0.21, wpb=65536, bsz=128, num_updates=37300, lr=0.000163737, gnorm=0.659, loss_scale=8, train_wall=463, gb_free=10, wall=178111
2022-02-13 19:13:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:15:59 | INFO | train_inner | epoch 024:   1279 / 1576 loss=4.909, ppl=30.05, wps=13715.6, ups=0.21, wpb=65536, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.616, loss_scale=8, train_wall=467, gb_free=10, wall=178589
2022-02-13 19:23:53 | INFO | train_inner | epoch 024:   1379 / 1576 loss=4.904, ppl=29.94, wps=13845, ups=0.21, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.654, loss_scale=8, train_wall=463, gb_free=10, wall=179062
2022-02-13 19:31:46 | INFO | train_inner | epoch 024:   1479 / 1576 loss=4.914, ppl=30.16, wps=13856.5, ups=0.21, wpb=65536, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.611, loss_scale=8, train_wall=462, gb_free=10, wall=179535
2022-02-13 19:36:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:39:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 19:39:26 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.091 | ppl 34.08 | wps 37177 | wpb 1021.8 | bsz 2 | num_updates 37696 | best_loss 5.091
2022-02-13 19:39:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 37696 updates
2022-02-13 19:39:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint24.pt
2022-02-13 19:39:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint24.pt
2022-02-13 19:39:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint24.pt (epoch 24 @ 37696 updates, score 5.091) (writing took 29.051049417816103 seconds)
2022-02-13 19:39:56 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-13 19:39:56 | INFO | train | epoch 024 | loss 4.876 | ppl 29.36 | wps 13744 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 37696 | lr 0.000162874 | gnorm 0.64 | loss_scale 8 | train_wall 7286 | gb_free 10 | wall 180025
2022-02-13 19:39:56 | INFO | fairseq.trainer | begin training epoch 25
2022-02-13 19:39:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 19:40:15 | INFO | train_inner | epoch 025:      4 / 1576 loss=4.93, ppl=30.48, wps=12765.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=37700, lr=0.000162866, gnorm=0.652, loss_scale=8, train_wall=463, gb_free=10, wall=180044
2022-02-13 19:48:07 | INFO | train_inner | epoch 025:    104 / 1576 loss=4.805, ppl=27.96, wps=13866.4, ups=0.21, wpb=65536, bsz=128, num_updates=37800, lr=0.00016265, gnorm=0.637, loss_scale=8, train_wall=462, gb_free=10, wall=180517
2022-02-13 19:56:00 | INFO | train_inner | epoch 025:    204 / 1576 loss=4.815, ppl=28.15, wps=13846.1, ups=0.21, wpb=65536, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.634, loss_scale=8, train_wall=463, gb_free=10, wall=180990
2022-02-13 20:00:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:03:58 | INFO | train_inner | epoch 025:    305 / 1576 loss=4.823, ppl=28.31, wps=13715.3, ups=0.21, wpb=65536, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.653, loss_scale=8, train_wall=467, gb_free=10, wall=181468
2022-02-13 20:11:52 | INFO | train_inner | epoch 025:    405 / 1576 loss=4.834, ppl=28.52, wps=13846, ups=0.21, wpb=65536, bsz=128, num_updates=38100, lr=0.000162008, gnorm=0.623, loss_scale=8, train_wall=463, gb_free=10, wall=181941
2022-02-13 20:19:45 | INFO | train_inner | epoch 025:    505 / 1576 loss=4.849, ppl=28.83, wps=13847.4, ups=0.21, wpb=65536, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.65, loss_scale=8, train_wall=463, gb_free=10, wall=182415
2022-02-13 20:26:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:27:43 | INFO | train_inner | epoch 025:    606 / 1576 loss=4.863, ppl=29.09, wps=13716.4, ups=0.21, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.633, loss_scale=8, train_wall=467, gb_free=10, wall=182892
2022-02-13 20:35:36 | INFO | train_inner | epoch 025:    706 / 1576 loss=4.854, ppl=28.93, wps=13850.6, ups=0.21, wpb=65536, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.67, loss_scale=8, train_wall=463, gb_free=10, wall=183366
2022-02-13 20:43:29 | INFO | train_inner | epoch 025:    806 / 1576 loss=4.873, ppl=29.31, wps=13854.8, ups=0.21, wpb=65536, bsz=128, num_updates=38500, lr=0.000161165, gnorm=0.623, loss_scale=8, train_wall=463, gb_free=10, wall=183839
2022-02-13 20:46:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:51:27 | INFO | train_inner | epoch 025:    907 / 1576 loss=4.883, ppl=29.51, wps=13714.4, ups=0.21, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.657, loss_scale=8, train_wall=467, gb_free=10, wall=184316
2022-02-13 20:59:20 | INFO | train_inner | epoch 025:   1007 / 1576 loss=4.885, ppl=29.54, wps=13845.3, ups=0.21, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.648, loss_scale=8, train_wall=463, gb_free=10, wall=184790
2022-02-13 21:07:13 | INFO | train_inner | epoch 025:   1107 / 1576 loss=4.891, ppl=29.67, wps=13847.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=38800, lr=0.00016054, gnorm=0.619, loss_scale=16, train_wall=463, gb_free=10, wall=185263
2022-02-13 21:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:15:11 | INFO | train_inner | epoch 025:   1208 / 1576 loss=4.882, ppl=29.48, wps=13714.6, ups=0.21, wpb=65536, bsz=128, num_updates=38900, lr=0.000160334, gnorm=0.666, loss_scale=8, train_wall=467, gb_free=10, wall=185741
2022-02-13 21:23:04 | INFO | train_inner | epoch 025:   1308 / 1576 loss=4.902, ppl=29.9, wps=13854.3, ups=0.21, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.633, loss_scale=8, train_wall=463, gb_free=10, wall=186214
2022-02-13 21:30:58 | INFO | train_inner | epoch 025:   1408 / 1576 loss=4.902, ppl=29.9, wps=13843.2, ups=0.21, wpb=65536, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.636, loss_scale=16, train_wall=463, gb_free=10, wall=186687
2022-02-13 21:32:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:38:56 | INFO | train_inner | epoch 025:   1509 / 1576 loss=4.916, ppl=30.19, wps=13709.1, ups=0.21, wpb=65536, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.652, loss_scale=8, train_wall=467, gb_free=10, wall=187165
2022-02-13 21:44:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 21:44:14 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.085 | ppl 33.95 | wps 37177.4 | wpb 1021.8 | bsz 2 | num_updates 39267 | best_loss 5.085
2022-02-13 21:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 39267 updates
2022-02-13 21:44:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint25.pt
2022-02-13 21:44:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint25.pt
2022-02-13 21:44:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint25.pt (epoch 25 @ 39267 updates, score 5.085) (writing took 29.03567955084145 seconds)
2022-02-13 21:44:44 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-13 21:44:44 | INFO | train | epoch 025 | loss 4.867 | ppl 29.19 | wps 13741.9 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 39267 | lr 0.000159583 | gnorm 0.643 | loss_scale 8 | train_wall 7287 | gb_free 10 | wall 187513
2022-02-13 21:44:44 | INFO | fairseq.trainer | begin training epoch 26
2022-02-13 21:44:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 21:47:20 | INFO | train_inner | epoch 026:     33 / 1576 loss=4.876, ppl=29.36, wps=12891.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=39300, lr=0.000159516, gnorm=0.658, loss_scale=8, train_wall=458, gb_free=10, wall=187669
2022-02-13 21:54:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:55:18 | INFO | train_inner | epoch 026:    134 / 1576 loss=4.803, ppl=27.91, wps=13708.5, ups=0.21, wpb=65536, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.639, loss_scale=8, train_wall=467, gb_free=10, wall=188147
2022-02-13 22:03:11 | INFO | train_inner | epoch 026:    234 / 1576 loss=4.818, ppl=28.21, wps=13841.9, ups=0.21, wpb=65536, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.653, loss_scale=8, train_wall=463, gb_free=10, wall=188621
2022-02-13 22:11:04 | INFO | train_inner | epoch 026:    334 / 1576 loss=4.819, ppl=28.22, wps=13845.2, ups=0.21, wpb=65536, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.67, loss_scale=8, train_wall=463, gb_free=10, wall=189094
2022-02-13 22:14:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:19:03 | INFO | train_inner | epoch 026:    435 / 1576 loss=4.835, ppl=28.54, wps=13707.3, ups=0.21, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=0.634, loss_scale=8, train_wall=467, gb_free=10, wall=189572
2022-02-13 22:26:56 | INFO | train_inner | epoch 026:    535 / 1576 loss=4.831, ppl=28.46, wps=13839.9, ups=0.21, wpb=65536, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.642, loss_scale=8, train_wall=463, gb_free=10, wall=190046
2022-02-13 22:34:49 | INFO | train_inner | epoch 026:    635 / 1576 loss=4.865, ppl=29.15, wps=13846.6, ups=0.21, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.629, loss_scale=16, train_wall=463, gb_free=10, wall=190519
2022-02-13 22:35:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:42:48 | INFO | train_inner | epoch 026:    736 / 1576 loss=4.86, ppl=29.05, wps=13705.9, ups=0.21, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.664, loss_scale=8, train_wall=468, gb_free=10, wall=190997
2022-02-13 22:50:41 | INFO | train_inner | epoch 026:    836 / 1576 loss=4.863, ppl=29.1, wps=13848.1, ups=0.21, wpb=65536, bsz=128, num_updates=40100, lr=0.000157917, gnorm=0.63, loss_scale=8, train_wall=463, gb_free=10, wall=191471
2022-02-13 22:56:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:58:39 | INFO | train_inner | epoch 026:    937 / 1576 loss=4.861, ppl=29.06, wps=13709.6, ups=0.21, wpb=65536, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.651, loss_scale=8, train_wall=467, gb_free=10, wall=191949
2022-02-13 23:06:32 | INFO | train_inner | epoch 026:   1037 / 1576 loss=4.875, ppl=29.35, wps=13842.7, ups=0.21, wpb=65536, bsz=128, num_updates=40300, lr=0.000157524, gnorm=0.695, loss_scale=8, train_wall=463, gb_free=10, wall=192422
2022-02-13 23:14:26 | INFO | train_inner | epoch 026:   1137 / 1576 loss=4.867, ppl=29.19, wps=13845.4, ups=0.21, wpb=65536, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.637, loss_scale=8, train_wall=463, gb_free=10, wall=192895
2022-02-13 23:18:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 23:22:24 | INFO | train_inner | epoch 026:   1238 / 1576 loss=4.894, ppl=29.73, wps=13707, ups=0.21, wpb=65536, bsz=128, num_updates=40500, lr=0.000157135, gnorm=0.638, loss_scale=8, train_wall=468, gb_free=10, wall=193373
2022-02-13 23:30:17 | INFO | train_inner | epoch 026:   1338 / 1576 loss=4.897, ppl=29.79, wps=13840.7, ups=0.21, wpb=65532.3, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.649, loss_scale=8, train_wall=463, gb_free=10, wall=193847
2022-02-13 23:38:11 | INFO | train_inner | epoch 026:   1438 / 1576 loss=4.898, ppl=29.81, wps=13835.4, ups=0.21, wpb=65536, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.628, loss_scale=8, train_wall=463, gb_free=10, wall=194321
2022-02-13 23:38:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 23:46:09 | INFO | train_inner | epoch 026:   1539 / 1576 loss=4.902, ppl=29.91, wps=13703.6, ups=0.21, wpb=65536, bsz=128, num_updates=40800, lr=0.000156556, gnorm=0.64, loss_scale=8, train_wall=468, gb_free=10, wall=194799
2022-02-13 23:49:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 23:49:06 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.082 | ppl 33.88 | wps 37069.7 | wpb 1021.8 | bsz 2 | num_updates 40837 | best_loss 5.082
2022-02-13 23:49:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 40837 updates
2022-02-13 23:49:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint26.pt
2022-02-13 23:49:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint26.pt
2022-02-13 23:49:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint26.pt (epoch 26 @ 40837 updates, score 5.082) (writing took 29.0990915261209 seconds)
2022-02-13 23:49:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-13 23:49:35 | INFO | train | epoch 026 | loss 4.859 | ppl 29.01 | wps 13726.2 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 40837 | lr 0.000156485 | gnorm 0.647 | loss_scale 8 | train_wall 7291 | gb_free 10 | wall 195005
2022-02-13 23:49:35 | INFO | fairseq.trainer | begin training epoch 27
2022-02-13 23:49:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 23:54:33 | INFO | train_inner | epoch 027:     63 / 1576 loss=4.83, ppl=28.44, wps=12886.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=40900, lr=0.000156365, gnorm=0.671, loss_scale=8, train_wall=458, gb_free=10, wall=195303
2022-02-14 00:00:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:02:31 | INFO | train_inner | epoch 027:    164 / 1576 loss=4.793, ppl=27.73, wps=13705.4, ups=0.21, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.635, loss_scale=8, train_wall=467, gb_free=10, wall=195781
2022-02-14 00:10:25 | INFO | train_inner | epoch 027:    264 / 1576 loss=4.817, ppl=28.18, wps=13841.7, ups=0.21, wpb=65536, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.652, loss_scale=8, train_wall=463, gb_free=10, wall=196255
2022-02-14 00:18:18 | INFO | train_inner | epoch 027:    364 / 1576 loss=4.803, ppl=27.91, wps=13843.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=41200, lr=0.000155794, gnorm=0.658, loss_scale=8, train_wall=463, gb_free=10, wall=196728
2022-02-14 00:20:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:26:16 | INFO | train_inner | epoch 027:    465 / 1576 loss=4.833, ppl=28.5, wps=13709.1, ups=0.21, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.641, loss_scale=8, train_wall=467, gb_free=10, wall=197206
2022-02-14 00:34:10 | INFO | train_inner | epoch 027:    565 / 1576 loss=4.838, ppl=28.6, wps=13846, ups=0.21, wpb=65536, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.637, loss_scale=8, train_wall=463, gb_free=10, wall=197679
2022-02-14 00:42:03 | INFO | train_inner | epoch 027:    665 / 1576 loss=4.845, ppl=28.73, wps=13844.2, ups=0.21, wpb=65536, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.668, loss_scale=16, train_wall=463, gb_free=10, wall=198153
2022-02-14 00:42:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:50:01 | INFO | train_inner | epoch 027:    766 / 1576 loss=4.865, ppl=29.14, wps=13705.7, ups=0.21, wpb=65536, bsz=128, num_updates=41600, lr=0.000155043, gnorm=0.651, loss_scale=8, train_wall=468, gb_free=10, wall=198631
2022-02-14 00:57:55 | INFO | train_inner | epoch 027:    866 / 1576 loss=4.85, ppl=28.83, wps=13842.8, ups=0.21, wpb=65536, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.64, loss_scale=8, train_wall=463, gb_free=10, wall=199104
2022-02-14 01:02:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:05:53 | INFO | train_inner | epoch 027:    967 / 1576 loss=4.858, ppl=29, wps=13706.7, ups=0.21, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.654, loss_scale=8, train_wall=468, gb_free=10, wall=199582
2022-02-14 01:13:46 | INFO | train_inner | epoch 027:   1067 / 1576 loss=4.864, ppl=29.11, wps=13841.2, ups=0.21, wpb=65536, bsz=128, num_updates=41900, lr=0.000154487, gnorm=0.663, loss_scale=8, train_wall=463, gb_free=10, wall=200056
2022-02-14 01:21:40 | INFO | train_inner | epoch 027:   1167 / 1576 loss=4.872, ppl=29.27, wps=13847, ups=0.21, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=0.672, loss_scale=8, train_wall=463, gb_free=10, wall=200529
2022-02-14 01:23:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:29:38 | INFO | train_inner | epoch 027:   1268 / 1576 loss=4.88, ppl=29.45, wps=13708.1, ups=0.21, wpb=65536, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.641, loss_scale=8, train_wall=467, gb_free=10, wall=201007
2022-02-14 01:37:31 | INFO | train_inner | epoch 027:   1368 / 1576 loss=4.889, ppl=29.64, wps=13837.6, ups=0.21, wpb=65536, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.659, loss_scale=8, train_wall=463, gb_free=10, wall=201481
2022-02-14 01:43:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:45:29 | INFO | train_inner | epoch 027:   1469 / 1576 loss=4.898, ppl=29.82, wps=13706.8, ups=0.21, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.644, loss_scale=8, train_wall=467, gb_free=10, wall=201959
2022-02-14 01:53:23 | INFO | train_inner | epoch 027:   1569 / 1576 loss=4.895, ppl=29.75, wps=13844.1, ups=0.21, wpb=65536, bsz=128, num_updates=42400, lr=0.000153574, gnorm=0.653, loss_scale=8, train_wall=463, gb_free=10, wall=202432
2022-02-14 01:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 01:53:58 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.078 | ppl 33.77 | wps 37265.5 | wpb 1021.8 | bsz 2 | num_updates 42407 | best_loss 5.078
2022-02-14 01:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 42407 updates
2022-02-14 01:53:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint27.pt
2022-02-14 01:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint27.pt
2022-02-14 01:54:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint27.pt (epoch 27 @ 42407 updates, score 5.078) (writing took 29.09471952728927 seconds)
2022-02-14 01:54:27 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-14 01:54:27 | INFO | train | epoch 027 | loss 4.851 | ppl 28.86 | wps 13726.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 42407 | lr 0.000153561 | gnorm 0.653 | loss_scale 8 | train_wall 7290 | gb_free 10 | wall 202497
2022-02-14 01:54:27 | INFO | fairseq.trainer | begin training epoch 28
2022-02-14 01:54:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 02:01:47 | INFO | train_inner | epoch 028:     93 / 1576 loss=4.788, ppl=27.62, wps=12883.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=42500, lr=0.000153393, gnorm=0.713, loss_scale=8, train_wall=459, gb_free=10, wall=202937
2022-02-14 02:05:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:09:45 | INFO | train_inner | epoch 028:    194 / 1576 loss=4.792, ppl=27.71, wps=13705.5, ups=0.21, wpb=65536, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.63, loss_scale=8, train_wall=467, gb_free=10, wall=203415
2022-02-14 02:17:39 | INFO | train_inner | epoch 028:    294 / 1576 loss=4.816, ppl=28.17, wps=13842, ups=0.21, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.681, loss_scale=8, train_wall=463, gb_free=10, wall=203888
2022-02-14 02:25:32 | INFO | train_inner | epoch 028:    394 / 1576 loss=4.817, ppl=28.19, wps=13845.3, ups=0.21, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=0.635, loss_scale=8, train_wall=463, gb_free=10, wall=204362
2022-02-14 02:26:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:33:30 | INFO | train_inner | epoch 028:    495 / 1576 loss=4.818, ppl=28.21, wps=13706.3, ups=0.21, wpb=65536, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.645, loss_scale=8, train_wall=467, gb_free=10, wall=204840
2022-02-14 02:41:24 | INFO | train_inner | epoch 028:    595 / 1576 loss=4.819, ppl=28.23, wps=13843.1, ups=0.21, wpb=65536, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.658, loss_scale=8, train_wall=463, gb_free=10, wall=205313
2022-02-14 02:48:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:49:21 | INFO | train_inner | epoch 028:    696 / 1576 loss=4.84, ppl=28.65, wps=13711.1, ups=0.21, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.64, loss_scale=8, train_wall=467, gb_free=10, wall=205791
2022-02-14 02:57:15 | INFO | train_inner | epoch 028:    796 / 1576 loss=4.843, ppl=28.7, wps=13850, ups=0.21, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=0.666, loss_scale=8, train_wall=463, gb_free=10, wall=206264
2022-02-14 03:05:08 | INFO | train_inner | epoch 028:    896 / 1576 loss=4.85, ppl=28.84, wps=13849.1, ups=0.21, wpb=65536, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.652, loss_scale=8, train_wall=463, gb_free=10, wall=206738
2022-02-14 03:08:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:13:06 | INFO | train_inner | epoch 028:    997 / 1576 loss=4.869, ppl=29.22, wps=13709.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=43400, lr=0.000151794, gnorm=0.652, loss_scale=8, train_wall=467, gb_free=10, wall=207216
2022-02-14 03:20:59 | INFO | train_inner | epoch 028:   1097 / 1576 loss=4.862, ppl=29.09, wps=13843.8, ups=0.21, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.66, loss_scale=8, train_wall=463, gb_free=10, wall=207689
2022-02-14 03:28:53 | INFO | train_inner | epoch 028:   1197 / 1576 loss=4.859, ppl=29.01, wps=13849, ups=0.21, wpb=65536, bsz=128, num_updates=43600, lr=0.000151446, gnorm=0.652, loss_scale=8, train_wall=463, gb_free=10, wall=208162
2022-02-14 03:29:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:36:50 | INFO | train_inner | epoch 028:   1298 / 1576 loss=4.874, ppl=29.33, wps=13710.9, ups=0.21, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.657, loss_scale=8, train_wall=467, gb_free=10, wall=208640
2022-02-14 03:44:44 | INFO | train_inner | epoch 028:   1398 / 1576 loss=4.883, ppl=29.51, wps=13836.3, ups=0.21, wpb=65536, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.647, loss_scale=8, train_wall=463, gb_free=10, wall=209114
2022-02-14 03:50:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:52:42 | INFO | train_inner | epoch 028:   1499 / 1576 loss=4.876, ppl=29.36, wps=13713.9, ups=0.21, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.655, loss_scale=8, train_wall=467, gb_free=10, wall=209592
2022-02-14 03:58:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 03:58:48 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.073 | ppl 33.67 | wps 37048.7 | wpb 1021.8 | bsz 2 | num_updates 43977 | best_loss 5.073
2022-02-14 03:58:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 43977 updates
2022-02-14 03:58:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint28.pt
2022-02-14 03:58:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint28.pt
2022-02-14 03:59:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint28.pt (epoch 28 @ 43977 updates, score 5.073) (writing took 29.111204091459513 seconds)
2022-02-14 03:59:17 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-14 03:59:17 | INFO | train | epoch 028 | loss 4.843 | ppl 28.7 | wps 13728.4 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 43977 | lr 0.000150795 | gnorm 0.657 | loss_scale 8 | train_wall 7289 | gb_free 10 | wall 209987
2022-02-14 03:59:17 | INFO | fairseq.trainer | begin training epoch 29
2022-02-14 03:59:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 04:01:06 | INFO | train_inner | epoch 029:     23 / 1576 loss=4.868, ppl=29.2, wps=12887.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=44000, lr=0.000150756, gnorm=0.673, loss_scale=8, train_wall=458, gb_free=10, wall=210096
2022-02-14 04:08:59 | INFO | train_inner | epoch 029:    123 / 1576 loss=4.774, ppl=27.36, wps=13846, ups=0.21, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.657, loss_scale=8, train_wall=463, gb_free=10, wall=210569
2022-02-14 04:11:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 04:16:57 | INFO | train_inner | epoch 029:    224 / 1576 loss=4.784, ppl=27.56, wps=13710.5, ups=0.21, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.644, loss_scale=8, train_wall=467, gb_free=10, wall=211047
2022-02-14 04:24:51 | INFO | train_inner | epoch 029:    324 / 1576 loss=4.806, ppl=27.98, wps=13845.4, ups=0.21, wpb=65536, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.656, loss_scale=8, train_wall=463, gb_free=10, wall=211520
2022-02-14 04:32:44 | INFO | train_inner | epoch 029:    424 / 1576 loss=4.81, ppl=28.06, wps=13838.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=44400, lr=0.000150075, gnorm=0.65, loss_scale=16, train_wall=463, gb_free=10, wall=211994
2022-02-14 04:34:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 04:40:43 | INFO | train_inner | epoch 029:    525 / 1576 loss=4.811, ppl=28.06, wps=13700.4, ups=0.21, wpb=65536, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.669, loss_scale=8, train_wall=468, gb_free=10, wall=212472
2022-02-14 04:48:36 | INFO | train_inner | epoch 029:    625 / 1576 loss=4.825, ppl=28.35, wps=13850.2, ups=0.21, wpb=65536, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.658, loss_scale=8, train_wall=463, gb_free=10, wall=212946
2022-02-14 04:54:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 04:56:34 | INFO | train_inner | epoch 029:    726 / 1576 loss=4.843, ppl=28.69, wps=13705.5, ups=0.21, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.669, loss_scale=8, train_wall=467, gb_free=10, wall=213424
2022-02-14 05:04:27 | INFO | train_inner | epoch 029:    826 / 1576 loss=4.846, ppl=28.75, wps=13846.4, ups=0.21, wpb=65536, bsz=128, num_updates=44800, lr=0.000149404, gnorm=0.671, loss_scale=8, train_wall=463, gb_free=10, wall=213897
2022-02-14 05:12:20 | INFO | train_inner | epoch 029:    926 / 1576 loss=4.839, ppl=28.62, wps=13852.2, ups=0.21, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.673, loss_scale=8, train_wall=463, gb_free=10, wall=214370
2022-02-14 05:15:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:20:18 | INFO | train_inner | epoch 029:   1027 / 1576 loss=4.848, ppl=28.8, wps=13710.4, ups=0.21, wpb=65536, bsz=128, num_updates=45000, lr=0.000149071, gnorm=0.652, loss_scale=8, train_wall=467, gb_free=10, wall=214848
2022-02-14 05:28:12 | INFO | train_inner | epoch 029:   1127 / 1576 loss=4.864, ppl=29.12, wps=13846.3, ups=0.21, wpb=65536, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.665, loss_scale=8, train_wall=463, gb_free=10, wall=215321
2022-02-14 05:36:05 | INFO | train_inner | epoch 029:   1227 / 1576 loss=4.869, ppl=29.22, wps=13848.1, ups=0.21, wpb=65536, bsz=128, num_updates=45200, lr=0.000148741, gnorm=0.662, loss_scale=16, train_wall=463, gb_free=10, wall=215795
2022-02-14 05:36:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:44:03 | INFO | train_inner | epoch 029:   1328 / 1576 loss=4.856, ppl=28.95, wps=13710.3, ups=0.21, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.669, loss_scale=8, train_wall=467, gb_free=10, wall=216273
2022-02-14 05:51:56 | INFO | train_inner | epoch 029:   1428 / 1576 loss=4.873, ppl=29.31, wps=13846.2, ups=0.21, wpb=65536, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.702, loss_scale=8, train_wall=463, gb_free=10, wall=216746
2022-02-14 05:58:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:59:54 | INFO | train_inner | epoch 029:   1529 / 1576 loss=4.885, ppl=29.55, wps=13710.8, ups=0.21, wpb=65536, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.662, loss_scale=8, train_wall=467, gb_free=10, wall=217224
2022-02-14 06:03:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 06:03:38 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.072 | ppl 33.64 | wps 37182.1 | wpb 1021.8 | bsz 2 | num_updates 45547 | best_loss 5.072
2022-02-14 06:03:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 45547 updates
2022-02-14 06:03:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint29.pt
2022-02-14 06:03:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint29.pt
2022-02-14 06:04:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint29.pt (epoch 29 @ 45547 updates, score 5.072) (writing took 29.071738292463124 seconds)
2022-02-14 06:04:08 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-14 06:04:08 | INFO | train | epoch 029 | loss 4.836 | ppl 28.57 | wps 13729.1 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 45547 | lr 0.000148173 | gnorm 0.664 | loss_scale 8 | train_wall 7289 | gb_free 10 | wall 217477
2022-02-14 06:04:08 | INFO | fairseq.trainer | begin training epoch 30
2022-02-14 06:04:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 06:08:18 | INFO | train_inner | epoch 030:     53 / 1576 loss=4.815, ppl=28.15, wps=12891.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=45600, lr=0.000148087, gnorm=0.655, loss_scale=8, train_wall=458, gb_free=10, wall=217728
2022-02-14 06:16:12 | INFO | train_inner | epoch 030:    153 / 1576 loss=4.777, ppl=27.41, wps=13844.8, ups=0.21, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.638, loss_scale=8, train_wall=463, gb_free=10, wall=218201
2022-02-14 06:19:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:24:10 | INFO | train_inner | epoch 030:    254 / 1576 loss=4.775, ppl=27.38, wps=13707.5, ups=0.21, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.665, loss_scale=8, train_wall=467, gb_free=10, wall=218679
2022-02-14 06:32:03 | INFO | train_inner | epoch 030:    354 / 1576 loss=4.793, ppl=27.73, wps=13848.5, ups=0.21, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=0.682, loss_scale=8, train_wall=463, gb_free=10, wall=219153
2022-02-14 06:39:56 | INFO | train_inner | epoch 030:    454 / 1576 loss=4.818, ppl=28.2, wps=13846.7, ups=0.21, wpb=65536, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.65, loss_scale=16, train_wall=463, gb_free=10, wall=219626
2022-02-14 06:40:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:47:54 | INFO | train_inner | epoch 030:    555 / 1576 loss=4.827, ppl=28.39, wps=13710.7, ups=0.21, wpb=65536, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.692, loss_scale=8, train_wall=467, gb_free=10, wall=220104
2022-02-14 06:55:48 | INFO | train_inner | epoch 030:    655 / 1576 loss=4.824, ppl=28.33, wps=13841.1, ups=0.21, wpb=65536, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.691, loss_scale=8, train_wall=463, gb_free=10, wall=220577
2022-02-14 07:03:41 | INFO | train_inner | epoch 030:    755 / 1576 loss=4.833, ppl=28.49, wps=13834.4, ups=0.21, wpb=65536, bsz=128, num_updates=46300, lr=0.000146964, gnorm=0.649, loss_scale=16, train_wall=463, gb_free=10, wall=221051
2022-02-14 07:04:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:11:39 | INFO | train_inner | epoch 030:    856 / 1576 loss=4.839, ppl=28.62, wps=13708.9, ups=0.21, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.674, loss_scale=8, train_wall=467, gb_free=10, wall=221529
2022-02-14 07:19:33 | INFO | train_inner | epoch 030:    956 / 1576 loss=4.837, ppl=28.59, wps=13848.2, ups=0.21, wpb=65536, bsz=128, num_updates=46500, lr=0.000146647, gnorm=0.667, loss_scale=8, train_wall=463, gb_free=10, wall=222002
2022-02-14 07:25:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:27:31 | INFO | train_inner | epoch 030:   1057 / 1576 loss=4.851, ppl=28.85, wps=13712.4, ups=0.21, wpb=65536, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.654, loss_scale=8, train_wall=467, gb_free=10, wall=222480
2022-02-14 07:35:24 | INFO | train_inner | epoch 030:   1157 / 1576 loss=4.857, ppl=28.98, wps=13845.3, ups=0.21, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=0.672, loss_scale=8, train_wall=463, gb_free=10, wall=222954
2022-02-14 07:43:17 | INFO | train_inner | epoch 030:   1257 / 1576 loss=4.861, ppl=29.07, wps=13848.7, ups=0.21, wpb=65532.3, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.654, loss_scale=8, train_wall=463, gb_free=10, wall=223427
2022-02-14 07:45:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:51:15 | INFO | train_inner | epoch 030:   1358 / 1576 loss=4.858, ppl=29.01, wps=13704.9, ups=0.21, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.68, loss_scale=8, train_wall=468, gb_free=10, wall=223905
2022-02-14 07:59:09 | INFO | train_inner | epoch 030:   1458 / 1576 loss=4.862, ppl=29.07, wps=13846, ups=0.21, wpb=65536, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.679, loss_scale=8, train_wall=463, gb_free=10, wall=224378
2022-02-14 08:05:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:07:07 | INFO | train_inner | epoch 030:   1559 / 1576 loss=4.876, ppl=29.36, wps=13707.1, ups=0.21, wpb=65536, bsz=128, num_updates=47100, lr=0.00014571, gnorm=0.676, loss_scale=8, train_wall=467, gb_free=10, wall=224857
2022-02-14 08:08:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 08:08:29 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.067 | ppl 33.52 | wps 37139.1 | wpb 1021.8 | bsz 2 | num_updates 47117 | best_loss 5.067
2022-02-14 08:08:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 47117 updates
2022-02-14 08:08:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint30.pt
2022-02-14 08:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint30.pt
2022-02-14 08:08:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint30.pt (epoch 30 @ 47117 updates, score 5.067) (writing took 29.061282316222787 seconds)
2022-02-14 08:08:58 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-14 08:08:58 | INFO | train | epoch 030 | loss 4.83 | ppl 28.44 | wps 13728.5 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 47117 | lr 0.000145684 | gnorm 0.668 | loss_scale 8 | train_wall 7289 | gb_free 10 | wall 224968
2022-02-14 08:08:58 | INFO | fairseq.trainer | begin training epoch 31
2022-02-14 08:08:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 08:15:31 | INFO | train_inner | epoch 031:     83 / 1576 loss=4.771, ppl=27.31, wps=12891.7, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=47200, lr=0.000145556, gnorm=0.675, loss_scale=8, train_wall=458, gb_free=10, wall=225360
2022-02-14 08:23:24 | INFO | train_inner | epoch 031:    183 / 1576 loss=4.773, ppl=27.34, wps=13844, ups=0.21, wpb=65536, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.658, loss_scale=8, train_wall=463, gb_free=10, wall=225834
2022-02-14 08:26:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:31:22 | INFO | train_inner | epoch 031:    284 / 1576 loss=4.786, ppl=27.6, wps=13703.9, ups=0.21, wpb=65536, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.682, loss_scale=8, train_wall=468, gb_free=10, wall=226312
2022-02-14 08:39:16 | INFO | train_inner | epoch 031:    384 / 1576 loss=4.788, ppl=27.63, wps=13840.2, ups=0.21, wpb=65536, bsz=128, num_updates=47500, lr=0.000145095, gnorm=0.676, loss_scale=8, train_wall=463, gb_free=10, wall=226786
2022-02-14 08:46:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:47:14 | INFO | train_inner | epoch 031:    485 / 1576 loss=4.803, ppl=27.92, wps=13706, ups=0.21, wpb=65536, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.675, loss_scale=8, train_wall=468, gb_free=10, wall=227264
2022-02-14 08:55:07 | INFO | train_inner | epoch 031:    585 / 1576 loss=4.812, ppl=28.09, wps=13845.1, ups=0.21, wpb=65536, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.684, loss_scale=8, train_wall=463, gb_free=10, wall=227737
2022-02-14 09:03:01 | INFO | train_inner | epoch 031:    685 / 1576 loss=4.81, ppl=28.06, wps=13848, ups=0.21, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.654, loss_scale=8, train_wall=463, gb_free=10, wall=228210
2022-02-14 09:07:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:10:59 | INFO | train_inner | epoch 031:    786 / 1576 loss=4.822, ppl=28.29, wps=13709.6, ups=0.21, wpb=65536, bsz=128, num_updates=47900, lr=0.000144488, gnorm=0.655, loss_scale=8, train_wall=467, gb_free=10, wall=228688
2022-02-14 09:18:52 | INFO | train_inner | epoch 031:    886 / 1576 loss=4.826, ppl=28.35, wps=13838.7, ups=0.21, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.683, loss_scale=8, train_wall=463, gb_free=10, wall=229162
2022-02-14 09:25:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 09:26:50 | INFO | train_inner | epoch 031:    987 / 1576 loss=4.842, ppl=28.67, wps=13704.5, ups=0.21, wpb=65536, bsz=128, num_updates=48100, lr=0.000144187, gnorm=0.673, loss_scale=4, train_wall=468, gb_free=10, wall=229640
2022-02-14 09:34:44 | INFO | train_inner | epoch 031:   1087 / 1576 loss=4.858, ppl=28.99, wps=13847, ups=0.21, wpb=65536, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.65, loss_scale=4, train_wall=463, gb_free=10, wall=230113
2022-02-14 09:42:37 | INFO | train_inner | epoch 031:   1187 / 1576 loss=4.85, ppl=28.84, wps=13853.2, ups=0.21, wpb=65536, bsz=128, num_updates=48300, lr=0.000143889, gnorm=0.661, loss_scale=4, train_wall=463, gb_free=10, wall=230587
2022-02-14 09:50:30 | INFO | train_inner | epoch 031:   1287 / 1576 loss=4.847, ppl=28.79, wps=13847.5, ups=0.21, wpb=65536, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.678, loss_scale=8, train_wall=463, gb_free=10, wall=231060
2022-02-14 09:58:24 | INFO | train_inner | epoch 031:   1387 / 1576 loss=4.855, ppl=28.95, wps=13839.3, ups=0.21, wpb=65536, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.652, loss_scale=8, train_wall=463, gb_free=10, wall=231533
2022-02-14 10:05:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:06:22 | INFO | train_inner | epoch 031:   1488 / 1576 loss=4.871, ppl=29.27, wps=13701.3, ups=0.21, wpb=65536, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.671, loss_scale=8, train_wall=468, gb_free=10, wall=232012
2022-02-14 10:13:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 10:13:20 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.068 | ppl 33.54 | wps 37133.4 | wpb 1021.8 | bsz 2 | num_updates 48688 | best_loss 5.067
2022-02-14 10:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 48688 updates
2022-02-14 10:13:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint31.pt
2022-02-14 10:13:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint31.pt
2022-02-14 10:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint31.pt (epoch 31 @ 48688 updates, score 5.068) (writing took 19.284327213652432 seconds)
2022-02-14 10:13:40 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-14 10:13:40 | INFO | train | epoch 031 | loss 4.824 | ppl 28.32 | wps 13753.9 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 48688 | lr 0.000143314 | gnorm 0.669 | loss_scale 8 | train_wall 7290 | gb_free 10 | wall 232449
2022-02-14 10:13:40 | INFO | fairseq.trainer | begin training epoch 32
2022-02-14 10:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 10:14:37 | INFO | train_inner | epoch 032:     12 / 1576 loss=4.86, ppl=29.05, wps=13134.2, ups=0.2, wpb=64958.8, bsz=126.9, num_updates=48700, lr=0.000143296, gnorm=0.687, loss_scale=8, train_wall=459, gb_free=10, wall=232506
2022-02-14 10:22:30 | INFO | train_inner | epoch 032:    112 / 1576 loss=4.753, ppl=26.97, wps=13844.4, ups=0.21, wpb=65536, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.652, loss_scale=8, train_wall=463, gb_free=10, wall=232980
2022-02-14 10:26:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:30:28 | INFO | train_inner | epoch 032:    213 / 1576 loss=4.77, ppl=27.29, wps=13711, ups=0.21, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.686, loss_scale=8, train_wall=467, gb_free=10, wall=233458
2022-02-14 10:38:21 | INFO | train_inner | epoch 032:    313 / 1576 loss=4.781, ppl=27.5, wps=13852, ups=0.21, wpb=65536, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.683, loss_scale=8, train_wall=463, gb_free=10, wall=233931
2022-02-14 10:46:14 | INFO | train_inner | epoch 032:    413 / 1576 loss=4.792, ppl=27.71, wps=13844.7, ups=0.21, wpb=65536, bsz=128, num_updates=49100, lr=0.000142712, gnorm=0.697, loss_scale=8, train_wall=463, gb_free=10, wall=234404
2022-02-14 10:46:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:54:12 | INFO | train_inner | epoch 032:    514 / 1576 loss=4.793, ppl=27.72, wps=13716.1, ups=0.21, wpb=65536, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.673, loss_scale=8, train_wall=467, gb_free=10, wall=234882
2022-02-14 11:02:05 | INFO | train_inner | epoch 032:    614 / 1576 loss=4.819, ppl=28.22, wps=13849.6, ups=0.21, wpb=65536, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.655, loss_scale=8, train_wall=463, gb_free=10, wall=235355
2022-02-14 11:08:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:10:03 | INFO | train_inner | epoch 032:    715 / 1576 loss=4.816, ppl=28.17, wps=13707.8, ups=0.21, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.673, loss_scale=8, train_wall=467, gb_free=10, wall=235833
2022-02-14 11:17:57 | INFO | train_inner | epoch 032:    815 / 1576 loss=4.809, ppl=28.03, wps=13842.4, ups=0.21, wpb=65536, bsz=128, num_updates=49500, lr=0.000142134, gnorm=0.667, loss_scale=8, train_wall=463, gb_free=10, wall=236307
2022-02-14 11:25:50 | INFO | train_inner | epoch 032:    915 / 1576 loss=4.834, ppl=28.53, wps=13849.8, ups=0.21, wpb=65536, bsz=128, num_updates=49600, lr=0.00014199, gnorm=0.646, loss_scale=8, train_wall=463, gb_free=10, wall=236780
2022-02-14 11:28:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:33:48 | INFO | train_inner | epoch 032:   1016 / 1576 loss=4.838, ppl=28.59, wps=13709.3, ups=0.21, wpb=65536, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.674, loss_scale=8, train_wall=467, gb_free=10, wall=237258
2022-02-14 11:41:41 | INFO | train_inner | epoch 032:   1116 / 1576 loss=4.829, ppl=28.42, wps=13871.8, ups=0.21, wpb=65536, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.661, loss_scale=8, train_wall=462, gb_free=10, wall=237730
2022-02-14 11:49:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:49:38 | INFO | train_inner | epoch 032:   1217 / 1576 loss=4.847, ppl=28.77, wps=13736.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=49900, lr=0.000141563, gnorm=0.667, loss_scale=8, train_wall=466, gb_free=10, wall=238207
2022-02-14 11:57:31 | INFO | train_inner | epoch 032:   1317 / 1576 loss=4.859, ppl=29.01, wps=13857.8, ups=0.21, wpb=65536, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.682, loss_scale=8, train_wall=462, gb_free=10, wall=238680
2022-02-14 11:57:31 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2022-02-14 11:57:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 11:57:36 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.062 | ppl 33.4 | wps 37356.7 | wpb 1021.8 | bsz 2 | num_updates 50000 | best_loss 5.062
2022-02-14 11:57:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 50000 updates
2022-02-14 11:57:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint_best.pt
2022-02-14 11:57:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint_best.pt
2022-02-14 11:57:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 32 @ 50000 updates, score 5.062) (writing took 19.287216846831143 seconds)
2022-02-14 11:57:56 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-14 11:57:56 | INFO | train | epoch 032 | loss 4.81 | ppl 28.05 | wps 13743.8 | ups 0.21 | wpb 65535.7 | bsz 128 | num_updates 50000 | lr 0.000141421 | gnorm 0.671 | loss_scale 8 | train_wall 6092 | gb_free 10 | wall 238705
2022-02-14 11:57:56 | INFO | fairseq_cli.train | done training in 238704.9 seconds
