Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 210582244: <iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3> in cluster <euler> Exited

Job <iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3> was submitted from host <eu-login-06> by user <andriusb> in cluster <euler> at Wed Mar 23 09:31:39 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Wed Mar 23 09:32:14 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 09:32:14 2022
Terminated at Wed Mar 23 09:32:25 2022
Results reported at Wed Mar 23 09:32:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en --save-dir /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3 --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas "(0.9, 0.98)" --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.45 --max-tokens 32768 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --fp16 --no-epoch-checkpoints --patience 3 --seed 66575613 --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   7.32 sec.
    Max Memory :                                 2599 MB
    Average Memory :                             1068.67 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               17401.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                7
    Run time :                                   23 sec.
    Turnaround time :                            46 sec.

The output (if any) follows:

2022-03-23 09:32:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 32768, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 32768, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.45, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=32768, max_tokens_valid=32768, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=66575613, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.45, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2022-03-23 09:32:20 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2022-03-23 09:32:20 | INFO | fairseq_cli.train | task: TranslationTask
2022-03-23 09:32:20 | INFO | fairseq_cli.train | model: TransformerModel
2022-03-23 09:32:20 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-23 09:32:20 | INFO | fairseq_cli.train | num. shared model params: 39,469,056 (num. trained: 39,469,056)
2022-03-23 09:32:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 09:32:20 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-03-23 09:32:20 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 142, in main
    trainer = Trainer(cfg, task, model, criterion, quantizer)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 109, in __init__
    self._model = self._model.to(device=self.device)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 612, in to
    return self._apply(convert)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 359, in _apply
    module._apply(fn)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 359, in _apply
    module._apply(fn)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 381, in _apply
    param_applied = fn(param)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 610, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-049>
Subject: Job 210582257: <iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3> in cluster <euler> Exited

Job <iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3> was submitted from host <eu-login-06> by user <andriusb> in cluster <euler> at Wed Mar 23 09:31:39 2022
Job was executed on host(s) <eu-g3-049>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Wed Mar 23 09:32:14 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 09:32:14 2022
Terminated at Wed Mar 23 09:32:25 2022
Results reported at Wed Mar 23 09:32:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en --save-dir /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3 --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas "(0.9, 0.98)" --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.45 --max-tokens 32768 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --fp16 --no-epoch-checkpoints --patience 3 --seed 66575613 --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   9.05 sec.
    Max Memory :                                 2660 MB
    Average Memory :                             1869.00 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               17340.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   11 sec.
    Turnaround time :                            46 sec.

The output (if any) follows:

2022-03-23 09:32:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 32768, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 32768, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.45, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=32768, max_tokens_valid=32768, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=66575613, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.45, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2022-03-23 09:32:21 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2022-03-23 09:32:21 | INFO | fairseq_cli.train | task: TranslationTask
2022-03-23 09:32:21 | INFO | fairseq_cli.train | model: TransformerModel
2022-03-23 09:32:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-23 09:32:21 | INFO | fairseq_cli.train | num. shared model params: 39,469,056 (num. trained: 39,469,056)
2022-03-23 09:32:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 09:32:21 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-03-23 09:32:21 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-03-23 09:32:21 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2022-03-23 09:32:23 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 09:32:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 09:32:23 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 09:32:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 09:32:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 09:32:23 | INFO | fairseq_cli.train | max tokens per device = 32768 and max sentences per device = None
2022-03-23 09:32:23 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 09:32:23 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 09:32:23 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 09:32:23 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-03-23 09:32:23 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-03-23 09:32:23 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2022-03-23 09:32:23 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 09:32:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 210582236: <iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3> in cluster <euler> Done

Job <iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3> was submitted from host <eu-login-06> by user <andriusb> in cluster <euler> at Wed Mar 23 09:31:39 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Wed Mar 23 09:32:14 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 09:32:14 2022
Terminated at Wed Mar 23 10:54:17 2022
Results reported at Wed Mar 23 10:54:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en --save-dir /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3 --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas "(0.9, 0.98)" --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.45 --max-tokens 32768 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --fp16 --no-epoch-checkpoints --patience 3 --seed 66575613 --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4916.76 sec.
    Max Memory :                                 5111 MB
    Average Memory :                             3877.51 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14889.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   4936 sec.
    Turnaround time :                            4958 sec.

The output (if any) follows:

2022-03-23 09:32:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 32768, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 32768, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.45, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=32768, max_tokens_valid=32768, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=66575613, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.45, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2022-03-23 09:32:20 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2022-03-23 09:32:20 | INFO | fairseq_cli.train | task: TranslationTask
2022-03-23 09:32:20 | INFO | fairseq_cli.train | model: TransformerModel
2022-03-23 09:32:20 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-23 09:32:20 | INFO | fairseq_cli.train | num. shared model params: 39,469,056 (num. trained: 39,469,056)
2022-03-23 09:32:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 09:32:20 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-03-23 09:32:20 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-03-23 09:32:20 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2022-03-23 09:32:24 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 09:32:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 09:32:24 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 09:32:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 09:32:24 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 09:32:24 | INFO | fairseq_cli.train | max tokens per device = 32768 and max sentences per device = None
2022-03-23 09:32:24 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 09:32:24 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 09:32:24 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 09:32:24 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-03-23 09:32:24 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-03-23 09:32:24 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2022-03-23 09:32:24 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 09:32:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:32:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-23 09:32:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 09:32:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 09:32:59 | INFO | train_inner | epoch 001:    103 / 157 loss=12.55, nll_loss=11.832, ppl=3645.18, wps=79468.4, ups=3.2, wpb=24814.2, bsz=999.7, num_updates=100, lr=1.25e-05, gnorm=2.069, loss_scale=16, train_wall=34, gb_free=14.7, wall=35
2022-03-23 09:33:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:33:19 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-23 09:33:19 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:33:21 | INFO | fairseq.tasks.translation | example hypothesis: .....
2022-03-23 09:33:21 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:33:24 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-23 09:33:24 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:33:26 | INFO | fairseq.tasks.translation | example hypothesis: ..
2022-03-23 09:33:26 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:33:28 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-23 09:33:28 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:33:31 | INFO | fairseq.tasks.translation | example hypothesis: .....
2022-03-23 09:33:31 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:33:34 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,
2022-03-23 09:33:34 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:33:36 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,
2022-03-23 09:33:36 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:33:40 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,
2022-03-23 09:33:40 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:33:42 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,
2022-03-23 09:33:42 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:33:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.545 | nll_loss 10.081 | ppl 1083.13 | bleu 0.01 | wps 7217.9 | wpb 17862.2 | bsz 728.3 | num_updates 154
2022-03-23 09:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 154 updates
2022-03-23 09:33:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:33:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:33:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 1 @ 154 updates, score 0.01) (writing took 1.6092590739717707 seconds)
2022-03-23 09:33:43 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-23 09:33:43 | INFO | train | epoch 001 | loss 12.276 | nll_loss 11.357 | ppl 2623.38 | wps 50993.7 | ups 2.03 | wpb 25055.8 | bsz 1020.5 | num_updates 154 | lr 1.925e-05 | gnorm 1.653 | loss_scale 16 | train_wall 51 | gb_free 13.7 | wall 79
2022-03-23 09:33:44 | INFO | fairseq.trainer | begin training epoch 2
2022-03-23 09:33:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:33:58 | INFO | train_inner | epoch 002:     46 / 157 loss=11.658, nll_loss=10.302, ppl=1262.67, wps=42698.1, ups=1.68, wpb=25368.2, bsz=1102.4, num_updates=200, lr=2.5e-05, gnorm=0.761, loss_scale=16, train_wall=31, gb_free=14.3, wall=94
2022-03-23 09:34:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-23 09:34:30 | INFO | train_inner | epoch 002:    147 / 157 loss=11.29, nll_loss=9.627, ppl=790.46, wps=78705.7, ups=3.13, wpb=25109.7, bsz=970.3, num_updates=300, lr=3.75e-05, gnorm=0.73, loss_scale=8, train_wall=32, gb_free=14.3, wall=126
2022-03-23 09:34:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:34:36 | INFO | fairseq.tasks.translation | example hypothesis: the the the the the the.
2022-03-23 09:34:36 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:34:40 | INFO | fairseq.tasks.translation | example hypothesis: and the the the.
2022-03-23 09:34:40 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:34:44 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 09:34:44 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:34:48 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 09:34:48 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:34:53 | INFO | fairseq.tasks.translation | example hypothesis: the the the the the the the the the the the the the the the the the the the the the.
2022-03-23 09:34:53 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:34:57 | INFO | fairseq.tasks.translation | example hypothesis: and and and the the the the the the the the the the the the the the the the the the the the the the the the the the the.
2022-03-23 09:34:57 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:35:03 | INFO | fairseq.tasks.translation | example hypothesis: i i,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 09:35:03 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:35:08 | INFO | fairseq.tasks.translation | example hypothesis: and and and and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2022-03-23 09:35:08 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:35:16 | INFO | fairseq.tasks.translation | example hypothesis: and the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 09:35:16 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:35:18 | INFO | fairseq.tasks.translation | example hypothesis: and the the the the,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 09:35:18 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:35:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.088 | nll_loss 9.135 | ppl 562.16 | bleu 0.03 | wps 3941.2 | wpb 17862.2 | bsz 728.3 | num_updates 310 | best_bleu 0.03
2022-03-23 09:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 310 updates
2022-03-23 09:35:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:35:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:35:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 2 @ 310 updates, score 0.03) (writing took 1.7125652150134556 seconds)
2022-03-23 09:35:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-23 09:35:20 | INFO | train | epoch 002 | loss 11.349 | nll_loss 9.742 | ppl 856.24 | wps 40607.3 | ups 1.62 | wpb 25120.3 | bsz 1017.9 | num_updates 310 | lr 3.875e-05 | gnorm 0.692 | loss_scale 8 | train_wall 48 | gb_free 13.9 | wall 176
2022-03-23 09:35:20 | INFO | fairseq.trainer | begin training epoch 3
2022-03-23 09:35:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:35:49 | INFO | train_inner | epoch 003:     90 / 157 loss=11.132, nll_loss=9.286, ppl=624.38, wps=31447.7, ups=1.27, wpb=24671.1, bsz=1008.8, num_updates=400, lr=5e-05, gnorm=0.879, loss_scale=8, train_wall=31, gb_free=13.9, wall=205
2022-03-23 09:36:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:36:13 | INFO | fairseq.tasks.translation | example hypothesis: but you, you of the.
2022-03-23 09:36:13 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:36:16 | INFO | fairseq.tasks.translation | example hypothesis: it's.
2022-03-23 09:36:16 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:36:20 | INFO | fairseq.tasks.translation | example hypothesis: and you, you, you, you, you, you, you, you.
2022-03-23 09:36:20 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:36:24 | INFO | fairseq.tasks.translation | example hypothesis: and it's, the, the, the, and the, and the, and the, and the.
2022-03-23 09:36:24 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:36:29 | INFO | fairseq.tasks.translation | example hypothesis: so we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we we
2022-03-23 09:36:29 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:36:34 | INFO | fairseq.tasks.translation | example hypothesis: and the in the in the of the of the of the of the of the in the of the of the of the in the in the in the in the in the in the.
2022-03-23 09:36:34 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:36:40 | INFO | fairseq.tasks.translation | example hypothesis: and i, i, i, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "
2022-03-23 09:36:40 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:36:46 | INFO | fairseq.tasks.translation | example hypothesis: and we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we we we we, we, we, we, we, we, we, we, we, we, we, we, we, we, we we, we, we
2022-03-23 09:36:46 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:36:54 | INFO | fairseq.tasks.translation | example hypothesis: and we, we, "" "" "" "" "" "" "" "" "" "" "" "" "" "," "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""
2022-03-23 09:36:54 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:36:56 | INFO | fairseq.tasks.translation | example hypothesis: so we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we we we, we, we, we, we, we, we, we, we, we, we, we, we, we, we we we we we we we, we, we, we we we we, we, we, we, we, we, we, we, we, we, we, we, we we, we, we, we, we, we, we, we, we, we, we, we, we we we we we we, we, we, we, we, we, we we we we we, we we we we we we we, we we, we, we, we we we we we, we we we, we, we, we
2022-03-23 09:36:56 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:36:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.989 | nll_loss 8.843 | ppl 459.27 | bleu 0.14 | wps 3823.6 | wpb 17862.2 | bsz 728.3 | num_updates 467 | best_bleu 0.14
2022-03-23 09:36:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 467 updates
2022-03-23 09:36:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:36:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 3 @ 467 updates, score 0.14) (writing took 1.7083667509723455 seconds)
2022-03-23 09:36:58 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-23 09:36:58 | INFO | train | epoch 003 | loss 11.096 | nll_loss 9.213 | ppl 593.38 | wps 40348 | ups 1.6 | wpb 25153.6 | bsz 1020.6 | num_updates 467 | lr 5.8375e-05 | gnorm 0.854 | loss_scale 8 | train_wall 49 | gb_free 14 | wall 274
2022-03-23 09:36:58 | INFO | fairseq.trainer | begin training epoch 4
2022-03-23 09:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:37:08 | INFO | train_inner | epoch 004:     33 / 157 loss=11.061, nll_loss=9.144, ppl=565.74, wps=31893.5, ups=1.25, wpb=25422.2, bsz=970.1, num_updates=500, lr=6.25e-05, gnorm=0.888, loss_scale=8, train_wall=31, gb_free=14.2, wall=284
2022-03-23 09:37:40 | INFO | train_inner | epoch 004:    133 / 157 loss=10.876, nll_loss=8.772, ppl=437.02, wps=79832.6, ups=3.17, wpb=25170.9, bsz=1078.8, num_updates=600, lr=7.5e-05, gnorm=0.944, loss_scale=8, train_wall=31, gb_free=14.8, wall=316
2022-03-23 09:37:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:37:51 | INFO | fairseq.tasks.translation | example hypothesis: it's a of the.
2022-03-23 09:37:51 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:37:55 | INFO | fairseq.tasks.translation | example hypothesis: and he can can can can can can can can do.
2022-03-23 09:37:55 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:37:59 | INFO | fairseq.tasks.translation | example hypothesis: and if if if you see you see you can see you can see you can see you can see you can see you can see you can see you can see you can see the
2022-03-23 09:37:59 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:38:04 | INFO | fairseq.tasks.translation | example hypothesis: and it's the, and it's it's it's the world, and it's it's the.
2022-03-23 09:38:04 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:38:09 | INFO | fairseq.tasks.translation | example hypothesis: and we have the world, we have the world, but we have the world, but we've've've've do we have the world.
2022-03-23 09:38:09 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:38:14 | INFO | fairseq.tasks.translation | example hypothesis: and this is the world of the world of the world, and it's the world, and it's the world.
2022-03-23 09:38:14 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:38:20 | INFO | fairseq.tasks.translation | example hypothesis: and i said, and i said, "" "i said," "" "" "" "" "" "" "" "" "" "" "" "" "i think, and i said, and i said, and i said, and i said, and i said, and i said, and i said,"
2022-03-23 09:38:20 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:38:26 | INFO | fairseq.tasks.translation | example hypothesis: and we have the world, and we have the world, and we have the world, and we have the world, and we have the world, and we can can can can have the world, and we have the world, and we have the world, and we have the world, and we have the world, and we have the world, and we have the world, and that we have the world, and we have the world, and we can have the
2022-03-23 09:38:26 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:38:33 | INFO | fairseq.tasks.translation | example hypothesis: and "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "
2022-03-23 09:38:33 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:38:36 | INFO | fairseq.tasks.translation | example hypothesis: and it's the world, and we're to do that we're to do that we're to do you can do you can do you can do that you know, and you can can can can can can can can can can do you can can do the to to to to be the the the the the the world, and you can can can can can do that you can do that you can can can can can can can can can can can can can can to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to the the the the the the the the world, and it, and it, and it, and it, and it, and it, and it, and it, and it, and it, and it's the world, and it's the world, and it's the world, and it's the world, and it's the
2022-03-23 09:38:36 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:38:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.736 | nll_loss 8.303 | ppl 315.73 | bleu 0.98 | wps 3669 | wpb 17862.2 | bsz 728.3 | num_updates 624 | best_bleu 0.98
2022-03-23 09:38:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 624 updates
2022-03-23 09:38:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:38:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:38:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 4 @ 624 updates, score 0.98) (writing took 1.7030442010145634 seconds)
2022-03-23 09:38:37 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-23 09:38:37 | INFO | train | epoch 004 | loss 10.906 | nll_loss 8.833 | ppl 455.92 | wps 39625.5 | ups 1.58 | wpb 25153.6 | bsz 1020.6 | num_updates 624 | lr 7.8e-05 | gnorm 0.935 | loss_scale 8 | train_wall 49 | gb_free 14.7 | wall 373
2022-03-23 09:38:38 | INFO | fairseq.trainer | begin training epoch 5
2022-03-23 09:38:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:39:02 | INFO | train_inner | epoch 005:     76 / 157 loss=10.753, nll_loss=8.524, ppl=368.04, wps=31081.7, ups=1.22, wpb=25415.1, bsz=1012.8, num_updates=700, lr=8.75e-05, gnorm=0.832, loss_scale=8, train_wall=31, gb_free=13.7, wall=398
2022-03-23 09:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:39:31 | INFO | fairseq.tasks.translation | example hypothesis: it's a lot of the world, you have a lot of the world.
2022-03-23 09:39:31 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:39:35 | INFO | fairseq.tasks.translation | example hypothesis: he can can see it.
2022-03-23 09:39:35 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:39:40 | INFO | fairseq.tasks.translation | example hypothesis: and if you can see you can see you can see you can see you can see you can see you can see you can see you can see you can see you can see the
2022-03-23 09:39:40 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:39:45 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot, and it's the world, and it's the world, and it's the world, and it's the world, and it's the world, and it's the world.
2022-03-23 09:39:45 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:39:51 | INFO | fairseq.tasks.translation | example hypothesis: we've have the world of the world, we've have the world, but we've have the world, we have to be the world, we have the world, we've have the world.
2022-03-23 09:39:51 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:39:56 | INFO | fairseq.tasks.translation | example hypothesis: and it's the world of the world of the world of the world of the world, and the world of the world, and the world, and the world, and it's the world in the world in the world, and it's the world in the world, and the world of the
2022-03-23 09:39:56 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:40:02 | INFO | fairseq.tasks.translation | example hypothesis: and i said, "i'm going to think," "" we're going to say, "" "i think," we're going to think, "we're going to do we're going to be the world," "" we're going to be the world, "" "" "" "i think," "" "
2022-03-23 09:40:02 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:40:08 | INFO | fairseq.tasks.translation | example hypothesis: so we can see the world, we can see that we can see the world, we can see the world, we can see the world, and we can see the world, we can see the world, we can see the world, we can see the world, we can see the world, we can see the world, we can see the world, we can see the world, we can see the world, we can see that we can see the world,
2022-03-23 09:40:08 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:40:16 | INFO | fairseq.tasks.translation | example hypothesis: and we said, "" "" "" "" we said, "" "" "" "" we're, "it's the world," "" "" "" "" "" "" "" "" we're the world, "it's," it's the world, "it's the world," it's the world, "it's the world," "" "it's the world," it's the world, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "we're the world," it's the world, "" "" "" "" "" "" "" we're the world,
2022-03-23 09:40:16 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:40:19 | INFO | fairseq.tasks.translation | example hypothesis: if we're a lot of the world, we're the world, we're going to be a lot of the world, and we're going to be the world, and we're going to be the world, we're the world, and we're going to be the world of the world, and we're going to be the world, and we're going to be the world, and we're going to be the world, and we're going to be the world of the world, and we're going to be the world, and we're going to be the world, and we're going to be the world, and we're going to be the world of the world, we're the world of the world, we're the world, and we're going to be the world that we're the world of the world of the world of the world of the world, and we're the world that we're the world of the world, we're the world, it's the world that we can see the world, we're going to the world,
2022-03-23 09:40:19 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:40:19 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.567 | nll_loss 7.953 | ppl 247.85 | bleu 1.12 | wps 3422.6 | wpb 17862.2 | bsz 728.3 | num_updates 781 | best_bleu 1.12
2022-03-23 09:40:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 781 updates
2022-03-23 09:40:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:40:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:40:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 5 @ 781 updates, score 1.12) (writing took 1.8852009699912742 seconds)
2022-03-23 09:40:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-23 09:40:20 | INFO | train | epoch 005 | loss 10.695 | nll_loss 8.405 | ppl 338.97 | wps 38270.2 | ups 1.52 | wpb 25153.6 | bsz 1020.6 | num_updates 781 | lr 9.7625e-05 | gnorm 0.888 | loss_scale 8 | train_wall 48 | gb_free 14.5 | wall 477
2022-03-23 09:40:21 | INFO | fairseq.trainer | begin training epoch 6
2022-03-23 09:40:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:40:27 | INFO | train_inner | epoch 006:     19 / 157 loss=10.644, nll_loss=8.3, ppl=315.07, wps=29084.8, ups=1.17, wpb=24754.2, bsz=1021.5, num_updates=800, lr=0.0001, gnorm=0.952, loss_scale=8, train_wall=31, gb_free=14, wall=483
2022-03-23 09:40:58 | INFO | train_inner | epoch 006:    119 / 157 loss=10.532, nll_loss=8.07, ppl=268.67, wps=80135.3, ups=3.16, wpb=25388.7, bsz=1075.3, num_updates=900, lr=0.0001125, gnorm=0.889, loss_scale=8, train_wall=31, gb_free=13.7, wall=515
2022-03-23 09:41:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:41:15 | INFO | fairseq.tasks.translation | example hypothesis: you have a lot of the world, you have a lot of the world.
2022-03-23 09:41:15 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:41:19 | INFO | fairseq.tasks.translation | example hypothesis: and he can be a lot of the world, he can be a lot of the world.
2022-03-23 09:41:19 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:41:24 | INFO | fairseq.tasks.translation | example hypothesis: and if you see you see you see you see you see you see you can see you see you see you see you see you see it.
2022-03-23 09:41:24 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:41:29 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of the world, and it's a lot of the world, and it's a lot of the world.
2022-03-23 09:41:29 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:41:35 | INFO | fairseq.tasks.translation | example hypothesis: and we have to have to have to be the world that we have to have to have to have to have the world that we have to be the world.
2022-03-23 09:41:35 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:41:41 | INFO | fairseq.tasks.translation | example hypothesis: and in the world, and the world is the people in the people in the people in the world, and the world, and the people in the people in the world, and the world, and the people in the world, and the world, and the people in the world, and the
2022-03-23 09:41:41 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:41:46 | INFO | fairseq.tasks.translation | example hypothesis: and i'm going to say, and i think that we're going to be a lot of the world, "" "and i think, and i think that we're going to be a lot of this is a lot of this is a lot of the world," "" "" "" "" "and i think, and i think,
2022-03-23 09:41:46 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:41:52 | INFO | fairseq.tasks.translation | example hypothesis: and so we can see that we can see that we can see that we can see the world, and we can see the world, and we can see that we can see the world, and we can see that we can see the world, and we can see that we can see the world, and we can see that we can see that we can see the world, and we can see that we can see that we can see that we can see the world,
2022-03-23 09:41:52 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:42:00 | INFO | fairseq.tasks.translation | example hypothesis: and he said, "" "" "" "" "" "" "" "" "" "" you know, "" "" "" "you know," "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""
2022-03-23 09:42:00 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:42:02 | INFO | fairseq.tasks.translation | example hypothesis: and if we have to be a lot of the way that we have to be a lot of the way that we have to make the world, and we have to make the world, and we have to be the world, and we have to make the world, and we have to make the world, and we have to make the world, and we have to be the world, and that we have to make the world, and that we have to make the world, and that we have to be the world, that we have to be the world, that we have to be the world, that we have to be the world, and the world, that we have to be the world, and we have to be the world, and we have to be the world, and we have to be the world, that we have to be the world, and we have to be the world, and we have to be the world, and we have to be the world, and that we have to be the world, that we have to be the world, the
2022-03-23 09:42:02 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:42:02 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.394 | nll_loss 7.612 | ppl 195.63 | bleu 1.54 | wps 3420.5 | wpb 17862.2 | bsz 728.3 | num_updates 938 | best_bleu 1.54
2022-03-23 09:42:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 938 updates
2022-03-23 09:42:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:42:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:42:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 6 @ 938 updates, score 1.54) (writing took 1.7255369179765694 seconds)
2022-03-23 09:42:04 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-23 09:42:04 | INFO | train | epoch 006 | loss 10.538 | nll_loss 8.082 | ppl 270.88 | wps 38084.6 | ups 1.51 | wpb 25153.6 | bsz 1020.6 | num_updates 938 | lr 0.00011725 | gnorm 0.905 | loss_scale 8 | train_wall 49 | gb_free 13.6 | wall 580
2022-03-23 09:42:04 | INFO | fairseq.trainer | begin training epoch 7
2022-03-23 09:42:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:42:24 | INFO | train_inner | epoch 007:     62 / 157 loss=10.447, nll_loss=7.897, ppl=238.3, wps=29848.8, ups=1.16, wpb=25662.8, bsz=1035, num_updates=1000, lr=0.000125, gnorm=0.864, loss_scale=8, train_wall=31, gb_free=14.2, wall=601
2022-03-23 09:42:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:42:58 | INFO | fairseq.tasks.translation | example hypothesis: you've got a lot of the time.
2022-03-23 09:42:58 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:43:02 | INFO | fairseq.tasks.translation | example hypothesis: in fact, it's a lot of course.
2022-03-23 09:43:02 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:43:06 | INFO | fairseq.tasks.translation | example hypothesis: and if you see you see, you can see a lot of the world, you can see you can see you see it.
2022-03-23 09:43:06 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:43:10 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of the world.
2022-03-23 09:43:10 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:43:15 | INFO | fairseq.tasks.translation | example hypothesis: we have a lot of the world that we have a lot of the world.
2022-03-23 09:43:15 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:43:19 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of people in the world, and in the world, and it's a lot of people in the world.
2022-03-23 09:43:19 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:43:24 | INFO | fairseq.tasks.translation | example hypothesis: and i'm going to say that we're going to say, and we're going to say, "and we're going to say, and we're going to say, and we're going to say, and we're going to say, and we're going to be a lot of a lot of the world."
2022-03-23 09:43:24 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:43:29 | INFO | fairseq.tasks.translation | example hypothesis: so if we have a lot of the world, and we're going to take the world, and we're going to take the world, and we can see the world.
2022-03-23 09:43:29 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:43:35 | INFO | fairseq.tasks.translation | example hypothesis: and i said, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know, and you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know,
2022-03-23 09:43:35 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:43:37 | INFO | fairseq.tasks.translation | example hypothesis: and if we're going to be a lot of the world, we have to be a lot of the world, and we have to be a lot of the world, and we have to be a lot of the world, and we have to be a lot of the world.
2022-03-23 09:43:37 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:43:37 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.281 | nll_loss 7.326 | ppl 160.48 | bleu 2.44 | wps 4155.8 | wpb 17862.2 | bsz 728.3 | num_updates 1095 | best_bleu 2.44
2022-03-23 09:43:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1095 updates
2022-03-23 09:43:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:43:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 7 @ 1095 updates, score 2.44) (writing took 1.7449723880272359 seconds)
2022-03-23 09:43:39 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-23 09:43:39 | INFO | train | epoch 007 | loss 10.409 | nll_loss 7.816 | ppl 225.37 | wps 41516.3 | ups 1.65 | wpb 25153.6 | bsz 1020.6 | num_updates 1095 | lr 0.000136875 | gnorm 0.852 | loss_scale 8 | train_wall 49 | gb_free 14.7 | wall 675
2022-03-23 09:43:40 | INFO | fairseq.trainer | begin training epoch 8
2022-03-23 09:43:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:43:41 | INFO | train_inner | epoch 008:      5 / 157 loss=10.413, nll_loss=7.822, ppl=226.3, wps=32084.3, ups=1.3, wpb=24658.6, bsz=952.1, num_updates=1100, lr=0.0001375, gnorm=0.803, loss_scale=8, train_wall=31, gb_free=13.5, wall=677
2022-03-23 09:44:13 | INFO | train_inner | epoch 008:    105 / 157 loss=10.286, nll_loss=7.568, ppl=189.75, wps=79191.1, ups=3.13, wpb=25311.1, bsz=1029.7, num_updates=1200, lr=0.00015, gnorm=0.689, loss_scale=8, train_wall=32, gb_free=13.4, wall=709
2022-03-23 09:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:44:33 | INFO | fairseq.tasks.translation | example hypothesis: so, you've got a woman, you've got a woman.
2022-03-23 09:44:33 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:44:37 | INFO | fairseq.tasks.translation | example hypothesis: now, let's look at a lot of course.
2022-03-23 09:44:37 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:44:42 | INFO | fairseq.tasks.translation | example hypothesis: and if you see you see you see, you can see a little little little bit of you see, you can see it.
2022-03-23 09:44:42 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:44:46 | INFO | fairseq.tasks.translation | example hypothesis: now, there are a lot of course, and there are a lot of the world.
2022-03-23 09:44:46 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:44:51 | INFO | fairseq.tasks.translation | example hypothesis: we've have a lot of the same way that we have a lot of the same way that we have a lot of the brain.
2022-03-23 09:44:51 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:44:56 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, in fact, in fact, for the people who are a lot of people for the people for the people for the people, for the people, for the people for the people for the people, for the people, for the people who have to make a lot of people.
2022-03-23 09:44:56 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:45:01 | INFO | fairseq.tasks.translation | example hypothesis: and that's a kind of the way that we're going to say, "how we're going to be in the world," how we're going to be in the world. "
2022-03-23 09:45:01 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:45:07 | INFO | fairseq.tasks.translation | example hypothesis: so if we're going to take the world, we can see that we can have a lot of the world, and we can see that we can have a very very very very different of the world, and then we can see that we can see that we can see that we can see the brain.
2022-03-23 09:45:07 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:45:14 | INFO | fairseq.tasks.translation | example hypothesis: you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know,"
2022-03-23 09:45:14 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:45:16 | INFO | fairseq.tasks.translation | example hypothesis: now, it's the most thing that we're going to have to have to be able to be a little bit of the way that we're going to be a little bit of the world, which is that we're going to be a little little bit of the world, which we're going to have a little little bit of the world, which is that we're going to have to have to have to have to have to have to be a little little bit of the world.
2022-03-23 09:45:16 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:45:16 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.122 | nll_loss 7.04 | ppl 131.59 | bleu 2.95 | wps 3819.2 | wpb 17862.2 | bsz 728.3 | num_updates 1252 | best_bleu 2.95
2022-03-23 09:45:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1252 updates
2022-03-23 09:45:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:45:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:45:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 8 @ 1252 updates, score 2.95) (writing took 1.7252262069960125 seconds)
2022-03-23 09:45:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-23 09:45:18 | INFO | train | epoch 008 | loss 10.269 | nll_loss 7.535 | ppl 185.42 | wps 40076.3 | ups 1.59 | wpb 25153.6 | bsz 1020.6 | num_updates 1252 | lr 0.0001565 | gnorm 0.715 | loss_scale 8 | train_wall 49 | gb_free 13.8 | wall 774
2022-03-23 09:45:18 | INFO | fairseq.trainer | begin training epoch 9
2022-03-23 09:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:45:34 | INFO | train_inner | epoch 009:     48 / 157 loss=10.247, nll_loss=7.489, ppl=179.62, wps=30736.4, ups=1.24, wpb=24698.2, bsz=934.7, num_updates=1300, lr=0.0001625, gnorm=0.749, loss_scale=8, train_wall=31, gb_free=14, wall=790
2022-03-23 09:46:05 | INFO | train_inner | epoch 009:    148 / 157 loss=10.082, nll_loss=7.157, ppl=142.7, wps=81127.5, ups=3.18, wpb=25477.4, bsz=1098.5, num_updates=1400, lr=0.000175, gnorm=0.74, loss_scale=8, train_wall=31, gb_free=14.1, wall=821
2022-03-23 09:46:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:46:13 | INFO | fairseq.tasks.translation | example hypothesis: in a man, you've got a woman, you've got a woman.
2022-03-23 09:46:13 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:46:18 | INFO | fairseq.tasks.translation | example hypothesis: now, the last year, he can be about a year in the last year.
2022-03-23 09:46:18 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:46:22 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the look at the look at the look at a little little bit of you see you see you see a little little little little little little little little little bit
2022-03-23 09:46:22 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:46:27 | INFO | fairseq.tasks.translation | example hypothesis: now, it's a lot of the world, and it's going on the world.
2022-03-23 09:46:27 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:46:32 | INFO | fairseq.tasks.translation | example hypothesis: we've found that there's a little lot of the way that there are a lot of the brain.
2022-03-23 09:46:32 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:46:37 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, people like people for people, for people, for people, and it's a lot of people in the world.
2022-03-23 09:46:37 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:46:42 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think, "i think," i think, "i think," how we're going to live in a lot of life. "
2022-03-23 09:46:42 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:46:47 | INFO | fairseq.tasks.translation | example hypothesis: so if we look at the way that we can be able to be able to be able to be able to be able to make a lot of the brain, and then we can make a lot of the world, and we can be able to be able to make a lot of the world, and then we can see that we can be able to make a lot of the world.
2022-03-23 09:46:47 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:46:54 | INFO | fairseq.tasks.translation | example hypothesis: one: it's a lot of people who said, "" well, "" it's a lot of you know, "well," you know, "you know," you know, "you know," you know, "you know," you know, "it's going to say," "it's a lot of you know," you know, "it's going to do it's a lot of it's going to say," "and it's a lot of the" "" "" "" "" and it's going to do it's a lot of you know, "you know," it's going to do it's a lot of you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," well, "
2022-03-23 09:46:54 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:46:56 | INFO | fairseq.tasks.translation | example hypothesis: now, in fact, the fact, the way that we're going to have a lot of the way that we're going to have to have to be able to be able to be a way that we're going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be a
2022-03-23 09:46:56 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:46:56 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.002 | nll_loss 6.824 | ppl 113.3 | bleu 3.68 | wps 3820.9 | wpb 17862.2 | bsz 728.3 | num_updates 1409 | best_bleu 3.68
2022-03-23 09:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1409 updates
2022-03-23 09:46:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:46:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:46:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 9 @ 1409 updates, score 3.68) (writing took 1.7460644869715907 seconds)
2022-03-23 09:46:58 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-23 09:46:58 | INFO | train | epoch 009 | loss 10.139 | nll_loss 7.269 | ppl 154.27 | wps 39491.5 | ups 1.57 | wpb 25153.6 | bsz 1020.6 | num_updates 1409 | lr 0.000176125 | gnorm 0.739 | loss_scale 8 | train_wall 49 | gb_free 14.1 | wall 874
2022-03-23 09:46:58 | INFO | fairseq.trainer | begin training epoch 10
2022-03-23 09:46:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:47:27 | INFO | train_inner | epoch 010:     91 / 157 loss=10.062, nll_loss=7.115, ppl=138.61, wps=30537.5, ups=1.22, wpb=25045.2, bsz=1031.8, num_updates=1500, lr=0.0001875, gnorm=0.818, loss_scale=8, train_wall=31, gb_free=13.8, wall=903
2022-03-23 09:47:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:47:51 | INFO | fairseq.tasks.translation | example hypothesis: may, you've got a woman, and you've got a woman.
2022-03-23 09:47:51 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:47:56 | INFO | fairseq.tasks.translation | example hypothesis: now, it can be about about about 30 years.
2022-03-23 09:47:56 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:48:00 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the light, you can see the light, you see a little little little little little bit.
2022-03-23 09:48:00 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:48:04 | INFO | fairseq.tasks.translation | example hypothesis: very much, and it's going on the world, and it's the world, and the world are in the united states.
2022-03-23 09:48:04 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:48:09 | INFO | fairseq.tasks.translation | example hypothesis: we've found that there's a little little little little bit that there are in the same way that there are actually actually actually actually actually in the same way.
2022-03-23 09:48:09 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:48:13 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, in the people like the people, the people who had been a lot of people, and it's a lot of people, and it's a lot of people, and it's in the world.
2022-03-23 09:48:13 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:48:18 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think how i think i think i think about how to live in a life, "in our life," in our life, in a life. "
2022-03-23 09:48:18 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:48:23 | INFO | fairseq.tasks.translation | example hypothesis: so if we're going to use the data, we can see this, we can see this, and then we can see that, and we can see a little bit of the brain, and we can see the way with a little bit of the brain.
2022-03-23 09:48:23 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:48:30 | INFO | fairseq.tasks.translation | example hypothesis: now, one of the reasons, and it's really interesting, and it's really interesting for me, "and we've got to say," "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, ""
2022-03-23 09:48:30 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:48:32 | INFO | fairseq.tasks.translation | example hypothesis: now, in fact, the fact, the same time, and the work, and if we have a little bit of the way that we could have to have a little bit of the way that we could have to have to have a little bit of the way that we could have to have to have to do with a little bit of the way that we have to do with a little bit of the way that we could have to have to work, if we have to do with a little bit of the way that we could have a little bit of the world, if we could have a little bit of the way that we could have to have a little bit of the way that we could have to have to have to do with a little bit of the way that we could have to be a little bit of the way that we could have to have to do with a little bit of the way that if we could have a little bit of the same time, if we could have to be a little bit of the same time, if we could have a little bit,
2022-03-23 09:48:32 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:48:32 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.866 | nll_loss 6.394 | ppl 84.12 | bleu 6.04 | wps 4021.9 | wpb 17862.2 | bsz 728.3 | num_updates 1566 | best_bleu 6.04
2022-03-23 09:48:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1566 updates
2022-03-23 09:48:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:48:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:48:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 10 @ 1566 updates, score 6.04) (writing took 1.7719994540093467 seconds)
2022-03-23 09:48:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-23 09:48:34 | INFO | train | epoch 010 | loss 10.032 | nll_loss 7.054 | ppl 132.88 | wps 40991.1 | ups 1.63 | wpb 25153.6 | bsz 1020.6 | num_updates 1566 | lr 0.00019575 | gnorm 0.749 | loss_scale 8 | train_wall 49 | gb_free 14.1 | wall 970
2022-03-23 09:48:34 | INFO | fairseq.trainer | begin training epoch 11
2022-03-23 09:48:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:48:45 | INFO | train_inner | epoch 011:     34 / 157 loss=9.983, nll_loss=6.951, ppl=123.72, wps=32300.5, ups=1.28, wpb=25263, bsz=1016, num_updates=1600, lr=0.0002, gnorm=0.738, loss_scale=8, train_wall=31, gb_free=14.3, wall=981
2022-03-23 09:49:17 | INFO | train_inner | epoch 011:    134 / 157 loss=9.895, nll_loss=6.778, ppl=109.77, wps=79048.5, ups=3.17, wpb=24935, bsz=1011.8, num_updates=1700, lr=0.0002125, gnorm=0.674, loss_scale=8, train_wall=31, gb_free=13.6, wall=1013
2022-03-23 09:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:49:28 | INFO | fairseq.tasks.translation | example hypothesis: maa, you've got the woman who had a woman.
2022-03-23 09:49:28 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:49:32 | INFO | fairseq.tasks.translation | example hypothesis: now, he can be about about about 80,000 people in the world.
2022-03-23 09:49:32 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:49:36 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the light, you look at a little little bit of the little little bit.
2022-03-23 09:49:36 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:49:40 | INFO | fairseq.tasks.translation | example hypothesis: very important, and it's going to go on the countries, and the united states, the united states are the united states.
2022-03-23 09:49:40 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:49:44 | INFO | fairseq.tasks.translation | example hypothesis: we've found that there's a little little little bit that there are actually actually actually actually actually actually actually actually actually in fact.
2022-03-23 09:49:44 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:49:48 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamamay of people like people who had been used to get a lot of people, and it's a lot of people.
2022-03-23 09:49:48 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:49:52 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think, as well, i think, in a lot of life, "how we should live in our life."
2022-03-23 09:49:52 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:49:57 | INFO | fairseq.tasks.translation | example hypothesis: so, if we use the information that we can see this information, we can see it with a kind of information, and we can see that's the information that's going to be able to be able.
2022-03-23 09:49:57 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:50:01 | INFO | fairseq.tasks.translation | example hypothesis: and one of the reasons of the reasons that it's interesting for me for me, "well," well, "well," well, "well," well, "we've got to say," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "
2022-03-23 09:50:01 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:50:02 | INFO | fairseq.tasks.translation | example hypothesis: now, the need to have always always always always always have a mother, and we're going to have a lot of the way that we're going to do it, and we had to do it, if we had to have a big way to be a big way to be able to do it.
2022-03-23 09:50:02 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:50:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.71 | nll_loss 6.085 | ppl 67.89 | bleu 8.93 | wps 4809.2 | wpb 17862.2 | bsz 728.3 | num_updates 1723 | best_bleu 8.93
2022-03-23 09:50:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1723 updates
2022-03-23 09:50:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:50:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:50:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 11 @ 1723 updates, score 8.93) (writing took 1.7322404779843055 seconds)
2022-03-23 09:50:04 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-23 09:50:04 | INFO | train | epoch 011 | loss 9.9 | nll_loss 6.788 | ppl 110.5 | wps 44068.7 | ups 1.75 | wpb 25153.6 | bsz 1020.6 | num_updates 1723 | lr 0.000215375 | gnorm 0.72 | loss_scale 8 | train_wall 49 | gb_free 13.5 | wall 1060
2022-03-23 09:50:04 | INFO | fairseq.trainer | begin training epoch 12
2022-03-23 09:50:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:50:29 | INFO | train_inner | epoch 012:     77 / 157 loss=9.805, nll_loss=6.601, ppl=97.06, wps=35363.1, ups=1.39, wpb=25416.2, bsz=1033, num_updates=1800, lr=0.000225, gnorm=0.723, loss_scale=8, train_wall=31, gb_free=13.6, wall=1085
2022-03-23 09:50:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:50:58 | INFO | fairseq.tasks.translation | example hypothesis: mamaa, you know, you were the woman who was the woman.
2022-03-23 09:50:58 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:51:02 | INFO | fairseq.tasks.translation | example hypothesis: this year can be about about about 880,000 people in the world.
2022-03-23 09:51:02 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:51:06 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top, you see, you see, you see a little bit of a little bit of a little bit.
2022-03-23 09:51:06 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:51:11 | INFO | fairseq.tasks.translation | example hypothesis: very important, it's very difficult to get it on the united states, and the countries in the united states, the united states are the united states.
2022-03-23 09:51:11 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:51:15 | INFO | fairseq.tasks.translation | example hypothesis: we've seen that there's a little little bit of the little bit of the dna that there are actually actually actually actually actually actually actually actually actually actually actually actually actually actually in the number of the number.
2022-03-23 09:51:15 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:51:20 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamamamamamay of how people had been used to make people for the number of the number of the number of the number of the number.
2022-03-23 09:51:20 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:51:24 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think the idea of how i think i think i think i think about how a lot of a lot of life is going to be in our lives. "
2022-03-23 09:51:24 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:51:28 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information of information that we can use the information of these kinds of information that we can start able to start with a lot of information, and the information that's all the information that all the information that all the information.
2022-03-23 09:51:28 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:51:33 | INFO | fairseq.tasks.translation | example hypothesis: th: one of one of the reasons that it's interesting for interesting, and it's interesting for me for me for me for me for me that we've got to say that there's a lot of women, and then we've got to say, "and then you've got a lot of the best time for you know, and then you know, and then you've got a lot of you've got a lot of you've got to say that you've got to say, and then you've got to have a lot of you know, and then you've got a lot of you've got a lot of the best for you've got to say, and then you've got to say that you know, and then you've got a lot of you've got a lot of you've got a lot of the best for you've got a lot of the best for you've got to say,"
2022-03-23 09:51:33 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:51:35 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, yet, the need is still still the mother of the mother, and the great way we have a lot of work on our work that we have a lot of the problems that you have to have to see that we had to have to see a lot of a lot of a lot of the problems that we had to see that we had to see that we have to have to see that if you have to see that we had to have to have to have to have to see it in a huge system in a lot of a lot of the united united states, or a lot of a lot of the united states in a lot of the united states in a lot of the united states, or a lot of a lot of a lot of the united states, to see that we have to see that we have to see that we have to see that if you have to see that we have to see that we have to see that we have to see that we have to see that we have to see that we have to see a lot of a lot of a lot
2022-03-23 09:51:35 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:51:35 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.66 | nll_loss 5.888 | ppl 59.21 | bleu 9.33 | wps 4341.2 | wpb 17862.2 | bsz 728.3 | num_updates 1880 | best_bleu 9.33
2022-03-23 09:51:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1880 updates
2022-03-23 09:51:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:51:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:51:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 12 @ 1880 updates, score 9.33) (writing took 1.778139169968199 seconds)
2022-03-23 09:51:37 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-23 09:51:37 | INFO | train | epoch 012 | loss 9.755 | nll_loss 6.499 | ppl 90.46 | wps 42217.8 | ups 1.68 | wpb 25153.6 | bsz 1020.6 | num_updates 1880 | lr 0.000235 | gnorm 0.677 | loss_scale 8 | train_wall 49 | gb_free 13.7 | wall 1153
2022-03-23 09:51:38 | INFO | fairseq.trainer | begin training epoch 13
2022-03-23 09:51:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:51:44 | INFO | train_inner | epoch 013:     20 / 157 loss=9.69, nll_loss=6.369, ppl=82.65, wps=33941.7, ups=1.32, wpb=25665.9, bsz=1044.7, num_updates=1900, lr=0.0002375, gnorm=0.63, loss_scale=8, train_wall=31, gb_free=14, wall=1160
2022-03-23 09:52:16 | INFO | train_inner | epoch 013:    120 / 157 loss=9.626, nll_loss=6.243, ppl=75.74, wps=78495.2, ups=3.17, wpb=24768.3, bsz=1025, num_updates=2000, lr=0.00025, gnorm=0.668, loss_scale=8, train_wall=31, gb_free=13.8, wall=1192
2022-03-23 09:52:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:52:31 | INFO | fairseq.tasks.translation | example hypothesis: mamaa, you know, you've got the woman, the woman.
2022-03-23 09:52:31 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:52:35 | INFO | fairseq.tasks.translation | example hypothesis: now, he can be about about 880s.
2022-03-23 09:52:35 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:52:39 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top, you see, you see, you see, you see a little little bit.
2022-03-23 09:52:39 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:52:43 | INFO | fairseq.tasks.translation | example hypothesis: very interested in fact, it's where there are countries, and the united states, the united states, the united states, the united states.
2022-03-23 09:52:43 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:52:48 | INFO | fairseq.tasks.translation | example hypothesis: we've found that there's small dna on the dna that there's actually actually actually actually actually actually actually actually the cell.
2022-03-23 09:52:48 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:52:52 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamamamamamamacy, as people were used for the number of the number of animals, and this is a number of the people.
2022-03-23 09:52:52 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:52:56 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think, as i think, i think, in a very, in fact, in fact, in fact, we should say, "we should live in our lives."
2022-03-23 09:52:56 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:53:01 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information of this information that we can actually start with a lot of the traditional traditional body, we can start able to start with a lot of information, and the structure of the structure of the information, and the whole structure of the whole structure of the information, and the whole information, and the whole structure of the whole information that we're all the whole information.
2022-03-23 09:53:01 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:53:06 | INFO | fairseq.tasks.translation | example hypothesis: 19th: one of the reasons that it's interesting for me, and for me, for me, for me, "is that we've got to tell you," well, "well, you know, the best time, the best time," the best thing that we're going to say, "well," well, "you're going to say," well, "well," well, "we've got to do it's the best for you know," well, "you've got to do it's the best for you know," well, "well," well, "you've got to do it's a long long long long long long long long time."
2022-03-23 09:53:06 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:53:08 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, the need is still the mother of the mother, and the great work of our work that we're working on our work on our work, which is that we've had to use it to use it to be a little bit of the ground, if we had to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to see it with a little bit of a little bit of a little bit of the ground, or to see that we've been able to be able to be able to see that if you're going to see that if you're going to see that we're going to see that we're going to see that we're going to see it in a little bit of the ground, or the ground, or a little bit of a little bit of the ground, or a big big big big big, we've been able to see it's the
2022-03-23 09:53:08 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:53:08 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.421 | nll_loss 5.456 | ppl 43.89 | bleu 11.81 | wps 4447.5 | wpb 17862.2 | bsz 728.3 | num_updates 2037 | best_bleu 11.81
2022-03-23 09:53:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2037 updates
2022-03-23 09:53:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 13 @ 2037 updates, score 11.81) (writing took 1.800606450997293 seconds)
2022-03-23 09:53:10 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-23 09:53:10 | INFO | train | epoch 013 | loss 9.619 | nll_loss 6.229 | ppl 75.03 | wps 42725.6 | ups 1.7 | wpb 25153.6 | bsz 1020.6 | num_updates 2037 | lr 0.000254625 | gnorm 0.648 | loss_scale 8 | train_wall 48 | gb_free 13.7 | wall 1246
2022-03-23 09:53:10 | INFO | fairseq.trainer | begin training epoch 14
2022-03-23 09:53:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:53:30 | INFO | train_inner | epoch 014:     63 / 157 loss=9.537, nll_loss=6.065, ppl=66.96, wps=33843.5, ups=1.34, wpb=25212.4, bsz=1009.9, num_updates=2100, lr=0.0002625, gnorm=0.626, loss_scale=8, train_wall=31, gb_free=14.9, wall=1266
2022-03-23 09:54:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:54:04 | INFO | fairseq.tasks.translation | example hypothesis: maira, you have the woman.
2022-03-23 09:54:04 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:54:08 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 80,000 people in the restaurant.
2022-03-23 09:54:08 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:54:11 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the left, you see a little bit.
2022-03-23 09:54:11 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:54:15 | INFO | fairseq.tasks.translation | example hypothesis: it's very interested in japan, and there are the countries that are the united states.
2022-03-23 09:54:15 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:54:18 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are small dna that are actually actually actually in fact.
2022-03-23 09:54:18 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:54:22 | INFO | fairseq.tasks.translation | example hypothesis: and in the macy, how people were used for the responsibility, the number of animals, and this is a number of animals.
2022-03-23 09:54:22 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:54:26 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think i think i find in a very nice place in a lot of scientific scientific life, he says, "he says," how we should live our lives. "
2022-03-23 09:54:26 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:54:30 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that we can actually start with this particular amount of information, we can start able to start with the traditional traditional information and start through the structure of the information, and the whole structure of all the information, which is all the information.
2022-03-23 09:54:30 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:54:34 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons, and it's interesting for me to be able to be here for me, "yes, you know, you know, you know," you know, you know, the best revolution. "
2022-03-23 09:54:34 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:54:35 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, it's still the mother, and it's a big design that we're working on our work on our work that we have to make a lot of these problems that we've had to do it, and if we have to make a little bit of the power of a lot of the power.
2022-03-23 09:54:35 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:54:35 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.359 | nll_loss 5.284 | ppl 38.97 | bleu 11.57 | wps 5308.8 | wpb 17862.2 | bsz 728.3 | num_updates 2194 | best_bleu 11.81
2022-03-23 09:54:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2194 updates
2022-03-23 09:54:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 09:54:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 09:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 14 @ 2194 updates, score 11.57) (writing took 0.8043814239790663 seconds)
2022-03-23 09:54:36 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-23 09:54:36 | INFO | train | epoch 014 | loss 9.466 | nll_loss 5.924 | ppl 60.71 | wps 45972 | ups 1.83 | wpb 25153.6 | bsz 1020.6 | num_updates 2194 | lr 0.00027425 | gnorm 0.618 | loss_scale 8 | train_wall 49 | gb_free 14 | wall 1332
2022-03-23 09:54:36 | INFO | fairseq.trainer | begin training epoch 15
2022-03-23 09:54:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:54:38 | INFO | train_inner | epoch 015:      6 / 157 loss=9.454, nll_loss=5.898, ppl=59.62, wps=37009.5, ups=1.48, wpb=25033, bsz=987.9, num_updates=2200, lr=0.000275, gnorm=0.604, loss_scale=8, train_wall=31, gb_free=13.7, wall=1334
2022-03-23 09:55:10 | INFO | train_inner | epoch 015:    106 / 157 loss=9.365, nll_loss=5.719, ppl=52.67, wps=78765.9, ups=3.17, wpb=24891.7, bsz=1017.4, num_updates=2300, lr=0.0002875, gnorm=0.604, loss_scale=8, train_wall=31, gb_free=13.8, wall=1366
2022-03-23 09:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:55:29 | INFO | fairseq.tasks.translation | example hypothesis: maa, you know, you know, the anana woman, the woman, the woman's woman.
2022-03-23 09:55:29 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:55:34 | INFO | fairseq.tasks.translation | example hypothesis: now, he can be about about 8,000 people in the restaurant in the restaurant.
2022-03-23 09:55:34 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:55:38 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top of the corner, then you see a little dark point.
2022-03-23 09:55:38 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:55:42 | INFO | fairseq.tasks.translation | example hypothesis: especially, it goes on japan, japan, and there are the countries that are the united states, and the united states, the united states.
2022-03-23 09:55:42 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:55:47 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are small dna on the dna of the cancer, in fact, that there's actually the cell cell cell.
2022-03-23 09:55:47 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:55:51 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamacy of the responsibility, as people came up to the life, and this is a lot of people, and that has become an iniiians.
2022-03-23 09:55:51 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:55:55 | INFO | fairseq.tasks.translation | example hypothesis: and that's a idea that i think, as i think about paul paul, in a very nice, in fact, he says, "how we should live in life."
2022-03-23 09:55:55 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:56:00 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that we can use the information from this particular particular, we can start able to start with a traditional level, and you can start through the information of the information, and the whole structure of the information, and the whole structure, and the whole structure, and the whole structure of the information that all the information that all the information.
2022-03-23 09:56:00 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:56:05 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting, and it's interesting for me that we've been talking to me, "yeah, when we've got the best revolution, and then we say," well, "when we've been talking about this one of them."
2022-03-23 09:56:05 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:56:07 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, the need is still the mother of the mother, and the great design of the design that we've had to be able to see that it was a little bit of these problems that we were able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to use a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of the ground, or a little bit of the ground, or a little bit of a little bit of the ground, or a little bit of the ground, or a little bit of the ground
2022-03-23 09:56:07 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:56:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.219 | nll_loss 5.006 | ppl 32.13 | bleu 13.51 | wps 4328 | wpb 17862.2 | bsz 728.3 | num_updates 2351 | best_bleu 13.51
2022-03-23 09:56:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2351 updates
2022-03-23 09:56:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:56:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:56:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 15 @ 2351 updates, score 13.51) (writing took 1.8018293179920875 seconds)
2022-03-23 09:56:09 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-23 09:56:09 | INFO | train | epoch 015 | loss 9.357 | nll_loss 5.701 | ppl 52.04 | wps 42258.7 | ups 1.68 | wpb 25153.6 | bsz 1020.6 | num_updates 2351 | lr 0.000293875 | gnorm 0.63 | loss_scale 8 | train_wall 49 | gb_free 13.7 | wall 1425
2022-03-23 09:56:09 | INFO | fairseq.trainer | begin training epoch 16
2022-03-23 09:56:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:56:25 | INFO | train_inner | epoch 016:     49 / 157 loss=9.288, nll_loss=5.565, ppl=47.33, wps=34004.2, ups=1.32, wpb=25714, bsz=1049, num_updates=2400, lr=0.0003, gnorm=0.622, loss_scale=8, train_wall=31, gb_free=14.3, wall=1441
2022-03-23 09:56:57 | INFO | train_inner | epoch 016:    149 / 157 loss=9.231, nll_loss=5.448, ppl=43.66, wps=78098, ups=3.18, wpb=24560.8, bsz=958.1, num_updates=2500, lr=0.0003125, gnorm=0.507, loss_scale=8, train_wall=31, gb_free=14.1, wall=1473
2022-03-23 09:56:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:57:03 | INFO | fairseq.tasks.translation | example hypothesis: maira, you know, the image of this woman.
2022-03-23 09:57:03 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:57:07 | INFO | fairseq.tasks.translation | example hypothesis: now, it can be about about 80,000 places in the restaurant.
2022-03-23 09:57:07 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:57:11 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top corner, then you see a little dark point.
2022-03-23 09:57:11 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:57:15 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, it turns on japan, japan, and korea, the countries that are the united states.
2022-03-23 09:57:15 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:57:19 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there's a small dna on the specific cancancers that are actually actually in fact, in fact, in fact, the cell.
2022-03-23 09:57:19 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:57:23 | INFO | fairseq.tasks.translation | example hypothesis: and in the macy of how the responsibility was made for the responsibility, the number of animals, and that's a number of animals.
2022-03-23 09:57:23 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:57:27 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think paul paul paul's very nice, in a very nice case, he says, "he says," it's like our lives. "
2022-03-23 09:57:27 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:57:32 | INFO | fairseq.tasks.translation | example hypothesis: so, if we use the information that comes from this reflection, we can start with a traditional traditional face of the traditional face, and you can begin to start with a big shape of the potential of the body, and you can start through it, and you can get it through the whole structure, and you can get it through the whole structure.
2022-03-23 09:57:32 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:57:36 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that's interesting, and it's interesting for me to be in tedtedwomen, and that's... yes,... yes, it's the best thing that the best thing we said, "and then," if you're talking to you're talking about, "and then," and then we've got a long time. "
2022-03-23 09:57:36 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:57:38 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, it's still the mother of the invention, and a great design that we're going to use on on our airplane on our airplane, and that was a unique result that we had to solve a unique problem that we had to be able to solve the ground, and if you're able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to use the ground with the ground in the ground, or the ground, or the ground, or the ground with the ground, or the ground with the ground, or the ground.
2022-03-23 09:57:38 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:57:38 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.05 | nll_loss 4.671 | ppl 25.47 | bleu 16.82 | wps 4633 | wpb 17862.2 | bsz 728.3 | num_updates 2508 | best_bleu 16.82
2022-03-23 09:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2508 updates
2022-03-23 09:57:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:57:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:57:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 16 @ 2508 updates, score 16.82) (writing took 1.809595750994049 seconds)
2022-03-23 09:57:40 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-23 09:57:40 | INFO | train | epoch 016 | loss 9.218 | nll_loss 5.425 | ppl 42.97 | wps 43343.7 | ups 1.72 | wpb 25153.6 | bsz 1020.6 | num_updates 2508 | lr 0.0003135 | gnorm 0.533 | loss_scale 8 | train_wall 49 | gb_free 13.6 | wall 1516
2022-03-23 09:57:41 | INFO | fairseq.trainer | begin training epoch 17
2022-03-23 09:57:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:58:10 | INFO | train_inner | epoch 017:     92 / 157 loss=9.098, nll_loss=5.19, ppl=36.49, wps=34706.7, ups=1.36, wpb=25455.4, bsz=1051.8, num_updates=2600, lr=0.000325, gnorm=0.541, loss_scale=8, train_wall=31, gb_free=13.4, wall=1546
2022-03-23 09:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 09:58:34 | INFO | fairseq.tasks.translation | example hypothesis: maira, you know, the image, the woman woman's wife.
2022-03-23 09:58:34 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 09:58:38 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 8,000 places in the restaurant in the restaurant.
2022-03-23 09:58:38 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 09:58:42 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top corner, you see a little dark point.
2022-03-23 09:58:42 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 09:58:46 | INFO | fairseq.tasks.translation | example hypothesis: especially focus on japan, korea and australia, australia, countries, the united states are the united states.
2022-03-23 09:58:46 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 09:58:50 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are small pieces of dna on the special breast cancer that actually react on the cell.
2022-03-23 09:58:50 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 09:58:54 | INFO | fairseq.tasks.translation | example hypothesis: and in the mapping like the responsibility for the wild, the number of animals, and this has become a process of the animals.
2022-03-23 09:58:54 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 09:58:57 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think paul bubububububug was very nice to live in a very common say, "how we should live in our lives."
2022-03-23 09:58:57 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 09:59:01 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional face of traditional facial face, which is the very real shape of the information, and then we're able to get it through the whole structure of the whole structure, which is all the structure of this structure.
2022-03-23 09:59:01 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 09:59:06 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting and measure for me, for tedwomen, is that it's the best thing that somebody said, "well, it's the best thing that we're going to support you.
2022-03-23 09:59:06 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 09:59:08 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, the need is still the mother of the invention, and a big part of the design that we have to solve the aircraft, the result of it was a unique problems that we had to be connected to the ground.
2022-03-23 09:59:08 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 09:59:08 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.975 | nll_loss 4.493 | ppl 22.52 | bleu 18.16 | wps 4848.2 | wpb 17862.2 | bsz 728.3 | num_updates 2665 | best_bleu 18.16
2022-03-23 09:59:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2665 updates
2022-03-23 09:59:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:59:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 09:59:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 17 @ 2665 updates, score 18.16) (writing took 1.8482033020118251 seconds)
2022-03-23 09:59:10 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-23 09:59:10 | INFO | train | epoch 017 | loss 9.088 | nll_loss 5.169 | ppl 35.98 | wps 44095.6 | ups 1.75 | wpb 25153.6 | bsz 1020.6 | num_updates 2665 | lr 0.000333125 | gnorm 0.532 | loss_scale 8 | train_wall 49 | gb_free 13.8 | wall 1606
2022-03-23 09:59:10 | INFO | fairseq.trainer | begin training epoch 18
2022-03-23 09:59:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 09:59:21 | INFO | train_inner | epoch 018:     35 / 157 loss=9.034, nll_loss=5.064, ppl=33.45, wps=35109.4, ups=1.4, wpb=25072.8, bsz=991.1, num_updates=2700, lr=0.0003375, gnorm=0.495, loss_scale=8, train_wall=31, gb_free=13.9, wall=1618
2022-03-23 09:59:53 | INFO | train_inner | epoch 018:    135 / 157 loss=8.946, nll_loss=4.887, ppl=29.59, wps=81053.8, ups=3.13, wpb=25881.5, bsz=1031.7, num_updates=2800, lr=0.00035, gnorm=0.475, loss_scale=8, train_wall=32, gb_free=13.9, wall=1649
2022-03-23 09:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:00:04 | INFO | fairseq.tasks.translation | example hypothesis: maira, you know, the image of this woman's wife.
2022-03-23 10:00:04 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:00:08 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 8,000 places in the restaurant in the restaurant.
2022-03-23 10:00:08 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:00:12 | INFO | fairseq.tasks.translation | example hypothesis: and if you see the top left corner, you see a little dark point.
2022-03-23 10:00:12 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:00:16 | INFO | fairseq.tasks.translation | example hypothesis: especially focus on japan, korea, korea and australia, australia, the countries that are in the united states.
2022-03-23 10:00:16 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:00:20 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces of dna on the specific factors that are actually actually react to the cells that actually react on the cells.
2022-03-23 10:00:20 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:00:25 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, people like the responsibility for the wild responsibility, the number of life grew up again, and this is a foundation of the animals.
2022-03-23 10:00:25 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:00:29 | INFO | fairseq.tasks.translation | example hypothesis: and that's a idea that i find, paul bububububububububud in a very nice case that he says, "products are like we should live."
2022-03-23 10:00:29 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:00:33 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional face, we can begin to start with a traditional face of a traditional face, and the real shape of the information, and the most basic information, through the whole structure, the whole structure of this structure that all the structure and all the structure of this structure, and all the structure of this structure that we can get a structure.
2022-03-23 10:00:33 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:00:38 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting, and it's interesting to measure it and measure interesting for me to be here in tedwomen, "yeah,...... yeah, when it was the best thing that somebody said," somebody said, "somebody's going to give you a table," and then when we're going to do it, "] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["
2022-03-23 10:00:38 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:00:41 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, the need is still the mother of the invention, and a big part of the design of design that we're in our plane, is that we had to solve the unique problems that we had to be connected to the ground, to the ground -- everything that was connected to the ground -- and the mother of a big thing that we're still connected to a large part of a system, or a big part of the design, or a big part of the design of the design of the design of the design of the design of the design of the design of the design of the design, or the design, or a big things that we're going to see that we're going to use it is that we're going to use to use to use to use to use to use, or a large large large large large large, to use to see that we're going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to use,
2022-03-23 10:00:41 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:00:41 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.889 | nll_loss 4.322 | ppl 20 | bleu 19.82 | wps 4441 | wpb 17862.2 | bsz 728.3 | num_updates 2822 | best_bleu 19.82
2022-03-23 10:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2822 updates
2022-03-23 10:00:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 18 @ 2822 updates, score 19.82) (writing took 1.7820483579998836 seconds)
2022-03-23 10:00:43 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-23 10:00:43 | INFO | train | epoch 018 | loss 8.953 | nll_loss 4.902 | ppl 29.91 | wps 42561.8 | ups 1.69 | wpb 25153.6 | bsz 1020.6 | num_updates 2822 | lr 0.00035275 | gnorm 0.486 | loss_scale 8 | train_wall 49 | gb_free 14.2 | wall 1699
2022-03-23 10:00:43 | INFO | fairseq.trainer | begin training epoch 19
2022-03-23 10:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:01:08 | INFO | train_inner | epoch 019:     78 / 157 loss=8.868, nll_loss=4.734, ppl=26.62, wps=33580.8, ups=1.34, wpb=25030.7, bsz=1072.7, num_updates=2900, lr=0.0003625, gnorm=0.501, loss_scale=8, train_wall=30, gb_free=14.3, wall=1724
2022-03-23 10:01:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:01:37 | INFO | fairseq.tasks.translation | example hypothesis: maira, you know, the image of the woman.
2022-03-23 10:01:37 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:01:41 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 8,000 places in the restaurant in the restaurant.
2022-03-23 10:01:41 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:01:45 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top left corner, you see a little dark point.
2022-03-23 10:01:45 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:01:49 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly particularly focused on japan, korea, korea and australia, australia, countries who are connected to the united states.
2022-03-23 10:01:49 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:01:53 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces on the specific genes that are actually actually react to the extra cellular matrix.
2022-03-23 10:01:53 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:01:57 | INFO | fairseq.tasks.translation | example hypothesis: and in the mack like the responsibility for the wild, the number of animals grew up again, and that's a foundation for the wild animals.
2022-03-23 10:01:57 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:02:02 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think paul bububububububububububutteran has been very nice in a recent esessay, "are products like we should live," how we should live our lives. "
2022-03-23 10:02:02 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:02:07 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection of reflection, we can start with a traditional face that can start with a traditional face, the big concrete of the great constructions and the real shape, and the shape of the information, through the information, which is the whole structure and the structure.
2022-03-23 10:02:07 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:02:11 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting and measure it, for me to be here at tedwomen, is that... well, in fact, in the best one of the reasons, when someone said, "you know," the men, and then you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, the truth, you know, you know, you know, you know, you know, you know, you know, you have a long, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know,
2022-03-23 10:02:11 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:02:14 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, the mother is still the invention of the invention, and a big part of the design work that we're at our plane, was a little bit of the aircraft, which is a result that we had to solve the unique problems that were connected to the ground -- and it's all the variation of a specific system, if you're going to be able to see that if you're going to be able to use the aircraft, and you're going to use a mechanism, or a mechanism, or the aircraft, if you're going to see that you're going to be able to use it's the aircraft, or the aircraft, you're going to see that you're going to be able to be able to see that you're going to be able to be able to use the aircraft, you're going to be able to use the aircraft, if you're in our plane, you're going on the aircraft, you're on the aircraft, you're on the aircraft, or the aircraft
2022-03-23 10:02:14 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:02:14 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.777 | nll_loss 4.065 | ppl 16.74 | bleu 21.9 | wps 4452.3 | wpb 17862.2 | bsz 728.3 | num_updates 2979 | best_bleu 21.9
2022-03-23 10:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 2979 updates
2022-03-23 10:02:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:02:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:02:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 19 @ 2979 updates, score 21.9) (writing took 1.9070093460031785 seconds)
2022-03-23 10:02:16 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-23 10:02:16 | INFO | train | epoch 019 | loss 8.86 | nll_loss 4.718 | ppl 26.32 | wps 42455.2 | ups 1.69 | wpb 25153.6 | bsz 1020.6 | num_updates 2979 | lr 0.000372375 | gnorm 0.496 | loss_scale 8 | train_wall 49 | gb_free 14.7 | wall 1792
2022-03-23 10:02:16 | INFO | fairseq.trainer | begin training epoch 20
2022-03-23 10:02:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:02:23 | INFO | train_inner | epoch 020:     21 / 157 loss=8.85, nll_loss=4.697, ppl=25.94, wps=32683.1, ups=1.34, wpb=24435, bsz=964.7, num_updates=3000, lr=0.000375, gnorm=0.497, loss_scale=8, train_wall=31, gb_free=14.3, wall=1799
2022-03-23 10:02:54 | INFO | train_inner | epoch 020:    121 / 157 loss=8.797, nll_loss=4.594, ppl=24.16, wps=78966, ups=3.18, wpb=24836.8, bsz=1022.4, num_updates=3100, lr=0.0003875, gnorm=0.513, loss_scale=8, train_wall=31, gb_free=13.5, wall=1830
2022-03-23 10:03:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:03:09 | INFO | fairseq.tasks.translation | example hypothesis: mairira, the image woman.
2022-03-23 10:03:09 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:03:13 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 8,000 places in the restaurant.
2022-03-23 10:03:13 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:03:17 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top left corner, you see a little dark point.
2022-03-23 10:03:17 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:03:20 | INFO | fairseq.tasks.translation | example hypothesis: it's very particularly focused on japan, korea, and australia, countries that are interconnected.
2022-03-23 10:03:20 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:03:24 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces on the specific factors that actually react to the extra cellular matrix.
2022-03-23 10:03:24 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:03:28 | INFO | fairseq.tasks.translation | example hypothesis: and in the mapping like the people who used responsibility for the wild animals, and this is a basis for the natural protection.
2022-03-23 10:03:28 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:03:32 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea that i think paul buburean has been very nice in a recent say, "products are like we should live."
2022-03-23 10:03:32 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:03:36 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional face.
2022-03-23 10:03:36 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:03:40 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting, and then, for me, "is that... well, at the top, when somebody said to you."
2022-03-23 10:03:40 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:03:40 | INFO | fairseq.tasks.translation | example hypothesis: luluckily, the mother of invention, and a big part of the design that we're in the plane.
2022-03-23 10:03:40 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:03:40 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.756 | nll_loss 4.029 | ppl 16.32 | bleu 20.11 | wps 5286.8 | wpb 17862.2 | bsz 728.3 | num_updates 3136 | best_bleu 21.9
2022-03-23 10:03:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3136 updates
2022-03-23 10:03:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:03:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:03:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 20 @ 3136 updates, score 20.11) (writing took 0.8310153260244988 seconds)
2022-03-23 10:03:41 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-23 10:03:41 | INFO | train | epoch 020 | loss 8.78 | nll_loss 4.561 | ppl 23.61 | wps 46062.4 | ups 1.83 | wpb 25153.6 | bsz 1020.6 | num_updates 3136 | lr 0.000392 | gnorm 0.476 | loss_scale 8 | train_wall 49 | gb_free 14.4 | wall 1877
2022-03-23 10:03:42 | INFO | fairseq.trainer | begin training epoch 21
2022-03-23 10:03:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:04:02 | INFO | train_inner | epoch 021:     64 / 157 loss=8.731, nll_loss=4.465, ppl=22.09, wps=37449.4, ups=1.47, wpb=25399, bsz=960.1, num_updates=3200, lr=0.0004, gnorm=0.404, loss_scale=8, train_wall=31, gb_free=13.9, wall=1898
2022-03-23 10:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:04:35 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the image woman who was doing.
2022-03-23 10:04:35 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:04:39 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 8,000 places in the restaurant.
2022-03-23 10:04:39 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:04:43 | INFO | fairseq.tasks.translation | example hypothesis: and if you see the top left corner, you see a little dark point.
2022-03-23 10:04:43 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:04:46 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focus on japan, korea and australia, countries that are connected to the united states.
2022-03-23 10:04:46 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:04:50 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces on the specific fish that actually respond to the extra cellular matrix matrix matrix matrix.
2022-03-23 10:04:50 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:04:54 | INFO | fairseq.tasks.translation | example hypothesis: and in the masteribia, people like the responsibility for the wild, grew up the number of wild animals, and this is a basis for the natural protection in namibia.
2022-03-23 10:04:54 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:04:59 | INFO | fairseq.tasks.translation | example hypothesis: and this is a idea of how i think paul bubuan was very nice in a recent essay that he says, "products for how we should live with our lives."
2022-03-23 10:04:59 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:05:03 | INFO | fairseq.tasks.translation | example hypothesis: so, if we use the information that comes from this reflection, we can start with a traditional facial face, the big concrete of the face and the basic shape of the face, and the basic shape of the information that we use, the entire portion of this reflection, the whole structure, the whole structure of this reflection, the whole structure, the whole structure of this reflection, the whole structure that's all the whole structure,
2022-03-23 10:05:03 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:05:08 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting, and then, for me to be here at tedwomen, is that... well, at the best one of the best, when someone said, "as somebody said," the way to say, "the men, and say it's interesting for a table and say," the revolution, "the truth is that we've been supported to be here."
2022-03-23 10:05:08 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:05:10 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother is still the invention of invention, and a big part of design work that we're using on the aircraft, which is a result that we had to solve the unique problems that we had to solve the unique problems so that it was connected to the ground -- and it's all a great part of the continents, and a large part of the design system that we're going to be able to be able to be able to be able to be able to be able to be able to be able to solve with an aircraft in the aircraft, or an aircraft, or an aircraft that we're able to be able to be able to be able to be able to be able to solve with an aircraft, or an aircraft, or a particular refrightension that we're able to be able to be able to be able to solve with an aircraft, to solve in the aircraft, to solve with a particular refrightension that we're able to solve with a particular refrightension
2022-03-23 10:05:10 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:05:10 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.672 | nll_loss 3.872 | ppl 14.64 | bleu 23.65 | wps 4656.2 | wpb 17862.2 | bsz 728.3 | num_updates 3293 | best_bleu 23.65
2022-03-23 10:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3293 updates
2022-03-23 10:05:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:05:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 21 @ 3293 updates, score 23.65) (writing took 1.9974008309654891 seconds)
2022-03-23 10:05:12 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-23 10:05:12 | INFO | train | epoch 021 | loss 8.683 | nll_loss 4.375 | ppl 20.75 | wps 43405.7 | ups 1.73 | wpb 25153.6 | bsz 1020.6 | num_updates 3293 | lr 0.000411625 | gnorm 0.431 | loss_scale 8 | train_wall 49 | gb_free 14.7 | wall 1968
2022-03-23 10:05:13 | INFO | fairseq.trainer | begin training epoch 22
2022-03-23 10:05:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:05:15 | INFO | train_inner | epoch 022:      7 / 157 loss=8.659, nll_loss=4.331, ppl=20.12, wps=34668.5, ups=1.37, wpb=25331.2, bsz=1074.2, num_updates=3300, lr=0.0004125, gnorm=0.445, loss_scale=8, train_wall=31, gb_free=13.5, wall=1971
2022-03-23 10:05:47 | INFO | train_inner | epoch 022:    107 / 157 loss=8.613, nll_loss=4.24, ppl=18.9, wps=79556.2, ups=3.15, wpb=25281.9, bsz=1058.1, num_updates=3400, lr=0.000425, gnorm=0.427, loss_scale=8, train_wall=31, gb_free=13.9, wall=2003
2022-03-23 10:06:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:06:06 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the image woman.
2022-03-23 10:06:06 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:06:10 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 8,000 places in the restaurant.
2022-03-23 10:06:10 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:06:14 | INFO | fairseq.tasks.translation | example hypothesis: and if you see on the top left corner, you see a little dark point.
2022-03-23 10:06:14 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:06:18 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focus on japan, korea and australia, countries that are connected to the united states.
2022-03-23 10:06:18 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:06:22 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces of the breast factors that actually react to the extra cellular matrix.
2022-03-23 10:06:22 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:06:25 | INFO | fairseq.tasks.translation | example hypothesis: and in the mashit like people who had responsibility for the wild, grew up the number of wild animals, and that's a basis for the natural protection in namibia.
2022-03-23 10:06:25 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:06:29 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul bubullan has put very nice in a recent essay that he says, "products are living arguments of how we should live our lives."
2022-03-23 10:06:29 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:06:33 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection of reflection, we can start with a traditional face, the big concrete of the face and the basic shape of the face, and through that information, which is the entire ports and fold.
2022-03-23 10:06:33 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:06:37 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's very interesting and measure it for me to be here at tedwomen, is that -- tweaked at the best than someone said, "wade you to the men in a table and the revolution that starts to help you."
2022-03-23 10:06:37 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:06:39 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we were at our airplane, was a result that we had to solve the unique problems that were connected to the ground -- everything from a continued to refrigerate and refrightening that if you're going to use it in the aircraft.
2022-03-23 10:06:39 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:06:39 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.589 | nll_loss 3.687 | ppl 12.88 | bleu 24.8 | wps 5037.6 | wpb 17862.2 | bsz 728.3 | num_updates 3450 | best_bleu 24.8
2022-03-23 10:06:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3450 updates
2022-03-23 10:06:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:06:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:06:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 22 @ 3450 updates, score 24.8) (writing took 1.8452528270427138 seconds)
2022-03-23 10:06:40 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-23 10:06:40 | INFO | train | epoch 022 | loss 8.611 | nll_loss 4.236 | ppl 18.84 | wps 44790 | ups 1.78 | wpb 25153.6 | bsz 1020.6 | num_updates 3450 | lr 0.00043125 | gnorm 0.413 | loss_scale 8 | train_wall 49 | gb_free 14.3 | wall 2057
2022-03-23 10:06:41 | INFO | fairseq.trainer | begin training epoch 23
2022-03-23 10:06:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:06:57 | INFO | train_inner | epoch 023:     50 / 157 loss=8.538, nll_loss=4.1, ppl=17.14, wps=36186.7, ups=1.42, wpb=25464.9, bsz=1071.3, num_updates=3500, lr=0.0004375, gnorm=0.393, loss_scale=8, train_wall=31, gb_free=13.8, wall=2073
2022-03-23 10:07:29 | INFO | train_inner | epoch 023:    150 / 157 loss=8.587, nll_loss=4.194, ppl=18.3, wps=78686.8, ups=3.18, wpb=24747.1, bsz=959.6, num_updates=3600, lr=0.00045, gnorm=0.418, loss_scale=8, train_wall=31, gb_free=13.7, wall=2105
2022-03-23 10:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:07:34 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the image woman.
2022-03-23 10:07:34 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:07:38 | INFO | fairseq.tasks.translation | example hypothesis: it can be about 8,000 places in the restaurant.
2022-03-23 10:07:38 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:07:42 | INFO | fairseq.tasks.translation | example hypothesis: and if you see the top left corner, you see a little dark point.
2022-03-23 10:07:42 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:07:46 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are connected to the united states.
2022-03-23 10:07:46 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:07:49 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces of breast factors that actually respond to the extracellular matrix matrix matrix.
2022-03-23 10:07:49 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:07:53 | INFO | fairseq.tasks.translation | example hypothesis: and in the mack like the people who did responsibility for the wild animals. and this is a basis for the natural protection.
2022-03-23 10:07:53 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:07:56 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul buchanan was very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:07:56 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:08:00 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial face and the basic shape of the face, and the basic form of the face.
2022-03-23 10:08:00 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:08:02 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's highly interesting to me in tedwomen, and then we love "--" well, "for a long time."
2022-03-23 10:08:02 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:08:03 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we're on the plane, or a result of it is that we had to solve the unique problems that were connected to the ground -- all of a continually variation.
2022-03-23 10:08:03 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:08:03 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.6 | nll_loss 3.768 | ppl 13.63 | bleu 20.02 | wps 5654.3 | wpb 17862.2 | bsz 728.3 | num_updates 3607 | best_bleu 24.8
2022-03-23 10:08:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3607 updates
2022-03-23 10:08:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:08:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:08:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 23 @ 3607 updates, score 20.02) (writing took 0.8591386099578813 seconds)
2022-03-23 10:08:04 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-23 10:08:04 | INFO | train | epoch 023 | loss 8.557 | nll_loss 4.137 | ppl 17.59 | wps 47079.4 | ups 1.87 | wpb 25153.6 | bsz 1020.6 | num_updates 3607 | lr 0.000450875 | gnorm 0.419 | loss_scale 8 | train_wall 49 | gb_free 14.7 | wall 2140
2022-03-23 10:08:05 | INFO | fairseq.trainer | begin training epoch 24
2022-03-23 10:08:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:08:34 | INFO | train_inner | epoch 024:     93 / 157 loss=8.483, nll_loss=3.994, ppl=15.93, wps=38819, ups=1.52, wpb=25500.2, bsz=1051.8, num_updates=3700, lr=0.0004625, gnorm=0.388, loss_scale=8, train_wall=31, gb_free=14.7, wall=2170
2022-03-23 10:08:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:08:58 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the image woman.
2022-03-23 10:08:58 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:09:02 | INFO | fairseq.tasks.translation | example hypothesis: over year, he can talk about 8,000 places in the restaurant.
2022-03-23 10:09:02 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:09:06 | INFO | fairseq.tasks.translation | example hypothesis: and if you see the top left corner, you see a little dark point.
2022-03-23 10:09:06 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:09:10 | INFO | fairseq.tasks.translation | example hypothesis: in particular, it's focused on japan, korea and australia, countries, which are the enabled of the united states.
2022-03-23 10:09:10 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:09:14 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces of dna on the particular one of the breast expressions that actually respond to the extracellular matrix.
2022-03-23 10:09:14 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:09:18 | INFO | fairseq.tasks.translation | example hypothesis: and in the mapping of how people were taking responsibility for the wild, the number of wild animals grew back, and that's a basis for the natural protection in namibia.
2022-03-23 10:09:18 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:09:22 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i find, paul richard buchanan was very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:09:22 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:09:26 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this reflection of reflection, we can start with a traditional face, the big constructions of the face, and the basic shape of information that refits all the ports and fold it through the whole structure.
2022-03-23 10:09:26 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:09:31 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's been interesting and measuring it to be here at tedwomen is that -- tyeah, when it's been put together the best thing when someone said, "turn you to the men on a table and say," if the revolution starts to support you, "you know," the truth is that we've been supported to you, "in this time."
2022-03-23 10:09:31 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:09:33 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the invention, and a big part of the design work that we are at our plane, was a result that we had to solve the unique problems that were connected to the ground -- everything from a continuous variation and a refrigering system, and it allows us to use the refrigeration of a refrightening machine, and if you're going to go to the ground, if you're in the ground, if you're going to be able to be able to see the ground, or if you're going to be able to be able to be able to be able to be able to be able to be able to refrightened to refrightened to refrightened to be able to refrightened to be able to be able to refrightened to be able to be able to refrightened, if you're in a refrightened, if you're going to be able to be able to
2022-03-23 10:09:33 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:09:33 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.448 | nll_loss 3.476 | ppl 11.12 | bleu 27.61 | wps 4714.8 | wpb 17862.2 | bsz 728.3 | num_updates 3764 | best_bleu 27.61
2022-03-23 10:09:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3764 updates
2022-03-23 10:09:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:09:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:09:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 24 @ 3764 updates, score 27.61) (writing took 1.8269788129837252 seconds)
2022-03-23 10:09:35 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-23 10:09:35 | INFO | train | epoch 024 | loss 8.495 | nll_loss 4.017 | ppl 16.19 | wps 43549.5 | ups 1.73 | wpb 25153.6 | bsz 1020.6 | num_updates 3764 | lr 0.0004705 | gnorm 0.396 | loss_scale 8 | train_wall 49 | gb_free 14.3 | wall 2231
2022-03-23 10:09:35 | INFO | fairseq.trainer | begin training epoch 25
2022-03-23 10:09:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:09:47 | INFO | train_inner | epoch 025:     36 / 157 loss=8.47, nll_loss=3.972, ppl=15.69, wps=33941.2, ups=1.38, wpb=24644.8, bsz=1024.2, num_updates=3800, lr=0.000475, gnorm=0.407, loss_scale=8, train_wall=31, gb_free=13.4, wall=2243
2022-03-23 10:10:19 | INFO | train_inner | epoch 025:    136 / 157 loss=8.438, nll_loss=3.911, ppl=15.04, wps=79877.6, ups=3.15, wpb=25372.2, bsz=1002.1, num_updates=3900, lr=0.0004875, gnorm=0.37, loss_scale=8, train_wall=31, gb_free=13.8, wall=2275
2022-03-23 10:10:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:10:29 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the trained woman.
2022-03-23 10:10:29 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:10:33 | INFO | fairseq.tasks.translation | example hypothesis: he can translate about 8,000 places in the restaurant.
2022-03-23 10:10:33 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:10:36 | INFO | fairseq.tasks.translation | example hypothesis: and if you see on the upper left corner, you see a little dark point.
2022-03-23 10:10:36 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:10:40 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are connected in the united states.
2022-03-23 10:10:40 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:10:44 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces of breast expressions that actually respond to extracellular matrix.
2022-03-23 10:10:44 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:10:48 | INFO | fairseq.tasks.translation | example hypothesis: and in the mammals like the people's responsibility for the wild, the number of wild animals grew back again, and that's a basis for conservation in namibia.
2022-03-23 10:10:48 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:10:52 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul buchanan was very nice in a current essay where he says, "products are alive arguments for how we should live our lives."
2022-03-23 10:10:52 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:10:56 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this reflection, we can start with a traditional facial, which is the big configuration of the face and the basic shape, and restore it through the denial information, which is the whole portion structure.
2022-03-23 10:10:56 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:10:59 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that's going to be highly interesting and measured to me at tedwomen, is that... tyes, when he said, "turn you to the men on a table and say," when the revolution starts, "well, we're going to support you."
2022-03-23 10:10:59 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:11:00 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of the invention, and a great part of the design work that we're on the stones on our airplane, was a result that we had to solve the unique problems that were connected to it to the ground -- everything from a continuous variation and refrigerator, and that allows us to use an aircraft.
2022-03-23 10:11:00 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:11:00 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.433 | nll_loss 3.409 | ppl 10.62 | bleu 27.47 | wps 5204.5 | wpb 17862.2 | bsz 728.3 | num_updates 3921 | best_bleu 27.61
2022-03-23 10:11:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3921 updates
2022-03-23 10:11:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 25 @ 3921 updates, score 27.47) (writing took 0.8860454629757442 seconds)
2022-03-23 10:11:01 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-23 10:11:01 | INFO | train | epoch 025 | loss 8.429 | nll_loss 3.895 | ppl 14.88 | wps 45830.9 | ups 1.82 | wpb 25153.6 | bsz 1020.6 | num_updates 3921 | lr 0.000490125 | gnorm 0.371 | loss_scale 8 | train_wall 49 | gb_free 14 | wall 2317
2022-03-23 10:11:02 | INFO | fairseq.trainer | begin training epoch 26
2022-03-23 10:11:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:11:27 | INFO | train_inner | epoch 026:     79 / 157 loss=8.395, nll_loss=3.83, ppl=14.22, wps=36461.3, ups=1.47, wpb=24726, bsz=997.6, num_updates=4000, lr=0.0005, gnorm=0.366, loss_scale=8, train_wall=31, gb_free=14, wall=2343
2022-03-23 10:11:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:11:55 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the image woman.
2022-03-23 10:11:55 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:11:59 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can take about 8,000 places in the restaurant.
2022-03-23 10:11:59 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:12:03 | INFO | fairseq.tasks.translation | example hypothesis: and if you see on the upper left corner, you see a little dark point.
2022-03-23 10:12:03 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:12:06 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are endangered by the united states.
2022-03-23 10:12:06 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:12:10 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of breast factors that actually respond to the extracellular matrix.
2022-03-23 10:12:10 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:12:14 | INFO | fairseq.tasks.translation | example hypothesis: and in the mapping like the people's responsibility for the wild, the number of wild animals grew back, and that's a basis for conservation in namibia.
2022-03-23 10:12:14 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:12:18 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think, paul richard buchanan was very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:12:18 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:12:22 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can, which gives the big configuration of the face and the basic form of the face, and restore it through the whole structure and all the fungs.
2022-03-23 10:12:22 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:12:27 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons it's been interesting and measured to be here for me at tedwomen, is that... tyes, when it was best summared when someone said, "turn you to the men on a table and say," if the revolution starts to support you. "
2022-03-23 10:12:27 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:12:29 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the invention, and a big part of the design work that we're on our plane at the stest, was a result that we had to solve the unique problems that were connected to it on the ground -- everything from a continuous variation, and a system of refrigeration, which allows us to use a refrigerator in the aircraft, if you're either to use a refrigerator, or you're in the aircraft, or you can see that you can use a refrigerator, if you can use it, if you're in a specific, or you can use a system that you can use it, or you can use a car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car, and you can use a system, if you can use a system, if you can use a system, if you're
2022-03-23 10:12:29 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:12:29 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.385 | nll_loss 3.339 | ppl 10.12 | bleu 29.07 | wps 4813.8 | wpb 17862.2 | bsz 728.3 | num_updates 4078 | best_bleu 29.07
2022-03-23 10:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4078 updates
2022-03-23 10:12:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:12:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 26 @ 4078 updates, score 29.07) (writing took 1.7987217319896445 seconds)
2022-03-23 10:12:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-23 10:12:31 | INFO | train | epoch 026 | loss 8.39 | nll_loss 3.821 | ppl 14.13 | wps 43994.4 | ups 1.75 | wpb 25153.6 | bsz 1020.6 | num_updates 4078 | lr 0.000495195 | gnorm 0.371 | loss_scale 8 | train_wall 49 | gb_free 13.8 | wall 2407
2022-03-23 10:12:31 | INFO | fairseq.trainer | begin training epoch 27
2022-03-23 10:12:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:12:38 | INFO | train_inner | epoch 027:     22 / 157 loss=8.388, nll_loss=3.818, ppl=14.1, wps=35478.2, ups=1.39, wpb=25456.2, bsz=1016.5, num_updates=4100, lr=0.000493865, gnorm=0.38, loss_scale=8, train_wall=31, gb_free=14.8, wall=2414
2022-03-23 10:13:10 | INFO | train_inner | epoch 027:    122 / 157 loss=8.348, nll_loss=3.745, ppl=13.41, wps=79619.4, ups=3.14, wpb=25318.3, bsz=1001.8, num_updates=4200, lr=0.00048795, gnorm=0.343, loss_scale=8, train_wall=31, gb_free=13.5, wall=2446
2022-03-23 10:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:13:25 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, who trained woman.
2022-03-23 10:13:25 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:13:28 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can protect about 8,000 places in the restaurant.
2022-03-23 10:13:28 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:13:32 | INFO | fairseq.tasks.translation | example hypothesis: and if you see on the upper left corner, you can see a little dark point.
2022-03-23 10:13:32 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:13:36 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are endangered of the united states.
2022-03-23 10:13:36 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:13:40 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific of the breast expressions that are actually responding to the extracellular matrix.
2022-03-23 10:13:40 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:13:44 | INFO | fairseq.tasks.translation | example hypothesis: and it's like the people who took responsibility for the wild, the number of wild animals grew back, and it has become a basis for conservation in namibia.
2022-03-23 10:13:44 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:13:48 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul buchanan put very nice in a current essay where he says, "products are living arguments for how to live our lives."
2022-03-23 10:13:48 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:13:52 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can, which gives the big configurations of the face and the basic form of information that refers all the porports.
2022-03-23 10:13:52 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:13:55 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it was really interesting and measured for me here at tedwomen, is that... tyes, it was the best thing when someone said, "turn you to your men at your table and say," if the revolution starts. "
2022-03-23 10:13:55 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:13:57 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother of invention is still the invention, and a big part of the design work that we're at our airplane on the stumber was a result that we had to solve the unique problems that were connected to it on the ground -- everything from a continuous variation and refrigerating system that allows us to use an aircraft in the ground, or a specific traffic system that allows us to use.
2022-03-23 10:13:57 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:13:57 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.386 | nll_loss 3.332 | ppl 10.07 | bleu 28.47 | wps 5104.9 | wpb 17862.2 | bsz 728.3 | num_updates 4235 | best_bleu 29.07
2022-03-23 10:13:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4235 updates
2022-03-23 10:13:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:13:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 27 @ 4235 updates, score 28.47) (writing took 0.8025039010099135 seconds)
2022-03-23 10:13:58 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-23 10:13:58 | INFO | train | epoch 027 | loss 8.342 | nll_loss 3.733 | ppl 13.3 | wps 45586.6 | ups 1.81 | wpb 25153.6 | bsz 1020.6 | num_updates 4235 | lr 0.00048593 | gnorm 0.355 | loss_scale 8 | train_wall 49 | gb_free 13.3 | wall 2494
2022-03-23 10:13:58 | INFO | fairseq.trainer | begin training epoch 28
2022-03-23 10:13:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:14:19 | INFO | train_inner | epoch 028:     65 / 157 loss=8.323, nll_loss=3.699, ppl=12.99, wps=36101.1, ups=1.46, wpb=24779.6, bsz=996.8, num_updates=4300, lr=0.000482243, gnorm=0.355, loss_scale=8, train_wall=31, gb_free=14.7, wall=2515
2022-03-23 10:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:14:51 | INFO | fairseq.tasks.translation | example hypothesis: maira calcalman, the trained woman.
2022-03-23 10:14:51 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:14:55 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can take about 8,000 places in the restaurant.
2022-03-23 10:14:55 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:14:59 | INFO | fairseq.tasks.translation | example hypothesis: and if you see on the upper left corner, you see a little dark point.
2022-03-23 10:14:59 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:15:03 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are endangered in the united states.
2022-03-23 10:15:03 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:15:07 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific species of breast expressions that actually respond to the extracellular matrix.
2022-03-23 10:15:07 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:15:11 | INFO | fairseq.tasks.translation | example hypothesis: and in the mapping like the people's responsibility for the wild, the number of wild animals grew back again, and this has become a basis for the conservation in namibia.
2022-03-23 10:15:11 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:15:15 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul buchanan was very nice in a recent essay where he says, "products are living arguments of how we should live our lives."
2022-03-23 10:15:15 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:15:19 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can, which gives the big configurations of the face and the basic shape of the face, and refers it through the kind of information that the whole portion structure and all the fits.
2022-03-23 10:15:19 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:15:22 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it interesting and measuring to me here at tedwomen is that... well, at the dinner dinner, it was best summarized when someone said, "turn you to the men at your table and say," if the revolution starts to help you. "
2022-03-23 10:15:22 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:15:24 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we're on our airplane on the pristine was a result that we had to solve the unique problems that were connected to operations on the ground -- everything from a continuously variable and a refrigerated system that allows us to use an aircraft to either use the most unique problems that we had to be able to solve the ground.
2022-03-23 10:15:24 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:15:24 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.325 | nll_loss 3.233 | ppl 9.4 | bleu 29.84 | wps 5068.3 | wpb 17862.2 | bsz 728.3 | num_updates 4392 | best_bleu 29.84
2022-03-23 10:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4392 updates
2022-03-23 10:15:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:15:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:15:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 28 @ 4392 updates, score 29.84) (writing took 1.8798214690177701 seconds)
2022-03-23 10:15:26 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-23 10:15:26 | INFO | train | epoch 028 | loss 8.284 | nll_loss 3.626 | ppl 12.35 | wps 44862.1 | ups 1.78 | wpb 25153.6 | bsz 1020.6 | num_updates 4392 | lr 0.000477165 | gnorm 0.316 | loss_scale 8 | train_wall 49 | gb_free 13.4 | wall 2582
2022-03-23 10:15:26 | INFO | fairseq.trainer | begin training epoch 29
2022-03-23 10:15:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:15:29 | INFO | train_inner | epoch 029:      8 / 157 loss=8.246, nll_loss=3.556, ppl=11.76, wps=36280.4, ups=1.43, wpb=25354.5, bsz=1100.2, num_updates=4400, lr=0.000476731, gnorm=0.298, loss_scale=8, train_wall=31, gb_free=13.7, wall=2585
2022-03-23 10:16:01 | INFO | train_inner | epoch 029:    108 / 157 loss=8.29, nll_loss=3.639, ppl=12.45, wps=80606.9, ups=3.13, wpb=25722.7, bsz=969.7, num_updates=4500, lr=0.000471405, gnorm=0.343, loss_scale=8, train_wall=31, gb_free=13.5, wall=2617
2022-03-23 10:16:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:16:20 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the trained woman.
2022-03-23 10:16:20 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:16:23 | INFO | fairseq.tasks.translation | example hypothesis: over the year he can take about 8,000 places in the restaurant.
2022-03-23 10:16:23 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:16:27 | INFO | fairseq.tasks.translation | example hypothesis: and if you see the upper left corner, you see a little dark point.
2022-03-23 10:16:27 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:16:31 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are enabled to the united states.
2022-03-23 10:16:31 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:16:35 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces of breast expressions that are actually responding to the extracellular matrix.
2022-03-23 10:16:35 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:16:39 | INFO | fairseq.tasks.translation | example hypothesis: and in the mapping of how people took responsibility for the wild, the number of wild animals grew back, and that's become a basis for conservation in namibia.
2022-03-23 10:16:39 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:16:43 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan has been very nice in a current essay where he says, "products are living arguments for how to live our lives."
2022-03-23 10:16:43 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:16:47 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can that gives the gross constructions of the face and restore the basic shape, and refuse it through the front of the information that pulls the entire portion structure and all the fits.
2022-03-23 10:16:47 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:16:52 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that's highly interesting and measuring me to be here at tedwomen is that... tyes, when dinner was best summarized when somebody said, "turn you to the men on your table and tell you, 'if the revolution starts to support you.' the truth is women, we've already started to support you at this long time with silly spring."
2022-03-23 10:16:52 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:16:54 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we are at our airplane at the stack, was a result that we had to solve the unique problems that were connected to operating it on the ground -- everything from a continuous variation and a refrigeration system that allows us to use an aircraft to go and use it in particular traffic, if you're in the ground, or if you have to see the most accuracy, or you're going to see the most accuracy of a constitulation of a constitulation of a constant, or if you're in your car car car car car car car car car, or if you're going to the wrong, or if you're in your car car car car car car car car car car car car car car, or if you're in the earth, or if you're going to the wrong, or if you're in your car, or you're in your car, it, or you're going to the
2022-03-23 10:16:54 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:16:54 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.291 | nll_loss 3.175 | ppl 9.03 | bleu 30.19 | wps 4751.4 | wpb 17862.2 | bsz 728.3 | num_updates 4549 | best_bleu 30.19
2022-03-23 10:16:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4549 updates
2022-03-23 10:16:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:16:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 29 @ 4549 updates, score 30.19) (writing took 1.870910959027242 seconds)
2022-03-23 10:16:56 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-23 10:16:56 | INFO | train | epoch 029 | loss 8.261 | nll_loss 3.585 | ppl 12 | wps 43715.9 | ups 1.74 | wpb 25153.6 | bsz 1020.6 | num_updates 4549 | lr 0.000468859 | gnorm 0.331 | loss_scale 8 | train_wall 48 | gb_free 13.4 | wall 2672
2022-03-23 10:16:56 | INFO | fairseq.trainer | begin training epoch 30
2022-03-23 10:16:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:17:13 | INFO | train_inner | epoch 030:     51 / 157 loss=8.216, nll_loss=3.499, ppl=11.31, wps=34305.4, ups=1.38, wpb=24771, bsz=1034.2, num_updates=4600, lr=0.000466252, gnorm=0.301, loss_scale=8, train_wall=31, gb_free=13.8, wall=2689
2022-03-23 10:17:44 | INFO | train_inner | epoch 030:    151 / 157 loss=8.216, nll_loss=3.503, ppl=11.33, wps=80104.9, ups=3.18, wpb=25196.8, bsz=1046.6, num_updates=4700, lr=0.000461266, gnorm=0.325, loss_scale=8, train_wall=31, gb_free=14.6, wall=2720
2022-03-23 10:17:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:17:50 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the trained woman.
2022-03-23 10:17:50 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:17:53 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can talk about 8,000 places in the restaurant.
2022-03-23 10:17:53 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:17:57 | INFO | fairseq.tasks.translation | example hypothesis: and if you see the upper left corner, you see a little dark dot.
2022-03-23 10:17:57 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:18:01 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tied in the united states.
2022-03-23 10:18:01 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:18:05 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the special breast expressions that are actually responding to the extracellular matrix.
2022-03-23 10:18:05 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:18:09 | INFO | fairseq.tasks.translation | example hypothesis: and in the mess of how people took responsibility for the wild, the number of wild animals grew back, and this has become a basis for conservation in namibia.
2022-03-23 10:18:09 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:18:13 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan wrote very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:18:13 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:18:17 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this mirror reflection, we can start with a traditional facial can, which gives the gross constructions of the face and the basic shape, and recovers it through the one of the things that refers the whole portion structure and all the fone.
2022-03-23 10:18:17 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:18:21 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it really interesting and measured to me here at tedwomen is that -- well, at the dinner, it was best summarized when someone said, "turn you to the men on your table and tell you," when the revolution begins, we support you. "the truth is that we've already supported you for this long time."
2022-03-23 10:18:21 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:18:23 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we are at our plane, was a result that we had to solve the unique problems that were connected to the ground -- everything from a continuous variation, and a refrigerator that allows us to use an aircraft, until you're using it, or if you're in the ground.
2022-03-23 10:18:23 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:18:23 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.274 | nll_loss 3.149 | ppl 8.87 | bleu 30.49 | wps 4937.1 | wpb 17862.2 | bsz 728.3 | num_updates 4706 | best_bleu 30.49
2022-03-23 10:18:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4706 updates
2022-03-23 10:18:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:18:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:18:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 30 @ 4706 updates, score 30.49) (writing took 1.927943552029319 seconds)
2022-03-23 10:18:25 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-23 10:18:25 | INFO | train | epoch 030 | loss 8.215 | nll_loss 3.5 | ppl 11.31 | wps 44416.9 | ups 1.77 | wpb 25153.6 | bsz 1020.6 | num_updates 4706 | lr 0.000460971 | gnorm 0.315 | loss_scale 8 | train_wall 49 | gb_free 14.4 | wall 2761
2022-03-23 10:18:25 | INFO | fairseq.trainer | begin training epoch 31
2022-03-23 10:18:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:18:55 | INFO | train_inner | epoch 031:     94 / 157 loss=8.177, nll_loss=3.429, ppl=10.77, wps=35139.5, ups=1.41, wpb=24869.8, bsz=1086.7, num_updates=4800, lr=0.000456435, gnorm=0.332, loss_scale=8, train_wall=31, gb_free=13.5, wall=2791
2022-03-23 10:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:19:19 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the trained woman.
2022-03-23 10:19:19 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:19:23 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can take about 8,000 places in the restaurant.
2022-03-23 10:19:23 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:19:27 | INFO | fairseq.tasks.translation | example hypothesis: and if you see on the upper left corner, you see a little dark point.
2022-03-23 10:19:27 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:19:31 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are the end allies of the united states.
2022-03-23 10:19:31 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:19:35 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of breast expresses that actually react to the extracellular matrix.
2022-03-23 10:19:35 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:19:39 | INFO | fairseq.tasks.translation | example hypothesis: and in the mess of how people took responsibility for the wildlife, the number of wildlife grew back again, and this has become a foundation for conservation in namibia.
2022-03-23 10:19:39 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:19:43 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that, as i think, paul richard buchanan put very nice in a current essay, where he says, "products are living arguments of how we should live our lives."
2022-03-23 10:19:43 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:19:47 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can, which gives the gross configurations of the face and the basic shape, and through the threshold information that refers the whole porter structure and all the fone.
2022-03-23 10:19:47 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:19:52 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's highly interesting and measured to me here at tedwomen is that -- tyes, when he was best summarized when someone said, "turn you to the men of your table and say," if the revolution starts to be here, 'the truth is that we've already supported you for this long time. "
2022-03-23 10:19:52 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:19:54 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a big part of the design work that we're on at the pristine aircraft, was a result that we had to solve the unique problems that were connected to it to the ground -- everything from a continuous variation, and a refrigerator system, that allows us to use an aircraft machine, to be able to use it in the aircraft, or a particular way, if you look at the ground, or if you look at the propellism, or if you're in the wrong way, or if you're in the ground, or if you're in the wrong way, you're going to operations, you're going to operated, you had to operate it, you're going to operated, you have to operate, you have to operate it, you have to do it, you're going to operate it, you have to operate it, you have to operate it, you can see it, you have to operate it on the propelled by a
2022-03-23 10:19:54 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:19:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.223 | nll_loss 3.138 | ppl 8.8 | bleu 31.53 | wps 4590.1 | wpb 17862.2 | bsz 728.3 | num_updates 4863 | best_bleu 31.53
2022-03-23 10:19:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4863 updates
2022-03-23 10:19:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:19:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:19:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 31 @ 4863 updates, score 31.53) (writing took 1.8772723840083927 seconds)
2022-03-23 10:19:56 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-23 10:19:56 | INFO | train | epoch 031 | loss 8.186 | nll_loss 3.446 | ppl 10.9 | wps 43223.6 | ups 1.72 | wpb 25153.6 | bsz 1020.6 | num_updates 4863 | lr 0.000453469 | gnorm 0.309 | loss_scale 8 | train_wall 49 | gb_free 14 | wall 2852
2022-03-23 10:19:57 | INFO | fairseq.trainer | begin training epoch 32
2022-03-23 10:19:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:20:09 | INFO | train_inner | epoch 032:     37 / 157 loss=8.187, nll_loss=3.447, ppl=10.9, wps=34242, ups=1.36, wpb=25241.1, bsz=932.8, num_updates=4900, lr=0.000451754, gnorm=0.294, loss_scale=8, train_wall=31, gb_free=13.7, wall=2865
2022-03-23 10:20:40 | INFO | train_inner | epoch 032:    137 / 157 loss=8.151, nll_loss=3.382, ppl=10.43, wps=79080.2, ups=3.15, wpb=25078.9, bsz=1052.1, num_updates=5000, lr=0.000447214, gnorm=0.302, loss_scale=8, train_wall=31, gb_free=13.9, wall=2897
2022-03-23 10:20:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:20:50 | INFO | fairseq.tasks.translation | example hypothesis: maira calcalman, the educated woman.
2022-03-23 10:20:50 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:20:54 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can talk about 8,000 places in the restaurant.
2022-03-23 10:20:54 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:20:58 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left, you can see a little dark point.
2022-03-23 10:20:58 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:21:02 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tied to the united states.
2022-03-23 10:21:02 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:21:06 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of breast expressions that actually respond to the extracellular matrix.
2022-03-23 10:21:06 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:21:10 | INFO | fairseq.tasks.translation | example hypothesis: and in the mess of how people think of responsibility for the wildlife, the number of wild animals grew back again, and this has become a basis for conservation in namibia.
2022-03-23 10:21:10 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:21:14 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul buchanan expressed very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:21:14 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:21:18 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can, which gives the grounds of the face and the basic form, and the whole information that pulls the entire portion structure and all the fits.
2022-03-23 10:21:18 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:21:23 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons it was really interesting and measured to me here at tedwomen is that -- tyes, when dinner was best summarized, when someone said, "turn you to the men in your table and say to you," when the revolution starts, "when the revolution starts, we support you." the truth is that we've been supporting you for you for a long time, "stumber," and then we've started with the future of sand. "and then we're going to be in the future." stumber. "the future.
2022-03-23 10:21:23 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:21:25 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we are at our airplane was a stumber, which is a result that we had to solve the unique problems that were connected to it to operating it on the ground -- everything from a continuous variation and a refrigerator system that allows us to use the aircraft in the ground, or if you look at the wrong way, or if you're going to operate it, or if you can see it, or if you're in the false, or if you're in the land, or if you're either, or if you can see it's the wrong.
2022-03-23 10:21:25 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:21:25 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.235 | nll_loss 3.119 | ppl 8.69 | bleu 31.71 | wps 4686.8 | wpb 17862.2 | bsz 728.3 | num_updates 5020 | best_bleu 31.71
2022-03-23 10:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5020 updates
2022-03-23 10:21:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:21:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:21:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 32 @ 5020 updates, score 31.71) (writing took 1.8723958460032009 seconds)
2022-03-23 10:21:27 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-23 10:21:27 | INFO | train | epoch 032 | loss 8.155 | nll_loss 3.389 | ppl 10.47 | wps 43469.5 | ups 1.73 | wpb 25153.6 | bsz 1020.6 | num_updates 5020 | lr 0.000446322 | gnorm 0.312 | loss_scale 8 | train_wall 49 | gb_free 13.7 | wall 2943
2022-03-23 10:21:27 | INFO | fairseq.trainer | begin training epoch 33
2022-03-23 10:21:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:21:53 | INFO | train_inner | epoch 033:     80 / 157 loss=8.145, nll_loss=3.368, ppl=10.32, wps=34973.8, ups=1.38, wpb=25390.8, bsz=964.9, num_updates=5100, lr=0.000442807, gnorm=0.305, loss_scale=8, train_wall=31, gb_free=14.7, wall=2969
2022-03-23 10:22:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:22:21 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the trained woman.
2022-03-23 10:22:21 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:22:25 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can take about 8,000 places in the restaurant.
2022-03-23 10:22:25 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:22:29 | INFO | fairseq.tasks.translation | example hypothesis: and if you see on the upper left corner, you see a little dark dot.
2022-03-23 10:22:29 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:22:33 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tied in the united states.
2022-03-23 10:22:33 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:22:37 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little dna pieces on the specific genes of breast expressions that actually respond to the extracellular matrix.
2022-03-23 10:22:37 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:22:41 | INFO | fairseq.tasks.translation | example hypothesis: and in the mapping of how people took responsibility for wildlife, the number of wildlife grew up again, and this has become a basis of conservation in namibia.
2022-03-23 10:22:41 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:22:45 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan has expressed very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:22:45 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:22:49 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can, which gives the gross configurations of the face and the basic shape, and refers it through that information that refers the whole porter structure and all the fits.
2022-03-23 10:22:49 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:22:54 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to me here at tedwomen is that... well, at the dinner, it was best summarized when someone said, "turn you to the men in your table and tell you," when the revolution starts, we support you. "the truth is that we've already started this topic for you in this long time."
2022-03-23 10:22:54 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:22:56 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of invention, and a big part of the design work that we're most proud of at our airplane was a result that we had to solve the unique problems that were connected to it on the ground -- everything from a continuous variation, and a cooling system that allows us to use an aircraft in the aircraft, in particular, if you look at the ground, or if you're going to see the car, if you're going to be able to operated in the aircraft, or if you're going to operate -- everything from a distorted by the aircraft, from a distorted by the ground -- everything from a continued by a continuous variation, from a continuous variation, from a continuous variation, from a continuous variation, and a continuous variation, and a refrigerm, and a refrigeration, and a refrigerator, and a refrigerator, and a system of the
2022-03-23 10:22:56 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:22:56 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.199 | nll_loss 3.07 | ppl 8.4 | bleu 31.85 | wps 4753 | wpb 17862.2 | bsz 728.3 | num_updates 5177 | best_bleu 31.85
2022-03-23 10:22:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5177 updates
2022-03-23 10:22:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:22:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:22:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 33 @ 5177 updates, score 31.85) (writing took 1.9622388469870202 seconds)
2022-03-23 10:22:58 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-23 10:22:58 | INFO | train | epoch 033 | loss 8.123 | nll_loss 3.33 | ppl 10.06 | wps 43466.3 | ups 1.73 | wpb 25153.6 | bsz 1020.6 | num_updates 5177 | lr 0.000439502 | gnorm 0.289 | loss_scale 8 | train_wall 49 | gb_free 13.4 | wall 3034
2022-03-23 10:22:58 | INFO | fairseq.trainer | begin training epoch 34
2022-03-23 10:22:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:23:06 | INFO | train_inner | epoch 034:     23 / 157 loss=8.115, nll_loss=3.315, ppl=9.95, wps=34373.3, ups=1.38, wpb=24996.2, bsz=1022.6, num_updates=5200, lr=0.000438529, gnorm=0.291, loss_scale=8, train_wall=31, gb_free=13.9, wall=3042
2022-03-23 10:23:37 | INFO | train_inner | epoch 034:    123 / 157 loss=8.103, nll_loss=3.295, ppl=9.82, wps=79712.3, ups=3.15, wpb=25267.4, bsz=1102.1, num_updates=5300, lr=0.000434372, gnorm=0.325, loss_scale=8, train_wall=31, gb_free=13.7, wall=3074
2022-03-23 10:23:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:23:52 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:23:52 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:23:55 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can take about 8,000 places in the restaurant.
2022-03-23 10:23:55 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:23:59 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:23:59 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:24:03 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tied allies in the united states.
2022-03-23 10:24:03 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:24:07 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of breast expressions that are actually responding to the extracellular matrix.
2022-03-23 10:24:07 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:24:12 | INFO | fairseq.tasks.translation | example hypothesis: and in the mess of the human responsibility for the wildlife, the number of wild animals grew up again, and this has become a basis for conservation in namibia.
2022-03-23 10:24:12 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:24:16 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that, as i think, paul richard buchanan has expressed very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:24:16 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:24:20 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can that gives the grows of the face and the basic shape, and recovers it by the one of the information that pulls the entire pork structure and all the features into it.
2022-03-23 10:24:20 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:24:25 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it was highly interesting and measured to me here at tedwomen is that... well, when we were striking dinner, it was best summarized when someone said, "turn you to the men of your table and tell you, 'when the revolution begins, we support you.' when the revolution starts to be here at tedwomen, we've already been supported for you for a long time."
2022-03-23 10:24:25 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:24:27 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of the invention, and a big part of the design work that we're at the pristine toes on our plane was a result that we had to solve the unique problems that were connected to operating it on the ground -- all, from a continuous variable gear, and a refrigerator system that allows us to use an aircraft in the ground, or one particular passenger traffic, or if you look at the ground, or if you're the wrong mechanism, or if you're in the ground, or you're in the ground, or if you look at the ground, or if you're in the ground, or if you're in the car, or you're in the ground, or if you're in the fake, or if you're in the ground, or if you're in the ground, or if you're in the ground, or you're in the ground, or you're in the ground, or you're in the ground, or you're in the car
2022-03-23 10:24:27 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:24:27 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.204 | nll_loss 3.051 | ppl 8.29 | bleu 32.17 | wps 4621.9 | wpb 17862.2 | bsz 728.3 | num_updates 5334 | best_bleu 32.17
2022-03-23 10:24:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5334 updates
2022-03-23 10:24:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:24:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:24:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 34 @ 5334 updates, score 32.17) (writing took 1.8441798210260458 seconds)
2022-03-23 10:24:29 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-23 10:24:29 | INFO | train | epoch 034 | loss 8.115 | nll_loss 3.315 | ppl 9.95 | wps 43389.8 | ups 1.72 | wpb 25153.6 | bsz 1020.6 | num_updates 5334 | lr 0.000432986 | gnorm 0.322 | loss_scale 8 | train_wall 49 | gb_free 13.9 | wall 3125
2022-03-23 10:24:29 | INFO | fairseq.trainer | begin training epoch 35
2022-03-23 10:24:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:24:50 | INFO | train_inner | epoch 035:     66 / 157 loss=8.075, nll_loss=3.241, ppl=9.46, wps=35115, ups=1.37, wpb=25575.3, bsz=1061.1, num_updates=5400, lr=0.000430331, gnorm=0.292, loss_scale=8, train_wall=31, gb_free=13.6, wall=3146
2022-03-23 10:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:25:23 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the trained woman.
2022-03-23 10:25:23 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:25:26 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can protect about 8,000 places in the restaurant.
2022-03-23 10:25:26 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:25:30 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark point.
2022-03-23 10:25:30 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:25:34 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tight allies in the united states.
2022-03-23 10:25:34 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:25:39 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific breast expressions that are actually responding to the extracellular matrix.
2022-03-23 10:25:39 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:25:43 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people overtook responsibility for the wildlife, the number of wild animals grew back, and this has become a basis for conservation in namibia.
2022-03-23 10:25:43 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:25:47 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:25:47 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:25:51 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection of reflection, we can start with a traditional facial can that gives the grows of the face and recover the basic information that refers the whole portion structure and all the fits.
2022-03-23 10:25:51 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:25:56 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to me here at tedwomen is that... tyes, we've already started to support you for this topic for a long time, when someone said, "turn you to the men of your table and tell you, 'when the revolution starts, we support you.'" 'the truth is that we've already started to support you.' "
2022-03-23 10:25:56 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:25:58 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of the invention, and a big part of the design work that we're most proud of at our airplane was a result that we had to solve the unique problems that were connected to the ground -- all, from a continuous variables, and a cooling system of refrigeration, that allows us to use a aircraft in the most stump of the aircraft in the aircraft, to use it in the most stumber traffic, to a specific result that we had to be able to be able to use that if you can see it in the ground, or if you're going to see it's the decrease it in the car, or if you're going to see it's the wrong.
2022-03-23 10:25:58 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:25:58 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.177 | nll_loss 3.023 | ppl 8.13 | bleu 32.34 | wps 4627.9 | wpb 17862.2 | bsz 728.3 | num_updates 5491 | best_bleu 32.34
2022-03-23 10:25:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5491 updates
2022-03-23 10:25:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:25:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:26:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 35 @ 5491 updates, score 32.34) (writing took 1.8648113809758797 seconds)
2022-03-23 10:26:00 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-23 10:26:00 | INFO | train | epoch 035 | loss 8.084 | nll_loss 3.257 | ppl 9.56 | wps 43389.9 | ups 1.72 | wpb 25153.6 | bsz 1020.6 | num_updates 5491 | lr 0.000426751 | gnorm 0.29 | loss_scale 8 | train_wall 49 | gb_free 14.2 | wall 3216
2022-03-23 10:26:00 | INFO | fairseq.trainer | begin training epoch 36
2022-03-23 10:26:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:26:03 | INFO | train_inner | epoch 036:      9 / 157 loss=8.103, nll_loss=3.294, ppl=9.81, wps=33714.6, ups=1.37, wpb=24627.3, bsz=956.5, num_updates=5500, lr=0.000426401, gnorm=0.293, loss_scale=8, train_wall=31, gb_free=13.7, wall=3219
2022-03-23 10:26:35 | INFO | train_inner | epoch 036:    109 / 157 loss=8.089, nll_loss=3.267, ppl=9.63, wps=78320, ups=3.17, wpb=24673.5, bsz=934, num_updates=5600, lr=0.000422577, gnorm=0.305, loss_scale=8, train_wall=31, gb_free=13.8, wall=3251
2022-03-23 10:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:26:54 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, who trained woman.
2022-03-23 10:26:54 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:26:57 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occur about 8,000 places in the restaurant.
2022-03-23 10:26:57 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:27:01 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:27:01 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:27:05 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tied in the united states.
2022-03-23 10:27:05 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:27:09 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific breast expressions that actually respond to extracellular matrix.
2022-03-23 10:27:09 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:27:13 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people overtook responsibility for the wildlife, the number of wildlife grew back, and this has become a basis for conservation in namibia.
2022-03-23 10:27:13 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:27:17 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a recent essay in which he says, "products are living arguments for how we should live our lives."
2022-03-23 10:27:17 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:27:21 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this reflection, we can start with a traditional facial can, which gives the gross configurations of the face and the basic form, and then refers it through the one of the things that refers the entire portion structure and all the fits.
2022-03-23 10:27:21 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:27:26 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it highly interesting and measured to me here at tedwomen is that... well, when we were striking dinner, it was best summared when someone said, "turn you to the men of your table and tell them, 'when the revolution begins, we support you.' the truth is that we've already started supporting you in this topic for a long time."
2022-03-23 10:27:26 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:27:28 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still part of the design work that we're most proud of in our airplane was a result that we had to solve the unique problems that were connected to it on the ground -- everything from a continuous variable operating and refrigeration system that allows us to use an aircraft machine in the aircraft in the aircraft in the air, or in particular, or in the air, to use it to use it, or to use it, or to use it, or to use it in particular, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, or to use it, in the
2022-03-23 10:27:28 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:27:28 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.169 | nll_loss 3.016 | ppl 8.09 | bleu 31.98 | wps 4740.3 | wpb 17862.2 | bsz 728.3 | num_updates 5648 | best_bleu 32.34
2022-03-23 10:27:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5648 updates
2022-03-23 10:27:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:27:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:27:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 36 @ 5648 updates, score 31.98) (writing took 0.835792500001844 seconds)
2022-03-23 10:27:29 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-23 10:27:29 | INFO | train | epoch 036 | loss 8.07 | nll_loss 3.233 | ppl 9.4 | wps 44329.8 | ups 1.76 | wpb 25153.6 | bsz 1020.6 | num_updates 5648 | lr 0.000420778 | gnorm 0.298 | loss_scale 8 | train_wall 49 | gb_free 14.9 | wall 3305
2022-03-23 10:27:29 | INFO | fairseq.trainer | begin training epoch 37
2022-03-23 10:27:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:27:46 | INFO | train_inner | epoch 037:     52 / 157 loss=8.022, nll_loss=3.145, ppl=8.85, wps=36370.4, ups=1.4, wpb=25959.6, bsz=1143, num_updates=5700, lr=0.000418854, gnorm=0.269, loss_scale=8, train_wall=31, gb_free=14.1, wall=3322
2022-03-23 10:28:17 | INFO | train_inner | epoch 037:    152 / 157 loss=8.059, nll_loss=3.212, ppl=9.27, wps=79989, ups=3.2, wpb=24958.5, bsz=957.1, num_updates=5800, lr=0.000415227, gnorm=0.29, loss_scale=8, train_wall=31, gb_free=13.8, wall=3354
2022-03-23 10:28:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:28:23 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:28:23 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:28:26 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can take about 8,000 places in the restaurant.
2022-03-23 10:28:26 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:28:31 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:28:31 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:28:35 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are close allies in the united states.
2022-03-23 10:28:35 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:28:39 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific ones of the breast expresses that are actually responding to the extracellular matrix.
2022-03-23 10:28:39 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:28:43 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people took responsibility for the wildlife, the number of wildlife grew up again, and that's become a basis for conservation in namibia.
2022-03-23 10:28:43 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:28:47 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan has very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:28:47 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:28:51 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can, which gives the grows of the face, and restores the basic form, and pulls it through the one that refers the whole portion structure and all the fine folds.
2022-03-23 10:28:51 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:28:55 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it was very interesting and measured to be here at tedwomen is that... well, at the dinner, it was best summarized when someone said, "turn to the men on your table and tell them," when the revolution begins, then we support you. "'" the truth is that we've already started to help you with this topic for a long time. "
2022-03-23 10:28:55 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:28:58 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of the invention, and a large part of the design work that we're on our airplane on the stumber, was a result that we had to solve the unique problems that were connected to it on the ground -- all, from a continuously variable operating and a cooling system, that allows us to use an aircraft in the aircraft in the aircraft, or in a special transportation, to either use the drift, or to the drill of the car, or if you can see the wrongside, or to see it, or if you can use it, or the most vulnerable to see it, or to see it, or to the most vulnerable to the most, or to the most, or to the most, or to the most dilated, or to the most, or to see it, or to see it, or to the most dilaccomplish the most, or to see it, or to be able, or to the
2022-03-23 10:28:58 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:28:58 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.151 | nll_loss 2.975 | ppl 7.86 | bleu 32.97 | wps 4679.7 | wpb 17862.2 | bsz 728.3 | num_updates 5805 | best_bleu 32.97
2022-03-23 10:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 5805 updates
2022-03-23 10:28:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:28:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:28:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 37 @ 5805 updates, score 32.97) (writing took 1.8068002220243216 seconds)
2022-03-23 10:28:59 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-23 10:28:59 | INFO | train | epoch 037 | loss 8.039 | nll_loss 3.176 | ppl 9.04 | wps 43660.6 | ups 1.74 | wpb 25153.6 | bsz 1020.6 | num_updates 5805 | lr 0.000415049 | gnorm 0.277 | loss_scale 8 | train_wall 48 | gb_free 14.8 | wall 3396
2022-03-23 10:29:00 | INFO | fairseq.trainer | begin training epoch 38
2022-03-23 10:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:29:30 | INFO | train_inner | epoch 038:     95 / 157 loss=8.022, nll_loss=3.145, ppl=8.84, wps=34252.8, ups=1.38, wpb=24877.5, bsz=1026.6, num_updates=5900, lr=0.000411693, gnorm=0.274, loss_scale=8, train_wall=31, gb_free=13.6, wall=3426
2022-03-23 10:29:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:29:53 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:29:53 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:29:57 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can protect about 8,000 places in the restaurant.
2022-03-23 10:29:57 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:30:01 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:30:01 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:30:05 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tied allies in the united states.
2022-03-23 10:30:05 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:30:09 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast expresses that actually respond to the extracellular matrix.
2022-03-23 10:30:09 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:30:13 | INFO | fairseq.tasks.translation | example hypothesis: and just like the people who overtook responsibility for wildlife, the number of wild animals grew up again, and that's become a basis for conservation in namibia.
2022-03-23 10:30:13 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:30:17 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a current essay in which he says, "products are living arguments for how we should live our lives."
2022-03-23 10:30:17 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:30:21 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information coming from this mirror reflection, we can start with a traditional facial can, which gives the gross configurations of the face and the basic shape, and recover it through the one of the things that refers the whole porn structure and all the fine folds.
2022-03-23 10:30:21 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:30:26 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons to do it highly interesting and measured to be here at tedwomen is to be here at tedwomen is that... well, at the controversial dinner, it was best summarized when someone said, "turn you to the men on your table and tell them," when the revolution starts, then we support you. "'the truth is that we've already started you with this topic for a long time."
2022-03-23 10:30:26 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:30:28 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a big part of the design work that we're most proud of at our airplane, was a result that we had to solve the unique problems that were connected to doing it on the ground -- all, from a continuous variables, and a refrigeration system, that allows us to use an aircraft in the stop, to a special vehicle, or to the driver, if you're going to be able to be able to operate in the ground, or if you're going to be able to be able to be able to operated, if you're going to operate at the car, or if you're going to be able to be able to be able to be able to be able to be able to be able to be able to be able to operate at the wrong.
2022-03-23 10:30:28 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:30:28 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.166 | nll_loss 2.979 | ppl 7.88 | bleu 32.88 | wps 4734.4 | wpb 17862.2 | bsz 728.3 | num_updates 5962 | best_bleu 32.97
2022-03-23 10:30:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 5962 updates
2022-03-23 10:30:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:30:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:30:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 38 @ 5962 updates, score 32.88) (writing took 0.8173043779679574 seconds)
2022-03-23 10:30:29 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-23 10:30:29 | INFO | train | epoch 038 | loss 8.022 | nll_loss 3.143 | ppl 8.84 | wps 44309.5 | ups 1.76 | wpb 25153.6 | bsz 1020.6 | num_updates 5962 | lr 0.000409547 | gnorm 0.273 | loss_scale 8 | train_wall 49 | gb_free 14.1 | wall 3485
2022-03-23 10:30:29 | INFO | fairseq.trainer | begin training epoch 39
2022-03-23 10:30:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:30:41 | INFO | train_inner | epoch 039:     38 / 157 loss=8.034, nll_loss=3.166, ppl=8.98, wps=35299.7, ups=1.41, wpb=24979.3, bsz=966.4, num_updates=6000, lr=0.000408248, gnorm=0.304, loss_scale=8, train_wall=31, gb_free=13.8, wall=3497
2022-03-23 10:31:13 | INFO | train_inner | epoch 039:    138 / 157 loss=8.004, nll_loss=3.11, ppl=8.63, wps=79444.6, ups=3.14, wpb=25331.2, bsz=1048.8, num_updates=6100, lr=0.000404888, gnorm=0.274, loss_scale=8, train_wall=31, gb_free=13.7, wall=3529
2022-03-23 10:31:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:31:22 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:31:22 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:31:26 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:31:26 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:31:30 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:31:30 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:31:34 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tight allies in the united states.
2022-03-23 10:31:34 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:31:38 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast glands that actually respond to the extracellular matrix.
2022-03-23 10:31:38 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:31:42 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people overtook responsibility for wildlife, the number of wildlife grew up again, and that's become a basis for conservation in namibia.
2022-03-23 10:31:42 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:31:46 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan wrote very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:31:46 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:31:50 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this mirror reflection, we can start with a traditional facial can that gives the big configurations of the face and the basic form of information, which refers the whole portion structure and all the fine.
2022-03-23 10:31:50 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:31:55 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons to do it highly interesting and measured to me here at tedwomen is that -- well, at the controversial dinner, it was best summarized when someone said, "turn to the men on your table and tell you, 'when the revolution starts, we support you.'" the truth, women, we've already started to help you with rachel carthera's future, "and then we're going to downstream," and then we're going to call it "-- and then we're going to call it to call it to be a stack up to be a stumbust to sandman's future," and then we're "and then we're going to be like," -- and then we're going to be a stumbust to be a stumbling on, and then we're going to be
2022-03-23 10:31:55 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:31:57 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a big part of the design work that we're most proud of on our airplane, was a result that we had to solve the unique problems that were connected to doing it on the ground -- everything from a continuous variables and a refrigerator system that allows us to use an aircraft in the stop traffic to a particular passenger drive, if you can either see it on the ground, or when you're going to be able to run the wrong mechanism, or if you're in the car mechanism, or if you're going to be able to see it on the ground, or if you're going to be able to be able to see it in the car mechanism, or if you're going to be able to see it, or if you're going to be able to be able to be able to see it on the ground, or if you're going to be able to be able to be able to be able to be able to be able to be able to run the
2022-03-23 10:31:57 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:31:57 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.126 | nll_loss 2.957 | ppl 7.77 | bleu 33.07 | wps 4730.1 | wpb 17862.2 | bsz 728.3 | num_updates 6119 | best_bleu 33.07
2022-03-23 10:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 6119 updates
2022-03-23 10:31:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:31:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:31:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 39 @ 6119 updates, score 33.07) (writing took 1.8215559660457075 seconds)
2022-03-23 10:31:59 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-23 10:31:59 | INFO | train | epoch 039 | loss 8.013 | nll_loss 3.128 | ppl 8.74 | wps 43825 | ups 1.74 | wpb 25153.6 | bsz 1020.6 | num_updates 6119 | lr 0.000404259 | gnorm 0.291 | loss_scale 8 | train_wall 49 | gb_free 13.6 | wall 3575
2022-03-23 10:31:59 | INFO | fairseq.trainer | begin training epoch 40
2022-03-23 10:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:32:25 | INFO | train_inner | epoch 040:     81 / 157 loss=7.985, nll_loss=3.077, ppl=8.44, wps=34953.1, ups=1.39, wpb=25218.2, bsz=1033.4, num_updates=6200, lr=0.00040161, gnorm=0.271, loss_scale=8, train_wall=31, gb_free=13.7, wall=3601
2022-03-23 10:32:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:32:52 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:32:52 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:32:56 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can protect about 8,000 places in the restaurant.
2022-03-23 10:32:56 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:33:01 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark point.
2022-03-23 10:33:01 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:33:04 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are close allies in the united states.
2022-03-23 10:33:04 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:33:08 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast expresses that actually respond to the extracellular matrix.
2022-03-23 10:33:08 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:33:12 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people were responsible for the wildlife, the number of wildlife grew back again. and this has become a foundation for conservation in namibia.
2022-03-23 10:33:12 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:33:16 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:33:16 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:33:20 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflective reflection, we can start with a traditional facial can that gives the gross configurations of the face and reform the basic form and bring it through that kind of information that pulls the whole pork structure and all the fine folds.
2022-03-23 10:33:20 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:33:25 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to be here at tedwomen is that... well, when he was strived dinner, it was best summarized when someone said, "turn you to the men on your table and tell you," when the revolution begins, we support you. '"'" 'the truth, women, love is that we've already started to help you with this topic for a long time. at rachel carchson's "with silent siltheo's" "] ["] ["'" '"'" '"'" '"'" '"'" '"'" '"'"] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["
2022-03-23 10:33:25 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:33:27 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of the invention, and a big part of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuous variables and a refrigerator system with liquid that allows us to use an aircraft in the stop and traffic until a special passenger drive, either to see the ground, or if you see the false mechanism, or if you can see the fly, or if you can see it on the ground, or if you can see it's the wrong mechanism.
2022-03-23 10:33:27 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:33:27 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.143 | nll_loss 2.944 | ppl 7.7 | bleu 32.95 | wps 4738.6 | wpb 17862.2 | bsz 728.3 | num_updates 6276 | best_bleu 33.07
2022-03-23 10:33:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 6276 updates
2022-03-23 10:33:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:33:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:33:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 40 @ 6276 updates, score 32.95) (writing took 0.7852055039838888 seconds)
2022-03-23 10:33:28 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-23 10:33:28 | INFO | train | epoch 040 | loss 7.987 | nll_loss 3.08 | ppl 8.46 | wps 44308 | ups 1.76 | wpb 25153.6 | bsz 1020.6 | num_updates 6276 | lr 0.000399171 | gnorm 0.269 | loss_scale 8 | train_wall 49 | gb_free 14.7 | wall 3664
2022-03-23 10:33:28 | INFO | fairseq.trainer | begin training epoch 41
2022-03-23 10:33:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:33:36 | INFO | train_inner | epoch 041:     24 / 157 loss=7.974, nll_loss=3.056, ppl=8.32, wps=35633.3, ups=1.41, wpb=25342, bsz=1044.3, num_updates=6300, lr=0.00039841, gnorm=0.259, loss_scale=8, train_wall=31, gb_free=13.9, wall=3672
2022-03-23 10:34:08 | INFO | train_inner | epoch 041:    124 / 157 loss=7.996, nll_loss=3.096, ppl=8.55, wps=78996, ups=3.16, wpb=24982.7, bsz=948.1, num_updates=6400, lr=0.000395285, gnorm=0.279, loss_scale=8, train_wall=31, gb_free=13.6, wall=3704
2022-03-23 10:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:34:21 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the trained woman.
2022-03-23 10:34:21 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:34:25 | INFO | fairseq.tasks.translation | example hypothesis: overyear, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:34:25 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:34:30 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:34:30 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:34:34 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tight allies in the united states.
2022-03-23 10:34:34 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:34:38 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast glands that actually respond to extracellular matrix.
2022-03-23 10:34:38 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:34:42 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people overtook responsibility for the wildlife was, the number of wild animals grew back up again, and this has become a basis for conservation in namibia.
2022-03-23 10:34:42 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:34:46 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan put out very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:34:46 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:34:50 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can that gives the gross configurations of the face and restore it through the information that pulls the whole portion structure and all the fine fold into it.
2022-03-23 10:34:50 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:34:54 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to be here at tedwomen is that... well, at the controversial dinner, it was best summarized when someone said, "turn to the men on your table and tell them," if the revolution starts to support you. "" the truth, the truth, love of women is that we've already started with you on this topic for a long time.
2022-03-23 10:34:54 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:34:56 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of invention, and a big part of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuously variable, and a refrigeration system that allows us to use an aircraft machine in the stop and traffic to a particular passenger drive, if you can either see it on the ground, or the fake of a propmatic kind of thing, to see it all the way down to the car, and we're going to be able to see it's going to be able.
2022-03-23 10:34:56 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:34:56 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.11 | nll_loss 2.942 | ppl 7.69 | bleu 33.44 | wps 4703.9 | wpb 17862.2 | bsz 728.3 | num_updates 6433 | best_bleu 33.44
2022-03-23 10:34:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 6433 updates
2022-03-23 10:34:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:34:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 41 @ 6433 updates, score 33.44) (writing took 1.803114953043405 seconds)
2022-03-23 10:34:58 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-23 10:34:58 | INFO | train | epoch 041 | loss 7.973 | nll_loss 3.054 | ppl 8.3 | wps 43711.6 | ups 1.74 | wpb 25153.6 | bsz 1020.6 | num_updates 6433 | lr 0.00039427 | gnorm 0.27 | loss_scale 8 | train_wall 49 | gb_free 14.3 | wall 3754
2022-03-23 10:34:58 | INFO | fairseq.trainer | begin training epoch 42
2022-03-23 10:34:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:35:20 | INFO | train_inner | epoch 042:     67 / 157 loss=7.95, nll_loss=3.009, ppl=8.05, wps=34143.5, ups=1.38, wpb=24703, bsz=1060.4, num_updates=6500, lr=0.000392232, gnorm=0.278, loss_scale=8, train_wall=31, gb_free=13.5, wall=3776
2022-03-23 10:35:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:35:52 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:35:52 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:35:56 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occur about 8,000 places in the restaurant.
2022-03-23 10:35:56 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:36:00 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:36:00 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:36:04 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tight allies in the united states.
2022-03-23 10:36:04 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:36:08 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific ones of breast glands that actually respond to the extracellular matrix.
2022-03-23 10:36:08 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:36:12 | INFO | fairseq.tasks.translation | example hypothesis: and in a sense like people were responsibility for wildlife, the number of wildlife grew back, and that's become a foundation for conservation in namibia.
2022-03-23 10:36:12 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:36:16 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan put very nice in a current essay where he says, "products are living arguments for how to live our lives."
2022-03-23 10:36:16 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:36:20 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial vision, which gives the gross configurations of the face and the basic shape, and then recover it through the information that pulls the whole pork structure and all the fine folds.
2022-03-23 10:36:20 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:36:24 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons to be highly interesting and measured to me here at tedwomen is that... well, at the controversial dinner, it was best summarized when someone said, "turn to the men on your table and say," when the revolution begins, we support you. '"the truth is that we've already started a long time for you. at chel carreson's"
2022-03-23 10:36:24 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:36:27 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a big part of the design work that we're most proud of at our airplane, was a result that we had to solve the unique problems that were linked to operate on the ground -- everything from a continuous variables to a refrigeration system that allows us to use an aircraft in stop traffic, to a particular drift that either drives the propmatic problems that drives the ground, or if you can see it on the ground, or if you can see it operations, or if you can see it operations, or if you can see it operations, or the false.
2022-03-23 10:36:27 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:36:27 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.123 | nll_loss 2.925 | ppl 7.59 | bleu 33.28 | wps 4746.5 | wpb 17862.2 | bsz 728.3 | num_updates 6590 | best_bleu 33.44
2022-03-23 10:36:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 6590 updates
2022-03-23 10:36:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:36:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:36:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 42 @ 6590 updates, score 33.28) (writing took 0.840793251991272 seconds)
2022-03-23 10:36:27 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-23 10:36:27 | INFO | train | epoch 042 | loss 7.959 | nll_loss 3.027 | ppl 8.15 | wps 44255.1 | ups 1.76 | wpb 25153.6 | bsz 1020.6 | num_updates 6590 | lr 0.000389545 | gnorm 0.274 | loss_scale 8 | train_wall 49 | gb_free 13.4 | wall 3843
2022-03-23 10:36:28 | INFO | fairseq.trainer | begin training epoch 43
2022-03-23 10:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:36:31 | INFO | train_inner | epoch 043:     10 / 157 loss=7.952, nll_loss=3.017, ppl=8.1, wps=36391, ups=1.4, wpb=25909.8, bsz=1071.2, num_updates=6600, lr=0.000389249, gnorm=0.265, loss_scale=8, train_wall=31, gb_free=13.7, wall=3847
2022-03-23 10:37:03 | INFO | train_inner | epoch 043:    110 / 157 loss=7.944, nll_loss=3.001, ppl=8, wps=79617.7, ups=3.15, wpb=25308.6, bsz=1039.1, num_updates=6700, lr=0.000386334, gnorm=0.277, loss_scale=8, train_wall=31, gb_free=14.9, wall=3879
2022-03-23 10:37:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:37:21 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:37:21 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:37:25 | INFO | fairseq.tasks.translation | example hypothesis: overyear, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:37:25 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:37:29 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:37:29 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:37:33 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tightly allies in the united states.
2022-03-23 10:37:33 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:37:38 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific part of the breast gland that actually react to the extracellular matrix.
2022-03-23 10:37:38 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:37:42 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people have taken responsibility for wildlife, the number of wildlife grew up again, and that's become a basis for conservation in namibia.
2022-03-23 10:37:42 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:37:46 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:37:46 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:37:50 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this mirror reflection, we can start with a traditional facial can that gives the gross configurations of the face and reclaiming it through that kind of information that refers the entire porter structure and all the fine folds.
2022-03-23 10:37:50 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:37:54 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to be here at tedwomen is that... well, at the dinner, it was best summarized when someone said, "turn you to the men at your table," and say, "if the revolution starts, we'll support you. '" the truth is that we've already started with you for a long time. at rachel siltheo, "and then we're going to downstream.
2022-03-23 10:37:54 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:37:56 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of the invention, and a big part of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were connected to operating it on the ground -- everything from a continuous variable operating and refrigeration system that allows us to use an aircraft in the stop and go traffic to a special passenger, or if you're going to see it on the ground, or if you're going to see it, you're going to see it in the wrong way, or if you're going to see it, you're going to see it, or if you're going to see it, you're going to see it, you're going to be able, you're going to see it, you're going to be able to see it, you're going to see it on the ground, or if you're going to see it, you're going to be able to see it, you're going to be able to
2022-03-23 10:37:56 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:37:56 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.098 | nll_loss 2.918 | ppl 7.56 | bleu 33.59 | wps 4689.9 | wpb 17862.2 | bsz 728.3 | num_updates 6747 | best_bleu 33.59
2022-03-23 10:37:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 6747 updates
2022-03-23 10:37:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:37:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:37:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 43 @ 6747 updates, score 33.59) (writing took 1.830735112016555 seconds)
2022-03-23 10:37:58 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-23 10:37:58 | INFO | train | epoch 043 | loss 7.955 | nll_loss 3.021 | ppl 8.12 | wps 43586.5 | ups 1.73 | wpb 25153.6 | bsz 1020.6 | num_updates 6747 | lr 0.000384986 | gnorm 0.288 | loss_scale 8 | train_wall 49 | gb_free 15.1 | wall 3934
2022-03-23 10:37:58 | INFO | fairseq.trainer | begin training epoch 44
2022-03-23 10:37:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:38:15 | INFO | train_inner | epoch 044:     53 / 157 loss=7.97, nll_loss=3.046, ppl=8.26, wps=33582.9, ups=1.39, wpb=24233.6, bsz=893.1, num_updates=6800, lr=0.000383482, gnorm=0.283, loss_scale=8, train_wall=31, gb_free=13.9, wall=3951
2022-03-23 10:38:47 | INFO | train_inner | epoch 044:    153 / 157 loss=7.92, nll_loss=2.958, ppl=7.77, wps=81470.3, ups=3.17, wpb=25671.3, bsz=1111.6, num_updates=6900, lr=0.000380693, gnorm=0.265, loss_scale=8, train_wall=31, gb_free=14.2, wall=3983
2022-03-23 10:38:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:38:52 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:38:52 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:38:55 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:38:55 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:38:59 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:38:59 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:39:03 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tight allies in the united states.
2022-03-23 10:39:03 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:39:08 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific breast glands that actually respond to the extracellular matrix.
2022-03-23 10:39:08 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:39:11 | INFO | fairseq.tasks.translation | example hypothesis: and in the size of how people took responsibility for wildlife, the number of wildlife grew up again, and this has become a basis for conservation in namibia.
2022-03-23 10:39:11 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:39:15 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan has expressed very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:39:15 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:39:19 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this reflection, we can start with a traditional facial can, which gives the gross configurations of the face and recovery, and then refuse it through that kind of information that pulls all the pores structure and all the fine folds.
2022-03-23 10:39:19 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:39:23 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to be here at tedwomen is that -- well, at the strict dinner, it was best summarized when someone said, "turn to the men on your table and tell them," if the revolution begins to support you. "the truth, women, love, is that we've been supporting you at this topic for a long time."
2022-03-23 10:39:23 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:39:25 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention and a lot of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were linked to operate on the ground -- everything from a continuous variables and a refrigerator system with fluid that allows us to use an aircraft in the stop and go traffic to a special drift, either when you're going to see the false mechanism, or if you're going to see the false.
2022-03-23 10:39:25 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:39:25 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.123 | nll_loss 2.898 | ppl 7.45 | bleu 33.19 | wps 4954.1 | wpb 17862.2 | bsz 728.3 | num_updates 6904 | best_bleu 33.59
2022-03-23 10:39:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 6904 updates
2022-03-23 10:39:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:39:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:39:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 44 @ 6904 updates, score 33.19) (writing took 0.8384131539496593 seconds)
2022-03-23 10:39:26 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-23 10:39:26 | INFO | train | epoch 044 | loss 7.931 | nll_loss 2.976 | ppl 7.87 | wps 45099.1 | ups 1.79 | wpb 25153.6 | bsz 1020.6 | num_updates 6904 | lr 0.000380583 | gnorm 0.263 | loss_scale 8 | train_wall 49 | gb_free 14.8 | wall 4022
2022-03-23 10:39:26 | INFO | fairseq.trainer | begin training epoch 45
2022-03-23 10:39:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:39:57 | INFO | train_inner | epoch 045:     96 / 157 loss=7.912, nll_loss=2.939, ppl=7.67, wps=35793.4, ups=1.42, wpb=25136.5, bsz=1031.8, num_updates=7000, lr=0.000377964, gnorm=0.266, loss_scale=8, train_wall=31, gb_free=13.4, wall=4053
2022-03-23 10:40:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:40:19 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the woman who's formed.
2022-03-23 10:40:19 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:40:23 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occur about 8,000 places in the restaurant.
2022-03-23 10:40:23 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:40:27 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:40:27 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:40:31 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are closely allies in the united states.
2022-03-23 10:40:31 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:40:35 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are small pieces of dna on the specific genes of the breast glands that actually respond to the extracellular matrix.
2022-03-23 10:40:35 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:40:40 | INFO | fairseq.tasks.translation | example hypothesis: and in terms of the way that people took responsibility for wildlife, the number of wildlife grew up again, and this has become a basis for conservation in namibia.
2022-03-23 10:40:40 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:40:44 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan has expressed very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:40:44 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:40:48 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can that gives the grows of the face and restore the basic form, and then recover it through the information that refers the whole portion structure and all the fine folds.
2022-03-23 10:40:48 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:40:52 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to be here at tedwomen is that... well, at dinner, it was best summed together when someone said, "turn you to the men at your table and say to you, 'if the revolution begins to support you, then we support you.'" the truth is that we've started you at this topic for a long time.
2022-03-23 10:40:52 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:40:53 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother and a large part of the design work that we're most proud of on our plane was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuous variable operating and a cooling system with fluid that allows us to use an aviation machine in the stop traffic until a special passenger that drives either the propmatic problems that you see on the ground, or when you see the wrongness of a car.
2022-03-23 10:40:53 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:40:53 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.086 | nll_loss 2.907 | ppl 7.5 | bleu 33.94 | wps 4857.3 | wpb 17862.2 | bsz 728.3 | num_updates 7061 | best_bleu 33.94
2022-03-23 10:40:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 7061 updates
2022-03-23 10:40:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:40:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:40:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 45 @ 7061 updates, score 33.94) (writing took 2.3962708220351487 seconds)
2022-03-23 10:40:56 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-23 10:40:56 | INFO | train | epoch 045 | loss 7.917 | nll_loss 2.95 | ppl 7.73 | wps 43831.7 | ups 1.74 | wpb 25153.6 | bsz 1020.6 | num_updates 7061 | lr 0.000376328 | gnorm 0.261 | loss_scale 8 | train_wall 49 | gb_free 13.4 | wall 4112
2022-03-23 10:40:56 | INFO | fairseq.trainer | begin training epoch 46
2022-03-23 10:40:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:41:08 | INFO | train_inner | epoch 046:     39 / 157 loss=7.911, nll_loss=2.94, ppl=7.68, wps=35096.5, ups=1.4, wpb=25147.9, bsz=1032.8, num_updates=7100, lr=0.000375293, gnorm=0.263, loss_scale=8, train_wall=31, gb_free=13.7, wall=4125
2022-03-23 10:41:40 | INFO | train_inner | epoch 046:    139 / 157 loss=7.915, nll_loss=2.947, ppl=7.71, wps=79312.8, ups=3.15, wpb=25190.1, bsz=985.3, num_updates=7200, lr=0.000372678, gnorm=0.271, loss_scale=8, train_wall=31, gb_free=14.1, wall=4156
2022-03-23 10:41:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:41:49 | INFO | fairseq.tasks.translation | example hypothesis: maira calman, the educated woman.
2022-03-23 10:41:49 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:41:53 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:41:53 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:41:57 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:41:57 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:42:01 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are close allies in the united states.
2022-03-23 10:42:01 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:42:05 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast glands that are actually responding to the extracellular matrix.
2022-03-23 10:42:05 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:42:09 | INFO | fairseq.tasks.translation | example hypothesis: and as people were taking responsibility for wildlife, wildlife grew up again, and this has become a basis for conservation in namibia.
2022-03-23 10:42:09 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:42:13 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a current essay in which he says, "products are living arguments for how we should live our lives."
2022-03-23 10:42:13 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:42:17 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can that gives the grounds of the face and reform it through the information that refers all the pores structure and all the features.
2022-03-23 10:42:17 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:42:22 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to be here at tedwomen is that... well, when dinner was stripped, it was best summarized when someone said, "turn to the men at your table and tell them, 'when the revolution begins, we support you.'" the truth, women, we've already started a long time on this topic.
2022-03-23 10:42:22 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:42:24 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of invention, and a large part of the design work that we're most proud of on our plane was a result that we had to solve the unique problems that were connected to it -- everything from a continuous variables to a refrigerator system that allows us to use an aircraft in stop traffic to a special passenger.
2022-03-23 10:42:24 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:42:24 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.087 | nll_loss 2.918 | ppl 7.56 | bleu 33.72 | wps 4730.1 | wpb 17862.2 | bsz 728.3 | num_updates 7218 | best_bleu 33.94
2022-03-23 10:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 7218 updates
2022-03-23 10:42:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:42:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 46 @ 7218 updates, score 33.72) (writing took 0.899659308954142 seconds)
2022-03-23 10:42:25 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-23 10:42:25 | INFO | train | epoch 046 | loss 7.91 | nll_loss 2.937 | ppl 7.66 | wps 44190.2 | ups 1.76 | wpb 25153.6 | bsz 1020.6 | num_updates 7218 | lr 0.000372213 | gnorm 0.279 | loss_scale 8 | train_wall 49 | gb_free 13.8 | wall 4201
2022-03-23 10:42:25 | INFO | fairseq.trainer | begin training epoch 47
2022-03-23 10:42:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:42:52 | INFO | train_inner | epoch 047:     82 / 157 loss=7.9, nll_loss=2.917, ppl=7.55, wps=35195.9, ups=1.4, wpb=25164.1, bsz=1007.8, num_updates=7300, lr=0.000370117, gnorm=0.269, loss_scale=8, train_wall=31, gb_free=13.7, wall=4228
2022-03-23 10:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:43:19 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:43:19 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:43:23 | INFO | fairseq.tasks.translation | example hypothesis: overyear, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:43:23 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:43:26 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:43:26 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:43:30 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are close allies in the united states.
2022-03-23 10:43:30 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:43:34 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast gland that actually respond to the extracellular matrix.
2022-03-23 10:43:34 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:43:38 | INFO | fairseq.tasks.translation | example hypothesis: and in the sense of how people took responsibility for wildlife, the number of wildlife grew back, and this has become a foundation for conservation in namibia.
2022-03-23 10:43:38 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:43:42 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan has expressed very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:43:42 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:43:46 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional face can, which gives the gross configurations of the face and reform the basic shape, and then then then add it through the information that refers the entire porter structure and all the fine folds.
2022-03-23 10:43:46 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:43:51 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to me here at tedwomen is that... well, at dinner it was best summarized when someone said, "turn the men to your table and tell you, 'when the revolution begins, then we support you.'" 'the truth, love is that we've already started to support you for a long time. at rachel carchson with siltheo's future. "
2022-03-23 10:43:51 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:43:52 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a large part of the design work that we're most stumbling on on our airplane was a result that we had to solve the unique problems that were connected to doing it on the ground -- everything from a continuous variable drivers and a refrigerator system that allows us to use an aircraft in stop traffic to a special passenger that drives the car or if you look at the ground.
2022-03-23 10:43:52 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:43:52 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.088 | nll_loss 2.904 | ppl 7.48 | bleu 33.6 | wps 4960.1 | wpb 17862.2 | bsz 728.3 | num_updates 7375 | best_bleu 33.94
2022-03-23 10:43:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 7375 updates
2022-03-23 10:43:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:43:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:43:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 47 @ 7375 updates, score 33.6) (writing took 0.8187064149533398 seconds)
2022-03-23 10:43:53 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-23 10:43:53 | INFO | train | epoch 047 | loss 7.895 | nll_loss 2.909 | ppl 7.51 | wps 45054.5 | ups 1.79 | wpb 25153.6 | bsz 1020.6 | num_updates 7375 | lr 0.00036823 | gnorm 0.262 | loss_scale 8 | train_wall 49 | gb_free 14 | wall 4289
2022-03-23 10:43:53 | INFO | fairseq.trainer | begin training epoch 48
2022-03-23 10:43:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:44:01 | INFO | train_inner | epoch 048:     25 / 157 loss=7.9, nll_loss=2.922, ppl=7.58, wps=36366.3, ups=1.44, wpb=25256.8, bsz=1011.8, num_updates=7400, lr=0.000367607, gnorm=0.263, loss_scale=8, train_wall=31, gb_free=14.1, wall=4297
2022-03-23 10:44:33 | INFO | train_inner | epoch 048:    125 / 157 loss=7.875, nll_loss=2.874, ppl=7.33, wps=79648.8, ups=3.15, wpb=25303.3, bsz=1074.6, num_updates=7500, lr=0.000365148, gnorm=0.266, loss_scale=8, train_wall=31, gb_free=14.4, wall=4329
2022-03-23 10:44:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:44:47 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:44:47 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:44:51 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:44:51 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:44:55 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark point.
2022-03-23 10:44:55 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:44:59 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are closely allies in the united states.
2022-03-23 10:44:59 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:45:03 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the particular one of the breast glands that actually respond to the extracellular matrix.
2022-03-23 10:45:03 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:45:06 | INFO | fairseq.tasks.translation | example hypothesis: and in a sense, like people took responsibility for wildlife, the number of wildlife grew back up again, and this has become a basis for conservation in namibia.
2022-03-23 10:45:06 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:45:11 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a recent essay in which he says, "products are living arguments for how we should live our lives."
2022-03-23 10:45:11 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:45:15 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can that will restore the chores of the face and repeat it through the information that refers all the pores structure and all the fine wrinkles.
2022-03-23 10:45:15 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:45:19 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate for me to be here at tedwomen is that... well, at the strict dinner, it was best summarized when someone said, "turn to the men on your table and tell them," when the revolution begins, we'll support you. '"the truth, love women, we've already started you in this topic for a long time. at rachel carchson siltheo's"
2022-03-23 10:45:19 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:45:21 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still part of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were connected to operate on the ground -- all, from a continuous variables and a refrigerator system that allows us to use an aircraft in closest stop and go traffic to a particular passenger drivers when you see it on the ground, or when you see the wrongside a mechanism, or if you see the wrongside of the wrongside of a car waste.
2022-03-23 10:45:21 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:45:21 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.082 | nll_loss 2.893 | ppl 7.43 | bleu 33.98 | wps 4827.5 | wpb 17862.2 | bsz 728.3 | num_updates 7532 | best_bleu 33.98
2022-03-23 10:45:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 7532 updates
2022-03-23 10:45:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:45:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:45:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 48 @ 7532 updates, score 33.98) (writing took 1.817231487017125 seconds)
2022-03-23 10:45:23 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-23 10:45:23 | INFO | train | epoch 048 | loss 7.884 | nll_loss 2.89 | ppl 7.41 | wps 43778.7 | ups 1.74 | wpb 25153.6 | bsz 1020.6 | num_updates 7532 | lr 0.000364372 | gnorm 0.264 | loss_scale 8 | train_wall 49 | gb_free 13.6 | wall 4379
2022-03-23 10:45:23 | INFO | fairseq.trainer | begin training epoch 49
2022-03-23 10:45:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:45:45 | INFO | train_inner | epoch 049:     68 / 157 loss=7.879, nll_loss=2.88, ppl=7.36, wps=34625.5, ups=1.39, wpb=24971.1, bsz=976.5, num_updates=7600, lr=0.000362738, gnorm=0.262, loss_scale=8, train_wall=31, gb_free=13.8, wall=4401
2022-03-23 10:46:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:46:17 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:46:17 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:46:21 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:46:21 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:46:25 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you'll see a little dark dot.
2022-03-23 10:46:25 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:46:29 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are closely allies in the united states.
2022-03-23 10:46:29 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:46:33 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific parts of the breast gland that actually respond to the extracellular matrix.
2022-03-23 10:46:33 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:46:37 | INFO | fairseq.tasks.translation | example hypothesis: and in terms of the way people took responsibility for wildlife, wildlife grew back up again. and that's become a basis for conservation in namibia.
2022-03-23 10:46:37 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:46:41 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:46:41 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:46:45 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information coming from this mirror reflection, we can start with a traditional facial can, which gives the scores of the face and the basic shape back, and adding it through the information that the whole pore structure and all the fine folds.
2022-03-23 10:46:45 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:46:49 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to be here at tedwomen is that... well, at dinner it was best summarized when someone said, "turn to the men on your table and tell them, 'if the revolution begins, we will support you.' the truth, love is that we've already started you on this topic for a long time. at rachel silson spring, it was" stumber "] ["] ["] ["] ["] ["] ["] ["] spring] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["]
2022-03-23 10:46:49 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:46:51 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a big part of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were linked to operate on the ground -- everything from a continuous variables and a refrigeration system that allows us to use an aircraft in stop and traffic until a special passenger to run, either the propmatic, or if you're going to be able to see it on the ground, or be able to see the wrinkling, the mechanism, the wrinkled by the ground.
2022-03-23 10:46:51 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:46:51 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.094 | nll_loss 2.884 | ppl 7.38 | bleu 33.89 | wps 4790.1 | wpb 17862.2 | bsz 728.3 | num_updates 7689 | best_bleu 33.98
2022-03-23 10:46:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 7689 updates
2022-03-23 10:46:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:46:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:46:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 49 @ 7689 updates, score 33.89) (writing took 0.8716615879675373 seconds)
2022-03-23 10:46:52 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-23 10:46:52 | INFO | train | epoch 049 | loss 7.872 | nll_loss 2.867 | ppl 7.3 | wps 44485.7 | ups 1.77 | wpb 25153.6 | bsz 1020.6 | num_updates 7689 | lr 0.000360633 | gnorm 0.266 | loss_scale 8 | train_wall 49 | gb_free 14 | wall 4468
2022-03-23 10:46:52 | INFO | fairseq.trainer | begin training epoch 50
2022-03-23 10:46:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:46:56 | INFO | train_inner | epoch 050:     11 / 157 loss=7.871, nll_loss=2.865, ppl=7.29, wps=35660.5, ups=1.41, wpb=25209.7, bsz=1042.2, num_updates=7700, lr=0.000360375, gnorm=0.267, loss_scale=8, train_wall=31, gb_free=14, wall=4472
2022-03-23 10:47:27 | INFO | train_inner | epoch 050:    111 / 157 loss=7.858, nll_loss=2.843, ppl=7.18, wps=79408.6, ups=3.16, wpb=25114.3, bsz=1036.7, num_updates=7800, lr=0.000358057, gnorm=0.273, loss_scale=8, train_wall=31, gb_free=14.1, wall=4504
2022-03-23 10:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:47:45 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:47:45 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:47:49 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:47:49 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:47:53 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark dot.
2022-03-23 10:47:53 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:47:58 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are closely allies in the united states.
2022-03-23 10:47:58 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:48:02 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast gland that are actually responding to the extracellular matrix.
2022-03-23 10:48:02 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:48:06 | INFO | fairseq.tasks.translation | example hypothesis: and in terms of how people took responsibility for wildlife, wildlife is growing up again, and that's become a basis for conservation in namibia.
2022-03-23 10:48:06 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:48:10 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a current essay where he says, "products are living arguments on how we should live our lives."
2022-03-23 10:48:10 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:48:14 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial can, which gives the gross configurations of the face and reform the basic information, which refers the whole porter structure and all the fine wrinkles.
2022-03-23 10:48:14 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:48:18 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to be here at tedwomen is that, well, at the controversial dinner, it was best summarized when someone said, "turn to your men at your table and tell them, 'if the revolution begins, we will support you.'" the truth, love is that we've already supported you for a long time. "
2022-03-23 10:48:18 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:48:20 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a big part of the design work that we're at our plane at the most proud, was a result that we had to solve the unique problems that were connected to doing it on the ground -- everything from a continuous variables and a cooling system of refrigeration that allows us to use an aircraft in the stop and go traffic, to a special passenger, either the propmatized, or if you're on the ground, to see it all the way down the way down to the car, to the way down to the ground.
2022-03-23 10:48:20 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:48:20 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.069 | nll_loss 2.852 | ppl 7.22 | bleu 34.19 | wps 4809.8 | wpb 17862.2 | bsz 728.3 | num_updates 7846 | best_bleu 34.19
2022-03-23 10:48:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 7846 updates
2022-03-23 10:48:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:48:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:48:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 50 @ 7846 updates, score 34.19) (writing took 1.9240225729881786 seconds)
2022-03-23 10:48:21 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-23 10:48:21 | INFO | train | epoch 050 | loss 7.866 | nll_loss 2.856 | ppl 7.24 | wps 43956.6 | ups 1.75 | wpb 25153.6 | bsz 1020.6 | num_updates 7846 | lr 0.000357006 | gnorm 0.268 | loss_scale 8 | train_wall 49 | gb_free 14.4 | wall 4558
2022-03-23 10:48:22 | INFO | fairseq.trainer | begin training epoch 51
2022-03-23 10:48:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:48:39 | INFO | train_inner | epoch 051:     54 / 157 loss=7.865, nll_loss=2.852, ppl=7.22, wps=34538.4, ups=1.4, wpb=24654.5, bsz=965.4, num_updates=7900, lr=0.000355784, gnorm=0.271, loss_scale=8, train_wall=31, gb_free=14.8, wall=4575
2022-03-23 10:49:10 | INFO | train_inner | epoch 051:    154 / 157 loss=7.859, nll_loss=2.844, ppl=7.18, wps=80624.1, ups=3.16, wpb=25474.4, bsz=1058.9, num_updates=8000, lr=0.000353553, gnorm=0.262, loss_scale=8, train_wall=31, gb_free=14, wall=4606
2022-03-23 10:49:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:49:15 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:49:15 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:49:19 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:49:19 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:49:23 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark point.
2022-03-23 10:49:23 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:49:27 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are closely allies in the united states.
2022-03-23 10:49:27 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:49:30 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast gland that actually respond to the extracellular matrix.
2022-03-23 10:49:30 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:49:34 | INFO | fairseq.tasks.translation | example hypothesis: and in terms of the way that people took responsibility for wildlife, the number of wildlife grew back up again, and that has become a basis for conservation in namibia.
2022-03-23 10:49:34 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:49:39 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan expressed very nice in a current essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:49:39 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:49:43 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can, which gives the grounds of the face and reform the basic information that refers all the pores structure and all the fine folds.
2022-03-23 10:49:43 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:49:47 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to be here at tedwomen is that... well, at the strict dinner it was best summarized when someone said, "turn to the men on your table and tell them, 'when the revolution begins, then we support you.'" 'the truth, love women is that we've been supporting you on this topic for a long time. at chel racarson silthera's future. "
2022-03-23 10:49:47 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:49:48 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a big part of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were connected to operate on the ground -- all, from a continuous variables and a refrigerator system that allows us to use an aircraft in the stop and go traffic to a particular passenger drive, either when you're going to see the wrong mechanism, or if you're going to see the wrinkling.
2022-03-23 10:49:48 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:49:48 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.061 | nll_loss 2.851 | ppl 7.22 | bleu 34.38 | wps 4923.8 | wpb 17862.2 | bsz 728.3 | num_updates 8003 | best_bleu 34.38
2022-03-23 10:49:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 8003 updates
2022-03-23 10:49:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:49:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt
2022-03-23 10:49:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_best.pt (epoch 51 @ 8003 updates, score 34.38) (writing took 1.9961858880124055 seconds)
2022-03-23 10:49:50 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-23 10:49:50 | INFO | train | epoch 051 | loss 7.856 | nll_loss 2.837 | ppl 7.15 | wps 44383.3 | ups 1.76 | wpb 25153.6 | bsz 1020.6 | num_updates 8003 | lr 0.000353487 | gnorm 0.268 | loss_scale 8 | train_wall 49 | gb_free 13.9 | wall 4647
2022-03-23 10:49:51 | INFO | fairseq.trainer | begin training epoch 52
2022-03-23 10:49:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:50:22 | INFO | train_inner | epoch 052:     97 / 157 loss=7.852, nll_loss=2.831, ppl=7.11, wps=35670.4, ups=1.41, wpb=25378.8, bsz=980.5, num_updates=8100, lr=0.000351364, gnorm=0.272, loss_scale=8, train_wall=31, gb_free=14.3, wall=4678
2022-03-23 10:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:50:44 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:50:44 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:50:48 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:50:48 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:50:52 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you'll see a little dark point.
2022-03-23 10:50:52 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:50:55 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are closely allies in the united states.
2022-03-23 10:50:55 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:50:59 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast gland that actually respond to the extracellular matrix.
2022-03-23 10:50:59 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:51:03 | INFO | fairseq.tasks.translation | example hypothesis: and as people think of wildlife responsibility, the number of wild animals grew back again, and that's a basis for conservation in namibia.
2022-03-23 10:51:03 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:51:07 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan put very nice in a recent essay where he says, "products are living arguments for how to live our lives."
2022-03-23 10:51:07 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:51:11 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial sscan that restore the gross configurations of the face, and restore it through the information that refers the entire pores structure and all the fine folds.
2022-03-23 10:51:11 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:51:15 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to be here at tedwomen is that... well, when dinner was best summarized, when someone said, "turn to the men on your table and tell them, 'when the revolution begins, then we support you.'" the truth, love women is that we've already started a long time. at chel silthera's future stumbling to sandstream. "
2022-03-23 10:51:15 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:51:17 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a large part of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuous variables and a refrigerator system that allows us to use an aircraft at stop traffic until a particular passenger drive, either propmatic thing that drives the ground, or if you see the wrong mechanism in the fall.
2022-03-23 10:51:17 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:51:17 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.081 | nll_loss 2.857 | ppl 7.25 | bleu 34.11 | wps 5043.2 | wpb 17862.2 | bsz 728.3 | num_updates 8160 | best_bleu 34.38
2022-03-23 10:51:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 8160 updates
2022-03-23 10:51:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 52 @ 8160 updates, score 34.11) (writing took 0.9456529610324651 seconds)
2022-03-23 10:51:17 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-23 10:51:17 | INFO | train | epoch 052 | loss 7.848 | nll_loss 2.823 | ppl 7.07 | wps 45397.5 | ups 1.8 | wpb 25153.6 | bsz 1020.6 | num_updates 8160 | lr 0.00035007 | gnorm 0.272 | loss_scale 8 | train_wall 49 | gb_free 13.7 | wall 4734
2022-03-23 10:51:18 | INFO | fairseq.trainer | begin training epoch 53
2022-03-23 10:51:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:51:31 | INFO | train_inner | epoch 053:     40 / 157 loss=7.832, nll_loss=2.793, ppl=6.93, wps=36383.7, ups=1.44, wpb=25240, bsz=1087.2, num_updates=8200, lr=0.000349215, gnorm=0.268, loss_scale=8, train_wall=31, gb_free=14.3, wall=4747
2022-03-23 10:52:02 | INFO | train_inner | epoch 053:    140 / 157 loss=7.862, nll_loss=2.846, ppl=7.19, wps=78996.2, ups=3.2, wpb=24699.2, bsz=911.6, num_updates=8300, lr=0.000347105, gnorm=0.283, loss_scale=8, train_wall=31, gb_free=13.8, wall=4778
2022-03-23 10:52:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:52:12 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:52:12 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:52:16 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:52:16 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:52:19 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the upper left corner, you see a little dark point.
2022-03-23 10:52:19 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:52:23 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are tight allies in the united states.
2022-03-23 10:52:23 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:52:27 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific genes of the breast glands that actually respond to the extracellular matrix.
2022-03-23 10:52:27 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:52:31 | INFO | fairseq.tasks.translation | example hypothesis: and how people have taken responsibility for wildlife revenues, the number of wild animals grew back up again, and this has become a basis for conservation in namibia.
2022-03-23 10:52:31 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:52:35 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan put very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:52:35 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:52:39 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can that gives us the grounds of the face and resurrection the basic information that refers the whole porter structure and all the fine wrinkles.
2022-03-23 10:52:39 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:52:44 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate for me to be here at tedwomen is that -- well, at dinner, it was best summarized when someone said, "turn to the men on your table and tell them, 'when the revolution starts, we support you.'" the truth, we've already been supporting you on this topic for a long time. at rachel siltheo, "it was best summarized at rachel siltheo," when someone said, "turn to the men down to your grave future of sand," let's down to grave, "and say," let's support you. "
2022-03-23 10:52:44 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:52:46 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still an invention, and a lot of the design work that we're most proud of on our airplane was a result that we had to solve the unique problems that were connected to it on the ground -- everything from a continuously variable gear to a cooling system that allows us to use an aircraft in stop and go to a particular passenger drive, either the propeller that drives the ground when you fly to the ground -- all the way down the way down the way down the road -- all the way down the way down the way down the way down the road, to see the way down the road, to the safety system, to the security system, to the road, to see the road, to the freedoms, that's been refrigerator, that will be refrigerator, that we use it, that will be able, that will be able to see that allows us to be able to use it, that will be able, that allows us to use it
2022-03-23 10:52:46 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:52:46 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.07 | nll_loss 2.849 | ppl 7.21 | bleu 34.2 | wps 4808.7 | wpb 17862.2 | bsz 728.3 | num_updates 8317 | best_bleu 34.38
2022-03-23 10:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 8317 updates
2022-03-23 10:52:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:52:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 53 @ 8317 updates, score 34.2) (writing took 0.8832929939962924 seconds)
2022-03-23 10:52:47 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-23 10:52:47 | INFO | train | epoch 053 | loss 7.838 | nll_loss 2.804 | ppl 6.98 | wps 44224.9 | ups 1.76 | wpb 25153.6 | bsz 1020.6 | num_updates 8317 | lr 0.00034675 | gnorm 0.272 | loss_scale 8 | train_wall 48 | gb_free 14.3 | wall 4823
2022-03-23 10:52:47 | INFO | fairseq.trainer | begin training epoch 54
2022-03-23 10:52:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 10:53:13 | INFO | train_inner | epoch 054:     83 / 157 loss=7.826, nll_loss=2.784, ppl=6.89, wps=34703.4, ups=1.41, wpb=24693, bsz=1033.4, num_updates=8400, lr=0.000345033, gnorm=0.272, loss_scale=8, train_wall=30, gb_free=14.5, wall=4849
2022-03-23 10:53:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 10:53:40 | INFO | fairseq.tasks.translation | example hypothesis: maira kalman, the educated woman.
2022-03-23 10:53:40 | INFO | fairseq.tasks.translation | example reference: maira kalman: the illustrated woman
2022-03-23 10:53:44 | INFO | fairseq.tasks.translation | example hypothesis: over the year, he can occupy about 8,000 places in the restaurant.
2022-03-23 10:53:44 | INFO | fairseq.tasks.translation | example reference: he can seat, throughout the year, he can seat 8,000 people.
2022-03-23 10:53:49 | INFO | fairseq.tasks.translation | example hypothesis: and if you look at the top left corner, you'll see a little dark point.
2022-03-23 10:53:49 | INFO | fairseq.tasks.translation | example reference: and if you look at the top left, you see a little teeny dark dot.
2022-03-23 10:53:52 | INFO | fairseq.tasks.translation | example hypothesis: it's particularly focused on japan, korea and australia, countries that are closely allies in the united states.
2022-03-23 10:53:52 | INFO | fairseq.tasks.translation | example reference: specifically, it targets japan and korea and australia, countries that are strong allies of the united states.
2022-03-23 10:53:56 | INFO | fairseq.tasks.translation | example hypothesis: we've shown that there are little pieces of dna on the specific breast glands that actually respond to the extracellular matrix.
2022-03-23 10:53:56 | INFO | fairseq.tasks.translation | example reference: we have shown that there's little pieces of dna on the specific genes of the mammary gland that actually respond to extracellular matrix.
2022-03-23 10:54:00 | INFO | fairseq.tasks.translation | example hypothesis: and as people were responsible for wildlife, the number of wildlife grew back up again, and this has become a basis for conservation in namibia.
2022-03-23 10:54:00 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 10:54:04 | INFO | fairseq.tasks.translation | example hypothesis: and this is an idea that i think paul richard buchanan put very nice in a recent essay where he says, "products are living arguments for how we should live our lives."
2022-03-23 10:54:04 | INFO | fairseq.tasks.translation | example reference: and that is an idea that, i think, that paul richard buchanan nicely put in a recent essay where he said, "products are vivid arguments about how we should live our lives."
2022-03-23 10:54:08 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can, which gives back the gross configurations of the face and the basic shape, and add it through the information that refers the whole porty structure and all the fine folds.
2022-03-23 10:54:08 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 10:54:12 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and appropriate to be here at tedwomen is that... well, at the controversial dinner, it was best summarized when someone said, "turn to the men at your table and tell them, 'when the revolution starts to support you.'" 'the truth, love women is that we've been supporting you at this topic for a long time. at rachel siltheo, "then stumbling our"
2022-03-23 10:54:12 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 10:54:13 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a large part of the design work that we're most proud of on our plane was a result that we had to solve the unique problems that were connected to it on the floor -- everything from a continuous variables to a refrigeration system that allows us to use an aircraft in stop and go traffic to a special passenger drive, either the propeller that drives the ground, or if you're flying, or if you're going to be able to be able to operate on the ground.
2022-03-23 10:54:13 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 10:54:13 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.069 | nll_loss 2.867 | ppl 7.3 | bleu 33.93 | wps 4968.9 | wpb 17862.2 | bsz 728.3 | num_updates 8474 | best_bleu 34.38
2022-03-23 10:54:13 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-23 10:54:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 8474 updates
2022-03-23 10:54:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:54:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt
2022-03-23 10:54:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_label_smoothing_0.45_#3/checkpoint_last.pt (epoch 54 @ 8474 updates, score 33.93) (writing took 0.9217557089868933 seconds)
2022-03-23 10:54:14 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-23 10:54:14 | INFO | train | epoch 054 | loss 7.828 | nll_loss 2.785 | ppl 6.89 | wps 45064.5 | ups 1.79 | wpb 25153.6 | bsz 1020.6 | num_updates 8474 | lr 0.000343523 | gnorm 0.264 | loss_scale 8 | train_wall 49 | gb_free 13.9 | wall 4910
2022-03-23 10:54:14 | INFO | fairseq_cli.train | done training in 4910.1 seconds
