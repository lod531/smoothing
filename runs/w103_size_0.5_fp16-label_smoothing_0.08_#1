Sender: LSF System <lsfadmin@eu-g3-016>
Subject: Job 205633402: <w103_size_0.5_fp16_label_smoothing_0.08_#1> in cluster <euler> Exited

Job <w103_size_0.5_fp16_label_smoothing_0.08_#1> was submitted from host <eu-login-37> by user <andriusb> in cluster <euler> at Fri Feb 18 08:23:34 2022
Job was executed on host(s) <eu-g3-016>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Feb 18 08:23:47 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 18 08:23:47 2022
Terminated at Sun Feb 20 06:53:46 2022
Results reported at Sun Feb 20 06:53:46 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.08 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 1321671 --fp16 --max-update 50000
------------------------------------------------------------

Exited with exit code 134.

Resource usage summary:

    CPU time :                                   167305.44 sec.
    Max Memory :                                 13508 MB
    Average Memory :                             3190.90 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               6492.00 MB
    Max Swap :                                   53 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   167399 sec.
    Turnaround time :                            167412 sec.

The output (if any) follows:

2022-02-18 08:23:53 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.08, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-18 08:23:54 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-18 08:23:59 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-18 08:23:59 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-18 08:23:59 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-18 08:23:59 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-18 08:23:59 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-18 08:23:59 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-18 08:23:59 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-18 08:24:02 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-18 08:24:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-18 08:24:02 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-02-18 08:24:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-18 08:24:02 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-18 08:24:02 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-02-18 08:24:02 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-02-18 08:24:02 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-02-18 08:24:02 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-18 08:24:02 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-18 08:24:02 | INFO | fairseq.trainer | begin training epoch 1
2022-02-18 08:24:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 08:24:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-18 08:24:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 08:24:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 08:24:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-18 08:34:32 | INFO | train_inner | epoch 001:    104 / 788 loss=17.532, nll_loss=17.363, ppl=168557, wps=11249.7, ups=0.17, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.109, loss_scale=8, train_wall=606, gb_free=3.3, wall=630
2022-02-18 08:37:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-18 08:44:24 | INFO | train_inner | epoch 001:    205 / 788 loss=15.316, nll_loss=14.956, ppl=31785.4, wps=11074.2, ups=0.17, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.514, loss_scale=8, train_wall=569, gb_free=3.3, wall=1222
2022-02-18 08:54:11 | INFO | train_inner | epoch 001:    305 / 788 loss=13.219, nll_loss=12.656, ppl=6455.53, wps=11152.2, ups=0.17, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.055, loss_scale=16, train_wall=565, gb_free=3.3, wall=1809
2022-02-18 09:03:58 | INFO | train_inner | epoch 001:    405 / 788 loss=11.702, nll_loss=10.938, ppl=1962.31, wps=11165.4, ups=0.17, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.622, loss_scale=32, train_wall=564, gb_free=3.3, wall=2396
2022-02-18 09:13:46 | INFO | train_inner | epoch 001:    505 / 788 loss=11.098, nll_loss=10.212, ppl=1185.68, wps=11160.8, ups=0.17, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.492, loss_scale=32, train_wall=564, gb_free=3.3, wall=2984
2022-02-18 09:15:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 09:23:39 | INFO | train_inner | epoch 001:    606 / 788 loss=10.773, nll_loss=9.832, ppl=911.7, wps=11042.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.535, loss_scale=32, train_wall=570, gb_free=3.3, wall=3577
2022-02-18 09:28:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 09:33:32 | INFO | train_inner | epoch 001:    707 / 788 loss=10.525, nll_loss=9.553, ppl=751.06, wps=11056.5, ups=0.17, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.619, loss_scale=32, train_wall=570, gb_free=3.3, wall=4170
2022-02-18 09:41:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 09:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 09:41:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.143 | nll_loss 9.126 | ppl 558.74 | wps 27319.7 | wpb 510.9 | bsz 1 | num_updates 780
2022-02-18 09:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 780 updates
2022-02-18 09:41:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint1.pt
2022-02-18 09:41:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint1.pt
2022-02-18 09:41:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint1.pt (epoch 1 @ 780 updates, score 10.143) (writing took 19.69232967402786 seconds)
2022-02-18 09:41:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-18 09:41:53 | INFO | train | epoch 001 | loss 12.62 | nll_loss 11.922 | ppl 3879.94 | wps 11050.2 | ups 0.17 | wpb 65497.1 | bsz 127.9 | num_updates 780 | lr 9.75805e-05 | gnorm 1.086 | loss_scale 32 | train_wall 4462 | gb_free 3.3 | wall 4671
2022-02-18 09:41:53 | INFO | fairseq.trainer | begin training epoch 2
2022-02-18 09:41:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 09:43:50 | INFO | train_inner | epoch 002:     20 / 788 loss=10.307, nll_loss=9.31, ppl=634.94, wps=10551.8, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.662, loss_scale=32, train_wall=567, gb_free=3.3, wall=4788
2022-02-18 09:53:37 | INFO | train_inner | epoch 002:    120 / 788 loss=10.097, nll_loss=9.076, ppl=539.84, wps=11169, ups=0.17, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.756, loss_scale=32, train_wall=564, gb_free=3.3, wall=5375
2022-02-18 09:54:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:01:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 10:03:35 | INFO | train_inner | epoch 002:    222 / 788 loss=9.911, nll_loss=8.872, ppl=468.38, wps=10957.1, ups=0.17, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.757, loss_scale=16, train_wall=575, gb_free=3.3, wall=5973
2022-02-18 10:13:21 | INFO | train_inner | epoch 002:    322 / 788 loss=9.747, nll_loss=8.69, ppl=412.94, wps=11176.7, ups=0.17, wpb=65534.7, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.846, loss_scale=16, train_wall=563, gb_free=3.3, wall=6559
2022-02-18 10:23:08 | INFO | train_inner | epoch 002:    422 / 788 loss=9.61, nll_loss=8.539, ppl=371.98, wps=11171.4, ups=0.17, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.873, loss_scale=32, train_wall=564, gb_free=3.3, wall=7146
2022-02-18 10:26:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:33:01 | INFO | train_inner | epoch 002:    523 / 788 loss=9.465, nll_loss=8.379, ppl=332.82, wps=11051.4, ups=0.17, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.833, loss_scale=32, train_wall=570, gb_free=3.3, wall=7739
2022-02-18 10:39:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:42:54 | INFO | train_inner | epoch 002:    624 / 788 loss=9.345, nll_loss=8.246, ppl=303.58, wps=11059.3, ups=0.17, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.891, loss_scale=32, train_wall=569, gb_free=3.3, wall=8332
2022-02-18 10:52:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 10:52:46 | INFO | train_inner | epoch 002:    725 / 788 loss=9.23, nll_loss=8.119, ppl=277.96, wps=11064.6, ups=0.17, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.867, loss_scale=32, train_wall=569, gb_free=3.3, wall=8924
2022-02-18 10:58:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 10:59:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.963 | nll_loss 7.815 | ppl 225.16 | wps 27394.1 | wpb 510.9 | bsz 1 | num_updates 1563 | best_loss 8.963
2022-02-18 10:59:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1563 updates
2022-02-18 10:59:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint2.pt
2022-02-18 10:59:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint2.pt
2022-02-18 10:59:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint2.pt (epoch 2 @ 1563 updates, score 8.963) (writing took 19.45476485043764 seconds)
2022-02-18 10:59:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-18 10:59:20 | INFO | train | epoch 002 | loss 9.605 | nll_loss 8.533 | ppl 370.51 | wps 11034.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 1563 | lr 0.000195436 | gnorm 0.833 | loss_scale 32 | train_wall 4438 | gb_free 3.3 | wall 9318
2022-02-18 10:59:20 | INFO | fairseq.trainer | begin training epoch 3
2022-02-18 10:59:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 11:02:58 | INFO | train_inner | epoch 003:     37 / 788 loss=9.1, nll_loss=7.975, ppl=251.52, wps=10664, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.889, loss_scale=32, train_wall=561, gb_free=3.3, wall=9536
2022-02-18 11:05:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:12:50 | INFO | train_inner | epoch 003:    138 / 788 loss=8.986, nll_loss=7.848, ppl=230.33, wps=11062.4, ups=0.17, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.835, loss_scale=32, train_wall=569, gb_free=3.3, wall=10128
2022-02-18 11:18:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:22:43 | INFO | train_inner | epoch 003:    239 / 788 loss=8.883, nll_loss=7.733, ppl=212.73, wps=11051.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.858, loss_scale=32, train_wall=570, gb_free=3.3, wall=10721
2022-02-18 11:32:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:32:37 | INFO | train_inner | epoch 003:    340 / 788 loss=8.795, nll_loss=7.636, ppl=198.89, wps=11041.7, ups=0.17, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.823, loss_scale=32, train_wall=570, gb_free=3.3, wall=11314
2022-02-18 11:42:24 | INFO | train_inner | epoch 003:    440 / 788 loss=8.716, nll_loss=7.549, ppl=187.24, wps=11146.8, ups=0.17, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.825, loss_scale=32, train_wall=565, gb_free=3.3, wall=11902
2022-02-18 11:44:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 11:49:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 11:52:24 | INFO | train_inner | epoch 003:    542 / 788 loss=8.639, nll_loss=7.464, ppl=176.51, wps=10936.1, ups=0.17, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.796, loss_scale=16, train_wall=576, gb_free=3.3, wall=12502
2022-02-18 12:02:11 | INFO | train_inner | epoch 003:    642 / 788 loss=8.553, nll_loss=7.368, ppl=165.2, wps=11156.3, ups=0.17, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.773, loss_scale=16, train_wall=564, gb_free=3.3, wall=13089
2022-02-18 12:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 12:12:05 | INFO | train_inner | epoch 003:    743 / 788 loss=8.488, nll_loss=7.296, ppl=157.18, wps=11044.1, ups=0.17, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.769, loss_scale=16, train_wall=570, gb_free=3.3, wall=13683
2022-02-18 12:16:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 12:16:34 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.275 | nll_loss 7.043 | ppl 131.9 | wps 27274.8 | wpb 510.9 | bsz 1 | num_updates 2345 | best_loss 8.275
2022-02-18 12:16:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2345 updates
2022-02-18 12:16:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint3.pt
2022-02-18 12:16:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint3.pt
2022-02-18 12:16:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint3.pt (epoch 3 @ 2345 updates, score 8.275) (writing took 19.48904345370829 seconds)
2022-02-18 12:16:54 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-18 12:16:54 | INFO | train | epoch 003 | loss 8.721 | nll_loss 7.554 | ppl 187.91 | wps 11006.7 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 2345 | lr 0.000293166 | gnorm 0.813 | loss_scale 16 | train_wall 4443 | gb_free 3.3 | wall 13972
2022-02-18 12:16:54 | INFO | fairseq.trainer | begin training epoch 4
2022-02-18 12:16:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 12:22:17 | INFO | train_inner | epoch 004:     55 / 788 loss=8.377, nll_loss=7.174, ppl=144.36, wps=10644, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.769, loss_scale=32, train_wall=562, gb_free=3.3, wall=14295
2022-02-18 12:32:06 | INFO | train_inner | epoch 004:    155 / 788 loss=8.293, nll_loss=7.08, ppl=135.33, wps=11130.7, ups=0.17, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.764, loss_scale=32, train_wall=565, gb_free=3.3, wall=14884
2022-02-18 12:33:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 12:42:00 | INFO | train_inner | epoch 004:    256 / 788 loss=8.228, nll_loss=7.009, ppl=128.76, wps=11035.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.731, loss_scale=32, train_wall=570, gb_free=3.3, wall=15478
2022-02-18 12:46:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 12:51:54 | INFO | train_inner | epoch 004:    357 / 788 loss=8.191, nll_loss=6.968, ppl=125.16, wps=11042.2, ups=0.17, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.741, loss_scale=32, train_wall=570, gb_free=3.3, wall=16072
2022-02-18 12:56:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 13:01:47 | INFO | train_inner | epoch 004:    458 / 788 loss=8.125, nll_loss=6.894, ppl=118.94, wps=11047, ups=0.17, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.714, loss_scale=16, train_wall=570, gb_free=3.3, wall=16665
2022-02-18 13:11:35 | INFO | train_inner | epoch 004:    558 / 788 loss=8.076, nll_loss=6.84, ppl=114.56, wps=11151.8, ups=0.17, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.703, loss_scale=32, train_wall=565, gb_free=3.3, wall=17252
2022-02-18 13:16:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 13:21:28 | INFO | train_inner | epoch 004:    659 / 788 loss=8.032, nll_loss=6.792, ppl=110.78, wps=11042.9, ups=0.17, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.711, loss_scale=16, train_wall=570, gb_free=3.3, wall=17846
2022-02-18 13:31:16 | INFO | train_inner | epoch 004:    759 / 788 loss=7.978, nll_loss=6.731, ppl=106.26, wps=11152, ups=0.17, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.686, loss_scale=32, train_wall=565, gb_free=3.3, wall=18434
2022-02-18 13:34:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 13:34:11 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.835 | nll_loss 6.553 | ppl 93.92 | wps 27320.6 | wpb 510.9 | bsz 1 | num_updates 3129 | best_loss 7.835
2022-02-18 13:34:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3129 updates
2022-02-18 13:34:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint4.pt
2022-02-18 13:34:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint4.pt
2022-02-18 13:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint4.pt (epoch 4 @ 3129 updates, score 7.835) (writing took 19.35298857744783 seconds)
2022-02-18 13:34:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-18 13:34:31 | INFO | train | epoch 004 | loss 8.139 | nll_loss 6.91 | ppl 120.26 | wps 11026.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 3129 | lr 0.000391147 | gnorm 0.722 | loss_scale 32 | train_wall 4447 | gb_free 3.3 | wall 18629
2022-02-18 13:34:31 | INFO | fairseq.trainer | begin training epoch 5
2022-02-18 13:34:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 13:41:28 | INFO | train_inner | epoch 005:     71 / 788 loss=7.862, nll_loss=6.604, ppl=97.28, wps=10648.8, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.681, loss_scale=32, train_wall=562, gb_free=3.3, wall=19046
2022-02-18 13:42:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 13:51:22 | INFO | train_inner | epoch 005:    172 / 788 loss=7.818, nll_loss=6.555, ppl=94.02, wps=11040.2, ups=0.17, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.67, loss_scale=32, train_wall=570, gb_free=3.3, wall=19640
2022-02-18 13:55:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 14:01:15 | INFO | train_inner | epoch 005:    273 / 788 loss=7.79, nll_loss=6.523, ppl=91.95, wps=11044.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.664, loss_scale=32, train_wall=570, gb_free=3.3, wall=20233
2022-02-18 14:08:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 14:11:09 | INFO | train_inner | epoch 005:    374 / 788 loss=7.756, nll_loss=6.486, ppl=89.62, wps=11042, ups=0.17, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.661, loss_scale=32, train_wall=570, gb_free=3.3, wall=20827
2022-02-18 14:19:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 14:21:03 | INFO | train_inner | epoch 005:    475 / 788 loss=7.724, nll_loss=6.451, ppl=87.48, wps=11019, ups=0.17, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.661, loss_scale=16, train_wall=571, gb_free=3.3, wall=21421
2022-02-18 14:30:51 | INFO | train_inner | epoch 005:    575 / 788 loss=7.699, nll_loss=6.423, ppl=85.79, wps=11158.9, ups=0.17, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.639, loss_scale=16, train_wall=564, gb_free=3.3, wall=22009
2022-02-18 14:40:38 | INFO | train_inner | epoch 005:    675 / 788 loss=7.674, nll_loss=6.395, ppl=84.18, wps=11151.8, ups=0.17, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.626, loss_scale=32, train_wall=564, gb_free=3.3, wall=22596
2022-02-18 14:45:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 14:45:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 14:50:38 | INFO | train_inner | epoch 005:    777 / 788 loss=7.639, nll_loss=6.358, ppl=82.01, wps=10937, ups=0.17, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.629, loss_scale=16, train_wall=576, gb_free=3.3, wall=23196
2022-02-18 14:51:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 14:51:48 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.553 | nll_loss 6.232 | ppl 75.17 | wps 27530.2 | wpb 510.9 | bsz 1 | num_updates 3911 | best_loss 7.553
2022-02-18 14:51:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3911 updates
2022-02-18 14:51:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint5.pt
2022-02-18 14:51:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint5.pt
2022-02-18 14:52:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint5.pt (epoch 5 @ 3911 updates, score 7.553) (writing took 19.35831317398697 seconds)
2022-02-18 14:52:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-18 14:52:07 | INFO | train | epoch 005 | loss 7.737 | nll_loss 6.465 | ppl 88.33 | wps 11000.1 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 3911 | lr 0.000488877 | gnorm 0.653 | loss_scale 16 | train_wall 4446 | gb_free 3.3 | wall 23285
2022-02-18 14:52:07 | INFO | fairseq.trainer | begin training epoch 6
2022-02-18 14:52:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 15:00:50 | INFO | train_inner | epoch 006:     89 / 788 loss=7.509, nll_loss=6.213, ppl=74.19, wps=10652.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.615, loss_scale=32, train_wall=562, gb_free=3.3, wall=23808
2022-02-18 15:10:38 | INFO | train_inner | epoch 006:    189 / 788 loss=7.486, nll_loss=6.188, ppl=72.91, wps=11147.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.612, loss_scale=64, train_wall=565, gb_free=3.3, wall=24396
2022-02-18 15:12:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 15:19:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 15:20:37 | INFO | train_inner | epoch 006:    291 / 788 loss=7.473, nll_loss=6.174, ppl=72.2, wps=10935.7, ups=0.17, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.596, loss_scale=16, train_wall=576, gb_free=3.3, wall=24995
2022-02-18 15:30:25 | INFO | train_inner | epoch 006:    391 / 788 loss=7.458, nll_loss=6.157, ppl=71.35, wps=11158.1, ups=0.17, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.57, loss_scale=16, train_wall=564, gb_free=3.3, wall=25583
2022-02-18 15:37:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 15:40:18 | INFO | train_inner | epoch 006:    492 / 788 loss=7.439, nll_loss=6.136, ppl=70.35, wps=11051.8, ups=0.17, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.56, loss_scale=16, train_wall=570, gb_free=3.3, wall=26176
2022-02-18 15:50:05 | INFO | train_inner | epoch 006:    592 / 788 loss=7.416, nll_loss=6.111, ppl=69.14, wps=11162.2, ups=0.17, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.556, loss_scale=32, train_wall=564, gb_free=3.3, wall=26763
2022-02-18 15:58:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 15:59:58 | INFO | train_inner | epoch 006:    693 / 788 loss=7.407, nll_loss=6.102, ppl=68.71, wps=11044, ups=0.17, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.55, loss_scale=16, train_wall=570, gb_free=3.3, wall=27356
2022-02-18 16:09:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 16:09:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.342 | nll_loss 6.007 | ppl 64.3 | wps 27204.2 | wpb 510.9 | bsz 1 | num_updates 4695 | best_loss 7.342
2022-02-18 16:09:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4695 updates
2022-02-18 16:09:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint6.pt
2022-02-18 16:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint6.pt
2022-02-18 16:09:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint6.pt (epoch 6 @ 4695 updates, score 7.342) (writing took 19.396150288172066 seconds)
2022-02-18 16:09:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-18 16:09:41 | INFO | train | epoch 006 | loss 7.444 | nll_loss 6.142 | ppl 70.64 | wps 11033.9 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 4695 | lr 0.000461511 | gnorm 0.576 | loss_scale 16 | train_wall 4444 | gb_free 3.3 | wall 27939
2022-02-18 16:09:41 | INFO | fairseq.trainer | begin training epoch 7
2022-02-18 16:09:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 16:10:10 | INFO | train_inner | epoch 007:      5 / 788 loss=7.376, nll_loss=6.068, ppl=67.08, wps=10654.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=4700, lr=0.000461266, gnorm=0.546, loss_scale=16, train_wall=561, gb_free=3.3, wall=27968
2022-02-18 16:19:58 | INFO | train_inner | epoch 007:    105 / 788 loss=7.229, nll_loss=5.906, ppl=59.97, wps=11153.7, ups=0.17, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.538, loss_scale=32, train_wall=564, gb_free=3.3, wall=28556
2022-02-18 16:21:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 16:29:51 | INFO | train_inner | epoch 007:    206 / 788 loss=7.238, nll_loss=5.915, ppl=60.34, wps=11054.9, ups=0.17, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.539, loss_scale=16, train_wall=569, gb_free=3.3, wall=29149
2022-02-18 16:39:38 | INFO | train_inner | epoch 007:    306 / 788 loss=7.222, nll_loss=5.897, ppl=59.61, wps=11162.4, ups=0.17, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.518, loss_scale=32, train_wall=564, gb_free=3.3, wall=29736
2022-02-18 16:46:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 16:49:31 | INFO | train_inner | epoch 007:    407 / 788 loss=7.231, nll_loss=5.908, ppl=60.06, wps=11056.3, ups=0.17, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.529, loss_scale=32, train_wall=569, gb_free=3.3, wall=30329
2022-02-18 16:55:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 16:59:23 | INFO | train_inner | epoch 007:    508 / 788 loss=7.225, nll_loss=5.902, ppl=59.79, wps=11054.2, ups=0.17, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.514, loss_scale=16, train_wall=570, gb_free=3.3, wall=30921
2022-02-18 17:09:11 | INFO | train_inner | epoch 007:    608 / 788 loss=7.216, nll_loss=5.892, ppl=59.38, wps=11163.2, ups=0.17, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.514, loss_scale=32, train_wall=564, gb_free=3.3, wall=31508
2022-02-18 17:18:57 | INFO | train_inner | epoch 007:    708 / 788 loss=7.214, nll_loss=5.89, ppl=59.3, wps=11165.8, ups=0.17, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.516, loss_scale=32, train_wall=564, gb_free=3.3, wall=32095
2022-02-18 17:22:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 17:25:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 17:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 17:26:53 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.204 | nll_loss 5.863 | ppl 58.22 | wps 27279.5 | wpb 510.9 | bsz 1 | num_updates 5478 | best_loss 7.204
2022-02-18 17:26:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5478 updates
2022-02-18 17:26:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint7.pt
2022-02-18 17:26:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint7.pt
2022-02-18 17:27:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint7.pt (epoch 7 @ 5478 updates, score 7.204) (writing took 20.45585585013032 seconds)
2022-02-18 17:27:13 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-18 17:27:13 | INFO | train | epoch 007 | loss 7.223 | nll_loss 5.899 | ppl 59.69 | wps 11023.8 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 5478 | lr 0.000427257 | gnorm 0.52 | loss_scale 16 | train_wall 4441 | gb_free 3.3 | wall 32591
2022-02-18 17:27:13 | INFO | fairseq.trainer | begin training epoch 8
2022-02-18 17:27:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 17:29:22 | INFO | train_inner | epoch 008:     22 / 788 loss=7.169, nll_loss=5.841, ppl=57.31, wps=10442.1, ups=0.16, wpb=65232.6, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.496, loss_scale=16, train_wall=573, gb_free=3.3, wall=32720
2022-02-18 17:39:09 | INFO | train_inner | epoch 008:    122 / 788 loss=7.049, nll_loss=5.708, ppl=52.27, wps=11159.9, ups=0.17, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.499, loss_scale=32, train_wall=564, gb_free=3.3, wall=33307
2022-02-18 17:48:57 | INFO | train_inner | epoch 008:    222 / 788 loss=7.065, nll_loss=5.725, ppl=52.89, wps=11159.5, ups=0.17, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.502, loss_scale=32, train_wall=564, gb_free=3.3, wall=33895
2022-02-18 17:51:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 17:55:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 17:58:56 | INFO | train_inner | epoch 008:    324 / 788 loss=7.075, nll_loss=5.737, ppl=53.32, wps=10943, ups=0.17, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.512, loss_scale=16, train_wall=575, gb_free=3.3, wall=34494
2022-02-18 18:08:43 | INFO | train_inner | epoch 008:    424 / 788 loss=7.067, nll_loss=5.728, ppl=53.01, wps=11149.7, ups=0.17, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.492, loss_scale=32, train_wall=564, gb_free=3.3, wall=35081
2022-02-18 18:18:31 | INFO | train_inner | epoch 008:    524 / 788 loss=7.075, nll_loss=5.737, ppl=53.32, wps=11160.8, ups=0.17, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.494, loss_scale=32, train_wall=564, gb_free=3.3, wall=35668
2022-02-18 18:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 18:28:23 | INFO | train_inner | epoch 008:    625 / 788 loss=7.071, nll_loss=5.733, ppl=53.18, wps=11060, ups=0.17, wpb=65534.7, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.488, loss_scale=16, train_wall=569, gb_free=3.3, wall=36261
2022-02-18 18:32:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 18:38:16 | INFO | train_inner | epoch 008:    726 / 788 loss=7.064, nll_loss=5.725, ppl=52.9, wps=11058.4, ups=0.17, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.507, loss_scale=16, train_wall=569, gb_free=3.3, wall=36854
2022-02-18 18:44:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 18:44:25 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.125 | nll_loss 5.755 | ppl 54.01 | wps 27377.7 | wpb 510.9 | bsz 1 | num_updates 6262 | best_loss 7.125
2022-02-18 18:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6262 updates
2022-02-18 18:44:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint8.pt
2022-02-18 18:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint8.pt
2022-02-18 18:44:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint8.pt (epoch 8 @ 6262 updates, score 7.125) (writing took 19.362848645076156 seconds)
2022-02-18 18:44:44 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-18 18:44:44 | INFO | train | epoch 008 | loss 7.067 | nll_loss 5.728 | ppl 53.02 | wps 11040 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 6262 | lr 0.000399617 | gnorm 0.498 | loss_scale 16 | train_wall 4441 | gb_free 3.3 | wall 37242
2022-02-18 18:44:44 | INFO | fairseq.trainer | begin training epoch 9
2022-02-18 18:44:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 18:48:27 | INFO | train_inner | epoch 009:     38 / 788 loss=7.017, nll_loss=5.674, ppl=51.05, wps=10664.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.481, loss_scale=32, train_wall=561, gb_free=3.3, wall=37465
2022-02-18 18:50:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 18:58:20 | INFO | train_inner | epoch 009:    139 / 788 loss=6.928, nll_loss=5.575, ppl=47.67, wps=11050.6, ups=0.17, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.489, loss_scale=16, train_wall=570, gb_free=3.3, wall=38058
2022-02-18 19:03:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 19:08:13 | INFO | train_inner | epoch 009:    240 / 788 loss=6.932, nll_loss=5.579, ppl=47.8, wps=11054.6, ups=0.17, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.494, loss_scale=16, train_wall=569, gb_free=3.3, wall=38651
2022-02-18 19:18:00 | INFO | train_inner | epoch 009:    340 / 788 loss=6.947, nll_loss=5.596, ppl=48.37, wps=11160.9, ups=0.17, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.473, loss_scale=32, train_wall=564, gb_free=3.3, wall=39238
2022-02-18 19:19:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 19:27:53 | INFO | train_inner | epoch 009:    441 / 788 loss=6.957, nll_loss=5.607, ppl=48.74, wps=11058.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.508, loss_scale=16, train_wall=569, gb_free=3.3, wall=39831
2022-02-18 19:37:40 | INFO | train_inner | epoch 009:    541 / 788 loss=6.959, nll_loss=5.61, ppl=48.83, wps=11163.3, ups=0.17, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.476, loss_scale=32, train_wall=564, gb_free=3.3, wall=40418
2022-02-18 19:44:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-18 19:47:33 | INFO | train_inner | epoch 009:    642 / 788 loss=6.967, nll_loss=5.618, ppl=49.11, wps=11051.6, ups=0.17, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.479, loss_scale=32, train_wall=570, gb_free=3.3, wall=41011
2022-02-18 19:47:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 19:57:27 | INFO | train_inner | epoch 009:    743 / 788 loss=6.961, nll_loss=5.612, ppl=48.9, wps=11043.6, ups=0.17, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.489, loss_scale=16, train_wall=570, gb_free=3.3, wall=41605
2022-02-18 20:01:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 20:01:56 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.05 | nll_loss 5.697 | ppl 51.88 | wps 27426.9 | wpb 510.9 | bsz 1 | num_updates 7045 | best_loss 7.05
2022-02-18 20:01:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7045 updates
2022-02-18 20:01:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint9.pt
2022-02-18 20:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint9.pt
2022-02-18 20:02:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint9.pt (epoch 9 @ 7045 updates, score 7.05) (writing took 19.423643907532096 seconds)
2022-02-18 20:02:15 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-18 20:02:15 | INFO | train | epoch 009 | loss 6.95 | nll_loss 5.599 | ppl 48.46 | wps 11026.6 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 7045 | lr 0.000376755 | gnorm 0.487 | loss_scale 32 | train_wall 4441 | gb_free 3.3 | wall 41893
2022-02-18 20:02:15 | INFO | fairseq.trainer | begin training epoch 10
2022-02-18 20:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 20:04:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 20:07:44 | INFO | train_inner | epoch 010:     56 / 788 loss=6.887, nll_loss=5.53, ppl=46.21, wps=10564.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.494, loss_scale=16, train_wall=566, gb_free=3.3, wall=42222
2022-02-18 20:16:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 20:17:37 | INFO | train_inner | epoch 010:    157 / 788 loss=6.827, nll_loss=5.463, ppl=44.12, wps=11055.3, ups=0.17, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.487, loss_scale=16, train_wall=569, gb_free=3.3, wall=42815
2022-02-18 20:27:24 | INFO | train_inner | epoch 010:    257 / 788 loss=6.84, nll_loss=5.478, ppl=44.56, wps=11164.2, ups=0.17, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.487, loss_scale=16, train_wall=564, gb_free=3.3, wall=43402
2022-02-18 20:33:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 20:37:17 | INFO | train_inner | epoch 010:    358 / 788 loss=6.856, nll_loss=5.495, ppl=45.1, wps=11055.1, ups=0.17, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.487, loss_scale=16, train_wall=569, gb_free=3.3, wall=43995
2022-02-18 20:47:04 | INFO | train_inner | epoch 010:    458 / 788 loss=6.862, nll_loss=5.503, ppl=45.34, wps=11166, ups=0.17, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.478, loss_scale=32, train_wall=564, gb_free=3.3, wall=44582
2022-02-18 20:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 20:56:56 | INFO | train_inner | epoch 010:    559 / 788 loss=6.872, nll_loss=5.513, ppl=45.68, wps=11059.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.497, loss_scale=16, train_wall=569, gb_free=3.3, wall=45174
2022-02-18 21:06:43 | INFO | train_inner | epoch 010:    659 / 788 loss=6.879, nll_loss=5.521, ppl=45.93, wps=11165.2, ups=0.17, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.491, loss_scale=32, train_wall=564, gb_free=3.3, wall=45761
2022-02-18 21:06:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 21:16:36 | INFO | train_inner | epoch 010:    760 / 788 loss=6.884, nll_loss=5.527, ppl=46.11, wps=11054.9, ups=0.17, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.496, loss_scale=16, train_wall=569, gb_free=3.3, wall=46354
2022-02-18 21:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 21:19:26 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.012 | nll_loss 5.64 | ppl 49.85 | wps 27218.9 | wpb 510.9 | bsz 1 | num_updates 7828 | best_loss 7.012
2022-02-18 21:19:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7828 updates
2022-02-18 21:19:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint10.pt
2022-02-18 21:19:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint10.pt
2022-02-18 21:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint10.pt (epoch 10 @ 7828 updates, score 7.012) (writing took 19.35557170584798 seconds)
2022-02-18 21:19:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-18 21:19:45 | INFO | train | epoch 010 | loss 6.858 | nll_loss 5.498 | ppl 45.18 | wps 11029.1 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 7828 | lr 0.000357416 | gnorm 0.488 | loss_scale 16 | train_wall 4440 | gb_free 3.3 | wall 46543
2022-02-18 21:19:45 | INFO | fairseq.trainer | begin training epoch 11
2022-02-18 21:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 21:24:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 21:26:54 | INFO | train_inner | epoch 011:     73 / 788 loss=6.776, nll_loss=5.407, ppl=42.44, wps=10560.1, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.482, loss_scale=16, train_wall=567, gb_free=3.3, wall=46972
2022-02-18 21:36:41 | INFO | train_inner | epoch 011:    173 / 788 loss=6.747, nll_loss=5.375, ppl=41.5, wps=11166.6, ups=0.17, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.478, loss_scale=16, train_wall=564, gb_free=3.3, wall=47559
2022-02-18 21:36:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 21:46:34 | INFO | train_inner | epoch 011:    274 / 788 loss=6.766, nll_loss=5.396, ppl=42.11, wps=11042.9, ups=0.17, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.484, loss_scale=16, train_wall=570, gb_free=3.3, wall=48152
2022-02-18 21:51:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 21:56:27 | INFO | train_inner | epoch 011:    375 / 788 loss=6.786, nll_loss=5.418, ppl=42.75, wps=11045.3, ups=0.17, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.5, loss_scale=16, train_wall=570, gb_free=3.3, wall=48745
2022-02-18 22:06:14 | INFO | train_inner | epoch 011:    475 / 788 loss=6.787, nll_loss=5.42, ppl=42.81, wps=11167.9, ups=0.17, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.478, loss_scale=32, train_wall=564, gb_free=3.3, wall=49332
2022-02-18 22:08:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:16:07 | INFO | train_inner | epoch 011:    576 / 788 loss=6.801, nll_loss=5.435, ppl=43.27, wps=11056.1, ups=0.17, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.49, loss_scale=16, train_wall=569, gb_free=3.3, wall=49925
2022-02-18 22:21:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:25:59 | INFO | train_inner | epoch 011:    677 / 788 loss=6.808, nll_loss=5.444, ppl=43.52, wps=11063.5, ups=0.17, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.486, loss_scale=16, train_wall=569, gb_free=3.3, wall=50517
2022-02-18 22:34:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:35:52 | INFO | train_inner | epoch 011:    778 / 788 loss=6.816, nll_loss=5.452, ppl=43.79, wps=11061.3, ups=0.17, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.47, loss_scale=16, train_wall=569, gb_free=3.3, wall=51110
2022-02-18 22:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 22:36:56 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.972 | nll_loss 5.6 | ppl 48.49 | wps 27415.8 | wpb 510.9 | bsz 1 | num_updates 8610 | best_loss 6.972
2022-02-18 22:36:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8610 updates
2022-02-18 22:36:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint11.pt
2022-02-18 22:37:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint11.pt
2022-02-18 22:37:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint11.pt (epoch 11 @ 8610 updates, score 6.972) (writing took 19.697105606086552 seconds)
2022-02-18 22:37:16 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-18 22:37:16 | INFO | train | epoch 011 | loss 6.783 | nll_loss 5.415 | ppl 42.66 | wps 11013.8 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 8610 | lr 0.000340799 | gnorm 0.485 | loss_scale 16 | train_wall 4440 | gb_free 3.3 | wall 51193
2022-02-18 22:37:16 | INFO | fairseq.trainer | begin training epoch 12
2022-02-18 22:37:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 22:46:04 | INFO | train_inner | epoch 012:     90 / 788 loss=6.678, nll_loss=5.3, ppl=39.39, wps=10660.3, ups=0.16, wpb=65232.6, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.489, loss_scale=16, train_wall=561, gb_free=3.3, wall=51722
2022-02-18 22:47:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 22:55:56 | INFO | train_inner | epoch 012:    191 / 788 loss=6.691, nll_loss=5.313, ppl=39.75, wps=11059.8, ups=0.17, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.502, loss_scale=16, train_wall=569, gb_free=3.3, wall=52314
2022-02-18 23:04:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:05:49 | INFO | train_inner | epoch 012:    292 / 788 loss=6.708, nll_loss=5.333, ppl=40.3, wps=11055.9, ups=0.17, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.498, loss_scale=16, train_wall=569, gb_free=3.3, wall=52907
2022-02-18 23:15:36 | INFO | train_inner | epoch 012:    392 / 788 loss=6.722, nll_loss=5.348, ppl=40.72, wps=11172.6, ups=0.17, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.487, loss_scale=16, train_wall=563, gb_free=3.3, wall=53494
2022-02-18 23:18:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:25:28 | INFO | train_inner | epoch 012:    493 / 788 loss=6.724, nll_loss=5.35, ppl=40.77, wps=11062.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.49, loss_scale=16, train_wall=569, gb_free=3.3, wall=54086
2022-02-18 23:31:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:35:21 | INFO | train_inner | epoch 012:    594 / 788 loss=6.733, nll_loss=5.36, ppl=41.06, wps=11059.6, ups=0.17, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.483, loss_scale=16, train_wall=569, gb_free=3.3, wall=54679
2022-02-18 23:45:09 | INFO | train_inner | epoch 012:    694 / 788 loss=6.748, nll_loss=5.376, ppl=41.54, wps=11147.2, ups=0.17, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.475, loss_scale=32, train_wall=565, gb_free=3.3, wall=55267
2022-02-18 23:45:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-18 23:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-18 23:54:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.951 | nll_loss 5.563 | ppl 47.27 | wps 27381.1 | wpb 510.9 | bsz 1 | num_updates 9393 | best_loss 6.951
2022-02-18 23:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9393 updates
2022-02-18 23:54:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint12.pt
2022-02-18 23:54:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint12.pt
2022-02-18 23:54:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint12.pt (epoch 12 @ 9393 updates, score 6.951) (writing took 19.40828888397664 seconds)
2022-02-18 23:54:45 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-18 23:54:45 | INFO | train | epoch 012 | loss 6.719 | nll_loss 5.345 | ppl 40.64 | wps 11030.9 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 9393 | lr 0.000326286 | gnorm 0.487 | loss_scale 16 | train_wall 4439 | gb_free 3.3 | wall 55843
2022-02-18 23:54:45 | INFO | fairseq.trainer | begin training epoch 13
2022-02-18 23:54:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-18 23:55:26 | INFO | train_inner | epoch 013:      7 / 788 loss=6.748, nll_loss=5.377, ppl=41.56, wps=10568.1, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=9400, lr=0.000326164, gnorm=0.476, loss_scale=16, train_wall=566, gb_free=3.3, wall=55884
2022-02-19 00:01:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:05:19 | INFO | train_inner | epoch 013:    108 / 788 loss=6.608, nll_loss=5.222, ppl=37.32, wps=11054.2, ups=0.17, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.486, loss_scale=16, train_wall=569, gb_free=3.3, wall=56477
2022-02-19 00:14:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:15:11 | INFO | train_inner | epoch 013:    209 / 788 loss=6.637, nll_loss=5.253, ppl=38.14, wps=11056, ups=0.17, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.48, loss_scale=16, train_wall=569, gb_free=3.3, wall=57069
2022-02-19 00:24:58 | INFO | train_inner | epoch 013:    309 / 788 loss=6.655, nll_loss=5.274, ppl=38.69, wps=11171.9, ups=0.17, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.484, loss_scale=16, train_wall=563, gb_free=3.3, wall=57656
2022-02-19 00:27:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:34:51 | INFO | train_inner | epoch 013:    410 / 788 loss=6.661, nll_loss=5.281, ppl=38.87, wps=11057, ups=0.17, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.487, loss_scale=16, train_wall=569, gb_free=3.3, wall=58249
2022-02-19 00:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 00:44:43 | INFO | train_inner | epoch 013:    511 / 788 loss=6.678, nll_loss=5.299, ppl=39.37, wps=11058.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.48, loss_scale=16, train_wall=569, gb_free=3.3, wall=58841
2022-02-19 00:54:30 | INFO | train_inner | epoch 013:    611 / 788 loss=6.694, nll_loss=5.317, ppl=39.86, wps=11170.3, ups=0.17, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.472, loss_scale=16, train_wall=564, gb_free=3.3, wall=59428
2022-02-19 00:56:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:04:23 | INFO | train_inner | epoch 013:    712 / 788 loss=6.7, nll_loss=5.323, ppl=40.03, wps=11062.1, ups=0.17, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.504, loss_scale=16, train_wall=569, gb_free=3.3, wall=60020
2022-02-19 01:10:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 01:11:54 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.926 | nll_loss 5.544 | ppl 46.65 | wps 27377.2 | wpb 510.9 | bsz 1 | num_updates 10175 | best_loss 6.926
2022-02-19 01:11:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10175 updates
2022-02-19 01:11:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint13.pt
2022-02-19 01:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint13.pt
2022-02-19 01:12:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint13.pt (epoch 13 @ 10175 updates, score 6.926) (writing took 19.487044922076166 seconds)
2022-02-19 01:12:13 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-19 01:12:13 | INFO | train | epoch 013 | loss 6.666 | nll_loss 5.286 | ppl 39 | wps 11017.9 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 10175 | lr 0.000313497 | gnorm 0.486 | loss_scale 16 | train_wall 4439 | gb_free 3.3 | wall 60491
2022-02-19 01:12:13 | INFO | fairseq.trainer | begin training epoch 14
2022-02-19 01:12:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 01:14:40 | INFO | train_inner | epoch 014:     25 / 788 loss=6.664, nll_loss=5.285, ppl=38.98, wps=10563.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.487, loss_scale=16, train_wall=567, gb_free=3.3, wall=60638
2022-02-19 01:24:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:24:33 | INFO | train_inner | epoch 014:    126 / 788 loss=6.56, nll_loss=5.169, ppl=35.98, wps=11058.3, ups=0.17, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.477, loss_scale=16, train_wall=569, gb_free=3.3, wall=61231
2022-02-19 01:34:20 | INFO | train_inner | epoch 014:    226 / 788 loss=6.592, nll_loss=5.204, ppl=36.85, wps=11152.9, ups=0.17, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.497, loss_scale=16, train_wall=564, gb_free=3.3, wall=61818
2022-02-19 01:37:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:44:13 | INFO | train_inner | epoch 014:    327 / 788 loss=6.608, nll_loss=5.222, ppl=37.31, wps=11053, ups=0.17, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.49, loss_scale=16, train_wall=569, gb_free=3.3, wall=62411
2022-02-19 01:50:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 01:54:06 | INFO | train_inner | epoch 014:    428 / 788 loss=6.621, nll_loss=5.236, ppl=37.7, wps=11057.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.478, loss_scale=16, train_wall=569, gb_free=3.3, wall=63004
2022-02-19 02:00:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 02:03:59 | INFO | train_inner | epoch 014:    529 / 788 loss=6.643, nll_loss=5.26, ppl=38.32, wps=11057.7, ups=0.17, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.482, loss_scale=8, train_wall=569, gb_free=3.3, wall=63597
2022-02-19 02:13:45 | INFO | train_inner | epoch 014:    629 / 788 loss=6.647, nll_loss=5.265, ppl=38.46, wps=11170.5, ups=0.17, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.504, loss_scale=16, train_wall=564, gb_free=3.3, wall=64183
2022-02-19 02:23:32 | INFO | train_inner | epoch 014:    729 / 788 loss=6.653, nll_loss=5.271, ppl=38.62, wps=11167, ups=0.17, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.486, loss_scale=16, train_wall=564, gb_free=3.3, wall=64770
2022-02-19 02:28:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 02:29:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 02:29:24 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.907 | nll_loss 5.536 | ppl 46.4 | wps 27484.7 | wpb 510.9 | bsz 1 | num_updates 10958 | best_loss 6.907
2022-02-19 02:29:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 10958 updates
2022-02-19 02:29:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint14.pt
2022-02-19 02:29:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint14.pt
2022-02-19 02:29:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint14.pt (epoch 14 @ 10958 updates, score 6.907) (writing took 19.350954472087324 seconds)
2022-02-19 02:29:43 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-19 02:29:43 | INFO | train | epoch 014 | loss 6.619 | nll_loss 5.234 | ppl 37.63 | wps 11029.5 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 10958 | lr 0.000302089 | gnorm 0.486 | loss_scale 16 | train_wall 4440 | gb_free 3.3 | wall 65141
2022-02-19 02:29:43 | INFO | fairseq.trainer | begin training epoch 15
2022-02-19 02:29:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 02:33:50 | INFO | train_inner | epoch 015:     42 / 788 loss=6.604, nll_loss=5.218, ppl=37.22, wps=10565.5, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.472, loss_scale=16, train_wall=567, gb_free=3.3, wall=65388
2022-02-19 02:41:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 02:43:42 | INFO | train_inner | epoch 015:    143 / 788 loss=6.53, nll_loss=5.134, ppl=35.13, wps=11056, ups=0.17, wpb=65534.7, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.51, loss_scale=16, train_wall=569, gb_free=3.3, wall=65980
2022-02-19 02:53:29 | INFO | train_inner | epoch 015:    243 / 788 loss=6.554, nll_loss=5.161, ppl=35.78, wps=11173.5, ups=0.17, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.487, loss_scale=16, train_wall=563, gb_free=3.3, wall=66567
2022-02-19 02:55:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:03:21 | INFO | train_inner | epoch 015:    344 / 788 loss=6.579, nll_loss=5.19, ppl=36.5, wps=11060.2, ups=0.17, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.489, loss_scale=16, train_wall=569, gb_free=3.3, wall=67159
2022-02-19 03:08:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:13:14 | INFO | train_inner | epoch 015:    445 / 788 loss=6.576, nll_loss=5.186, ppl=36.4, wps=11060.7, ups=0.17, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.493, loss_scale=16, train_wall=569, gb_free=3.3, wall=67752
2022-02-19 03:21:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:23:07 | INFO | train_inner | epoch 015:    546 / 788 loss=6.595, nll_loss=5.207, ppl=36.94, wps=11054.7, ups=0.17, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.485, loss_scale=16, train_wall=569, gb_free=3.3, wall=68345
2022-02-19 03:32:54 | INFO | train_inner | epoch 015:    646 / 788 loss=6.607, nll_loss=5.22, ppl=37.27, wps=11151.9, ups=0.17, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.491, loss_scale=16, train_wall=564, gb_free=3.3, wall=68932
2022-02-19 03:38:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 03:42:47 | INFO | train_inner | epoch 015:    747 / 788 loss=6.611, nll_loss=5.225, ppl=37.41, wps=11057.1, ups=0.17, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.485, loss_scale=16, train_wall=569, gb_free=3.3, wall=69525
2022-02-19 03:46:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 03:46:53 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.893 | nll_loss 5.51 | ppl 45.58 | wps 27309 | wpb 510.9 | bsz 1 | num_updates 11741 | best_loss 6.893
2022-02-19 03:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11741 updates
2022-02-19 03:46:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint15.pt
2022-02-19 03:46:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint15.pt
2022-02-19 03:47:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint15.pt (epoch 15 @ 11741 updates, score 6.893) (writing took 19.79649887699634 seconds)
2022-02-19 03:47:13 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-19 03:47:13 | INFO | train | epoch 015 | loss 6.578 | nll_loss 5.188 | ppl 36.46 | wps 11029.7 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 11741 | lr 0.000291842 | gnorm 0.49 | loss_scale 16 | train_wall 4439 | gb_free 3.3 | wall 69791
2022-02-19 03:47:13 | INFO | fairseq.trainer | begin training epoch 16
2022-02-19 03:47:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 03:52:59 | INFO | train_inner | epoch 016:     59 / 788 loss=6.534, nll_loss=5.14, ppl=35.25, wps=10658.2, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.481, loss_scale=32, train_wall=561, gb_free=3.3, wall=70137
2022-02-19 03:53:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:02:52 | INFO | train_inner | epoch 016:    160 / 788 loss=6.503, nll_loss=5.105, ppl=34.42, wps=11047.4, ups=0.17, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.501, loss_scale=16, train_wall=570, gb_free=3.3, wall=70730
2022-02-19 04:06:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:12:45 | INFO | train_inner | epoch 016:    261 / 788 loss=6.523, nll_loss=5.127, ppl=34.95, wps=11061.3, ups=0.17, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.491, loss_scale=16, train_wall=569, gb_free=3.3, wall=71323
2022-02-19 04:19:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:22:37 | INFO | train_inner | epoch 016:    362 / 788 loss=6.532, nll_loss=5.138, ppl=35.2, wps=11062.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.48, loss_scale=16, train_wall=569, gb_free=3.3, wall=71915
2022-02-19 04:32:24 | INFO | train_inner | epoch 016:    462 / 788 loss=6.543, nll_loss=5.15, ppl=35.5, wps=11175.1, ups=0.17, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.497, loss_scale=32, train_wall=563, gb_free=3.3, wall=72502
2022-02-19 04:37:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:42:16 | INFO | train_inner | epoch 016:    563 / 788 loss=6.561, nll_loss=5.169, ppl=35.98, wps=11060.5, ups=0.17, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.474, loss_scale=16, train_wall=569, gb_free=3.3, wall=73094
2022-02-19 04:49:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 04:52:09 | INFO | train_inner | epoch 016:    664 / 788 loss=6.571, nll_loss=5.181, ppl=36.27, wps=11055.4, ups=0.17, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.504, loss_scale=16, train_wall=569, gb_free=3.3, wall=73687
2022-02-19 05:00:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 05:02:02 | INFO | train_inner | epoch 016:    765 / 788 loss=6.59, nll_loss=5.202, ppl=36.8, wps=11057.6, ups=0.17, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.499, loss_scale=8, train_wall=569, gb_free=3.3, wall=74280
2022-02-19 05:04:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 05:04:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.883 | nll_loss 5.493 | ppl 45.04 | wps 27308.5 | wpb 510.9 | bsz 1 | num_updates 12523 | best_loss 6.883
2022-02-19 05:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12523 updates
2022-02-19 05:04:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint16.pt
2022-02-19 05:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint16.pt
2022-02-19 05:04:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint16.pt (epoch 16 @ 12523 updates, score 6.883) (writing took 20.05174518469721 seconds)
2022-02-19 05:04:42 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-19 05:04:42 | INFO | train | epoch 016 | loss 6.542 | nll_loss 5.148 | ppl 35.45 | wps 11016.4 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 12523 | lr 0.000282583 | gnorm 0.491 | loss_scale 8 | train_wall 4439 | gb_free 3.3 | wall 74440
2022-02-19 05:04:42 | INFO | fairseq.trainer | begin training epoch 17
2022-02-19 05:04:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 05:12:14 | INFO | train_inner | epoch 017:     77 / 788 loss=6.474, nll_loss=5.073, ppl=33.66, wps=10653.5, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.493, loss_scale=8, train_wall=561, gb_free=3.3, wall=74892
2022-02-19 05:22:02 | INFO | train_inner | epoch 017:    177 / 788 loss=6.466, nll_loss=5.065, ppl=33.46, wps=11137.7, ups=0.17, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.494, loss_scale=16, train_wall=565, gb_free=3.3, wall=75480
2022-02-19 05:28:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 05:31:55 | INFO | train_inner | epoch 017:    278 / 788 loss=6.486, nll_loss=5.086, ppl=33.96, wps=11054.1, ups=0.17, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.489, loss_scale=16, train_wall=569, gb_free=3.3, wall=76073
2022-02-19 05:41:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 05:41:48 | INFO | train_inner | epoch 017:    379 / 788 loss=6.496, nll_loss=5.097, ppl=34.23, wps=11056.7, ups=0.17, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.494, loss_scale=16, train_wall=569, gb_free=3.3, wall=76666
2022-02-19 05:51:35 | INFO | train_inner | epoch 017:    479 / 788 loss=6.525, nll_loss=5.129, ppl=34.99, wps=11165.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.488, loss_scale=16, train_wall=564, gb_free=3.3, wall=77253
2022-02-19 05:53:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:01:27 | INFO | train_inner | epoch 017:    580 / 788 loss=6.527, nll_loss=5.131, ppl=35.05, wps=11065.6, ups=0.17, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.487, loss_scale=16, train_wall=569, gb_free=3.3, wall=77845
2022-02-19 06:08:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:11:20 | INFO | train_inner | epoch 017:    681 / 788 loss=6.549, nll_loss=5.157, ppl=35.67, wps=11057.4, ups=0.17, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.482, loss_scale=16, train_wall=569, gb_free=3.3, wall=78438
2022-02-19 06:21:06 | INFO | train_inner | epoch 017:    781 / 788 loss=6.561, nll_loss=5.17, ppl=36, wps=11173.3, ups=0.17, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.496, loss_scale=32, train_wall=563, gb_free=3.3, wall=79024
2022-02-19 06:21:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:21:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 06:21:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.871 | nll_loss 5.486 | ppl 44.83 | wps 27428.9 | wpb 510.9 | bsz 1 | num_updates 13306 | best_loss 6.871
2022-02-19 06:21:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13306 updates
2022-02-19 06:21:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint17.pt
2022-02-19 06:21:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint17.pt
2022-02-19 06:22:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint17.pt (epoch 17 @ 13306 updates, score 6.871) (writing took 19.460127969272435 seconds)
2022-02-19 06:22:12 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-19 06:22:12 | INFO | train | epoch 017 | loss 6.509 | nll_loss 5.112 | ppl 34.57 | wps 11028.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 13306 | lr 0.000274142 | gnorm 0.491 | loss_scale 16 | train_wall 4440 | gb_free 3.3 | wall 79090
2022-02-19 06:22:12 | INFO | fairseq.trainer | begin training epoch 18
2022-02-19 06:22:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 06:31:24 | INFO | train_inner | epoch 018:     94 / 788 loss=6.431, nll_loss=5.026, ppl=32.58, wps=10564.3, ups=0.16, wpb=65232.6, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.487, loss_scale=16, train_wall=567, gb_free=3.3, wall=79642
2022-02-19 06:34:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 06:41:17 | INFO | train_inner | epoch 018:    195 / 788 loss=6.442, nll_loss=5.037, ppl=32.84, wps=11058.6, ups=0.17, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.493, loss_scale=16, train_wall=569, gb_free=3.3, wall=80235
2022-02-19 06:45:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 06:51:09 | INFO | train_inner | epoch 018:    296 / 788 loss=6.456, nll_loss=5.052, ppl=33.18, wps=11064.1, ups=0.17, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.493, loss_scale=8, train_wall=569, gb_free=3.3, wall=80827
2022-02-19 07:00:56 | INFO | train_inner | epoch 018:    396 / 788 loss=6.48, nll_loss=5.079, ppl=33.81, wps=11167.4, ups=0.17, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.5, loss_scale=16, train_wall=564, gb_free=3.3, wall=81414
2022-02-19 07:10:43 | INFO | train_inner | epoch 018:    496 / 788 loss=6.498, nll_loss=5.099, ppl=34.27, wps=11155.5, ups=0.17, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.494, loss_scale=32, train_wall=564, gb_free=3.3, wall=82001
2022-02-19 07:11:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 07:20:36 | INFO | train_inner | epoch 018:    597 / 788 loss=6.493, nll_loss=5.093, ppl=34.14, wps=11049.2, ups=0.17, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.503, loss_scale=16, train_wall=570, gb_free=3.3, wall=82594
2022-02-19 07:24:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 07:30:29 | INFO | train_inner | epoch 018:    698 / 788 loss=6.52, nll_loss=5.123, ppl=34.86, wps=11061.8, ups=0.17, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.492, loss_scale=16, train_wall=569, gb_free=3.3, wall=83187
2022-02-19 07:39:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 07:39:22 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.871 | nll_loss 5.489 | ppl 44.91 | wps 27316.3 | wpb 510.9 | bsz 1 | num_updates 14090 | best_loss 6.871
2022-02-19 07:39:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14090 updates
2022-02-19 07:39:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint18.pt
2022-02-19 07:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint18.pt
2022-02-19 07:39:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint18.pt (epoch 18 @ 14090 updates, score 6.871) (writing took 19.3606921331957 seconds)
2022-02-19 07:39:42 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-19 07:39:42 | INFO | train | epoch 018 | loss 6.48 | nll_loss 5.079 | ppl 33.8 | wps 11044.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 14090 | lr 0.000266406 | gnorm 0.494 | loss_scale 32 | train_wall 4440 | gb_free 3.3 | wall 83740
2022-02-19 07:39:42 | INFO | fairseq.trainer | begin training epoch 19
2022-02-19 07:39:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 07:40:41 | INFO | train_inner | epoch 019:     10 / 788 loss=6.51, nll_loss=5.113, ppl=34.61, wps=10664.6, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=14100, lr=0.000266312, gnorm=0.486, loss_scale=32, train_wall=561, gb_free=3.3, wall=83798
2022-02-19 07:41:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 07:41:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 07:50:39 | INFO | train_inner | epoch 019:    112 / 788 loss=6.395, nll_loss=4.985, ppl=31.67, wps=10949.1, ups=0.17, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.506, loss_scale=8, train_wall=575, gb_free=3.3, wall=84397
2022-02-19 08:00:26 | INFO | train_inner | epoch 019:    212 / 788 loss=6.418, nll_loss=5.011, ppl=32.24, wps=11165.9, ups=0.17, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.495, loss_scale=16, train_wall=564, gb_free=3.3, wall=84984
2022-02-19 08:04:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 08:10:18 | INFO | train_inner | epoch 019:    313 / 788 loss=6.437, nll_loss=5.032, ppl=32.71, wps=11063.1, ups=0.17, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.503, loss_scale=8, train_wall=569, gb_free=3.3, wall=85576
2022-02-19 08:20:05 | INFO | train_inner | epoch 019:    413 / 788 loss=6.449, nll_loss=5.044, ppl=33, wps=11170.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.498, loss_scale=16, train_wall=564, gb_free=3.3, wall=86163
2022-02-19 08:29:52 | INFO | train_inner | epoch 019:    513 / 788 loss=6.477, nll_loss=5.076, ppl=33.73, wps=11166.8, ups=0.17, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.512, loss_scale=32, train_wall=564, gb_free=3.3, wall=86750
2022-02-19 08:30:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 08:39:44 | INFO | train_inner | epoch 019:    614 / 788 loss=6.477, nll_loss=5.076, ppl=33.73, wps=11060.9, ups=0.17, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.502, loss_scale=16, train_wall=569, gb_free=3.3, wall=87342
2022-02-19 08:43:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 08:49:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 08:49:43 | INFO | train_inner | epoch 019:    716 / 788 loss=6.491, nll_loss=5.092, ppl=34.11, wps=10950.2, ups=0.17, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.501, loss_scale=8, train_wall=575, gb_free=3.3, wall=87941
2022-02-19 08:56:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 08:56:51 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.854 | nll_loss 5.47 | ppl 44.34 | wps 27283.9 | wpb 510.9 | bsz 1 | num_updates 14872 | best_loss 6.854
2022-02-19 08:56:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14872 updates
2022-02-19 08:56:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint19.pt
2022-02-19 08:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint19.pt
2022-02-19 08:57:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint19.pt (epoch 19 @ 14872 updates, score 6.854) (writing took 19.45284014660865 seconds)
2022-02-19 08:57:10 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-19 08:57:10 | INFO | train | epoch 019 | loss 6.453 | nll_loss 5.049 | ppl 33.1 | wps 11017.9 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 14872 | lr 0.000259308 | gnorm 0.503 | loss_scale 8 | train_wall 4438 | gb_free 3.3 | wall 88388
2022-02-19 08:57:11 | INFO | fairseq.trainer | begin training epoch 20
2022-02-19 08:57:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 08:59:55 | INFO | train_inner | epoch 020:     28 / 788 loss=6.459, nll_loss=5.056, ppl=33.26, wps=10660.8, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=14900, lr=0.000259064, gnorm=0.506, loss_scale=8, train_wall=561, gb_free=3.3, wall=88553
2022-02-19 09:09:43 | INFO | train_inner | epoch 020:    128 / 788 loss=6.371, nll_loss=4.958, ppl=31.08, wps=11146, ups=0.17, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.499, loss_scale=16, train_wall=565, gb_free=3.3, wall=89141
2022-02-19 09:16:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:19:36 | INFO | train_inner | epoch 020:    229 / 788 loss=6.395, nll_loss=4.985, ppl=31.68, wps=11053.4, ups=0.17, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.498, loss_scale=16, train_wall=569, gb_free=3.3, wall=89734
2022-02-19 09:28:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:29:28 | INFO | train_inner | epoch 020:    330 / 788 loss=6.403, nll_loss=4.994, ppl=31.86, wps=11059.6, ups=0.17, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.492, loss_scale=16, train_wall=569, gb_free=3.3, wall=90326
2022-02-19 09:39:15 | INFO | train_inner | epoch 020:    430 / 788 loss=6.428, nll_loss=5.022, ppl=32.49, wps=11168.8, ups=0.17, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.496, loss_scale=16, train_wall=564, gb_free=3.3, wall=90913
2022-02-19 09:42:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 09:45:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 09:49:13 | INFO | train_inner | epoch 020:    532 / 788 loss=6.449, nll_loss=5.045, ppl=33.01, wps=10952.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.522, loss_scale=8, train_wall=575, gb_free=3.3, wall=91511
2022-02-19 09:59:00 | INFO | train_inner | epoch 020:    632 / 788 loss=6.459, nll_loss=5.056, ppl=33.27, wps=11166.2, ups=0.17, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.512, loss_scale=16, train_wall=564, gb_free=3.3, wall=92098
2022-02-19 10:08:47 | INFO | train_inner | epoch 020:    732 / 788 loss=6.481, nll_loss=5.081, ppl=33.85, wps=11167.6, ups=0.17, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.506, loss_scale=16, train_wall=564, gb_free=3.3, wall=92685
2022-02-19 10:10:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 10:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 10:14:21 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.857 | nll_loss 5.465 | ppl 44.16 | wps 27331 | wpb 510.9 | bsz 1 | num_updates 15655 | best_loss 6.854
2022-02-19 10:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15655 updates
2022-02-19 10:14:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint20.pt
2022-02-19 10:14:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint20.pt
2022-02-19 10:14:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint20.pt (epoch 20 @ 15655 updates, score 6.857) (writing took 12.66006024274975 seconds)
2022-02-19 10:14:34 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-19 10:14:34 | INFO | train | epoch 020 | loss 6.428 | nll_loss 5.021 | ppl 32.48 | wps 11045 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 15655 | lr 0.00025274 | gnorm 0.503 | loss_scale 16 | train_wall 4439 | gb_free 3.3 | wall 93032
2022-02-19 10:14:34 | INFO | fairseq.trainer | begin training epoch 21
2022-02-19 10:14:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 10:18:58 | INFO | train_inner | epoch 021:     45 / 788 loss=6.417, nll_loss=5.009, ppl=32.21, wps=10681.2, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.504, loss_scale=16, train_wall=567, gb_free=3.3, wall=93296
2022-02-19 10:23:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 10:28:51 | INFO | train_inner | epoch 021:    146 / 788 loss=6.353, nll_loss=4.938, ppl=30.65, wps=11054.2, ups=0.17, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.503, loss_scale=16, train_wall=569, gb_free=3.3, wall=93889
2022-02-19 10:32:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 10:38:43 | INFO | train_inner | epoch 021:    247 / 788 loss=6.378, nll_loss=4.966, ppl=31.26, wps=11059.5, ups=0.17, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.515, loss_scale=8, train_wall=569, gb_free=3.3, wall=94481
2022-02-19 10:48:30 | INFO | train_inner | epoch 021:    347 / 788 loss=6.392, nll_loss=4.981, ppl=31.59, wps=11167.6, ups=0.17, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.499, loss_scale=16, train_wall=564, gb_free=3.3, wall=95068
2022-02-19 10:58:18 | INFO | train_inner | epoch 021:    447 / 788 loss=6.415, nll_loss=5.007, ppl=32.17, wps=11146.7, ups=0.17, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.49, loss_scale=32, train_wall=565, gb_free=3.3, wall=95656
2022-02-19 10:58:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:08:11 | INFO | train_inner | epoch 021:    548 / 788 loss=6.424, nll_loss=5.017, ppl=32.37, wps=11061.5, ups=0.17, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.506, loss_scale=16, train_wall=569, gb_free=3.3, wall=96249
2022-02-19 11:11:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:18:03 | INFO | train_inner | epoch 021:    649 / 788 loss=6.439, nll_loss=5.034, ppl=32.76, wps=11061.7, ups=0.17, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.502, loss_scale=16, train_wall=569, gb_free=3.3, wall=96841
2022-02-19 11:26:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:27:56 | INFO | train_inner | epoch 021:    750 / 788 loss=6.451, nll_loss=5.047, ppl=33.06, wps=11061.8, ups=0.17, wpb=65534.7, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.499, loss_scale=16, train_wall=569, gb_free=3.3, wall=97433
2022-02-19 11:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 11:31:44 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.85 | nll_loss 5.459 | ppl 43.99 | wps 27348.7 | wpb 510.9 | bsz 1 | num_updates 16438 | best_loss 6.85
2022-02-19 11:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16438 updates
2022-02-19 11:31:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint21.pt
2022-02-19 11:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint21.pt
2022-02-19 11:32:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint21.pt (epoch 21 @ 16438 updates, score 6.85) (writing took 19.413505502045155 seconds)
2022-02-19 11:32:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-19 11:32:03 | INFO | train | epoch 021 | loss 6.406 | nll_loss 4.997 | ppl 31.93 | wps 11030.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 16438 | lr 0.000246647 | gnorm 0.503 | loss_scale 16 | train_wall 4439 | gb_free 3.3 | wall 97681
2022-02-19 11:32:03 | INFO | fairseq.trainer | begin training epoch 22
2022-02-19 11:32:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 11:38:07 | INFO | train_inner | epoch 022:     62 / 788 loss=6.367, nll_loss=4.954, ppl=30.99, wps=10668.8, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.523, loss_scale=16, train_wall=561, gb_free=3.3, wall=98045
2022-02-19 11:39:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:48:00 | INFO | train_inner | epoch 022:    163 / 788 loss=6.331, nll_loss=4.914, ppl=30.15, wps=11050.3, ups=0.17, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.508, loss_scale=16, train_wall=570, gb_free=3.3, wall=98638
2022-02-19 11:51:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 11:57:53 | INFO | train_inner | epoch 022:    264 / 788 loss=6.358, nll_loss=4.943, ppl=30.76, wps=11056.8, ups=0.17, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.525, loss_scale=16, train_wall=569, gb_free=3.3, wall=99231
2022-02-19 12:04:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 12:07:45 | INFO | train_inner | epoch 022:    365 / 788 loss=6.382, nll_loss=4.971, ppl=31.36, wps=11061.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.5, loss_scale=16, train_wall=569, gb_free=3.3, wall=99823
2022-02-19 12:17:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 12:17:38 | INFO | train_inner | epoch 022:    466 / 788 loss=6.392, nll_loss=4.981, ppl=31.59, wps=11059.1, ups=0.17, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.514, loss_scale=16, train_wall=569, gb_free=3.3, wall=100416
2022-02-19 12:23:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 12:27:30 | INFO | train_inner | epoch 022:    567 / 788 loss=6.397, nll_loss=4.987, ppl=31.71, wps=11063.1, ups=0.17, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.519, loss_scale=8, train_wall=569, gb_free=3.3, wall=101008
2022-02-19 12:37:17 | INFO | train_inner | epoch 022:    667 / 788 loss=6.422, nll_loss=5.015, ppl=32.33, wps=11171.4, ups=0.17, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.507, loss_scale=16, train_wall=564, gb_free=3.3, wall=101595
2022-02-19 12:47:04 | INFO | train_inner | epoch 022:    767 / 788 loss=6.443, nll_loss=5.038, ppl=32.85, wps=11158.8, ups=0.17, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.499, loss_scale=16, train_wall=564, gb_free=3.3, wall=102182
2022-02-19 12:49:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 12:49:13 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.847 | nll_loss 5.466 | ppl 44.2 | wps 27197.8 | wpb 510.9 | bsz 1 | num_updates 17221 | best_loss 6.847
2022-02-19 12:49:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17221 updates
2022-02-19 12:49:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint22.pt
2022-02-19 12:49:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint22.pt
2022-02-19 12:49:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint22.pt (epoch 22 @ 17221 updates, score 6.847) (writing took 19.464052747003734 seconds)
2022-02-19 12:49:33 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-19 12:49:33 | INFO | train | epoch 022 | loss 6.385 | nll_loss 4.973 | ppl 31.41 | wps 11030.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 17221 | lr 0.000240974 | gnorm 0.511 | loss_scale 32 | train_wall 4439 | gb_free 3.3 | wall 102331
2022-02-19 12:49:33 | INFO | fairseq.trainer | begin training epoch 23
2022-02-19 12:49:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 12:49:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 12:57:22 | INFO | train_inner | epoch 023:     80 / 788 loss=6.335, nll_loss=4.919, ppl=30.24, wps=10558.3, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.507, loss_scale=16, train_wall=567, gb_free=3.3, wall=102800
2022-02-19 12:58:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 13:07:15 | INFO | train_inner | epoch 023:    181 / 788 loss=6.323, nll_loss=4.904, ppl=29.95, wps=11058.2, ups=0.17, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.521, loss_scale=8, train_wall=569, gb_free=3.3, wall=103393
2022-02-19 13:17:01 | INFO | train_inner | epoch 023:    281 / 788 loss=6.336, nll_loss=4.919, ppl=30.26, wps=11170.5, ups=0.17, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.507, loss_scale=16, train_wall=564, gb_free=3.3, wall=103979
2022-02-19 13:17:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 13:26:54 | INFO | train_inner | epoch 023:    382 / 788 loss=6.352, nll_loss=4.937, ppl=30.64, wps=11064.8, ups=0.17, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.508, loss_scale=8, train_wall=569, gb_free=3.3, wall=104572
2022-02-19 13:36:40 | INFO | train_inner | epoch 023:    482 / 788 loss=6.376, nll_loss=4.964, ppl=31.21, wps=11174.2, ups=0.17, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.515, loss_scale=16, train_wall=563, gb_free=3.3, wall=105158
2022-02-19 13:42:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 13:46:33 | INFO | train_inner | epoch 023:    583 / 788 loss=6.39, nll_loss=4.979, ppl=31.53, wps=11061.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.512, loss_scale=16, train_wall=569, gb_free=3.3, wall=105751
2022-02-19 13:55:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 13:56:25 | INFO | train_inner | epoch 023:    684 / 788 loss=6.407, nll_loss=4.998, ppl=31.96, wps=11059.8, ups=0.17, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.505, loss_scale=16, train_wall=569, gb_free=3.3, wall=106343
2022-02-19 14:06:12 | INFO | train_inner | epoch 023:    784 / 788 loss=6.416, nll_loss=5.008, ppl=32.17, wps=11169.6, ups=0.17, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.507, loss_scale=16, train_wall=564, gb_free=3.3, wall=106930
2022-02-19 14:06:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 14:06:41 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.842 | nll_loss 5.452 | ppl 43.78 | wps 27420.4 | wpb 510.9 | bsz 1 | num_updates 18004 | best_loss 6.842
2022-02-19 14:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18004 updates
2022-02-19 14:06:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint23.pt
2022-02-19 14:06:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint23.pt
2022-02-19 14:07:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint23.pt (epoch 23 @ 18004 updates, score 6.842) (writing took 19.405625975690782 seconds)
2022-02-19 14:07:00 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-19 14:07:00 | INFO | train | epoch 023 | loss 6.365 | nll_loss 4.952 | ppl 30.95 | wps 11034.8 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 18004 | lr 0.000235676 | gnorm 0.51 | loss_scale 16 | train_wall 4438 | gb_free 3.3 | wall 106978
2022-02-19 14:07:00 | INFO | fairseq.trainer | begin training epoch 24
2022-02-19 14:07:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 14:08:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:16:29 | INFO | train_inner | epoch 024:     97 / 788 loss=6.285, nll_loss=4.862, ppl=29.09, wps=10566.4, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.522, loss_scale=16, train_wall=566, gb_free=3.3, wall=107547
2022-02-19 14:21:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:26:22 | INFO | train_inner | epoch 024:    198 / 788 loss=6.297, nll_loss=4.876, ppl=29.36, wps=11048.6, ups=0.17, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.509, loss_scale=16, train_wall=570, gb_free=3.3, wall=108140
2022-02-19 14:34:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:36:15 | INFO | train_inner | epoch 024:    299 / 788 loss=6.333, nll_loss=4.915, ppl=30.17, wps=11051.5, ups=0.17, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.506, loss_scale=16, train_wall=569, gb_free=3.3, wall=108733
2022-02-19 14:46:03 | INFO | train_inner | epoch 024:    399 / 788 loss=6.342, nll_loss=4.926, ppl=30.4, wps=11145.4, ups=0.17, wpb=65534.7, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.516, loss_scale=16, train_wall=565, gb_free=3.3, wall=109321
2022-02-19 14:50:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 14:55:56 | INFO | train_inner | epoch 024:    500 / 788 loss=6.367, nll_loss=4.954, ppl=30.99, wps=11063.4, ups=0.17, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.501, loss_scale=16, train_wall=569, gb_free=3.3, wall=109914
2022-02-19 15:03:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:05:48 | INFO | train_inner | epoch 024:    601 / 788 loss=6.374, nll_loss=4.962, ppl=31.17, wps=11058.4, ups=0.17, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.515, loss_scale=16, train_wall=569, gb_free=3.3, wall=110506
2022-02-19 15:15:35 | INFO | train_inner | epoch 024:    701 / 788 loss=6.393, nll_loss=4.982, ppl=31.6, wps=11167.5, ups=0.17, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.505, loss_scale=16, train_wall=564, gb_free=3.3, wall=111093
2022-02-19 15:15:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:24:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 15:24:11 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.844 | nll_loss 5.456 | ppl 43.89 | wps 27309.6 | wpb 510.9 | bsz 1 | num_updates 18786 | best_loss 6.842
2022-02-19 15:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18786 updates
2022-02-19 15:24:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint24.pt
2022-02-19 15:24:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint24.pt
2022-02-19 15:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint24.pt (epoch 24 @ 18786 updates, score 6.844) (writing took 12.70003472827375 seconds)
2022-02-19 15:24:24 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-19 15:24:24 | INFO | train | epoch 024 | loss 6.348 | nll_loss 4.932 | ppl 30.53 | wps 11029.7 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 18786 | lr 0.000230719 | gnorm 0.511 | loss_scale 16 | train_wall 4440 | gb_free 3.3 | wall 111622
2022-02-19 15:24:24 | INFO | fairseq.trainer | begin training epoch 25
2022-02-19 15:24:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 15:25:46 | INFO | train_inner | epoch 025:     14 / 788 loss=6.379, nll_loss=4.967, ppl=31.27, wps=10682.2, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=18800, lr=0.000230633, gnorm=0.517, loss_scale=16, train_wall=566, gb_free=3.3, wall=111704
2022-02-19 15:29:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:35:38 | INFO | train_inner | epoch 025:    115 / 788 loss=6.273, nll_loss=4.849, ppl=28.82, wps=11061.3, ups=0.17, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.516, loss_scale=16, train_wall=569, gb_free=3.3, wall=112296
2022-02-19 15:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:45:31 | INFO | train_inner | epoch 025:    216 / 788 loss=6.298, nll_loss=4.876, ppl=29.37, wps=11054.1, ups=0.17, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.51, loss_scale=16, train_wall=569, gb_free=3.3, wall=112889
2022-02-19 15:54:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 15:54:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 15:55:29 | INFO | train_inner | epoch 025:    318 / 788 loss=6.314, nll_loss=4.894, ppl=29.74, wps=10956.4, ups=0.17, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.526, loss_scale=8, train_wall=575, gb_free=3.3, wall=113487
2022-02-19 16:05:15 | INFO | train_inner | epoch 025:    418 / 788 loss=6.331, nll_loss=4.914, ppl=30.14, wps=11184.1, ups=0.17, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.517, loss_scale=8, train_wall=563, gb_free=3.3, wall=114073
2022-02-19 16:15:02 | INFO | train_inner | epoch 025:    518 / 788 loss=6.349, nll_loss=4.933, ppl=30.55, wps=11176.9, ups=0.17, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.527, loss_scale=16, train_wall=563, gb_free=3.3, wall=114660
2022-02-19 16:20:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 16:24:55 | INFO | train_inner | epoch 025:    619 / 788 loss=6.358, nll_loss=4.943, ppl=30.76, wps=11056.3, ups=0.17, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.526, loss_scale=16, train_wall=569, gb_free=3.3, wall=115252
2022-02-19 16:33:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 16:34:48 | INFO | train_inner | epoch 025:    720 / 788 loss=6.368, nll_loss=4.955, ppl=31.01, wps=11046.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.514, loss_scale=16, train_wall=570, gb_free=3.3, wall=115846
2022-02-19 16:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 16:41:33 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.844 | nll_loss 5.457 | ppl 43.92 | wps 27346.8 | wpb 510.9 | bsz 1 | num_updates 19568 | best_loss 6.842
2022-02-19 16:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19568 updates
2022-02-19 16:41:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint25.pt
2022-02-19 16:41:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint25.pt
2022-02-19 16:41:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint25.pt (epoch 25 @ 19568 updates, score 6.844) (writing took 13.052193406037986 seconds)
2022-02-19 16:41:46 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-19 16:41:46 | INFO | train | epoch 025 | loss 6.331 | nll_loss 4.913 | ppl 30.13 | wps 11033.6 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 19568 | lr 0.000226062 | gnorm 0.519 | loss_scale 16 | train_wall 4438 | gb_free 3.3 | wall 116264
2022-02-19 16:41:46 | INFO | fairseq.trainer | begin training epoch 26
2022-02-19 16:41:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 16:44:53 | INFO | train_inner | epoch 026:     32 / 788 loss=6.34, nll_loss=4.923, ppl=30.34, wps=10769.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=19600, lr=0.000225877, gnorm=0.515, loss_scale=16, train_wall=561, gb_free=3.3, wall=116451
2022-02-19 16:49:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 16:54:45 | INFO | train_inner | epoch 026:    133 / 788 loss=6.256, nll_loss=4.83, ppl=28.44, wps=11071.3, ups=0.17, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.513, loss_scale=16, train_wall=569, gb_free=3.3, wall=117043
2022-02-19 17:02:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 17:04:39 | INFO | train_inner | epoch 026:    234 / 788 loss=6.288, nll_loss=4.865, ppl=29.14, wps=11048.7, ups=0.17, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.52, loss_scale=16, train_wall=570, gb_free=3.3, wall=117637
2022-02-19 17:14:25 | INFO | train_inner | epoch 026:    334 / 788 loss=6.301, nll_loss=4.88, ppl=29.44, wps=11170.2, ups=0.17, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.513, loss_scale=16, train_wall=564, gb_free=3.3, wall=118223
2022-02-19 17:15:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 17:24:18 | INFO | train_inner | epoch 026:    435 / 788 loss=6.317, nll_loss=4.898, ppl=29.81, wps=11064.7, ups=0.17, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.521, loss_scale=16, train_wall=569, gb_free=3.3, wall=118816
2022-02-19 17:29:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 17:34:10 | INFO | train_inner | epoch 026:    536 / 788 loss=6.332, nll_loss=4.914, ppl=30.15, wps=11069.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.523, loss_scale=16, train_wall=569, gb_free=3.3, wall=119408
2022-02-19 17:42:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 17:44:02 | INFO | train_inner | epoch 026:    637 / 788 loss=6.349, nll_loss=4.933, ppl=30.55, wps=11059.5, ups=0.17, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.521, loss_scale=16, train_wall=569, gb_free=3.3, wall=120000
2022-02-19 17:53:48 | INFO | train_inner | epoch 026:    737 / 788 loss=6.358, nll_loss=4.943, ppl=30.76, wps=11179.3, ups=0.17, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.52, loss_scale=16, train_wall=563, gb_free=3.3, wall=120586
2022-02-19 17:58:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 17:58:53 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.841 | nll_loss 5.447 | ppl 43.61 | wps 27470.7 | wpb 510.9 | bsz 1 | num_updates 20351 | best_loss 6.841
2022-02-19 17:58:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20351 updates
2022-02-19 17:58:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint26.pt
2022-02-19 17:58:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint26.pt
2022-02-19 17:59:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint26.pt (epoch 26 @ 20351 updates, score 6.841) (writing took 19.552129168063402 seconds)
2022-02-19 17:59:12 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-19 17:59:12 | INFO | train | epoch 026 | loss 6.316 | nll_loss 4.896 | ppl 29.78 | wps 11037.6 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 20351 | lr 0.00022167 | gnorm 0.518 | loss_scale 32 | train_wall 4436 | gb_free 3.3 | wall 120910
2022-02-19 17:59:12 | INFO | fairseq.trainer | begin training epoch 27
2022-02-19 17:59:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 18:00:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 18:04:05 | INFO | train_inner | epoch 027:     50 / 788 loss=6.307, nll_loss=4.886, ppl=29.58, wps=10575.3, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.517, loss_scale=16, train_wall=566, gb_free=3.3, wall=121203
2022-02-19 18:04:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-19 18:13:58 | INFO | train_inner | epoch 027:    151 / 788 loss=6.246, nll_loss=4.818, ppl=28.21, wps=11051.4, ups=0.17, wpb=65534.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.532, loss_scale=8, train_wall=569, gb_free=3.3, wall=121796
2022-02-19 18:23:45 | INFO | train_inner | epoch 027:    251 / 788 loss=6.265, nll_loss=4.839, ppl=28.63, wps=11174, ups=0.17, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.523, loss_scale=16, train_wall=563, gb_free=3.3, wall=122383
2022-02-19 18:30:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 18:33:38 | INFO | train_inner | epoch 027:    352 / 788 loss=6.3, nll_loss=4.878, ppl=29.4, wps=11053.5, ups=0.17, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.538, loss_scale=16, train_wall=569, gb_free=3.3, wall=122976
2022-02-19 18:43:24 | INFO | train_inner | epoch 027:    452 / 788 loss=6.305, nll_loss=4.884, ppl=29.53, wps=11179.1, ups=0.17, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.524, loss_scale=32, train_wall=563, gb_free=3.3, wall=123562
2022-02-19 18:46:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 18:53:16 | INFO | train_inner | epoch 027:    553 / 788 loss=6.331, nll_loss=4.913, ppl=30.12, wps=11071.6, ups=0.17, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.521, loss_scale=16, train_wall=569, gb_free=3.3, wall=124154
2022-02-19 19:00:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:03:08 | INFO | train_inner | epoch 027:    654 / 788 loss=6.329, nll_loss=4.911, ppl=30.08, wps=11059.6, ups=0.17, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.52, loss_scale=16, train_wall=569, gb_free=3.3, wall=124746
2022-02-19 19:12:54 | INFO | train_inner | epoch 027:    754 / 788 loss=6.349, nll_loss=4.933, ppl=30.54, wps=11182.8, ups=0.17, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.533, loss_scale=16, train_wall=563, gb_free=3.3, wall=125332
2022-02-19 19:14:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:16:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 19:16:19 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.844 | nll_loss 5.463 | ppl 44.11 | wps 27418.9 | wpb 510.9 | bsz 1 | num_updates 21133 | best_loss 6.841
2022-02-19 19:16:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21133 updates
2022-02-19 19:16:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint27.pt
2022-02-19 19:16:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint27.pt
2022-02-19 19:16:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint27.pt (epoch 27 @ 21133 updates, score 6.844) (writing took 12.672015398740768 seconds)
2022-02-19 19:16:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-19 19:16:32 | INFO | train | epoch 027 | loss 6.301 | nll_loss 4.88 | ppl 29.44 | wps 11039.7 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 21133 | lr 0.00021753 | gnorm 0.527 | loss_scale 16 | train_wall 4436 | gb_free 3.3 | wall 125550
2022-02-19 19:16:32 | INFO | fairseq.trainer | begin training epoch 28
2022-02-19 19:16:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 19:23:06 | INFO | train_inner | epoch 028:     67 / 788 loss=6.264, nll_loss=4.838, ppl=28.61, wps=10675.7, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.522, loss_scale=16, train_wall=567, gb_free=3.3, wall=125943
2022-02-19 19:27:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:32:58 | INFO | train_inner | epoch 028:    168 / 788 loss=6.242, nll_loss=4.815, ppl=28.14, wps=11055, ups=0.17, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.529, loss_scale=16, train_wall=569, gb_free=3.3, wall=126536
2022-02-19 19:42:45 | INFO | train_inner | epoch 028:    268 / 788 loss=6.254, nll_loss=4.827, ppl=28.39, wps=11170, ups=0.17, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.519, loss_scale=32, train_wall=563, gb_free=3.3, wall=127123
2022-02-19 19:43:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 19:52:39 | INFO | train_inner | epoch 028:    369 / 788 loss=6.278, nll_loss=4.854, ppl=28.93, wps=11032.9, ups=0.17, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.516, loss_scale=16, train_wall=570, gb_free=3.3, wall=127717
2022-02-19 19:59:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:02:34 | INFO | train_inner | epoch 028:    470 / 788 loss=6.299, nll_loss=4.877, ppl=29.39, wps=11008.1, ups=0.17, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.535, loss_scale=16, train_wall=571, gb_free=3.3, wall=128312
2022-02-19 20:12:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:12:28 | INFO | train_inner | epoch 028:    571 / 788 loss=6.32, nll_loss=4.9, ppl=29.87, wps=11047.6, ups=0.17, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.519, loss_scale=16, train_wall=570, gb_free=3.3, wall=128906
2022-02-19 20:22:16 | INFO | train_inner | epoch 028:    671 / 788 loss=6.323, nll_loss=4.904, ppl=29.95, wps=11135.4, ups=0.17, wpb=65534.7, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.529, loss_scale=16, train_wall=565, gb_free=3.3, wall=129494
2022-02-19 20:24:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:32:10 | INFO | train_inner | epoch 028:    772 / 788 loss=6.336, nll_loss=4.918, ppl=30.24, wps=11043.5, ups=0.17, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.542, loss_scale=16, train_wall=570, gb_free=3.3, wall=130088
2022-02-19 20:33:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 20:33:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.841 | nll_loss 5.447 | ppl 43.63 | wps 27403.1 | wpb 510.9 | bsz 1 | num_updates 21916 | best_loss 6.841
2022-02-19 20:33:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 21916 updates
2022-02-19 20:33:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint28.pt
2022-02-19 20:33:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint28.pt
2022-02-19 20:34:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint28.pt (epoch 28 @ 21916 updates, score 6.841) (writing took 20.213604441843927 seconds)
2022-02-19 20:34:09 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-19 20:34:09 | INFO | train | epoch 028 | loss 6.288 | nll_loss 4.865 | ppl 29.15 | wps 11011.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 21916 | lr 0.000213609 | gnorm 0.528 | loss_scale 16 | train_wall 4445 | gb_free 3.3 | wall 130207
2022-02-19 20:34:09 | INFO | fairseq.trainer | begin training epoch 29
2022-02-19 20:34:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 20:41:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 20:42:28 | INFO | train_inner | epoch 029:     85 / 788 loss=6.24, nll_loss=4.812, ppl=28.09, wps=10541.4, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.525, loss_scale=16, train_wall=567, gb_free=3.3, wall=130706
2022-02-19 20:52:16 | INFO | train_inner | epoch 029:    185 / 788 loss=6.226, nll_loss=4.796, ppl=27.77, wps=11151, ups=0.17, wpb=65534.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.526, loss_scale=16, train_wall=564, gb_free=3.3, wall=131294
2022-02-19 20:56:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 21:02:10 | INFO | train_inner | epoch 029:    286 / 788 loss=6.25, nll_loss=4.823, ppl=28.3, wps=11042.2, ups=0.17, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.528, loss_scale=16, train_wall=570, gb_free=3.3, wall=131888
2022-02-19 21:10:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 21:12:03 | INFO | train_inner | epoch 029:    387 / 788 loss=6.272, nll_loss=4.848, ppl=28.79, wps=11045.5, ups=0.17, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.524, loss_scale=16, train_wall=570, gb_free=3.3, wall=132481
2022-02-19 21:21:50 | INFO | train_inner | epoch 029:    487 / 788 loss=6.286, nll_loss=4.862, ppl=29.09, wps=11157.6, ups=0.17, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.544, loss_scale=16, train_wall=564, gb_free=3.3, wall=133068
2022-02-19 21:23:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 21:31:44 | INFO | train_inner | epoch 029:    588 / 788 loss=6.294, nll_loss=4.872, ppl=29.28, wps=11046.8, ups=0.17, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.531, loss_scale=16, train_wall=570, gb_free=3.3, wall=133662
2022-02-19 21:36:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 21:41:37 | INFO | train_inner | epoch 029:    689 / 788 loss=6.316, nll_loss=4.897, ppl=29.79, wps=11046.1, ups=0.17, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.543, loss_scale=16, train_wall=570, gb_free=3.3, wall=134255
2022-02-19 21:50:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 21:51:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 21:51:27 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.845 | nll_loss 5.456 | ppl 43.89 | wps 26835.9 | wpb 510.9 | bsz 1 | num_updates 22698 | best_loss 6.841
2022-02-19 21:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22698 updates
2022-02-19 21:51:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint29.pt
2022-02-19 21:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint29.pt
2022-02-19 21:51:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint29.pt (epoch 29 @ 22698 updates, score 6.845) (writing took 12.789172939956188 seconds)
2022-02-19 21:51:39 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-19 21:51:39 | INFO | train | epoch 029 | loss 6.274 | nll_loss 4.85 | ppl 28.83 | wps 11014.1 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 22698 | lr 0.000209897 | gnorm 0.53 | loss_scale 16 | train_wall 4445 | gb_free 3.3 | wall 134857
2022-02-19 21:51:40 | INFO | fairseq.trainer | begin training epoch 30
2022-02-19 21:51:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 21:51:51 | INFO | train_inner | epoch 030:      2 / 788 loss=6.32, nll_loss=4.901, ppl=29.88, wps=10616, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=22700, lr=0.000209888, gnorm=0.526, loss_scale=16, train_wall=569, gb_free=3.3, wall=134869
2022-02-19 22:01:42 | INFO | train_inner | epoch 030:    102 / 788 loss=6.196, nll_loss=4.763, ppl=27.15, wps=11097.8, ups=0.17, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.531, loss_scale=16, train_wall=566, gb_free=3.3, wall=135460
2022-02-19 22:03:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 22:11:36 | INFO | train_inner | epoch 030:    203 / 788 loss=6.225, nll_loss=4.795, ppl=27.76, wps=11029.7, ups=0.17, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.525, loss_scale=16, train_wall=571, gb_free=3.3, wall=136054
2022-02-19 22:19:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 22:21:30 | INFO | train_inner | epoch 030:    304 / 788 loss=6.247, nll_loss=4.819, ppl=28.22, wps=11042.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.53, loss_scale=16, train_wall=570, gb_free=3.3, wall=136648
2022-02-19 22:31:17 | INFO | train_inner | epoch 030:    404 / 788 loss=6.266, nll_loss=4.84, ppl=28.64, wps=11149.7, ups=0.17, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.531, loss_scale=16, train_wall=565, gb_free=3.3, wall=137235
2022-02-19 22:31:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 22:41:11 | INFO | train_inner | epoch 030:    505 / 788 loss=6.276, nll_loss=4.852, ppl=28.87, wps=11044.4, ups=0.17, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.532, loss_scale=16, train_wall=570, gb_free=3.3, wall=137829
2022-02-19 22:45:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 22:51:04 | INFO | train_inner | epoch 030:    606 / 788 loss=6.286, nll_loss=4.863, ppl=29.1, wps=11048.2, ups=0.17, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.538, loss_scale=16, train_wall=570, gb_free=3.3, wall=138422
2022-02-19 23:00:52 | INFO | train_inner | epoch 030:    706 / 788 loss=6.302, nll_loss=4.88, ppl=29.45, wps=11149.3, ups=0.17, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.528, loss_scale=32, train_wall=565, gb_free=3.3, wall=139010
2022-02-19 23:01:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 23:08:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-19 23:08:59 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.848 | nll_loss 5.459 | ppl 43.98 | wps 27248.1 | wpb 510.9 | bsz 1 | num_updates 23481 | best_loss 6.841
2022-02-19 23:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23481 updates
2022-02-19 23:08:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint30.pt
2022-02-19 23:09:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint30.pt
2022-02-19 23:09:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint30.pt (epoch 30 @ 23481 updates, score 6.848) (writing took 12.856863909401 seconds)
2022-02-19 23:09:12 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-19 23:09:12 | INFO | train | epoch 030 | loss 6.262 | nll_loss 4.836 | ppl 28.57 | wps 11023.5 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 23481 | lr 0.000206368 | gnorm 0.531 | loss_scale 16 | train_wall 4449 | gb_free 3.3 | wall 139510
2022-02-19 23:09:12 | INFO | fairseq.trainer | begin training epoch 31
2022-02-19 23:09:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-19 23:11:03 | INFO | train_inner | epoch 031:     19 / 788 loss=6.288, nll_loss=4.864, ppl=29.13, wps=10664.6, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=23500, lr=0.000206284, gnorm=0.533, loss_scale=16, train_wall=568, gb_free=3.3, wall=139621
2022-02-19 23:16:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 23:20:57 | INFO | train_inner | epoch 031:    120 / 788 loss=6.184, nll_loss=4.749, ppl=26.89, wps=11039.3, ups=0.17, wpb=65534.7, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.531, loss_scale=16, train_wall=570, gb_free=3.3, wall=140215
2022-02-19 23:30:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 23:30:51 | INFO | train_inner | epoch 031:    221 / 788 loss=6.209, nll_loss=4.777, ppl=27.42, wps=11038.2, ups=0.17, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.538, loss_scale=16, train_wall=570, gb_free=3.3, wall=140809
2022-02-19 23:40:39 | INFO | train_inner | epoch 031:    321 / 788 loss=6.238, nll_loss=4.809, ppl=28.03, wps=11146.9, ups=0.17, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.538, loss_scale=16, train_wall=565, gb_free=3.3, wall=141397
2022-02-19 23:44:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-19 23:50:32 | INFO | train_inner | epoch 031:    422 / 788 loss=6.254, nll_loss=4.827, ppl=28.39, wps=11046.4, ups=0.17, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.543, loss_scale=16, train_wall=570, gb_free=3.3, wall=141990
2022-02-19 23:57:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 00:00:25 | INFO | train_inner | epoch 031:    523 / 788 loss=6.269, nll_loss=4.843, ppl=28.7, wps=11046, ups=0.17, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.549, loss_scale=16, train_wall=570, gb_free=3.3, wall=142583
2022-02-20 00:10:13 | INFO | train_inner | epoch 031:    623 / 788 loss=6.291, nll_loss=4.868, ppl=29.21, wps=11157.2, ups=0.17, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.532, loss_scale=16, train_wall=564, gb_free=3.3, wall=143171
2022-02-20 00:10:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 00:20:06 | INFO | train_inner | epoch 031:    724 / 788 loss=6.297, nll_loss=4.875, ppl=29.34, wps=11052.6, ups=0.17, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.523, loss_scale=16, train_wall=570, gb_free=3.3, wall=143764
2022-02-20 00:23:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 00:26:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 00:26:27 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.843 | nll_loss 5.456 | ppl 43.9 | wps 27161 | wpb 510.9 | bsz 1 | num_updates 24263 | best_loss 6.841
2022-02-20 00:26:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24263 updates
2022-02-20 00:26:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint31.pt
2022-02-20 00:26:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint31.pt
2022-02-20 00:26:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint31.pt (epoch 31 @ 24263 updates, score 6.843) (writing took 12.789006565697491 seconds)
2022-02-20 00:26:40 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-20 00:26:40 | INFO | train | epoch 031 | loss 6.251 | nll_loss 4.824 | ppl 28.32 | wps 11018.3 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 24263 | lr 0.000203015 | gnorm 0.537 | loss_scale 16 | train_wall 4446 | gb_free 3.3 | wall 144158
2022-02-20 00:26:40 | INFO | fairseq.trainer | begin training epoch 32
2022-02-20 00:26:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 00:30:18 | INFO | train_inner | epoch 032:     37 / 788 loss=6.249, nll_loss=4.822, ppl=28.29, wps=10660.1, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=24300, lr=0.00020286, gnorm=0.541, loss_scale=16, train_wall=567, gb_free=3.3, wall=144376
2022-02-20 00:37:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 00:40:11 | INFO | train_inner | epoch 032:    138 / 788 loss=6.19, nll_loss=4.755, ppl=27.01, wps=11042.5, ups=0.17, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.544, loss_scale=16, train_wall=570, gb_free=3.3, wall=144969
2022-02-20 00:49:59 | INFO | train_inner | epoch 032:    238 / 788 loss=6.206, nll_loss=4.774, ppl=27.36, wps=11150.8, ups=0.17, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.531, loss_scale=16, train_wall=565, gb_free=3.3, wall=145557
2022-02-20 00:50:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 00:59:52 | INFO | train_inner | epoch 032:    339 / 788 loss=6.225, nll_loss=4.794, ppl=27.74, wps=11045.5, ups=0.17, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.534, loss_scale=16, train_wall=570, gb_free=3.3, wall=146150
2022-02-20 01:04:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 01:09:46 | INFO | train_inner | epoch 032:    440 / 788 loss=6.247, nll_loss=4.819, ppl=28.23, wps=11045.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.536, loss_scale=16, train_wall=570, gb_free=3.3, wall=146743
2022-02-20 01:19:33 | INFO | train_inner | epoch 032:    540 / 788 loss=6.264, nll_loss=4.838, ppl=28.6, wps=11159.2, ups=0.17, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.548, loss_scale=32, train_wall=564, gb_free=3.3, wall=147331
2022-02-20 01:19:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 01:29:26 | INFO | train_inner | epoch 032:    641 / 788 loss=6.276, nll_loss=4.851, ppl=28.86, wps=11048.4, ups=0.17, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.538, loss_scale=16, train_wall=570, gb_free=3.3, wall=147924
2022-02-20 01:35:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 01:39:19 | INFO | train_inner | epoch 032:    742 / 788 loss=6.282, nll_loss=4.858, ppl=29.01, wps=11050.5, ups=0.17, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.529, loss_scale=16, train_wall=570, gb_free=3.3, wall=148517
2022-02-20 01:43:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 01:43:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.843 | nll_loss 5.457 | ppl 43.92 | wps 27239.2 | wpb 510.9 | bsz 1 | num_updates 25046 | best_loss 6.841
2022-02-20 01:43:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25046 updates
2022-02-20 01:43:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint32.pt
2022-02-20 01:44:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint32.pt
2022-02-20 01:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint32.pt (epoch 32 @ 25046 updates, score 6.843) (writing took 12.951091913506389 seconds)
2022-02-20 01:44:08 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-20 01:44:08 | INFO | train | epoch 032 | loss 6.241 | nll_loss 4.812 | ppl 28.09 | wps 11035.1 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 25046 | lr 0.000199816 | gnorm 0.537 | loss_scale 16 | train_wall 4445 | gb_free 3.3 | wall 148806
2022-02-20 01:44:08 | INFO | fairseq.trainer | begin training epoch 33
2022-02-20 01:44:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 01:49:25 | INFO | train_inner | epoch 033:     54 / 788 loss=6.212, nll_loss=4.78, ppl=27.47, wps=10769.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=25100, lr=0.000199601, gnorm=0.536, loss_scale=32, train_wall=562, gb_free=3.3, wall=149123
2022-02-20 01:50:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 01:59:18 | INFO | train_inner | epoch 033:    155 / 788 loss=6.174, nll_loss=4.738, ppl=26.68, wps=11040.7, ups=0.17, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.545, loss_scale=16, train_wall=570, gb_free=3.3, wall=149716
2022-02-20 02:04:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 02:09:12 | INFO | train_inner | epoch 033:    256 / 788 loss=6.205, nll_loss=4.772, ppl=27.31, wps=11039.5, ups=0.17, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.549, loss_scale=16, train_wall=570, gb_free=3.3, wall=150310
2022-02-20 02:17:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 02:19:06 | INFO | train_inner | epoch 033:    357 / 788 loss=6.223, nll_loss=4.792, ppl=27.71, wps=11041.7, ups=0.17, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.542, loss_scale=16, train_wall=570, gb_free=3.3, wall=150904
2022-02-20 02:28:53 | INFO | train_inner | epoch 033:    457 / 788 loss=6.244, nll_loss=4.815, ppl=28.16, wps=11152.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.539, loss_scale=16, train_wall=565, gb_free=3.3, wall=151491
2022-02-20 02:31:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 02:38:47 | INFO | train_inner | epoch 033:    558 / 788 loss=6.255, nll_loss=4.827, ppl=28.39, wps=11046.6, ups=0.17, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.537, loss_scale=16, train_wall=570, gb_free=3.3, wall=152084
2022-02-20 02:44:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 02:48:40 | INFO | train_inner | epoch 033:    659 / 788 loss=6.259, nll_loss=4.832, ppl=28.49, wps=11050.6, ups=0.17, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.539, loss_scale=16, train_wall=570, gb_free=3.3, wall=152678
2022-02-20 02:58:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 02:58:35 | INFO | train_inner | epoch 033:    760 / 788 loss=6.286, nll_loss=4.862, ppl=29.08, wps=11002.1, ups=0.17, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.536, loss_scale=16, train_wall=570, gb_free=3.3, wall=153273
2022-02-20 03:01:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 03:01:27 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.841 | nll_loss 5.442 | ppl 43.48 | wps 27025.2 | wpb 510.9 | bsz 1 | num_updates 25828 | best_loss 6.841
2022-02-20 03:01:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25828 updates
2022-02-20 03:01:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint33.pt
2022-02-20 03:01:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint33.pt
2022-02-20 03:01:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint33.pt (epoch 33 @ 25828 updates, score 6.841) (writing took 22.180866269394755 seconds)
2022-02-20 03:01:49 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-20 03:01:49 | INFO | train | epoch 033 | loss 6.23 | nll_loss 4.8 | ppl 27.86 | wps 10988.5 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 25828 | lr 0.000196768 | gnorm 0.541 | loss_scale 16 | train_wall 4446 | gb_free 3.3 | wall 153467
2022-02-20 03:01:49 | INFO | fairseq.trainer | begin training epoch 34
2022-02-20 03:01:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 03:08:53 | INFO | train_inner | epoch 034:     72 / 788 loss=6.187, nll_loss=4.752, ppl=26.94, wps=10552.4, ups=0.16, wpb=65232.6, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.542, loss_scale=16, train_wall=562, gb_free=3.3, wall=153891
2022-02-20 03:10:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 03:18:47 | INFO | train_inner | epoch 034:    173 / 788 loss=6.171, nll_loss=4.734, ppl=26.62, wps=11044.9, ups=0.17, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.557, loss_scale=8, train_wall=570, gb_free=3.3, wall=154485
2022-02-20 03:28:34 | INFO | train_inner | epoch 034:    273 / 788 loss=6.197, nll_loss=4.763, ppl=27.16, wps=11152.1, ups=0.17, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.538, loss_scale=16, train_wall=565, gb_free=3.3, wall=155072
2022-02-20 03:37:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 03:38:30 | INFO | train_inner | epoch 034:    374 / 788 loss=6.201, nll_loss=4.768, ppl=27.24, wps=11003.2, ups=0.17, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.553, loss_scale=16, train_wall=571, gb_free=3.3, wall=155668
2022-02-20 03:48:21 | INFO | train_inner | epoch 034:    474 / 788 loss=6.233, nll_loss=4.803, ppl=27.91, wps=11097.7, ups=0.17, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.542, loss_scale=16, train_wall=565, gb_free=3.3, wall=156259
2022-02-20 03:50:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 03:58:14 | INFO | train_inner | epoch 034:    575 / 788 loss=6.251, nll_loss=4.824, ppl=28.32, wps=11043.5, ups=0.17, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.555, loss_scale=16, train_wall=570, gb_free=3.3, wall=156852
2022-02-20 04:03:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 04:08:08 | INFO | train_inner | epoch 034:    676 / 788 loss=6.262, nll_loss=4.835, ppl=28.55, wps=11042.6, ups=0.17, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.542, loss_scale=16, train_wall=570, gb_free=3.3, wall=157446
2022-02-20 04:16:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 04:18:01 | INFO | train_inner | epoch 034:    777 / 788 loss=6.274, nll_loss=4.849, ppl=28.82, wps=11041.8, ups=0.17, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.545, loss_scale=16, train_wall=570, gb_free=3.3, wall=158039
2022-02-20 04:19:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 04:19:11 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.847 | nll_loss 5.46 | ppl 44.02 | wps 27333.4 | wpb 510.9 | bsz 1 | num_updates 26611 | best_loss 6.841
2022-02-20 04:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 26611 updates
2022-02-20 04:19:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint34.pt
2022-02-20 04:19:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint34.pt
2022-02-20 04:19:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint34.pt (epoch 34 @ 26611 updates, score 6.847) (writing took 14.483738332986832 seconds)
2022-02-20 04:19:26 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-20 04:19:26 | INFO | train | epoch 034 | loss 6.221 | nll_loss 4.79 | ppl 27.66 | wps 11012.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 26611 | lr 0.000193852 | gnorm 0.548 | loss_scale 16 | train_wall 4447 | gb_free 3.3 | wall 158124
2022-02-20 04:19:26 | INFO | fairseq.trainer | begin training epoch 35
2022-02-20 04:19:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 04:28:10 | INFO | train_inner | epoch 035:     89 / 788 loss=6.149, nll_loss=4.71, ppl=26.18, wps=10713.7, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.557, loss_scale=16, train_wall=562, gb_free=3.3, wall=158648
2022-02-20 04:30:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 04:38:04 | INFO | train_inner | epoch 035:    190 / 788 loss=6.174, nll_loss=4.737, ppl=26.67, wps=11041.8, ups=0.17, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.55, loss_scale=16, train_wall=570, gb_free=3.3, wall=159241
2022-02-20 04:42:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 04:47:57 | INFO | train_inner | epoch 035:    291 / 788 loss=6.185, nll_loss=4.75, ppl=26.91, wps=11039.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.553, loss_scale=16, train_wall=571, gb_free=3.3, wall=159835
2022-02-20 04:53:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 04:57:52 | INFO | train_inner | epoch 035:    392 / 788 loss=6.211, nll_loss=4.779, ppl=27.45, wps=11026, ups=0.17, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.551, loss_scale=8, train_wall=570, gb_free=3.3, wall=160430
2022-02-20 05:07:39 | INFO | train_inner | epoch 035:    492 / 788 loss=6.215, nll_loss=4.783, ppl=27.53, wps=11149.8, ups=0.17, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.552, loss_scale=16, train_wall=565, gb_free=3.3, wall=161017
2022-02-20 05:11:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 05:17:33 | INFO | train_inner | epoch 035:    593 / 788 loss=6.242, nll_loss=4.813, ppl=28.12, wps=11041, ups=0.17, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.559, loss_scale=8, train_wall=570, gb_free=3.3, wall=161611
2022-02-20 05:27:21 | INFO | train_inner | epoch 035:    693 / 788 loss=6.247, nll_loss=4.818, ppl=28.21, wps=11151.9, ups=0.17, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.548, loss_scale=16, train_wall=565, gb_free=3.3, wall=162199
2022-02-20 05:36:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 05:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 05:36:44 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.849 | nll_loss 5.457 | ppl 43.92 | wps 27272.3 | wpb 510.9 | bsz 1 | num_updates 27394 | best_loss 6.841
2022-02-20 05:36:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 27394 updates
2022-02-20 05:36:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint35.pt
2022-02-20 05:36:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint35.pt
2022-02-20 05:36:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint35.pt (epoch 35 @ 27394 updates, score 6.849) (writing took 13.075662780553102 seconds)
2022-02-20 05:36:57 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-20 05:36:57 | INFO | train | epoch 035 | loss 6.211 | nll_loss 4.779 | ppl 27.45 | wps 11025.5 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 27394 | lr 0.000191061 | gnorm 0.553 | loss_scale 16 | train_wall 4448 | gb_free 3.3 | wall 162775
2022-02-20 05:36:57 | INFO | fairseq.trainer | begin training epoch 36
2022-02-20 05:36:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-20 05:37:32 | INFO | train_inner | epoch 036:      6 / 788 loss=6.266, nll_loss=4.84, ppl=28.65, wps=10661.2, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=27400, lr=0.00019104, gnorm=0.555, loss_scale=16, train_wall=568, gb_free=3.3, wall=162810
2022-02-20 05:47:16 | INFO | train_inner | epoch 036:    106 / 788 loss=6.135, nll_loss=4.694, ppl=25.88, wps=11223.5, ups=0.17, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.539, loss_scale=16, train_wall=561, gb_free=3.3, wall=163394
2022-02-20 05:49:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 05:57:06 | INFO | train_inner | epoch 036:    207 / 788 loss=6.16, nll_loss=4.722, ppl=26.4, wps=11121, ups=0.17, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.545, loss_scale=16, train_wall=567, gb_free=3.3, wall=163984
2022-02-20 06:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 06:06:55 | INFO | train_inner | epoch 036:    308 / 788 loss=6.191, nll_loss=4.756, ppl=27.02, wps=11118, ups=0.17, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.548, loss_scale=16, train_wall=567, gb_free=3.3, wall=164573
2022-02-20 06:15:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 06:16:45 | INFO | train_inner | epoch 036:    409 / 788 loss=6.195, nll_loss=4.761, ppl=27.11, wps=11114.7, ups=0.17, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.543, loss_scale=16, train_wall=567, gb_free=3.3, wall=165163
2022-02-20 06:26:28 | INFO | train_inner | epoch 036:    509 / 788 loss=6.228, nll_loss=4.797, ppl=27.81, wps=11228.7, ups=0.17, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.546, loss_scale=16, train_wall=561, gb_free=3.3, wall=165746
2022-02-20 06:28:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 06:36:18 | INFO | train_inner | epoch 036:    610 / 788 loss=6.225, nll_loss=4.794, ppl=27.75, wps=11117.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.556, loss_scale=16, train_wall=567, gb_free=3.3, wall=166336
2022-02-20 06:41:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-20 06:44:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-20 06:46:13 | INFO | train_inner | epoch 036:    712 / 788 loss=6.245, nll_loss=4.817, ppl=28.18, wps=11012, ups=0.17, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.573, loss_scale=8, train_wall=572, gb_free=3.3, wall=166931
2022-02-20 06:53:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-20 06:53:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.846 | nll_loss 5.456 | ppl 43.89 | wps 27857.4 | wpb 510.9 | bsz 1 | num_updates 28176 | best_loss 6.841
2022-02-20 06:53:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 28176 updates
2022-02-20 06:53:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1/checkpoint36.pt
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 5290880 vs 5290768
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x2b3f69c506a7 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x2b3ed0be9500 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x2b3ed0be56d3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x2b3ed0bea609 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x2b3ed0beb141 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x2b3ed0beb935 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x6543f3 (0x2b3ece0bb3f3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x2c2c60 (0x2b3ecdd29c60 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x2c3dce (0x2b3ecdd2adce in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
<omitting python frames>
frame #46: __libc_start_main + 0xf5 (0x2b3ebbca2555 in /lib64/libc.so.6)
frame #47: /cluster/home/andriusb/fq/env/bin/python() [0x40071e]

/cluster/shadow/.lsbatch/1645169014.205633402: line 8: 64422 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.08_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.08 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 1321671 --fp16 --max-update 50000
