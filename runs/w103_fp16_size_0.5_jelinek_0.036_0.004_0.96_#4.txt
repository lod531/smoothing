Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 206457062: <w103_fp16_size_0.5_jelinek_0.036_0.004_0.96_#4> in cluster <euler> Exited

Job <w103_fp16_size_0.5_jelinek_0.036_0.004_0.96_#4> was submitted from host <eu-login-05> by user <andriusb> in cluster <euler> at Fri Feb 25 10:44:30 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Feb 25 10:44:48 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 25 10:44:48 2022
Terminated at Sun Feb 27 10:45:20 2022
Results reported at Sun Feb 27 10:45:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.036, 0.004, 0.96)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321674 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   172402.00 sec.
    Max Memory :                                 11154 MB
    Average Memory :                             3199.03 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               8846.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   172832 sec.
    Turnaround time :                            172850 sec.

The output (if any) follows:

2022-02-25 10:44:55 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321674, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321674, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.036, 0.004, 0.96)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-25 10:44:56 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-25 10:45:01 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
Calculating frequency stats:
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 679/900675 [00:00<02:12, 6776.77it/s]  0%|          | 1357/900675 [00:00<02:33, 5859.87it/s]  0%|          | 1951/900675 [00:00<02:40, 5592.46it/s]  0%|          | 2515/900675 [00:00<02:40, 5604.69it/s]  0%|          | 3213/900675 [00:00<02:27, 6071.72it/s]  0%|          | 3825/900675 [00:00<02:29, 6011.89it/s]  1%|          | 4538/900675 [00:00<02:20, 6364.54it/s]  1%|          | 5238/900675 [00:00<02:16, 6558.75it/s]  1%|          | 5924/900675 [00:00<02:14, 6640.58it/s]  1%|          | 6590/900675 [00:01<02:28, 6037.57it/s]  1%|          | 7210/900675 [00:01<02:26, 6082.11it/s]  1%|          | 7827/900675 [00:01<02:27, 6052.89it/s]  1%|          | 8438/900675 [00:01<02:32, 5849.67it/s]  1%|          | 9087/900675 [00:01<02:28, 6018.12it/s]  1%|          | 9693/900675 [00:01<02:31, 5892.73it/s]  1%|          | 10327/900675 [00:01<02:27, 6018.29it/s]  1%|          | 10932/900675 [00:01<02:29, 5932.07it/s]  1%|▏         | 11528/900675 [00:01<02:32, 5842.99it/s]  1%|▏         | 12153/900675 [00:02<02:29, 5955.51it/s]  1%|▏         | 12750/900675 [00:02<02:32, 5817.81it/s]  1%|▏         | 13375/900675 [00:02<02:29, 5939.10it/s]  2%|▏         | 14030/900675 [00:02<02:24, 6115.73it/s]  2%|▏         | 14643/900675 [00:02<02:24, 6116.88it/s]  2%|▏         | 15303/900675 [00:02<02:21, 6253.39it/s]  2%|▏         | 15930/900675 [00:02<02:29, 5932.50it/s]  2%|▏         | 16528/900675 [00:02<02:31, 5852.20it/s]  2%|▏         | 17137/900675 [00:02<02:29, 5917.11it/s]  2%|▏         | 17731/900675 [00:02<02:30, 5869.60it/s]  2%|▏         | 18320/900675 [00:03<02:31, 5841.11it/s]  2%|▏         | 19018/900675 [00:03<02:22, 6173.94it/s]  2%|▏         | 19682/900675 [00:03<02:19, 6305.84it/s]  2%|▏         | 20314/900675 [00:03<02:27, 5962.26it/s]  2%|▏         | 20954/900675 [00:03<02:24, 6084.97it/s]  2%|▏         | 21567/900675 [00:03<02:32, 5773.61it/s]  2%|▏         | 22205/900675 [00:03<02:27, 5941.72it/s]  3%|▎         | 22850/900675 [00:03<02:24, 6086.30it/s]  3%|▎         | 23488/900675 [00:03<02:22, 6168.15it/s]  3%|▎         | 24226/900675 [00:03<02:14, 6514.42it/s]  3%|▎         | 24932/900675 [00:04<02:11, 6672.03it/s]  3%|▎         | 25602/900675 [00:04<02:12, 6621.77it/s]  3%|▎         | 26266/900675 [00:04<02:20, 6245.41it/s]  3%|▎         | 26896/900675 [00:04<02:25, 5992.56it/s]  3%|▎         | 27501/900675 [00:04<02:31, 5764.85it/s]  3%|▎         | 28127/900675 [00:04<02:27, 5901.72it/s]  3%|▎         | 28822/900675 [00:04<02:20, 6198.73it/s]  3%|▎         | 29447/900675 [00:04<02:28, 5859.10it/s]  3%|▎         | 30119/900675 [00:04<02:22, 6098.17it/s]  3%|▎         | 30735/900675 [00:05<02:29, 5805.36it/s]  3%|▎         | 31322/900675 [00:05<02:30, 5757.40it/s]  4%|▎         | 31906/900675 [00:05<02:30, 5772.81it/s]  4%|▎         | 32487/900675 [00:05<02:30, 5753.84it/s]  4%|▎         | 33065/900675 [00:05<02:36, 5555.06it/s]  4%|▎         | 33649/900675 [00:05<02:33, 5636.21it/s]  4%|▍         | 34218/900675 [00:05<02:33, 5650.15it/s]  4%|▍         | 34926/900675 [00:05<02:22, 6063.48it/s]  4%|▍         | 35535/900675 [00:05<02:26, 5907.01it/s]  4%|▍         | 36134/900675 [00:06<02:25, 5928.37it/s]  4%|▍         | 36739/900675 [00:06<02:24, 5960.50it/s]  4%|▍         | 37337/900675 [00:06<02:31, 5682.94it/s]  4%|▍         | 37909/900675 [00:06<02:33, 5621.87it/s]  4%|▍         | 38498/900675 [00:06<02:31, 5696.88it/s]  4%|▍         | 39072/900675 [00:06<02:31, 5705.73it/s]  4%|▍         | 39666/900675 [00:06<02:29, 5771.11it/s]  4%|▍         | 40301/900675 [00:06<02:24, 5940.16it/s]  5%|▍         | 40969/900675 [00:06<02:19, 6154.36it/s]  5%|▍         | 41586/900675 [00:06<02:24, 5944.53it/s]  5%|▍         | 42183/900675 [00:07<02:32, 5645.62it/s]  5%|▍         | 42752/900675 [00:07<02:32, 5628.04it/s]  5%|▍         | 43318/900675 [00:07<02:33, 5603.24it/s]  5%|▍         | 43971/900675 [00:07<02:26, 5866.20it/s]  5%|▍         | 44611/900675 [00:07<02:22, 6017.45it/s]  5%|▌         | 45215/900675 [00:07<02:25, 5865.06it/s]  5%|▌         | 45881/900675 [00:07<02:20, 6094.15it/s]  5%|▌         | 46555/900675 [00:07<02:15, 6283.21it/s]  5%|▌         | 47276/900675 [00:07<02:10, 6555.67it/s]  5%|▌         | 48131/900675 [00:07<01:59, 7145.02it/s]  5%|▌         | 48848/900675 [00:08<02:04, 6859.55it/s]  6%|▌         | 49594/900675 [00:08<02:01, 7025.62it/s]  6%|▌         | 50300/900675 [00:08<02:11, 6454.79it/s]  6%|▌         | 50956/900675 [00:08<02:16, 6221.75it/s]  6%|▌         | 51586/900675 [00:08<02:18, 6149.20it/s]  6%|▌         | 52446/900675 [00:08<02:04, 6833.32it/s]  6%|▌         | 53138/900675 [00:08<02:10, 6506.53it/s]  6%|▌         | 53797/900675 [00:08<02:12, 6407.25it/s]  6%|▌         | 54444/900675 [00:09<02:21, 5979.91it/s]  6%|▌         | 55050/900675 [00:09<02:21, 5988.05it/s]  6%|▌         | 55737/900675 [00:09<02:15, 6230.11it/s]  6%|▋         | 56366/900675 [00:09<02:17, 6135.71it/s]  6%|▋         | 56984/900675 [00:09<02:25, 5779.73it/s]  6%|▋         | 57568/900675 [00:09<02:26, 5756.92it/s]  6%|▋         | 58179/900675 [00:09<02:23, 5853.08it/s]  7%|▋         | 58890/900675 [00:09<02:15, 6211.98it/s]  7%|▋         | 59516/900675 [00:09<02:22, 5883.60it/s]  7%|▋         | 60111/900675 [00:09<02:23, 5850.24it/s]  7%|▋         | 60776/900675 [00:10<02:18, 6078.13it/s]  7%|▋         | 61513/900675 [00:10<02:10, 6443.20it/s]  7%|▋         | 62162/900675 [00:10<02:15, 6182.66it/s]  7%|▋         | 62785/900675 [00:10<02:15, 6178.91it/s]  7%|▋         | 63407/900675 [00:10<02:18, 6055.28it/s]  7%|▋         | 64015/900675 [00:10<02:18, 6053.83it/s]  7%|▋         | 64623/900675 [00:10<02:19, 5998.06it/s]  7%|▋         | 65224/900675 [00:10<02:20, 5956.44it/s]  7%|▋         | 65886/900675 [00:10<02:15, 6149.72it/s]  7%|▋         | 66502/900675 [00:10<02:16, 6123.47it/s]  7%|▋         | 67116/900675 [00:11<02:18, 5999.86it/s]  8%|▊         | 67717/900675 [00:11<02:24, 5769.14it/s]  8%|▊         | 68303/900675 [00:11<02:23, 5794.70it/s]  8%|▊         | 69035/900675 [00:11<02:13, 6233.88it/s]  8%|▊         | 69663/900675 [00:11<02:13, 6246.72it/s]  8%|▊         | 70290/900675 [00:11<02:13, 6202.16it/s]  8%|▊         | 70912/900675 [00:11<02:17, 6027.45it/s]  8%|▊         | 71543/900675 [00:11<02:15, 6106.47it/s]  8%|▊         | 72158/900675 [00:11<02:15, 6119.02it/s]  8%|▊         | 72771/900675 [00:12<02:15, 6104.07it/s]  8%|▊         | 73484/900675 [00:12<02:09, 6407.60it/s]  8%|▊         | 74277/900675 [00:12<02:00, 6856.23it/s]  8%|▊         | 74964/900675 [00:12<02:03, 6669.55it/s]  8%|▊         | 75633/900675 [00:12<02:11, 6294.30it/s]  8%|▊         | 76300/900675 [00:12<02:08, 6397.65it/s]  9%|▊         | 76944/900675 [00:12<02:10, 6325.42it/s]  9%|▊         | 77580/900675 [00:12<02:10, 6311.04it/s]  9%|▊         | 78214/900675 [00:12<02:13, 6183.50it/s]  9%|▉         | 78834/900675 [00:12<02:21, 5788.00it/s]  9%|▉         | 79419/900675 [00:13<02:26, 5622.76it/s]  9%|▉         | 80012/900675 [00:13<02:23, 5705.95it/s]  9%|▉         | 80586/900675 [00:13<02:24, 5663.07it/s]  9%|▉         | 81318/900675 [00:13<02:13, 6135.77it/s]  9%|▉         | 81936/900675 [00:13<02:16, 5977.34it/s]  9%|▉         | 82557/900675 [00:13<02:15, 6041.38it/s]  9%|▉         | 83257/900675 [00:13<02:09, 6319.05it/s]  9%|▉         | 83892/900675 [00:13<02:11, 6225.40it/s]  9%|▉         | 84517/900675 [00:13<02:15, 6039.38it/s]  9%|▉         | 85124/900675 [00:14<02:16, 5993.42it/s] 10%|▉         | 85763/900675 [00:14<02:13, 6099.88it/s] 10%|▉         | 86375/900675 [00:14<02:14, 6064.50it/s] 10%|▉         | 87071/900675 [00:14<02:08, 6315.38it/s] 10%|▉         | 87712/900675 [00:14<02:08, 6338.50it/s] 10%|▉         | 88347/900675 [00:14<02:08, 6326.59it/s] 10%|▉         | 88981/900675 [00:14<02:18, 5846.87it/s] 10%|▉         | 89573/900675 [00:14<02:18, 5864.63it/s] 10%|█         | 90165/900675 [00:14<02:21, 5715.45it/s] 10%|█         | 90741/900675 [00:14<02:22, 5689.43it/s] 10%|█         | 91415/900675 [00:15<02:15, 5981.36it/s] 10%|█         | 92036/900675 [00:15<02:13, 6039.78it/s] 10%|█         | 92721/900675 [00:15<02:08, 6272.22it/s] 10%|█         | 93351/900675 [00:15<02:14, 5994.60it/s] 10%|█         | 93955/900675 [00:15<02:15, 5940.09it/s] 11%|█         | 94624/900675 [00:15<02:10, 6155.95it/s] 11%|█         | 95243/900675 [00:15<02:11, 6128.04it/s] 11%|█         | 95858/900675 [00:15<02:12, 6095.62it/s] 11%|█         | 96618/900675 [00:15<02:03, 6536.54it/s] 11%|█         | 97322/900675 [00:16<02:00, 6681.76it/s] 11%|█         | 97992/900675 [00:16<02:06, 6329.13it/s] 11%|█         | 98630/900675 [00:16<02:18, 5800.96it/s] 11%|█         | 99284/900675 [00:16<02:13, 5994.92it/s] 11%|█         | 99893/900675 [00:16<02:18, 5781.41it/s] 11%|█         | 100479/900675 [00:16<02:21, 5650.28it/s] 11%|█         | 101123/900675 [00:16<02:16, 5868.27it/s] 11%|█▏        | 101715/900675 [00:16<02:16, 5860.99it/s] 11%|█▏        | 102347/900675 [00:16<02:13, 5992.78it/s] 11%|█▏        | 102950/900675 [00:16<02:12, 5999.02it/s] 11%|█▏        | 103553/900675 [00:17<02:16, 5848.77it/s] 12%|█▏        | 104140/900675 [00:17<02:19, 5718.32it/s] 12%|█▏        | 104784/900675 [00:17<02:14, 5919.47it/s] 12%|█▏        | 105378/900675 [00:17<02:14, 5891.94it/s] 12%|█▏        | 106015/900675 [00:17<02:11, 6025.61it/s] 12%|█▏        | 106619/900675 [00:17<02:12, 6000.48it/s] 12%|█▏        | 107220/900675 [00:17<02:16, 5827.55it/s] 12%|█▏        | 107843/900675 [00:17<02:13, 5941.33it/s] 12%|█▏        | 108452/900675 [00:17<02:12, 5981.96it/s] 12%|█▏        | 109052/900675 [00:18<02:19, 5694.93it/s] 12%|█▏        | 109626/900675 [00:18<02:18, 5707.46it/s] 12%|█▏        | 110228/900675 [00:18<02:16, 5796.79it/s] 12%|█▏        | 110877/900675 [00:18<02:11, 5999.42it/s] 12%|█▏        | 111539/900675 [00:18<02:07, 6182.03it/s] 12%|█▏        | 112159/900675 [00:18<02:10, 6060.46it/s] 13%|█▎        | 112806/900675 [00:18<02:07, 6173.11it/s] 13%|█▎        | 113425/900675 [00:18<02:10, 6010.19it/s] 13%|█▎        | 114028/900675 [00:18<02:11, 5976.49it/s] 13%|█▎        | 114627/900675 [00:18<02:13, 5900.77it/s] 13%|█▎        | 115241/900675 [00:19<02:11, 5954.98it/s] 13%|█▎        | 115838/900675 [00:19<02:19, 5632.49it/s] 13%|█▎        | 116418/900675 [00:19<02:18, 5676.11it/s] 13%|█▎        | 116989/900675 [00:19<02:18, 5678.03it/s] 13%|█▎        | 117559/900675 [00:19<02:18, 5660.07it/s] 13%|█▎        | 118127/900675 [00:19<02:18, 5637.62it/s] 13%|█▎        | 118775/900675 [00:19<02:12, 5880.38it/s] 13%|█▎        | 119365/900675 [00:19<02:16, 5738.20it/s] 13%|█▎        | 120005/900675 [00:19<02:11, 5930.88it/s] 13%|█▎        | 120721/900675 [00:19<02:04, 6284.90it/s] 13%|█▎        | 121352/900675 [00:20<02:06, 6157.60it/s] 14%|█▎        | 121970/900675 [00:20<02:12, 5881.33it/s] 14%|█▎        | 122600/900675 [00:20<02:09, 6000.21it/s] 14%|█▎        | 123203/900675 [00:20<02:12, 5889.65it/s] 14%|█▎        | 123806/900675 [00:20<02:11, 5926.91it/s] 14%|█▍        | 124401/900675 [00:20<02:12, 5874.39it/s] 14%|█▍        | 125001/900675 [00:20<02:11, 5909.74it/s] 14%|█▍        | 125599/900675 [00:20<02:10, 5929.05it/s] 14%|█▍        | 126239/900675 [00:20<02:07, 6068.38it/s] 14%|█▍        | 126936/900675 [00:21<02:02, 6331.96it/s] 14%|█▍        | 127570/900675 [00:21<02:08, 5999.09it/s] 14%|█▍        | 128193/900675 [00:21<02:07, 6059.89it/s] 14%|█▍        | 128803/900675 [00:21<02:09, 5946.35it/s] 14%|█▍        | 129480/900675 [00:21<02:04, 6184.19it/s] 14%|█▍        | 130101/900675 [00:21<02:12, 5814.77it/s] 15%|█▍        | 130689/900675 [00:21<02:13, 5759.50it/s] 15%|█▍        | 131324/900675 [00:21<02:10, 5914.19it/s] 15%|█▍        | 131919/900675 [00:21<02:25, 5280.75it/s] 15%|█▍        | 132613/900675 [00:22<02:14, 5724.39it/s] 15%|█▍        | 133304/900675 [00:22<02:06, 6046.08it/s] 15%|█▍        | 133922/900675 [00:22<02:06, 6038.61it/s] 15%|█▍        | 134619/900675 [00:22<02:01, 6302.27it/s] 15%|█▌        | 135350/900675 [00:22<01:56, 6593.07it/s] 15%|█▌        | 136016/900675 [00:22<01:59, 6393.14it/s] 15%|█▌        | 136741/900675 [00:22<01:55, 6639.65it/s] 15%|█▌        | 137461/900675 [00:22<01:52, 6800.85it/s] 15%|█▌        | 138145/900675 [00:22<02:00, 6327.56it/s] 15%|█▌        | 138787/900675 [00:22<02:04, 6118.39it/s] 15%|█▌        | 139406/900675 [00:23<02:07, 5993.44it/s] 16%|█▌        | 140010/900675 [00:23<02:06, 5993.54it/s] 16%|█▌        | 140636/900675 [00:23<02:05, 6068.32it/s] 16%|█▌        | 141246/900675 [00:23<02:08, 5925.64it/s] 16%|█▌        | 141841/900675 [00:23<02:09, 5866.22it/s] 16%|█▌        | 142437/900675 [00:23<02:08, 5887.05it/s] 16%|█▌        | 143027/900675 [00:23<02:12, 5708.81it/s] 16%|█▌        | 143785/900675 [00:23<02:01, 6246.67it/s] 16%|█▌        | 144414/900675 [00:23<02:02, 6153.24it/s] 16%|█▌        | 145153/900675 [00:24<01:56, 6506.42it/s] 16%|█▌        | 145807/900675 [00:24<01:57, 6430.83it/s] 16%|█▋        | 146453/900675 [00:24<02:00, 6234.36it/s] 16%|█▋        | 147079/900675 [00:24<02:05, 6004.28it/s] 16%|█▋        | 147683/900675 [00:24<02:09, 5802.28it/s] 16%|█▋        | 148266/900675 [00:24<02:09, 5794.04it/s] 17%|█▋        | 148888/900675 [00:24<02:07, 5912.92it/s] 17%|█▋        | 149482/900675 [00:24<02:06, 5919.03it/s] 17%|█▋        | 150076/900675 [00:24<02:09, 5805.88it/s] 17%|█▋        | 150658/900675 [00:24<02:10, 5742.21it/s] 17%|█▋        | 151233/900675 [00:25<02:14, 5590.83it/s] 17%|█▋        | 151846/900675 [00:25<02:10, 5739.46it/s] 17%|█▋        | 152422/900675 [00:25<02:10, 5739.00it/s] 17%|█▋        | 153081/900675 [00:25<02:04, 5985.81it/s] 17%|█▋        | 153681/900675 [00:25<02:05, 5965.10it/s] 17%|█▋        | 154279/900675 [00:25<02:06, 5908.95it/s] 17%|█▋        | 154871/900675 [00:25<02:06, 5896.66it/s] 17%|█▋        | 155505/900675 [00:25<02:03, 6027.72it/s] 17%|█▋        | 156152/900675 [00:25<02:00, 6158.46it/s] 17%|█▋        | 156769/900675 [00:25<02:02, 6086.25it/s] 17%|█▋        | 157450/900675 [00:26<01:58, 6293.26it/s] 18%|█▊        | 158101/900675 [00:26<01:56, 6353.93it/s] 18%|█▊        | 158738/900675 [00:26<01:56, 6353.36it/s] 18%|█▊        | 159374/900675 [00:26<02:02, 6047.70it/s] 18%|█▊        | 160067/900675 [00:26<01:57, 6298.61it/s] 18%|█▊        | 160701/900675 [00:26<02:03, 5999.16it/s] 18%|█▊        | 161361/900675 [00:26<01:59, 6169.47it/s] 18%|█▊        | 161983/900675 [00:26<02:02, 6026.89it/s] 18%|█▊        | 162639/900675 [00:26<01:59, 6179.21it/s] 18%|█▊        | 163278/900675 [00:27<01:58, 6239.00it/s] 18%|█▊        | 163905/900675 [00:27<02:04, 5915.62it/s] 18%|█▊        | 164502/900675 [00:27<02:04, 5926.11it/s] 18%|█▊        | 165189/900675 [00:27<01:58, 6195.86it/s] 18%|█▊        | 165861/900675 [00:27<01:55, 6346.90it/s] 18%|█▊        | 166499/900675 [00:27<01:57, 6271.88it/s] 19%|█▊        | 167180/900675 [00:27<01:54, 6429.42it/s] 19%|█▊        | 167825/900675 [00:27<01:58, 6200.13it/s] 19%|█▊        | 168448/900675 [00:27<02:03, 5908.06it/s] 19%|█▉        | 169123/900675 [00:27<01:59, 6145.00it/s] 19%|█▉        | 169742/900675 [00:28<02:02, 5955.70it/s] 19%|█▉        | 170342/900675 [00:28<02:09, 5634.98it/s] 19%|█▉        | 170948/900675 [00:28<02:07, 5742.47it/s] 19%|█▉        | 171527/900675 [00:28<02:07, 5716.58it/s] 19%|█▉        | 172102/900675 [00:28<02:07, 5721.77it/s] 19%|█▉        | 172704/900675 [00:28<02:05, 5807.76it/s] 19%|█▉        | 173287/900675 [00:28<02:06, 5762.20it/s] 19%|█▉        | 173921/900675 [00:28<02:02, 5931.36it/s] 19%|█▉        | 174516/900675 [00:28<02:04, 5844.10it/s] 19%|█▉        | 175155/900675 [00:29<02:00, 5999.45it/s] 20%|█▉        | 175756/900675 [00:29<02:05, 5771.10it/s] 20%|█▉        | 176366/900675 [00:29<02:03, 5865.02it/s] 20%|█▉        | 176955/900675 [00:29<02:05, 5746.39it/s] 20%|█▉        | 177532/900675 [00:29<02:14, 5395.61it/s] 20%|█▉        | 178093/900675 [00:29<02:12, 5454.95it/s] 20%|█▉        | 178767/900675 [00:29<02:04, 5810.41it/s] 20%|█▉        | 179359/900675 [00:29<02:03, 5840.01it/s] 20%|█▉        | 179947/900675 [00:29<02:06, 5698.24it/s] 20%|██        | 180638/900675 [00:29<01:59, 6041.81it/s] 20%|██        | 181246/900675 [00:30<02:03, 5818.10it/s] 20%|██        | 181832/900675 [00:30<02:07, 5620.82it/s] 20%|██        | 182398/900675 [00:30<02:08, 5574.88it/s] 20%|██        | 183034/900675 [00:30<02:03, 5796.26it/s] 20%|██        | 183617/900675 [00:30<02:07, 5644.13it/s] 20%|██        | 184184/900675 [00:30<02:17, 5228.94it/s] 21%|██        | 184863/900675 [00:30<02:06, 5655.82it/s] 21%|██        | 185437/900675 [00:30<02:15, 5275.12it/s] 21%|██        | 185995/900675 [00:30<02:13, 5354.64it/s] 21%|██        | 186539/900675 [00:31<02:13, 5355.65it/s] 21%|██        | 187189/900675 [00:31<02:05, 5672.75it/s] 21%|██        | 187791/900675 [00:31<02:03, 5771.15it/s] 21%|██        | 188411/900675 [00:31<02:00, 5894.00it/s] 21%|██        | 189029/900675 [00:31<01:59, 5975.88it/s] 21%|██        | 189659/900675 [00:31<01:57, 6064.84it/s] 21%|██        | 190268/900675 [00:31<01:59, 5960.23it/s] 21%|██        | 190882/900675 [00:31<01:58, 6009.23it/s] 21%|██▏       | 191485/900675 [00:31<02:00, 5865.42it/s] 21%|██▏       | 192145/900675 [00:32<01:56, 6072.53it/s] 21%|██▏       | 192818/900675 [00:32<01:53, 6256.77it/s] 21%|██▏       | 193446/900675 [00:32<01:54, 6187.57it/s] 22%|██▏       | 194081/900675 [00:32<01:53, 6234.01it/s] 22%|██▏       | 194706/900675 [00:32<01:57, 5985.87it/s] 22%|██▏       | 195345/900675 [00:32<01:55, 6098.34it/s] 22%|██▏       | 195976/900675 [00:32<01:54, 6155.24it/s] 22%|██▏       | 196594/900675 [00:32<01:59, 5873.39it/s] 22%|██▏       | 197470/900675 [00:32<01:45, 6695.24it/s] 22%|██▏       | 198146/900675 [00:32<01:49, 6391.80it/s] 22%|██▏       | 198792/900675 [00:33<01:53, 6158.02it/s] 22%|██▏       | 199453/900675 [00:33<01:51, 6282.61it/s] 22%|██▏       | 200087/900675 [00:33<01:57, 5956.46it/s] 22%|██▏       | 200689/900675 [00:33<02:03, 5680.89it/s] 22%|██▏       | 201316/900675 [00:33<01:59, 5837.20it/s] 22%|██▏       | 201925/900675 [00:33<01:58, 5904.90it/s] 22%|██▏       | 202520/900675 [00:33<01:58, 5868.35it/s] 23%|██▎       | 203163/900675 [00:33<01:55, 6024.39it/s] 23%|██▎       | 203873/900675 [00:33<01:49, 6336.56it/s] 23%|██▎       | 204510/900675 [00:34<01:56, 5987.79it/s] 23%|██▎       | 205115/900675 [00:34<01:58, 5871.43it/s] 23%|██▎       | 205708/900675 [00:34<01:58, 5881.12it/s] 23%|██▎       | 206342/900675 [00:34<01:55, 6012.87it/s] 23%|██▎       | 206946/900675 [00:34<01:58, 5846.07it/s] 23%|██▎       | 207533/900675 [00:34<02:00, 5769.03it/s] 23%|██▎       | 208112/900675 [00:34<02:00, 5771.07it/s] 23%|██▎       | 208691/900675 [00:34<02:03, 5597.04it/s] 23%|██▎       | 209253/900675 [00:34<02:04, 5554.54it/s] 23%|██▎       | 209887/900675 [00:34<01:59, 5780.07it/s] 23%|██▎       | 210546/900675 [00:35<01:54, 6016.50it/s] 23%|██▎       | 211150/900675 [00:35<01:54, 6021.20it/s] 24%|██▎       | 211754/900675 [00:35<01:56, 5925.53it/s] 24%|██▎       | 212386/900675 [00:35<01:54, 6033.44it/s] 24%|██▎       | 213101/900675 [00:35<01:48, 6359.15it/s] 24%|██▎       | 213739/900675 [00:35<01:57, 5861.44it/s] 24%|██▍       | 214334/900675 [00:35<01:58, 5789.87it/s] 24%|██▍       | 214919/900675 [00:35<02:07, 5367.69it/s] 24%|██▍       | 215478/900675 [00:35<02:06, 5425.42it/s] 24%|██▍       | 216076/900675 [00:36<02:02, 5575.08it/s] 24%|██▍       | 216640/900675 [00:36<02:02, 5571.11it/s] 24%|██▍       | 217224/900675 [00:36<02:01, 5647.58it/s] 24%|██▍       | 217886/900675 [00:36<01:55, 5931.35it/s] 24%|██▍       | 218513/900675 [00:36<01:53, 6024.86it/s] 24%|██▍       | 219138/900675 [00:36<01:51, 6089.15it/s] 24%|██▍       | 219749/900675 [00:36<01:55, 5885.17it/s] 24%|██▍       | 220366/900675 [00:36<01:54, 5966.77it/s] 25%|██▍       | 220965/900675 [00:36<01:55, 5876.24it/s] 25%|██▍       | 221584/900675 [00:36<01:53, 5959.58it/s] 25%|██▍       | 222182/900675 [00:37<01:57, 5775.17it/s] 25%|██▍       | 222762/900675 [00:37<01:58, 5718.44it/s] 25%|██▍       | 223336/900675 [00:37<02:03, 5477.76it/s] 25%|██▍       | 223916/900675 [00:37<02:01, 5568.09it/s] 25%|██▍       | 224594/900675 [00:37<01:54, 5917.06it/s] 25%|██▌       | 225249/900675 [00:37<01:50, 6101.18it/s] 25%|██▌       | 225987/900675 [00:37<01:44, 6476.37it/s] 25%|██▌       | 226638/900675 [00:37<01:46, 6351.80it/s] 25%|██▌       | 227276/900675 [00:37<01:49, 6130.70it/s] 25%|██▌       | 227892/900675 [00:38<01:54, 5887.91it/s] 25%|██▌       | 228485/900675 [00:38<01:56, 5761.94it/s] 25%|██▌       | 229064/900675 [00:38<01:56, 5768.58it/s] 25%|██▌       | 229643/900675 [00:38<02:02, 5468.82it/s] 26%|██▌       | 230233/900675 [00:38<02:00, 5584.90it/s] 26%|██▌       | 230841/900675 [00:38<01:57, 5722.88it/s] 26%|██▌       | 231417/900675 [00:38<01:58, 5638.81it/s] 26%|██▌       | 231983/900675 [00:38<02:00, 5565.35it/s] 26%|██▌       | 232630/900675 [00:38<01:54, 5827.06it/s] 26%|██▌       | 233334/900675 [00:38<01:48, 6175.30it/s] 26%|██▌       | 233957/900675 [00:39<01:47, 6191.37it/s] 26%|██▌       | 234651/900675 [00:39<01:43, 6404.18it/s] 26%|██▌       | 235322/900675 [00:39<01:42, 6486.84it/s] 26%|██▌       | 235972/900675 [00:39<01:44, 6338.48it/s] 26%|██▋       | 236608/900675 [00:39<01:52, 5911.97it/s] 26%|██▋       | 237216/900675 [00:39<01:51, 5957.53it/s] 26%|██▋       | 237817/900675 [00:39<01:53, 5848.07it/s] 26%|██▋       | 238542/900675 [00:39<01:45, 6248.36it/s] 27%|██▋       | 239172/900675 [00:39<01:49, 6021.15it/s] 27%|██▋       | 239779/900675 [00:40<01:51, 5940.30it/s] 27%|██▋       | 240380/900675 [00:40<01:50, 5959.63it/s] 27%|██▋       | 241181/900675 [00:40<01:40, 6551.00it/s] 27%|██▋       | 241840/900675 [00:40<01:43, 6368.53it/s] 27%|██▋       | 242481/900675 [00:40<01:45, 6268.14it/s] 27%|██▋       | 243111/900675 [00:40<01:46, 6175.47it/s] 27%|██▋       | 243754/900675 [00:40<01:45, 6240.35it/s] 27%|██▋       | 244380/900675 [00:40<01:47, 6124.13it/s] 27%|██▋       | 244994/900675 [00:40<01:49, 6006.79it/s] 27%|██▋       | 245651/900675 [00:40<01:46, 6169.20it/s] 27%|██▋       | 246275/900675 [00:41<01:45, 6189.57it/s] 27%|██▋       | 247088/900675 [00:41<01:36, 6757.93it/s] 28%|██▊       | 247766/900675 [00:41<01:41, 6450.99it/s] 28%|██▊       | 248415/900675 [00:41<01:44, 6228.10it/s] 28%|██▊       | 249112/900675 [00:41<01:41, 6437.38it/s] 28%|██▊       | 249800/900675 [00:41<01:39, 6560.32it/s] 28%|██▊       | 250460/900675 [00:41<01:42, 6324.71it/s] 28%|██▊       | 251105/900675 [00:41<01:42, 6358.42it/s] 28%|██▊       | 251744/900675 [00:41<01:42, 6360.98it/s] 28%|██▊       | 252382/900675 [00:42<01:44, 6218.01it/s] 28%|██▊       | 253006/900675 [00:42<01:46, 6062.83it/s] 28%|██▊       | 253638/900675 [00:42<01:45, 6136.33it/s] 28%|██▊       | 254277/900675 [00:42<01:44, 6201.83it/s] 28%|██▊       | 254899/900675 [00:42<01:46, 6040.19it/s] 28%|██▊       | 255505/900675 [00:42<01:48, 5931.97it/s] 28%|██▊       | 256188/900675 [00:42<01:44, 6187.31it/s] 29%|██▊       | 256873/900675 [00:42<01:40, 6376.30it/s] 29%|██▊       | 257513/900675 [00:42<01:43, 6232.77it/s] 29%|██▊       | 258138/900675 [00:42<01:46, 6059.87it/s] 29%|██▊       | 258746/900675 [00:43<01:53, 5638.58it/s] 29%|██▉       | 259316/900675 [00:43<01:55, 5572.23it/s] 29%|██▉       | 259972/900675 [00:43<01:49, 5845.21it/s] 29%|██▉       | 260687/900675 [00:43<01:42, 6215.86it/s] 29%|██▉       | 261314/900675 [00:43<01:48, 5879.40it/s] 29%|██▉       | 261971/900675 [00:43<01:45, 6066.02it/s] 29%|██▉       | 262584/900675 [00:43<01:50, 5780.66it/s] 29%|██▉       | 263169/900675 [00:43<01:51, 5737.87it/s] 29%|██▉       | 263791/900675 [00:43<01:48, 5872.59it/s] 29%|██▉       | 264382/900675 [00:44<01:54, 5566.00it/s] 29%|██▉       | 265024/900675 [00:44<01:49, 5803.69it/s] 29%|██▉       | 265610/900675 [00:44<01:49, 5811.53it/s] 30%|██▉       | 266195/900675 [00:44<01:52, 5634.07it/s] 30%|██▉       | 266762/900675 [00:44<01:53, 5603.22it/s] 30%|██▉       | 267386/900675 [00:44<01:49, 5781.25it/s] 30%|██▉       | 267967/900675 [00:44<01:51, 5665.51it/s] 30%|██▉       | 268626/900675 [00:44<01:46, 5927.93it/s] 30%|██▉       | 269284/900675 [00:44<01:43, 6115.26it/s] 30%|██▉       | 269898/900675 [00:45<01:50, 5722.16it/s] 30%|███       | 270567/900675 [00:45<01:45, 5989.67it/s] 30%|███       | 271173/900675 [00:45<01:44, 6004.94it/s] 30%|███       | 271782/900675 [00:45<01:44, 6029.49it/s] 30%|███       | 272513/900675 [00:45<01:38, 6403.42it/s] 30%|███       | 273217/900675 [00:45<01:35, 6579.91it/s] 30%|███       | 273878/900675 [00:45<01:36, 6520.60it/s] 30%|███       | 274532/900675 [00:45<01:39, 6280.53it/s] 31%|███       | 275163/900675 [00:45<01:45, 5942.97it/s] 31%|███       | 275763/900675 [00:45<01:46, 5873.24it/s] 31%|███       | 276354/900675 [00:46<01:50, 5632.50it/s] 31%|███       | 276950/900675 [00:46<01:49, 5720.71it/s] 31%|███       | 277525/900675 [00:46<01:49, 5715.46it/s] 31%|███       | 278130/900675 [00:46<01:47, 5807.00it/s] 31%|███       | 278786/900675 [00:46<01:43, 6021.88it/s] 31%|███       | 279390/900675 [00:46<01:47, 5793.77it/s] 31%|███       | 279999/900675 [00:46<01:45, 5877.09it/s] 31%|███       | 280626/900675 [00:46<01:43, 5979.39it/s] 31%|███       | 281226/900675 [00:46<01:46, 5833.55it/s] 31%|███▏      | 281861/900675 [00:47<01:43, 5979.34it/s] 31%|███▏      | 282461/900675 [00:47<01:47, 5738.85it/s] 31%|███▏      | 283098/900675 [00:47<01:44, 5913.79it/s] 32%|███▏      | 283737/900675 [00:47<01:42, 6047.97it/s] 32%|███▏      | 284345/900675 [00:47<01:42, 6030.94it/s] 32%|███▏      | 284950/900675 [00:47<01:43, 5943.65it/s] 32%|███▏      | 285546/900675 [00:47<01:43, 5922.20it/s] 32%|███▏      | 286161/900675 [00:47<01:42, 5980.99it/s] 32%|███▏      | 286760/900675 [00:47<01:44, 5891.47it/s] 32%|███▏      | 287350/900675 [00:47<01:46, 5775.24it/s] 32%|███▏      | 288051/900675 [00:48<01:39, 6133.77it/s] 32%|███▏      | 288683/900675 [00:48<01:39, 6175.71it/s] 32%|███▏      | 289302/900675 [00:48<01:40, 6077.73it/s] 32%|███▏      | 289911/900675 [00:48<01:44, 5825.68it/s] 32%|███▏      | 290497/900675 [00:48<01:45, 5781.62it/s] 32%|███▏      | 291077/900675 [00:48<01:45, 5768.08it/s] 32%|███▏      | 291704/900675 [00:48<01:42, 5913.93it/s] 32%|███▏      | 292297/900675 [00:48<01:44, 5824.22it/s] 33%|███▎      | 292881/900675 [00:48<01:45, 5782.60it/s] 33%|███▎      | 293503/900675 [00:48<01:42, 5905.42it/s] 33%|███▎      | 294095/900675 [00:49<01:42, 5903.50it/s] 33%|███▎      | 294732/900675 [00:49<01:40, 6034.80it/s] 33%|███▎      | 295405/900675 [00:49<01:37, 6235.31it/s] 33%|███▎      | 296030/900675 [00:49<01:39, 6084.51it/s] 33%|███▎      | 296640/900675 [00:49<01:40, 5998.86it/s] 33%|███▎      | 297270/900675 [00:49<01:39, 6080.04it/s] 33%|███▎      | 297937/900675 [00:49<01:36, 6248.66it/s] 33%|███▎      | 298563/900675 [00:49<01:39, 6045.84it/s] 33%|███▎      | 299170/900675 [00:49<01:40, 5972.06it/s] 33%|███▎      | 299887/900675 [00:50<01:35, 6318.14it/s] 33%|███▎      | 300521/900675 [00:50<01:37, 6159.81it/s] 33%|███▎      | 301139/900675 [00:50<01:41, 5918.66it/s] 34%|███▎      | 301734/900675 [00:50<01:41, 5888.67it/s] 34%|███▎      | 302325/900675 [00:50<01:42, 5861.02it/s] 34%|███▎      | 302948/900675 [00:50<01:40, 5965.33it/s] 34%|███▎      | 303615/900675 [00:50<01:36, 6167.74it/s] 34%|███▍      | 304234/900675 [00:50<01:37, 6127.52it/s] 34%|███▍      | 304848/900675 [00:50<01:38, 6032.84it/s] 34%|███▍      | 305453/900675 [00:50<01:40, 5913.80it/s] 34%|███▍      | 306071/900675 [00:51<01:39, 5989.73it/s] 34%|███▍      | 306671/900675 [00:51<01:41, 5824.91it/s] 34%|███▍      | 307312/900675 [00:51<01:39, 5991.06it/s] 34%|███▍      | 307913/900675 [00:51<01:39, 5986.85it/s] 34%|███▍      | 308513/900675 [00:51<01:47, 5484.18it/s] 34%|███▍      | 309316/900675 [00:51<01:35, 6191.66it/s] 34%|███▍      | 310033/900675 [00:51<01:31, 6464.35it/s] 34%|███▍      | 310689/900675 [00:51<01:34, 6271.74it/s] 35%|███▍      | 311324/900675 [00:51<01:36, 6112.87it/s] 35%|███▍      | 311941/900675 [00:52<01:36, 6108.95it/s] 35%|███▍      | 312576/900675 [00:52<01:35, 6170.43it/s] 35%|███▍      | 313213/900675 [00:52<01:34, 6224.30it/s] 35%|███▍      | 313935/900675 [00:52<01:30, 6515.65it/s] 35%|███▍      | 314589/900675 [00:52<01:33, 6288.67it/s] 35%|███▍      | 315221/900675 [00:52<01:38, 5930.34it/s] 35%|███▌      | 315849/900675 [00:52<01:37, 6028.11it/s] 35%|███▌      | 316457/900675 [00:52<01:41, 5743.58it/s] 35%|███▌      | 317080/900675 [00:52<01:39, 5879.44it/s] 35%|███▌      | 317673/900675 [00:52<01:45, 5547.80it/s] 35%|███▌      | 318329/900675 [00:53<01:40, 5820.14it/s] 35%|███▌      | 318918/900675 [00:53<01:42, 5689.59it/s] 35%|███▌      | 319565/900675 [00:53<01:38, 5909.97it/s] 36%|███▌      | 320161/900675 [00:53<01:40, 5778.20it/s] 36%|███▌      | 320767/900675 [00:53<01:38, 5858.32it/s] 36%|███▌      | 321470/900675 [00:53<01:33, 6196.08it/s] 36%|███▌      | 322093/900675 [00:53<01:37, 5934.53it/s] 36%|███▌      | 322691/900675 [00:53<01:40, 5753.08it/s] 36%|███▌      | 323311/900675 [00:53<01:38, 5878.95it/s] 36%|███▌      | 323902/900675 [00:54<01:39, 5786.19it/s] 36%|███▌      | 324483/900675 [00:54<01:39, 5777.87it/s] 36%|███▌      | 325063/900675 [00:54<01:40, 5706.35it/s] 36%|███▌      | 325635/900675 [00:54<01:45, 5446.51it/s] 36%|███▌      | 326240/900675 [00:54<01:42, 5613.95it/s] 36%|███▋      | 326927/900675 [00:54<01:36, 5972.48it/s] 36%|███▋      | 327589/900675 [00:54<01:33, 6160.73it/s] 36%|███▋      | 328343/900675 [00:54<01:27, 6564.73it/s] 37%|███▋      | 329003/900675 [00:54<01:27, 6557.94it/s] 37%|███▋      | 329661/900675 [00:54<01:33, 6116.39it/s] 37%|███▋      | 330280/900675 [00:55<01:37, 5825.30it/s] 37%|███▋      | 330870/900675 [00:55<01:37, 5816.00it/s] 37%|███▋      | 331457/900675 [00:55<01:39, 5694.97it/s] 37%|███▋      | 332076/900675 [00:55<01:37, 5832.69it/s] 37%|███▋      | 332691/900675 [00:55<01:35, 5918.34it/s] 37%|███▋      | 333286/900675 [00:55<01:36, 5903.16it/s] 37%|███▋      | 333934/900675 [00:55<01:33, 6062.95it/s] 37%|███▋      | 334542/900675 [00:55<01:35, 5934.97it/s] 37%|███▋      | 335170/900675 [00:55<01:33, 6029.03it/s] 37%|███▋      | 335784/900675 [00:56<01:33, 6055.61it/s] 37%|███▋      | 336391/900675 [00:56<01:33, 6042.73it/s] 37%|███▋      | 337014/900675 [00:56<01:32, 6094.45it/s] 37%|███▋      | 337681/900675 [00:56<01:29, 6265.42it/s] 38%|███▊      | 338309/900675 [00:56<01:30, 6199.15it/s] 38%|███▊      | 338990/900675 [00:56<01:28, 6374.68it/s] 38%|███▊      | 339628/900675 [00:56<01:31, 6143.67it/s] 38%|███▊      | 340245/900675 [00:56<01:40, 5603.07it/s] 38%|███▊      | 340862/900675 [00:56<01:37, 5751.44it/s] 38%|███▊      | 341484/900675 [00:56<01:35, 5879.01it/s] 38%|███▊      | 342098/900675 [00:57<01:34, 5941.92it/s] 38%|███▊      | 342697/900675 [00:57<01:33, 5944.49it/s] 38%|███▊      | 343299/900675 [00:57<01:33, 5962.19it/s] 38%|███▊      | 344011/900675 [00:57<01:28, 6299.55it/s] 38%|███▊      | 344644/900675 [00:57<01:33, 5954.54it/s] 38%|███▊      | 345307/900675 [00:57<01:30, 6143.60it/s] 38%|███▊      | 345926/900675 [00:57<01:38, 5624.46it/s] 38%|███▊      | 346624/900675 [00:57<01:32, 5989.42it/s] 39%|███▊      | 347234/900675 [00:57<01:33, 5937.53it/s] 39%|███▊      | 347839/900675 [00:58<01:32, 5966.41it/s] 39%|███▊      | 348442/900675 [00:58<01:34, 5856.20it/s] 39%|███▉      | 349056/900675 [00:58<01:32, 5937.09it/s] 39%|███▉      | 349653/900675 [00:58<01:32, 5930.81it/s] 39%|███▉      | 350491/900675 [00:58<01:22, 6640.80it/s] 39%|███▉      | 351159/900675 [00:58<01:28, 6228.72it/s] 39%|███▉      | 351789/900675 [00:58<01:29, 6160.63it/s] 39%|███▉      | 352410/900675 [00:58<01:37, 5632.68it/s] 39%|███▉      | 353020/900675 [00:58<01:35, 5757.72it/s] 39%|███▉      | 353605/900675 [00:59<01:35, 5729.02it/s] 39%|███▉      | 354184/900675 [00:59<01:36, 5663.54it/s] 39%|███▉      | 354887/900675 [00:59<01:30, 6050.28it/s] 39%|███▉      | 355497/900675 [00:59<01:34, 5769.61it/s] 40%|███▉      | 356130/900675 [00:59<01:32, 5918.28it/s] 40%|███▉      | 356727/900675 [00:59<01:32, 5905.87it/s] 40%|███▉      | 357321/900675 [00:59<01:40, 5428.72it/s] 40%|███▉      | 358001/900675 [00:59<01:33, 5806.19it/s] 40%|███▉      | 358592/900675 [00:59<01:33, 5822.03it/s] 40%|███▉      | 359184/900675 [00:59<01:32, 5849.38it/s] 40%|███▉      | 359777/900675 [01:00<01:32, 5866.22it/s] 40%|████      | 360422/900675 [01:00<01:29, 6037.05it/s] 40%|████      | 361087/900675 [01:00<01:26, 6217.43it/s] 40%|████      | 361776/900675 [01:00<01:23, 6416.76it/s] 40%|████      | 362420/900675 [01:00<01:24, 6362.33it/s] 40%|████      | 363090/900675 [01:00<01:23, 6460.58it/s] 40%|████      | 363738/900675 [01:00<01:23, 6422.51it/s] 40%|████      | 364395/900675 [01:00<01:22, 6466.12it/s] 41%|████      | 365075/900675 [01:00<01:21, 6558.63it/s] 41%|████      | 365732/900675 [01:00<01:23, 6407.91it/s] 41%|████      | 366374/900675 [01:01<01:25, 6242.38it/s] 41%|████      | 367000/900675 [01:01<01:26, 6154.48it/s] 41%|████      | 367617/900675 [01:01<01:27, 6113.37it/s] 41%|████      | 368229/900675 [01:01<01:33, 5670.79it/s] 41%|████      | 368895/900675 [01:01<01:29, 5936.44it/s] 41%|████      | 369516/900675 [01:01<01:28, 6013.90it/s] 41%|████      | 370139/900675 [01:01<01:27, 6075.42it/s] 41%|████      | 370751/900675 [01:01<01:30, 5855.85it/s] 41%|████      | 371358/900675 [01:01<01:29, 5910.14it/s] 41%|████▏     | 371952/900675 [01:02<01:31, 5778.04it/s] 41%|████▏     | 372589/900675 [01:02<01:28, 5944.38it/s] 41%|████▏     | 373194/900675 [01:02<01:28, 5970.10it/s] 42%|████▏     | 373793/900675 [01:02<01:29, 5867.78it/s] 42%|████▏     | 374479/900675 [01:02<01:25, 6151.47it/s] 42%|████▏     | 375096/900675 [01:02<01:26, 6093.61it/s] 42%|████▏     | 375707/900675 [01:02<01:31, 5736.50it/s] 42%|████▏     | 376318/900675 [01:02<01:29, 5841.70it/s] 42%|████▏     | 376907/900675 [01:02<01:36, 5452.87it/s] 42%|████▏     | 377499/900675 [01:03<01:33, 5578.51it/s] 42%|████▏     | 378191/900675 [01:03<01:27, 5957.99it/s] 42%|████▏     | 378804/900675 [01:03<01:26, 6005.81it/s] 42%|████▏     | 379440/900675 [01:03<01:25, 6104.45it/s] 42%|████▏     | 380168/900675 [01:03<01:20, 6448.06it/s] 42%|████▏     | 380816/900675 [01:03<01:21, 6345.51it/s] 42%|████▏     | 381454/900675 [01:03<01:21, 6350.22it/s] 42%|████▏     | 382091/900675 [01:03<01:27, 5902.76it/s] 42%|████▏     | 382689/900675 [01:03<01:30, 5741.56it/s] 43%|████▎     | 383272/900675 [01:03<01:29, 5761.97it/s] 43%|████▎     | 383852/900675 [01:04<01:30, 5738.36it/s] 43%|████▎     | 384579/900675 [01:04<01:23, 6180.47it/s] 43%|████▎     | 385201/900675 [01:04<01:24, 6094.51it/s] 43%|████▎     | 385813/900675 [01:04<01:24, 6087.52it/s] 43%|████▎     | 386424/900675 [01:04<01:29, 5758.57it/s] 43%|████▎     | 387128/900675 [01:04<01:23, 6117.04it/s] 43%|████▎     | 387873/900675 [01:04<01:18, 6500.83it/s] 43%|████▎     | 388529/900675 [01:04<01:24, 6060.83it/s] 43%|████▎     | 389145/900675 [01:04<01:24, 6073.09it/s] 43%|████▎     | 389801/900675 [01:05<01:22, 6210.90it/s] 43%|████▎     | 390428/900675 [01:05<01:22, 6176.11it/s] 43%|████▎     | 391053/900675 [01:05<01:22, 6197.34it/s] 43%|████▎     | 391676/900675 [01:05<01:28, 5767.13it/s] 44%|████▎     | 392260/900675 [01:05<01:29, 5690.17it/s] 44%|████▎     | 392834/900675 [01:05<01:29, 5697.40it/s] 44%|████▎     | 393408/900675 [01:05<01:30, 5594.75it/s] 44%|████▎     | 394022/900675 [01:05<01:28, 5749.45it/s] 44%|████▍     | 394650/900675 [01:05<01:25, 5903.17it/s] 44%|████▍     | 395243/900675 [01:05<01:26, 5873.87it/s] 44%|████▍     | 395857/900675 [01:06<01:24, 5951.76it/s] 44%|████▍     | 396454/900675 [01:06<01:25, 5874.98it/s] 44%|████▍     | 397043/900675 [01:06<01:28, 5720.41it/s] 44%|████▍     | 397617/900675 [01:06<01:28, 5662.78it/s] 44%|████▍     | 398307/900675 [01:06<01:23, 6015.24it/s] 44%|████▍     | 398975/900675 [01:06<01:20, 6203.41it/s] 44%|████▍     | 399655/900675 [01:06<01:18, 6377.65it/s] 44%|████▍     | 400295/900675 [01:06<01:20, 6228.93it/s] 45%|████▍     | 400920/900675 [01:06<01:20, 6234.73it/s] 45%|████▍     | 401545/900675 [01:06<01:22, 6032.67it/s] 45%|████▍     | 402151/900675 [01:07<01:28, 5637.77it/s] 45%|████▍     | 402721/900675 [01:07<01:29, 5579.68it/s] 45%|████▍     | 403331/900675 [01:07<01:26, 5725.83it/s] 45%|████▍     | 403908/900675 [01:07<01:27, 5660.40it/s] 45%|████▍     | 404566/900675 [01:07<01:23, 5923.85it/s] 45%|████▍     | 405162/900675 [01:07<01:26, 5723.52it/s] 45%|████▌     | 405738/900675 [01:07<01:27, 5661.17it/s] 45%|████▌     | 406307/900675 [01:07<01:27, 5646.60it/s] 45%|████▌     | 406874/900675 [01:07<01:29, 5514.31it/s] 45%|████▌     | 407480/900675 [01:08<01:27, 5667.03it/s] 45%|████▌     | 408049/900675 [01:08<01:30, 5472.62it/s] 45%|████▌     | 408723/900675 [01:08<01:24, 5833.39it/s] 45%|████▌     | 409349/900675 [01:08<01:22, 5949.73it/s] 46%|████▌     | 409947/900675 [01:08<01:26, 5693.50it/s] 46%|████▌     | 410561/900675 [01:08<01:24, 5820.21it/s] 46%|████▌     | 411147/900675 [01:08<01:24, 5795.50it/s] 46%|████▌     | 411749/900675 [01:08<01:23, 5858.64it/s] 46%|████▌     | 412337/900675 [01:08<01:28, 5492.39it/s] 46%|████▌     | 412892/900675 [01:09<01:30, 5398.58it/s] 46%|████▌     | 413490/900675 [01:09<01:27, 5559.76it/s] 46%|████▌     | 414050/900675 [01:09<01:31, 5324.08it/s] 46%|████▌     | 414592/900675 [01:09<01:30, 5350.74it/s] 46%|████▌     | 415137/900675 [01:09<01:30, 5375.98it/s] 46%|████▌     | 415764/900675 [01:09<01:26, 5630.72it/s] 46%|████▌     | 416330/900675 [01:09<01:30, 5348.87it/s] 46%|████▋     | 416950/900675 [01:09<01:26, 5587.41it/s] 46%|████▋     | 417547/900675 [01:09<01:24, 5696.26it/s] 46%|████▋     | 418206/900675 [01:09<01:21, 5954.72it/s] 46%|████▋     | 418805/900675 [01:10<01:22, 5838.77it/s] 47%|████▋     | 419421/900675 [01:10<01:21, 5931.17it/s] 47%|████▋     | 420017/900675 [01:10<01:21, 5921.67it/s] 47%|████▋     | 420618/900675 [01:10<01:20, 5940.58it/s] 47%|████▋     | 421214/900675 [01:10<01:25, 5612.90it/s] 47%|████▋     | 421780/900675 [01:10<01:25, 5623.17it/s] 47%|████▋     | 422378/900675 [01:10<01:23, 5722.86it/s] 47%|████▋     | 423060/900675 [01:10<01:19, 6033.60it/s] 47%|████▋     | 423666/900675 [01:10<01:21, 5877.12it/s] 47%|████▋     | 424268/900675 [01:10<01:20, 5912.74it/s] 47%|████▋     | 424862/900675 [01:11<01:22, 5760.78it/s] 47%|████▋     | 425440/900675 [01:11<01:23, 5690.28it/s] 47%|████▋     | 426091/900675 [01:11<01:20, 5924.11it/s] 47%|████▋     | 426823/900675 [01:11<01:14, 6325.80it/s] 47%|████▋     | 427458/900675 [01:11<01:16, 6206.75it/s] 48%|████▊     | 428081/900675 [01:11<01:18, 6027.72it/s] 48%|████▊     | 428822/900675 [01:11<01:13, 6419.27it/s] 48%|████▊     | 429467/900675 [01:11<01:14, 6308.69it/s] 48%|████▊     | 430102/900675 [01:11<01:14, 6313.63it/s] 48%|████▊     | 430735/900675 [01:12<01:16, 6137.23it/s] 48%|████▊     | 431351/900675 [01:12<01:20, 5797.72it/s] 48%|████▊     | 431936/900675 [01:12<01:24, 5566.88it/s] 48%|████▊     | 432823/900675 [01:12<01:12, 6478.49it/s] 48%|████▊     | 433481/900675 [01:12<01:15, 6203.29it/s] 48%|████▊     | 434110/900675 [01:12<01:16, 6127.92it/s] 48%|████▊     | 434729/900675 [01:12<01:20, 5769.01it/s] 48%|████▊     | 435313/900675 [01:12<01:20, 5758.99it/s] 48%|████▊     | 435950/900675 [01:12<01:18, 5925.51it/s] 48%|████▊     | 436547/900675 [01:13<01:19, 5822.65it/s] 49%|████▊     | 437204/900675 [01:13<01:16, 6035.68it/s] 49%|████▊     | 437813/900675 [01:13<01:16, 6051.30it/s] 49%|████▊     | 438421/900675 [01:13<01:18, 5861.89it/s] 49%|████▊     | 439010/900675 [01:13<01:19, 5785.89it/s] 49%|████▉     | 439591/900675 [01:13<01:21, 5649.21it/s] 49%|████▉     | 440158/900675 [01:13<01:21, 5629.49it/s] 49%|████▉     | 440722/900675 [01:13<01:22, 5606.92it/s] 49%|████▉     | 441406/900675 [01:13<01:17, 5960.55it/s] 49%|████▉     | 442023/900675 [01:13<01:16, 6017.46it/s] 49%|████▉     | 442626/900675 [01:14<01:17, 5898.73it/s] 49%|████▉     | 443304/900675 [01:14<01:14, 6153.54it/s] 49%|████▉     | 443941/900675 [01:14<01:13, 6211.53it/s] 49%|████▉     | 444564/900675 [01:14<01:17, 5872.88it/s] 49%|████▉     | 445156/900675 [01:14<01:21, 5618.12it/s] 49%|████▉     | 445748/900675 [01:14<01:19, 5700.83it/s] 50%|████▉     | 446322/900675 [01:14<01:21, 5574.79it/s] 50%|████▉     | 446937/900675 [01:14<01:19, 5738.53it/s] 50%|████▉     | 447514/900675 [01:14<01:20, 5645.27it/s] 50%|████▉     | 448108/900675 [01:15<01:18, 5730.16it/s] 50%|████▉     | 448774/900675 [01:15<01:15, 6000.91it/s] 50%|████▉     | 449377/900675 [01:15<01:17, 5806.00it/s] 50%|████▉     | 449961/900675 [01:15<01:19, 5637.51it/s] 50%|█████     | 450646/900675 [01:15<01:15, 5979.02it/s] 50%|█████     | 451253/900675 [01:15<01:14, 6003.03it/s] 50%|█████     | 451856/900675 [01:15<01:16, 5876.61it/s] 50%|█████     | 452446/900675 [01:15<01:18, 5697.18it/s] 50%|█████     | 453040/900675 [01:15<01:17, 5762.61it/s] 50%|█████     | 453645/900675 [01:15<01:16, 5837.57it/s] 50%|█████     | 454231/900675 [01:16<01:17, 5758.36it/s] 50%|█████     | 454808/900675 [01:16<01:18, 5661.08it/s] 51%|█████     | 455376/900675 [01:16<01:18, 5649.78it/s] 51%|█████     | 455958/900675 [01:16<01:18, 5694.36it/s] 51%|█████     | 456644/900675 [01:16<01:13, 6034.00it/s] 51%|█████     | 457354/900675 [01:16<01:09, 6343.90it/s] 51%|█████     | 457990/900675 [01:16<01:12, 6146.28it/s] 51%|█████     | 458607/900675 [01:16<01:14, 5922.41it/s] 51%|█████     | 459279/900675 [01:16<01:11, 6142.69it/s] 51%|█████     | 459897/900675 [01:17<01:13, 5991.97it/s] 51%|█████     | 460499/900675 [01:17<01:14, 5922.81it/s] 51%|█████     | 461093/900675 [01:17<01:15, 5799.45it/s] 51%|█████▏    | 461730/900675 [01:17<01:13, 5962.56it/s] 51%|█████▏    | 462328/900675 [01:17<01:16, 5744.24it/s] 51%|█████▏    | 462938/900675 [01:17<01:14, 5845.38it/s] 51%|█████▏    | 463534/900675 [01:17<01:14, 5876.88it/s] 52%|█████▏    | 464124/900675 [01:17<01:16, 5732.75it/s] 52%|█████▏    | 464714/900675 [01:17<01:15, 5781.04it/s] 52%|█████▏    | 465341/900675 [01:17<01:13, 5916.57it/s] 52%|█████▏    | 465964/900675 [01:18<01:12, 6007.09it/s] 52%|█████▏    | 466566/900675 [01:18<01:13, 5912.79it/s] 52%|█████▏    | 467159/900675 [01:18<01:13, 5870.16it/s] 52%|█████▏    | 467792/900675 [01:18<01:12, 5997.47it/s] 52%|█████▏    | 468405/900675 [01:18<01:11, 6035.16it/s] 52%|█████▏    | 469011/900675 [01:18<01:11, 6036.26it/s] 52%|█████▏    | 469615/900675 [01:18<01:12, 5937.18it/s] 52%|█████▏    | 470210/900675 [01:18<01:15, 5736.87it/s] 52%|█████▏    | 470906/900675 [01:18<01:10, 6087.26it/s] 52%|█████▏    | 471538/900675 [01:18<01:09, 6152.43it/s] 52%|█████▏    | 472156/900675 [01:19<01:14, 5756.25it/s] 52%|█████▏    | 472780/900675 [01:19<01:12, 5891.56it/s] 53%|█████▎    | 473425/900675 [01:19<01:10, 6047.56it/s] 53%|█████▎    | 474034/900675 [01:19<01:13, 5812.11it/s] 53%|█████▎    | 474620/900675 [01:19<01:13, 5819.30it/s] 53%|█████▎    | 475365/900675 [01:19<01:07, 6288.56it/s] 53%|█████▎    | 476075/900675 [01:19<01:05, 6523.92it/s] 53%|█████▎    | 476731/900675 [01:19<01:09, 6080.01it/s] 53%|█████▎    | 477347/900675 [01:19<01:11, 5948.39it/s] 53%|█████▎    | 477948/900675 [01:20<01:11, 5878.38it/s] 53%|█████▎    | 478649/900675 [01:20<01:08, 6197.59it/s] 53%|█████▎    | 479335/900675 [01:20<01:05, 6387.81it/s] 53%|█████▎    | 479978/900675 [01:20<01:09, 6067.18it/s] 53%|█████▎    | 480603/900675 [01:20<01:08, 6117.33it/s] 53%|█████▎    | 481219/900675 [01:20<01:09, 6077.74it/s] 53%|█████▎    | 481830/900675 [01:20<01:12, 5801.64it/s] 54%|█████▎    | 482415/900675 [01:20<01:13, 5705.30it/s] 54%|█████▎    | 483073/900675 [01:20<01:10, 5949.66it/s] 54%|█████▎    | 483672/900675 [01:21<01:11, 5806.53it/s] 54%|█████▍    | 484302/900675 [01:21<01:10, 5943.36it/s] 54%|█████▍    | 485030/900675 [01:21<01:05, 6327.77it/s] 54%|█████▍    | 485704/900675 [01:21<01:04, 6445.49it/s] 54%|█████▍    | 486351/900675 [01:21<01:08, 6072.87it/s] 54%|█████▍    | 486964/900675 [01:21<01:09, 5948.64it/s] 54%|█████▍    | 487607/900675 [01:21<01:07, 6082.46it/s] 54%|█████▍    | 488230/900675 [01:21<01:07, 6115.30it/s] 54%|█████▍    | 488845/900675 [01:21<01:08, 6023.74it/s] 54%|█████▍    | 489450/900675 [01:21<01:12, 5656.34it/s] 54%|█████▍    | 490021/900675 [01:22<01:13, 5561.66it/s] 54%|█████▍    | 490581/900675 [01:22<01:14, 5510.64it/s] 55%|█████▍    | 491135/900675 [01:22<01:14, 5511.83it/s] 55%|█████▍    | 491789/900675 [01:22<01:10, 5808.48it/s] 55%|█████▍    | 492397/900675 [01:22<01:09, 5884.70it/s] 55%|█████▍    | 493000/900675 [01:22<01:08, 5924.59it/s] 55%|█████▍    | 493594/900675 [01:22<01:10, 5797.18it/s] 55%|█████▍    | 494222/900675 [01:22<01:08, 5934.42it/s] 55%|█████▍    | 494924/900675 [01:22<01:04, 6244.10it/s] 55%|█████▌    | 495550/900675 [01:23<01:07, 6035.75it/s] 55%|█████▌    | 496176/900675 [01:23<01:06, 6097.95it/s] 55%|█████▌    | 496792/900675 [01:23<01:06, 6113.93it/s] 55%|█████▌    | 497405/900675 [01:23<01:05, 6112.62it/s] 55%|█████▌    | 498022/900675 [01:23<01:05, 6127.50it/s] 55%|█████▌    | 498636/900675 [01:23<01:05, 6116.06it/s] 55%|█████▌    | 499274/900675 [01:23<01:04, 6185.47it/s] 56%|█████▌    | 499934/900675 [01:23<01:03, 6305.70it/s] 56%|█████▌    | 500565/900675 [01:23<01:03, 6303.27it/s] 56%|█████▌    | 501196/900675 [01:23<01:03, 6283.40it/s] 56%|█████▌    | 501825/900675 [01:24<01:06, 6001.96it/s] 56%|█████▌    | 502428/900675 [01:24<01:07, 5941.14it/s] 56%|█████▌    | 503024/900675 [01:24<01:08, 5814.67it/s] 56%|█████▌    | 503675/900675 [01:24<01:06, 6006.94it/s] 56%|█████▌    | 504278/900675 [01:24<01:09, 5711.56it/s] 56%|█████▌    | 504853/900675 [01:24<01:09, 5720.88it/s] 56%|█████▌    | 505518/900675 [01:24<01:05, 5987.56it/s] 56%|█████▌    | 506120/900675 [01:24<01:07, 5868.53it/s] 56%|█████▋    | 506808/900675 [01:24<01:03, 6157.71it/s] 56%|█████▋    | 507427/900675 [01:24<01:05, 5967.99it/s] 56%|█████▋    | 508027/900675 [01:25<01:09, 5642.00it/s] 56%|█████▋    | 508596/900675 [01:25<01:09, 5631.66it/s] 57%|█████▋    | 509286/900675 [01:25<01:05, 5991.91it/s] 57%|█████▋    | 509963/900675 [01:25<01:02, 6216.48it/s] 57%|█████▋    | 510589/900675 [01:25<01:05, 5978.05it/s] 57%|█████▋    | 511304/900675 [01:25<01:01, 6312.54it/s] 57%|█████▋    | 511940/900675 [01:25<01:02, 6184.66it/s] 57%|█████▋    | 512562/900675 [01:25<01:03, 6089.57it/s] 57%|█████▋    | 513267/900675 [01:25<01:00, 6364.65it/s] 57%|█████▋    | 513907/900675 [01:26<01:01, 6243.15it/s] 57%|█████▋    | 514534/900675 [01:26<01:03, 6105.64it/s] 57%|█████▋    | 515147/900675 [01:26<01:06, 5806.96it/s] 57%|█████▋    | 515796/900675 [01:26<01:04, 5997.10it/s] 57%|█████▋    | 516423/900675 [01:26<01:03, 6072.48it/s] 57%|█████▋    | 517034/900675 [01:26<01:06, 5788.29it/s] 57%|█████▋    | 517625/900675 [01:26<01:05, 5816.75it/s] 58%|█████▊    | 518308/900675 [01:26<01:02, 6104.15it/s] 58%|█████▊    | 518922/900675 [01:26<01:02, 6109.74it/s] 58%|█████▊    | 519536/900675 [01:26<01:02, 6073.80it/s] 58%|█████▊    | 520146/900675 [01:27<01:03, 5960.57it/s] 58%|█████▊    | 520760/900675 [01:27<01:03, 6011.82it/s] 58%|█████▊    | 521363/900675 [01:27<01:03, 5970.06it/s] 58%|█████▊    | 522057/900675 [01:27<01:00, 6252.72it/s] 58%|█████▊    | 522684/900675 [01:27<01:04, 5881.41it/s] 58%|█████▊    | 523278/900675 [01:27<01:04, 5833.09it/s] 58%|█████▊    | 523867/900675 [01:27<01:04, 5848.77it/s] 58%|█████▊    | 524471/900675 [01:27<01:03, 5902.32it/s] 58%|█████▊    | 525064/900675 [01:27<01:03, 5874.65it/s] 58%|█████▊    | 525653/900675 [01:28<01:07, 5595.13it/s] 58%|█████▊    | 526335/900675 [01:28<01:03, 5939.64it/s] 59%|█████▊    | 527101/900675 [01:28<00:58, 6428.03it/s] 59%|█████▊    | 527749/900675 [01:28<01:04, 5809.66it/s] 59%|█████▊    | 528427/900675 [01:28<01:01, 6066.33it/s] 59%|█████▊    | 529065/900675 [01:28<01:00, 6146.73it/s] 59%|█████▉    | 529689/900675 [01:28<01:01, 6017.39it/s] 59%|█████▉    | 530298/900675 [01:28<01:02, 5912.56it/s] 59%|█████▉    | 530894/900675 [01:28<01:05, 5649.96it/s] 59%|█████▉    | 531464/900675 [01:29<01:05, 5601.48it/s] 59%|█████▉    | 532086/900675 [01:29<01:03, 5772.44it/s] 59%|█████▉    | 532676/900675 [01:29<01:03, 5802.06it/s] 59%|█████▉    | 533300/900675 [01:29<01:01, 5927.68it/s] 59%|█████▉    | 533895/900675 [01:29<01:04, 5698.36it/s] 59%|█████▉    | 534468/900675 [01:29<01:05, 5614.33it/s] 59%|█████▉    | 535079/900675 [01:29<01:03, 5755.81it/s] 59%|█████▉    | 535675/900675 [01:29<01:02, 5812.82it/s] 60%|█████▉    | 536258/900675 [01:29<01:03, 5752.37it/s] 60%|█████▉    | 536893/900675 [01:29<01:01, 5926.39it/s] 60%|█████▉    | 537487/900675 [01:30<01:04, 5656.23it/s] 60%|█████▉    | 538131/900675 [01:30<01:01, 5876.29it/s] 60%|█████▉    | 538802/900675 [01:30<00:59, 6118.14it/s] 60%|█████▉    | 539417/900675 [01:30<00:59, 6094.87it/s] 60%|█████▉    | 540029/900675 [01:30<01:01, 5825.29it/s] 60%|██████    | 540762/900675 [01:30<00:57, 6252.99it/s] 60%|██████    | 541392/900675 [01:30<01:02, 5715.31it/s] 60%|██████    | 542016/900675 [01:30<01:01, 5859.14it/s] 60%|██████    | 542662/900675 [01:30<00:59, 6028.17it/s] 60%|██████    | 543335/900675 [01:31<00:57, 6229.32it/s] 60%|██████    | 543965/900675 [01:31<00:58, 6086.83it/s] 60%|██████    | 544579/900675 [01:31<01:00, 5869.53it/s] 61%|██████    | 545171/900675 [01:31<01:01, 5803.48it/s] 61%|██████    | 545792/900675 [01:31<00:59, 5919.34it/s] 61%|██████    | 546466/900675 [01:31<00:57, 6146.68it/s] 61%|██████    | 547084/900675 [01:31<01:00, 5814.44it/s] 61%|██████    | 547728/900675 [01:31<00:58, 5985.41it/s] 61%|██████    | 548411/900675 [01:31<00:56, 6204.63it/s] 61%|██████    | 549036/900675 [01:31<00:58, 6032.02it/s] 61%|██████    | 549643/900675 [01:32<00:58, 6023.13it/s] 61%|██████    | 550269/900675 [01:32<00:57, 6087.26it/s] 61%|██████    | 550900/900675 [01:32<00:56, 6147.93it/s] 61%|██████    | 551517/900675 [01:32<00:57, 6075.27it/s] 61%|██████▏   | 552126/900675 [01:32<00:58, 5970.55it/s] 61%|██████▏   | 552725/900675 [01:32<00:59, 5828.50it/s] 61%|██████▏   | 553315/900675 [01:32<00:59, 5843.69it/s] 62%|██████▏   | 553933/900675 [01:32<00:58, 5939.60it/s] 62%|██████▏   | 554528/900675 [01:32<00:59, 5789.35it/s] 62%|██████▏   | 555109/900675 [01:32<01:00, 5727.52it/s] 62%|██████▏   | 555808/900675 [01:33<00:56, 6093.36it/s] 62%|██████▏   | 556420/900675 [01:33<00:56, 6055.19it/s] 62%|██████▏   | 557027/900675 [01:33<00:57, 5935.47it/s] 62%|██████▏   | 557622/900675 [01:33<00:58, 5872.18it/s] 62%|██████▏   | 558211/900675 [01:33<00:59, 5774.42it/s] 62%|██████▏   | 558790/900675 [01:33<01:00, 5616.78it/s] 62%|██████▏   | 559353/900675 [01:33<01:01, 5522.93it/s] 62%|██████▏   | 559917/900675 [01:33<01:01, 5550.08it/s] 62%|██████▏   | 560526/900675 [01:33<00:59, 5703.62it/s] 62%|██████▏   | 561156/900675 [01:34<00:57, 5874.85it/s] 62%|██████▏   | 561785/900675 [01:34<00:56, 5992.22it/s] 62%|██████▏   | 562473/900675 [01:34<00:54, 6254.78it/s] 63%|██████▎   | 563100/900675 [01:34<00:53, 6258.59it/s] 63%|██████▎   | 563727/900675 [01:34<00:57, 5820.18it/s] 63%|██████▎   | 564350/900675 [01:34<00:56, 5932.02it/s] 63%|██████▎   | 564949/900675 [01:34<00:59, 5652.44it/s] 63%|██████▎   | 565626/900675 [01:34<00:56, 5965.41it/s] 63%|██████▎   | 566229/900675 [01:34<00:56, 5880.58it/s] 63%|██████▎   | 566822/900675 [01:35<01:00, 5484.36it/s] 63%|██████▎   | 567548/900675 [01:35<00:55, 5974.26it/s] 63%|██████▎   | 568185/900675 [01:35<00:54, 6083.75it/s] 63%|██████▎   | 568801/900675 [01:35<00:54, 6078.44it/s] 63%|██████▎   | 569417/900675 [01:35<00:54, 6098.74it/s] 63%|██████▎   | 570082/900675 [01:35<00:52, 6260.21it/s] 63%|██████▎   | 570712/900675 [01:35<00:52, 6270.98it/s] 63%|██████▎   | 571546/900675 [01:35<00:47, 6878.27it/s] 64%|██████▎   | 572236/900675 [01:35<00:52, 6289.41it/s] 64%|██████▎   | 572876/900675 [01:35<00:53, 6171.75it/s] 64%|██████▎   | 573501/900675 [01:36<00:56, 5836.84it/s] 64%|██████▎   | 574103/900675 [01:36<00:55, 5878.44it/s] 64%|██████▍   | 574697/900675 [01:36<00:56, 5729.75it/s] 64%|██████▍   | 575275/900675 [01:36<00:56, 5710.42it/s] 64%|██████▍   | 575849/900675 [01:36<00:56, 5708.28it/s] 64%|██████▍   | 576432/900675 [01:36<00:56, 5739.96it/s] 64%|██████▍   | 577073/900675 [01:36<00:54, 5935.44it/s] 64%|██████▍   | 577669/900675 [01:36<00:54, 5925.85it/s] 64%|██████▍   | 578263/900675 [01:36<00:58, 5521.04it/s] 64%|██████▍   | 578867/900675 [01:37<00:56, 5666.19it/s] 64%|██████▍   | 579524/900675 [01:37<00:54, 5922.48it/s] 64%|██████▍   | 580155/900675 [01:37<00:53, 6034.89it/s] 64%|██████▍   | 580806/900675 [01:37<00:51, 6170.32it/s] 65%|██████▍   | 581426/900675 [01:37<00:52, 6050.48it/s] 65%|██████▍   | 582053/900675 [01:37<00:52, 6110.95it/s] 65%|██████▍   | 582676/900675 [01:37<00:51, 6141.48it/s] 65%|██████▍   | 583330/900675 [01:37<00:50, 6258.28it/s] 65%|██████▍   | 584075/900675 [01:37<00:47, 6605.03it/s] 65%|██████▍   | 584772/900675 [01:37<00:47, 6713.02it/s] 65%|██████▌   | 585445/900675 [01:38<00:47, 6597.82it/s] 65%|██████▌   | 586106/900675 [01:38<00:50, 6200.79it/s] 65%|██████▌   | 586732/900675 [01:38<00:51, 6133.97it/s] 65%|██████▌   | 587349/900675 [01:38<00:55, 5648.56it/s] 65%|██████▌   | 588006/900675 [01:38<00:53, 5898.85it/s] 65%|██████▌   | 588605/900675 [01:38<00:53, 5819.79it/s] 65%|██████▌   | 589263/900675 [01:38<00:51, 6027.67it/s] 65%|██████▌   | 589872/900675 [01:38<00:52, 5901.70it/s] 66%|██████▌   | 590569/900675 [01:38<00:49, 6202.80it/s] 66%|██████▌   | 591194/900675 [01:39<00:51, 6030.65it/s] 66%|██████▌   | 591850/900675 [01:39<00:49, 6179.75it/s] 66%|██████▌   | 592472/900675 [01:39<00:53, 5745.68it/s] 66%|██████▌   | 593055/900675 [01:39<00:53, 5705.60it/s] 66%|██████▌   | 593631/900675 [01:39<00:53, 5717.12it/s] 66%|██████▌   | 594267/900675 [01:39<00:51, 5901.60it/s] 66%|██████▌   | 594910/900675 [01:39<00:50, 6052.73it/s] 66%|██████▌   | 595519/900675 [01:39<00:53, 5745.91it/s] 66%|██████▌   | 596129/900675 [01:39<00:52, 5843.25it/s] 66%|██████▋   | 596751/900675 [01:39<00:51, 5951.75it/s] 66%|██████▋   | 597367/900675 [01:40<00:50, 6007.53it/s] 66%|██████▋   | 597971/900675 [01:40<00:51, 5862.87it/s] 66%|██████▋   | 598606/900675 [01:40<00:50, 6002.38it/s] 67%|██████▋   | 599209/900675 [01:40<00:50, 5952.93it/s] 67%|██████▋   | 599853/900675 [01:40<00:49, 6088.29it/s] 67%|██████▋   | 600464/900675 [01:40<00:50, 5981.46it/s] 67%|██████▋   | 601067/900675 [01:40<00:50, 5991.98it/s] 67%|██████▋   | 601668/900675 [01:40<00:54, 5466.82it/s] 67%|██████▋   | 602224/900675 [01:40<00:55, 5332.30it/s] 67%|██████▋   | 602924/900675 [01:41<00:51, 5793.48it/s] 67%|██████▋   | 603590/900675 [01:41<00:49, 6038.03it/s] 67%|██████▋   | 604201/900675 [01:41<00:50, 5927.15it/s] 67%|██████▋   | 604799/900675 [01:41<00:49, 5937.34it/s] 67%|██████▋   | 605397/900675 [01:41<00:50, 5809.23it/s] 67%|██████▋   | 605981/900675 [01:41<00:51, 5774.23it/s] 67%|██████▋   | 606707/900675 [01:41<00:47, 6194.94it/s] 67%|██████▋   | 607330/900675 [01:41<00:49, 5940.66it/s] 68%|██████▊   | 607957/900675 [01:41<00:48, 6028.43it/s] 68%|██████▊   | 608563/900675 [01:41<00:50, 5799.59it/s] 68%|██████▊   | 609147/900675 [01:42<00:51, 5672.78it/s] 68%|██████▊   | 609779/900675 [01:42<00:49, 5850.44it/s] 68%|██████▊   | 610501/900675 [01:42<00:46, 6240.33it/s] 68%|██████▊   | 611129/900675 [01:42<00:47, 6125.29it/s] 68%|██████▊   | 611745/900675 [01:42<00:47, 6127.55it/s] 68%|██████▊   | 612360/900675 [01:42<00:48, 5957.49it/s] 68%|██████▊   | 612974/900675 [01:42<00:47, 6005.17it/s] 68%|██████▊   | 613577/900675 [01:42<00:48, 5885.42it/s] 68%|██████▊   | 614167/900675 [01:42<00:49, 5760.24it/s] 68%|██████▊   | 614745/900675 [01:43<00:50, 5652.21it/s] 68%|██████▊   | 615312/900675 [01:43<00:51, 5490.51it/s] 68%|██████▊   | 615863/900675 [01:43<00:52, 5407.33it/s] 68%|██████▊   | 616502/900675 [01:43<00:50, 5680.87it/s] 69%|██████▊   | 617298/900675 [01:43<00:44, 6336.55it/s] 69%|██████▊   | 617980/900675 [01:43<00:43, 6471.03it/s] 69%|██████▊   | 618667/900675 [01:43<00:42, 6575.32it/s] 69%|██████▉   | 619327/900675 [01:43<00:42, 6558.49it/s] 69%|██████▉   | 620143/900675 [01:43<00:39, 7027.49it/s] 69%|██████▉   | 620848/900675 [01:43<00:40, 6849.38it/s] 69%|██████▉   | 621535/900675 [01:44<00:44, 6245.80it/s] 69%|██████▉   | 622173/900675 [01:44<00:44, 6271.79it/s] 69%|██████▉   | 622900/900675 [01:44<00:42, 6546.56it/s] 69%|██████▉   | 623562/900675 [01:44<00:42, 6558.45it/s] 69%|██████▉   | 624223/900675 [01:44<00:45, 6047.31it/s] 69%|██████▉   | 624839/900675 [01:44<00:46, 5916.26it/s] 69%|██████▉   | 625661/900675 [01:44<00:41, 6553.17it/s] 70%|██████▉   | 626327/900675 [01:44<00:42, 6398.89it/s] 70%|██████▉   | 626975/900675 [01:44<00:45, 6075.81it/s] 70%|██████▉   | 627605/900675 [01:45<00:44, 6131.56it/s] 70%|██████▉   | 628224/900675 [01:45<00:45, 6053.78it/s] 70%|██████▉   | 628845/900675 [01:45<00:44, 6094.58it/s] 70%|██████▉   | 629504/900675 [01:45<00:43, 6233.78it/s] 70%|██████▉   | 630130/900675 [01:45<00:46, 5877.44it/s] 70%|███████   | 630812/900675 [01:45<00:43, 6136.06it/s] 70%|███████   | 631524/900675 [01:45<00:41, 6418.23it/s] 70%|███████   | 632171/900675 [01:45<00:41, 6394.62it/s] 70%|███████   | 632858/900675 [01:45<00:40, 6532.55it/s] 70%|███████   | 633553/900675 [01:45<00:40, 6650.00it/s] 70%|███████   | 634221/900675 [01:46<00:41, 6418.45it/s] 70%|███████   | 634866/900675 [01:46<00:42, 6217.49it/s] 71%|███████   | 635497/900675 [01:46<00:42, 6243.48it/s] 71%|███████   | 636166/900675 [01:46<00:41, 6363.77it/s] 71%|███████   | 636805/900675 [01:46<00:44, 5958.51it/s] 71%|███████   | 637411/900675 [01:46<00:44, 5970.31it/s] 71%|███████   | 638129/900675 [01:46<00:41, 6313.09it/s] 71%|███████   | 638765/900675 [01:46<00:42, 6201.25it/s] 71%|███████   | 639389/900675 [01:46<00:42, 6111.65it/s] 71%|███████   | 640003/900675 [01:47<00:44, 5922.52it/s] 71%|███████   | 640708/900675 [01:47<00:41, 6243.82it/s] 71%|███████   | 641476/900675 [01:47<00:38, 6654.32it/s] 71%|███████▏  | 642146/900675 [01:47<00:40, 6390.66it/s] 71%|███████▏  | 642790/900675 [01:47<00:40, 6341.37it/s] 71%|███████▏  | 643428/900675 [01:47<00:44, 5830.29it/s] 72%|███████▏  | 644029/900675 [01:47<00:43, 5866.43it/s] 72%|███████▏  | 644623/900675 [01:47<00:44, 5799.17it/s] 72%|███████▏  | 645208/900675 [01:47<00:44, 5744.53it/s] 72%|███████▏  | 645960/900675 [01:48<00:40, 6242.81it/s] 72%|███████▏  | 646589/900675 [01:48<00:43, 5837.20it/s] 72%|███████▏  | 647181/900675 [01:48<00:43, 5805.80it/s] 72%|███████▏  | 647767/900675 [01:48<00:44, 5737.38it/s] 72%|███████▏  | 648345/900675 [01:48<00:47, 5276.97it/s] 72%|███████▏  | 648989/900675 [01:48<00:45, 5590.04it/s] 72%|███████▏  | 649557/900675 [01:48<00:46, 5392.64it/s] 72%|███████▏  | 650142/900675 [01:48<00:45, 5519.19it/s] 72%|███████▏  | 650703/900675 [01:48<00:45, 5540.65it/s] 72%|███████▏  | 651284/900675 [01:49<00:44, 5616.10it/s] 72%|███████▏  | 651959/900675 [01:49<00:41, 5941.87it/s] 72%|███████▏  | 652557/900675 [01:49<00:42, 5843.58it/s] 73%|███████▎  | 653274/900675 [01:49<00:39, 6218.77it/s] 73%|███████▎  | 653899/900675 [01:49<00:40, 6070.64it/s] 73%|███████▎  | 654581/900675 [01:49<00:39, 6280.36it/s] 73%|███████▎  | 655212/900675 [01:49<00:40, 6079.59it/s] 73%|███████▎  | 655823/900675 [01:49<00:40, 6054.00it/s] 73%|███████▎  | 656431/900675 [01:49<00:41, 5940.94it/s] 73%|███████▎  | 657041/900675 [01:49<00:40, 5980.17it/s] 73%|███████▎  | 657747/900675 [01:50<00:38, 6291.35it/s] 73%|███████▎  | 658378/900675 [01:50<00:41, 5839.05it/s] 73%|███████▎  | 658989/900675 [01:50<00:40, 5912.84it/s] 73%|███████▎  | 659595/900675 [01:50<00:40, 5948.02it/s] 73%|███████▎  | 660234/900675 [01:50<00:39, 6075.39it/s] 73%|███████▎  | 660845/900675 [01:50<00:39, 6018.02it/s] 73%|███████▎  | 661450/900675 [01:50<00:40, 5869.75it/s] 74%|███████▎  | 662106/900675 [01:50<00:39, 6065.37it/s] 74%|███████▎  | 662715/900675 [01:50<00:39, 6070.94it/s] 74%|███████▎  | 663324/900675 [01:50<00:40, 5912.25it/s] 74%|███████▎  | 663917/900675 [01:51<00:40, 5915.46it/s] 74%|███████▍  | 664562/900675 [01:51<00:38, 6070.13it/s] 74%|███████▍  | 665171/900675 [01:51<00:38, 6042.45it/s] 74%|███████▍  | 665828/900675 [01:51<00:37, 6197.99it/s] 74%|███████▍  | 666449/900675 [01:51<00:37, 6181.57it/s] 74%|███████▍  | 667068/900675 [01:51<00:39, 5980.08it/s] 74%|███████▍  | 667668/900675 [01:51<00:39, 5958.43it/s] 74%|███████▍  | 668293/900675 [01:51<00:38, 6041.76it/s] 74%|███████▍  | 668958/900675 [01:51<00:37, 6219.70it/s] 74%|███████▍  | 669649/900675 [01:52<00:35, 6420.29it/s] 74%|███████▍  | 670292/900675 [01:52<00:37, 6193.40it/s] 74%|███████▍  | 670938/900675 [01:52<00:36, 6267.63it/s] 75%|███████▍  | 671571/900675 [01:52<00:36, 6280.48it/s] 75%|███████▍  | 672201/900675 [01:52<00:36, 6222.14it/s] 75%|███████▍  | 672869/900675 [01:52<00:35, 6352.05it/s] 75%|███████▍  | 673506/900675 [01:52<00:36, 6260.82it/s] 75%|███████▍  | 674133/900675 [01:52<00:37, 6072.06it/s] 75%|███████▍  | 674742/900675 [01:52<00:37, 6061.55it/s] 75%|███████▍  | 675392/900675 [01:52<00:36, 6185.45it/s] 75%|███████▌  | 676039/900675 [01:53<00:35, 6258.45it/s] 75%|███████▌  | 676666/900675 [01:53<00:38, 5858.98it/s] 75%|███████▌  | 677271/900675 [01:53<00:37, 5905.18it/s] 75%|███████▌  | 677894/900675 [01:53<00:37, 5998.35it/s] 75%|███████▌  | 678498/900675 [01:53<00:37, 5863.66it/s] 75%|███████▌  | 679101/900675 [01:53<00:37, 5908.88it/s] 75%|███████▌  | 679694/900675 [01:53<00:38, 5706.88it/s] 76%|███████▌  | 680431/900675 [01:53<00:35, 6176.91it/s] 76%|███████▌  | 681100/900675 [01:53<00:34, 6325.52it/s] 76%|███████▌  | 681736/900675 [01:53<00:35, 6171.74it/s] 76%|███████▌  | 682356/900675 [01:54<00:36, 6010.97it/s] 76%|███████▌  | 682960/900675 [01:54<00:37, 5870.41it/s] 76%|███████▌  | 683633/900675 [01:54<00:35, 6114.77it/s] 76%|███████▌  | 684317/900675 [01:54<00:34, 6319.02it/s] 76%|███████▌  | 684980/900675 [01:54<00:33, 6403.30it/s] 76%|███████▌  | 685623/900675 [01:54<00:35, 6080.96it/s] 76%|███████▌  | 686255/900675 [01:54<00:34, 6148.73it/s] 76%|███████▋  | 686874/900675 [01:54<00:36, 5823.64it/s] 76%|███████▋  | 687462/900675 [01:54<00:36, 5817.43it/s] 76%|███████▋  | 688071/900675 [01:55<00:36, 5884.56it/s] 76%|███████▋  | 688672/900675 [01:55<00:35, 5918.80it/s] 77%|███████▋  | 689291/900675 [01:55<00:35, 5994.27it/s] 77%|███████▋  | 689893/900675 [01:55<00:36, 5767.49it/s] 77%|███████▋  | 690473/900675 [01:55<00:36, 5687.23it/s] 77%|███████▋  | 691047/900675 [01:55<00:36, 5694.24it/s] 77%|███████▋  | 691618/900675 [01:55<00:38, 5481.48it/s] 77%|███████▋  | 692172/900675 [01:55<00:37, 5496.23it/s] 77%|███████▋  | 692724/900675 [01:55<00:38, 5424.42it/s] 77%|███████▋  | 693436/900675 [01:55<00:35, 5915.55it/s] 77%|███████▋  | 694031/900675 [01:56<00:36, 5674.82it/s] 77%|███████▋  | 694677/900675 [01:56<00:34, 5893.92it/s] 77%|███████▋  | 695270/900675 [01:56<00:36, 5678.96it/s] 77%|███████▋  | 695899/900675 [01:56<00:35, 5846.08it/s] 77%|███████▋  | 696487/900675 [01:56<00:35, 5774.25it/s] 77%|███████▋  | 697067/900675 [01:56<00:35, 5772.48it/s] 77%|███████▋  | 697708/900675 [01:56<00:34, 5950.99it/s] 78%|███████▊  | 698356/900675 [01:56<00:33, 6099.38it/s] 78%|███████▊  | 698968/900675 [01:56<00:35, 5677.26it/s] 78%|███████▊  | 699600/900675 [01:57<00:34, 5858.30it/s] 78%|███████▊  | 700192/900675 [01:57<00:35, 5604.00it/s] 78%|███████▊  | 700825/900675 [01:57<00:34, 5799.07it/s] 78%|███████▊  | 701451/900675 [01:57<00:33, 5928.77it/s] 78%|███████▊  | 702174/900675 [01:57<00:31, 6302.67it/s] 78%|███████▊  | 702809/900675 [01:57<00:31, 6207.48it/s] 78%|███████▊  | 703433/900675 [01:57<00:32, 6076.74it/s] 78%|███████▊  | 704047/900675 [01:57<00:32, 6089.77it/s] 78%|███████▊  | 704658/900675 [01:57<00:32, 6089.40it/s] 78%|███████▊  | 705269/900675 [01:58<00:33, 5801.58it/s] 78%|███████▊  | 705853/900675 [01:58<00:35, 5536.18it/s] 78%|███████▊  | 706427/900675 [01:58<00:34, 5593.05it/s] 78%|███████▊  | 707007/900675 [01:58<00:34, 5650.16it/s] 79%|███████▊  | 707577/900675 [01:58<00:34, 5658.27it/s] 79%|███████▊  | 708170/900675 [01:58<00:33, 5735.70it/s] 79%|███████▊  | 708831/900675 [01:58<00:32, 5991.52it/s] 79%|███████▉  | 709547/900675 [01:58<00:30, 6337.32it/s] 79%|███████▉  | 710183/900675 [01:58<00:30, 6259.11it/s] 79%|███████▉  | 710811/900675 [01:58<00:31, 6052.31it/s] 79%|███████▉  | 711500/900675 [01:59<00:30, 6289.85it/s] 79%|███████▉  | 712132/900675 [01:59<00:30, 6154.70it/s] 79%|███████▉  | 712794/900675 [01:59<00:29, 6281.43it/s] 79%|███████▉  | 713424/900675 [01:59<00:29, 6269.63it/s] 79%|███████▉  | 714053/900675 [01:59<00:29, 6227.80it/s] 79%|███████▉  | 714677/900675 [01:59<00:30, 6067.18it/s] 79%|███████▉  | 715285/900675 [01:59<00:30, 6017.77it/s] 79%|███████▉  | 715919/900675 [01:59<00:30, 6111.00it/s] 80%|███████▉  | 716568/900675 [01:59<00:29, 6219.23it/s] 80%|███████▉  | 717191/900675 [01:59<00:29, 6177.97it/s] 80%|███████▉  | 717810/900675 [02:00<00:31, 5855.54it/s] 80%|███████▉  | 718400/900675 [02:00<00:31, 5796.73it/s] 80%|███████▉  | 718984/900675 [02:00<00:31, 5804.86it/s] 80%|███████▉  | 719567/900675 [02:00<00:31, 5687.06it/s] 80%|███████▉  | 720138/900675 [02:00<00:32, 5612.05it/s] 80%|████████  | 720725/900675 [02:00<00:31, 5682.31it/s] 80%|████████  | 721317/900675 [02:00<00:31, 5750.75it/s] 80%|████████  | 721939/900675 [02:00<00:30, 5884.72it/s] 80%|████████  | 722590/900675 [02:00<00:29, 6068.72it/s] 80%|████████  | 723198/900675 [02:01<00:31, 5723.24it/s] 80%|████████  | 723911/900675 [02:01<00:28, 6124.48it/s] 80%|████████  | 724529/900675 [02:01<00:29, 6012.86it/s] 81%|████████  | 725135/900675 [02:01<00:29, 6022.36it/s] 81%|████████  | 725740/900675 [02:01<00:31, 5579.92it/s] 81%|████████  | 726363/900675 [02:01<00:30, 5753.57it/s] 81%|████████  | 727003/900675 [02:01<00:29, 5928.85it/s] 81%|████████  | 727608/900675 [02:01<00:29, 5963.17it/s] 81%|████████  | 728209/900675 [02:01<00:29, 5917.75it/s] 81%|████████  | 728804/900675 [02:01<00:29, 5826.22it/s] 81%|████████  | 729389/900675 [02:02<00:30, 5650.79it/s] 81%|████████  | 729997/900675 [02:02<00:29, 5772.98it/s] 81%|████████  | 730577/900675 [02:02<00:29, 5731.87it/s] 81%|████████  | 731152/900675 [02:02<00:29, 5737.00it/s] 81%|████████  | 731727/900675 [02:02<00:29, 5736.00it/s] 81%|████████▏ | 732319/900675 [02:02<00:29, 5782.58it/s] 81%|████████▏ | 732976/900675 [02:02<00:27, 6012.01it/s] 81%|████████▏ | 733719/900675 [02:02<00:25, 6427.12it/s] 82%|████████▏ | 734398/900675 [02:02<00:25, 6530.01it/s] 82%|████████▏ | 735052/900675 [02:03<00:26, 6213.84it/s] 82%|████████▏ | 735677/900675 [02:03<00:28, 5780.16it/s] 82%|████████▏ | 736364/900675 [02:03<00:27, 6076.98it/s] 82%|████████▏ | 737199/900675 [02:03<00:24, 6718.77it/s] 82%|████████▏ | 737880/900675 [02:03<00:25, 6431.54it/s] 82%|████████▏ | 738532/900675 [02:03<00:26, 6218.17it/s] 82%|████████▏ | 739190/900675 [02:03<00:25, 6315.73it/s] 82%|████████▏ | 739827/900675 [02:03<00:26, 6067.24it/s] 82%|████████▏ | 740439/900675 [02:03<00:26, 5973.23it/s] 82%|████████▏ | 741102/900675 [02:03<00:25, 6147.47it/s] 82%|████████▏ | 741720/900675 [02:04<00:27, 5836.18it/s] 82%|████████▏ | 742309/900675 [02:04<00:28, 5545.96it/s] 82%|████████▏ | 742914/900675 [02:04<00:27, 5674.77it/s] 83%|████████▎ | 743575/900675 [02:04<00:26, 5938.07it/s] 83%|████████▎ | 744174/900675 [02:04<00:28, 5543.31it/s] 83%|████████▎ | 744794/900675 [02:04<00:27, 5722.44it/s] 83%|████████▎ | 745389/900675 [02:04<00:26, 5784.64it/s] 83%|████████▎ | 745973/900675 [02:04<00:27, 5625.47it/s] 83%|████████▎ | 746655/900675 [02:04<00:25, 5962.46it/s] 83%|████████▎ | 747256/900675 [02:05<00:26, 5879.11it/s] 83%|████████▎ | 747848/900675 [02:05<00:27, 5656.38it/s] 83%|████████▎ | 748418/900675 [02:05<00:27, 5621.56it/s] 83%|████████▎ | 749070/900675 [02:05<00:25, 5878.91it/s] 83%|████████▎ | 749661/900675 [02:05<00:26, 5808.07it/s] 83%|████████▎ | 750283/900675 [02:05<00:25, 5919.42it/s] 83%|████████▎ | 750905/900675 [02:05<00:24, 6002.74it/s] 83%|████████▎ | 751507/900675 [02:05<00:25, 5748.41it/s] 84%|████████▎ | 752086/900675 [02:05<00:25, 5758.89it/s] 84%|████████▎ | 752685/900675 [02:06<00:25, 5819.21it/s] 84%|████████▎ | 753315/900675 [02:06<00:24, 5958.04it/s] 84%|████████▎ | 753974/900675 [02:06<00:23, 6144.29it/s] 84%|████████▍ | 754590/900675 [02:06<00:24, 5846.71it/s] 84%|████████▍ | 755179/900675 [02:06<00:25, 5716.31it/s] 84%|████████▍ | 755754/900675 [02:06<00:25, 5642.06it/s] 84%|████████▍ | 756321/900675 [02:06<00:26, 5394.26it/s] 84%|████████▍ | 756958/900675 [02:06<00:25, 5667.67it/s] 84%|████████▍ | 757578/900675 [02:06<00:24, 5811.37it/s] 84%|████████▍ | 758198/900675 [02:06<00:24, 5921.31it/s] 84%|████████▍ | 758793/900675 [02:07<00:24, 5905.63it/s] 84%|████████▍ | 759432/900675 [02:07<00:23, 6038.91it/s] 84%|████████▍ | 760069/900675 [02:07<00:22, 6132.99it/s] 84%|████████▍ | 760762/900675 [02:07<00:21, 6368.48it/s] 85%|████████▍ | 761400/900675 [02:07<00:22, 6121.48it/s] 85%|████████▍ | 762048/900675 [02:07<00:22, 6221.98it/s] 85%|████████▍ | 762673/900675 [02:07<00:23, 5877.83it/s] 85%|████████▍ | 763266/900675 [02:07<00:24, 5541.79it/s] 85%|████████▍ | 763889/900675 [02:07<00:23, 5731.19it/s] 85%|████████▍ | 764516/900675 [02:08<00:23, 5876.92it/s] 85%|████████▍ | 765109/900675 [02:08<00:23, 5691.76it/s] 85%|████████▌ | 765683/900675 [02:08<00:23, 5637.15it/s] 85%|████████▌ | 766370/900675 [02:08<00:22, 5987.95it/s] 85%|████████▌ | 767021/900675 [02:08<00:21, 6135.37it/s] 85%|████████▌ | 767638/900675 [02:08<00:21, 6109.69it/s] 85%|████████▌ | 768252/900675 [02:08<00:22, 5938.00it/s] 85%|████████▌ | 768849/900675 [02:08<00:22, 5739.82it/s] 85%|████████▌ | 769426/900675 [02:08<00:23, 5634.62it/s] 85%|████████▌ | 769992/900675 [02:08<00:23, 5539.09it/s] 86%|████████▌ | 770582/900675 [02:09<00:23, 5634.03it/s] 86%|████████▌ | 771147/900675 [02:09<00:23, 5558.29it/s] 86%|████████▌ | 771824/900675 [02:09<00:21, 5904.49it/s] 86%|████████▌ | 772417/900675 [02:09<00:22, 5708.53it/s] 86%|████████▌ | 773110/900675 [02:09<00:21, 6058.95it/s] 86%|████████▌ | 773731/900675 [02:09<00:20, 6102.33it/s] 86%|████████▌ | 774360/900675 [02:09<00:20, 6153.43it/s] 86%|████████▌ | 774978/900675 [02:09<00:21, 5843.11it/s] 86%|████████▌ | 775575/900675 [02:09<00:21, 5876.78it/s] 86%|████████▌ | 776166/900675 [02:10<00:21, 5724.01it/s] 86%|████████▌ | 776742/900675 [02:10<00:22, 5540.67it/s] 86%|████████▋ | 777415/900675 [02:10<00:20, 5877.75it/s] 86%|████████▋ | 778046/900675 [02:10<00:20, 5997.50it/s] 86%|████████▋ | 778661/900675 [02:10<00:20, 6038.74it/s] 87%|████████▋ | 779268/900675 [02:10<00:20, 5811.10it/s] 87%|████████▋ | 779885/900675 [02:10<00:20, 5909.02it/s] 87%|████████▋ | 780479/900675 [02:10<00:21, 5597.99it/s] 87%|████████▋ | 781060/900675 [02:10<00:21, 5655.36it/s] 87%|████████▋ | 781858/900675 [02:10<00:18, 6322.51it/s] 87%|████████▋ | 782496/900675 [02:11<00:19, 5969.04it/s] 87%|████████▋ | 783101/900675 [02:11<00:21, 5410.32it/s] 87%|████████▋ | 783666/900675 [02:11<00:21, 5470.19it/s] 87%|████████▋ | 784223/900675 [02:11<00:21, 5460.42it/s] 87%|████████▋ | 784777/900675 [02:11<00:21, 5481.47it/s] 87%|████████▋ | 785399/900675 [02:11<00:20, 5687.45it/s] 87%|████████▋ | 785981/900675 [02:11<00:20, 5725.81it/s] 87%|████████▋ | 786663/900675 [02:11<00:18, 6040.74it/s] 87%|████████▋ | 787271/900675 [02:11<00:19, 5816.83it/s] 87%|████████▋ | 787918/900675 [02:12<00:18, 6004.56it/s] 88%|████████▊ | 788522/900675 [02:12<00:19, 5857.08it/s] 88%|████████▊ | 789111/900675 [02:12<00:19, 5811.97it/s] 88%|████████▊ | 789724/900675 [02:12<00:18, 5900.03it/s] 88%|████████▊ | 790369/900675 [02:12<00:18, 6060.28it/s] 88%|████████▊ | 790997/900675 [02:12<00:17, 6119.70it/s] 88%|████████▊ | 791611/900675 [02:12<00:18, 5809.45it/s] 88%|████████▊ | 792196/900675 [02:12<00:19, 5660.08it/s] 88%|████████▊ | 792916/900675 [02:12<00:17, 6095.75it/s] 88%|████████▊ | 793531/900675 [02:12<00:17, 6109.08it/s] 88%|████████▊ | 794146/900675 [02:13<00:19, 5543.89it/s] 88%|████████▊ | 794796/900675 [02:13<00:18, 5803.01it/s] 88%|████████▊ | 795405/900675 [02:13<00:17, 5878.92it/s] 88%|████████▊ | 796033/900675 [02:13<00:17, 5990.60it/s] 88%|████████▊ | 796639/900675 [02:13<00:17, 5983.06it/s] 89%|████████▊ | 797261/900675 [02:13<00:17, 6052.04it/s] 89%|████████▊ | 797870/900675 [02:13<00:17, 5873.68it/s] 89%|████████▊ | 798489/900675 [02:13<00:17, 5958.21it/s] 89%|████████▊ | 799103/900675 [02:13<00:16, 6003.70it/s] 89%|████████▉ | 799706/900675 [02:14<00:17, 5937.29it/s] 89%|████████▉ | 800315/900675 [02:14<00:16, 5976.54it/s] 89%|████████▉ | 800914/900675 [02:14<00:17, 5863.65it/s] 89%|████████▉ | 801502/900675 [02:14<00:17, 5820.13it/s] 89%|████████▉ | 802151/900675 [02:14<00:16, 6005.69it/s] 89%|████████▉ | 802753/900675 [02:14<00:17, 5495.83it/s] 89%|████████▉ | 803449/900675 [02:14<00:16, 5898.08it/s] 89%|████████▉ | 804048/900675 [02:14<00:16, 5748.66it/s] 89%|████████▉ | 804698/900675 [02:14<00:16, 5958.15it/s] 89%|████████▉ | 805329/900675 [02:14<00:15, 6058.31it/s] 89%|████████▉ | 805940/900675 [02:15<00:16, 5791.28it/s] 90%|████████▉ | 806544/900675 [02:15<00:16, 5857.75it/s] 90%|████████▉ | 807162/900675 [02:15<00:15, 5943.33it/s] 90%|████████▉ | 807760/900675 [02:15<00:16, 5756.54it/s] 90%|████████▉ | 808339/900675 [02:15<00:17, 5415.79it/s] 90%|████████▉ | 809022/900675 [02:15<00:15, 5808.58it/s] 90%|████████▉ | 809610/900675 [02:15<00:16, 5651.50it/s] 90%|████████▉ | 810182/900675 [02:15<00:15, 5665.70it/s] 90%|█████████ | 810753/900675 [02:15<00:16, 5364.17it/s] 90%|█████████ | 811295/900675 [02:16<00:16, 5361.56it/s] 90%|█████████ | 811967/900675 [02:16<00:15, 5735.76it/s] 90%|█████████ | 812545/900675 [02:16<00:15, 5566.96it/s] 90%|█████████ | 813220/900675 [02:16<00:14, 5903.13it/s] 90%|█████████ | 813908/900675 [02:16<00:14, 6180.33it/s] 90%|█████████ | 814537/900675 [02:16<00:13, 6209.13it/s] 91%|█████████ | 815246/900675 [02:16<00:13, 6466.97it/s] 91%|█████████ | 815896/900675 [02:16<00:13, 6401.95it/s] 91%|█████████ | 816621/900675 [02:16<00:12, 6648.23it/s] 91%|█████████ | 817314/900675 [02:16<00:12, 6729.96it/s] 91%|█████████ | 818044/900675 [02:17<00:11, 6893.63it/s] 91%|█████████ | 818735/900675 [02:17<00:12, 6399.09it/s] 91%|█████████ | 819383/900675 [02:17<00:13, 6181.47it/s] 91%|█████████ | 820041/900675 [02:17<00:12, 6290.13it/s] 91%|█████████ | 820699/900675 [02:17<00:12, 6371.39it/s] 91%|█████████ | 821359/900675 [02:17<00:12, 6428.93it/s] 91%|█████████▏| 822005/900675 [02:17<00:13, 5805.34it/s] 91%|█████████▏| 822624/900675 [02:17<00:13, 5910.39it/s] 91%|█████████▏| 823271/900675 [02:17<00:12, 6063.43it/s] 91%|█████████▏| 823895/900675 [02:18<00:12, 6113.36it/s] 92%|█████████▏| 824513/900675 [02:18<00:12, 5911.53it/s] 92%|█████████▏| 825110/900675 [02:18<00:13, 5440.53it/s] 92%|█████████▏| 825664/900675 [02:18<00:14, 5255.43it/s] 92%|█████████▏| 826263/900675 [02:18<00:13, 5448.22it/s] 92%|█████████▏| 826855/900675 [02:18<00:13, 5576.36it/s] 92%|█████████▏| 827496/900675 [02:18<00:12, 5811.49it/s] 92%|█████████▏| 828106/900675 [02:18<00:12, 5894.38it/s] 92%|█████████▏| 828700/900675 [02:18<00:12, 5725.01it/s] 92%|█████████▏| 829276/900675 [02:19<00:13, 5146.49it/s] 92%|█████████▏| 829960/900675 [02:19<00:12, 5601.13it/s] 92%|█████████▏| 830534/900675 [02:19<00:12, 5499.11it/s] 92%|█████████▏| 831125/900675 [02:19<00:12, 5612.42it/s] 92%|█████████▏| 831722/900675 [02:19<00:12, 5702.84it/s] 92%|█████████▏| 832298/900675 [02:19<00:12, 5641.89it/s] 92%|█████████▏| 832944/900675 [02:19<00:11, 5877.77it/s] 93%|█████████▎| 833593/900675 [02:19<00:11, 6051.64it/s] 93%|█████████▎| 834202/900675 [02:19<00:11, 5984.90it/s] 93%|█████████▎| 834803/900675 [02:20<00:11, 5961.30it/s] 93%|█████████▎| 835410/900675 [02:20<00:10, 5990.77it/s] 93%|█████████▎| 836022/900675 [02:20<00:10, 6025.89it/s] 93%|█████████▎| 836642/900675 [02:20<00:10, 6073.88it/s] 93%|█████████▎| 837292/900675 [02:20<00:10, 6200.60it/s] 93%|█████████▎| 837913/900675 [02:20<00:10, 6161.67it/s] 93%|█████████▎| 838530/900675 [02:20<00:10, 6115.02it/s] 93%|█████████▎| 839142/900675 [02:20<00:10, 5757.60it/s] 93%|█████████▎| 839769/900675 [02:20<00:10, 5903.60it/s] 93%|█████████▎| 840450/900675 [02:20<00:09, 6165.78it/s] 93%|█████████▎| 841071/900675 [02:21<00:09, 6048.65it/s] 93%|█████████▎| 841679/900675 [02:21<00:09, 6042.83it/s] 94%|█████████▎| 842289/900675 [02:21<00:09, 6052.16it/s] 94%|█████████▎| 842896/900675 [02:21<00:10, 5672.21it/s] 94%|█████████▎| 843469/900675 [02:21<00:10, 5296.32it/s] 94%|█████████▎| 844007/900675 [02:21<00:10, 5260.31it/s] 94%|█████████▍| 844538/900675 [02:21<00:11, 5071.64it/s] 94%|█████████▍| 845097/900675 [02:21<00:10, 5206.21it/s] 94%|█████████▍| 845715/900675 [02:21<00:10, 5480.19it/s] 94%|█████████▍| 846268/900675 [02:22<00:09, 5455.27it/s] 94%|█████████▍| 846898/900675 [02:22<00:09, 5696.70it/s] 94%|█████████▍| 847484/900675 [02:22<00:09, 5738.47it/s] 94%|█████████▍| 848079/900675 [02:22<00:09, 5797.65it/s] 94%|█████████▍| 848690/900675 [02:22<00:08, 5888.52it/s] 94%|█████████▍| 849281/900675 [02:22<00:08, 5860.22it/s] 94%|█████████▍| 849894/900675 [02:22<00:08, 5940.15it/s] 94%|█████████▍| 850489/900675 [02:22<00:08, 5938.56it/s] 94%|█████████▍| 851084/900675 [02:22<00:08, 5545.29it/s] 95%|█████████▍| 851700/900675 [02:22<00:08, 5713.51it/s] 95%|█████████▍| 852313/900675 [02:23<00:08, 5829.38it/s] 95%|█████████▍| 852900/900675 [02:23<00:08, 5505.77it/s] 95%|█████████▍| 853457/900675 [02:23<00:08, 5518.75it/s] 95%|█████████▍| 854079/900675 [02:23<00:08, 5718.67it/s] 95%|█████████▍| 854670/900675 [02:23<00:07, 5767.53it/s] 95%|█████████▍| 855250/900675 [02:23<00:07, 5743.90it/s] 95%|█████████▌| 855892/900675 [02:23<00:07, 5933.73it/s] 95%|█████████▌| 856488/900675 [02:23<00:07, 5919.87it/s] 95%|█████████▌| 857140/900675 [02:23<00:07, 6088.85it/s] 95%|█████████▌| 857750/900675 [02:23<00:07, 5918.23it/s] 95%|█████████▌| 858344/900675 [02:24<00:07, 5795.30it/s] 95%|█████████▌| 858925/900675 [02:24<00:07, 5713.08it/s] 95%|█████████▌| 859549/900675 [02:24<00:07, 5863.05it/s] 95%|█████████▌| 860137/900675 [02:24<00:06, 5839.69it/s] 96%|█████████▌| 860903/900675 [02:24<00:06, 6369.41it/s] 96%|█████████▌| 861542/900675 [02:24<00:06, 6256.58it/s] 96%|█████████▌| 862170/900675 [02:24<00:06, 6026.77it/s] 96%|█████████▌| 862830/900675 [02:24<00:06, 6189.55it/s] 96%|█████████▌| 863460/900675 [02:24<00:05, 6218.94it/s] 96%|█████████▌| 864084/900675 [02:25<00:06, 5925.50it/s] 96%|█████████▌| 864681/900675 [02:25<00:06, 5913.28it/s] 96%|█████████▌| 865293/900675 [02:25<00:05, 5972.42it/s] 96%|█████████▌| 865893/900675 [02:25<00:06, 5460.06it/s] 96%|█████████▌| 866552/900675 [02:25<00:05, 5765.95it/s] 96%|█████████▋| 867138/900675 [02:25<00:06, 5564.74it/s] 96%|█████████▋| 867755/900675 [02:25<00:05, 5733.54it/s] 96%|█████████▋| 868382/900675 [02:25<00:05, 5885.76it/s] 96%|█████████▋| 868976/900675 [02:25<00:05, 5707.65it/s] 97%|█████████▋| 869552/900675 [02:25<00:05, 5685.56it/s] 97%|█████████▋| 870251/900675 [02:26<00:05, 6055.72it/s] 97%|█████████▋| 870869/900675 [02:26<00:04, 6089.30it/s] 97%|█████████▋| 871555/900675 [02:26<00:04, 6313.16it/s] 97%|█████████▋| 872189/900675 [02:26<00:04, 6115.59it/s] 97%|█████████▋| 872804/900675 [02:26<00:04, 5682.33it/s] 97%|█████████▋| 873504/900675 [02:26<00:04, 6035.01it/s] 97%|█████████▋| 874201/900675 [02:26<00:04, 6297.70it/s] 97%|█████████▋| 874838/900675 [02:26<00:04, 6060.44it/s] 97%|█████████▋| 875455/900675 [02:26<00:04, 6089.40it/s] 97%|█████████▋| 876094/900675 [02:27<00:03, 6171.49it/s] 97%|█████████▋| 876734/900675 [02:27<00:03, 6235.90it/s] 97%|█████████▋| 877361/900675 [02:27<00:03, 5907.68it/s] 97%|█████████▋| 877957/900675 [02:27<00:03, 5776.94it/s] 98%|█████████▊| 878557/900675 [02:27<00:03, 5836.25it/s] 98%|█████████▊| 879191/900675 [02:27<00:03, 5974.88it/s] 98%|█████████▊| 879791/900675 [02:27<00:03, 5685.54it/s] 98%|█████████▊| 880374/900675 [02:27<00:03, 5726.31it/s] 98%|█████████▊| 880950/900675 [02:27<00:03, 5628.86it/s] 98%|█████████▊| 881516/900675 [02:27<00:03, 5613.13it/s] 98%|█████████▊| 882079/900675 [02:28<00:03, 5251.72it/s] 98%|█████████▊| 882703/900675 [02:28<00:03, 5524.17it/s] 98%|█████████▊| 883356/900675 [02:28<00:02, 5809.00it/s] 98%|█████████▊| 883943/900675 [02:28<00:02, 5742.25it/s] 98%|█████████▊| 884523/900675 [02:28<00:02, 5750.21it/s] 98%|█████████▊| 885136/900675 [02:28<00:02, 5859.66it/s] 98%|█████████▊| 885760/900675 [02:28<00:02, 5966.80it/s] 98%|█████████▊| 886502/900675 [02:28<00:02, 6393.87it/s] 98%|█████████▊| 887144/900675 [02:28<00:02, 6202.21it/s] 99%|█████████▊| 887767/900675 [02:29<00:02, 5865.57it/s] 99%|█████████▊| 888380/900675 [02:29<00:02, 5939.99it/s] 99%|█████████▊| 888978/900675 [02:29<00:02, 5789.87it/s] 99%|█████████▉| 889645/900675 [02:29<00:01, 6038.35it/s] 99%|█████████▉| 890253/900675 [02:29<00:01, 5941.84it/s] 99%|█████████▉| 890932/900675 [02:29<00:01, 6184.99it/s] 99%|█████████▉| 891554/900675 [02:29<00:01, 5917.41it/s] 99%|█████████▉| 892150/900675 [02:29<00:01, 5759.25it/s] 99%|█████████▉| 892742/900675 [02:29<00:01, 5800.95it/s] 99%|█████████▉| 893325/900675 [02:30<00:01, 5573.00it/s] 99%|█████████▉| 893886/900675 [02:30<00:01, 5436.99it/s] 99%|█████████▉| 894432/900675 [02:30<00:01, 5439.49it/s] 99%|█████████▉| 894980/900675 [02:30<00:01, 5439.72it/s] 99%|█████████▉| 895589/900675 [02:30<00:00, 5622.36it/s]100%|█████████▉| 896278/900675 [02:30<00:00, 5988.80it/s]100%|█████████▉| 896879/900675 [02:30<00:00, 5833.45it/s]100%|█████████▉| 897465/900675 [02:30<00:00, 5834.14it/s]100%|█████████▉| 898148/900675 [02:30<00:00, 6123.90it/s]100%|█████████▉| 898784/900675 [02:30<00:00, 6188.89it/s]100%|█████████▉| 899419/900675 [02:31<00:00, 6234.37it/s]100%|█████████▉| 900044/900675 [02:31<00:00, 6168.75it/s]100%|█████████▉| 900662/900675 [02:31<00:00, 6109.05it/s]100%|██████████| 900675/900675 [02:31<00:00, 5954.95it/s]

gathering stats for n=1
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 1916/900675 [00:00<00:46, 19145.55it/s]  0%|          | 3985/900675 [00:00<00:44, 20052.51it/s]  1%|          | 6231/900675 [00:00<00:42, 21139.00it/s]  1%|          | 8345/900675 [00:00<00:43, 20391.28it/s]  1%|          | 10388/900675 [00:00<00:43, 20286.07it/s]  1%|▏         | 12419/900675 [00:00<00:43, 20220.25it/s]  2%|▏         | 14473/900675 [00:00<00:43, 20314.93it/s]  2%|▏         | 16506/900675 [00:00<00:43, 20200.95it/s]  2%|▏         | 18527/900675 [00:00<00:43, 20145.41it/s]  2%|▏         | 20737/900675 [00:01<00:42, 20735.15it/s]  3%|▎         | 22812/900675 [00:01<00:42, 20577.69it/s]  3%|▎         | 25139/900675 [00:01<00:40, 21385.17it/s]  3%|▎         | 27279/900675 [00:01<00:42, 20562.51it/s]  3%|▎         | 29343/900675 [00:01<00:42, 20534.27it/s]  3%|▎         | 31402/900675 [00:01<00:43, 20180.14it/s]  4%|▎         | 33425/900675 [00:01<00:43, 19794.87it/s]  4%|▍         | 35504/900675 [00:01<00:43, 20083.28it/s]  4%|▍         | 37516/900675 [00:01<00:43, 19760.86it/s]  4%|▍         | 39495/900675 [00:01<00:43, 19755.00it/s]  5%|▍         | 41580/900675 [00:02<00:42, 20068.48it/s]  5%|▍         | 43589/900675 [00:02<00:43, 19714.31it/s]  5%|▌         | 45673/900675 [00:02<00:42, 20043.60it/s]  5%|▌         | 48085/900675 [00:02<00:40, 21242.31it/s]  6%|▌         | 50213/900675 [00:02<00:40, 21240.16it/s]  6%|▌         | 52449/900675 [00:02<00:39, 21568.19it/s]  6%|▌         | 54608/900675 [00:02<00:40, 20738.34it/s]  6%|▋         | 56696/900675 [00:02<00:40, 20779.22it/s]  7%|▋         | 58780/900675 [00:02<00:40, 20658.42it/s]  7%|▋         | 60850/900675 [00:02<00:41, 20437.33it/s]  7%|▋         | 62946/900675 [00:03<00:40, 20589.18it/s]  7%|▋         | 65008/900675 [00:03<00:40, 20415.44it/s]  7%|▋         | 67056/900675 [00:03<00:40, 20432.21it/s]  8%|▊         | 69101/900675 [00:03<00:40, 20400.07it/s]  8%|▊         | 71176/900675 [00:03<00:40, 20503.48it/s]  8%|▊         | 73437/900675 [00:03<00:39, 21128.21it/s]  8%|▊         | 75625/900675 [00:03<00:38, 21351.95it/s]  9%|▊         | 77763/900675 [00:03<00:38, 21360.17it/s]  9%|▉         | 79900/900675 [00:03<00:40, 20357.92it/s]  9%|▉         | 81975/900675 [00:03<00:39, 20468.77it/s]  9%|▉         | 84106/900675 [00:04<00:39, 20713.71it/s] 10%|▉         | 86184/900675 [00:04<00:39, 20529.61it/s] 10%|▉         | 88377/900675 [00:04<00:38, 20938.51it/s] 10%|█         | 90475/900675 [00:04<00:40, 20108.20it/s] 10%|█         | 92609/900675 [00:04<00:39, 20458.93it/s] 11%|█         | 94663/900675 [00:04<00:39, 20471.90it/s] 11%|█         | 96901/900675 [00:04<00:38, 21027.79it/s] 11%|█         | 99009/900675 [00:04<00:38, 20618.54it/s] 11%|█         | 101076/900675 [00:04<00:39, 20240.85it/s] 11%|█▏        | 103109/900675 [00:05<00:39, 20261.93it/s] 12%|█▏        | 105139/900675 [00:05<00:39, 20046.50it/s] 12%|█▏        | 107146/900675 [00:05<00:39, 20048.82it/s] 12%|█▏        | 109153/900675 [00:05<00:39, 19833.47it/s] 12%|█▏        | 111268/900675 [00:05<00:39, 20214.50it/s] 13%|█▎        | 113292/900675 [00:05<00:39, 20116.79it/s] 13%|█▎        | 115313/900675 [00:05<00:39, 20135.08it/s] 13%|█▎        | 117328/900675 [00:05<00:39, 19596.40it/s] 13%|█▎        | 119353/900675 [00:05<00:39, 19784.37it/s] 13%|█▎        | 121475/900675 [00:05<00:38, 20202.39it/s] 14%|█▎        | 123498/900675 [00:06<00:38, 20055.40it/s] 14%|█▍        | 125506/900675 [00:06<00:38, 20024.75it/s] 14%|█▍        | 127636/900675 [00:06<00:37, 20398.34it/s] 14%|█▍        | 129693/900675 [00:06<00:37, 20442.85it/s] 15%|█▍        | 131739/900675 [00:06<00:38, 19723.72it/s] 15%|█▍        | 133863/900675 [00:06<00:38, 20165.21it/s] 15%|█▌        | 136148/900675 [00:06<00:36, 20953.30it/s] 15%|█▌        | 138308/900675 [00:06<00:36, 21143.50it/s] 16%|█▌        | 140427/900675 [00:06<00:36, 20897.97it/s] 16%|█▌        | 142521/900675 [00:06<00:36, 20740.62it/s] 16%|█▌        | 144643/900675 [00:07<00:36, 20874.74it/s] 16%|█▋        | 146735/900675 [00:07<00:36, 20887.70it/s] 17%|█▋        | 148826/900675 [00:07<00:36, 20447.17it/s] 17%|█▋        | 150874/900675 [00:07<00:37, 20084.80it/s] 17%|█▋        | 152886/900675 [00:07<00:37, 20019.01it/s] 17%|█▋        | 154933/900675 [00:07<00:37, 20147.07it/s] 17%|█▋        | 157065/900675 [00:07<00:36, 20493.08it/s] 18%|█▊        | 159116/900675 [00:07<00:36, 20441.56it/s] 18%|█▊        | 161280/900675 [00:07<00:35, 20795.65it/s] 18%|█▊        | 163369/900675 [00:07<00:35, 20818.00it/s] 18%|█▊        | 165452/900675 [00:08<00:35, 20772.25it/s] 19%|█▊        | 167610/900675 [00:08<00:34, 21012.48it/s] 19%|█▉        | 169712/900675 [00:08<00:35, 20558.09it/s] 19%|█▉        | 171771/900675 [00:08<00:36, 20152.61it/s] 19%|█▉        | 173790/900675 [00:08<00:36, 20027.59it/s] 20%|█▉        | 175795/900675 [00:08<00:36, 19791.26it/s] 20%|█▉        | 177776/900675 [00:08<00:37, 19244.81it/s] 20%|█▉        | 179866/900675 [00:08<00:36, 19722.78it/s] 20%|██        | 181843/900675 [00:08<00:36, 19604.10it/s] 20%|██        | 183807/900675 [00:09<00:37, 19333.48it/s] 21%|██        | 185743/900675 [00:09<00:37, 19312.77it/s] 21%|██        | 187747/900675 [00:09<00:36, 19521.86it/s] 21%|██        | 189805/900675 [00:09<00:35, 19825.66it/s] 21%|██▏       | 191870/900675 [00:09<00:35, 20070.03it/s] 22%|██▏       | 193906/900675 [00:09<00:35, 20151.29it/s] 22%|██▏       | 195961/900675 [00:09<00:34, 20265.05it/s] 22%|██▏       | 198139/900675 [00:09<00:33, 20717.07it/s] 22%|██▏       | 200212/900675 [00:09<00:34, 20428.48it/s] 22%|██▏       | 202257/900675 [00:09<00:34, 20146.72it/s] 23%|██▎       | 204316/900675 [00:10<00:34, 20270.55it/s] 23%|██▎       | 206345/900675 [00:10<00:34, 20220.67it/s] 23%|██▎       | 208368/900675 [00:10<00:34, 19851.25it/s] 23%|██▎       | 210355/900675 [00:10<00:34, 19816.36it/s] 24%|██▎       | 212406/900675 [00:10<00:34, 20016.39it/s] 24%|██▍       | 214409/900675 [00:10<00:34, 19935.95it/s] 24%|██▍       | 216404/900675 [00:10<00:34, 19614.12it/s] 24%|██▍       | 218470/900675 [00:10<00:34, 19916.80it/s] 24%|██▍       | 220464/900675 [00:10<00:34, 19809.16it/s] 25%|██▍       | 222447/900675 [00:10<00:34, 19451.84it/s] 25%|██▍       | 224451/900675 [00:11<00:34, 19618.20it/s] 25%|██▌       | 226622/900675 [00:11<00:33, 20229.28it/s] 25%|██▌       | 228648/900675 [00:11<00:34, 19750.24it/s] 26%|██▌       | 230627/900675 [00:11<00:34, 19563.86it/s] 26%|██▌       | 232586/900675 [00:11<00:34, 19565.44it/s] 26%|██▌       | 234822/900675 [00:11<00:32, 20384.43it/s] 26%|██▋       | 236864/900675 [00:11<00:32, 20153.26it/s] 27%|██▋       | 238970/900675 [00:11<00:32, 20418.21it/s] 27%|██▋       | 241117/900675 [00:11<00:31, 20728.99it/s] 27%|██▋       | 243192/900675 [00:11<00:31, 20638.37it/s] 27%|██▋       | 245258/900675 [00:12<00:31, 20486.09it/s] 27%|██▋       | 247517/900675 [00:12<00:30, 21105.35it/s] 28%|██▊       | 249694/900675 [00:12<00:30, 21302.60it/s] 28%|██▊       | 251826/900675 [00:12<00:30, 21056.42it/s] 28%|██▊       | 253933/900675 [00:12<00:30, 20971.88it/s] 28%|██▊       | 256032/900675 [00:12<00:30, 20851.73it/s] 29%|██▊       | 258118/900675 [00:12<00:30, 20772.13it/s] 29%|██▉       | 260196/900675 [00:12<00:31, 20145.54it/s] 29%|██▉       | 262264/900675 [00:12<00:31, 20299.65it/s] 29%|██▉       | 264298/900675 [00:13<00:31, 19899.82it/s] 30%|██▉       | 266292/900675 [00:13<00:31, 19833.59it/s] 30%|██▉       | 268278/900675 [00:13<00:32, 19753.48it/s] 30%|███       | 270368/900675 [00:13<00:31, 20090.25it/s] 30%|███       | 272530/900675 [00:13<00:30, 20542.80it/s] 30%|███       | 274626/900675 [00:13<00:30, 20660.70it/s] 31%|███       | 276694/900675 [00:13<00:31, 19942.66it/s] 31%|███       | 278791/900675 [00:13<00:30, 20241.34it/s] 31%|███       | 280821/900675 [00:13<00:31, 19928.69it/s] 31%|███▏      | 282818/900675 [00:13<00:30, 19940.21it/s] 32%|███▏      | 284901/900675 [00:14<00:30, 20201.09it/s] 32%|███▏      | 286924/900675 [00:14<00:30, 20054.33it/s] 32%|███▏      | 289016/900675 [00:14<00:30, 20303.43it/s] 32%|███▏      | 291048/900675 [00:14<00:30, 19822.73it/s] 33%|███▎      | 293034/900675 [00:14<00:30, 19702.41it/s] 33%|███▎      | 295152/900675 [00:14<00:30, 20135.56it/s] 33%|███▎      | 297169/900675 [00:14<00:30, 20088.01it/s] 33%|███▎      | 299193/900675 [00:14<00:29, 20130.80it/s] 33%|███▎      | 301234/900675 [00:14<00:29, 20213.41it/s] 34%|███▎      | 303262/900675 [00:14<00:29, 20229.59it/s] 34%|███▍      | 305286/900675 [00:15<00:29, 20179.68it/s] 34%|███▍      | 307305/900675 [00:15<00:29, 20141.26it/s] 34%|███▍      | 309343/900675 [00:15<00:29, 20210.72it/s] 35%|███▍      | 311425/900675 [00:15<00:28, 20389.43it/s] 35%|███▍      | 313606/900675 [00:15<00:28, 20807.57it/s] 35%|███▌      | 315688/900675 [00:15<00:28, 20556.88it/s] 35%|███▌      | 317745/900675 [00:15<00:29, 19948.37it/s] 36%|███▌      | 319828/900675 [00:15<00:28, 20200.66it/s] 36%|███▌      | 321852/900675 [00:15<00:28, 20119.47it/s] 36%|███▌      | 323867/900675 [00:15<00:28, 20011.13it/s] 36%|███▌      | 325870/900675 [00:16<00:29, 19758.93it/s] 36%|███▋      | 328214/900675 [00:16<00:27, 20839.52it/s] 37%|███▋      | 330302/900675 [00:16<00:28, 20254.06it/s] 37%|███▋      | 332333/900675 [00:16<00:28, 20159.52it/s] 37%|███▋      | 334353/900675 [00:16<00:28, 20088.05it/s] 37%|███▋      | 336437/900675 [00:16<00:27, 20308.55it/s] 38%|███▊      | 338577/900675 [00:16<00:27, 20630.83it/s] 38%|███▊      | 340643/900675 [00:16<00:27, 20086.94it/s] 38%|███▊      | 342735/900675 [00:16<00:27, 20322.68it/s] 38%|███▊      | 344817/900675 [00:16<00:27, 20460.79it/s] 39%|███▊      | 346866/900675 [00:17<00:27, 20202.06it/s] 39%|███▊      | 348889/900675 [00:17<00:27, 20105.99it/s] 39%|███▉      | 351087/900675 [00:17<00:26, 20652.57it/s] 39%|███▉      | 353155/900675 [00:17<00:27, 20078.87it/s] 39%|███▉      | 355168/900675 [00:17<00:27, 19966.40it/s] 40%|███▉      | 357168/900675 [00:17<00:27, 19807.37it/s] 40%|███▉      | 359162/900675 [00:17<00:27, 19841.52it/s] 40%|████      | 361318/900675 [00:17<00:26, 20345.87it/s] 40%|████      | 363446/900675 [00:17<00:26, 20622.19it/s] 41%|████      | 365629/900675 [00:18<00:25, 20979.17it/s] 41%|████      | 367729/900675 [00:18<00:25, 20547.57it/s] 41%|████      | 369787/900675 [00:18<00:25, 20522.73it/s] 41%|████▏     | 371842/900675 [00:18<00:26, 20127.26it/s] 42%|████▏     | 373858/900675 [00:18<00:26, 20118.01it/s] 42%|████▏     | 375872/900675 [00:18<00:26, 20056.89it/s] 42%|████▏     | 377879/900675 [00:18<00:26, 20012.07it/s] 42%|████▏     | 380011/900675 [00:18<00:25, 20397.44it/s] 42%|████▏     | 382052/900675 [00:18<00:25, 20347.28it/s] 43%|████▎     | 384088/900675 [00:18<00:25, 20117.58it/s] 43%|████▎     | 386133/900675 [00:19<00:25, 20213.72it/s] 43%|████▎     | 388334/900675 [00:19<00:24, 20740.46it/s] 43%|████▎     | 390410/900675 [00:19<00:24, 20518.60it/s] 44%|████▎     | 392463/900675 [00:19<00:25, 20099.03it/s] 44%|████▍     | 394476/900675 [00:19<00:25, 19835.46it/s] 44%|████▍     | 396468/900675 [00:19<00:25, 19858.80it/s] 44%|████▍     | 398502/900675 [00:19<00:25, 19999.19it/s] 44%|████▍     | 400697/900675 [00:19<00:24, 20571.38it/s] 45%|████▍     | 402756/900675 [00:19<00:25, 19644.72it/s] 45%|████▍     | 404811/900675 [00:19<00:24, 19896.90it/s] 45%|████▌     | 406809/900675 [00:20<00:25, 19535.67it/s] 45%|████▌     | 408831/900675 [00:20<00:24, 19730.62it/s] 46%|████▌     | 410825/900675 [00:20<00:24, 19790.09it/s] 46%|████▌     | 412808/900675 [00:20<00:25, 19078.97it/s] 46%|████▌     | 414723/900675 [00:20<00:25, 18752.81it/s] 46%|████▋     | 416675/900675 [00:20<00:25, 18973.71it/s] 46%|████▋     | 418659/900675 [00:20<00:25, 19223.21it/s] 47%|████▋     | 420735/900675 [00:20<00:24, 19670.02it/s] 47%|████▋     | 422706/900675 [00:20<00:24, 19635.26it/s] 47%|████▋     | 424672/900675 [00:20<00:24, 19474.21it/s] 47%|████▋     | 426820/900675 [00:21<00:23, 20066.92it/s] 48%|████▊     | 428866/900675 [00:21<00:23, 20169.66it/s] 48%|████▊     | 430900/900675 [00:21<00:23, 20219.81it/s] 48%|████▊     | 433063/900675 [00:21<00:22, 20636.71it/s] 48%|████▊     | 435128/900675 [00:21<00:23, 19996.37it/s] 49%|████▊     | 437133/900675 [00:21<00:23, 20007.05it/s] 49%|████▉     | 439138/900675 [00:21<00:23, 19959.59it/s] 49%|████▉     | 441137/900675 [00:21<00:23, 19622.44it/s] 49%|████▉     | 443233/900675 [00:21<00:22, 20011.22it/s] 49%|████▉     | 445237/900675 [00:22<00:23, 19582.42it/s] 50%|████▉     | 447268/900675 [00:22<00:22, 19790.94it/s] 50%|████▉     | 449320/900675 [00:22<00:22, 20004.89it/s] 50%|█████     | 451354/900675 [00:22<00:22, 20102.78it/s] 50%|█████     | 453367/900675 [00:22<00:22, 19868.48it/s] 51%|█████     | 455356/900675 [00:22<00:22, 19548.22it/s] 51%|█████     | 457549/900675 [00:22<00:21, 20241.49it/s] 51%|█████     | 459577/900675 [00:22<00:21, 20227.62it/s] 51%|█████▏    | 461602/900675 [00:22<00:21, 19971.62it/s] 51%|█████▏    | 463602/900675 [00:22<00:22, 19824.15it/s] 52%|█████▏    | 465586/900675 [00:23<00:21, 19805.21it/s] 52%|█████▏    | 467630/900675 [00:23<00:21, 19992.16it/s] 52%|█████▏    | 469631/900675 [00:23<00:21, 19914.61it/s] 52%|█████▏    | 471693/900675 [00:23<00:21, 20122.01it/s] 53%|█████▎    | 473706/900675 [00:23<00:21, 20059.35it/s] 53%|█████▎    | 475890/900675 [00:23<00:20, 20589.62it/s] 53%|█████▎    | 477950/900675 [00:23<00:21, 19986.43it/s] 53%|█████▎    | 480082/900675 [00:23<00:20, 20362.16it/s] 54%|█████▎    | 482122/900675 [00:23<00:20, 20076.76it/s] 54%|█████▍    | 484154/900675 [00:23<00:20, 20147.56it/s] 54%|█████▍    | 486314/900675 [00:24<00:20, 20573.88it/s] 54%|█████▍    | 488374/900675 [00:24<00:20, 20468.21it/s] 54%|█████▍    | 490423/900675 [00:24<00:20, 19962.66it/s] 55%|█████▍    | 492479/900675 [00:24<00:20, 20130.44it/s] 55%|█████▍    | 494563/900675 [00:24<00:19, 20338.93it/s] 55%|█████▌    | 496600/900675 [00:24<00:19, 20276.61it/s] 55%|█████▌    | 498630/900675 [00:24<00:19, 20217.47it/s] 56%|█████▌    | 500734/900675 [00:24<00:19, 20461.18it/s] 56%|█████▌    | 502782/900675 [00:24<00:19, 20122.94it/s] 56%|█████▌    | 504797/900675 [00:24<00:19, 19865.75it/s] 56%|█████▋    | 506946/900675 [00:25<00:19, 20336.47it/s] 57%|█████▋    | 508982/900675 [00:25<00:19, 19852.25it/s] 57%|█████▋    | 511063/900675 [00:25<00:19, 20128.41it/s] 57%|█████▋    | 513215/900675 [00:25<00:18, 20536.14it/s] 57%|█████▋    | 515272/900675 [00:25<00:19, 20070.34it/s] 57%|█████▋    | 517283/900675 [00:25<00:19, 19973.28it/s] 58%|█████▊    | 519303/900675 [00:25<00:19, 20038.53it/s] 58%|█████▊    | 521309/900675 [00:25<00:18, 20028.52it/s] 58%|█████▊    | 523314/900675 [00:25<00:18, 19962.83it/s] 58%|█████▊    | 525312/900675 [00:26<00:18, 19843.44it/s] 59%|█████▊    | 527392/900675 [00:26<00:18, 20126.06it/s] 59%|█████▉    | 529406/900675 [00:26<00:18, 20110.34it/s] 59%|█████▉    | 531418/900675 [00:26<00:18, 19789.74it/s] 59%|█████▉    | 533476/900675 [00:26<00:18, 20021.69it/s] 59%|█████▉    | 535480/900675 [00:26<00:18, 19658.88it/s] 60%|█████▉    | 537448/900675 [00:26<00:18, 19484.58it/s] 60%|█████▉    | 539550/900675 [00:26<00:18, 19927.98it/s] 60%|██████    | 541545/900675 [00:26<00:18, 19826.24it/s] 60%|██████    | 543596/900675 [00:26<00:17, 20023.35it/s] 61%|██████    | 545600/900675 [00:27<00:17, 19915.31it/s] 61%|██████    | 547593/900675 [00:27<00:17, 19827.34it/s] 61%|██████    | 549672/900675 [00:27<00:17, 20106.65it/s] 61%|██████▏   | 551712/900675 [00:27<00:17, 20186.31it/s] 61%|██████▏   | 553732/900675 [00:27<00:17, 19875.56it/s] 62%|██████▏   | 555792/900675 [00:27<00:17, 20086.77it/s] 62%|██████▏   | 557802/900675 [00:27<00:17, 19903.76it/s] 62%|██████▏   | 559794/900675 [00:27<00:17, 19553.35it/s] 62%|██████▏   | 561827/900675 [00:27<00:17, 19774.94it/s] 63%|██████▎   | 563845/900675 [00:27<00:16, 19893.72it/s] 63%|██████▎   | 565843/900675 [00:28<00:16, 19918.77it/s] 63%|██████▎   | 567836/900675 [00:28<00:16, 19818.22it/s] 63%|██████▎   | 569924/900675 [00:28<00:16, 20133.55it/s] 64%|██████▎   | 572131/900675 [00:28<00:15, 20705.48it/s] 64%|██████▍   | 574203/900675 [00:28<00:16, 20272.16it/s] 64%|██████▍   | 576233/900675 [00:28<00:16, 20050.73it/s] 64%|██████▍   | 578240/900675 [00:28<00:16, 19663.32it/s] 64%|██████▍   | 580317/900675 [00:28<00:16, 19985.38it/s] 65%|██████▍   | 582372/900675 [00:28<00:15, 20150.83it/s] 65%|██████▍   | 584716/900675 [00:28<00:14, 21123.01it/s] 65%|██████▌   | 586832/900675 [00:29<00:15, 20834.23it/s] 65%|██████▌   | 588919/900675 [00:29<00:15, 20200.90it/s] 66%|██████▌   | 591004/900675 [00:29<00:15, 20384.44it/s] 66%|██████▌   | 593047/900675 [00:29<00:15, 20016.53it/s] 66%|██████▌   | 595128/900675 [00:29<00:15, 20242.99it/s] 66%|██████▋   | 597156/900675 [00:29<00:15, 20144.92it/s] 67%|██████▋   | 599173/900675 [00:29<00:15, 20005.52it/s] 67%|██████▋   | 601176/900675 [00:29<00:15, 19829.50it/s] 67%|██████▋   | 603161/900675 [00:29<00:15, 19438.18it/s] 67%|██████▋   | 605223/900675 [00:29<00:14, 19777.65it/s] 67%|██████▋   | 607251/900675 [00:30<00:14, 19921.09it/s] 68%|██████▊   | 609245/900675 [00:30<00:14, 19599.46it/s] 68%|██████▊   | 611404/900675 [00:30<00:14, 20181.27it/s] 68%|██████▊   | 613431/900675 [00:30<00:14, 20201.28it/s] 68%|██████▊   | 615454/900675 [00:30<00:14, 19831.95it/s] 69%|██████▊   | 617669/900675 [00:30<00:13, 20508.92it/s] 69%|██████▉   | 620008/900675 [00:30<00:13, 21356.05it/s] 69%|██████▉   | 622148/900675 [00:30<00:13, 21051.95it/s] 69%|██████▉   | 624257/900675 [00:30<00:13, 20457.54it/s] 70%|██████▉   | 626452/900675 [00:31<00:13, 20883.08it/s] 70%|██████▉   | 628546/900675 [00:31<00:13, 20400.20it/s] 70%|███████   | 630591/900675 [00:31<00:13, 20373.41it/s] 70%|███████   | 632856/900675 [00:31<00:12, 21037.81it/s] 70%|███████   | 634964/900675 [00:31<00:12, 20863.27it/s] 71%|███████   | 637054/900675 [00:31<00:12, 20606.79it/s] 71%|███████   | 639169/900675 [00:31<00:12, 20762.26it/s] 71%|███████   | 641371/900675 [00:31<00:12, 21128.87it/s] 71%|███████▏  | 643486/900675 [00:31<00:12, 20442.66it/s] 72%|███████▏  | 645536/900675 [00:31<00:12, 20371.99it/s] 72%|███████▏  | 647577/900675 [00:32<00:12, 20090.03it/s] 72%|███████▏  | 649589/900675 [00:32<00:13, 19276.72it/s] 72%|███████▏  | 651628/900675 [00:32<00:12, 19590.98it/s] 73%|███████▎  | 653643/900675 [00:32<00:12, 19751.80it/s] 73%|███████▎  | 655679/900675 [00:32<00:12, 19921.27it/s] 73%|███████▎  | 657764/900675 [00:32<00:12, 20194.81it/s] 73%|███████▎  | 659787/900675 [00:32<00:11, 20167.98it/s] 73%|███████▎  | 661862/900675 [00:32<00:11, 20340.14it/s] 74%|███████▎  | 663898/900675 [00:32<00:11, 20068.09it/s] 74%|███████▍  | 666015/900675 [00:32<00:11, 20391.73it/s] 74%|███████▍  | 668056/900675 [00:33<00:11, 20285.01it/s] 74%|███████▍  | 670194/900675 [00:33<00:11, 20605.96it/s] 75%|███████▍  | 672288/900675 [00:33<00:11, 20700.94it/s] 75%|███████▍  | 674360/900675 [00:33<00:10, 20680.32it/s] 75%|███████▌  | 676429/900675 [00:33<00:10, 20615.03it/s] 75%|███████▌  | 678491/900675 [00:33<00:10, 20324.52it/s] 76%|███████▌  | 680606/900675 [00:33<00:10, 20568.13it/s] 76%|███████▌  | 682664/900675 [00:33<00:10, 20281.04it/s] 76%|███████▌  | 684873/900675 [00:33<00:10, 20812.97it/s] 76%|███████▋  | 686957/900675 [00:33<00:10, 20320.98it/s] 76%|███████▋  | 688993/900675 [00:34<00:10, 20228.54it/s] 77%|███████▋  | 691019/900675 [00:34<00:10, 20012.65it/s] 77%|███████▋  | 693022/900675 [00:34<00:10, 19508.69it/s] 77%|███████▋  | 695036/900675 [00:34<00:10, 19690.38it/s] 77%|███████▋  | 697008/900675 [00:34<00:10, 19670.22it/s] 78%|███████▊  | 698982/900675 [00:34<00:10, 19686.36it/s] 78%|███████▊  | 701084/900675 [00:34<00:09, 20080.31it/s] 78%|███████▊  | 703285/900675 [00:34<00:09, 20653.58it/s] 78%|███████▊  | 705352/900675 [00:34<00:09, 20115.72it/s] 79%|███████▊  | 707368/900675 [00:35<00:09, 19687.65it/s] 79%|███████▉  | 709555/900675 [00:35<00:09, 20320.70it/s] 79%|███████▉  | 711598/900675 [00:35<00:09, 20347.70it/s] 79%|███████▉  | 713711/900675 [00:35<00:09, 20575.01it/s] 79%|███████▉  | 715772/900675 [00:35<00:09, 20417.44it/s] 80%|███████▉  | 717816/900675 [00:35<00:08, 20333.97it/s] 80%|███████▉  | 719851/900675 [00:35<00:09, 19830.83it/s] 80%|████████  | 721869/900675 [00:35<00:08, 19925.06it/s] 80%|████████  | 723951/900675 [00:35<00:08, 20185.80it/s] 81%|████████  | 725972/900675 [00:35<00:08, 19841.86it/s] 81%|████████  | 728094/900675 [00:36<00:08, 20236.86it/s] 81%|████████  | 730121/900675 [00:36<00:08, 19684.99it/s] 81%|████████▏ | 732129/900675 [00:36<00:08, 19796.74it/s] 82%|████████▏ | 734358/900675 [00:36<00:08, 20526.09it/s] 82%|████████▏ | 736415/900675 [00:36<00:08, 20098.65it/s] 82%|████████▏ | 738619/900675 [00:36<00:07, 20665.75it/s] 82%|████████▏ | 740691/900675 [00:36<00:07, 20473.72it/s] 82%|████████▏ | 742742/900675 [00:36<00:07, 20168.19it/s] 83%|████████▎ | 744762/900675 [00:36<00:07, 19998.64it/s] 83%|████████▎ | 746768/900675 [00:36<00:07, 20014.66it/s] 83%|████████▎ | 748771/900675 [00:37<00:07, 19596.86it/s] 83%|████████▎ | 750886/900675 [00:37<00:07, 20048.45it/s] 84%|████████▎ | 752894/900675 [00:37<00:07, 19976.93it/s] 84%|████████▍ | 754894/900675 [00:37<00:07, 19802.73it/s] 84%|████████▍ | 756876/900675 [00:37<00:07, 19438.24it/s] 84%|████████▍ | 758936/900675 [00:37<00:07, 19775.34it/s] 85%|████████▍ | 761079/900675 [00:37<00:06, 20260.52it/s] 85%|████████▍ | 763108/900675 [00:37<00:06, 19729.07it/s] 85%|████████▍ | 765159/900675 [00:37<00:06, 19950.88it/s] 85%|████████▌ | 767219/900675 [00:38<00:06, 20137.56it/s] 85%|████████▌ | 769236/900675 [00:38<00:06, 19692.43it/s] 86%|████████▌ | 771209/900675 [00:38<00:06, 19324.41it/s] 86%|████████▌ | 773297/900675 [00:38<00:06, 19768.82it/s] 86%|████████▌ | 775314/900675 [00:38<00:06, 19883.51it/s] 86%|████████▋ | 777322/900675 [00:38<00:06, 19940.47it/s] 87%|████████▋ | 779319/900675 [00:38<00:06, 19875.80it/s] 87%|████████▋ | 781309/900675 [00:38<00:06, 19747.76it/s] 87%|████████▋ | 783285/900675 [00:38<00:05, 19641.38it/s] 87%|████████▋ | 785250/900675 [00:38<00:05, 19497.48it/s] 87%|████████▋ | 787312/900675 [00:39<00:05, 19829.43it/s] 88%|████████▊ | 789296/900675 [00:39<00:05, 19774.42it/s] 88%|████████▊ | 791362/900675 [00:39<00:05, 20034.30it/s] 88%|████████▊ | 793367/900675 [00:39<00:05, 19915.76it/s] 88%|████████▊ | 795360/900675 [00:39<00:05, 19823.50it/s] 89%|████████▊ | 797415/900675 [00:39<00:05, 20039.14it/s] 89%|████████▉ | 799420/900675 [00:39<00:05, 19996.15it/s] 89%|████████▉ | 801420/900675 [00:39<00:04, 19888.02it/s] 89%|████████▉ | 803425/900675 [00:39<00:04, 19923.11it/s] 89%|████████▉ | 805445/900675 [00:39<00:04, 19996.65it/s] 90%|████████▉ | 807445/900675 [00:40<00:04, 19861.61it/s] 90%|████████▉ | 809432/900675 [00:40<00:04, 19486.79it/s] 90%|█████████ | 811383/900675 [00:40<00:04, 19028.10it/s] 90%|█████████ | 813488/900675 [00:40<00:04, 19608.56it/s] 91%|█████████ | 815701/900675 [00:40<00:04, 20342.70it/s] 91%|█████████ | 818044/900675 [00:40<00:03, 21252.49it/s] 91%|█████████ | 820174/900675 [00:40<00:03, 20990.05it/s] 91%|█████████▏| 822277/900675 [00:40<00:03, 20469.95it/s] 92%|█████████▏| 824381/900675 [00:40<00:03, 20634.69it/s] 92%|█████████▏| 826449/900675 [00:40<00:03, 19778.78it/s] 92%|█████████▏| 828540/900675 [00:41<00:03, 20100.42it/s] 92%|█████████▏| 830558/900675 [00:41<00:03, 19270.12it/s] 92%|█████████▏| 832496/900675 [00:41<00:03, 19271.61it/s] 93%|█████████▎| 834603/900675 [00:41<00:03, 19787.62it/s] 93%|█████████▎| 836665/900675 [00:41<00:03, 20026.44it/s] 93%|█████████▎| 838709/900675 [00:41<00:03, 20145.91it/s] 93%|█████████▎| 840824/900675 [00:41<00:02, 20442.33it/s] 94%|█████████▎| 842872/900675 [00:41<00:02, 20032.33it/s] 94%|█████████▍| 844880/900675 [00:41<00:02, 18920.19it/s] 94%|█████████▍| 846851/900675 [00:42<00:02, 19144.00it/s] 94%|█████████▍| 848821/900675 [00:42<00:02, 19300.75it/s] 94%|█████████▍| 850808/900675 [00:42<00:02, 19458.28it/s] 95%|█████████▍| 852761/900675 [00:42<00:02, 19129.39it/s] 95%|█████████▍| 854765/900675 [00:42<00:02, 19393.20it/s] 95%|█████████▌| 856766/900675 [00:42<00:02, 19573.03it/s] 95%|█████████▌| 858735/900675 [00:42<00:02, 19604.19it/s] 96%|█████████▌| 860910/900675 [00:42<00:01, 20241.55it/s] 96%|█████████▌| 863011/900675 [00:42<00:01, 20469.67it/s] 96%|█████████▌| 865060/900675 [00:42<00:01, 20136.80it/s] 96%|█████████▋| 867076/900675 [00:43<00:01, 19991.94it/s] 96%|█████████▋| 869081/900675 [00:43<00:01, 20001.68it/s] 97%|█████████▋| 871280/900675 [00:43<00:01, 20588.76it/s] 97%|█████████▋| 873341/900675 [00:43<00:01, 20224.72it/s] 97%|█████████▋| 875385/900675 [00:43<00:01, 20284.55it/s] 97%|█████████▋| 877416/900675 [00:43<00:01, 20251.23it/s] 98%|█████████▊| 879443/900675 [00:43<00:01, 20070.80it/s] 98%|█████████▊| 881452/900675 [00:43<00:00, 19715.32it/s] 98%|█████████▊| 883426/900675 [00:43<00:00, 19549.13it/s] 98%|█████████▊| 885409/900675 [00:43<00:00, 19631.10it/s] 99%|█████████▊| 887499/900675 [00:44<00:00, 20003.87it/s] 99%|█████████▉| 889501/900675 [00:44<00:00, 19958.17it/s] 99%|█████████▉| 891527/900675 [00:44<00:00, 20041.21it/s] 99%|█████████▉| 893532/900675 [00:44<00:00, 19763.06it/s] 99%|█████████▉| 895510/900675 [00:44<00:00, 19277.79it/s]100%|█████████▉| 897564/900675 [00:44<00:00, 19645.68it/s]100%|█████████▉| 899681/900675 [00:44<00:00, 20093.27it/s]100%|██████████| 900675/900675 [00:44<00:00, 20137.90it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 15.30it/s]2022-02-25 10:48:28 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-25 10:48:28 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-25 10:48:28 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-25 10:48:28 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-02-25 10:48:28 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-25 10:48:28 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-25 10:48:28 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-25 10:48:28 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-25 10:48:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 10:48:28 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-02-25 10:48:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 10:48:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-25 10:48:28 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-02-25 10:48:28 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_last.pt
2022-02-25 10:48:28 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_last.pt
2022-02-25 10:48:28 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-25 10:48:28 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-25 10:48:29 | INFO | fairseq.trainer | begin training epoch 1
2022-02-25 10:48:29 | INFO | fairseq_cli.train | Start iterating over samples

2022-02-25 10:48:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-25 10:48:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 10:49:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 10:49:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 10:49:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-25 11:00:33 | INFO | train_inner | epoch 001:    105 / 788 loss=17.557, ppl=192870, wps=10174.3, ups=0.16, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.569, loss_scale=4, train_wall=719, gb_free=4, wall=725
2022-02-25 11:11:17 | INFO | train_inner | epoch 001:    205 / 788 loss=15.105, ppl=35230.1, wps=10182.2, ups=0.16, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.609, loss_scale=4, train_wall=639, gb_free=4, wall=1368
2022-02-25 11:22:00 | INFO | train_inner | epoch 001:    305 / 788 loss=12.913, ppl=7711.13, wps=10183.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.101, loss_scale=4, train_wall=639, gb_free=4, wall=2012
2022-02-25 11:32:43 | INFO | train_inner | epoch 001:    405 / 788 loss=11.288, ppl=2500.26, wps=10190.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.651, loss_scale=4, train_wall=638, gb_free=4, wall=2655
2022-02-25 11:43:26 | INFO | train_inner | epoch 001:    505 / 788 loss=10.609, ppl=1561.53, wps=10188.9, ups=0.16, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.495, loss_scale=4, train_wall=638, gb_free=4, wall=3298
2022-02-25 11:54:10 | INFO | train_inner | epoch 001:    605 / 788 loss=10.275, ppl=1239.18, wps=10184.4, ups=0.16, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.537, loss_scale=8, train_wall=639, gb_free=4, wall=3942
2022-02-25 12:04:53 | INFO | train_inner | epoch 001:    705 / 788 loss=10, ppl=1024.13, wps=10184.1, ups=0.16, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.643, loss_scale=8, train_wall=639, gb_free=4, wall=4585
2022-02-25 12:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 12:13:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.622 | ppl 787.83 | wps 23605.7 | wpb 2034.1 | bsz 4 | num_updates 783
2022-02-25 12:13:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 783 updates
2022-02-25 12:13:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 12:14:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 12:14:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 1 @ 783 updates, score 9.622) (writing took 6.693924360908568 seconds)
2022-02-25 12:14:01 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-25 12:14:01 | INFO | train | epoch 001 | loss 12.246 | ppl 4858.93 | wps 10151.9 | ups 0.15 | wpb 65497.3 | bsz 127.9 | num_updates 783 | lr 9.79554e-05 | gnorm 1.173 | loss_scale 8 | train_wall 5078 | gb_free 4 | wall 5132
2022-02-25 12:14:01 | INFO | fairseq.trainer | begin training epoch 2
2022-02-25 12:14:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 12:15:50 | INFO | train_inner | epoch 002:     17 / 788 loss=9.776, ppl=876.78, wps=9936.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.745, loss_scale=8, train_wall=636, gb_free=4, wall=5242
2022-02-25 12:26:34 | INFO | train_inner | epoch 002:    117 / 788 loss=9.559, ppl=754.08, wps=10184.5, ups=0.16, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.793, loss_scale=8, train_wall=639, gb_free=4, wall=5885
2022-02-25 12:37:17 | INFO | train_inner | epoch 002:    217 / 788 loss=9.359, ppl=656.45, wps=10181.8, ups=0.16, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.879, loss_scale=8, train_wall=639, gb_free=4, wall=6529
2022-02-25 12:48:01 | INFO | train_inner | epoch 002:    317 / 788 loss=9.185, ppl=582.06, wps=10180.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.9, loss_scale=16, train_wall=639, gb_free=4, wall=7172
2022-02-25 12:58:45 | INFO | train_inner | epoch 002:    417 / 788 loss=9.038, ppl=525.76, wps=10181.1, ups=0.16, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.886, loss_scale=16, train_wall=639, gb_free=4, wall=7816
2022-02-25 13:09:28 | INFO | train_inner | epoch 002:    517 / 788 loss=8.891, ppl=474.81, wps=10181.5, ups=0.16, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.904, loss_scale=16, train_wall=639, gb_free=4, wall=8460
2022-02-25 13:20:12 | INFO | train_inner | epoch 002:    617 / 788 loss=8.759, ppl=433.33, wps=10178.8, ups=0.16, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.926, loss_scale=16, train_wall=639, gb_free=4, wall=9104
2022-02-25 13:30:56 | INFO | train_inner | epoch 002:    717 / 788 loss=8.639, ppl=398.73, wps=10180.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.926, loss_scale=16, train_wall=639, gb_free=4, wall=9747
2022-02-25 13:38:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 13:38:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.355 | ppl 327.38 | wps 23567 | wpb 2034.1 | bsz 4 | num_updates 1571 | best_loss 8.355
2022-02-25 13:38:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1571 updates
2022-02-25 13:38:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 13:38:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 13:38:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 2 @ 1571 updates, score 8.355) (writing took 6.438576926011592 seconds)
2022-02-25 13:38:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-25 13:38:46 | INFO | train | epoch 002 | loss 9.028 | ppl 522.14 | wps 10149.9 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 1571 | lr 0.000196436 | gnorm 0.891 | loss_scale 32 | train_wall 5031 | gb_free 4 | wall 10217
2022-02-25 13:38:46 | INFO | fairseq.trainer | begin training epoch 3
2022-02-25 13:38:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 13:41:52 | INFO | train_inner | epoch 003:     29 / 788 loss=8.519, ppl=366.76, wps=9936.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.912, loss_scale=32, train_wall=636, gb_free=4, wall=10404
2022-02-25 13:52:36 | INFO | train_inner | epoch 003:    129 / 788 loss=8.36, ppl=328.6, wps=10177, ups=0.16, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.909, loss_scale=32, train_wall=639, gb_free=4, wall=11048
2022-02-25 14:03:20 | INFO | train_inner | epoch 003:    229 / 788 loss=8.272, ppl=309.04, wps=10176.6, ups=0.16, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.891, loss_scale=32, train_wall=639, gb_free=4, wall=11692
2022-02-25 14:14:04 | INFO | train_inner | epoch 003:    329 / 788 loss=8.186, ppl=291.27, wps=10177, ups=0.16, wpb=65520.6, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.891, loss_scale=32, train_wall=639, gb_free=4, wall=12336
2022-02-25 14:24:48 | INFO | train_inner | epoch 003:    429 / 788 loss=8.08, ppl=270.53, wps=10178.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.869, loss_scale=32, train_wall=639, gb_free=4, wall=12980
2022-02-25 14:31:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 14:35:38 | INFO | train_inner | epoch 003:    530 / 788 loss=7.999, ppl=255.9, wps=10079.4, ups=0.15, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.856, loss_scale=32, train_wall=645, gb_free=4, wall=13630
2022-02-25 14:46:22 | INFO | train_inner | epoch 003:    630 / 788 loss=7.92, ppl=242.17, wps=10174.8, ups=0.16, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.845, loss_scale=32, train_wall=639, gb_free=4, wall=14274
2022-02-25 14:57:06 | INFO | train_inner | epoch 003:    730 / 788 loss=7.836, ppl=228.48, wps=10176.1, ups=0.16, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.823, loss_scale=32, train_wall=639, gb_free=4, wall=14918
2022-02-25 15:03:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 15:03:26 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.624 | ppl 197.25 | wps 23526.9 | wpb 2034.1 | bsz 4 | num_updates 2358 | best_loss 7.624
2022-02-25 15:03:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2358 updates
2022-02-25 15:03:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 15:03:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 15:03:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 3 @ 2358 updates, score 7.624) (writing took 6.327157418942079 seconds)
2022-02-25 15:03:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-25 15:03:33 | INFO | train | epoch 003 | loss 8.084 | ppl 271.34 | wps 10132.6 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 2358 | lr 0.000294791 | gnorm 0.867 | loss_scale 32 | train_wall 5033 | gb_free 4 | wall 15304
2022-02-25 15:03:33 | INFO | fairseq.trainer | begin training epoch 4
2022-02-25 15:03:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 15:08:03 | INFO | train_inner | epoch 004:     42 / 788 loss=7.735, ppl=212.99, wps=9931.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.803, loss_scale=32, train_wall=637, gb_free=4, wall=15575
2022-02-25 15:18:47 | INFO | train_inner | epoch 004:    142 / 788 loss=7.631, ppl=198.26, wps=10174.8, ups=0.16, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.8, loss_scale=32, train_wall=639, gb_free=4, wall=16219
2022-02-25 15:26:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 15:29:38 | INFO | train_inner | epoch 004:    243 / 788 loss=7.587, ppl=192.32, wps=10076.4, ups=0.15, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.806, loss_scale=32, train_wall=645, gb_free=4, wall=16869
2022-02-25 15:40:22 | INFO | train_inner | epoch 004:    343 / 788 loss=7.511, ppl=182.4, wps=10175.2, ups=0.16, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.769, loss_scale=32, train_wall=639, gb_free=4, wall=17514
2022-02-25 15:43:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 15:51:12 | INFO | train_inner | epoch 004:    444 / 788 loss=7.467, ppl=176.93, wps=10083.1, ups=0.15, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.773, loss_scale=16, train_wall=645, gb_free=4, wall=18164
2022-02-25 16:01:55 | INFO | train_inner | epoch 004:    544 / 788 loss=7.409, ppl=169.92, wps=10186.9, ups=0.16, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.749, loss_scale=16, train_wall=639, gb_free=4, wall=18807
2022-02-25 16:12:39 | INFO | train_inner | epoch 004:    644 / 788 loss=7.364, ppl=164.74, wps=10179.8, ups=0.16, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.752, loss_scale=16, train_wall=639, gb_free=4, wall=19451
2022-02-25 16:23:23 | INFO | train_inner | epoch 004:    744 / 788 loss=7.306, ppl=158.23, wps=10180.7, ups=0.16, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.734, loss_scale=16, train_wall=639, gb_free=4, wall=20094
2022-02-25 16:28:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 16:28:12 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.165 | ppl 143.5 | wps 23552.6 | wpb 2034.1 | bsz 4 | num_updates 3144 | best_loss 7.165
2022-02-25 16:28:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3144 updates
2022-02-25 16:28:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 16:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 16:28:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 4 @ 3144 updates, score 7.165) (writing took 6.326458791969344 seconds)
2022-02-25 16:28:19 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-25 16:28:19 | INFO | train | epoch 004 | loss 7.467 | ppl 176.92 | wps 10122.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 3144 | lr 0.000393021 | gnorm 0.769 | loss_scale 16 | train_wall 5032 | gb_free 4 | wall 20390
2022-02-25 16:28:19 | INFO | fairseq.trainer | begin training epoch 5
2022-02-25 16:28:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 16:34:19 | INFO | train_inner | epoch 005:     56 / 788 loss=7.198, ppl=146.8, wps=9939.8, ups=0.15, wpb=65232.6, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.735, loss_scale=16, train_wall=636, gb_free=4, wall=20751
2022-02-25 16:45:03 | INFO | train_inner | epoch 005:    156 / 788 loss=7.12, ppl=139.11, wps=10180.9, ups=0.16, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.704, loss_scale=32, train_wall=639, gb_free=4, wall=21394
2022-02-25 16:45:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 16:55:53 | INFO | train_inner | epoch 005:    257 / 788 loss=7.098, ppl=136.95, wps=10078.5, ups=0.15, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.721, loss_scale=16, train_wall=645, gb_free=4, wall=22045
2022-02-25 17:06:37 | INFO | train_inner | epoch 005:    357 / 788 loss=7.071, ppl=134.41, wps=10180.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.705, loss_scale=16, train_wall=639, gb_free=4, wall=22688
2022-02-25 17:17:20 | INFO | train_inner | epoch 005:    457 / 788 loss=7.038, ppl=131.43, wps=10181.1, ups=0.16, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.705, loss_scale=16, train_wall=639, gb_free=4, wall=23332
2022-02-25 17:28:04 | INFO | train_inner | epoch 005:    557 / 788 loss=7.013, ppl=129.16, wps=10180.4, ups=0.16, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.695, loss_scale=16, train_wall=639, gb_free=4, wall=23976
2022-02-25 17:38:48 | INFO | train_inner | epoch 005:    657 / 788 loss=6.971, ppl=125.45, wps=10180.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.657, loss_scale=16, train_wall=639, gb_free=4, wall=24619
2022-02-25 17:43:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 17:49:38 | INFO | train_inner | epoch 005:    758 / 788 loss=6.944, ppl=123.13, wps=10077.9, ups=0.15, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.677, loss_scale=16, train_wall=645, gb_free=4, wall=25270
2022-02-25 17:52:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 17:52:58 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.854 | ppl 115.66 | wps 23528.3 | wpb 2034.1 | bsz 4 | num_updates 3930 | best_loss 6.854
2022-02-25 17:52:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3930 updates
2022-02-25 17:52:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 17:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 17:53:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 5 @ 3930 updates, score 6.854) (writing took 5.989525269949809 seconds)
2022-02-25 17:53:04 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-25 17:53:04 | INFO | train | epoch 005 | loss 7.04 | ppl 131.61 | wps 10123.9 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 3930 | lr 0.000491252 | gnorm 0.697 | loss_scale 16 | train_wall 5031 | gb_free 4 | wall 25475
2022-02-25 17:53:04 | INFO | fairseq.trainer | begin training epoch 6
2022-02-25 17:53:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 18:00:34 | INFO | train_inner | epoch 006:     70 / 788 loss=6.822, ppl=113.12, wps=9941.1, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.672, loss_scale=16, train_wall=636, gb_free=4, wall=25926
2022-02-25 18:11:18 | INFO | train_inner | epoch 006:    170 / 788 loss=6.769, ppl=109.04, wps=10180.5, ups=0.16, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.657, loss_scale=16, train_wall=639, gb_free=4, wall=26570
2022-02-25 18:22:02 | INFO | train_inner | epoch 006:    270 / 788 loss=6.761, ppl=108.49, wps=10184, ups=0.16, wpb=65534.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.623, loss_scale=16, train_wall=639, gb_free=4, wall=27213
2022-02-25 18:32:45 | INFO | train_inner | epoch 006:    370 / 788 loss=6.738, ppl=106.74, wps=10183.1, ups=0.16, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.625, loss_scale=16, train_wall=639, gb_free=4, wall=27857
2022-02-25 18:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 18:43:35 | INFO | train_inner | epoch 006:    471 / 788 loss=6.732, ppl=106.31, wps=10081.3, ups=0.15, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.618, loss_scale=16, train_wall=645, gb_free=4, wall=28507
2022-02-25 18:54:19 | INFO | train_inner | epoch 006:    571 / 788 loss=6.715, ppl=105.09, wps=10185.9, ups=0.16, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.579, loss_scale=16, train_wall=639, gb_free=4, wall=29150
2022-02-25 19:05:02 | INFO | train_inner | epoch 006:    671 / 788 loss=6.701, ppl=104.03, wps=10184.4, ups=0.16, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.594, loss_scale=16, train_wall=639, gb_free=4, wall=29794
2022-02-25 19:15:45 | INFO | train_inner | epoch 006:    771 / 788 loss=6.662, ppl=101.28, wps=10187.2, ups=0.16, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.566, loss_scale=16, train_wall=638, gb_free=4, wall=30437
2022-02-25 19:17:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 19:17:41 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.644 | ppl 99.99 | wps 23534.7 | wpb 2034.1 | bsz 4 | num_updates 4717 | best_loss 6.644
2022-02-25 19:17:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4717 updates
2022-02-25 19:17:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 19:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 19:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 6 @ 4717 updates, score 6.644) (writing took 6.120593606960028 seconds)
2022-02-25 19:17:47 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-25 19:17:47 | INFO | train | epoch 006 | loss 6.729 | ppl 106.11 | wps 10139.5 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 4717 | lr 0.000460434 | gnorm 0.614 | loss_scale 16 | train_wall 5030 | gb_free 4 | wall 30559
2022-02-25 19:17:47 | INFO | fairseq.trainer | begin training epoch 7
2022-02-25 19:17:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 19:26:42 | INFO | train_inner | epoch 007:     83 / 788 loss=6.528, ppl=92.29, wps=9933.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.582, loss_scale=16, train_wall=637, gb_free=4, wall=31094
2022-02-25 19:34:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 19:37:34 | INFO | train_inner | epoch 007:    184 / 788 loss=6.513, ppl=91.3, wps=10060.9, ups=0.15, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.559, loss_scale=16, train_wall=646, gb_free=4, wall=31745
2022-02-25 19:48:18 | INFO | train_inner | epoch 007:    284 / 788 loss=6.506, ppl=90.89, wps=10165.8, ups=0.16, wpb=65519.3, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.573, loss_scale=16, train_wall=640, gb_free=4, wall=32390
2022-02-25 19:59:02 | INFO | train_inner | epoch 007:    384 / 788 loss=6.497, ppl=90.29, wps=10172.5, ups=0.16, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.565, loss_scale=16, train_wall=639, gb_free=4, wall=33034
2022-02-25 20:09:46 | INFO | train_inner | epoch 007:    484 / 788 loss=6.501, ppl=90.59, wps=10177.8, ups=0.16, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.56, loss_scale=16, train_wall=639, gb_free=4, wall=33678
2022-02-25 20:20:30 | INFO | train_inner | epoch 007:    584 / 788 loss=6.491, ppl=89.92, wps=10179, ups=0.16, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.562, loss_scale=16, train_wall=639, gb_free=4, wall=34322
2022-02-25 20:30:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 20:31:20 | INFO | train_inner | epoch 007:    685 / 788 loss=6.488, ppl=89.74, wps=10080.5, ups=0.15, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.546, loss_scale=16, train_wall=645, gb_free=4, wall=34972
2022-02-25 20:42:04 | INFO | train_inner | epoch 007:    785 / 788 loss=6.471, ppl=88.69, wps=10183.5, ups=0.16, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.54, loss_scale=16, train_wall=639, gb_free=4, wall=35615
2022-02-25 20:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 20:42:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.498 | ppl 90.39 | wps 23530.6 | wpb 2034.1 | bsz 4 | num_updates 5503 | best_loss 6.498
2022-02-25 20:42:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5503 updates
2022-02-25 20:42:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 20:42:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 20:42:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 7 @ 5503 updates, score 6.498) (writing took 5.989334192825481 seconds)
2022-02-25 20:42:36 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-25 20:42:36 | INFO | train | epoch 007 | loss 6.495 | ppl 90.21 | wps 10117.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 5503 | lr 0.000426285 | gnorm 0.56 | loss_scale 16 | train_wall 5034 | gb_free 4 | wall 35647
2022-02-25 20:42:36 | INFO | fairseq.trainer | begin training epoch 8
2022-02-25 20:42:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 20:53:00 | INFO | train_inner | epoch 008:     97 / 788 loss=6.303, ppl=78.97, wps=9939.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=5600, lr=0.000422577, gnorm=0.551, loss_scale=16, train_wall=636, gb_free=4, wall=36272
2022-02-25 21:03:44 | INFO | train_inner | epoch 008:    197 / 788 loss=6.33, ppl=80.43, wps=10181.2, ups=0.16, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.537, loss_scale=16, train_wall=639, gb_free=4, wall=36916
2022-02-25 21:14:28 | INFO | train_inner | epoch 008:    297 / 788 loss=6.325, ppl=80.19, wps=10181.7, ups=0.16, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.569, loss_scale=16, train_wall=639, gb_free=4, wall=37559
2022-02-25 21:25:11 | INFO | train_inner | epoch 008:    397 / 788 loss=6.33, ppl=80.47, wps=10181.8, ups=0.16, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.54, loss_scale=16, train_wall=639, gb_free=4, wall=38203
2022-02-25 21:30:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 21:36:01 | INFO | train_inner | epoch 008:    498 / 788 loss=6.346, ppl=81.37, wps=10080.5, ups=0.15, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.521, loss_scale=16, train_wall=645, gb_free=4, wall=38853
2022-02-25 21:46:45 | INFO | train_inner | epoch 008:    598 / 788 loss=6.335, ppl=80.76, wps=10185, ups=0.16, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.54, loss_scale=16, train_wall=639, gb_free=4, wall=39497
2022-02-25 21:57:29 | INFO | train_inner | epoch 008:    698 / 788 loss=6.342, ppl=81.1, wps=10180.2, ups=0.16, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.521, loss_scale=16, train_wall=639, gb_free=4, wall=40140
2022-02-25 22:07:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 22:07:14 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.411 | ppl 85.12 | wps 23556.6 | wpb 2034.1 | bsz 4 | num_updates 6290 | best_loss 6.411
2022-02-25 22:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6290 updates
2022-02-25 22:07:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 22:07:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 22:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 8 @ 6290 updates, score 6.411) (writing took 5.94608300505206 seconds)
2022-02-25 22:07:20 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-25 22:07:20 | INFO | train | epoch 008 | loss 6.33 | ppl 80.46 | wps 10137.7 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 6290 | lr 0.000398726 | gnorm 0.541 | loss_scale 16 | train_wall 5031 | gb_free 4 | wall 40732
2022-02-25 22:07:20 | INFO | fairseq.trainer | begin training epoch 9
2022-02-25 22:07:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 22:08:25 | INFO | train_inner | epoch 009:     10 / 788 loss=6.315, ppl=79.62, wps=9944.4, ups=0.15, wpb=65232.6, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.55, loss_scale=16, train_wall=636, gb_free=4, wall=40796
2022-02-25 22:19:08 | INFO | train_inner | epoch 009:    110 / 788 loss=6.175, ppl=72.24, wps=10182.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.536, loss_scale=16, train_wall=639, gb_free=4, wall=41440
2022-02-25 22:26:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 22:29:58 | INFO | train_inner | epoch 009:    211 / 788 loss=6.196, ppl=73.31, wps=10082.8, ups=0.15, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.526, loss_scale=16, train_wall=645, gb_free=4, wall=42090
2022-02-25 22:40:42 | INFO | train_inner | epoch 009:    311 / 788 loss=6.195, ppl=73.25, wps=10185.4, ups=0.16, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.543, loss_scale=16, train_wall=639, gb_free=4, wall=42733
2022-02-25 22:51:25 | INFO | train_inner | epoch 009:    411 / 788 loss=6.208, ppl=73.91, wps=10183.3, ups=0.16, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.538, loss_scale=16, train_wall=639, gb_free=4, wall=43377
2022-02-25 23:02:09 | INFO | train_inner | epoch 009:    511 / 788 loss=6.221, ppl=74.58, wps=10184.2, ups=0.16, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.539, loss_scale=16, train_wall=639, gb_free=4, wall=44020
2022-02-25 23:12:52 | INFO | train_inner | epoch 009:    611 / 788 loss=6.214, ppl=74.24, wps=10181.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.527, loss_scale=16, train_wall=639, gb_free=4, wall=44664
2022-02-25 23:23:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 23:23:42 | INFO | train_inner | epoch 009:    712 / 788 loss=6.225, ppl=74.79, wps=10078.3, ups=0.15, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.531, loss_scale=16, train_wall=645, gb_free=4, wall=45314
2022-02-25 23:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 23:31:58 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.354 | ppl 81.79 | wps 23530.7 | wpb 2034.1 | bsz 4 | num_updates 7076 | best_loss 6.354
2022-02-25 23:31:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7076 updates
2022-02-25 23:31:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 23:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-25 23:32:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 9 @ 7076 updates, score 6.354) (writing took 5.959606549935415 seconds)
2022-02-25 23:32:04 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-25 23:32:04 | INFO | train | epoch 009 | loss 6.206 | ppl 73.82 | wps 10126.2 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 7076 | lr 0.000375929 | gnorm 0.533 | loss_scale 16 | train_wall 5030 | gb_free 4 | wall 45816
2022-02-25 23:32:04 | INFO | fairseq.trainer | begin training epoch 10
2022-02-25 23:32:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 23:34:39 | INFO | train_inner | epoch 010:     24 / 788 loss=6.185, ppl=72.75, wps=9943.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.517, loss_scale=16, train_wall=636, gb_free=4, wall=45970
2022-02-25 23:40:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 23:45:29 | INFO | train_inner | epoch 010:    125 / 788 loss=6.068, ppl=67.07, wps=10084.6, ups=0.15, wpb=65534.7, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.533, loss_scale=8, train_wall=645, gb_free=4, wall=46620
2022-02-25 23:56:12 | INFO | train_inner | epoch 010:    225 / 788 loss=6.084, ppl=67.84, wps=10185, ups=0.16, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.537, loss_scale=8, train_wall=639, gb_free=4, wall=47264
2022-02-26 00:06:55 | INFO | train_inner | epoch 010:    325 / 788 loss=6.106, ppl=68.9, wps=10182.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.524, loss_scale=8, train_wall=639, gb_free=4, wall=47907
2022-02-26 00:17:39 | INFO | train_inner | epoch 010:    425 / 788 loss=6.116, ppl=69.36, wps=10184.1, ups=0.16, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.52, loss_scale=8, train_wall=639, gb_free=4, wall=48551
2022-02-26 00:28:23 | INFO | train_inner | epoch 010:    525 / 788 loss=6.112, ppl=69.16, wps=10179.9, ups=0.16, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.532, loss_scale=8, train_wall=639, gb_free=4, wall=49194
2022-02-26 00:39:07 | INFO | train_inner | epoch 010:    625 / 788 loss=6.119, ppl=69.53, wps=10179.8, ups=0.16, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.54, loss_scale=16, train_wall=639, gb_free=4, wall=49838
2022-02-26 00:49:50 | INFO | train_inner | epoch 010:    725 / 788 loss=6.143, ppl=70.67, wps=10181.9, ups=0.16, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.521, loss_scale=16, train_wall=639, gb_free=4, wall=50482
2022-02-26 00:56:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 00:56:42 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.303 | ppl 78.98 | wps 23584.6 | wpb 2034.1 | bsz 4 | num_updates 7863 | best_loss 6.303
2022-02-26 00:56:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7863 updates
2022-02-26 00:56:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 00:56:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 00:56:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 10 @ 7863 updates, score 6.303) (writing took 5.967722256900743 seconds)
2022-02-26 00:56:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-26 00:56:48 | INFO | train | epoch 010 | loss 6.108 | ppl 68.95 | wps 10138.8 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 7863 | lr 0.00035662 | gnorm 0.529 | loss_scale 16 | train_wall 5030 | gb_free 4 | wall 50900
2022-02-26 00:56:48 | INFO | fairseq.trainer | begin training epoch 11
2022-02-26 00:56:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 01:00:46 | INFO | train_inner | epoch 011:     37 / 788 loss=6.071, ppl=67.24, wps=9943.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.515, loss_scale=16, train_wall=636, gb_free=4, wall=51138
2022-02-26 01:11:30 | INFO | train_inner | epoch 011:    137 / 788 loss=5.99, ppl=63.54, wps=10182.4, ups=0.16, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.536, loss_scale=16, train_wall=639, gb_free=4, wall=51782
2022-02-26 01:11:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 01:22:20 | INFO | train_inner | epoch 011:    238 / 788 loss=6.005, ppl=64.21, wps=10080.8, ups=0.15, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.536, loss_scale=8, train_wall=645, gb_free=4, wall=52432
2022-02-26 01:33:04 | INFO | train_inner | epoch 011:    338 / 788 loss=6.017, ppl=64.75, wps=10185.1, ups=0.16, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.531, loss_scale=8, train_wall=639, gb_free=4, wall=53075
2022-02-26 01:43:47 | INFO | train_inner | epoch 011:    438 / 788 loss=6.031, ppl=65.38, wps=10184, ups=0.16, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.545, loss_scale=8, train_wall=639, gb_free=4, wall=53719
2022-02-26 01:54:30 | INFO | train_inner | epoch 011:    538 / 788 loss=6.04, ppl=65.81, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.525, loss_scale=8, train_wall=638, gb_free=4, wall=54362
2022-02-26 02:05:14 | INFO | train_inner | epoch 011:    638 / 788 loss=6.053, ppl=66.39, wps=10185.8, ups=0.16, wpb=65519.3, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.513, loss_scale=8, train_wall=638, gb_free=4, wall=55005
2022-02-26 02:07:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:16:04 | INFO | train_inner | epoch 011:    739 / 788 loss=6.062, ppl=66.82, wps=10082.1, ups=0.15, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.553, loss_scale=8, train_wall=645, gb_free=4, wall=55655
2022-02-26 02:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 02:21:26 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.267 | ppl 76.99 | wps 23588 | wpb 2034.1 | bsz 4 | num_updates 8649 | best_loss 6.267
2022-02-26 02:21:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8649 updates
2022-02-26 02:21:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 02:21:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 02:21:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 11 @ 8649 updates, score 6.267) (writing took 5.911599833983928 seconds)
2022-02-26 02:21:31 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-26 02:21:31 | INFO | train | epoch 011 | loss 6.028 | ppl 65.25 | wps 10127.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 8649 | lr 0.00034003 | gnorm 0.531 | loss_scale 8 | train_wall 5030 | gb_free 4 | wall 55983
2022-02-26 02:21:31 | INFO | fairseq.trainer | begin training epoch 12
2022-02-26 02:21:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 02:27:00 | INFO | train_inner | epoch 012:     51 / 788 loss=5.979, ppl=63.07, wps=9944.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.508, loss_scale=8, train_wall=636, gb_free=4, wall=56311
2022-02-26 02:37:43 | INFO | train_inner | epoch 012:    151 / 788 loss=5.915, ppl=60.32, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.525, loss_scale=8, train_wall=638, gb_free=4, wall=56955
2022-02-26 02:48:27 | INFO | train_inner | epoch 012:    251 / 788 loss=5.94, ppl=61.39, wps=10183.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.537, loss_scale=8, train_wall=639, gb_free=4, wall=57598
2022-02-26 02:59:10 | INFO | train_inner | epoch 012:    351 / 788 loss=5.962, ppl=62.36, wps=10183.4, ups=0.16, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.548, loss_scale=8, train_wall=639, gb_free=4, wall=58242
2022-02-26 03:09:54 | INFO | train_inner | epoch 012:    451 / 788 loss=5.967, ppl=62.57, wps=10182, ups=0.16, wpb=65520.6, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.515, loss_scale=16, train_wall=639, gb_free=4, wall=58885
2022-02-26 03:16:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 03:20:44 | INFO | train_inner | epoch 012:    552 / 788 loss=5.978, ppl=63.04, wps=10081.9, ups=0.15, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.553, loss_scale=8, train_wall=645, gb_free=4, wall=59535
2022-02-26 03:31:27 | INFO | train_inner | epoch 012:    652 / 788 loss=5.986, ppl=63.39, wps=10182.5, ups=0.16, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.524, loss_scale=8, train_wall=639, gb_free=4, wall=60179
2022-02-26 03:42:11 | INFO | train_inner | epoch 012:    752 / 788 loss=5.992, ppl=63.64, wps=10186.9, ups=0.16, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.53, loss_scale=8, train_wall=638, gb_free=4, wall=60822
2022-02-26 03:46:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 03:46:09 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.248 | ppl 76.01 | wps 23537.1 | wpb 2034.1 | bsz 4 | num_updates 9436 | best_loss 6.248
2022-02-26 03:46:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9436 updates
2022-02-26 03:46:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 03:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 03:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 12 @ 9436 updates, score 6.248) (writing took 5.939139958936721 seconds)
2022-02-26 03:46:15 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-26 03:46:15 | INFO | train | epoch 012 | loss 5.961 | ppl 62.3 | wps 10140.3 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 9436 | lr 0.000325541 | gnorm 0.532 | loss_scale 8 | train_wall 5030 | gb_free 4 | wall 61066
2022-02-26 03:46:15 | INFO | fairseq.trainer | begin training epoch 13
2022-02-26 03:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 03:53:07 | INFO | train_inner | epoch 013:     64 / 788 loss=5.904, ppl=59.89, wps=9947, ups=0.15, wpb=65248, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.547, loss_scale=8, train_wall=636, gb_free=4, wall=61478
2022-02-26 04:03:50 | INFO | train_inner | epoch 013:    164 / 788 loss=5.869, ppl=58.43, wps=10187.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.541, loss_scale=8, train_wall=638, gb_free=4, wall=62122
2022-02-26 04:14:33 | INFO | train_inner | epoch 013:    264 / 788 loss=5.878, ppl=58.79, wps=10184.3, ups=0.16, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.538, loss_scale=16, train_wall=639, gb_free=4, wall=62765
2022-02-26 04:21:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:25:23 | INFO | train_inner | epoch 013:    365 / 788 loss=5.909, ppl=60.07, wps=10082, ups=0.15, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.536, loss_scale=8, train_wall=645, gb_free=4, wall=63415
2022-02-26 04:36:07 | INFO | train_inner | epoch 013:    465 / 788 loss=5.909, ppl=60.08, wps=10182.3, ups=0.16, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.532, loss_scale=8, train_wall=639, gb_free=4, wall=64059
2022-02-26 04:46:51 | INFO | train_inner | epoch 013:    565 / 788 loss=5.92, ppl=60.56, wps=10182.7, ups=0.16, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.559, loss_scale=8, train_wall=639, gb_free=4, wall=64702
2022-02-26 04:57:34 | INFO | train_inner | epoch 013:    665 / 788 loss=5.936, ppl=61.2, wps=10186.8, ups=0.16, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.547, loss_scale=8, train_wall=639, gb_free=4, wall=65346
2022-02-26 05:08:17 | INFO | train_inner | epoch 013:    765 / 788 loss=5.934, ppl=61.13, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.539, loss_scale=8, train_wall=638, gb_free=4, wall=65989
2022-02-26 05:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 05:10:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.233 | ppl 75.2 | wps 23550.1 | wpb 2034.1 | bsz 4 | num_updates 10223 | best_loss 6.233
2022-02-26 05:10:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10223 updates
2022-02-26 05:10:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 05:10:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 05:10:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 13 @ 10223 updates, score 6.233) (writing took 5.963582536205649 seconds)
2022-02-26 05:10:58 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 05:10:58 | INFO | train | epoch 013 | loss 5.904 | ppl 59.86 | wps 10141.1 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 10223 | lr 0.00031276 | gnorm 0.541 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 66149
2022-02-26 05:10:58 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 05:10:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 05:19:14 | INFO | train_inner | epoch 014:     77 / 788 loss=5.831, ppl=56.93, wps=9941.9, ups=0.15, wpb=65248, bsz=127.4, num_updates=10300, lr=0.000311588, gnorm=0.519, loss_scale=16, train_wall=636, gb_free=4, wall=66645
2022-02-26 05:29:58 | INFO | train_inner | epoch 014:    177 / 788 loss=5.814, ppl=56.26, wps=10171.2, ups=0.16, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.525, loss_scale=16, train_wall=639, gb_free=4, wall=67290
2022-02-26 05:40:42 | INFO | train_inner | epoch 014:    277 / 788 loss=5.833, ppl=57, wps=10180.1, ups=0.16, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.528, loss_scale=16, train_wall=639, gb_free=4, wall=67933
2022-02-26 05:51:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:51:32 | INFO | train_inner | epoch 014:    378 / 788 loss=5.849, ppl=57.65, wps=10083.6, ups=0.15, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.546, loss_scale=8, train_wall=645, gb_free=4, wall=68583
2022-02-26 06:02:15 | INFO | train_inner | epoch 014:    478 / 788 loss=5.866, ppl=58.32, wps=10185.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.545, loss_scale=8, train_wall=638, gb_free=4, wall=69227
2022-02-26 06:12:58 | INFO | train_inner | epoch 014:    578 / 788 loss=5.878, ppl=58.8, wps=10186, ups=0.16, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.522, loss_scale=8, train_wall=639, gb_free=4, wall=69870
2022-02-26 06:23:42 | INFO | train_inner | epoch 014:    678 / 788 loss=5.884, ppl=59.07, wps=10184.8, ups=0.16, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.545, loss_scale=8, train_wall=639, gb_free=4, wall=70513
2022-02-26 06:34:25 | INFO | train_inner | epoch 014:    778 / 788 loss=5.895, ppl=59.49, wps=10188.2, ups=0.16, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.529, loss_scale=8, train_wall=638, gb_free=4, wall=71157
2022-02-26 06:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 06:35:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.209 | ppl 73.97 | wps 23629.7 | wpb 2034.1 | bsz 4 | num_updates 11010 | best_loss 6.209
2022-02-26 06:35:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 11010 updates
2022-02-26 06:35:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 06:35:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 06:35:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 14 @ 11010 updates, score 6.209) (writing took 5.985840480076149 seconds)
2022-02-26 06:35:42 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 06:35:42 | INFO | train | epoch 014 | loss 5.854 | ppl 57.84 | wps 10139 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 11010 | lr 0.000301374 | gnorm 0.534 | loss_scale 8 | train_wall 5030 | gb_free 4 | wall 71233
2022-02-26 06:35:42 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 06:35:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 06:45:21 | INFO | train_inner | epoch 015:     90 / 788 loss=5.765, ppl=54.39, wps=9949.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11100, lr=0.00030015, gnorm=0.547, loss_scale=8, train_wall=636, gb_free=4, wall=71812
2022-02-26 06:56:04 | INFO | train_inner | epoch 015:    190 / 788 loss=5.779, ppl=54.9, wps=10182, ups=0.16, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.533, loss_scale=16, train_wall=639, gb_free=4, wall=72456
2022-02-26 07:06:48 | INFO | train_inner | epoch 015:    290 / 788 loss=5.794, ppl=55.5, wps=10182.8, ups=0.16, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.528, loss_scale=16, train_wall=639, gb_free=4, wall=73100
2022-02-26 07:09:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:17:38 | INFO | train_inner | epoch 015:    391 / 788 loss=5.798, ppl=55.62, wps=10083.7, ups=0.15, wpb=65534.7, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.56, loss_scale=8, train_wall=645, gb_free=4, wall=73750
2022-02-26 07:28:21 | INFO | train_inner | epoch 015:    491 / 788 loss=5.828, ppl=56.82, wps=10185.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.527, loss_scale=8, train_wall=638, gb_free=4, wall=74393
2022-02-26 07:39:05 | INFO | train_inner | epoch 015:    591 / 788 loss=5.843, ppl=57.41, wps=10185, ups=0.16, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.532, loss_scale=8, train_wall=639, gb_free=4, wall=75036
2022-02-26 07:49:48 | INFO | train_inner | epoch 015:    691 / 788 loss=5.839, ppl=57.24, wps=10185.2, ups=0.16, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.526, loss_scale=8, train_wall=639, gb_free=4, wall=75680
2022-02-26 08:00:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 08:00:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.198 | ppl 73.44 | wps 23527.9 | wpb 2034.1 | bsz 4 | num_updates 11797 | best_loss 6.198
2022-02-26 08:00:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11797 updates
2022-02-26 08:00:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 08:00:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 08:00:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 15 @ 11797 updates, score 6.198) (writing took 6.0319042368792 seconds)
2022-02-26 08:00:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 08:00:25 | INFO | train | epoch 015 | loss 5.811 | ppl 56.13 | wps 10141.2 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 11797 | lr 0.000291148 | gnorm 0.539 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 76316
2022-02-26 08:00:25 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 08:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 08:00:44 | INFO | train_inner | epoch 016:      3 / 788 loss=5.845, ppl=57.47, wps=9948.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.556, loss_scale=8, train_wall=636, gb_free=4, wall=76336
2022-02-26 08:11:28 | INFO | train_inner | epoch 016:    103 / 788 loss=5.704, ppl=52.13, wps=10182.3, ups=0.16, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.538, loss_scale=16, train_wall=639, gb_free=4, wall=76979
2022-02-26 08:16:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:22:18 | INFO | train_inner | epoch 016:    204 / 788 loss=5.742, ppl=53.54, wps=10082.4, ups=0.15, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.542, loss_scale=8, train_wall=645, gb_free=4, wall=77629
2022-02-26 08:33:01 | INFO | train_inner | epoch 016:    304 / 788 loss=5.752, ppl=53.91, wps=10183.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.551, loss_scale=8, train_wall=639, gb_free=4, wall=78273
2022-02-26 08:43:45 | INFO | train_inner | epoch 016:    404 / 788 loss=5.762, ppl=54.25, wps=10183.8, ups=0.16, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.538, loss_scale=8, train_wall=639, gb_free=4, wall=78916
2022-02-26 08:54:28 | INFO | train_inner | epoch 016:    504 / 788 loss=5.783, ppl=55.06, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.547, loss_scale=8, train_wall=638, gb_free=4, wall=79560
2022-02-26 09:05:11 | INFO | train_inner | epoch 016:    604 / 788 loss=5.791, ppl=55.39, wps=10187.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.539, loss_scale=8, train_wall=638, gb_free=4, wall=80203
2022-02-26 09:14:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:16:01 | INFO | train_inner | epoch 016:    705 / 788 loss=5.825, ppl=56.7, wps=10086.9, ups=0.15, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.543, loss_scale=8, train_wall=645, gb_free=4, wall=80852
2022-02-26 09:24:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 09:25:01 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.188 | ppl 72.89 | wps 23551.5 | wpb 2034.1 | bsz 4 | num_updates 12583 | best_loss 6.188
2022-02-26 09:25:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12583 updates
2022-02-26 09:25:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 09:25:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 09:25:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 16 @ 12583 updates, score 6.188) (writing took 5.9388207860756665 seconds)
2022-02-26 09:25:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 09:25:07 | INFO | train | epoch 016 | loss 5.771 | ppl 54.63 | wps 10129.3 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 12583 | lr 0.000281908 | gnorm 0.544 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 81399
2022-02-26 09:25:07 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 09:25:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 09:26:56 | INFO | train_inner | epoch 017:     17 / 788 loss=5.793, ppl=55.43, wps=9952, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.551, loss_scale=8, train_wall=636, gb_free=4, wall=81508
2022-02-26 09:37:40 | INFO | train_inner | epoch 017:    117 / 788 loss=5.68, ppl=51.25, wps=10181.3, ups=0.16, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.544, loss_scale=8, train_wall=639, gb_free=4, wall=82152
2022-02-26 09:48:24 | INFO | train_inner | epoch 017:    217 / 788 loss=5.69, ppl=51.64, wps=10185.6, ups=0.16, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.538, loss_scale=8, train_wall=638, gb_free=4, wall=82795
2022-02-26 09:59:07 | INFO | train_inner | epoch 017:    317 / 788 loss=5.717, ppl=52.61, wps=10184.1, ups=0.16, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.547, loss_scale=8, train_wall=639, gb_free=4, wall=83439
2022-02-26 10:09:50 | INFO | train_inner | epoch 017:    417 / 788 loss=5.745, ppl=53.62, wps=10187.6, ups=0.16, wpb=65519.3, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.541, loss_scale=8, train_wall=638, gb_free=4, wall=84082
2022-02-26 10:20:34 | INFO | train_inner | epoch 017:    517 / 788 loss=5.756, ppl=54.04, wps=10182.9, ups=0.16, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.558, loss_scale=16, train_wall=639, gb_free=4, wall=84725
2022-02-26 10:31:18 | INFO | train_inner | epoch 017:    617 / 788 loss=5.766, ppl=54.4, wps=10178.4, ups=0.16, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.55, loss_scale=16, train_wall=639, gb_free=4, wall=85369
2022-02-26 10:37:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 10:42:08 | INFO | train_inner | epoch 017:    718 / 788 loss=5.777, ppl=54.85, wps=10082.5, ups=0.15, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.529, loss_scale=8, train_wall=645, gb_free=4, wall=86019
2022-02-26 10:49:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 10:49:45 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.181 | ppl 72.55 | wps 23532.5 | wpb 2034.1 | bsz 4 | num_updates 13370 | best_loss 6.181
2022-02-26 10:49:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13370 updates
2022-02-26 10:49:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 10:49:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 10:49:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 17 @ 13370 updates, score 6.181) (writing took 6.075998381944373 seconds)
2022-02-26 10:49:51 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 10:49:51 | INFO | train | epoch 017 | loss 5.737 | ppl 53.35 | wps 10139.7 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 13370 | lr 0.000273485 | gnorm 0.543 | loss_scale 8 | train_wall 5030 | gb_free 4 | wall 86482
2022-02-26 10:49:51 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 10:49:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 10:53:04 | INFO | train_inner | epoch 018:     30 / 788 loss=5.751, ppl=53.86, wps=9945.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.55, loss_scale=8, train_wall=636, gb_free=4, wall=86675
2022-02-26 11:03:50 | INFO | train_inner | epoch 018:    130 / 788 loss=5.645, ppl=50.03, wps=10136.8, ups=0.15, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.552, loss_scale=8, train_wall=642, gb_free=4, wall=87322
2022-02-26 11:14:33 | INFO | train_inner | epoch 018:    230 / 788 loss=5.68, ppl=51.26, wps=10188.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.556, loss_scale=8, train_wall=638, gb_free=4, wall=87965
2022-02-26 11:25:17 | INFO | train_inner | epoch 018:    330 / 788 loss=5.693, ppl=51.75, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.557, loss_scale=8, train_wall=638, gb_free=4, wall=88608
2022-02-26 11:36:00 | INFO | train_inner | epoch 018:    430 / 788 loss=5.708, ppl=52.28, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.532, loss_scale=16, train_wall=638, gb_free=4, wall=89252
2022-02-26 11:39:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 11:46:50 | INFO | train_inner | epoch 018:    531 / 788 loss=5.727, ppl=52.96, wps=10080.1, ups=0.15, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.561, loss_scale=8, train_wall=645, gb_free=4, wall=89902
2022-02-26 11:57:33 | INFO | train_inner | epoch 018:    631 / 788 loss=5.734, ppl=53.21, wps=10185.7, ups=0.16, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.553, loss_scale=8, train_wall=639, gb_free=4, wall=90545
2022-02-26 12:08:17 | INFO | train_inner | epoch 018:    731 / 788 loss=5.751, ppl=53.84, wps=10182.2, ups=0.16, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.531, loss_scale=8, train_wall=639, gb_free=4, wall=91189
2022-02-26 12:14:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 12:14:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.174 | ppl 72.18 | wps 23529.8 | wpb 2034.1 | bsz 4 | num_updates 14157 | best_loss 6.174
2022-02-26 12:14:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14157 updates
2022-02-26 12:14:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 12:14:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 12:14:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 18 @ 14157 updates, score 6.174) (writing took 6.176007502013817 seconds)
2022-02-26 12:14:36 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 12:14:36 | INFO | train | epoch 018 | loss 5.706 | ppl 52.19 | wps 10135.2 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 14157 | lr 0.000265775 | gnorm 0.55 | loss_scale 8 | train_wall 5032 | gb_free 4 | wall 91568
2022-02-26 12:14:37 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 12:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 12:19:13 | INFO | train_inner | epoch 019:     43 / 788 loss=5.689, ppl=51.58, wps=9945.6, ups=0.15, wpb=65248, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.551, loss_scale=8, train_wall=636, gb_free=4, wall=91845
2022-02-26 12:29:57 | INFO | train_inner | epoch 019:    143 / 788 loss=5.619, ppl=49.14, wps=10186.4, ups=0.16, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.549, loss_scale=8, train_wall=638, gb_free=4, wall=92488
2022-02-26 12:40:40 | INFO | train_inner | epoch 019:    243 / 788 loss=5.647, ppl=50.12, wps=10181.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.549, loss_scale=16, train_wall=639, gb_free=4, wall=93132
2022-02-26 12:46:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 12:51:30 | INFO | train_inner | epoch 019:    344 / 788 loss=5.665, ppl=50.75, wps=10080, ups=0.15, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.561, loss_scale=8, train_wall=645, gb_free=4, wall=93782
2022-02-26 13:02:14 | INFO | train_inner | epoch 019:    444 / 788 loss=5.682, ppl=51.34, wps=10184.9, ups=0.16, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.55, loss_scale=8, train_wall=639, gb_free=4, wall=94425
2022-02-26 13:12:57 | INFO | train_inner | epoch 019:    544 / 788 loss=5.692, ppl=51.7, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.533, loss_scale=8, train_wall=638, gb_free=4, wall=95069
2022-02-26 13:23:41 | INFO | train_inner | epoch 019:    644 / 788 loss=5.722, ppl=52.77, wps=10184, ups=0.16, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.575, loss_scale=8, train_wall=639, gb_free=4, wall=95712
2022-02-26 13:34:24 | INFO | train_inner | epoch 019:    744 / 788 loss=5.729, ppl=53.05, wps=10184.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=96356
2022-02-26 13:39:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 13:39:14 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.166 | ppl 71.83 | wps 23514.3 | wpb 2034.1 | bsz 4 | num_updates 14944 | best_loss 6.166
2022-02-26 13:39:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14944 updates
2022-02-26 13:39:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 13:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 13:39:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 19 @ 14944 updates, score 6.166) (writing took 6.534624893916771 seconds)
2022-02-26 13:39:20 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 13:39:20 | INFO | train | epoch 019 | loss 5.678 | ppl 51.19 | wps 10139.6 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 14944 | lr 0.000258682 | gnorm 0.553 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 96652
2022-02-26 13:39:20 | INFO | fairseq.trainer | begin training epoch 20
2022-02-26 13:39:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 13:45:21 | INFO | train_inner | epoch 020:     56 / 788 loss=5.642, ppl=49.93, wps=9938.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.542, loss_scale=16, train_wall=636, gb_free=4, wall=97012
2022-02-26 13:48:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 13:56:10 | INFO | train_inner | epoch 020:    157 / 788 loss=5.606, ppl=48.69, wps=10085.2, ups=0.15, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.568, loss_scale=8, train_wall=645, gb_free=4, wall=97662
2022-02-26 14:06:54 | INFO | train_inner | epoch 020:    257 / 788 loss=5.623, ppl=49.27, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.554, loss_scale=8, train_wall=638, gb_free=4, wall=98305
2022-02-26 14:17:37 | INFO | train_inner | epoch 020:    357 / 788 loss=5.647, ppl=50.11, wps=10186.4, ups=0.16, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.56, loss_scale=8, train_wall=638, gb_free=4, wall=98949
2022-02-26 14:28:20 | INFO | train_inner | epoch 020:    457 / 788 loss=5.664, ppl=50.71, wps=10185.4, ups=0.16, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.547, loss_scale=8, train_wall=639, gb_free=4, wall=99592
2022-02-26 14:39:04 | INFO | train_inner | epoch 020:    557 / 788 loss=5.675, ppl=51.08, wps=10184.3, ups=0.16, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.572, loss_scale=8, train_wall=639, gb_free=4, wall=100236
2022-02-26 14:49:48 | INFO | train_inner | epoch 020:    657 / 788 loss=5.673, ppl=51.02, wps=10182.6, ups=0.16, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.532, loss_scale=16, train_wall=639, gb_free=4, wall=100879
2022-02-26 14:52:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 15:00:37 | INFO | train_inner | epoch 020:    758 / 788 loss=5.7, ppl=51.97, wps=10084, ups=0.15, wpb=65520.6, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.575, loss_scale=8, train_wall=645, gb_free=4, wall=101529
2022-02-26 15:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 15:03:57 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.175 | ppl 72.23 | wps 23544.2 | wpb 2034.1 | bsz 4 | num_updates 15730 | best_loss 6.166
2022-02-26 15:03:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15730 updates
2022-02-26 15:03:57 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-26 15:03:57 | INFO | train | epoch 020 | loss 5.652 | ppl 50.27 | wps 10140.8 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 15730 | lr 0.000252136 | gnorm 0.557 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 101728
2022-02-26 15:03:57 | INFO | fairseq.trainer | begin training epoch 21
2022-02-26 15:03:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 15:11:27 | INFO | train_inner | epoch 021:     70 / 788 loss=5.593, ppl=48.28, wps=10042.5, ups=0.15, wpb=65248, bsz=127.4, num_updates=15800, lr=0.000251577, gnorm=0.551, loss_scale=8, train_wall=636, gb_free=4, wall=102179
2022-02-26 15:22:11 | INFO | train_inner | epoch 021:    170 / 788 loss=5.568, ppl=47.43, wps=10184.6, ups=0.16, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.566, loss_scale=8, train_wall=639, gb_free=4, wall=102822
2022-02-26 15:32:54 | INFO | train_inner | epoch 021:    270 / 788 loss=5.61, ppl=48.84, wps=10188.4, ups=0.16, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.544, loss_scale=8, train_wall=638, gb_free=4, wall=103465
2022-02-26 15:43:37 | INFO | train_inner | epoch 021:    370 / 788 loss=5.624, ppl=49.33, wps=10185.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.59, loss_scale=8, train_wall=639, gb_free=4, wall=104109
2022-02-26 15:54:21 | INFO | train_inner | epoch 021:    470 / 788 loss=5.644, ppl=50, wps=10177.6, ups=0.16, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.549, loss_scale=16, train_wall=639, gb_free=4, wall=104753
2022-02-26 15:56:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:05:11 | INFO | train_inner | epoch 021:    571 / 788 loss=5.651, ppl=50.25, wps=10087.7, ups=0.15, wpb=65520.6, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.546, loss_scale=8, train_wall=645, gb_free=4, wall=105402
2022-02-26 16:15:54 | INFO | train_inner | epoch 021:    671 / 788 loss=5.669, ppl=50.89, wps=10186.9, ups=0.16, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.563, loss_scale=8, train_wall=638, gb_free=4, wall=106046
2022-02-26 16:26:37 | INFO | train_inner | epoch 021:    771 / 788 loss=5.675, ppl=51.08, wps=10184.5, ups=0.16, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.583, loss_scale=8, train_wall=639, gb_free=4, wall=106689
2022-02-26 16:28:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 16:28:33 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.169 | ppl 71.95 | wps 23537.5 | wpb 2034.1 | bsz 4 | num_updates 16517 | best_loss 6.166
2022-02-26 16:28:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16517 updates
2022-02-26 16:28:33 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-26 16:28:33 | INFO | train | epoch 021 | loss 5.628 | ppl 49.45 | wps 10154 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 16517 | lr 0.000246056 | gnorm 0.564 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 106805
2022-02-26 16:28:33 | INFO | fairseq.trainer | begin training epoch 22
2022-02-26 16:28:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 16:37:27 | INFO | train_inner | epoch 022:     83 / 788 loss=5.557, ppl=47.07, wps=10040.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=16600, lr=0.00024544, gnorm=0.557, loss_scale=8, train_wall=636, gb_free=4, wall=107339
2022-02-26 16:48:11 | INFO | train_inner | epoch 022:    183 / 788 loss=5.558, ppl=47.11, wps=10183.6, ups=0.16, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.57, loss_scale=8, train_wall=639, gb_free=4, wall=107982
2022-02-26 16:57:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:59:01 | INFO | train_inner | epoch 022:    284 / 788 loss=5.586, ppl=48.02, wps=10084.3, ups=0.15, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.553, loss_scale=8, train_wall=645, gb_free=4, wall=108632
2022-02-26 17:09:44 | INFO | train_inner | epoch 022:    384 / 788 loss=5.597, ppl=48.4, wps=10185.6, ups=0.16, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.567, loss_scale=8, train_wall=639, gb_free=4, wall=109276
2022-02-26 17:20:27 | INFO | train_inner | epoch 022:    484 / 788 loss=5.623, ppl=49.28, wps=10185.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.571, loss_scale=8, train_wall=638, gb_free=4, wall=109919
2022-02-26 17:31:11 | INFO | train_inner | epoch 022:    584 / 788 loss=5.635, ppl=49.68, wps=10187.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=110562
2022-02-26 17:41:54 | INFO | train_inner | epoch 022:    684 / 788 loss=5.641, ppl=49.89, wps=10186, ups=0.16, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.558, loss_scale=8, train_wall=638, gb_free=4, wall=111206
2022-02-26 17:52:38 | INFO | train_inner | epoch 022:    784 / 788 loss=5.664, ppl=50.71, wps=10185.6, ups=0.16, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.559, loss_scale=16, train_wall=639, gb_free=4, wall=111849
2022-02-26 17:53:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 17:53:10 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.161 | ppl 71.54 | wps 23558.5 | wpb 2034.1 | bsz 4 | num_updates 17304 | best_loss 6.161
2022-02-26 17:53:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17304 updates
2022-02-26 17:53:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 17:53:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 17:53:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 22 @ 17304 updates, score 6.161) (writing took 6.577608199091628 seconds)
2022-02-26 17:53:16 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-26 17:53:16 | INFO | train | epoch 022 | loss 5.606 | ppl 48.7 | wps 10140.9 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 17304 | lr 0.000240396 | gnorm 0.559 | loss_scale 16 | train_wall 5029 | gb_free 4 | wall 111888
2022-02-26 17:53:16 | INFO | fairseq.trainer | begin training epoch 23
2022-02-26 17:53:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 17:54:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 18:03:40 | INFO | train_inner | epoch 023:     97 / 788 loss=5.51, ppl=45.56, wps=9843.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=17400, lr=0.000239732, gnorm=0.554, loss_scale=8, train_wall=642, gb_free=4, wall=112512
2022-02-26 18:14:24 | INFO | train_inner | epoch 023:    197 / 788 loss=5.532, ppl=46.27, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.568, loss_scale=8, train_wall=638, gb_free=4, wall=113155
2022-02-26 18:25:07 | INFO | train_inner | epoch 023:    297 / 788 loss=5.555, ppl=47.01, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.554, loss_scale=8, train_wall=638, gb_free=4, wall=113799
2022-02-26 18:35:50 | INFO | train_inner | epoch 023:    397 / 788 loss=5.591, ppl=48.2, wps=10188.4, ups=0.16, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.566, loss_scale=8, train_wall=638, gb_free=4, wall=114442
2022-02-26 18:46:34 | INFO | train_inner | epoch 023:    497 / 788 loss=5.604, ppl=48.65, wps=10187.7, ups=0.16, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=115085
2022-02-26 18:52:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 18:57:23 | INFO | train_inner | epoch 023:    598 / 788 loss=5.625, ppl=49.34, wps=10089.6, ups=0.15, wpb=65534.7, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.57, loss_scale=8, train_wall=645, gb_free=4, wall=115735
2022-02-26 19:08:06 | INFO | train_inner | epoch 023:    698 / 788 loss=5.625, ppl=49.36, wps=10190.8, ups=0.16, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=116378
2022-02-26 19:17:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:51 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.164 | ppl 71.72 | wps 23571.7 | wpb 2034.1 | bsz 4 | num_updates 18090 | best_loss 6.161
2022-02-26 19:17:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18090 updates
2022-02-26 19:17:51 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-26 19:17:51 | INFO | train | epoch 023 | loss 5.586 | ppl 48.02 | wps 10143.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 18090 | lr 0.000235115 | gnorm 0.56 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 116963
2022-02-26 19:17:52 | INFO | fairseq.trainer | begin training epoch 24
2022-02-26 19:17:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:56 | INFO | train_inner | epoch 024:     10 / 788 loss=5.636, ppl=49.74, wps=10039, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.577, loss_scale=8, train_wall=636, gb_free=4, wall=117028
2022-02-26 19:29:39 | INFO | train_inner | epoch 024:    110 / 788 loss=5.494, ppl=45.05, wps=10184.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.582, loss_scale=8, train_wall=638, gb_free=4, wall=117671
2022-02-26 19:40:23 | INFO | train_inner | epoch 024:    210 / 788 loss=5.526, ppl=46.06, wps=10185.6, ups=0.16, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=118314
2022-02-26 19:48:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 19:51:13 | INFO | train_inner | epoch 024:    311 / 788 loss=5.558, ppl=47.11, wps=10084.2, ups=0.15, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.565, loss_scale=8, train_wall=645, gb_free=4, wall=118964
2022-02-26 20:01:56 | INFO | train_inner | epoch 024:    411 / 788 loss=5.566, ppl=47.37, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.576, loss_scale=8, train_wall=638, gb_free=4, wall=119608
2022-02-26 20:12:39 | INFO | train_inner | epoch 024:    511 / 788 loss=5.576, ppl=47.71, wps=10185.3, ups=0.16, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.55, loss_scale=8, train_wall=639, gb_free=4, wall=120251
2022-02-26 20:23:23 | INFO | train_inner | epoch 024:    611 / 788 loss=5.595, ppl=48.35, wps=10184.3, ups=0.16, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.578, loss_scale=8, train_wall=639, gb_free=4, wall=120895
2022-02-26 20:34:06 | INFO | train_inner | epoch 024:    711 / 788 loss=5.615, ppl=49.01, wps=10183.6, ups=0.16, wpb=65534.7, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.564, loss_scale=8, train_wall=639, gb_free=4, wall=121538
2022-02-26 20:42:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 20:42:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.162 | ppl 71.59 | wps 23554.8 | wpb 2034.1 | bsz 4 | num_updates 18877 | best_loss 6.161
2022-02-26 20:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18877 updates
2022-02-26 20:42:29 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-26 20:42:29 | INFO | train | epoch 024 | loss 5.567 | ppl 47.42 | wps 10152.9 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 18877 | lr 0.000230162 | gnorm 0.569 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 122040
2022-02-26 20:42:29 | INFO | fairseq.trainer | begin training epoch 25
2022-02-26 20:42:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 20:44:57 | INFO | train_inner | epoch 025:     23 / 788 loss=5.599, ppl=48.46, wps=10035.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.576, loss_scale=16, train_wall=636, gb_free=4, wall=122188
2022-02-26 20:45:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 20:55:46 | INFO | train_inner | epoch 025:    124 / 788 loss=5.48, ppl=44.64, wps=10085.5, ups=0.15, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.568, loss_scale=8, train_wall=645, gb_free=4, wall=122838
2022-02-26 21:06:30 | INFO | train_inner | epoch 025:    224 / 788 loss=5.521, ppl=45.92, wps=10184.7, ups=0.16, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.576, loss_scale=8, train_wall=639, gb_free=4, wall=123482
2022-02-26 21:17:13 | INFO | train_inner | epoch 025:    324 / 788 loss=5.526, ppl=46.09, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.571, loss_scale=8, train_wall=639, gb_free=4, wall=124125
2022-02-26 21:27:57 | INFO | train_inner | epoch 025:    424 / 788 loss=5.552, ppl=46.9, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.577, loss_scale=8, train_wall=638, gb_free=4, wall=124768
2022-02-26 21:38:40 | INFO | train_inner | epoch 025:    524 / 788 loss=5.565, ppl=47.35, wps=10186.2, ups=0.16, wpb=65520.6, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.563, loss_scale=8, train_wall=638, gb_free=4, wall=125412
2022-02-26 21:40:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 21:49:30 | INFO | train_inner | epoch 025:    625 / 788 loss=5.581, ppl=47.88, wps=10084.6, ups=0.15, wpb=65534.7, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.597, loss_scale=8, train_wall=645, gb_free=4, wall=126061
2022-02-26 22:00:13 | INFO | train_inner | epoch 025:    725 / 788 loss=5.593, ppl=48.26, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.603, loss_scale=8, train_wall=638, gb_free=4, wall=126705
2022-02-26 22:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 22:07:05 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.154 | ppl 71.23 | wps 23580 | wpb 2034.1 | bsz 4 | num_updates 19663 | best_loss 6.154
2022-02-26 22:07:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19663 updates
2022-02-26 22:07:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 22:07:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-26 22:07:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 25 @ 19663 updates, score 6.154) (writing took 6.616079810075462 seconds)
2022-02-26 22:07:11 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-26 22:07:11 | INFO | train | epoch 025 | loss 5.549 | ppl 46.82 | wps 10128.5 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 19663 | lr 0.000225515 | gnorm 0.577 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 127123
2022-02-26 22:07:11 | INFO | fairseq.trainer | begin training epoch 26
2022-02-26 22:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 22:11:09 | INFO | train_inner | epoch 026:     37 / 788 loss=5.55, ppl=46.86, wps=9939.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.571, loss_scale=8, train_wall=636, gb_free=4, wall=127361
2022-02-26 22:21:53 | INFO | train_inner | epoch 026:    137 / 788 loss=5.47, ppl=44.31, wps=10188.8, ups=0.16, wpb=65519.3, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.588, loss_scale=8, train_wall=638, gb_free=4, wall=128004
2022-02-26 22:32:36 | INFO | train_inner | epoch 026:    237 / 788 loss=5.501, ppl=45.28, wps=10183.9, ups=0.16, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.571, loss_scale=8, train_wall=639, gb_free=4, wall=128648
2022-02-26 22:37:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 22:43:26 | INFO | train_inner | epoch 026:    338 / 788 loss=5.518, ppl=45.84, wps=10085.3, ups=0.15, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.57, loss_scale=8, train_wall=645, gb_free=4, wall=129298
2022-02-26 22:54:09 | INFO | train_inner | epoch 026:    438 / 788 loss=5.538, ppl=46.45, wps=10184.6, ups=0.16, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.567, loss_scale=8, train_wall=639, gb_free=4, wall=129941
2022-02-26 23:04:53 | INFO | train_inner | epoch 026:    538 / 788 loss=5.554, ppl=46.97, wps=10187.3, ups=0.16, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.58, loss_scale=8, train_wall=638, gb_free=4, wall=130584
2022-02-26 23:15:36 | INFO | train_inner | epoch 026:    638 / 788 loss=5.566, ppl=47.37, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.583, loss_scale=8, train_wall=639, gb_free=4, wall=131228
2022-02-26 23:26:19 | INFO | train_inner | epoch 026:    738 / 788 loss=5.58, ppl=47.82, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.574, loss_scale=8, train_wall=638, gb_free=4, wall=131871
2022-02-26 23:31:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 23:31:47 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.164 | ppl 71.69 | wps 23546.3 | wpb 2034.1 | bsz 4 | num_updates 20450 | best_loss 6.154
2022-02-26 23:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20450 updates
2022-02-26 23:31:47 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-26 23:31:47 | INFO | train | epoch 026 | loss 5.532 | ppl 46.28 | wps 10154.6 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 20450 | lr 0.000221133 | gnorm 0.576 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 132199
2022-02-26 23:31:48 | INFO | fairseq.trainer | begin training epoch 27
2022-02-26 23:31:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 23:33:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 23:37:16 | INFO | train_inner | epoch 027:     51 / 788 loss=5.517, ppl=45.78, wps=9942.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=20500, lr=0.000220863, gnorm=0.573, loss_scale=8, train_wall=642, gb_free=4, wall=132527
2022-02-26 23:47:59 | INFO | train_inner | epoch 027:    151 / 788 loss=5.46, ppl=44, wps=10183.5, ups=0.16, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.574, loss_scale=8, train_wall=639, gb_free=4, wall=133171
2022-02-26 23:58:43 | INFO | train_inner | epoch 027:    251 / 788 loss=5.491, ppl=44.96, wps=10182.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.589, loss_scale=8, train_wall=639, gb_free=4, wall=133814
2022-02-27 00:09:26 | INFO | train_inner | epoch 027:    351 / 788 loss=5.508, ppl=45.51, wps=10184.1, ups=0.16, wpb=65520.6, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.581, loss_scale=8, train_wall=638, gb_free=4, wall=134458
2022-02-27 00:20:10 | INFO | train_inner | epoch 027:    451 / 788 loss=5.512, ppl=45.64, wps=10184.8, ups=0.16, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.594, loss_scale=8, train_wall=639, gb_free=4, wall=135101
2022-02-27 00:30:53 | INFO | train_inner | epoch 027:    551 / 788 loss=5.538, ppl=46.46, wps=10185, ups=0.16, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.591, loss_scale=16, train_wall=639, gb_free=4, wall=135745
2022-02-27 00:34:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 00:41:43 | INFO | train_inner | epoch 027:    652 / 788 loss=5.562, ppl=47.25, wps=10086, ups=0.15, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.573, loss_scale=8, train_wall=645, gb_free=4, wall=136394
2022-02-27 00:52:26 | INFO | train_inner | epoch 027:    752 / 788 loss=5.57, ppl=47.5, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.58, loss_scale=8, train_wall=638, gb_free=4, wall=137038
2022-02-27 00:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 00:56:24 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.155 | ppl 71.27 | wps 23535.9 | wpb 2034.1 | bsz 4 | num_updates 21236 | best_loss 6.154
2022-02-27 00:56:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21236 updates
2022-02-27 00:56:24 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-27 00:56:24 | INFO | train | epoch 027 | loss 5.517 | ppl 45.81 | wps 10140.5 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 21236 | lr 0.000217002 | gnorm 0.583 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 137276
2022-02-27 00:56:24 | INFO | fairseq.trainer | begin training epoch 28
2022-02-27 00:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 01:03:16 | INFO | train_inner | epoch 028:     64 / 788 loss=5.482, ppl=44.68, wps=10039.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=21300, lr=0.000216676, gnorm=0.583, loss_scale=8, train_wall=636, gb_free=4, wall=137688
2022-02-27 01:13:59 | INFO | train_inner | epoch 028:    164 / 788 loss=5.45, ppl=43.71, wps=10188, ups=0.16, wpb=65520.6, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.579, loss_scale=8, train_wall=638, gb_free=4, wall=138331
2022-02-27 01:24:42 | INFO | train_inner | epoch 028:    264 / 788 loss=5.468, ppl=44.26, wps=10188.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.578, loss_scale=8, train_wall=638, gb_free=4, wall=138974
2022-02-27 01:35:26 | INFO | train_inner | epoch 028:    364 / 788 loss=5.489, ppl=44.92, wps=10183.9, ups=0.16, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.571, loss_scale=16, train_wall=639, gb_free=4, wall=139618
2022-02-27 01:36:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 01:46:16 | INFO | train_inner | epoch 028:    465 / 788 loss=5.518, ppl=45.81, wps=10085.1, ups=0.15, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.593, loss_scale=8, train_wall=645, gb_free=4, wall=140267
2022-02-27 01:56:59 | INFO | train_inner | epoch 028:    565 / 788 loss=5.532, ppl=46.28, wps=10187.3, ups=0.16, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.573, loss_scale=8, train_wall=638, gb_free=4, wall=140911
2022-02-27 02:07:43 | INFO | train_inner | epoch 028:    665 / 788 loss=5.541, ppl=46.56, wps=10184.5, ups=0.16, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.597, loss_scale=8, train_wall=639, gb_free=4, wall=141554
2022-02-27 02:18:26 | INFO | train_inner | epoch 028:    765 / 788 loss=5.558, ppl=47.11, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.608, loss_scale=8, train_wall=638, gb_free=4, wall=142197
2022-02-27 02:20:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 02:21:00 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.152 | ppl 71.09 | wps 23522.2 | wpb 2034.1 | bsz 4 | num_updates 22023 | best_loss 6.152
2022-02-27 02:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 22023 updates
2022-02-27 02:21:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-27 02:21:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt
2022-02-27 02:21:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#4/checkpoint_best.pt (epoch 28 @ 22023 updates, score 6.152) (writing took 6.648373448988423 seconds)
2022-02-27 02:21:07 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-27 02:21:07 | INFO | train | epoch 028 | loss 5.503 | ppl 45.34 | wps 10141.6 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 22023 | lr 0.000213089 | gnorm 0.584 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 142359
2022-02-27 02:21:07 | INFO | fairseq.trainer | begin training epoch 29
2022-02-27 02:21:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 02:29:23 | INFO | train_inner | epoch 029:     77 / 788 loss=5.445, ppl=43.56, wps=9935.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22100, lr=0.000212718, gnorm=0.57, loss_scale=8, train_wall=636, gb_free=4, wall=142854
2022-02-27 02:34:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 02:40:13 | INFO | train_inner | epoch 029:    178 / 788 loss=5.44, ppl=43.42, wps=10083.3, ups=0.15, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.579, loss_scale=8, train_wall=645, gb_free=4, wall=143504
2022-02-27 02:50:56 | INFO | train_inner | epoch 029:    278 / 788 loss=5.453, ppl=43.81, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.586, loss_scale=8, train_wall=638, gb_free=4, wall=144148
2022-02-27 03:01:39 | INFO | train_inner | epoch 029:    378 / 788 loss=5.489, ppl=44.92, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.586, loss_scale=8, train_wall=639, gb_free=4, wall=144791
2022-02-27 03:12:23 | INFO | train_inner | epoch 029:    478 / 788 loss=5.495, ppl=45.08, wps=10185.4, ups=0.16, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.592, loss_scale=8, train_wall=639, gb_free=4, wall=145434
2022-02-27 03:23:06 | INFO | train_inner | epoch 029:    578 / 788 loss=5.52, ppl=45.88, wps=10182, ups=0.16, wpb=65534.7, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.579, loss_scale=8, train_wall=639, gb_free=4, wall=146078
2022-02-27 03:31:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 03:33:56 | INFO | train_inner | epoch 029:    679 / 788 loss=5.533, ppl=46.31, wps=10087.5, ups=0.15, wpb=65520.6, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.598, loss_scale=8, train_wall=645, gb_free=4, wall=146728
2022-02-27 03:43:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-27 03:44:46 | INFO | train_inner | epoch 029:    780 / 788 loss=5.541, ppl=46.58, wps=10086, ups=0.15, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.604, loss_scale=4, train_wall=645, gb_free=4, wall=147377
2022-02-27 03:45:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 03:45:44 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.156 | ppl 71.28 | wps 23563.8 | wpb 2034.1 | bsz 4 | num_updates 22808 | best_loss 6.152
2022-02-27 03:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22808 updates
2022-02-27 03:45:44 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-27 03:45:44 | INFO | train | epoch 029 | loss 5.489 | ppl 44.9 | wps 10128 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 22808 | lr 0.00020939 | gnorm 0.588 | loss_scale 4 | train_wall 5029 | gb_free 4 | wall 147435
2022-02-27 03:45:44 | INFO | fairseq.trainer | begin training epoch 30
2022-02-27 03:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 03:55:35 | INFO | train_inner | epoch 030:     92 / 788 loss=5.418, ppl=42.74, wps=10040.7, ups=0.15, wpb=65248, bsz=127.4, num_updates=22900, lr=0.000208969, gnorm=0.596, loss_scale=4, train_wall=636, gb_free=4, wall=148027
2022-02-27 04:06:19 | INFO | train_inner | epoch 030:    192 / 788 loss=5.43, ppl=43.1, wps=10187.6, ups=0.16, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.592, loss_scale=4, train_wall=638, gb_free=4, wall=148670
2022-02-27 04:17:02 | INFO | train_inner | epoch 030:    292 / 788 loss=5.448, ppl=43.66, wps=10188.5, ups=0.16, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.6, loss_scale=4, train_wall=638, gb_free=4, wall=149314
2022-02-27 04:27:45 | INFO | train_inner | epoch 030:    392 / 788 loss=5.468, ppl=44.26, wps=10187.6, ups=0.16, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.591, loss_scale=4, train_wall=638, gb_free=4, wall=149957
2022-02-27 04:38:28 | INFO | train_inner | epoch 030:    492 / 788 loss=5.495, ppl=45.11, wps=10191.1, ups=0.16, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.589, loss_scale=8, train_wall=638, gb_free=4, wall=150600
2022-02-27 04:49:12 | INFO | train_inner | epoch 030:    592 / 788 loss=5.505, ppl=45.42, wps=10188.7, ups=0.16, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.596, loss_scale=8, train_wall=638, gb_free=4, wall=151243
2022-02-27 04:59:55 | INFO | train_inner | epoch 030:    692 / 788 loss=5.521, ppl=45.91, wps=10186.7, ups=0.16, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.604, loss_scale=8, train_wall=638, gb_free=4, wall=151887
2022-02-27 05:10:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 05:10:19 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.155 | ppl 71.24 | wps 23558.9 | wpb 2034.1 | bsz 4 | num_updates 23596 | best_loss 6.152
2022-02-27 05:10:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23596 updates
2022-02-27 05:10:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-27 05:10:19 | INFO | train | epoch 030 | loss 5.476 | ppl 44.5 | wps 10168.9 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 23596 | lr 0.000205864 | gnorm 0.596 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 152511
2022-02-27 05:10:19 | INFO | fairseq.trainer | begin training epoch 31
2022-02-27 05:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 05:10:45 | INFO | train_inner | epoch 031:      4 / 788 loss=5.526, ppl=46.07, wps=10037.3, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.607, loss_scale=8, train_wall=636, gb_free=4, wall=152536
2022-02-27 05:21:28 | INFO | train_inner | epoch 031:    104 / 788 loss=5.394, ppl=42.06, wps=10185.5, ups=0.16, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.598, loss_scale=8, train_wall=639, gb_free=4, wall=153180
2022-02-27 05:32:12 | INFO | train_inner | epoch 031:    204 / 788 loss=5.41, ppl=42.52, wps=10185.8, ups=0.16, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.589, loss_scale=8, train_wall=638, gb_free=4, wall=153823
2022-02-27 05:35:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 05:43:01 | INFO | train_inner | epoch 031:    305 / 788 loss=5.441, ppl=43.45, wps=10087.4, ups=0.15, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.597, loss_scale=8, train_wall=645, gb_free=4, wall=154473
2022-02-27 05:53:45 | INFO | train_inner | epoch 031:    405 / 788 loss=5.456, ppl=43.9, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.604, loss_scale=8, train_wall=638, gb_free=4, wall=155116
2022-02-27 06:04:28 | INFO | train_inner | epoch 031:    505 / 788 loss=5.491, ppl=44.97, wps=10188.5, ups=0.16, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.606, loss_scale=8, train_wall=638, gb_free=4, wall=155760
2022-02-27 06:15:11 | INFO | train_inner | epoch 031:    605 / 788 loss=5.491, ppl=44.97, wps=10185.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.602, loss_scale=8, train_wall=638, gb_free=4, wall=156403
2022-02-27 06:25:54 | INFO | train_inner | epoch 031:    705 / 788 loss=5.511, ppl=45.6, wps=10186.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.602, loss_scale=8, train_wall=638, gb_free=4, wall=157046
2022-02-27 06:31:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 06:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 06:34:55 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.154 | ppl 71.22 | wps 23666.4 | wpb 2034.1 | bsz 4 | num_updates 24382 | best_loss 6.152
2022-02-27 06:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24382 updates
2022-02-27 06:34:55 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-27 06:34:55 | INFO | train | epoch 031 | loss 5.463 | ppl 44.12 | wps 10142.5 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 24382 | lr 0.000202519 | gnorm 0.6 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 157586
2022-02-27 06:34:55 | INFO | fairseq.trainer | begin training epoch 32
2022-02-27 06:34:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 06:36:51 | INFO | train_inner | epoch 032:     18 / 788 loss=5.491, ppl=44.97, wps=9943.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.593, loss_scale=8, train_wall=642, gb_free=4, wall=157702
2022-02-27 06:47:34 | INFO | train_inner | epoch 032:    118 / 788 loss=5.383, ppl=41.74, wps=10185.7, ups=0.16, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.589, loss_scale=8, train_wall=638, gb_free=4, wall=158346
2022-02-27 06:58:17 | INFO | train_inner | epoch 032:    218 / 788 loss=5.407, ppl=42.43, wps=10187.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.599, loss_scale=8, train_wall=638, gb_free=4, wall=158989
2022-02-27 07:09:01 | INFO | train_inner | epoch 032:    318 / 788 loss=5.432, ppl=43.18, wps=10185.2, ups=0.16, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.599, loss_scale=8, train_wall=639, gb_free=4, wall=159632
2022-02-27 07:19:44 | INFO | train_inner | epoch 032:    418 / 788 loss=5.449, ppl=43.68, wps=10186.8, ups=0.16, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.585, loss_scale=8, train_wall=638, gb_free=4, wall=160276
2022-02-27 07:30:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 07:30:35 | INFO | train_inner | epoch 032:    519 / 788 loss=5.475, ppl=44.48, wps=10068, ups=0.15, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.612, loss_scale=8, train_wall=646, gb_free=4, wall=160927
2022-02-27 07:41:19 | INFO | train_inner | epoch 032:    619 / 788 loss=5.494, ppl=45.07, wps=10183.7, ups=0.16, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.603, loss_scale=8, train_wall=639, gb_free=4, wall=161570
2022-02-27 07:52:03 | INFO | train_inner | epoch 032:    719 / 788 loss=5.5, ppl=45.25, wps=10168.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.6, loss_scale=8, train_wall=639, gb_free=4, wall=162215
2022-02-27 07:59:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 07:59:34 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.162 | ppl 71.63 | wps 23514 | wpb 2034.1 | bsz 4 | num_updates 25169 | best_loss 6.152
2022-02-27 07:59:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25169 updates
2022-02-27 07:59:34 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-27 07:59:34 | INFO | train | epoch 032 | loss 5.452 | ppl 43.77 | wps 10147.7 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 25169 | lr 0.000199327 | gnorm 0.598 | loss_scale 8 | train_wall 5031 | gb_free 4 | wall 162666
2022-02-27 07:59:34 | INFO | fairseq.trainer | begin training epoch 33
2022-02-27 07:59:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 08:02:54 | INFO | train_inner | epoch 033:     31 / 788 loss=5.472, ppl=44.39, wps=10023.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.594, loss_scale=8, train_wall=637, gb_free=4, wall=162866
2022-02-27 08:13:37 | INFO | train_inner | epoch 033:    131 / 788 loss=5.377, ppl=41.56, wps=10185.1, ups=0.16, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.608, loss_scale=8, train_wall=639, gb_free=4, wall=163509
2022-02-27 08:24:21 | INFO | train_inner | epoch 033:    231 / 788 loss=5.4, ppl=42.22, wps=10187.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.602, loss_scale=8, train_wall=638, gb_free=4, wall=164152
2022-02-27 08:25:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 08:35:11 | INFO | train_inner | epoch 033:    332 / 788 loss=5.423, ppl=42.91, wps=10085.8, ups=0.15, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.593, loss_scale=8, train_wall=645, gb_free=4, wall=164802
2022-02-27 08:45:54 | INFO | train_inner | epoch 033:    432 / 788 loss=5.452, ppl=43.76, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.598, loss_scale=8, train_wall=639, gb_free=4, wall=165445
2022-02-27 08:56:37 | INFO | train_inner | epoch 033:    532 / 788 loss=5.456, ppl=43.88, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.611, loss_scale=8, train_wall=638, gb_free=4, wall=166089
2022-02-27 09:07:20 | INFO | train_inner | epoch 033:    632 / 788 loss=5.467, ppl=44.23, wps=10187.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.617, loss_scale=8, train_wall=638, gb_free=4, wall=166732
2022-02-27 09:18:04 | INFO | train_inner | epoch 033:    732 / 788 loss=5.498, ppl=45.2, wps=10188.4, ups=0.16, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.594, loss_scale=8, train_wall=638, gb_free=4, wall=167375
2022-02-27 09:20:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 09:24:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 09:24:10 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.155 | ppl 71.25 | wps 23529.6 | wpb 2034.1 | bsz 4 | num_updates 25955 | best_loss 6.152
2022-02-27 09:24:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25955 updates
2022-02-27 09:24:10 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-27 09:24:10 | INFO | train | epoch 033 | loss 5.441 | ppl 43.45 | wps 10142.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 25955 | lr 0.000196286 | gnorm 0.602 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 167742
2022-02-27 09:24:10 | INFO | fairseq.trainer | begin training epoch 34
2022-02-27 09:24:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 09:29:00 | INFO | train_inner | epoch 034:     45 / 788 loss=5.441, ppl=43.44, wps=9943.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=26000, lr=0.000196116, gnorm=0.592, loss_scale=8, train_wall=642, gb_free=4, wall=168031
2022-02-27 09:39:43 | INFO | train_inner | epoch 034:    145 / 788 loss=5.374, ppl=41.48, wps=10186.4, ups=0.16, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.592, loss_scale=8, train_wall=639, gb_free=4, wall=168675
2022-02-27 09:50:27 | INFO | train_inner | epoch 034:    245 / 788 loss=5.395, ppl=42.09, wps=10184.4, ups=0.16, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.619, loss_scale=8, train_wall=639, gb_free=4, wall=169318
2022-02-27 10:01:11 | INFO | train_inner | epoch 034:    345 / 788 loss=5.424, ppl=42.92, wps=10176.8, ups=0.16, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.605, loss_scale=8, train_wall=639, gb_free=4, wall=169962
2022-02-27 10:11:54 | INFO | train_inner | epoch 034:    445 / 788 loss=5.427, ppl=43.02, wps=10189.1, ups=0.16, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.6, loss_scale=8, train_wall=638, gb_free=4, wall=170605
2022-02-27 10:21:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 10:22:43 | INFO | train_inner | epoch 034:    546 / 788 loss=5.448, ppl=43.64, wps=10087.9, ups=0.15, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.61, loss_scale=8, train_wall=645, gb_free=4, wall=171255
2022-02-27 10:33:26 | INFO | train_inner | epoch 034:    646 / 788 loss=5.471, ppl=44.35, wps=10188.2, ups=0.16, wpb=65519.3, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.601, loss_scale=8, train_wall=638, gb_free=4, wall=171898
2022-02-27 10:44:10 | INFO | train_inner | epoch 034:    746 / 788 loss=5.481, ppl=44.67, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.62, loss_scale=8, train_wall=638, gb_free=4, wall=172541
User defined signal 2
