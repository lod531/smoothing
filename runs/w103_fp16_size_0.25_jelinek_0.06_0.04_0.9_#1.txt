Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 206859323: <w103_fp16_size_0.25_jelinek_0.06_0.04_0.9_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.06_0.04_0.9_#1> was submitted from host <eu-login-36> by user <andriusb> in cluster <euler> at Wed Mar  2 08:33:39 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Wed Mar  2 10:37:06 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar  2 10:37:06 2022
Terminated at Thu Mar  3 09:44:41 2022
Results reported at Thu Mar  3 09:44:41 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.06, 0.04, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321671 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   83208.52 sec.
    Max Memory :                                 8547 MB
    Average Memory :                             3162.16 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11453.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   83255 sec.
    Turnaround time :                            90662 sec.

The output (if any) follows:

2022-03-02 10:37:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.06, 0.04, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-02 10:37:16 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-02 10:37:20 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 678/450337 [00:00<01:06, 6770.19it/s]  0%|          | 1356/450337 [00:00<01:16, 5890.91it/s]  0%|          | 1953/450337 [00:00<01:19, 5647.96it/s]  1%|          | 2524/450337 [00:00<01:19, 5664.34it/s]  1%|          | 3223/450337 [00:00<01:13, 6117.36it/s]  1%|          | 3839/450337 [00:00<01:13, 6074.50it/s]  1%|          | 4551/450337 [00:00<01:09, 6405.51it/s]  1%|          | 5256/450337 [00:00<01:07, 6603.37it/s]  1%|▏         | 5951/450337 [00:00<01:06, 6703.80it/s]  1%|▏         | 6624/450337 [00:01<01:12, 6118.16it/s]  2%|▏         | 7247/450337 [00:01<01:12, 6121.77it/s]  2%|▏         | 7867/450337 [00:01<01:13, 6059.41it/s]  2%|▏         | 8478/450337 [00:01<01:15, 5871.08it/s]  2%|▏         | 9123/450337 [00:01<01:13, 6032.79it/s]  2%|▏         | 9731/450337 [00:01<01:14, 5948.49it/s]  2%|▏         | 10374/450337 [00:01<01:12, 6084.73it/s]  2%|▏         | 10985/450337 [00:01<01:13, 6007.69it/s]  3%|▎         | 11588/450337 [00:01<01:14, 5914.00it/s]  3%|▎         | 12221/450337 [00:02<01:12, 6031.28it/s]  3%|▎         | 12826/450337 [00:02<01:12, 6011.73it/s]  3%|▎         | 13429/450337 [00:02<01:13, 5974.36it/s]  3%|▎         | 14058/450337 [00:02<01:11, 6063.23it/s]  3%|▎         | 14686/450337 [00:02<01:11, 6125.79it/s]  3%|▎         | 15334/450337 [00:02<01:09, 6228.93it/s]  4%|▎         | 15958/450337 [00:02<01:13, 5933.87it/s]  4%|▎         | 16555/450337 [00:02<01:13, 5875.38it/s]  4%|▍         | 17171/450337 [00:02<01:12, 5950.49it/s]  4%|▍         | 17798/450337 [00:02<01:11, 6039.47it/s]  4%|▍         | 18404/450337 [00:03<01:12, 5954.00it/s]  4%|▍         | 19100/450337 [00:03<01:09, 6239.43it/s]  4%|▍         | 19768/450337 [00:03<01:07, 6367.33it/s]  5%|▍         | 20406/450337 [00:03<01:11, 6039.39it/s]  5%|▍         | 21040/450337 [00:03<01:10, 6123.70it/s]  5%|▍         | 21656/450337 [00:03<01:13, 5865.10it/s]  5%|▍         | 22316/450337 [00:03<01:10, 6056.62it/s]  5%|▌         | 22976/450337 [00:03<01:08, 6200.34it/s]  5%|▌         | 23650/450337 [00:03<01:07, 6351.45it/s]  5%|▌         | 24359/450337 [00:03<01:04, 6565.41it/s]  6%|▌         | 25052/450337 [00:04<01:03, 6672.14it/s]  6%|▌         | 25721/450337 [00:04<01:05, 6508.32it/s]  6%|▌         | 26374/450337 [00:04<01:09, 6119.95it/s]  6%|▌         | 26992/450337 [00:04<01:10, 5997.21it/s]  6%|▌         | 27596/450337 [00:04<01:12, 5827.85it/s]  6%|▋         | 28229/450337 [00:04<01:10, 5967.38it/s]  6%|▋         | 28903/450337 [00:04<01:08, 6188.05it/s]  7%|▋         | 29525/450337 [00:04<01:10, 5986.92it/s]  7%|▋         | 30147/450337 [00:04<01:09, 6053.14it/s]  7%|▋         | 30755/450337 [00:05<01:12, 5779.05it/s]  7%|▋         | 31340/450337 [00:05<01:12, 5791.21it/s]  7%|▋         | 31922/450337 [00:05<01:12, 5795.67it/s]  7%|▋         | 32504/450337 [00:05<01:12, 5772.76it/s]  7%|▋         | 33083/450337 [00:05<01:15, 5563.16it/s]  7%|▋         | 33692/450337 [00:05<01:12, 5714.27it/s]  8%|▊         | 34271/450337 [00:05<01:12, 5735.90it/s]  8%|▊         | 34986/450337 [00:05<01:07, 6147.08it/s]  8%|▊         | 35603/450337 [00:05<01:09, 6002.99it/s]  8%|▊         | 36206/450337 [00:05<01:09, 5978.33it/s]  8%|▊         | 36806/450337 [00:06<01:09, 5951.74it/s]  8%|▊         | 37403/450337 [00:06<01:13, 5630.20it/s]  8%|▊         | 37970/450337 [00:06<01:13, 5610.48it/s]  9%|▊         | 38562/450337 [00:06<01:12, 5699.20it/s]  9%|▊         | 39197/450337 [00:06<01:09, 5887.05it/s]  9%|▉         | 39788/450337 [00:06<01:10, 5830.05it/s]  9%|▉         | 40458/450337 [00:06<01:07, 6081.88it/s]  9%|▉         | 41068/450337 [00:06<01:07, 6061.25it/s]  9%|▉         | 41676/450337 [00:06<01:08, 5927.34it/s]  9%|▉         | 42271/450337 [00:07<01:11, 5689.54it/s] 10%|▉         | 42843/450337 [00:07<01:11, 5674.46it/s] 10%|▉         | 43413/450337 [00:07<01:12, 5594.78it/s] 10%|▉         | 44067/450337 [00:07<01:09, 5867.95it/s] 10%|▉         | 44738/450337 [00:07<01:06, 6101.43it/s] 10%|█         | 45350/450337 [00:07<01:07, 5993.86it/s] 10%|█         | 46035/450337 [00:07<01:04, 6242.67it/s] 10%|█         | 46685/450337 [00:07<01:03, 6317.04it/s] 11%|█         | 47570/450337 [00:07<00:57, 7058.97it/s] 11%|█         | 48296/450337 [00:07<00:56, 7107.31it/s] 11%|█         | 49009/450337 [00:08<00:57, 6952.02it/s] 11%|█         | 49706/450337 [00:08<00:58, 6876.34it/s] 11%|█         | 50395/450337 [00:08<01:02, 6348.67it/s] 11%|█▏        | 51038/450337 [00:08<01:03, 6261.75it/s] 11%|█▏        | 51672/450337 [00:08<01:03, 6278.00it/s] 12%|█▏        | 52487/450337 [00:08<00:58, 6811.30it/s] 12%|█▏        | 53174/450337 [00:08<01:00, 6538.94it/s] 12%|█▏        | 53834/450337 [00:08<01:02, 6367.17it/s] 12%|█▏        | 54475/450337 [00:08<01:06, 5927.13it/s] 12%|█▏        | 55109/450337 [00:09<01:05, 6037.29it/s] 12%|█▏        | 55780/450337 [00:09<01:03, 6225.34it/s] 13%|█▎        | 56409/450337 [00:09<01:03, 6179.04it/s] 13%|█▎        | 57031/450337 [00:09<01:07, 5836.80it/s] 13%|█▎        | 57621/450337 [00:09<01:07, 5789.79it/s] 13%|█▎        | 58240/450337 [00:09<01:06, 5898.52it/s] 13%|█▎        | 58958/450337 [00:09<01:02, 6262.56it/s] 13%|█▎        | 59589/450337 [00:09<01:04, 6014.28it/s] 13%|█▎        | 60195/450337 [00:09<01:05, 5927.89it/s] 14%|█▎        | 60924/450337 [00:09<01:01, 6312.97it/s] 14%|█▎        | 61580/450337 [00:10<01:00, 6378.22it/s] 14%|█▍        | 62221/450337 [00:10<01:02, 6170.05it/s] 14%|█▍        | 62859/450337 [00:10<01:02, 6226.00it/s] 14%|█▍        | 63485/450337 [00:10<01:03, 6126.45it/s] 14%|█▍        | 64100/450337 [00:10<01:03, 6043.25it/s] 14%|█▍        | 64706/450337 [00:10<01:04, 5933.67it/s] 15%|█▍        | 65301/450337 [00:10<01:06, 5829.90it/s] 15%|█▍        | 65970/450337 [00:10<01:03, 6076.55it/s] 15%|█▍        | 66620/450337 [00:10<01:01, 6196.27it/s] 15%|█▍        | 67242/450337 [00:11<01:03, 5999.35it/s] 15%|█▌        | 67844/450337 [00:11<01:06, 5718.40it/s] 15%|█▌        | 68436/450337 [00:11<01:06, 5773.85it/s] 15%|█▌        | 69172/450337 [00:11<01:01, 6225.10it/s] 16%|█▌        | 69851/450337 [00:11<00:59, 6389.26it/s] 16%|█▌        | 70494/450337 [00:11<00:59, 6382.99it/s] 16%|█▌        | 71135/450337 [00:11<01:02, 6071.38it/s] 16%|█▌        | 71750/450337 [00:11<01:02, 6092.36it/s] 16%|█▌        | 72366/450337 [00:11<01:01, 6109.22it/s] 16%|█▌        | 73128/450337 [00:11<00:57, 6546.04it/s] 16%|█▋        | 73820/450337 [00:12<00:56, 6651.48it/s] 17%|█▋        | 74598/450337 [00:12<00:53, 6985.03it/s] 17%|█▋        | 75299/450337 [00:12<00:56, 6664.24it/s] 17%|█▋        | 75970/450337 [00:12<00:57, 6504.75it/s] 17%|█▋        | 76624/450337 [00:12<00:57, 6472.18it/s] 17%|█▋        | 77274/450337 [00:12<00:59, 6224.17it/s] 17%|█▋        | 77963/450337 [00:12<00:58, 6412.86it/s] 17%|█▋        | 78608/450337 [00:12<01:02, 5984.86it/s] 18%|█▊        | 79214/450337 [00:12<01:05, 5654.01it/s] 18%|█▊        | 79805/450337 [00:13<01:04, 5723.47it/s] 18%|█▊        | 80400/450337 [00:13<01:04, 5778.99it/s] 18%|█▊        | 81030/450337 [00:13<01:02, 5927.01it/s] 18%|█▊        | 81683/450337 [00:13<01:00, 6097.10it/s] 18%|█▊        | 82300/450337 [00:13<01:00, 6112.98it/s] 18%|█▊        | 82914/450337 [00:13<01:00, 6040.61it/s] 19%|█▊        | 83593/450337 [00:13<00:58, 6259.00it/s] 19%|█▊        | 84221/450337 [00:13<00:59, 6154.05it/s] 19%|█▉        | 84896/450337 [00:13<00:57, 6326.06it/s] 19%|█▉        | 85531/450337 [00:14<01:00, 6051.86it/s] 19%|█▉        | 86140/450337 [00:14<01:00, 6035.69it/s] 19%|█▉        | 86803/450337 [00:14<00:58, 6206.57it/s] 19%|█▉        | 87482/450337 [00:14<00:56, 6377.33it/s] 20%|█▉        | 88157/450337 [00:14<00:55, 6485.43it/s] 20%|█▉        | 88808/450337 [00:14<01:00, 5940.20it/s] 20%|█▉        | 89412/450337 [00:14<01:02, 5796.72it/s] 20%|█▉        | 89999/450337 [00:14<01:02, 5784.44it/s] 20%|██        | 90583/450337 [00:14<01:02, 5784.20it/s] 20%|██        | 91274/450337 [00:14<00:58, 6099.98it/s] 20%|██        | 91888/450337 [00:15<00:59, 6056.50it/s] 21%|██        | 92583/450337 [00:15<00:56, 6311.94it/s] 21%|██        | 93217/450337 [00:15<00:59, 5983.22it/s] 21%|██        | 93832/450337 [00:15<00:59, 6026.55it/s] 21%|██        | 94450/450337 [00:15<00:58, 6067.94it/s] 21%|██        | 95150/450337 [00:15<00:56, 6339.67it/s] 21%|██▏       | 95787/450337 [00:15<00:58, 6045.91it/s] 21%|██▏       | 96525/450337 [00:15<00:55, 6424.59it/s] 22%|██▏       | 97261/450337 [00:15<00:52, 6695.16it/s] 22%|██▏       | 97935/450337 [00:16<00:56, 6286.72it/s] 22%|██▏       | 98572/450337 [00:16<00:57, 6137.77it/s] 22%|██▏       | 99192/450337 [00:16<00:57, 6110.22it/s] 22%|██▏       | 99807/450337 [00:16<00:58, 5955.48it/s] 22%|██▏       | 100406/450337 [00:16<01:00, 5788.67it/s] 22%|██▏       | 101053/450337 [00:16<00:58, 5980.16it/s] 23%|██▎       | 101660/450337 [00:16<00:58, 6003.29it/s] 23%|██▎       | 102315/450337 [00:16<00:56, 6162.14it/s] 23%|██▎       | 102934/450337 [00:16<00:57, 6093.60it/s] 23%|██▎       | 103545/450337 [00:16<00:58, 5896.42it/s] 23%|██▎       | 104137/450337 [00:17<00:59, 5770.62it/s] 23%|██▎       | 104788/450337 [00:17<00:57, 5972.42it/s] 23%|██▎       | 105388/450337 [00:17<00:57, 5950.97it/s] 24%|██▎       | 106019/450337 [00:17<00:56, 6054.62it/s] 24%|██▎       | 106626/450337 [00:17<00:57, 6024.11it/s] 24%|██▍       | 107230/450337 [00:17<00:58, 5868.82it/s] 24%|██▍       | 107851/450337 [00:17<00:57, 5966.03it/s] 24%|██▍       | 108461/450337 [00:17<00:56, 6002.34it/s] 24%|██▍       | 109063/450337 [00:17<00:59, 5735.15it/s] 24%|██▍       | 109640/450337 [00:18<00:59, 5731.39it/s] 24%|██▍       | 110258/450337 [00:18<00:58, 5861.15it/s] 25%|██▍       | 110858/450337 [00:18<00:57, 5898.71it/s] 25%|██▍       | 111517/450337 [00:18<00:55, 6098.06it/s] 25%|██▍       | 112129/450337 [00:18<00:56, 6009.70it/s] 25%|██▌       | 112791/450337 [00:18<00:54, 6178.12it/s] 25%|██▌       | 113410/450337 [00:18<00:55, 6024.92it/s] 25%|██▌       | 114014/450337 [00:18<00:56, 5981.39it/s] 25%|██▌       | 114614/450337 [00:18<00:56, 5942.62it/s] 26%|██▌       | 115224/450337 [00:18<00:55, 5986.78it/s] 26%|██▌       | 115824/450337 [00:19<00:58, 5674.42it/s] 26%|██▌       | 116399/450337 [00:19<00:58, 5694.27it/s] 26%|██▌       | 116991/450337 [00:19<00:57, 5756.17it/s] 26%|██▌       | 117573/450337 [00:19<00:57, 5770.85it/s] 26%|██▌       | 118152/450337 [00:19<00:57, 5746.32it/s] 26%|██▋       | 118792/450337 [00:19<00:55, 5935.77it/s] 27%|██▋       | 119387/450337 [00:19<00:56, 5821.93it/s] 27%|██▋       | 120036/450337 [00:19<00:54, 6017.83it/s] 27%|██▋       | 120733/450337 [00:19<00:52, 6298.51it/s] 27%|██▋       | 121365/450337 [00:19<00:53, 6191.86it/s] 27%|██▋       | 121986/450337 [00:20<00:55, 5890.12it/s] 27%|██▋       | 122629/450337 [00:20<00:54, 6043.38it/s] 27%|██▋       | 123237/450337 [00:20<00:55, 5846.01it/s] 28%|██▊       | 123867/450337 [00:20<00:54, 5971.47it/s] 28%|██▊       | 124467/450337 [00:20<00:55, 5920.67it/s] 28%|██▊       | 125089/450337 [00:20<00:54, 6004.84it/s] 28%|██▊       | 125708/450337 [00:20<00:53, 6056.42it/s] 28%|██▊       | 126374/450337 [00:20<00:52, 6225.23it/s] 28%|██▊       | 127034/450337 [00:20<00:51, 6336.09it/s] 28%|██▊       | 127669/450337 [00:21<00:53, 6047.41it/s] 28%|██▊       | 128286/450337 [00:21<00:52, 6082.52it/s] 29%|██▊       | 128897/450337 [00:21<00:53, 6039.46it/s] 29%|██▉       | 129557/450337 [00:21<00:51, 6197.08it/s] 29%|██▉       | 130179/450337 [00:21<00:55, 5807.45it/s] 29%|██▉       | 130766/450337 [00:21<00:54, 5816.15it/s] 29%|██▉       | 131379/450337 [00:21<00:54, 5905.62it/s] 29%|██▉       | 131973/450337 [00:21<00:59, 5367.97it/s] 29%|██▉       | 132664/450337 [00:21<00:54, 5789.05it/s] 30%|██▉       | 133329/450337 [00:21<00:52, 6024.17it/s] 30%|██▉       | 133973/450337 [00:22<00:51, 6141.89it/s] 30%|██▉       | 134655/450337 [00:22<00:49, 6336.98it/s] 30%|███       | 135404/450337 [00:22<00:47, 6672.07it/s] 30%|███       | 136076/450337 [00:22<00:48, 6452.69it/s] 30%|███       | 136810/450337 [00:22<00:46, 6706.73it/s] 31%|███       | 137513/450337 [00:22<00:46, 6795.90it/s] 31%|███       | 138196/450337 [00:22<00:48, 6373.23it/s] 31%|███       | 138841/450337 [00:22<00:49, 6294.74it/s] 31%|███       | 139476/450337 [00:22<00:50, 6177.07it/s] 31%|███       | 140098/450337 [00:23<00:50, 6133.48it/s] 31%|███       | 140718/450337 [00:23<00:50, 6143.33it/s] 31%|███▏      | 141334/450337 [00:23<00:52, 5837.12it/s] 32%|███▏      | 141986/450337 [00:23<00:51, 6027.93it/s] 32%|███▏      | 142593/450337 [00:23<00:52, 5892.29it/s] 32%|███▏      | 143186/450337 [00:23<00:53, 5735.69it/s] 32%|███▏      | 143972/450337 [00:23<00:48, 6338.64it/s] 32%|███▏      | 144611/450337 [00:23<00:48, 6339.11it/s] 32%|███▏      | 145299/450337 [00:23<00:46, 6491.51it/s] 32%|███▏      | 145955/450337 [00:23<00:46, 6510.89it/s] 33%|███▎      | 146609/450337 [00:24<00:48, 6296.01it/s] 33%|███▎      | 147242/450337 [00:24<00:49, 6081.76it/s] 33%|███▎      | 147854/450337 [00:24<00:52, 5739.54it/s] 33%|███▎      | 148477/450337 [00:24<00:51, 5874.46it/s] 33%|███▎      | 149124/450337 [00:24<00:49, 6041.19it/s] 33%|███▎      | 149733/450337 [00:24<00:51, 5886.60it/s] 33%|███▎      | 150325/450337 [00:24<00:52, 5698.95it/s] 34%|███▎      | 150915/450337 [00:24<00:52, 5753.29it/s] 34%|███▎      | 151533/450337 [00:24<00:50, 5870.98it/s] 34%|███▍      | 152123/450337 [00:25<00:52, 5628.99it/s] 34%|███▍      | 152763/450337 [00:25<00:50, 5847.63it/s] 34%|███▍      | 153469/450337 [00:25<00:47, 6189.68it/s] 34%|███▍      | 154092/450337 [00:25<00:50, 5871.31it/s] 34%|███▍      | 154685/450337 [00:25<00:50, 5834.44it/s] 34%|███▍      | 155327/450337 [00:25<00:49, 6001.26it/s] 35%|███▍      | 155943/450337 [00:25<00:48, 6046.70it/s] 35%|███▍      | 156614/450337 [00:25<00:47, 6239.50it/s] 35%|███▍      | 157280/450337 [00:25<00:46, 6359.30it/s] 35%|███▌      | 157931/450337 [00:25<00:45, 6400.52it/s] 35%|███▌      | 158592/450337 [00:26<00:45, 6462.16it/s] 35%|███▌      | 159240/450337 [00:26<00:48, 6011.81it/s] 36%|███▌      | 159946/450337 [00:26<00:46, 6308.11it/s] 36%|███▌      | 160584/450337 [00:26<00:46, 6168.45it/s] 36%|███▌      | 161249/450337 [00:26<00:45, 6300.18it/s] 36%|███▌      | 161883/450337 [00:26<00:48, 6006.39it/s] 36%|███▌      | 162545/450337 [00:26<00:46, 6174.93it/s] 36%|███▌      | 163205/450337 [00:26<00:45, 6296.99it/s] 36%|███▋      | 163839/450337 [00:26<00:48, 5953.28it/s] 37%|███▋      | 164450/450337 [00:27<00:47, 5995.66it/s] 37%|███▋      | 165127/450337 [00:27<00:45, 6215.96it/s] 37%|███▋      | 165816/450337 [00:27<00:44, 6409.20it/s] 37%|███▋      | 166461/450337 [00:27<00:44, 6334.90it/s] 37%|███▋      | 167152/450337 [00:27<00:43, 6496.21it/s] 37%|███▋      | 167804/450337 [00:27<00:44, 6293.67it/s] 37%|███▋      | 168436/450337 [00:27<00:46, 6000.82it/s] 38%|███▊      | 169130/450337 [00:27<00:44, 6262.33it/s] 38%|███▊      | 169761/450337 [00:27<00:46, 6039.51it/s] 38%|███▊      | 170369/450337 [00:28<00:48, 5714.46it/s] 38%|███▊      | 170977/450337 [00:28<00:48, 5814.97it/s] 38%|███▊      | 171616/450337 [00:28<00:46, 5977.30it/s] 38%|███▊      | 172218/450337 [00:28<00:48, 5723.19it/s] 38%|███▊      | 172858/450337 [00:28<00:46, 5905.29it/s] 39%|███▊      | 173461/450337 [00:28<00:46, 5940.69it/s] 39%|███▊      | 174072/450337 [00:28<00:46, 5980.28it/s] 39%|███▉      | 174770/450337 [00:28<00:43, 6265.00it/s] 39%|███▉      | 175399/450337 [00:28<00:46, 5954.85it/s] 39%|███▉      | 175999/450337 [00:28<00:47, 5834.66it/s] 39%|███▉      | 176586/450337 [00:29<00:47, 5739.25it/s] 39%|███▉      | 177185/450337 [00:29<00:47, 5806.13it/s] 39%|███▉      | 177768/450337 [00:29<00:51, 5316.01it/s] 40%|███▉      | 178448/450337 [00:29<00:47, 5718.59it/s] 40%|███▉      | 179085/450337 [00:29<00:45, 5899.96it/s] 40%|███▉      | 179683/450337 [00:29<00:46, 5817.08it/s] 40%|████      | 180271/450337 [00:29<00:46, 5817.85it/s] 40%|████      | 180870/450337 [00:29<00:45, 5866.23it/s] 40%|████      | 181460/450337 [00:29<00:46, 5756.04it/s] 40%|████      | 182038/450337 [00:30<00:46, 5760.55it/s] 41%|████      | 182616/450337 [00:30<00:47, 5664.01it/s] 41%|████      | 183194/450337 [00:30<00:46, 5691.80it/s] 41%|████      | 183765/450337 [00:30<00:50, 5318.43it/s] 41%|████      | 184331/450337 [00:30<00:49, 5411.12it/s] 41%|████      | 184946/450337 [00:30<00:47, 5622.13it/s] 41%|████      | 185513/450337 [00:30<00:48, 5467.50it/s] 41%|████▏     | 186080/450337 [00:30<00:47, 5517.05it/s] 41%|████▏     | 186686/450337 [00:30<00:46, 5669.56it/s] 42%|████▏     | 187318/450337 [00:30<00:44, 5857.17it/s] 42%|████▏     | 187906/450337 [00:31<00:45, 5736.35it/s] 42%|████▏     | 188645/450337 [00:31<00:42, 6217.75it/s] 42%|████▏     | 189270/450337 [00:31<00:43, 5982.85it/s] 42%|████▏     | 189930/450337 [00:31<00:42, 6157.64it/s] 42%|████▏     | 190549/450337 [00:31<00:42, 6152.51it/s] 42%|████▏     | 191167/450337 [00:31<00:42, 6145.41it/s] 43%|████▎     | 191784/450337 [00:31<00:43, 5988.62it/s] 43%|████▎     | 192419/450337 [00:31<00:42, 6090.19it/s] 43%|████▎     | 193030/450337 [00:31<00:42, 6019.23it/s] 43%|████▎     | 193654/450337 [00:31<00:42, 6075.48it/s] 43%|████▎     | 194274/450337 [00:32<00:41, 6108.53it/s] 43%|████▎     | 194886/450337 [00:32<00:42, 6045.03it/s] 43%|████▎     | 195502/450337 [00:32<00:41, 6078.81it/s] 44%|████▎     | 196111/450337 [00:32<00:42, 6003.33it/s] 44%|████▎     | 196712/450337 [00:32<00:43, 5866.84it/s] 44%|████▍     | 197677/450337 [00:32<00:36, 6965.31it/s] 44%|████▍     | 198378/450337 [00:32<00:39, 6329.17it/s] 44%|████▍     | 199024/450337 [00:32<00:39, 6340.87it/s] 44%|████▍     | 199668/450337 [00:32<00:40, 6135.90it/s] 44%|████▍     | 200289/450337 [00:33<00:41, 6032.69it/s] 45%|████▍     | 200897/450337 [00:33<00:41, 5954.57it/s] 45%|████▍     | 201496/450337 [00:33<00:42, 5838.43it/s] 45%|████▍     | 202125/450337 [00:33<00:41, 5957.23it/s] 45%|████▌     | 202723/450337 [00:33<00:41, 5899.74it/s] 45%|████▌     | 203424/450337 [00:33<00:39, 6214.13it/s] 45%|████▌     | 204071/450337 [00:33<00:39, 6286.92it/s] 45%|████▌     | 204702/450337 [00:33<00:40, 6013.83it/s] 46%|████▌     | 205307/450337 [00:33<00:40, 6014.94it/s] 46%|████▌     | 205911/450337 [00:34<00:41, 5954.34it/s] 46%|████▌     | 206526/450337 [00:34<00:40, 6009.56it/s] 46%|████▌     | 207129/450337 [00:34<00:42, 5759.41it/s] 46%|████▌     | 207731/450337 [00:34<00:41, 5831.99it/s] 46%|████▋     | 208317/450337 [00:34<00:42, 5744.49it/s] 46%|████▋     | 208894/450337 [00:34<00:43, 5558.19it/s] 47%|████▋     | 209459/450337 [00:34<00:43, 5577.83it/s] 47%|████▋     | 210163/450337 [00:34<00:40, 5998.77it/s] 47%|████▋     | 210800/450337 [00:34<00:39, 6104.16it/s] 47%|████▋     | 211413/450337 [00:34<00:39, 5992.45it/s] 47%|████▋     | 212014/450337 [00:35<00:39, 5959.62it/s] 47%|████▋     | 212736/450337 [00:35<00:37, 6318.84it/s] 47%|████▋     | 213370/450337 [00:35<00:38, 6146.36it/s] 48%|████▊     | 213987/450337 [00:35<00:40, 5842.40it/s] 48%|████▊     | 214575/450337 [00:35<00:40, 5813.90it/s] 48%|████▊     | 215159/450337 [00:35<00:42, 5578.41it/s] 48%|████▊     | 215720/450337 [00:35<00:43, 5429.49it/s] 48%|████▊     | 216344/450337 [00:35<00:41, 5657.17it/s] 48%|████▊     | 216913/450337 [00:35<00:41, 5630.50it/s] 48%|████▊     | 217520/450337 [00:36<00:40, 5757.20it/s] 48%|████▊     | 218165/450337 [00:36<00:38, 5956.51it/s] 49%|████▊     | 218778/450337 [00:36<00:38, 6005.44it/s] 49%|████▊     | 219434/450337 [00:36<00:37, 6165.90it/s] 49%|████▉     | 220052/450337 [00:36<00:38, 6011.91it/s] 49%|████▉     | 220655/450337 [00:36<00:39, 5867.41it/s] 49%|████▉     | 221314/450337 [00:36<00:37, 6076.55it/s] 49%|████▉     | 221924/450337 [00:36<00:38, 5890.12it/s] 49%|████▉     | 222516/450337 [00:36<00:40, 5628.51it/s] 50%|████▉     | 223117/450337 [00:36<00:39, 5731.79it/s] 50%|████▉     | 223694/450337 [00:37<00:41, 5454.32it/s] 50%|████▉     | 224405/450337 [00:37<00:38, 5916.31it/s] 50%|████▉     | 225033/450337 [00:37<00:37, 6019.11it/s] 50%|█████     | 225703/450337 [00:37<00:36, 6216.24it/s] 50%|█████     | 226383/450337 [00:37<00:35, 6380.07it/s] 50%|█████     | 227025/450337 [00:37<00:36, 6091.67it/s] 51%|█████     | 227639/450337 [00:37<00:36, 6031.14it/s] 51%|█████     | 228246/450337 [00:37<00:37, 5963.63it/s] 51%|█████     | 228845/450337 [00:37<00:38, 5727.28it/s] 51%|█████     | 229421/450337 [00:38<00:39, 5598.90it/s] 51%|█████     | 229983/450337 [00:38<00:40, 5464.04it/s] 51%|█████     | 230660/450337 [00:38<00:37, 5827.82it/s] 51%|█████▏    | 231246/450337 [00:38<00:37, 5815.08it/s] 51%|█████▏    | 231830/450337 [00:38<00:38, 5630.97it/s] 52%|█████▏    | 232413/450337 [00:38<00:38, 5687.70it/s] 52%|█████▏    | 233146/450337 [00:38<00:35, 6162.70it/s] 52%|█████▏    | 233824/450337 [00:38<00:34, 6342.46it/s] 52%|█████▏    | 234526/450337 [00:38<00:33, 6536.94it/s] 52%|█████▏    | 235182/450337 [00:38<00:33, 6494.74it/s] 52%|█████▏    | 235833/450337 [00:39<00:33, 6380.35it/s] 53%|█████▎    | 236473/450337 [00:39<00:35, 5966.52it/s] 53%|█████▎    | 237076/450337 [00:39<00:35, 5942.90it/s] 53%|█████▎    | 237713/450337 [00:39<00:35, 6060.50it/s] 53%|█████▎    | 238323/450337 [00:39<00:35, 6046.63it/s] 53%|█████▎    | 238975/450337 [00:39<00:34, 6179.16it/s] 53%|█████▎    | 239595/450337 [00:39<00:34, 6107.62it/s] 53%|█████▎    | 240208/450337 [00:39<00:35, 5849.02it/s] 54%|█████▎    | 240994/450337 [00:39<00:32, 6423.89it/s] 54%|█████▎    | 241642/450337 [00:39<00:32, 6411.74it/s] 54%|█████▍    | 242292/450337 [00:40<00:32, 6436.43it/s] 54%|█████▍    | 242939/450337 [00:40<00:33, 6221.56it/s] 54%|█████▍    | 243610/450337 [00:40<00:32, 6362.33it/s] 54%|█████▍    | 244249/450337 [00:40<00:33, 6128.00it/s] 54%|█████▍    | 244866/450337 [00:40<00:33, 6127.11it/s] 55%|█████▍    | 245481/450337 [00:40<00:33, 6118.89it/s] 55%|█████▍    | 246157/450337 [00:40<00:32, 6304.55it/s] 55%|█████▍    | 246958/450337 [00:40<00:29, 6806.64it/s] 55%|█████▍    | 247641/450337 [00:40<00:31, 6502.76it/s] 55%|█████▌    | 248296/450337 [00:41<00:32, 6297.87it/s] 55%|█████▌    | 248999/450337 [00:41<00:30, 6501.36it/s] 55%|█████▌    | 249690/450337 [00:41<00:30, 6613.90it/s] 56%|█████▌    | 250355/450337 [00:41<00:31, 6298.98it/s] 56%|█████▌    | 251008/450337 [00:41<00:31, 6357.52it/s] 56%|█████▌    | 251648/450337 [00:41<00:31, 6358.11it/s] 56%|█████▌    | 252287/450337 [00:41<00:31, 6329.99it/s] 56%|█████▌    | 252922/450337 [00:41<00:32, 6138.65it/s] 56%|█████▋    | 253538/450337 [00:41<00:32, 6134.16it/s] 56%|█████▋    | 254211/450337 [00:41<00:31, 6295.29it/s] 57%|█████▋    | 254842/450337 [00:42<00:32, 6088.01it/s] 57%|█████▋    | 255453/450337 [00:42<00:32, 6069.07it/s] 57%|█████▋    | 256138/450337 [00:42<00:30, 6285.77it/s] 57%|█████▋    | 256778/450337 [00:42<00:30, 6318.84it/s] 57%|█████▋    | 257412/450337 [00:42<00:30, 6276.66it/s] 57%|█████▋    | 258065/450337 [00:42<00:30, 6344.06it/s] 57%|█████▋    | 258701/450337 [00:42<00:33, 5690.27it/s] 58%|█████▊    | 259283/450337 [00:42<00:34, 5585.94it/s] 58%|█████▊    | 259937/450337 [00:42<00:32, 5848.74it/s] 58%|█████▊    | 260670/450337 [00:43<00:30, 6264.50it/s] 58%|█████▊    | 261305/450337 [00:43<00:31, 5923.61it/s] 58%|█████▊    | 261976/450337 [00:43<00:30, 6140.87it/s] 58%|█████▊    | 262598/450337 [00:43<00:32, 5816.37it/s] 58%|█████▊    | 263189/450337 [00:43<00:32, 5837.05it/s] 59%|█████▊    | 263824/450337 [00:43<00:31, 5978.88it/s] 59%|█████▊    | 264427/450337 [00:43<00:32, 5723.29it/s] 59%|█████▉    | 265071/450337 [00:43<00:31, 5921.70it/s] 59%|█████▉    | 265668/450337 [00:43<00:31, 5833.51it/s] 59%|█████▉    | 266255/450337 [00:44<00:32, 5692.19it/s] 59%|█████▉    | 266827/450337 [00:44<00:32, 5666.67it/s] 59%|█████▉    | 267442/450337 [00:44<00:31, 5801.99it/s] 60%|█████▉    | 268024/450337 [00:44<00:31, 5754.35it/s] 60%|█████▉    | 268676/450337 [00:44<00:30, 5974.28it/s] 60%|█████▉    | 269338/450337 [00:44<00:29, 6155.31it/s] 60%|█████▉    | 269955/450337 [00:44<00:30, 5833.31it/s] 60%|██████    | 270634/450337 [00:44<00:29, 6096.27it/s] 60%|██████    | 271248/450337 [00:44<00:29, 6044.68it/s] 60%|██████    | 271905/450337 [00:44<00:28, 6196.95it/s] 61%|██████    | 272642/450337 [00:45<00:27, 6540.36it/s] 61%|██████    | 273299/450337 [00:45<00:27, 6548.11it/s] 61%|██████    | 273956/450337 [00:45<00:27, 6515.06it/s] 61%|██████    | 274609/450337 [00:45<00:27, 6285.27it/s] 61%|██████    | 275240/450337 [00:45<00:29, 5909.54it/s] 61%|██████▏   | 275839/450337 [00:45<00:29, 5923.74it/s] 61%|██████▏   | 276436/450337 [00:45<00:30, 5658.99it/s] 62%|██████▏   | 277045/450337 [00:45<00:29, 5777.40it/s] 62%|██████▏   | 277627/450337 [00:45<00:30, 5722.63it/s] 62%|██████▏   | 278278/450337 [00:46<00:28, 5944.64it/s] 62%|██████▏   | 278954/450337 [00:46<00:27, 6177.20it/s] 62%|██████▏   | 279575/450337 [00:46<00:28, 5951.09it/s] 62%|██████▏   | 280226/450337 [00:46<00:27, 6109.49it/s] 62%|██████▏   | 280840/450337 [00:46<00:29, 5688.69it/s] 63%|██████▎   | 281465/450337 [00:46<00:28, 5841.98it/s] 63%|██████▎   | 282117/450337 [00:46<00:27, 6029.00it/s] 63%|██████▎   | 282726/450337 [00:46<00:29, 5762.34it/s] 63%|██████▎   | 283411/450337 [00:46<00:27, 6066.65it/s] 63%|██████▎   | 284036/450337 [00:46<00:27, 6117.12it/s] 63%|██████▎   | 284652/450337 [00:47<00:27, 6026.62it/s] 63%|██████▎   | 285264/450337 [00:47<00:27, 6048.42it/s] 63%|██████▎   | 285873/450337 [00:47<00:27, 6055.54it/s] 64%|██████▎   | 286481/450337 [00:47<00:27, 5902.71it/s] 64%|██████▎   | 287074/450337 [00:47<00:27, 5868.10it/s] 64%|██████▍   | 287662/450337 [00:47<00:27, 5848.01it/s] 64%|██████▍   | 288344/450337 [00:47<00:26, 6132.52it/s] 64%|██████▍   | 288973/450337 [00:47<00:26, 6178.56it/s] 64%|██████▍   | 289592/450337 [00:47<00:27, 5863.91it/s] 64%|██████▍   | 290183/450337 [00:48<00:28, 5662.94it/s] 65%|██████▍   | 290829/450337 [00:48<00:27, 5879.27it/s] 65%|██████▍   | 291421/450337 [00:48<00:27, 5864.78it/s] 65%|██████▍   | 292012/450337 [00:48<00:26, 5870.78it/s] 65%|██████▍   | 292665/450337 [00:48<00:26, 6062.91it/s] 65%|██████▌   | 293273/450337 [00:48<00:26, 5965.88it/s] 65%|██████▌   | 293871/450337 [00:48<00:26, 5808.00it/s] 65%|██████▌   | 294545/450337 [00:48<00:25, 6075.32it/s] 66%|██████▌   | 295208/450337 [00:48<00:24, 6225.74it/s] 66%|██████▌   | 295833/450337 [00:48<00:25, 6149.57it/s] 66%|██████▌   | 296450/450337 [00:49<00:25, 6061.70it/s] 66%|██████▌   | 297083/450337 [00:49<00:24, 6133.26it/s] 66%|██████▌   | 297698/450337 [00:49<00:24, 6131.30it/s] 66%|██████▌   | 298318/450337 [00:49<00:24, 6147.75it/s] 66%|██████▋   | 298934/450337 [00:49<00:25, 5974.82it/s] 67%|██████▋   | 299630/450337 [00:49<00:24, 6256.86it/s] 67%|██████▋   | 300258/450337 [00:49<00:24, 6120.79it/s] 67%|██████▋   | 300872/450337 [00:49<00:24, 5993.30it/s] 67%|██████▋   | 301473/450337 [00:49<00:25, 5893.50it/s] 67%|██████▋   | 302064/450337 [00:50<00:26, 5672.01it/s] 67%|██████▋   | 302756/450337 [00:50<00:24, 6018.86it/s] 67%|██████▋   | 303398/450337 [00:50<00:23, 6131.72it/s] 68%|██████▊   | 304038/450337 [00:50<00:23, 6206.34it/s] 68%|██████▊   | 304661/450337 [00:50<00:24, 5975.19it/s] 68%|██████▊   | 305276/450337 [00:50<00:24, 6019.21it/s] 68%|██████▊   | 305881/450337 [00:50<00:24, 5966.05it/s] 68%|██████▊   | 306480/450337 [00:50<00:24, 5871.98it/s] 68%|██████▊   | 307069/450337 [00:50<00:24, 5859.65it/s] 68%|██████▊   | 307761/450337 [00:50<00:23, 6169.05it/s] 68%|██████▊   | 308380/450337 [00:51<00:25, 5510.20it/s] 69%|██████▊   | 309147/450337 [00:51<00:23, 6099.56it/s] 69%|██████▉   | 309914/450337 [00:51<00:21, 6534.97it/s] 69%|██████▉   | 310582/450337 [00:51<00:22, 6285.07it/s] 69%|██████▉   | 311222/450337 [00:51<00:22, 6212.19it/s] 69%|██████▉   | 311851/450337 [00:51<00:22, 6197.39it/s] 69%|██████▉   | 312476/450337 [00:51<00:22, 6188.14it/s] 70%|██████▉   | 313136/450337 [00:51<00:21, 6306.30it/s] 70%|██████▉   | 313825/450337 [00:51<00:21, 6473.14it/s] 70%|██████▉   | 314504/450337 [00:51<00:20, 6559.66it/s] 70%|██████▉   | 315162/450337 [00:52<00:22, 5951.73it/s] 70%|███████   | 315776/450337 [00:52<00:22, 5998.95it/s] 70%|███████   | 316385/450337 [00:52<00:23, 5704.55it/s] 70%|███████   | 317034/450337 [00:52<00:22, 5917.93it/s] 71%|███████   | 317634/450337 [00:52<00:23, 5640.09it/s] 71%|███████   | 318288/450337 [00:52<00:22, 5884.68it/s] 71%|███████   | 318884/450337 [00:52<00:22, 5771.83it/s] 71%|███████   | 319531/450337 [00:52<00:21, 5967.12it/s] 71%|███████   | 320133/450337 [00:52<00:22, 5778.86it/s] 71%|███████   | 320765/450337 [00:53<00:21, 5931.45it/s] 71%|███████▏  | 321468/450337 [00:53<00:20, 6248.87it/s] 72%|███████▏  | 322097/450337 [00:53<00:21, 5978.91it/s] 72%|███████▏  | 322700/450337 [00:53<00:22, 5794.89it/s] 72%|███████▏  | 323321/450337 [00:53<00:21, 5906.28it/s] 72%|███████▏  | 323915/450337 [00:53<00:21, 5801.10it/s] 72%|███████▏  | 324587/450337 [00:53<00:20, 6064.67it/s] 72%|███████▏  | 325197/450337 [00:53<00:22, 5638.95it/s] 72%|███████▏  | 325789/450337 [00:53<00:21, 5716.80it/s] 72%|███████▏  | 326397/450337 [00:54<00:21, 5817.48it/s] 73%|███████▎  | 327079/450337 [00:54<00:20, 6106.83it/s] 73%|███████▎  | 327798/450337 [00:54<00:19, 6421.71it/s] 73%|███████▎  | 328524/450337 [00:54<00:18, 6668.17it/s] 73%|███████▎  | 329194/450337 [00:54<00:18, 6381.34it/s] 73%|███████▎  | 329837/450337 [00:54<00:19, 6179.23it/s] 73%|███████▎  | 330459/450337 [00:54<00:20, 5754.33it/s] 74%|███████▎  | 331126/450337 [00:54<00:19, 6000.20it/s] 74%|███████▎  | 331734/450337 [00:54<00:20, 5769.89it/s] 74%|███████▍  | 332317/450337 [00:55<00:20, 5757.52it/s] 74%|███████▍  | 332926/450337 [00:55<00:20, 5851.57it/s] 74%|███████▍  | 333515/450337 [00:55<00:20, 5839.41it/s] 74%|███████▍  | 334102/450337 [00:55<00:20, 5798.87it/s] 74%|███████▍  | 334746/450337 [00:55<00:19, 5983.88it/s] 74%|███████▍  | 335375/450337 [00:55<00:18, 6073.62it/s] 75%|███████▍  | 336011/450337 [00:55<00:18, 6151.53it/s] 75%|███████▍  | 336628/450337 [00:55<00:18, 6101.25it/s] 75%|███████▍  | 337256/450337 [00:55<00:18, 6150.29it/s] 75%|███████▌  | 337941/450337 [00:55<00:17, 6351.52it/s] 75%|███████▌  | 338577/450337 [00:56<00:17, 6265.12it/s] 75%|███████▌  | 339205/450337 [00:56<00:17, 6199.81it/s] 75%|███████▌  | 339826/450337 [00:56<00:18, 6041.07it/s] 76%|███████▌  | 340432/450337 [00:56<00:19, 5684.48it/s] 76%|███████▌  | 341040/450337 [00:56<00:18, 5787.40it/s] 76%|███████▌  | 341677/450337 [00:56<00:18, 5948.65it/s] 76%|███████▌  | 342301/450337 [00:56<00:17, 6030.45it/s] 76%|███████▌  | 342925/450337 [00:56<00:17, 6089.17it/s] 76%|███████▋  | 343549/450337 [00:56<00:17, 6126.86it/s] 76%|███████▋  | 344212/450337 [00:56<00:16, 6271.08it/s] 77%|███████▋  | 344841/450337 [00:57<00:17, 6100.54it/s] 77%|███████▋  | 345453/450337 [00:57<00:17, 6080.98it/s] 77%|███████▋  | 346063/450337 [00:57<00:18, 5606.54it/s] 77%|███████▋  | 346709/450337 [00:57<00:17, 5840.24it/s] 77%|███████▋  | 347332/450337 [00:57<00:17, 5950.68it/s] 77%|███████▋  | 347947/450337 [00:57<00:17, 6004.87it/s] 77%|███████▋  | 348552/450337 [00:57<00:17, 5921.17it/s] 78%|███████▊  | 349157/450337 [00:57<00:16, 5954.39it/s] 78%|███████▊  | 349755/450337 [00:57<00:17, 5908.65it/s] 78%|███████▊  | 350635/450337 [00:58<00:14, 6753.62it/s] 78%|███████▊  | 351314/450337 [00:58<00:15, 6367.80it/s] 78%|███████▊  | 351957/450337 [00:58<00:16, 6113.44it/s] 78%|███████▊  | 352574/450337 [00:58<00:17, 5522.08it/s] 78%|███████▊  | 353200/450337 [00:58<00:16, 5715.52it/s] 79%|███████▊  | 353783/450337 [00:58<00:17, 5649.70it/s] 79%|███████▊  | 354534/450337 [00:58<00:15, 6167.06it/s] 79%|███████▉  | 355161/450337 [00:58<00:16, 5756.35it/s] 79%|███████▉  | 355749/450337 [00:58<00:16, 5781.20it/s] 79%|███████▉  | 356449/450337 [00:59<00:15, 6120.66it/s] 79%|███████▉  | 357069/450337 [00:59<00:16, 5650.52it/s] 79%|███████▉  | 357671/450337 [00:59<00:16, 5749.32it/s] 80%|███████▉  | 358255/450337 [00:59<00:15, 5772.65it/s] 80%|███████▉  | 358904/450337 [00:59<00:15, 5975.92it/s] 80%|███████▉  | 359508/450337 [00:59<00:15, 5858.99it/s] 80%|███████▉  | 360098/450337 [00:59<00:15, 5835.11it/s] 80%|████████  | 360780/450337 [00:59<00:14, 6119.10it/s] 80%|████████  | 361414/450337 [00:59<00:14, 6183.54it/s] 80%|████████  | 362199/450337 [00:59<00:13, 6673.37it/s] 81%|████████  | 362869/450337 [01:00<00:13, 6503.09it/s] 81%|████████  | 363522/450337 [01:00<00:13, 6410.27it/s] 81%|████████  | 364165/450337 [01:00<00:13, 6395.59it/s] 81%|████████  | 364831/450337 [01:00<00:13, 6470.43it/s] 81%|████████  | 365502/450337 [01:00<00:12, 6535.14it/s] 81%|████████▏ | 366171/450337 [01:00<00:12, 6575.19it/s] 81%|████████▏ | 366830/450337 [01:00<00:13, 6123.62it/s] 82%|████████▏ | 367477/450337 [01:00<00:13, 6214.60it/s] 82%|████████▏ | 368104/450337 [01:00<00:14, 5676.80it/s] 82%|████████▏ | 368786/450337 [01:01<00:13, 5987.65it/s] 82%|████████▏ | 369396/450337 [01:01<00:13, 6004.60it/s] 82%|████████▏ | 370045/450337 [01:01<00:13, 6140.16it/s] 82%|████████▏ | 370666/450337 [01:01<00:13, 5908.34it/s] 82%|████████▏ | 371263/450337 [01:01<00:13, 5910.92it/s] 83%|████████▎ | 371859/450337 [01:01<00:13, 5834.32it/s] 83%|████████▎ | 372446/450337 [01:01<00:13, 5778.22it/s] 83%|████████▎ | 373084/450337 [01:01<00:12, 5945.32it/s] 83%|████████▎ | 373681/450337 [01:01<00:12, 5920.20it/s] 83%|████████▎ | 374366/450337 [01:01<00:12, 6189.51it/s] 83%|████████▎ | 374995/450337 [01:02<00:12, 6219.03it/s] 83%|████████▎ | 375618/450337 [01:02<00:13, 5744.92it/s] 84%|████████▎ | 376230/450337 [01:02<00:12, 5849.00it/s] 84%|████████▎ | 376821/450337 [01:02<00:13, 5585.28it/s] 84%|████████▍ | 377406/450337 [01:02<00:12, 5652.65it/s] 84%|████████▍ | 378063/450337 [01:02<00:12, 5909.16it/s] 84%|████████▍ | 378673/450337 [01:02<00:12, 5962.10it/s] 84%|████████▍ | 379306/450337 [01:02<00:11, 6065.01it/s] 84%|████████▍ | 380013/450337 [01:02<00:11, 6358.66it/s] 85%|████████▍ | 380652/450337 [01:03<00:11, 6314.46it/s] 85%|████████▍ | 381355/450337 [01:03<00:10, 6523.07it/s] 85%|████████▍ | 382009/450337 [01:03<00:11, 6079.94it/s] 85%|████████▍ | 382624/450337 [01:03<00:11, 5860.30it/s] 85%|████████▌ | 383216/450337 [01:03<00:11, 5792.00it/s] 85%|████████▌ | 383809/450337 [01:03<00:11, 5825.72it/s] 85%|████████▌ | 384460/450337 [01:03<00:10, 6019.01it/s] 86%|████████▌ | 385100/450337 [01:03<00:10, 6128.00it/s] 86%|████████▌ | 385716/450337 [01:03<00:10, 6119.24it/s] 86%|████████▌ | 386330/450337 [01:03<00:11, 5815.31it/s] 86%|████████▌ | 386975/450337 [01:04<00:10, 5993.74it/s] 86%|████████▌ | 387751/450337 [01:04<00:09, 6502.67it/s] 86%|████████▌ | 388406/450337 [01:04<00:09, 6305.06it/s] 86%|████████▋ | 389041/450337 [01:04<00:10, 6036.17it/s] 87%|████████▋ | 389681/450337 [01:04<00:09, 6130.71it/s] 87%|████████▋ | 390335/450337 [01:04<00:09, 6246.22it/s] 87%|████████▋ | 390963/450337 [01:04<00:09, 6172.90it/s] 87%|████████▋ | 391583/450337 [01:04<00:10, 5848.14it/s] 87%|████████▋ | 392173/450337 [01:04<00:10, 5665.86it/s] 87%|████████▋ | 392774/450337 [01:05<00:09, 5759.95it/s] 87%|████████▋ | 393354/450337 [01:05<00:10, 5637.43it/s] 87%|████████▋ | 393992/450337 [01:05<00:09, 5844.12it/s] 88%|████████▊ | 394595/450337 [01:05<00:09, 5895.76it/s] 88%|████████▊ | 395194/450337 [01:05<00:09, 5917.88it/s] 88%|████████▊ | 395806/450337 [01:05<00:09, 5974.92it/s] 88%|████████▊ | 396405/450337 [01:05<00:09, 5941.02it/s] 88%|████████▊ | 397000/450337 [01:05<00:09, 5812.74it/s] 88%|████████▊ | 397583/450337 [01:05<00:09, 5676.08it/s] 88%|████████▊ | 398282/450337 [01:05<00:08, 6053.11it/s] 89%|████████▊ | 398944/450337 [01:06<00:08, 6216.98it/s] 89%|████████▊ | 399633/450337 [01:06<00:07, 6412.03it/s] 89%|████████▉ | 400276/450337 [01:06<00:08, 6212.78it/s] 89%|████████▉ | 400914/450337 [01:06<00:07, 6261.10it/s] 89%|████████▉ | 401542/450337 [01:06<00:08, 6038.19it/s] 89%|████████▉ | 402149/450337 [01:06<00:08, 5653.70it/s] 89%|████████▉ | 402721/450337 [01:06<00:08, 5590.68it/s] 90%|████████▉ | 403338/450337 [01:06<00:08, 5747.83it/s] 90%|████████▉ | 403917/450337 [01:06<00:08, 5646.80it/s] 90%|████████▉ | 404589/450337 [01:07<00:07, 5950.32it/s] 90%|████████▉ | 405188/450337 [01:07<00:07, 5723.44it/s] 90%|█████████ | 405764/450337 [01:07<00:07, 5703.40it/s] 90%|█████████ | 406361/450337 [01:07<00:07, 5779.09it/s] 90%|█████████ | 406941/450337 [01:07<00:07, 5757.78it/s] 90%|█████████ | 407519/450337 [01:07<00:07, 5713.10it/s] 91%|█████████ | 408092/450337 [01:07<00:07, 5594.78it/s] 91%|█████████ | 408764/450337 [01:07<00:07, 5919.98it/s] 91%|█████████ | 409375/450337 [01:07<00:06, 5970.06it/s] 91%|█████████ | 409974/450337 [01:07<00:06, 5798.07it/s] 91%|█████████ | 410623/450337 [01:08<00:06, 5995.71it/s] 91%|█████████▏| 411225/450337 [01:08<00:06, 5877.95it/s] 91%|█████████▏| 411815/450337 [01:08<00:06, 5879.00it/s] 92%|█████████▏| 412405/450337 [01:08<00:07, 5392.10it/s] 92%|█████████▏| 412953/450337 [01:08<00:06, 5387.02it/s] 92%|█████████▏| 413572/450337 [01:08<00:06, 5609.72it/s] 92%|█████████▏| 414139/450337 [01:08<00:06, 5408.80it/s] 92%|█████████▏| 414685/450337 [01:08<00:06, 5312.23it/s] 92%|█████████▏| 415282/450337 [01:08<00:06, 5497.04it/s] 92%|█████████▏| 415869/450337 [01:09<00:06, 5595.12it/s] 92%|█████████▏| 416432/450337 [01:09<00:06, 5493.82it/s] 93%|█████████▎| 417016/450337 [01:09<00:05, 5590.38it/s] 93%|█████████▎| 417608/450337 [01:09<00:05, 5686.25it/s] 93%|█████████▎| 418267/450337 [01:09<00:05, 5951.98it/s] 93%|█████████▎| 418864/450337 [01:09<00:05, 5935.87it/s] 93%|█████████▎| 419459/450337 [01:09<00:05, 5917.85it/s] 93%|█████████▎| 420068/450337 [01:09<00:05, 5965.23it/s] 93%|█████████▎| 420666/450337 [01:09<00:05, 5933.22it/s] 94%|█████████▎| 421260/450337 [01:09<00:05, 5590.69it/s] 94%|█████████▎| 421844/450337 [01:10<00:05, 5659.81it/s] 94%|█████████▍| 422483/450337 [01:10<00:04, 5871.08it/s] 94%|█████████▍| 423122/450337 [01:10<00:04, 6019.71it/s] 94%|█████████▍| 423727/450337 [01:10<00:04, 5928.94it/s] 94%|█████████▍| 424322/450337 [01:10<00:04, 5919.21it/s] 94%|█████████▍| 424916/450337 [01:10<00:04, 5825.10it/s] 94%|█████████▍| 425500/450337 [01:10<00:04, 5696.97it/s] 95%|█████████▍| 426156/450337 [01:10<00:04, 5946.33it/s] 95%|█████████▍| 426880/450337 [01:10<00:03, 6322.80it/s] 95%|█████████▍| 427515/450337 [01:11<00:03, 6218.72it/s] 95%|█████████▌| 428139/450337 [01:11<00:03, 6012.24it/s] 95%|█████████▌| 428848/450337 [01:11<00:03, 6320.90it/s] 95%|█████████▌| 429489/450337 [01:11<00:03, 6344.55it/s] 96%|█████████▌| 430127/450337 [01:11<00:03, 6350.23it/s] 96%|█████████▌| 430764/450337 [01:11<00:03, 6175.33it/s] 96%|█████████▌| 431384/450337 [01:11<00:03, 5824.07it/s] 96%|█████████▌| 431972/450337 [01:11<00:03, 5583.47it/s] 96%|█████████▌| 432889/450337 [01:11<00:02, 6578.36it/s] 96%|█████████▋| 433558/450337 [01:11<00:02, 6271.51it/s] 96%|█████████▋| 434195/450337 [01:12<00:02, 5984.87it/s] 97%|█████████▋| 434802/450337 [01:12<00:02, 5852.09it/s] 97%|█████████▋| 435393/450337 [01:12<00:02, 5824.75it/s] 97%|█████████▋| 435980/450337 [01:12<00:02, 5811.48it/s] 97%|█████████▋| 436564/450337 [01:12<00:02, 5777.79it/s] 97%|█████████▋| 437245/450337 [01:12<00:02, 6074.09it/s] 97%|█████████▋| 437855/450337 [01:12<00:02, 6013.99it/s] 97%|█████████▋| 438458/450337 [01:12<00:02, 5818.85it/s] 97%|█████████▋| 439042/450337 [01:12<00:01, 5738.54it/s] 98%|█████████▊| 439618/450337 [01:13<00:01, 5610.07it/s] 98%|█████████▊| 440181/450337 [01:13<00:01, 5592.54it/s] 98%|█████████▊| 440743/450337 [01:13<00:01, 5599.73it/s] 98%|█████████▊| 441411/450337 [01:13<00:01, 5907.45it/s] 98%|█████████▊| 442031/450337 [01:13<00:01, 5981.74it/s] 98%|█████████▊| 442631/450337 [01:13<00:01, 5874.05it/s] 98%|█████████▊| 443305/450337 [01:13<00:01, 6125.92it/s] 99%|█████████▊| 443941/450337 [01:13<00:01, 6192.26it/s] 99%|█████████▊| 444562/450337 [01:13<00:00, 5853.56it/s] 99%|█████████▉| 445152/450337 [01:14<00:00, 5604.16it/s] 99%|█████████▉| 445742/450337 [01:14<00:00, 5685.36it/s] 99%|█████████▉| 446315/450337 [01:14<00:00, 5560.00it/s] 99%|█████████▉| 446930/450337 [01:14<00:00, 5726.48it/s] 99%|█████████▉| 447565/450337 [01:14<00:00, 5901.32it/s]100%|█████████▉| 448158/450337 [01:14<00:00, 5759.80it/s]100%|█████████▉| 448843/450337 [01:14<00:00, 6074.98it/s]100%|█████████▉| 449454/450337 [01:14<00:00, 5840.14it/s]100%|█████████▉| 450049/450337 [01:14<00:00, 5869.28it/s]100%|██████████| 450337/450337 [01:14<00:00, 6015.11it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1895/450337 [00:00<00:23, 18948.89it/s]  1%|          | 3950/450337 [00:00<00:22, 19888.45it/s]  1%|▏         | 6176/450337 [00:00<00:21, 20970.71it/s]  2%|▏         | 8274/450337 [00:00<00:22, 20041.93it/s]  2%|▏         | 10300/450337 [00:00<00:21, 20109.72it/s]  3%|▎         | 12315/450337 [00:00<00:22, 19837.64it/s]  3%|▎         | 14399/450337 [00:00<00:21, 20144.99it/s]  4%|▎         | 16417/450337 [00:00<00:21, 19949.65it/s]  4%|▍         | 18422/450337 [00:00<00:21, 19979.75it/s]  5%|▍         | 20528/450337 [00:01<00:21, 20304.19it/s]  5%|▌         | 22560/450337 [00:01<00:21, 20191.75it/s]  6%|▌         | 24820/450337 [00:01<00:20, 20912.70it/s]  6%|▌         | 26913/450337 [00:01<00:20, 20402.03it/s]  6%|▋         | 28957/450337 [00:01<00:20, 20287.31it/s]  7%|▋         | 30988/450337 [00:01<00:21, 19880.93it/s]  7%|▋         | 32979/450337 [00:01<00:21, 19672.01it/s]  8%|▊         | 35004/450337 [00:01<00:20, 19836.59it/s]  8%|▊         | 36990/450337 [00:01<00:21, 19574.10it/s]  9%|▊         | 38949/450337 [00:01<00:21, 19422.61it/s]  9%|▉         | 40995/450337 [00:02<00:20, 19721.17it/s] 10%|▉         | 42969/450337 [00:02<00:21, 19245.71it/s] 10%|▉         | 45023/450337 [00:02<00:20, 19622.29it/s] 10%|█         | 47116/450337 [00:02<00:20, 20005.97it/s] 11%|█         | 49595/450337 [00:02<00:18, 21411.43it/s] 11%|█▏        | 51741/450337 [00:02<00:19, 20775.65it/s] 12%|█▏        | 53935/450337 [00:02<00:18, 21109.74it/s] 12%|█▏        | 56052/450337 [00:02<00:19, 20701.05it/s] 13%|█▎        | 58127/450337 [00:02<00:19, 20060.94it/s] 13%|█▎        | 60147/450337 [00:02<00:19, 20093.54it/s] 14%|█▍        | 62258/450337 [00:03<00:19, 20388.18it/s] 14%|█▍        | 64301/450337 [00:03<00:19, 20165.34it/s] 15%|█▍        | 66321/450337 [00:03<00:19, 20119.10it/s] 15%|█▌        | 68335/450337 [00:03<00:19, 19896.63it/s] 16%|█▌        | 70535/450337 [00:03<00:18, 20514.15it/s] 16%|█▌        | 72589/450337 [00:03<00:18, 20289.06it/s] 17%|█▋        | 74966/450337 [00:03<00:17, 21308.90it/s] 17%|█▋        | 77101/450337 [00:03<00:17, 20819.86it/s] 18%|█▊        | 79188/450337 [00:03<00:18, 20213.35it/s] 18%|█▊        | 81229/450337 [00:04<00:18, 20268.75it/s] 19%|█▊        | 83313/450337 [00:04<00:17, 20432.77it/s] 19%|█▉        | 85360/450337 [00:04<00:18, 20194.72it/s] 19%|█▉        | 87510/450337 [00:04<00:17, 20576.48it/s] 20%|█▉        | 89571/450337 [00:04<00:17, 20042.76it/s] 20%|██        | 91580/450337 [00:04<00:18, 19922.84it/s] 21%|██        | 93583/450337 [00:04<00:17, 19946.62it/s] 21%|██        | 95580/450337 [00:04<00:17, 19895.28it/s] 22%|██▏       | 97771/450337 [00:04<00:17, 20485.20it/s] 22%|██▏       | 99822/450337 [00:04<00:17, 20108.43it/s] 23%|██▎       | 101836/450337 [00:05<00:17, 19956.31it/s] 23%|██▎       | 103834/450337 [00:05<00:17, 19882.31it/s] 24%|██▎       | 105869/450337 [00:05<00:17, 20014.79it/s] 24%|██▍       | 107872/450337 [00:05<00:17, 19600.42it/s] 24%|██▍       | 109835/450337 [00:05<00:17, 19408.41it/s] 25%|██▍       | 111914/450337 [00:05<00:17, 19812.64it/s] 25%|██▌       | 113898/450337 [00:05<00:17, 19772.91it/s] 26%|██▌       | 115877/450337 [00:05<00:17, 19512.75it/s] 26%|██▌       | 117830/450337 [00:05<00:17, 19329.80it/s] 27%|██▋       | 119822/450337 [00:05<00:16, 19498.42it/s] 27%|██▋       | 121888/450337 [00:06<00:16, 19838.46it/s] 28%|██▊       | 123874/450337 [00:06<00:16, 19786.81it/s] 28%|██▊       | 125893/450337 [00:06<00:16, 19906.33it/s] 28%|██▊       | 127956/450337 [00:06<00:16, 20117.39it/s] 29%|██▉       | 129969/450337 [00:06<00:16, 19800.93it/s] 29%|██▉       | 131951/450337 [00:06<00:16, 19190.16it/s] 30%|██▉       | 134100/450337 [00:06<00:15, 19853.63it/s] 30%|███       | 136295/450337 [00:06<00:15, 20463.92it/s] 31%|███       | 138492/450337 [00:06<00:14, 20902.73it/s] 31%|███       | 140587/450337 [00:06<00:15, 20547.84it/s] 32%|███▏      | 142646/450337 [00:07<00:15, 19943.77it/s] 32%|███▏      | 144772/450337 [00:07<00:15, 20323.16it/s] 33%|███▎      | 146830/450337 [00:07<00:14, 20379.10it/s] 33%|███▎      | 148872/450337 [00:07<00:15, 20052.97it/s] 34%|███▎      | 150881/450337 [00:07<00:15, 19693.52it/s] 34%|███▍      | 152854/450337 [00:07<00:15, 19640.12it/s] 34%|███▍      | 154838/450337 [00:07<00:15, 19695.40it/s] 35%|███▍      | 156931/450337 [00:07<00:14, 20059.10it/s] 35%|███▌      | 159018/450337 [00:07<00:14, 20293.59it/s] 36%|███▌      | 161122/450337 [00:08<00:14, 20512.26it/s] 36%|███▌      | 163175/450337 [00:08<00:14, 20371.69it/s] 37%|███▋      | 165214/450337 [00:08<00:14, 20224.67it/s] 37%|███▋      | 167366/450337 [00:08<00:13, 20601.52it/s] 38%|███▊      | 169428/450337 [00:08<00:13, 20252.21it/s] 38%|███▊      | 171455/450337 [00:08<00:14, 19604.30it/s] 39%|███▊      | 173421/450337 [00:08<00:14, 19486.63it/s] 39%|███▉      | 175373/450337 [00:08<00:14, 19488.48it/s] 39%|███▉      | 177325/450337 [00:08<00:14, 19218.60it/s] 40%|███▉      | 179249/450337 [00:08<00:14, 19153.86it/s] 40%|████      | 181166/450337 [00:09<00:14, 19143.59it/s] 41%|████      | 183095/450337 [00:09<00:13, 19185.64it/s] 41%|████      | 185015/450337 [00:09<00:14, 18857.84it/s] 42%|████▏     | 186903/450337 [00:09<00:14, 18734.67it/s] 42%|████▏     | 188957/450337 [00:09<00:13, 19265.39it/s] 42%|████▏     | 190979/450337 [00:09<00:13, 19541.48it/s] 43%|████▎     | 192980/450337 [00:09<00:13, 19679.08it/s] 43%|████▎     | 194994/450337 [00:09<00:12, 19813.93it/s] 44%|████▍     | 197203/450337 [00:09<00:12, 20491.82it/s] 44%|████▍     | 199254/450337 [00:09<00:12, 20448.24it/s] 45%|████▍     | 201300/450337 [00:10<00:12, 19900.09it/s] 45%|████▌     | 203335/450337 [00:10<00:12, 20024.93it/s] 46%|████▌     | 205341/450337 [00:10<00:12, 19888.10it/s] 46%|████▌     | 207332/450337 [00:10<00:12, 19643.67it/s] 46%|████▋     | 209299/450337 [00:10<00:12, 19390.18it/s] 47%|████▋     | 211353/450337 [00:10<00:12, 19726.23it/s] 47%|████▋     | 213426/450337 [00:10<00:11, 20019.68it/s] 48%|████▊     | 215430/450337 [00:10<00:12, 19433.42it/s] 48%|████▊     | 217378/450337 [00:10<00:12, 19376.56it/s] 49%|████▊     | 219448/450337 [00:10<00:11, 19756.32it/s] 49%|████▉     | 221427/450337 [00:11<00:11, 19704.03it/s] 50%|████▉     | 223400/450337 [00:11<00:11, 19072.65it/s] 50%|█████     | 225560/450337 [00:11<00:11, 19804.98it/s] 51%|█████     | 227598/450337 [00:11<00:11, 19967.58it/s] 51%|█████     | 229600/450337 [00:11<00:11, 19430.85it/s] 51%|█████▏    | 231549/450337 [00:11<00:11, 19387.24it/s] 52%|█████▏    | 233704/450337 [00:11<00:10, 20014.40it/s] 52%|█████▏    | 235825/450337 [00:11<00:10, 20364.62it/s] 53%|█████▎    | 237866/450337 [00:11<00:10, 19825.58it/s] 53%|█████▎    | 239915/450337 [00:12<00:10, 20008.64it/s] 54%|█████▍    | 242158/450337 [00:12<00:10, 20717.37it/s] 54%|█████▍    | 244234/450337 [00:12<00:10, 20418.07it/s] 55%|█████▍    | 246331/450337 [00:12<00:09, 20579.93it/s] 55%|█████▌    | 248456/450337 [00:12<00:09, 20771.41it/s] 56%|█████▌    | 250607/450337 [00:12<00:09, 20990.26it/s] 56%|█████▌    | 252708/450337 [00:12<00:09, 20837.28it/s] 57%|█████▋    | 254794/450337 [00:12<00:09, 20534.65it/s] 57%|█████▋    | 256922/450337 [00:12<00:09, 20752.90it/s] 58%|█████▊    | 258999/450337 [00:12<00:09, 20031.94it/s] 58%|█████▊    | 261095/450337 [00:13<00:09, 20298.90it/s] 58%|█████▊    | 263130/450337 [00:13<00:09, 19716.55it/s] 59%|█████▉    | 265110/450337 [00:13<00:09, 19736.99it/s] 59%|█████▉    | 267088/450337 [00:13<00:09, 19473.65it/s] 60%|█████▉    | 269127/450337 [00:13<00:09, 19737.63it/s] 60%|██████    | 271104/450337 [00:13<00:09, 19629.18it/s] 61%|██████    | 273326/450337 [00:13<00:08, 20389.78it/s] 61%|██████    | 275368/450337 [00:13<00:08, 19993.42it/s] 62%|██████▏   | 277371/450337 [00:13<00:08, 19646.58it/s] 62%|██████▏   | 279373/450337 [00:13<00:08, 19754.68it/s] 62%|██████▏   | 281379/450337 [00:14<00:08, 19843.53it/s] 63%|██████▎   | 283368/450337 [00:14<00:08, 19854.00it/s] 63%|██████▎   | 285356/450337 [00:14<00:08, 19860.97it/s] 64%|██████▍   | 287344/450337 [00:14<00:08, 19698.37it/s] 64%|██████▍   | 289447/450337 [00:14<00:08, 20089.82it/s] 65%|██████▍   | 291457/450337 [00:14<00:08, 19666.03it/s] 65%|██████▌   | 293427/450337 [00:14<00:07, 19647.98it/s] 66%|██████▌   | 295491/450337 [00:14<00:07, 19935.37it/s] 66%|██████▌   | 297487/450337 [00:14<00:07, 19908.97it/s] 67%|██████▋   | 299587/450337 [00:14<00:07, 20231.65it/s] 67%|██████▋   | 301612/450337 [00:15<00:07, 19971.88it/s] 67%|██████▋   | 303671/450337 [00:15<00:07, 20154.07it/s] 68%|██████▊   | 305688/450337 [00:15<00:07, 19836.46it/s] 68%|██████▊   | 307705/450337 [00:15<00:07, 19933.04it/s] 69%|██████▉   | 309826/450337 [00:15<00:06, 20306.41it/s] 69%|██████▉   | 311859/450337 [00:15<00:06, 20081.28it/s] 70%|██████▉   | 314038/450337 [00:15<00:06, 20581.01it/s] 70%|███████   | 316098/450337 [00:15<00:06, 19822.24it/s] 71%|███████   | 318087/450337 [00:15<00:06, 19383.22it/s] 71%|███████   | 320054/450337 [00:16<00:06, 19463.44it/s] 72%|███████▏  | 322049/450337 [00:16<00:06, 19604.63it/s] 72%|███████▏  | 324013/450337 [00:16<00:06, 19488.03it/s] 72%|███████▏  | 325965/450337 [00:16<00:06, 19207.07it/s] 73%|███████▎  | 328291/450337 [00:16<00:05, 20390.56it/s] 73%|███████▎  | 330335/450337 [00:16<00:06, 19755.15it/s] 74%|███████▍  | 332317/450337 [00:16<00:06, 19613.43it/s] 74%|███████▍  | 334283/450337 [00:16<00:05, 19556.70it/s] 75%|███████▍  | 336307/450337 [00:16<00:05, 19755.72it/s] 75%|███████▌  | 338391/450337 [00:16<00:05, 20066.53it/s] 76%|███████▌  | 340400/450337 [00:17<00:05, 19603.63it/s] 76%|███████▌  | 342457/450337 [00:17<00:05, 19881.37it/s] 76%|███████▋  | 344449/450337 [00:17<00:05, 19829.45it/s] 77%|███████▋  | 346435/450337 [00:17<00:05, 19782.16it/s] 77%|███████▋  | 348415/450337 [00:17<00:05, 19673.01it/s] 78%|███████▊  | 350650/450337 [00:17<00:04, 20465.29it/s] 78%|███████▊  | 352699/450337 [00:17<00:05, 19444.50it/s] 79%|███████▉  | 354763/450337 [00:17<00:04, 19787.29it/s] 79%|███████▉  | 356752/450337 [00:17<00:04, 19565.91it/s] 80%|███████▉  | 358716/450337 [00:17<00:04, 19381.68it/s] 80%|████████  | 360780/450337 [00:18<00:04, 19738.18it/s] 81%|████████  | 362965/450337 [00:18<00:04, 20352.71it/s] 81%|████████  | 365123/450337 [00:18<00:04, 20711.01it/s] 82%|████████▏ | 367198/450337 [00:18<00:04, 20569.45it/s] 82%|████████▏ | 369258/450337 [00:18<00:04, 20070.08it/s] 82%|████████▏ | 371269/450337 [00:18<00:03, 19931.45it/s] 83%|████████▎ | 373265/450337 [00:18<00:03, 19881.10it/s] 83%|████████▎ | 375304/450337 [00:18<00:03, 20029.13it/s] 84%|████████▍ | 377309/450337 [00:18<00:03, 19357.61it/s] 84%|████████▍ | 379350/450337 [00:19<00:03, 19661.90it/s] 85%|████████▍ | 381474/450337 [00:19<00:03, 20123.69it/s] 85%|████████▌ | 383491/450337 [00:19<00:03, 19465.99it/s] 86%|████████▌ | 385561/450337 [00:19<00:03, 19817.86it/s] 86%|████████▌ | 387659/450337 [00:19<00:03, 20157.32it/s] 87%|████████▋ | 389680/450337 [00:19<00:03, 20007.98it/s] 87%|████████▋ | 391685/450337 [00:19<00:02, 19737.74it/s] 87%|████████▋ | 393662/450337 [00:19<00:02, 19457.07it/s] 88%|████████▊ | 395685/450337 [00:19<00:02, 19682.04it/s] 88%|████████▊ | 397656/450337 [00:19<00:02, 19399.11it/s] 89%|████████▉ | 399879/450337 [00:20<00:02, 20226.57it/s] 89%|████████▉ | 401906/450337 [00:20<00:02, 19712.24it/s] 90%|████████▉ | 403882/450337 [00:20<00:02, 19280.66it/s] 90%|█████████ | 405815/450337 [00:20<00:02, 19222.47it/s] 91%|█████████ | 407741/450337 [00:20<00:02, 18889.24it/s] 91%|█████████ | 409744/450337 [00:20<00:02, 19214.39it/s] 91%|█████████▏| 411675/450337 [00:20<00:02, 19235.70it/s] 92%|█████████▏| 413601/450337 [00:20<00:01, 18753.27it/s] 92%|█████████▏| 415480/450337 [00:20<00:01, 18482.85it/s] 93%|█████████▎| 417409/450337 [00:20<00:01, 18717.04it/s] 93%|█████████▎| 419419/450337 [00:21<00:01, 19117.68it/s] 94%|█████████▎| 421334/450337 [00:21<00:01, 19003.03it/s] 94%|█████████▍| 423355/450337 [00:21<00:01, 19357.97it/s] 94%|█████████▍| 425293/450337 [00:21<00:01, 19256.13it/s] 95%|█████████▍| 427429/450337 [00:21<00:01, 19877.55it/s] 95%|█████████▌| 429497/450337 [00:21<00:01, 20113.60it/s] 96%|█████████▌| 431510/450337 [00:21<00:00, 19719.76it/s] 96%|█████████▋| 433691/450337 [00:21<00:00, 20329.77it/s] 97%|█████████▋| 435727/450337 [00:21<00:00, 19988.86it/s] 97%|█████████▋| 437729/450337 [00:21<00:00, 19872.11it/s] 98%|█████████▊| 439719/450337 [00:22<00:00, 19418.56it/s] 98%|█████████▊| 441693/450337 [00:22<00:00, 19506.31it/s] 99%|█████████▊| 443786/450337 [00:22<00:00, 19922.78it/s] 99%|█████████▉| 445781/450337 [00:22<00:00, 19288.80it/s] 99%|█████████▉| 447785/450337 [00:22<00:00, 19504.75it/s]100%|█████████▉| 449740/450337 [00:22<00:00, 19395.27it/s]100%|██████████| 450337/450337 [00:22<00:00, 19883.88it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 19.64it/s]2022-03-02 10:39:08 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-02 10:39:08 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-02 10:39:08 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-02 10:39:08 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-02 10:39:08 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-02 10:39:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-02 10:39:08 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-02 10:39:09 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-02 10:39:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-02 10:39:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-02 10:39:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-02 10:39:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-02 10:39:09 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-02 10:39:09 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_last.pt
2022-03-02 10:39:09 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_last.pt
2022-03-02 10:39:09 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-02 10:39:09 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-02 10:39:09 | INFO | fairseq.trainer | begin training epoch 1
2022-03-02 10:39:09 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-02 10:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-02 10:39:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-02 10:39:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 10:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 10:39:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-02 10:47:13 | INFO | train_inner | epoch 001:    105 / 393 loss=17.037, ppl=134453, wps=14682.3, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.214, loss_scale=4, train_wall=479, gb_free=10.1, wall=484
2022-03-02 10:54:39 | INFO | train_inner | epoch 001:    205 / 393 loss=14.727, ppl=27124.9, wps=14697.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.456, loss_scale=4, train_wall=441, gb_free=10.1, wall=930
2022-03-02 11:02:05 | INFO | train_inner | epoch 001:    305 / 393 loss=12.768, ppl=6972.9, wps=14701.3, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.982, loss_scale=4, train_wall=441, gb_free=10.1, wall=1376
2022-03-02 11:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 11:08:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.038 | ppl 2102.59 | wps 34015.8 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-02 11:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-02 11:08:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 11:08:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 11:08:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.038) (writing took 4.908273737877607 seconds)
2022-03-02 11:08:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-02 11:08:46 | INFO | train | epoch 001 | loss 14.073 | ppl 17239.1 | wps 14602.2 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.594 | loss_scale 4 | train_wall 1747 | gb_free 10.1 | wall 1777
2022-03-02 11:08:46 | INFO | fairseq.trainer | begin training epoch 2
2022-03-02 11:08:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 11:09:40 | INFO | train_inner | epoch 002:     12 / 393 loss=11.388, ppl=2679.78, wps=14343.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.587, loss_scale=4, train_wall=439, gb_free=10.1, wall=1831
2022-03-02 11:17:05 | INFO | train_inner | epoch 002:    112 / 393 loss=10.841, ppl=1833.83, wps=14713.1, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.475, loss_scale=4, train_wall=441, gb_free=10.1, wall=2276
2022-03-02 11:24:31 | INFO | train_inner | epoch 002:    212 / 393 loss=10.555, ppl=1504.37, wps=14704.3, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.514, loss_scale=8, train_wall=441, gb_free=10.1, wall=2722
2022-03-02 11:31:56 | INFO | train_inner | epoch 002:    312 / 393 loss=10.315, ppl=1273.9, wps=14704.8, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.588, loss_scale=8, train_wall=441, gb_free=10.1, wall=3168
2022-03-02 11:37:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 11:38:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.987 | ppl 1014.84 | wps 34045.6 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.987
2022-03-02 11:38:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-02 11:38:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 11:38:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 11:38:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.987) (writing took 4.961523658595979 seconds)
2022-03-02 11:38:07 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-02 11:38:07 | INFO | train | epoch 002 | loss 10.494 | ppl 1442.48 | wps 14610.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.543 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 3538
2022-03-02 11:38:07 | INFO | fairseq.trainer | begin training epoch 3
2022-03-02 11:38:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 11:39:32 | INFO | train_inner | epoch 003:     19 / 393 loss=10.104, ppl=1100.67, wps=14332.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.645, loss_scale=8, train_wall=439, gb_free=10.1, wall=3623
2022-03-02 11:46:57 | INFO | train_inner | epoch 003:    119 / 393 loss=9.899, ppl=954.8, wps=14701.9, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.737, loss_scale=8, train_wall=441, gb_free=10.1, wall=4069
2022-03-02 11:54:23 | INFO | train_inner | epoch 003:    219 / 393 loss=9.73, ppl=849.1, wps=14704.6, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.793, loss_scale=8, train_wall=441, gb_free=10.1, wall=4514
2022-03-02 12:01:49 | INFO | train_inner | epoch 003:    319 / 393 loss=9.577, ppl=763.52, wps=14697.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.803, loss_scale=16, train_wall=441, gb_free=10.1, wall=4960
2022-03-02 12:07:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 12:07:24 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.336 | ppl 646.45 | wps 33939.4 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 9.336
2022-03-02 12:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-02 12:07:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 12:07:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 12:07:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 3 @ 1174 updates, score 9.336) (writing took 5.286623851396143 seconds)
2022-03-02 12:07:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-02 12:07:29 | INFO | train | epoch 003 | loss 9.695 | ppl 829.02 | wps 14600.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.78 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 5300
2022-03-02 12:07:29 | INFO | fairseq.trainer | begin training epoch 4
2022-03-02 12:07:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 12:09:25 | INFO | train_inner | epoch 004:     26 / 393 loss=9.427, ppl=688.21, wps=14307.4, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.825, loss_scale=16, train_wall=439, gb_free=10.1, wall=5416
2022-03-02 12:16:51 | INFO | train_inner | epoch 004:    126 / 393 loss=9.278, ppl=620.88, wps=14690.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.78, loss_scale=16, train_wall=441, gb_free=10.1, wall=5862
2022-03-02 12:24:17 | INFO | train_inner | epoch 004:    226 / 393 loss=9.167, ppl=574.92, wps=14698.4, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.825, loss_scale=16, train_wall=441, gb_free=10.1, wall=6308
2022-03-02 12:31:43 | INFO | train_inner | epoch 004:    326 / 393 loss=9.064, ppl=535.24, wps=14692.7, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.835, loss_scale=16, train_wall=441, gb_free=10.1, wall=6754
2022-03-02 12:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 12:36:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.867 | ppl 466.91 | wps 34036.2 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.867
2022-03-02 12:36:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-02 12:36:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 12:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 12:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.867) (writing took 4.836976687423885 seconds)
2022-03-02 12:36:51 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-02 12:36:51 | INFO | train | epoch 004 | loss 9.147 | ppl 566.81 | wps 14597.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.822 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 7063
2022-03-02 12:36:51 | INFO | fairseq.trainer | begin training epoch 5
2022-03-02 12:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 12:39:18 | INFO | train_inner | epoch 005:     33 / 393 loss=8.932, ppl=488.38, wps=14322.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.829, loss_scale=32, train_wall=439, gb_free=10.1, wall=7210
2022-03-02 12:46:45 | INFO | train_inner | epoch 005:    133 / 393 loss=8.815, ppl=450.33, wps=14689.6, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.825, loss_scale=32, train_wall=441, gb_free=10.1, wall=7656
2022-03-02 12:54:11 | INFO | train_inner | epoch 005:    233 / 393 loss=8.733, ppl=425.53, wps=14691, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.82, loss_scale=32, train_wall=441, gb_free=10.1, wall=8102
2022-03-02 13:01:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 13:01:41 | INFO | train_inner | epoch 005:    334 / 393 loss=8.639, ppl=398.72, wps=14537.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.79, loss_scale=16, train_wall=446, gb_free=10.1, wall=8553
2022-03-02 13:06:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 13:06:09 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.523 | ppl 367.96 | wps 34039.4 | wpb 2034.1 | bsz 4 | num_updates 1959 | best_loss 8.523
2022-03-02 13:06:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1959 updates
2022-03-02 13:06:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 13:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 13:06:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 5 @ 1959 updates, score 8.523) (writing took 4.88907054066658 seconds)
2022-03-02 13:06:14 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-02 13:06:14 | INFO | train | epoch 005 | loss 8.717 | ppl 420.78 | wps 14555 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 1959 | lr 0.000244926 | gnorm 0.812 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 8826
2022-03-02 13:06:14 | INFO | fairseq.trainer | begin training epoch 6
2022-03-02 13:06:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 13:09:17 | INFO | train_inner | epoch 006:     41 / 393 loss=8.53, ppl=369.58, wps=14317.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.785, loss_scale=16, train_wall=439, gb_free=10.1, wall=9009
2022-03-02 13:16:43 | INFO | train_inner | epoch 006:    141 / 393 loss=8.429, ppl=344.73, wps=14690, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.784, loss_scale=16, train_wall=441, gb_free=10.1, wall=9455
2022-03-02 13:24:10 | INFO | train_inner | epoch 006:    241 / 393 loss=8.367, ppl=330.25, wps=14685.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.767, loss_scale=16, train_wall=441, gb_free=10.1, wall=9901
2022-03-02 13:31:36 | INFO | train_inner | epoch 006:    341 / 393 loss=8.301, ppl=315.34, wps=14684.4, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.768, loss_scale=16, train_wall=441, gb_free=10.1, wall=10347
2022-03-02 13:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 13:35:32 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.246 | ppl 303.61 | wps 33990.8 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 8.246
2022-03-02 13:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-02 13:35:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 13:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 13:35:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 6 @ 2352 updates, score 8.246) (writing took 4.905006784014404 seconds)
2022-03-02 13:35:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-02 13:35:37 | INFO | train | epoch 006 | loss 8.361 | ppl 328.71 | wps 14591.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.768 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 10589
2022-03-02 13:35:37 | INFO | fairseq.trainer | begin training epoch 7
2022-03-02 13:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 13:39:12 | INFO | train_inner | epoch 007:     48 / 393 loss=8.19, ppl=291.96, wps=14317.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.753, loss_scale=16, train_wall=439, gb_free=10.1, wall=10803
2022-03-02 13:40:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 13:46:42 | INFO | train_inner | epoch 007:    149 / 393 loss=8.099, ppl=274.12, wps=14540.9, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.734, loss_scale=16, train_wall=446, gb_free=10.1, wall=11254
2022-03-02 13:54:09 | INFO | train_inner | epoch 007:    249 / 393 loss=8.064, ppl=267.56, wps=14684.5, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.721, loss_scale=16, train_wall=441, gb_free=10.1, wall=11700
2022-03-02 14:01:35 | INFO | train_inner | epoch 007:    349 / 393 loss=8.01, ppl=257.77, wps=14689.2, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.715, loss_scale=16, train_wall=441, gb_free=10.1, wall=12146
2022-03-02 14:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 14:04:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.039 | ppl 262.98 | wps 33955.8 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 8.039
2022-03-02 14:04:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-02 14:04:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 14:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 14:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 7 @ 2744 updates, score 8.039) (writing took 4.763209125958383 seconds)
2022-03-02 14:05:00 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-02 14:05:00 | INFO | train | epoch 007 | loss 8.059 | ppl 266.62 | wps 14555.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.73 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 12352
2022-03-02 14:05:00 | INFO | fairseq.trainer | begin training epoch 8
2022-03-02 14:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 14:09:10 | INFO | train_inner | epoch 008:     56 / 393 loss=7.911, ppl=240.76, wps=14325.8, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.732, loss_scale=16, train_wall=439, gb_free=10.1, wall=12602
2022-03-02 14:16:36 | INFO | train_inner | epoch 008:    156 / 393 loss=7.825, ppl=226.69, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.697, loss_scale=16, train_wall=441, gb_free=10.1, wall=13048
2022-03-02 14:24:03 | INFO | train_inner | epoch 008:    256 / 393 loss=7.805, ppl=223.71, wps=14678.8, ups=0.22, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.702, loss_scale=32, train_wall=442, gb_free=10.1, wall=13494
2022-03-02 14:25:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 14:31:33 | INFO | train_inner | epoch 008:    357 / 393 loss=7.779, ppl=219.63, wps=14541.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.683, loss_scale=16, train_wall=446, gb_free=10.1, wall=13945
2022-03-02 14:34:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 14:34:19 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.861 | ppl 232.49 | wps 34055.3 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 7.861
2022-03-02 14:34:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-02 14:34:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 14:34:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 14:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 8 @ 3136 updates, score 7.861) (writing took 4.680102685466409 seconds)
2022-03-02 14:34:23 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-02 14:34:23 | INFO | train | epoch 008 | loss 7.808 | ppl 224.03 | wps 14554.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.697 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 14115
2022-03-02 14:34:23 | INFO | fairseq.trainer | begin training epoch 9
2022-03-02 14:34:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 14:39:09 | INFO | train_inner | epoch 009:     64 / 393 loss=7.67, ppl=203.61, wps=14321.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.676, loss_scale=16, train_wall=440, gb_free=10.1, wall=14401
2022-03-02 14:46:35 | INFO | train_inner | epoch 009:    164 / 393 loss=7.616, ppl=196.2, wps=14689.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.689, loss_scale=16, train_wall=441, gb_free=10.1, wall=14847
2022-03-02 14:54:01 | INFO | train_inner | epoch 009:    264 / 393 loss=7.602, ppl=194.23, wps=14688.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.656, loss_scale=16, train_wall=441, gb_free=10.1, wall=15293
2022-03-02 15:01:28 | INFO | train_inner | epoch 009:    364 / 393 loss=7.574, ppl=190.59, wps=14682.9, ups=0.22, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.636, loss_scale=16, train_wall=441, gb_free=10.1, wall=15739
2022-03-02 15:03:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 15:03:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.734 | ppl 212.96 | wps 34048.6 | wpb 2034.1 | bsz 4 | num_updates 3529 | best_loss 7.734
2022-03-02 15:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3529 updates
2022-03-02 15:03:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 15:03:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 15:03:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 9 @ 3529 updates, score 7.734) (writing took 4.6686590956524014 seconds)
2022-03-02 15:03:46 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-02 15:03:46 | INFO | train | epoch 009 | loss 7.599 | ppl 193.87 | wps 14593.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3529 | lr 0.000441137 | gnorm 0.666 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 15878
2022-03-02 15:03:46 | INFO | fairseq.trainer | begin training epoch 10
2022-03-02 15:03:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 15:09:03 | INFO | train_inner | epoch 010:     71 / 393 loss=7.462, ppl=176.36, wps=14327.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.648, loss_scale=32, train_wall=439, gb_free=10.1, wall=16195
2022-03-02 15:09:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 15:16:34 | INFO | train_inner | epoch 010:    172 / 393 loss=7.422, ppl=171.46, wps=14542.8, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.659, loss_scale=16, train_wall=446, gb_free=10.1, wall=16645
2022-03-02 15:24:00 | INFO | train_inner | epoch 010:    272 / 393 loss=7.428, ppl=172.2, wps=14687.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.632, loss_scale=16, train_wall=441, gb_free=10.1, wall=17091
2022-03-02 15:31:26 | INFO | train_inner | epoch 010:    372 / 393 loss=7.42, ppl=171.27, wps=14686.3, ups=0.22, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.638, loss_scale=16, train_wall=441, gb_free=10.1, wall=17538
2022-03-02 15:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 15:33:04 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.629 | ppl 198.02 | wps 33803.5 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.629
2022-03-02 15:33:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-02 15:33:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 15:33:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 15:33:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.629) (writing took 4.720575321465731 seconds)
2022-03-02 15:33:09 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-02 15:33:09 | INFO | train | epoch 010 | loss 7.421 | ppl 171.4 | wps 14556.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.639 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 17641
2022-03-02 15:33:09 | INFO | fairseq.trainer | begin training epoch 11
2022-03-02 15:33:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 15:39:02 | INFO | train_inner | epoch 011:     79 / 393 loss=7.281, ppl=155.54, wps=14325.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.632, loss_scale=16, train_wall=439, gb_free=10.1, wall=17993
2022-03-02 15:46:28 | INFO | train_inner | epoch 011:    179 / 393 loss=7.265, ppl=153.82, wps=14690.4, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.617, loss_scale=16, train_wall=441, gb_free=10.1, wall=18439
2022-03-02 15:48:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 15:53:58 | INFO | train_inner | epoch 011:    280 / 393 loss=7.268, ppl=154.16, wps=14542.9, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.585, loss_scale=16, train_wall=446, gb_free=10.1, wall=18890
2022-03-02 16:01:25 | INFO | train_inner | epoch 011:    380 / 393 loss=7.275, ppl=154.93, wps=14684.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=19336
2022-03-02 16:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 16:02:27 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.545 | ppl 186.77 | wps 33846 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 7.545
2022-03-02 16:02:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-02 16:02:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 16:02:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 16:02:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 11 @ 4313 updates, score 7.545) (writing took 4.7196786981076 seconds)
2022-03-02 16:02:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-02 16:02:32 | INFO | train | epoch 011 | loss 7.265 | ppl 153.79 | wps 14557 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.604 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 19403
2022-03-02 16:02:32 | INFO | fairseq.trainer | begin training epoch 12
2022-03-02 16:02:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 16:09:00 | INFO | train_inner | epoch 012:     87 / 393 loss=7.119, ppl=139.01, wps=14327, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.585, loss_scale=16, train_wall=439, gb_free=10.1, wall=19792
2022-03-02 16:16:26 | INFO | train_inner | epoch 012:    187 / 393 loss=7.116, ppl=138.68, wps=14681.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=20238
2022-03-02 16:23:53 | INFO | train_inner | epoch 012:    287 / 393 loss=7.128, ppl=139.84, wps=14668.6, ups=0.22, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.564, loss_scale=16, train_wall=442, gb_free=10.1, wall=20685
2022-03-02 16:29:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 16:31:24 | INFO | train_inner | epoch 012:    388 / 393 loss=7.12, ppl=139.12, wps=14542.8, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.554, loss_scale=16, train_wall=446, gb_free=10.1, wall=21135
2022-03-02 16:31:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 16:31:51 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.478 | ppl 178.31 | wps 33694 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.478
2022-03-02 16:31:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-02 16:31:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 16:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 16:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.478) (writing took 4.676565932109952 seconds)
2022-03-02 16:31:55 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-02 16:31:55 | INFO | train | epoch 012 | loss 7.116 | ppl 138.76 | wps 14551.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.567 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 21167
2022-03-02 16:31:55 | INFO | fairseq.trainer | begin training epoch 13
2022-03-02 16:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 16:38:59 | INFO | train_inner | epoch 013:     95 / 393 loss=6.973, ppl=125.62, wps=14326.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.561, loss_scale=16, train_wall=439, gb_free=10.1, wall=21591
2022-03-02 16:46:25 | INFO | train_inner | epoch 013:    195 / 393 loss=6.988, ppl=126.94, wps=14686.5, ups=0.22, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.55, loss_scale=16, train_wall=441, gb_free=10.1, wall=22037
2022-03-02 16:53:52 | INFO | train_inner | epoch 013:    295 / 393 loss=7.002, ppl=128.18, wps=14687.6, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.54, loss_scale=16, train_wall=441, gb_free=10.1, wall=22483
2022-03-02 17:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:01:13 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.443 | ppl 174.02 | wps 33569 | wpb 2034.1 | bsz 4 | num_updates 5098 | best_loss 7.443
2022-03-02 17:01:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5098 updates
2022-03-02 17:01:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 17:01:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 17:01:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 13 @ 5098 updates, score 7.443) (writing took 4.659586609341204 seconds)
2022-03-02 17:01:18 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-02 17:01:18 | INFO | train | epoch 013 | loss 6.993 | ppl 127.41 | wps 14595 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5098 | lr 0.000442894 | gnorm 0.548 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 22929
2022-03-02 17:01:18 | INFO | fairseq.trainer | begin training epoch 14
2022-03-02 17:01:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 17:01:27 | INFO | train_inner | epoch 014:      2 / 393 loss=7.014, ppl=129.29, wps=14327.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.544, loss_scale=16, train_wall=439, gb_free=10.1, wall=22939
2022-03-02 17:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 17:08:58 | INFO | train_inner | epoch 014:    103 / 393 loss=6.843, ppl=114.81, wps=14541.3, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.548, loss_scale=8, train_wall=446, gb_free=10.1, wall=23389
2022-03-02 17:16:24 | INFO | train_inner | epoch 014:    203 / 393 loss=6.886, ppl=118.3, wps=14688.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.527, loss_scale=8, train_wall=441, gb_free=10.1, wall=23835
2022-03-02 17:23:50 | INFO | train_inner | epoch 014:    303 / 393 loss=6.911, ppl=120.36, wps=14688, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.532, loss_scale=8, train_wall=441, gb_free=10.1, wall=24282
2022-03-02 17:30:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:30:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.405 | ppl 169.52 | wps 33576.1 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 7.405
2022-03-02 17:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-02 17:30:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 17:30:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 17:30:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 14 @ 5490 updates, score 7.405) (writing took 4.668424681760371 seconds)
2022-03-02 17:30:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-02 17:30:41 | INFO | train | epoch 014 | loss 6.888 | ppl 118.41 | wps 14557.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.535 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 24692
2022-03-02 17:30:41 | INFO | fairseq.trainer | begin training epoch 15
2022-03-02 17:30:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 17:31:25 | INFO | train_inner | epoch 015:     10 / 393 loss=6.896, ppl=119.1, wps=14329, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.528, loss_scale=8, train_wall=439, gb_free=10.1, wall=24737
2022-03-02 17:38:51 | INFO | train_inner | epoch 015:    110 / 393 loss=6.755, ppl=108.04, wps=14694.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.542, loss_scale=8, train_wall=441, gb_free=10.1, wall=25183
2022-03-02 17:46:17 | INFO | train_inner | epoch 015:    210 / 393 loss=6.791, ppl=110.71, wps=14695.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.533, loss_scale=16, train_wall=441, gb_free=10.1, wall=25629
2022-03-02 17:53:44 | INFO | train_inner | epoch 015:    310 / 393 loss=6.819, ppl=112.95, wps=14684.8, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.526, loss_scale=16, train_wall=441, gb_free=10.1, wall=26075
2022-03-02 17:59:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:59:59 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.383 | ppl 166.95 | wps 33679.1 | wpb 2034.1 | bsz 4 | num_updates 5883 | best_loss 7.383
2022-03-02 17:59:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5883 updates
2022-03-02 17:59:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 18:00:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 18:00:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 15 @ 5883 updates, score 7.383) (writing took 4.502552437596023 seconds)
2022-03-02 18:00:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-02 18:00:03 | INFO | train | epoch 015 | loss 6.796 | ppl 111.13 | wps 14598.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5883 | lr 0.000412288 | gnorm 0.53 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 26455
2022-03-02 18:00:03 | INFO | fairseq.trainer | begin training epoch 16
2022-03-02 18:00:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 18:01:19 | INFO | train_inner | epoch 016:     17 / 393 loss=6.802, ppl=111.61, wps=14328.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.53, loss_scale=16, train_wall=439, gb_free=10.1, wall=26530
2022-03-02 18:08:45 | INFO | train_inner | epoch 016:    117 / 393 loss=6.669, ppl=101.77, wps=14686.4, ups=0.22, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.535, loss_scale=16, train_wall=441, gb_free=10.1, wall=26977
2022-03-02 18:16:11 | INFO | train_inner | epoch 016:    217 / 393 loss=6.709, ppl=104.61, wps=14685.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.539, loss_scale=16, train_wall=441, gb_free=10.1, wall=27423
2022-03-02 18:23:38 | INFO | train_inner | epoch 016:    317 / 393 loss=6.743, ppl=107.12, wps=14684.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.53, loss_scale=16, train_wall=441, gb_free=10.1, wall=27869
2022-03-02 18:24:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 18:29:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 18:29:21 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.384 | ppl 166.98 | wps 33612.8 | wpb 2034.1 | bsz 4 | num_updates 6275 | best_loss 7.383
2022-03-02 18:29:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6275 updates
2022-03-02 18:29:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-02 18:29:21 | INFO | train | epoch 016 | loss 6.715 | ppl 105.06 | wps 14594 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6275 | lr 0.000399202 | gnorm 0.535 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 28213
2022-03-02 18:29:21 | INFO | fairseq.trainer | begin training epoch 17
2022-03-02 18:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 18:31:13 | INFO | train_inner | epoch 017:     25 / 393 loss=6.711, ppl=104.74, wps=14330.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.514, loss_scale=16, train_wall=444, gb_free=10.1, wall=28324
2022-03-02 18:38:39 | INFO | train_inner | epoch 017:    125 / 393 loss=6.606, ppl=97.39, wps=14687.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.56, loss_scale=16, train_wall=441, gb_free=10.1, wall=28771
2022-03-02 18:46:06 | INFO | train_inner | epoch 017:    225 / 393 loss=6.643, ppl=99.97, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.525, loss_scale=16, train_wall=441, gb_free=10.1, wall=29217
2022-03-02 18:53:32 | INFO | train_inner | epoch 017:    325 / 393 loss=6.666, ppl=101.53, wps=14688.9, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.532, loss_scale=16, train_wall=441, gb_free=10.1, wall=29663
2022-03-02 18:58:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 18:58:40 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.368 | ppl 165.15 | wps 33749.6 | wpb 2034.1 | bsz 4 | num_updates 6668 | best_loss 7.368
2022-03-02 18:58:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6668 updates
2022-03-02 18:58:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 18:58:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt
2022-03-02 18:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.06_0.04_0.9_#1/checkpoint_best.pt (epoch 17 @ 6668 updates, score 7.368) (writing took 4.6009304113686085 seconds)
2022-03-02 18:58:44 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-02 18:58:44 | INFO | train | epoch 017 | loss 6.643 | ppl 99.93 | wps 14594.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 6668 | lr 0.00038726 | gnorm 0.532 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 29976
2022-03-02 18:58:44 | INFO | fairseq.trainer | begin training epoch 18
2022-03-02 18:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 19:01:07 | INFO | train_inner | epoch 018:     32 / 393 loss=6.636, ppl=99.44, wps=14332.5, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.525, loss_scale=16, train_wall=439, gb_free=10.1, wall=30118
2022-03-02 19:03:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 19:08:38 | INFO | train_inner | epoch 018:    133 / 393 loss=6.53, ppl=92.43, wps=14543.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.535, loss_scale=16, train_wall=446, gb_free=10.1, wall=30569
2022-03-02 19:16:04 | INFO | train_inner | epoch 018:    233 / 393 loss=6.574, ppl=95.24, wps=14690.2, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.542, loss_scale=16, train_wall=441, gb_free=10.1, wall=31015
2022-03-02 19:23:30 | INFO | train_inner | epoch 018:    333 / 393 loss=6.615, ppl=98.05, wps=14689.9, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.526, loss_scale=16, train_wall=441, gb_free=10.1, wall=31461
2022-03-02 19:25:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 19:27:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 19:28:02 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.375 | ppl 166.03 | wps 33979.9 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 7.368
2022-03-02 19:28:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-02 19:28:02 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-02 19:28:02 | INFO | train | epoch 018 | loss 6.578 | ppl 95.52 | wps 14561.6 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.54 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 31733
2022-03-02 19:28:02 | INFO | fairseq.trainer | begin training epoch 19
2022-03-02 19:28:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 19:31:05 | INFO | train_inner | epoch 019:     41 / 393 loss=6.559, ppl=94.29, wps=14337.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.549, loss_scale=8, train_wall=444, gb_free=10.1, wall=31916
2022-03-02 19:38:31 | INFO | train_inner | epoch 019:    141 / 393 loss=6.485, ppl=89.54, wps=14692.5, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.541, loss_scale=8, train_wall=441, gb_free=10.1, wall=32362
2022-03-02 19:45:57 | INFO | train_inner | epoch 019:    241 / 393 loss=6.517, ppl=91.55, wps=14694.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.539, loss_scale=8, train_wall=441, gb_free=10.1, wall=32808
2022-03-02 19:53:23 | INFO | train_inner | epoch 019:    341 / 393 loss=6.551, ppl=93.77, wps=14690.4, ups=0.22, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.568, loss_scale=8, train_wall=441, gb_free=10.1, wall=33254
2022-03-02 19:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 19:57:19 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.378 | ppl 166.36 | wps 34071.2 | wpb 2034.1 | bsz 4 | num_updates 7452 | best_loss 7.368
2022-03-02 19:57:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7452 updates
2022-03-02 19:57:19 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-02 19:57:19 | INFO | train | epoch 019 | loss 6.518 | ppl 91.64 | wps 14637.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7452 | lr 0.000366322 | gnorm 0.544 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 33491
2022-03-02 19:57:19 | INFO | fairseq.trainer | begin training epoch 20
2022-03-02 19:57:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 20:00:54 | INFO | train_inner | epoch 020:     48 / 393 loss=6.486, ppl=89.63, wps=14480.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.531, loss_scale=8, train_wall=439, gb_free=10.1, wall=33705
2022-03-02 20:08:20 | INFO | train_inner | epoch 020:    148 / 393 loss=6.427, ppl=86.05, wps=14691.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.553, loss_scale=16, train_wall=441, gb_free=10.1, wall=34151
2022-03-02 20:15:46 | INFO | train_inner | epoch 020:    248 / 393 loss=6.466, ppl=88.4, wps=14686.2, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.556, loss_scale=16, train_wall=441, gb_free=10.1, wall=34597
2022-03-02 20:23:12 | INFO | train_inner | epoch 020:    348 / 393 loss=6.504, ppl=90.77, wps=14684, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.533, loss_scale=16, train_wall=441, gb_free=10.1, wall=35044
2022-03-02 20:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 20:26:37 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.379 | ppl 166.5 | wps 34019 | wpb 2034.1 | bsz 4 | num_updates 7845 | best_loss 7.368
2022-03-02 20:26:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7845 updates
2022-03-02 20:26:37 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-02 20:26:37 | INFO | train | epoch 020 | loss 6.463 | ppl 88.23 | wps 14633.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7845 | lr 0.000357029 | gnorm 0.545 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 35249
2022-03-02 20:26:37 | INFO | fairseq.trainer | begin training epoch 21
2022-03-02 20:26:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 20:29:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 20:30:47 | INFO | train_inner | epoch 021:     56 / 393 loss=6.426, ppl=85.96, wps=14334.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.535, loss_scale=8, train_wall=444, gb_free=10.1, wall=35499
2022-03-02 20:38:14 | INFO | train_inner | epoch 021:    156 / 393 loss=6.38, ppl=83.28, wps=14690, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.539, loss_scale=8, train_wall=441, gb_free=10.1, wall=35945
2022-03-02 20:45:40 | INFO | train_inner | epoch 021:    256 / 393 loss=6.425, ppl=85.9, wps=14690.9, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.557, loss_scale=8, train_wall=441, gb_free=10.1, wall=36391
2022-03-02 20:53:06 | INFO | train_inner | epoch 021:    356 / 393 loss=6.44, ppl=86.81, wps=14691.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.542, loss_scale=8, train_wall=441, gb_free=10.1, wall=36837
2022-03-02 20:55:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 20:55:55 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.389 | ppl 167.65 | wps 34016.6 | wpb 2034.1 | bsz 4 | num_updates 8237 | best_loss 7.368
2022-03-02 20:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8237 updates
2022-03-02 20:55:55 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-02 20:55:55 | INFO | train | epoch 021 | loss 6.412 | ppl 85.18 | wps 14598.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8237 | lr 0.00034843 | gnorm 0.542 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 37007
2022-03-02 20:55:55 | INFO | fairseq.trainer | begin training epoch 22
2022-03-02 20:55:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 21:00:36 | INFO | train_inner | epoch 022:     63 / 393 loss=6.369, ppl=82.68, wps=14479.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.541, loss_scale=8, train_wall=439, gb_free=10.1, wall=37288
2022-03-02 21:08:02 | INFO | train_inner | epoch 022:    163 / 393 loss=6.336, ppl=80.8, wps=14687.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.538, loss_scale=16, train_wall=441, gb_free=10.1, wall=37734
2022-03-02 21:15:29 | INFO | train_inner | epoch 022:    263 / 393 loss=6.375, ppl=83, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.542, loss_scale=16, train_wall=441, gb_free=10.1, wall=38180
2022-03-02 21:16:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 21:22:59 | INFO | train_inner | epoch 022:    364 / 393 loss=6.413, ppl=85.23, wps=14548, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.551, loss_scale=8, train_wall=446, gb_free=10.1, wall=38631
2022-03-02 21:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 21:25:13 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.392 | ppl 167.94 | wps 34038.8 | wpb 2034.1 | bsz 4 | num_updates 8629 | best_loss 7.368
2022-03-02 21:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8629 updates
2022-03-02 21:25:13 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-02 21:25:13 | INFO | train | epoch 022 | loss 6.366 | ppl 82.49 | wps 14598.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8629 | lr 0.000340424 | gnorm 0.547 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 38764
2022-03-02 21:25:13 | INFO | fairseq.trainer | begin training epoch 23
2022-03-02 21:25:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 21:30:30 | INFO | train_inner | epoch 023:     71 / 393 loss=6.301, ppl=78.83, wps=14474.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.566, loss_scale=8, train_wall=439, gb_free=10.1, wall=39081
2022-03-02 21:37:56 | INFO | train_inner | epoch 023:    171 / 393 loss=6.292, ppl=78.38, wps=14693.4, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.563, loss_scale=8, train_wall=441, gb_free=10.1, wall=39527
2022-03-02 21:45:22 | INFO | train_inner | epoch 023:    271 / 393 loss=6.338, ppl=80.9, wps=14690.3, ups=0.22, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.566, loss_scale=8, train_wall=441, gb_free=10.1, wall=39973
2022-03-02 21:52:48 | INFO | train_inner | epoch 023:    371 / 393 loss=6.37, ppl=82.74, wps=14692.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.548, loss_scale=8, train_wall=441, gb_free=10.1, wall=40419
2022-03-02 21:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 21:54:31 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.403 | ppl 169.22 | wps 33981.2 | wpb 2034.1 | bsz 4 | num_updates 9022 | best_loss 7.368
2022-03-02 21:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9022 updates
2022-03-02 21:54:31 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-02 21:54:31 | INFO | train | epoch 023 | loss 6.322 | ppl 79.99 | wps 14636.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9022 | lr 0.000332927 | gnorm 0.559 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 40522
2022-03-02 21:54:31 | INFO | fairseq.trainer | begin training epoch 24
2022-03-02 21:54:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 22:00:19 | INFO | train_inner | epoch 024:     78 / 393 loss=6.251, ppl=76.19, wps=14476.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.563, loss_scale=16, train_wall=439, gb_free=10.1, wall=40870
2022-03-02 22:04:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 22:07:49 | INFO | train_inner | epoch 024:    179 / 393 loss=6.252, ppl=76.24, wps=14544.5, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.553, loss_scale=8, train_wall=446, gb_free=10.1, wall=41321
2022-03-02 22:15:15 | INFO | train_inner | epoch 024:    279 / 393 loss=6.301, ppl=78.87, wps=14689.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.564, loss_scale=8, train_wall=441, gb_free=10.1, wall=41767
2022-03-02 22:22:42 | INFO | train_inner | epoch 024:    379 / 393 loss=6.33, ppl=80.44, wps=14690.6, ups=0.22, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.552, loss_scale=8, train_wall=441, gb_free=10.1, wall=42213
2022-03-02 22:23:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 22:23:49 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.428 | ppl 172.27 | wps 34110.7 | wpb 2034.1 | bsz 4 | num_updates 9414 | best_loss 7.368
2022-03-02 22:23:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9414 updates
2022-03-02 22:23:49 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-02 22:23:49 | INFO | train | epoch 024 | loss 6.281 | ppl 77.74 | wps 14597.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9414 | lr 0.000325921 | gnorm 0.558 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 42280
2022-03-02 22:23:49 | INFO | fairseq.trainer | begin training epoch 25
2022-03-02 22:23:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 22:30:12 | INFO | train_inner | epoch 025:     86 / 393 loss=6.2, ppl=73.51, wps=14472.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.569, loss_scale=8, train_wall=439, gb_free=10.1, wall=42664
2022-03-02 22:37:39 | INFO | train_inner | epoch 025:    186 / 393 loss=6.219, ppl=74.5, wps=14691.6, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.578, loss_scale=8, train_wall=441, gb_free=10.1, wall=43110
2022-03-02 22:45:05 | INFO | train_inner | epoch 025:    286 / 393 loss=6.257, ppl=76.46, wps=14685.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.556, loss_scale=16, train_wall=441, gb_free=10.1, wall=43556
2022-03-02 22:52:31 | INFO | train_inner | epoch 025:    386 / 393 loss=6.303, ppl=78.95, wps=14687.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.548, loss_scale=16, train_wall=441, gb_free=10.1, wall=44002
2022-03-02 22:53:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 22:53:07 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.43 | ppl 172.45 | wps 34031.7 | wpb 2034.1 | bsz 4 | num_updates 9807 | best_loss 7.368
2022-03-02 22:53:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9807 updates
2022-03-02 22:53:07 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-02 22:53:07 | INFO | train | epoch 025 | loss 6.242 | ppl 75.68 | wps 14632.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9807 | lr 0.000319324 | gnorm 0.562 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 44038
2022-03-02 22:53:07 | INFO | fairseq.trainer | begin training epoch 26
2022-03-02 22:53:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 22:54:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 23:00:06 | INFO | train_inner | epoch 026:     94 / 393 loss=6.152, ppl=71.13, wps=14337.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.564, loss_scale=8, train_wall=444, gb_free=10.1, wall=44457
2022-03-02 23:07:32 | INFO | train_inner | epoch 026:    194 / 393 loss=6.19, ppl=72.99, wps=14689.5, ups=0.22, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.566, loss_scale=8, train_wall=441, gb_free=10.1, wall=44904
2022-03-02 23:14:58 | INFO | train_inner | epoch 026:    294 / 393 loss=6.224, ppl=74.72, wps=14690.8, ups=0.22, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.606, loss_scale=8, train_wall=441, gb_free=10.1, wall=45350
2022-03-02 23:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 23:22:24 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.443 | ppl 174 | wps 34050.3 | wpb 2034.1 | bsz 4 | num_updates 10199 | best_loss 7.368
2022-03-02 23:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10199 updates
2022-03-02 23:22:24 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-02 23:22:24 | INFO | train | epoch 026 | loss 6.206 | ppl 73.81 | wps 14598.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10199 | lr 0.000313127 | gnorm 0.574 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 45796
2022-03-02 23:22:24 | INFO | fairseq.trainer | begin training epoch 27
2022-03-02 23:22:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 23:22:29 | INFO | train_inner | epoch 027:      1 / 393 loss=6.264, ppl=76.85, wps=14476.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.558, loss_scale=8, train_wall=439, gb_free=10.1, wall=45800
2022-03-02 23:29:55 | INFO | train_inner | epoch 027:    101 / 393 loss=6.104, ppl=68.8, wps=14690.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.563, loss_scale=8, train_wall=441, gb_free=10.1, wall=46246
2022-03-02 23:35:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 23:37:26 | INFO | train_inner | epoch 027:    202 / 393 loss=6.157, ppl=71.38, wps=14544.3, ups=0.22, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.591, loss_scale=8, train_wall=446, gb_free=10.1, wall=46697
2022-03-02 23:44:52 | INFO | train_inner | epoch 027:    302 / 393 loss=6.197, ppl=73.38, wps=14688.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.567, loss_scale=8, train_wall=441, gb_free=10.1, wall=47143
2022-03-02 23:51:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 23:51:42 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.454 | ppl 175.32 | wps 34081.2 | wpb 2034.1 | bsz 4 | num_updates 10591 | best_loss 7.368
2022-03-02 23:51:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10591 updates
2022-03-02 23:51:42 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-02 23:51:42 | INFO | train | epoch 027 | loss 6.172 | ppl 72.09 | wps 14598.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10591 | lr 0.000307278 | gnorm 0.58 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 47554
2022-03-02 23:51:42 | INFO | fairseq.trainer | begin training epoch 28
2022-03-02 23:51:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 23:52:22 | INFO | train_inner | epoch 028:      9 / 393 loss=6.217, ppl=74.41, wps=14480, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.597, loss_scale=8, train_wall=439, gb_free=10.1, wall=47594
2022-03-02 23:59:49 | INFO | train_inner | epoch 028:    109 / 393 loss=6.073, ppl=67.34, wps=14690.6, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.553, loss_scale=8, train_wall=441, gb_free=10.1, wall=48040
2022-03-03 00:07:15 | INFO | train_inner | epoch 028:    209 / 393 loss=6.123, ppl=69.67, wps=14690.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=48486
2022-03-03 00:14:41 | INFO | train_inner | epoch 028:    309 / 393 loss=6.17, ppl=71.98, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.602, loss_scale=16, train_wall=441, gb_free=10.1, wall=48932
2022-03-03 00:14:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 00:20:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 00:21:00 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.485 | ppl 179.14 | wps 34010.3 | wpb 2034.1 | bsz 4 | num_updates 10983 | best_loss 7.368
2022-03-03 00:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10983 updates
2022-03-03 00:21:00 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-03 00:21:00 | INFO | train | epoch 028 | loss 6.14 | ppl 70.52 | wps 14596.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10983 | lr 0.000301745 | gnorm 0.581 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 49312
2022-03-03 00:21:00 | INFO | fairseq.trainer | begin training epoch 29
2022-03-03 00:21:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 00:22:16 | INFO | train_inner | epoch 029:     17 / 393 loss=6.185, ppl=72.76, wps=14331.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.598, loss_scale=8, train_wall=444, gb_free=10.1, wall=49388
2022-03-03 00:29:42 | INFO | train_inner | epoch 029:    117 / 393 loss=6.054, ppl=66.45, wps=14685.5, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.569, loss_scale=8, train_wall=441, gb_free=10.1, wall=49834
2022-03-03 00:37:09 | INFO | train_inner | epoch 029:    217 / 393 loss=6.099, ppl=68.52, wps=14686.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.591, loss_scale=8, train_wall=441, gb_free=10.1, wall=50280
2022-03-03 00:44:35 | INFO | train_inner | epoch 029:    317 / 393 loss=6.145, ppl=70.75, wps=14690.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.581, loss_scale=8, train_wall=441, gb_free=10.1, wall=50726
2022-03-03 00:50:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 00:50:18 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.494 | ppl 180.26 | wps 34054.6 | wpb 2034.1 | bsz 4 | num_updates 11376 | best_loss 7.368
2022-03-03 00:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11376 updates
2022-03-03 00:50:18 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-03 00:50:18 | INFO | train | epoch 029 | loss 6.109 | ppl 69.03 | wps 14633 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11376 | lr 0.000296487 | gnorm 0.582 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 51070
2022-03-03 00:50:18 | INFO | fairseq.trainer | begin training epoch 30
2022-03-03 00:50:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 00:52:05 | INFO | train_inner | epoch 030:     24 / 393 loss=6.129, ppl=69.98, wps=14475.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.581, loss_scale=8, train_wall=439, gb_free=10.1, wall=51177
2022-03-03 00:55:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 00:59:36 | INFO | train_inner | epoch 030:    125 / 393 loss=6.034, ppl=65.51, wps=14544.8, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.592, loss_scale=8, train_wall=446, gb_free=10.1, wall=51627
2022-03-03 01:07:02 | INFO | train_inner | epoch 030:    225 / 393 loss=6.065, ppl=66.95, wps=14690.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.579, loss_scale=8, train_wall=441, gb_free=10.1, wall=52074
2022-03-03 01:14:28 | INFO | train_inner | epoch 030:    325 / 393 loss=6.114, ppl=69.26, wps=14691.1, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.601, loss_scale=8, train_wall=441, gb_free=10.1, wall=52520
2022-03-03 01:19:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 01:19:36 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.508 | ppl 182.06 | wps 34046.7 | wpb 2034.1 | bsz 4 | num_updates 11768 | best_loss 7.368
2022-03-03 01:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11768 updates
2022-03-03 01:19:36 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-03 01:19:36 | INFO | train | epoch 030 | loss 6.08 | ppl 67.63 | wps 14597.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11768 | lr 0.000291507 | gnorm 0.589 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 52828
2022-03-03 01:19:36 | INFO | fairseq.trainer | begin training epoch 31
2022-03-03 01:19:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 01:21:59 | INFO | train_inner | epoch 031:     32 / 393 loss=6.085, ppl=67.89, wps=14474.3, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.592, loss_scale=8, train_wall=439, gb_free=10.1, wall=52970
2022-03-03 01:29:25 | INFO | train_inner | epoch 031:    132 / 393 loss=6.004, ppl=64.17, wps=14692.5, ups=0.22, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=53416
2022-03-03 01:36:51 | INFO | train_inner | epoch 031:    232 / 393 loss=6.055, ppl=66.48, wps=14690.4, ups=0.22, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.603, loss_scale=16, train_wall=441, gb_free=10.1, wall=53863
2022-03-03 01:37:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 01:44:22 | INFO | train_inner | epoch 031:    333 / 393 loss=6.093, ppl=68.24, wps=14544, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.6, loss_scale=8, train_wall=446, gb_free=10.1, wall=54313
2022-03-03 01:48:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 01:48:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.516 | ppl 182.98 | wps 33973.2 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 7.368
2022-03-03 01:48:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12160 updates
2022-03-03 01:48:54 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-03 01:48:54 | INFO | train | epoch 031 | loss 6.053 | ppl 66.38 | wps 14598.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12160 | lr 0.00028677 | gnorm 0.599 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 54585
2022-03-03 01:48:54 | INFO | fairseq.trainer | begin training epoch 32
2022-03-03 01:48:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 01:51:53 | INFO | train_inner | epoch 032:     40 / 393 loss=6.039, ppl=65.76, wps=14471.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.605, loss_scale=8, train_wall=439, gb_free=10.1, wall=54764
2022-03-03 01:59:19 | INFO | train_inner | epoch 032:    140 / 393 loss=5.977, ppl=62.98, wps=14688.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.588, loss_scale=8, train_wall=441, gb_free=10.1, wall=55210
2022-03-03 02:06:45 | INFO | train_inner | epoch 032:    240 / 393 loss=6.034, ppl=65.53, wps=14692.3, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.596, loss_scale=8, train_wall=441, gb_free=10.1, wall=55656
2022-03-03 02:14:11 | INFO | train_inner | epoch 032:    340 / 393 loss=6.066, ppl=67, wps=14689.6, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.615, loss_scale=8, train_wall=441, gb_free=10.1, wall=56102
2022-03-03 02:18:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 02:18:12 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.534 | ppl 185.39 | wps 34041.6 | wpb 2034.1 | bsz 4 | num_updates 12553 | best_loss 7.368
2022-03-03 02:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12553 updates
2022-03-03 02:18:12 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-03 02:18:12 | INFO | train | epoch 032 | loss 6.027 | ppl 65.19 | wps 14634.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12553 | lr 0.000282245 | gnorm 0.601 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 56343
2022-03-03 02:18:12 | INFO | fairseq.trainer | begin training epoch 33
2022-03-03 02:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 02:18:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 02:21:46 | INFO | train_inner | epoch 033:     48 / 393 loss=6.021, ppl=64.92, wps=14338.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.604, loss_scale=8, train_wall=444, gb_free=10.1, wall=56557
2022-03-03 02:29:12 | INFO | train_inner | epoch 033:    148 / 393 loss=5.963, ppl=62.39, wps=14689.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=57004
2022-03-03 02:36:38 | INFO | train_inner | epoch 033:    248 / 393 loss=5.997, ppl=63.88, wps=14692.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.632, loss_scale=8, train_wall=441, gb_free=10.1, wall=57450
2022-03-03 02:44:04 | INFO | train_inner | epoch 033:    348 / 393 loss=6.043, ppl=65.94, wps=14688.2, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.623, loss_scale=8, train_wall=441, gb_free=10.1, wall=57896
2022-03-03 02:47:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 02:47:30 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.549 | ppl 187.32 | wps 34136.5 | wpb 2034.1 | bsz 4 | num_updates 12945 | best_loss 7.368
2022-03-03 02:47:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12945 updates
2022-03-03 02:47:30 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-03 02:47:30 | INFO | train | epoch 033 | loss 6.001 | ppl 64.06 | wps 14599.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12945 | lr 0.000277939 | gnorm 0.615 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 58101
2022-03-03 02:47:30 | INFO | fairseq.trainer | begin training epoch 34
2022-03-03 02:47:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 02:51:35 | INFO | train_inner | epoch 034:     55 / 393 loss=5.979, ppl=63.08, wps=14482.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.607, loss_scale=8, train_wall=439, gb_free=10.1, wall=58346
2022-03-03 02:57:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 02:59:05 | INFO | train_inner | epoch 034:    156 / 393 loss=5.933, ppl=61.1, wps=14548.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.613, loss_scale=8, train_wall=445, gb_free=10.1, wall=58797
2022-03-03 03:06:31 | INFO | train_inner | epoch 034:    256 / 393 loss=5.987, ppl=63.41, wps=14696.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.597, loss_scale=8, train_wall=441, gb_free=10.1, wall=59243
2022-03-03 03:13:57 | INFO | train_inner | epoch 034:    356 / 393 loss=6.026, ppl=65.17, wps=14689.4, ups=0.22, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.628, loss_scale=8, train_wall=441, gb_free=10.1, wall=59689
2022-03-03 03:16:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 03:16:47 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.567 | ppl 189.68 | wps 34045.9 | wpb 2034.1 | bsz 4 | num_updates 13337 | best_loss 7.368
2022-03-03 03:16:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13337 updates
2022-03-03 03:16:47 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-03 03:16:47 | INFO | train | epoch 034 | loss 5.978 | ppl 63.02 | wps 14602.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13337 | lr 0.000273824 | gnorm 0.609 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 59858
2022-03-03 03:16:47 | INFO | fairseq.trainer | begin training epoch 35
2022-03-03 03:16:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 03:21:28 | INFO | train_inner | epoch 035:     63 / 393 loss=5.948, ppl=61.74, wps=14480, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.605, loss_scale=8, train_wall=439, gb_free=10.1, wall=60139
2022-03-03 03:28:54 | INFO | train_inner | epoch 035:    163 / 393 loss=5.918, ppl=60.48, wps=14687.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.621, loss_scale=8, train_wall=441, gb_free=10.1, wall=60586
2022-03-03 03:36:20 | INFO | train_inner | epoch 035:    263 / 393 loss=5.969, ppl=62.63, wps=14691.2, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.611, loss_scale=16, train_wall=441, gb_free=10.1, wall=61032
2022-03-03 03:37:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 03:43:51 | INFO | train_inner | epoch 035:    364 / 393 loss=6.002, ppl=64.11, wps=14544.5, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.633, loss_scale=8, train_wall=446, gb_free=10.1, wall=61482
2022-03-03 03:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 03:46:05 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.57 | ppl 190.01 | wps 34095.6 | wpb 2034.1 | bsz 4 | num_updates 13729 | best_loss 7.368
2022-03-03 03:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13729 updates
2022-03-03 03:46:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-03 03:46:05 | INFO | train | epoch 035 | loss 5.955 | ppl 62.05 | wps 14597.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13729 | lr 0.000269886 | gnorm 0.621 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 61616
2022-03-03 03:46:05 | INFO | fairseq.trainer | begin training epoch 36
2022-03-03 03:46:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 03:51:22 | INFO | train_inner | epoch 036:     71 / 393 loss=5.913, ppl=60.26, wps=14476.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.642, loss_scale=8, train_wall=439, gb_free=10.1, wall=61933
2022-03-03 03:58:48 | INFO | train_inner | epoch 036:    171 / 393 loss=5.907, ppl=60.01, wps=14688.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.624, loss_scale=8, train_wall=441, gb_free=10.1, wall=62379
2022-03-03 04:06:14 | INFO | train_inner | epoch 036:    271 / 393 loss=5.945, ppl=61.61, wps=14691.3, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.644, loss_scale=8, train_wall=441, gb_free=10.1, wall=62825
2022-03-03 04:13:40 | INFO | train_inner | epoch 036:    371 / 393 loss=5.981, ppl=63.18, wps=14692.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.622, loss_scale=8, train_wall=441, gb_free=10.1, wall=63271
2022-03-03 04:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 04:15:23 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.586 | ppl 192.11 | wps 34166.9 | wpb 2034.1 | bsz 4 | num_updates 14122 | best_loss 7.368
2022-03-03 04:15:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14122 updates
2022-03-03 04:15:23 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-03 04:15:23 | INFO | train | epoch 036 | loss 5.934 | ppl 61.13 | wps 14635.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14122 | lr 0.000266104 | gnorm 0.632 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 63374
2022-03-03 04:15:23 | INFO | fairseq.trainer | begin training epoch 37
2022-03-03 04:15:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 04:15:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 04:21:15 | INFO | train_inner | epoch 037:     79 / 393 loss=5.87, ppl=58.5, wps=14338.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.62, loss_scale=8, train_wall=444, gb_free=10.1, wall=63726
2022-03-03 04:22:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 04:28:45 | INFO | train_inner | epoch 037:    180 / 393 loss=5.887, ppl=59.18, wps=14550.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.652, loss_scale=4, train_wall=445, gb_free=10.1, wall=64177
2022-03-03 04:36:11 | INFO | train_inner | epoch 037:    280 / 393 loss=5.931, ppl=61, wps=14696.8, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.62, loss_scale=4, train_wall=441, gb_free=10.1, wall=64623
2022-03-03 04:43:37 | INFO | train_inner | epoch 037:    380 / 393 loss=5.976, ppl=62.94, wps=14699.3, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.637, loss_scale=4, train_wall=441, gb_free=10.1, wall=65068
2022-03-03 04:44:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 04:44:39 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.6 | ppl 194.01 | wps 34050.4 | wpb 2034.1 | bsz 4 | num_updates 14513 | best_loss 7.368
2022-03-03 04:44:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14513 updates
2022-03-03 04:44:39 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-03 04:44:39 | INFO | train | epoch 037 | loss 5.913 | ppl 60.25 | wps 14568 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 14513 | lr 0.000262495 | gnorm 0.633 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 65131
2022-03-03 04:44:39 | INFO | fairseq.trainer | begin training epoch 38
2022-03-03 04:44:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 04:51:07 | INFO | train_inner | epoch 038:     87 / 393 loss=5.847, ppl=57.57, wps=14486.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.629, loss_scale=4, train_wall=439, gb_free=10.1, wall=65519
2022-03-03 04:58:33 | INFO | train_inner | epoch 038:    187 / 393 loss=5.866, ppl=58.32, wps=14696.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.632, loss_scale=4, train_wall=441, gb_free=10.1, wall=65965
2022-03-03 05:05:59 | INFO | train_inner | epoch 038:    287 / 393 loss=5.914, ppl=60.3, wps=14696.5, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.631, loss_scale=8, train_wall=441, gb_free=10.1, wall=66411
2022-03-03 05:13:25 | INFO | train_inner | epoch 038:    387 / 393 loss=5.956, ppl=62.07, wps=14690.7, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.642, loss_scale=8, train_wall=441, gb_free=10.1, wall=66857
2022-03-03 05:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 05:13:57 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.605 | ppl 194.63 | wps 34121.7 | wpb 2034.1 | bsz 4 | num_updates 14906 | best_loss 7.368
2022-03-03 05:13:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14906 updates
2022-03-03 05:13:57 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-03 05:13:57 | INFO | train | epoch 038 | loss 5.894 | ppl 59.48 | wps 14640.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14906 | lr 0.000259012 | gnorm 0.634 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 66888
2022-03-03 05:13:57 | INFO | fairseq.trainer | begin training epoch 39
2022-03-03 05:13:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 05:20:56 | INFO | train_inner | epoch 039:     94 / 393 loss=5.81, ppl=56.11, wps=14477.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.626, loss_scale=8, train_wall=439, gb_free=10.1, wall=67307
2022-03-03 05:28:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 05:28:27 | INFO | train_inner | epoch 039:    195 / 393 loss=5.858, ppl=58, wps=14547.4, ups=0.22, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.659, loss_scale=4, train_wall=446, gb_free=10.1, wall=67758
2022-03-03 05:35:52 | INFO | train_inner | epoch 039:    295 / 393 loss=5.903, ppl=59.84, wps=14698.3, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.638, loss_scale=4, train_wall=441, gb_free=10.1, wall=68204
2022-03-03 05:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 05:43:14 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.641 | ppl 199.63 | wps 34045.4 | wpb 2034.1 | bsz 4 | num_updates 15298 | best_loss 7.368
2022-03-03 05:43:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15298 updates
2022-03-03 05:43:14 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-03 05:43:14 | INFO | train | epoch 039 | loss 5.874 | ppl 58.66 | wps 14602 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15298 | lr 0.000255672 | gnorm 0.641 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 68645
2022-03-03 05:43:14 | INFO | fairseq.trainer | begin training epoch 40
2022-03-03 05:43:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 05:43:23 | INFO | train_inner | epoch 040:      2 / 393 loss=5.93, ppl=60.98, wps=14482.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.646, loss_scale=4, train_wall=439, gb_free=10.1, wall=68654
2022-03-03 05:50:49 | INFO | train_inner | epoch 040:    102 / 393 loss=5.788, ppl=55.27, wps=14697.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.637, loss_scale=4, train_wall=441, gb_free=10.1, wall=69100
2022-03-03 05:58:15 | INFO | train_inner | epoch 040:    202 / 393 loss=5.835, ppl=57.1, wps=14695.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.632, loss_scale=4, train_wall=441, gb_free=10.1, wall=69546
2022-03-03 06:05:41 | INFO | train_inner | epoch 040:    302 / 393 loss=5.883, ppl=59.03, wps=14694.9, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.657, loss_scale=4, train_wall=441, gb_free=10.1, wall=69992
2022-03-03 06:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 06:12:31 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.647 | ppl 200.5 | wps 33973.6 | wpb 2034.1 | bsz 4 | num_updates 15691 | best_loss 7.368
2022-03-03 06:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15691 updates
2022-03-03 06:12:31 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-03 06:12:31 | INFO | train | epoch 040 | loss 5.857 | ppl 57.95 | wps 14640.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15691 | lr 0.00025245 | gnorm 0.645 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 70403
2022-03-03 06:12:31 | INFO | fairseq.trainer | begin training epoch 41
2022-03-03 06:12:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 06:13:11 | INFO | train_inner | epoch 041:      9 / 393 loss=5.91, ppl=60.11, wps=14478.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.65, loss_scale=8, train_wall=439, gb_free=10.1, wall=70443
2022-03-03 06:20:37 | INFO | train_inner | epoch 041:    109 / 393 loss=5.771, ppl=54.61, wps=14694.3, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.64, loss_scale=8, train_wall=441, gb_free=10.1, wall=70889
2022-03-03 06:28:04 | INFO | train_inner | epoch 041:    209 / 393 loss=5.828, ppl=56.81, wps=14688.4, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.634, loss_scale=8, train_wall=441, gb_free=10.1, wall=71335
2022-03-03 06:35:30 | INFO | train_inner | epoch 041:    309 / 393 loss=5.867, ppl=58.38, wps=14692.2, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.646, loss_scale=8, train_wall=441, gb_free=10.1, wall=71781
2022-03-03 06:36:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 06:41:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 06:41:49 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.653 | ppl 201.3 | wps 33963.6 | wpb 2034.1 | bsz 4 | num_updates 16083 | best_loss 7.368
2022-03-03 06:41:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16083 updates
2022-03-03 06:41:49 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-03 06:41:49 | INFO | train | epoch 041 | loss 5.839 | ppl 57.25 | wps 14600.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16083 | lr 0.000249354 | gnorm 0.647 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 72160
2022-03-03 06:41:49 | INFO | fairseq.trainer | begin training epoch 42
2022-03-03 06:41:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 06:43:05 | INFO | train_inner | epoch 042:     17 / 393 loss=5.885, ppl=59.1, wps=14341.2, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.666, loss_scale=4, train_wall=443, gb_free=10.1, wall=72236
2022-03-03 06:50:31 | INFO | train_inner | epoch 042:    117 / 393 loss=5.766, ppl=54.42, wps=14695.2, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.634, loss_scale=4, train_wall=441, gb_free=10.1, wall=72682
2022-03-03 06:57:56 | INFO | train_inner | epoch 042:    217 / 393 loss=5.813, ppl=56.24, wps=14699, ups=0.22, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.656, loss_scale=4, train_wall=441, gb_free=10.1, wall=73128
2022-03-03 07:05:22 | INFO | train_inner | epoch 042:    317 / 393 loss=5.855, ppl=57.87, wps=14698.5, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.662, loss_scale=4, train_wall=441, gb_free=10.1, wall=73574
2022-03-03 07:10:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 07:11:06 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.662 | ppl 202.49 | wps 34041.8 | wpb 2034.1 | bsz 4 | num_updates 16476 | best_loss 7.368
2022-03-03 07:11:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16476 updates
2022-03-03 07:11:06 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-03 07:11:06 | INFO | train | epoch 042 | loss 5.823 | ppl 56.6 | wps 14642.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16476 | lr 0.000246362 | gnorm 0.647 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 73917
2022-03-03 07:11:06 | INFO | fairseq.trainer | begin training epoch 43
2022-03-03 07:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 07:12:53 | INFO | train_inner | epoch 043:     24 / 393 loss=5.841, ppl=57.31, wps=14480, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.639, loss_scale=4, train_wall=439, gb_free=10.1, wall=74024
2022-03-03 07:19:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 07:20:23 | INFO | train_inner | epoch 043:    125 / 393 loss=5.755, ppl=54.02, wps=14546.5, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.669, loss_scale=4, train_wall=446, gb_free=10.1, wall=74475
2022-03-03 07:27:50 | INFO | train_inner | epoch 043:    225 / 393 loss=5.798, ppl=55.63, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.656, loss_scale=4, train_wall=441, gb_free=10.1, wall=74921
2022-03-03 07:35:16 | INFO | train_inner | epoch 043:    325 / 393 loss=5.846, ppl=57.52, wps=14692.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.643, loss_scale=4, train_wall=441, gb_free=10.1, wall=75367
2022-03-03 07:40:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 07:40:23 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.708 | ppl 209.06 | wps 34000.1 | wpb 2034.1 | bsz 4 | num_updates 16868 | best_loss 7.368
2022-03-03 07:40:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16868 updates
2022-03-03 07:40:23 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-03 07:40:23 | INFO | train | epoch 043 | loss 5.807 | ppl 55.97 | wps 14599.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16868 | lr 0.000243483 | gnorm 0.656 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 75675
2022-03-03 07:40:23 | INFO | fairseq.trainer | begin training epoch 44
2022-03-03 07:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 07:42:46 | INFO | train_inner | epoch 044:     32 / 393 loss=5.817, ppl=56.39, wps=14481.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.657, loss_scale=4, train_wall=439, gb_free=10.1, wall=75818
2022-03-03 07:50:12 | INFO | train_inner | epoch 044:    132 / 393 loss=5.736, ppl=53.3, wps=14695.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.659, loss_scale=4, train_wall=441, gb_free=10.1, wall=76264
2022-03-03 07:57:38 | INFO | train_inner | epoch 044:    232 / 393 loss=5.789, ppl=55.31, wps=14696.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.665, loss_scale=4, train_wall=441, gb_free=10.1, wall=76709
2022-03-03 07:58:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 08:05:08 | INFO | train_inner | epoch 044:    333 / 393 loss=5.827, ppl=56.76, wps=14552.1, ups=0.22, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.678, loss_scale=4, train_wall=445, gb_free=10.1, wall=77160
2022-03-03 08:09:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 08:09:40 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.697 | ppl 207.53 | wps 34113.5 | wpb 2034.1 | bsz 4 | num_updates 17260 | best_loss 7.368
2022-03-03 08:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17260 updates
2022-03-03 08:09:40 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-03 08:09:40 | INFO | train | epoch 044 | loss 5.791 | ppl 55.38 | wps 14604.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17260 | lr 0.000240702 | gnorm 0.666 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 77432
2022-03-03 08:09:40 | INFO | fairseq.trainer | begin training epoch 45
2022-03-03 08:09:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 08:12:39 | INFO | train_inner | epoch 045:     40 / 393 loss=5.803, ppl=55.82, wps=14483.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.665, loss_scale=4, train_wall=439, gb_free=10.1, wall=77610
2022-03-03 08:20:05 | INFO | train_inner | epoch 045:    140 / 393 loss=5.733, ppl=53.2, wps=14695.2, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.652, loss_scale=4, train_wall=441, gb_free=10.1, wall=78056
2022-03-03 08:27:31 | INFO | train_inner | epoch 045:    240 / 393 loss=5.773, ppl=54.69, wps=14697.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.662, loss_scale=4, train_wall=441, gb_free=10.1, wall=78502
2022-03-03 08:34:57 | INFO | train_inner | epoch 045:    340 / 393 loss=5.821, ppl=56.54, wps=14699.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.668, loss_scale=4, train_wall=441, gb_free=10.1, wall=78948
2022-03-03 08:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 08:38:57 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.7 | ppl 207.98 | wps 34073.1 | wpb 2034.1 | bsz 4 | num_updates 17653 | best_loss 7.368
2022-03-03 08:38:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17653 updates
2022-03-03 08:38:57 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-03 08:38:57 | INFO | train | epoch 045 | loss 5.777 | ppl 54.84 | wps 14643.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17653 | lr 0.000238008 | gnorm 0.666 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 79189
2022-03-03 08:38:57 | INFO | fairseq.trainer | begin training epoch 46
2022-03-03 08:38:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 08:42:27 | INFO | train_inner | epoch 046:     47 / 393 loss=5.774, ppl=54.71, wps=14482.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.665, loss_scale=8, train_wall=439, gb_free=10.1, wall=79399
2022-03-03 08:49:53 | INFO | train_inner | epoch 046:    147 / 393 loss=5.717, ppl=52.61, wps=14691.9, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.675, loss_scale=8, train_wall=441, gb_free=10.1, wall=79845
2022-03-03 08:57:19 | INFO | train_inner | epoch 046:    247 / 393 loss=5.763, ppl=54.32, wps=14685, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.679, loss_scale=8, train_wall=441, gb_free=10.1, wall=80291
2022-03-03 09:04:45 | INFO | train_inner | epoch 046:    347 / 393 loss=5.811, ppl=56.13, wps=14692, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.675, loss_scale=8, train_wall=441, gb_free=10.1, wall=80737
2022-03-03 09:08:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 09:08:15 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.726 | ppl 211.73 | wps 34091.8 | wpb 2034.1 | bsz 4 | num_updates 18046 | best_loss 7.368
2022-03-03 09:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18046 updates
2022-03-03 09:08:15 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-03 09:08:15 | INFO | train | epoch 046 | loss 5.763 | ppl 54.31 | wps 14635.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18046 | lr 0.000235402 | gnorm 0.669 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 80947
2022-03-03 09:08:15 | INFO | fairseq.trainer | begin training epoch 47
2022-03-03 09:08:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 09:12:16 | INFO | train_inner | epoch 047:     54 / 393 loss=5.746, ppl=53.66, wps=14481.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.658, loss_scale=8, train_wall=439, gb_free=10.1, wall=81187
2022-03-03 09:16:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 09:19:47 | INFO | train_inner | epoch 047:    155 / 393 loss=5.705, ppl=52.17, wps=14536, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.67, loss_scale=8, train_wall=446, gb_free=10.1, wall=81638
2022-03-03 09:27:13 | INFO | train_inner | epoch 047:    255 / 393 loss=5.754, ppl=53.97, wps=14692.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.676, loss_scale=8, train_wall=441, gb_free=10.1, wall=82084
2022-03-03 09:34:39 | INFO | train_inner | epoch 047:    355 / 393 loss=5.801, ppl=55.75, wps=14690.1, ups=0.22, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.678, loss_scale=8, train_wall=441, gb_free=10.1, wall=82530
2022-03-03 09:37:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 09:37:33 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.717 | ppl 210.44 | wps 34035 | wpb 2034.1 | bsz 4 | num_updates 18438 | best_loss 7.368
2022-03-03 09:37:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18438 updates
2022-03-03 09:37:33 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-03 09:37:33 | INFO | train | epoch 047 | loss 5.749 | ppl 53.79 | wps 14597.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18438 | lr 0.000232886 | gnorm 0.672 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 82704
2022-03-03 09:37:33 | INFO | fairseq.trainer | begin training epoch 48
2022-03-03 09:37:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 09:42:10 | INFO | train_inner | epoch 048:     62 / 393 loss=5.723, ppl=52.81, wps=14478.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.68, loss_scale=8, train_wall=439, gb_free=10.1, wall=82981
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
