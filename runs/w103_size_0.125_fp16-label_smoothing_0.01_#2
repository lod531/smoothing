Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 207345368: <w103_size_0.125_fp16_label_smoothing_0.01_#2> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.01_#2> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 12:40:43 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Sun Mar  6 12:41:16 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 12:41:16 2022
Terminated at Mon Mar  7 07:04:29 2022
Results reported at Mon Mar  7 07:04:29 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.01 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

Exited.


The output (if any) follows:

2022-03-06 12:41:23 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.01, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 12:41:23 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-06 12:41:26 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-06 12:41:26 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 12:41:26 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 12:41:26 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-06 12:41:26 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-06 12:41:26 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 12:41:26 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-06 12:41:31 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 12:41:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:41:31 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-06 12:41:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:41:31 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 12:41:31 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 12:41:31 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 12:41:31 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 12:41:31 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 12:41:31 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-06 12:41:32 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 12:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:41:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 12:41:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 12:41:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 12:42:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 12:42:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 12:47:25 | INFO | train_inner | epoch 001:    105 / 196 loss=16.427, nll_loss=16.407, ppl=86920.1, wps=20819.9, ups=0.32, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.588, loss_scale=4, train_wall=330, gb_free=19.9, wall=354
2022-03-06 12:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 12:52:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.06 | nll_loss 13.005 | ppl 8219.89 | wps 41480.1 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-06 12:52:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-06 12:52:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 12:52:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 12:52:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.06) (writing took 6.701843552989885 seconds)
2022-03-06 12:52:23 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 12:52:23 | INFO | train | epoch 001 | loss 15.325 | nll_loss 15.294 | ppl 40181.1 | wps 20390.5 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.601 | loss_scale 8 | train_wall 596 | gb_free 19.9 | wall 652
2022-03-06 12:52:24 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 12:52:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:52:52 | INFO | train_inner | epoch 002:      9 / 196 loss=14.02, nll_loss=13.976, ppl=16115.8, wps=20010.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.493, loss_scale=8, train_wall=292, gb_free=19.9, wall=681
2022-03-06 12:58:08 | INFO | train_inner | epoch 002:    109 / 196 loss=12.075, nll_loss=12.008, ppl=4117.93, wps=20739.4, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.979, loss_scale=16, train_wall=293, gb_free=19.9, wall=997
2022-03-06 13:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:02:48 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.372 | nll_loss 10.275 | ppl 1238.98 | wps 41102.8 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.372
2022-03-06 13:02:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-06 13:02:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:02:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.372) (writing took 6.582131071947515 seconds)
2022-03-06 13:02:54 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 13:02:54 | INFO | train | epoch 002 | loss 11.55 | nll_loss 11.475 | ppl 2846.35 | wps 20336.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.824 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 1283
2022-03-06 13:02:54 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 13:02:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:03:36 | INFO | train_inner | epoch 003:     13 / 196 loss=10.734, nll_loss=10.645, ppl=1601.03, wps=19953.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.586, loss_scale=32, train_wall=293, gb_free=19.9, wall=1324
2022-03-06 13:08:51 | INFO | train_inner | epoch 003:    113 / 196 loss=10.153, nll_loss=10.048, ppl=1058.68, wps=20749, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.525, loss_scale=32, train_wall=293, gb_free=19.9, wall=1640
2022-03-06 13:09:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:13:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.668 | nll_loss 9.552 | ppl 750.52 | wps 40936.3 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.668
2022-03-06 13:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-06 13:13:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:13:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.668) (writing took 6.640753404935822 seconds)
2022-03-06 13:13:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 13:13:25 | INFO | train | epoch 003 | loss 10.044 | nll_loss 9.937 | ppl 979.97 | wps 20236.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.539 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 1914
2022-03-06 13:13:25 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 13:13:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:14:22 | INFO | train_inner | epoch 004:     18 / 196 loss=9.822, nll_loss=9.709, ppl=836.84, wps=19780.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.585, loss_scale=32, train_wall=296, gb_free=19.9, wall=1971
2022-03-06 13:15:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:19:41 | INFO | train_inner | epoch 004:    119 / 196 loss=9.519, nll_loss=9.4, ppl=675.44, wps=20529.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.701, loss_scale=16, train_wall=296, gb_free=19.9, wall=2290
2022-03-06 13:23:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:23:49 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.194 | nll_loss 9.067 | ppl 536.33 | wps 41244.1 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.194
2022-03-06 13:23:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-06 13:23:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:23:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.194) (writing took 7.249607951845974 seconds)
2022-03-06 13:23:56 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 13:23:56 | INFO | train | epoch 004 | loss 9.45 | nll_loss 9.33 | ppl 643.48 | wps 20211 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.724 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 2545
2022-03-06 13:23:56 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 13:23:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:25:09 | INFO | train_inner | epoch 005:     23 / 196 loss=9.271, nll_loss=9.147, ppl=566.79, wps=19922.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.742, loss_scale=32, train_wall=293, gb_free=19.9, wall=2618
2022-03-06 13:28:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:30:29 | INFO | train_inner | epoch 005:    124 / 196 loss=9.035, nll_loss=8.906, ppl=479.59, wps=20517.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.855, loss_scale=32, train_wall=297, gb_free=19.9, wall=2937
2022-03-06 13:34:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:34:19 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.794 | nll_loss 8.66 | ppl 404.38 | wps 41314.7 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 8.794
2022-03-06 13:34:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-06 13:34:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:34:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:34:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 5 @ 972 updates, score 8.794) (writing took 6.772463395027444 seconds)
2022-03-06 13:34:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 13:34:26 | INFO | train | epoch 005 | loss 8.982 | nll_loss 8.852 | ppl 462.01 | wps 20262.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.836 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 3175
2022-03-06 13:34:26 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 13:34:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:35:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:35:58 | INFO | train_inner | epoch 006:     29 / 196 loss=8.822, nll_loss=8.689, ppl=412.62, wps=19829.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.874, loss_scale=32, train_wall=295, gb_free=19.9, wall=3267
2022-03-06 13:41:14 | INFO | train_inner | epoch 006:    129 / 196 loss=8.62, nll_loss=8.483, ppl=357.79, wps=20731.4, ups=0.32, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.907, loss_scale=32, train_wall=294, gb_free=19.9, wall=3583
2022-03-06 13:42:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:44:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:44:51 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.467 | nll_loss 8.326 | ppl 320.87 | wps 39653.8 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.467
2022-03-06 13:44:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-06 13:44:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:44:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.467) (writing took 7.568263347027823 seconds)
2022-03-06 13:44:58 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 13:44:58 | INFO | train | epoch 006 | loss 8.593 | nll_loss 8.455 | ppl 350.97 | wps 20092.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.923 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 3807
2022-03-06 13:44:58 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 13:44:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:46:46 | INFO | train_inner | epoch 007:     34 / 196 loss=8.45, nll_loss=8.31, ppl=317.41, wps=19710.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.945, loss_scale=32, train_wall=296, gb_free=19.9, wall=3915
2022-03-06 13:50:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:52:06 | INFO | train_inner | epoch 007:    135 / 196 loss=8.285, nll_loss=8.141, ppl=282.34, wps=20484.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.923, loss_scale=32, train_wall=297, gb_free=19.9, wall=4235
2022-03-06 13:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:55:23 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.21 | nll_loss 8.061 | ppl 267.08 | wps 39666.6 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.21
2022-03-06 13:55:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-06 13:55:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:55:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 13:55:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 7 @ 1361 updates, score 8.21) (writing took 7.684371050912887 seconds)
2022-03-06 13:55:31 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 13:55:31 | INFO | train | epoch 007 | loss 8.267 | nll_loss 8.123 | ppl 278.83 | wps 20168.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.919 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 4440
2022-03-06 13:55:31 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 13:55:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:57:35 | INFO | train_inner | epoch 008:     39 / 196 loss=8.129, nll_loss=7.983, ppl=252.98, wps=19879, ups=0.3, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.937, loss_scale=64, train_wall=293, gb_free=19.9, wall=4564
2022-03-06 13:57:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:02:54 | INFO | train_inner | epoch 008:    140 / 196 loss=7.982, nll_loss=7.833, ppl=228, wps=20516, ups=0.31, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.953, loss_scale=32, train_wall=297, gb_free=19.9, wall=4883
2022-03-06 14:04:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:05:55 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.972 | nll_loss 7.821 | ppl 226.11 | wps 39787.2 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 7.972
2022-03-06 14:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-06 14:05:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:05:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 8 @ 1555 updates, score 7.972) (writing took 7.601873603183776 seconds)
2022-03-06 14:06:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 14:06:03 | INFO | train | epoch 008 | loss 7.974 | nll_loss 7.825 | ppl 226.77 | wps 20083.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 0.963 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 5072
2022-03-06 14:06:03 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 14:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:08:26 | INFO | train_inner | epoch 009:     45 / 196 loss=7.844, nll_loss=7.692, ppl=206.83, wps=19691.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.986, loss_scale=32, train_wall=296, gb_free=19.9, wall=5215
2022-03-06 14:11:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:13:46 | INFO | train_inner | epoch 009:    146 / 196 loss=7.705, nll_loss=7.551, ppl=187.54, wps=20511.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.951, loss_scale=32, train_wall=297, gb_free=19.9, wall=5534
2022-03-06 14:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 14:16:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:16:28 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.761 | nll_loss 7.606 | ppl 194.85 | wps 39374.8 | wpb 510.9 | bsz 1 | num_updates 1749 | best_loss 7.761
2022-03-06 14:16:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1749 updates
2022-03-06 14:16:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 9 @ 1749 updates, score 7.761) (writing took 7.709139409009367 seconds)
2022-03-06 14:16:36 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 14:16:36 | INFO | train | epoch 009 | loss 7.702 | nll_loss 7.548 | ppl 187.19 | wps 20070 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1749 | lr 0.000218681 | gnorm 0.965 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 5705
2022-03-06 14:16:36 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 14:16:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:19:16 | INFO | train_inner | epoch 010:     51 / 196 loss=7.569, nll_loss=7.412, ppl=170.37, wps=19768, ups=0.3, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.951, loss_scale=16, train_wall=295, gb_free=19.9, wall=5865
2022-03-06 14:24:32 | INFO | train_inner | epoch 010:    151 / 196 loss=7.443, nll_loss=7.285, ppl=155.9, wps=20761.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.962, loss_scale=32, train_wall=293, gb_free=19.9, wall=6181
2022-03-06 14:26:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:26:59 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.573 | nll_loss 7.412 | ppl 170.36 | wps 40085.7 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 7.573
2022-03-06 14:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-06 14:26:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:27:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:27:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 10 @ 1945 updates, score 7.573) (writing took 7.394820555113256 seconds)
2022-03-06 14:27:06 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 14:27:06 | INFO | train | epoch 010 | loss 7.446 | nll_loss 7.288 | ppl 156.25 | wps 20336.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.945 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 6335
2022-03-06 14:27:07 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 14:27:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:30:00 | INFO | train_inner | epoch 011:     55 / 196 loss=7.312, nll_loss=7.151, ppl=142.14, wps=19903.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.922, loss_scale=64, train_wall=293, gb_free=19.9, wall=6509
2022-03-06 14:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:35:20 | INFO | train_inner | epoch 011:    156 / 196 loss=7.202, nll_loss=7.039, ppl=131.53, wps=20510.7, ups=0.31, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.912, loss_scale=32, train_wall=297, gb_free=19.9, wall=6829
2022-03-06 14:37:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:37:32 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.441 | nll_loss 7.277 | ppl 155.09 | wps 39289.3 | wpb 510.9 | bsz 1 | num_updates 2140 | best_loss 7.441
2022-03-06 14:37:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2140 updates
2022-03-06 14:37:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:37:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:37:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 11 @ 2140 updates, score 7.441) (writing took 7.6293230298906565 seconds)
2022-03-06 14:37:39 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 14:37:39 | INFO | train | epoch 011 | loss 7.205 | nll_loss 7.042 | ppl 131.79 | wps 20171.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2140 | lr 0.000267547 | gnorm 0.915 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 6968
2022-03-06 14:37:39 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 14:37:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:39:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:40:52 | INFO | train_inner | epoch 012:     61 / 196 loss=7.072, nll_loss=6.906, ppl=119.95, wps=19679.8, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.9, loss_scale=32, train_wall=296, gb_free=19.9, wall=7161
2022-03-06 14:46:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:46:11 | INFO | train_inner | epoch 012:    162 / 196 loss=6.977, nll_loss=6.81, ppl=112.2, wps=20513.7, ups=0.31, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.917, loss_scale=32, train_wall=297, gb_free=19.9, wall=7480
2022-03-06 14:47:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:48:04 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.303 | nll_loss 7.138 | ppl 140.86 | wps 39450 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.303
2022-03-06 14:48:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-06 14:48:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:48:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.303) (writing took 7.66143510513939 seconds)
2022-03-06 14:48:12 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 14:48:12 | INFO | train | epoch 012 | loss 6.985 | nll_loss 6.818 | ppl 112.81 | wps 20072.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.905 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 7601
2022-03-06 14:48:12 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 14:48:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:51:40 | INFO | train_inner | epoch 013:     66 / 196 loss=6.847, nll_loss=6.678, ppl=102.38, wps=19877.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.891, loss_scale=32, train_wall=293, gb_free=19.9, wall=7809
2022-03-06 14:53:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:57:00 | INFO | train_inner | epoch 013:    167 / 196 loss=6.776, nll_loss=6.605, ppl=97.33, wps=20515.3, ups=0.31, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.892, loss_scale=32, train_wall=297, gb_free=19.9, wall=8129
2022-03-06 14:58:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:58:36 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.189 | nll_loss 7.019 | ppl 129.68 | wps 39508.3 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 7.189
2022-03-06 14:58:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-06 14:58:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:58:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 14:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 13 @ 2529 updates, score 7.189) (writing took 7.426843234105036 seconds)
2022-03-06 14:58:44 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 14:58:44 | INFO | train | epoch 013 | loss 6.779 | nll_loss 6.608 | ppl 97.56 | wps 20186.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.889 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 8233
2022-03-06 14:58:44 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 14:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:00:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:01:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:02:35 | INFO | train_inner | epoch 014:     73 / 196 loss=6.646, nll_loss=6.473, ppl=88.81, wps=19519, ups=0.3, wpb=65367, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.88, loss_scale=16, train_wall=299, gb_free=19.9, wall=8464
2022-03-06 15:07:49 | INFO | train_inner | epoch 014:    173 / 196 loss=6.582, nll_loss=6.407, ppl=84.87, wps=20815.8, ups=0.32, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.875, loss_scale=16, train_wall=293, gb_free=19.9, wall=8778
2022-03-06 15:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:09:07 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.109 | nll_loss 6.936 | ppl 122.41 | wps 39841.5 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 7.109
2022-03-06 15:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2723 updates
2022-03-06 15:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 14 @ 2723 updates, score 7.109) (writing took 7.749031224055216 seconds)
2022-03-06 15:09:15 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 15:09:15 | INFO | train | epoch 014 | loss 6.592 | nll_loss 6.417 | ppl 85.46 | wps 20124.4 | ups 0.31 | wpb 65448.9 | bsz 127.8 | num_updates 2723 | lr 0.000340407 | gnorm 0.876 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 8864
2022-03-06 15:09:15 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 15:09:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:13:18 | INFO | train_inner | epoch 015:     77 / 196 loss=6.465, nll_loss=6.288, ppl=78.13, wps=19879.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.888, loss_scale=32, train_wall=293, gb_free=19.9, wall=9107
2022-03-06 15:15:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:18:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:18:41 | INFO | train_inner | epoch 015:    179 / 196 loss=6.414, nll_loss=6.235, ppl=75.33, wps=20309.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.859, loss_scale=16, train_wall=300, gb_free=19.9, wall=9430
2022-03-06 15:19:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:19:40 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.024 | nll_loss 6.848 | ppl 115.19 | wps 39877.2 | wpb 510.9 | bsz 1 | num_updates 2917 | best_loss 7.024
2022-03-06 15:19:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2917 updates
2022-03-06 15:19:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:19:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 15 @ 2917 updates, score 7.024) (writing took 7.683566058054566 seconds)
2022-03-06 15:19:47 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 15:19:47 | INFO | train | epoch 015 | loss 6.42 | nll_loss 6.241 | ppl 75.66 | wps 20076.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2917 | lr 0.000364652 | gnorm 0.876 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 9496
2022-03-06 15:19:47 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 15:19:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:24:10 | INFO | train_inner | epoch 016:     83 / 196 loss=6.277, nll_loss=6.096, ppl=68.4, wps=19880.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.836, loss_scale=16, train_wall=293, gb_free=19.9, wall=9759
2022-03-06 15:29:26 | INFO | train_inner | epoch 016:    183 / 196 loss=6.266, nll_loss=6.085, ppl=67.87, wps=20711.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.856, loss_scale=32, train_wall=294, gb_free=19.9, wall=10075
2022-03-06 15:30:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:30:12 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.987 | nll_loss 6.81 | ppl 112.24 | wps 40395.6 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 6.987
2022-03-06 15:30:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3113 updates
2022-03-06 15:30:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:30:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:30:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 16 @ 3113 updates, score 6.987) (writing took 7.369123658863828 seconds)
2022-03-06 15:30:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 15:30:19 | INFO | train | epoch 016 | loss 6.257 | nll_loss 6.076 | ppl 67.45 | wps 20294.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3113 | lr 0.000389147 | gnorm 0.841 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 10128
2022-03-06 15:30:19 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 15:30:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:32:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:34:58 | INFO | train_inner | epoch 017:     88 / 196 loss=6.114, nll_loss=5.93, ppl=60.96, wps=19712.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.865, loss_scale=32, train_wall=296, gb_free=19.9, wall=10407
2022-03-06 15:37:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:40:17 | INFO | train_inner | epoch 017:    189 / 196 loss=6.115, nll_loss=5.93, ppl=60.97, wps=20526.4, ups=0.31, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.857, loss_scale=16, train_wall=297, gb_free=19.9, wall=10726
2022-03-06 15:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:40:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.964 | nll_loss 6.785 | ppl 110.32 | wps 39463.7 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 6.964
2022-03-06 15:40:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3307 updates
2022-03-06 15:40:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:40:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 17 @ 3307 updates, score 6.964) (writing took 7.634362603072077 seconds)
2022-03-06 15:40:52 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 15:40:52 | INFO | train | epoch 017 | loss 6.107 | nll_loss 5.922 | ppl 60.63 | wps 20080.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3307 | lr 0.000413392 | gnorm 0.861 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 10761
2022-03-06 15:40:52 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 15:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:45:46 | INFO | train_inner | epoch 018:     93 / 196 loss=5.958, nll_loss=5.771, ppl=54.61, wps=19875.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.836, loss_scale=32, train_wall=293, gb_free=19.9, wall=11055
2022-03-06 15:51:01 | INFO | train_inner | epoch 018:    193 / 196 loss=5.982, nll_loss=5.794, ppl=55.49, wps=20795.3, ups=0.32, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.863, loss_scale=32, train_wall=293, gb_free=19.9, wall=11370
2022-03-06 15:51:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:51:15 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.936 | nll_loss 6.757 | ppl 108.19 | wps 41252.3 | wpb 510.9 | bsz 1 | num_updates 3503 | best_loss 6.936
2022-03-06 15:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3503 updates
2022-03-06 15:51:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:51:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt
2022-03-06 15:51:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_best.pt (epoch 18 @ 3503 updates, score 6.936) (writing took 6.938531109830365 seconds)
2022-03-06 15:51:22 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 15:51:22 | INFO | train | epoch 018 | loss 5.964 | nll_loss 5.777 | ppl 54.82 | wps 20353.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3503 | lr 0.000437887 | gnorm 0.852 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 11391
2022-03-06 15:51:22 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 15:51:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:51:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:56:32 | INFO | train_inner | epoch 019:     98 / 196 loss=5.811, nll_loss=5.621, ppl=49.2, wps=19770.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.85, loss_scale=32, train_wall=296, gb_free=19.9, wall=11701
2022-03-06 15:58:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:59:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:01:46 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.966 | nll_loss 6.79 | ppl 110.63 | wps 41197 | wpb 510.9 | bsz 1 | num_updates 3696 | best_loss 6.936
2022-03-06 16:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3696 updates
2022-03-06 16:01:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:01:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:01:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 19 @ 3696 updates, score 6.966) (writing took 2.983887922950089 seconds)
2022-03-06 16:01:49 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 16:01:49 | INFO | train | epoch 019 | loss 5.826 | nll_loss 5.635 | ppl 49.7 | wps 20131.6 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 3696 | lr 0.000462008 | gnorm 0.843 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 12018
2022-03-06 16:01:49 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 16:01:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:02:02 | INFO | train_inner | epoch 020:      4 / 196 loss=5.838, nll_loss=5.647, ppl=50.11, wps=19787.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.84, loss_scale=16, train_wall=299, gb_free=19.9, wall=12031
2022-03-06 16:06:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:07:21 | INFO | train_inner | epoch 020:    105 / 196 loss=5.673, nll_loss=5.479, ppl=44.61, wps=20512.3, ups=0.31, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.849, loss_scale=16, train_wall=297, gb_free=19.9, wall=12350
2022-03-06 16:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:12:14 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.959 | nll_loss 6.78 | ppl 109.9 | wps 41123.7 | wpb 510.9 | bsz 1 | num_updates 3891 | best_loss 6.936
2022-03-06 16:12:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3891 updates
2022-03-06 16:12:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:12:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:12:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 20 @ 3891 updates, score 6.959) (writing took 2.994992998894304 seconds)
2022-03-06 16:12:17 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 16:12:17 | INFO | train | epoch 020 | loss 5.7 | nll_loss 5.507 | ppl 45.46 | wps 20331.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3891 | lr 0.000486378 | gnorm 0.842 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 12646
2022-03-06 16:12:17 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 16:12:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:12:45 | INFO | train_inner | epoch 021:      9 / 196 loss=5.714, nll_loss=5.521, ppl=45.9, wps=20180.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.838, loss_scale=16, train_wall=293, gb_free=19.9, wall=12674
2022-03-06 16:14:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:18:05 | INFO | train_inner | epoch 021:    110 / 196 loss=5.547, nll_loss=5.35, ppl=40.79, wps=20529.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.841, loss_scale=16, train_wall=296, gb_free=19.9, wall=12993
2022-03-06 16:21:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:22:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:22:41 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.96 | nll_loss 6.778 | ppl 109.74 | wps 41172.1 | wpb 510.9 | bsz 1 | num_updates 4085 | best_loss 6.936
2022-03-06 16:22:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4085 updates
2022-03-06 16:22:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:22:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:22:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 21 @ 4085 updates, score 6.96) (writing took 3.223935507936403 seconds)
2022-03-06 16:22:44 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 16:22:44 | INFO | train | epoch 021 | loss 5.575 | nll_loss 5.379 | ppl 41.62 | wps 20235.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4085 | lr 0.000494771 | gnorm 0.849 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 13273
2022-03-06 16:22:44 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 16:22:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:23:32 | INFO | train_inner | epoch 022:     15 / 196 loss=5.574, nll_loss=5.377, ppl=41.56, wps=19983.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.834, loss_scale=16, train_wall=296, gb_free=19.9, wall=13321
2022-03-06 16:28:48 | INFO | train_inner | epoch 022:    115 / 196 loss=5.43, nll_loss=5.231, ppl=37.56, wps=20729.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.829, loss_scale=32, train_wall=294, gb_free=19.9, wall=13637
2022-03-06 16:33:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:33:09 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.995 | nll_loss 6.809 | ppl 112.09 | wps 39641.6 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 6.936
2022-03-06 16:33:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-06 16:33:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:33:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 22 @ 4281 updates, score 6.995) (writing took 3.896571231074631 seconds)
2022-03-06 16:33:13 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 16:33:13 | INFO | train | epoch 022 | loss 5.445 | nll_loss 5.246 | ppl 37.95 | wps 20407.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.817 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 13902
2022-03-06 16:33:13 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 16:33:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:34:13 | INFO | train_inner | epoch 023:     19 / 196 loss=5.44, nll_loss=5.241, ppl=37.81, wps=20109.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.813, loss_scale=32, train_wall=293, gb_free=19.9, wall=13962
2022-03-06 16:35:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:39:31 | INFO | train_inner | epoch 023:    120 / 196 loss=5.298, nll_loss=5.096, ppl=34.2, wps=20615, ups=0.31, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.791, loss_scale=32, train_wall=295, gb_free=19.9, wall=14280
2022-03-06 16:40:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:43:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:43:35 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.016 | nll_loss 6.828 | ppl 113.65 | wps 41026.3 | wpb 510.9 | bsz 1 | num_updates 4475 | best_loss 6.936
2022-03-06 16:43:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4475 updates
2022-03-06 16:43:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:43:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:43:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 23 @ 4475 updates, score 7.016) (writing took 3.037483111023903 seconds)
2022-03-06 16:43:38 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 16:43:38 | INFO | train | epoch 023 | loss 5.318 | nll_loss 5.116 | ppl 34.69 | wps 20301.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4475 | lr 0.000472719 | gnorm 0.803 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 14527
2022-03-06 16:43:38 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 16:43:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:44:58 | INFO | train_inner | epoch 024:     25 / 196 loss=5.297, nll_loss=5.095, ppl=34.17, wps=19983, ups=0.31, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.799, loss_scale=16, train_wall=296, gb_free=19.9, wall=14607
2022-03-06 16:50:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:50:17 | INFO | train_inner | epoch 024:    126 / 196 loss=5.187, nll_loss=4.982, ppl=31.6, wps=20514.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.811, loss_scale=16, train_wall=297, gb_free=19.9, wall=14926
2022-03-06 16:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:54:03 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.003 | nll_loss 6.813 | ppl 112.44 | wps 41139 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 6.936
2022-03-06 16:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-06 16:54:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 16:54:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 24 @ 4670 updates, score 7.003) (writing took 2.9383485009893775 seconds)
2022-03-06 16:54:06 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 16:54:06 | INFO | train | epoch 024 | loss 5.201 | nll_loss 4.996 | ppl 31.92 | wps 20329.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.79 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 15155
2022-03-06 16:54:06 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 16:54:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:55:41 | INFO | train_inner | epoch 025:     30 / 196 loss=5.184, nll_loss=4.979, ppl=31.53, wps=20176.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.777, loss_scale=16, train_wall=293, gb_free=19.9, wall=15250
2022-03-06 16:57:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:01:01 | INFO | train_inner | epoch 025:    131 / 196 loss=5.078, nll_loss=4.87, ppl=29.25, wps=20520.5, ups=0.31, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.779, loss_scale=16, train_wall=297, gb_free=19.9, wall=15570
2022-03-06 17:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:04:31 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.059 | nll_loss 6.868 | ppl 116.78 | wps 41270 | wpb 510.9 | bsz 1 | num_updates 4865 | best_loss 6.936
2022-03-06 17:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4865 updates
2022-03-06 17:04:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:04:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:04:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 25 @ 4865 updates, score 7.059) (writing took 3.081905075116083 seconds)
2022-03-06 17:04:34 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:04:34 | INFO | train | epoch 025 | loss 5.093 | nll_loss 4.886 | ppl 29.56 | wps 20331.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4865 | lr 0.000453376 | gnorm 0.787 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 15783
2022-03-06 17:04:34 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:04:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:06:24 | INFO | train_inner | epoch 026:     35 / 196 loss=5.069, nll_loss=4.861, ppl=29.06, wps=20182.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.793, loss_scale=32, train_wall=293, gb_free=19.9, wall=15893
2022-03-06 17:11:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:11:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:11:47 | INFO | train_inner | epoch 026:    137 / 196 loss=4.982, nll_loss=4.771, ppl=27.31, wps=20313.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.788, loss_scale=16, train_wall=300, gb_free=19.9, wall=16216
2022-03-06 17:14:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:14:58 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.097 | nll_loss 6.904 | ppl 119.79 | wps 41132.2 | wpb 510.9 | bsz 1 | num_updates 5059 | best_loss 6.936
2022-03-06 17:14:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5059 updates
2022-03-06 17:14:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:15:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:15:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 26 @ 5059 updates, score 7.097) (writing took 3.191713875858113 seconds)
2022-03-06 17:15:02 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:15:02 | INFO | train | epoch 026 | loss 4.988 | nll_loss 4.778 | ppl 27.43 | wps 20225.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5059 | lr 0.000444598 | gnorm 0.786 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 16411
2022-03-06 17:15:02 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:15:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:17:11 | INFO | train_inner | epoch 027:     41 / 196 loss=4.947, nll_loss=4.736, ppl=26.64, wps=20170.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.778, loss_scale=16, train_wall=293, gb_free=19.9, wall=16540
2022-03-06 17:20:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:22:30 | INFO | train_inner | epoch 027:    142 / 196 loss=4.892, nll_loss=4.679, ppl=25.62, wps=20563.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.796, loss_scale=16, train_wall=296, gb_free=19.9, wall=16859
2022-03-06 17:25:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:25:24 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.133 | nll_loss 6.94 | ppl 122.83 | wps 40329.7 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 6.936
2022-03-06 17:25:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5254 updates
2022-03-06 17:25:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:25:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:25:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 27 @ 5254 updates, score 7.133) (writing took 3.5152722909115255 seconds)
2022-03-06 17:25:27 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:25:27 | INFO | train | epoch 027 | loss 4.891 | nll_loss 4.678 | ppl 25.6 | wps 20397.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5254 | lr 0.00043627 | gnorm 0.781 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 17036
2022-03-06 17:25:27 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:25:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:27:53 | INFO | train_inner | epoch 028:     46 / 196 loss=4.844, nll_loss=4.631, ppl=24.77, wps=20205, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.772, loss_scale=32, train_wall=292, gb_free=19.9, wall=17182
2022-03-06 17:32:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:33:13 | INFO | train_inner | epoch 028:    147 / 196 loss=4.795, nll_loss=4.58, ppl=23.92, wps=20519.2, ups=0.31, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.799, loss_scale=16, train_wall=297, gb_free=19.9, wall=17502
2022-03-06 17:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:35:52 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.18 | nll_loss 6.992 | ppl 127.26 | wps 41154.5 | wpb 510.9 | bsz 1 | num_updates 5449 | best_loss 6.936
2022-03-06 17:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5449 updates
2022-03-06 17:35:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 28 @ 5449 updates, score 7.18) (writing took 3.435746884904802 seconds)
2022-03-06 17:35:55 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:35:55 | INFO | train | epoch 028 | loss 4.798 | nll_loss 4.583 | ppl 23.97 | wps 20327.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5449 | lr 0.000428392 | gnorm 0.789 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 17664
2022-03-06 17:35:55 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:38:37 | INFO | train_inner | epoch 029:     51 / 196 loss=4.755, nll_loss=4.539, ppl=23.25, wps=20157.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.774, loss_scale=16, train_wall=293, gb_free=19.9, wall=17826
2022-03-06 17:43:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:43:57 | INFO | train_inner | epoch 029:    152 / 196 loss=4.714, nll_loss=4.497, ppl=22.58, wps=20509, ups=0.31, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.793, loss_scale=16, train_wall=297, gb_free=19.9, wall=18145
2022-03-06 17:46:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:46:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.214 | nll_loss 7.02 | ppl 129.77 | wps 40992.6 | wpb 510.9 | bsz 1 | num_updates 5644 | best_loss 6.936
2022-03-06 17:46:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5644 updates
2022-03-06 17:46:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:46:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:46:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 29 @ 5644 updates, score 7.214) (writing took 3.0477166660130024 seconds)
2022-03-06 17:46:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:46:23 | INFO | train | epoch 029 | loss 4.711 | nll_loss 4.493 | ppl 22.53 | wps 20335.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5644 | lr 0.000420927 | gnorm 0.788 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 18292
2022-03-06 17:46:23 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:46:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:49:21 | INFO | train_inner | epoch 030:     56 / 196 loss=4.66, nll_loss=4.441, ppl=21.72, wps=20173, ups=0.31, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.8, loss_scale=16, train_wall=293, gb_free=19.9, wall=18470
2022-03-06 17:54:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:54:40 | INFO | train_inner | epoch 030:    157 / 196 loss=4.636, nll_loss=4.417, ppl=21.36, wps=20497.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.797, loss_scale=16, train_wall=297, gb_free=19.9, wall=18789
2022-03-06 17:56:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:56:48 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.273 | nll_loss 7.081 | ppl 135.4 | wps 39525.5 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 6.936
2022-03-06 17:56:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-06 17:56:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:56:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 17:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.273) (writing took 3.7229022551327944 seconds)
2022-03-06 17:56:52 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 17:56:52 | INFO | train | epoch 030 | loss 4.627 | nll_loss 4.408 | ppl 21.23 | wps 20293.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.798 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 18921
2022-03-06 17:56:52 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 17:56:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:00:06 | INFO | train_inner | epoch 031:     61 / 196 loss=4.561, nll_loss=4.341, ppl=20.26, wps=20097.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.81, loss_scale=16, train_wall=293, gb_free=19.9, wall=19114
2022-03-06 18:05:22 | INFO | train_inner | epoch 031:    161 / 196 loss=4.568, nll_loss=4.347, ppl=20.35, wps=20718.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.815, loss_scale=32, train_wall=294, gb_free=19.9, wall=19431
2022-03-06 18:07:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:07:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.305 | nll_loss 7.113 | ppl 138.42 | wps 41054.4 | wpb 510.9 | bsz 1 | num_updates 6035 | best_loss 6.936
2022-03-06 18:07:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6035 updates
2022-03-06 18:07:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:07:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 31 @ 6035 updates, score 7.305) (writing took 3.398749043000862 seconds)
2022-03-06 18:07:20 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 18:07:20 | INFO | train | epoch 031 | loss 4.548 | nll_loss 4.326 | ppl 20.06 | wps 20417.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6035 | lr 0.000407063 | gnorm 0.806 | loss_scale 32 | train_wall 575 | gb_free 19.9 | wall 19549
2022-03-06 18:07:20 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 18:07:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:08:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 18:10:47 | INFO | train_inner | epoch 032:     66 / 196 loss=4.475, nll_loss=4.252, ppl=19.05, wps=20083.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.777, loss_scale=32, train_wall=294, gb_free=19.9, wall=19756
2022-03-06 18:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 18:16:07 | INFO | train_inner | epoch 032:    167 / 196 loss=4.495, nll_loss=4.271, ppl=19.31, wps=20518.7, ups=0.31, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.815, loss_scale=32, train_wall=297, gb_free=19.9, wall=20076
2022-03-06 18:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:17:43 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.358 | nll_loss 7.166 | ppl 143.6 | wps 39333.7 | wpb 510.9 | bsz 1 | num_updates 6229 | best_loss 6.936
2022-03-06 18:17:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6229 updates
2022-03-06 18:17:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 32 @ 6229 updates, score 7.358) (writing took 3.736423233989626 seconds)
2022-03-06 18:17:47 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 18:17:47 | INFO | train | epoch 032 | loss 4.471 | nll_loss 4.248 | ppl 19 | wps 20244.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6229 | lr 0.000400674 | gnorm 0.798 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 20176
2022-03-06 18:17:47 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 18:17:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:18:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:21:35 | INFO | train_inner | epoch 033:     72 / 196 loss=4.401, nll_loss=4.176, ppl=18.07, wps=19925.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.798, loss_scale=16, train_wall=296, gb_free=19.9, wall=20404
2022-03-06 18:25:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:26:54 | INFO | train_inner | epoch 033:    173 / 196 loss=4.427, nll_loss=4.201, ppl=18.4, wps=20522.9, ups=0.31, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.83, loss_scale=16, train_wall=297, gb_free=19.9, wall=20723
2022-03-06 18:28:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:28:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.417 | nll_loss 7.221 | ppl 149.17 | wps 39257.7 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 6.936
2022-03-06 18:28:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6423 updates
2022-03-06 18:28:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:28:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:28:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 33 @ 6423 updates, score 7.417) (writing took 3.879877721890807 seconds)
2022-03-06 18:28:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 18:28:16 | INFO | train | epoch 033 | loss 4.4 | nll_loss 4.174 | ppl 18.06 | wps 20199.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6423 | lr 0.000394576 | gnorm 0.824 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 20805
2022-03-06 18:28:16 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 18:28:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:32:19 | INFO | train_inner | epoch 034:     77 / 196 loss=4.315, nll_loss=4.087, ppl=17, wps=20103.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.795, loss_scale=16, train_wall=293, gb_free=19.9, wall=21048
2022-03-06 18:34:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:37:39 | INFO | train_inner | epoch 034:    178 / 196 loss=4.363, nll_loss=4.136, ppl=17.58, wps=20522.2, ups=0.31, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.809, loss_scale=16, train_wall=297, gb_free=19.9, wall=21367
2022-03-06 18:38:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:38:40 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.467 | nll_loss 7.273 | ppl 154.7 | wps 39495.6 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 6.936
2022-03-06 18:38:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-06 18:38:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:38:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:38:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 34 @ 6618 updates, score 7.467) (writing took 3.909694276051596 seconds)
2022-03-06 18:38:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 18:38:44 | INFO | train | epoch 034 | loss 4.332 | nll_loss 4.104 | ppl 17.2 | wps 20301 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.805 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 21433
2022-03-06 18:38:44 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 18:38:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:43:04 | INFO | train_inner | epoch 035:     82 / 196 loss=4.245, nll_loss=4.016, ppl=16.17, wps=20104.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.801, loss_scale=32, train_wall=293, gb_free=19.9, wall=21693
2022-03-06 18:45:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:48:23 | INFO | train_inner | epoch 035:    183 / 196 loss=4.309, nll_loss=4.08, ppl=16.92, wps=20534.6, ups=0.31, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.834, loss_scale=16, train_wall=296, gb_free=19.9, wall=22012
2022-03-06 18:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:49:09 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.508 | nll_loss 7.314 | ppl 159.09 | wps 39471.5 | wpb 510.9 | bsz 1 | num_updates 6813 | best_loss 6.936
2022-03-06 18:49:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6813 updates
2022-03-06 18:49:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:49:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:49:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 35 @ 6813 updates, score 7.508) (writing took 3.558568465989083 seconds)
2022-03-06 18:49:12 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 18:49:12 | INFO | train | epoch 035 | loss 4.267 | nll_loss 4.038 | ppl 16.43 | wps 20321.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6813 | lr 0.000383116 | gnorm 0.811 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 22061
2022-03-06 18:49:12 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 18:49:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:53:47 | INFO | train_inner | epoch 036:     87 / 196 loss=4.172, nll_loss=3.941, ppl=15.36, wps=20155.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.812, loss_scale=32, train_wall=293, gb_free=19.9, wall=22336
2022-03-06 18:55:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:59:05 | INFO | train_inner | epoch 036:    188 / 196 loss=4.249, nll_loss=4.019, ppl=16.21, wps=20609.9, ups=0.31, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.843, loss_scale=16, train_wall=295, gb_free=19.9, wall=22654
2022-03-06 18:59:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:59:35 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.536 | nll_loss 7.338 | ppl 161.76 | wps 39787.3 | wpb 510.9 | bsz 1 | num_updates 7008 | best_loss 6.936
2022-03-06 18:59:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7008 updates
2022-03-06 18:59:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:59:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 18:59:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 36 @ 7008 updates, score 7.536) (writing took 4.030375717906281 seconds)
2022-03-06 18:59:39 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 18:59:39 | INFO | train | epoch 036 | loss 4.204 | nll_loss 3.973 | ppl 15.7 | wps 20357.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7008 | lr 0.000377749 | gnorm 0.83 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 22688
2022-03-06 18:59:39 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 18:59:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:04:30 | INFO | train_inner | epoch 037:     92 / 196 loss=4.11, nll_loss=3.877, ppl=14.69, wps=20091.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.843, loss_scale=32, train_wall=293, gb_free=19.9, wall=22979
2022-03-06 19:05:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:09:49 | INFO | train_inner | epoch 037:    193 / 196 loss=4.187, nll_loss=3.955, ppl=15.51, wps=20548.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.825, loss_scale=16, train_wall=296, gb_free=19.9, wall=23298
2022-03-06 19:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:10:04 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.607 | nll_loss 7.411 | ppl 170.18 | wps 39498.1 | wpb 510.9 | bsz 1 | num_updates 7203 | best_loss 6.936
2022-03-06 19:10:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7203 updates
2022-03-06 19:10:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 37 @ 7203 updates, score 7.607) (writing took 3.6054765831213444 seconds)
2022-03-06 19:10:07 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 19:10:07 | INFO | train | epoch 037 | loss 4.145 | nll_loss 3.911 | ppl 15.05 | wps 20322.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7203 | lr 0.0003726 | gnorm 0.831 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 23316
2022-03-06 19:10:07 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 19:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:13:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:15:17 | INFO | train_inner | epoch 038:     98 / 196 loss=4.045, nll_loss=3.81, ppl=14.02, wps=19953, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.857, loss_scale=16, train_wall=296, gb_free=19.9, wall=23626
2022-03-06 19:20:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:20:31 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.674 | nll_loss 7.477 | ppl 178.2 | wps 39546.7 | wpb 510.9 | bsz 1 | num_updates 7398 | best_loss 6.936
2022-03-06 19:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7398 updates
2022-03-06 19:20:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:20:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:20:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 38 @ 7398 updates, score 7.674) (writing took 3.703960685990751 seconds)
2022-03-06 19:20:35 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 19:20:35 | INFO | train | epoch 038 | loss 4.088 | nll_loss 3.853 | ppl 14.45 | wps 20325.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7398 | lr 0.000367657 | gnorm 0.856 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 23944
2022-03-06 19:20:35 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 19:20:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:20:42 | INFO | train_inner | epoch 039:      2 / 196 loss=4.13, nll_loss=3.896, ppl=14.89, wps=20133.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.853, loss_scale=32, train_wall=293, gb_free=19.9, wall=23951
2022-03-06 19:21:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:26:01 | INFO | train_inner | epoch 039:    103 / 196 loss=3.982, nll_loss=3.745, ppl=13.41, wps=20529.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.834, loss_scale=16, train_wall=296, gb_free=19.9, wall=24270
2022-03-06 19:28:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:30:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:30:59 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.735 | nll_loss 7.538 | ppl 185.87 | wps 39527.9 | wpb 510.9 | bsz 1 | num_updates 7592 | best_loss 6.936
2022-03-06 19:31:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7592 updates
2022-03-06 19:31:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:31:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:31:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 39 @ 7592 updates, score 7.735) (writing took 4.284258871804923 seconds)
2022-03-06 19:31:04 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 19:31:04 | INFO | train | epoch 039 | loss 4.033 | nll_loss 3.796 | ppl 13.89 | wps 20197.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7592 | lr 0.000362929 | gnorm 0.839 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 24573
2022-03-06 19:31:04 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 19:31:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:31:29 | INFO | train_inner | epoch 040:      8 / 196 loss=4.071, nll_loss=3.835, ppl=14.27, wps=19904.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.843, loss_scale=16, train_wall=296, gb_free=19.9, wall=24598
2022-03-06 19:36:45 | INFO | train_inner | epoch 040:    108 / 196 loss=3.943, nll_loss=3.704, ppl=13.04, wps=20733.2, ups=0.32, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.834, loss_scale=32, train_wall=293, gb_free=19.9, wall=24914
2022-03-06 19:37:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:41:26 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.759 | nll_loss 7.562 | ppl 188.99 | wps 41130 | wpb 510.9 | bsz 1 | num_updates 7787 | best_loss 6.936
2022-03-06 19:41:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7787 updates
2022-03-06 19:41:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:41:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:41:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 40 @ 7787 updates, score 7.759) (writing took 3.0570356359239668 seconds)
2022-03-06 19:41:30 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 19:41:30 | INFO | train | epoch 040 | loss 3.98 | nll_loss 3.742 | ppl 13.38 | wps 20395.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7787 | lr 0.000358356 | gnorm 0.841 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 25198
2022-03-06 19:41:30 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 19:41:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:42:11 | INFO | train_inner | epoch 041:     13 / 196 loss=4.01, nll_loss=3.772, ppl=13.66, wps=20086.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.852, loss_scale=16, train_wall=294, gb_free=19.9, wall=25240
2022-03-06 19:44:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:47:30 | INFO | train_inner | epoch 041:    114 / 196 loss=3.893, nll_loss=3.653, ppl=12.58, wps=20530.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.832, loss_scale=16, train_wall=296, gb_free=19.9, wall=25559
2022-03-06 19:51:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:51:54 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.817 | nll_loss 7.62 | ppl 196.66 | wps 39453.7 | wpb 510.9 | bsz 1 | num_updates 7982 | best_loss 6.936
2022-03-06 19:51:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7982 updates
2022-03-06 19:51:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:51:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 19:51:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 41 @ 7982 updates, score 7.817) (writing took 3.563169299857691 seconds)
2022-03-06 19:51:57 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 19:51:57 | INFO | train | epoch 041 | loss 3.93 | nll_loss 3.691 | ppl 12.91 | wps 20332.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7982 | lr 0.000353952 | gnorm 0.847 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 25826
2022-03-06 19:51:57 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 19:51:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:52:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:52:57 | INFO | train_inner | epoch 042:     19 / 196 loss=3.956, nll_loss=3.716, ppl=13.14, wps=19961.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.871, loss_scale=16, train_wall=296, gb_free=19.9, wall=25886
2022-03-06 19:58:14 | INFO | train_inner | epoch 042:    119 / 196 loss=3.852, nll_loss=3.61, ppl=12.21, wps=20716.2, ups=0.32, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.853, loss_scale=16, train_wall=294, gb_free=19.9, wall=26203
2022-03-06 20:00:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:02:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:02:21 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.852 | nll_loss 7.652 | ppl 201.07 | wps 39549.7 | wpb 510.9 | bsz 1 | num_updates 8176 | best_loss 6.936
2022-03-06 20:02:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8176 updates
2022-03-06 20:02:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:02:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:02:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 42 @ 8176 updates, score 7.852) (writing took 3.8354375390335917 seconds)
2022-03-06 20:02:25 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 20:02:25 | INFO | train | epoch 042 | loss 3.881 | nll_loss 3.64 | ppl 12.47 | wps 20214.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8176 | lr 0.000349727 | gnorm 0.86 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 26454
2022-03-06 20:02:25 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 20:02:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:03:41 | INFO | train_inner | epoch 043:     24 / 196 loss=3.893, nll_loss=3.652, ppl=12.57, wps=19956.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.864, loss_scale=16, train_wall=295, gb_free=19.9, wall=26530
2022-03-06 20:08:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:09:01 | INFO | train_inner | epoch 043:    125 / 196 loss=3.816, nll_loss=3.572, ppl=11.9, wps=20527.5, ups=0.31, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.864, loss_scale=16, train_wall=297, gb_free=19.9, wall=26849
2022-03-06 20:12:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:12:49 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.93 | nll_loss 7.73 | ppl 212.29 | wps 39473.7 | wpb 510.9 | bsz 1 | num_updates 8371 | best_loss 6.936
2022-03-06 20:12:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8371 updates
2022-03-06 20:12:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:12:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:12:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 43 @ 8371 updates, score 7.93) (writing took 3.72172489692457 seconds)
2022-03-06 20:12:53 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 20:12:53 | INFO | train | epoch 043 | loss 3.835 | nll_loss 3.593 | ppl 12.07 | wps 20325.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8371 | lr 0.00034563 | gnorm 0.87 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 27082
2022-03-06 20:12:53 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 20:12:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:14:25 | INFO | train_inner | epoch 044:     29 / 196 loss=3.835, nll_loss=3.592, ppl=12.06, wps=20142.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.876, loss_scale=16, train_wall=293, gb_free=19.9, wall=27174
2022-03-06 20:15:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:19:44 | INFO | train_inner | epoch 044:    130 / 196 loss=3.775, nll_loss=3.53, ppl=11.55, wps=20531.9, ups=0.31, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.869, loss_scale=16, train_wall=296, gb_free=19.9, wall=27493
2022-03-06 20:23:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:23:17 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.966 | nll_loss 7.764 | ppl 217.44 | wps 39661 | wpb 510.9 | bsz 1 | num_updates 8566 | best_loss 6.936
2022-03-06 20:23:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8566 updates
2022-03-06 20:23:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:23:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 44 @ 8566 updates, score 7.966) (writing took 3.796501015080139 seconds)
2022-03-06 20:23:21 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 20:23:21 | INFO | train | epoch 044 | loss 3.79 | nll_loss 3.546 | ppl 11.68 | wps 20325.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8566 | lr 0.000341673 | gnorm 0.877 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 27710
2022-03-06 20:23:21 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 20:23:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:23:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:25:11 | INFO | train_inner | epoch 045:     35 / 196 loss=3.789, nll_loss=3.545, ppl=11.67, wps=19972.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.881, loss_scale=16, train_wall=295, gb_free=19.9, wall=27820
2022-03-06 20:30:26 | INFO | train_inner | epoch 045:    135 / 196 loss=3.735, nll_loss=3.489, ppl=11.23, wps=20852.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.874, loss_scale=32, train_wall=292, gb_free=19.9, wall=28135
2022-03-06 20:31:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:33:44 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.018 | nll_loss 7.817 | ppl 225.57 | wps 39490.3 | wpb 510.9 | bsz 1 | num_updates 8760 | best_loss 6.936
2022-03-06 20:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8760 updates
2022-03-06 20:33:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:33:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:33:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 45 @ 8760 updates, score 8.018) (writing took 3.9182513419073075 seconds)
2022-03-06 20:33:48 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 20:33:48 | INFO | train | epoch 045 | loss 3.745 | nll_loss 3.499 | ppl 11.31 | wps 20264.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8760 | lr 0.000337869 | gnorm 0.878 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 28337
2022-03-06 20:33:48 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 20:33:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:35:53 | INFO | train_inner | epoch 046:     40 / 196 loss=3.738, nll_loss=3.492, ppl=11.25, wps=19945.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.883, loss_scale=16, train_wall=295, gb_free=19.9, wall=28462
2022-03-06 20:39:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:41:13 | INFO | train_inner | epoch 046:    141 / 196 loss=3.697, nll_loss=3.45, ppl=10.93, wps=20539.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.882, loss_scale=16, train_wall=296, gb_free=19.9, wall=28781
2022-03-06 20:44:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:44:11 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.062 | nll_loss 7.862 | ppl 232.61 | wps 39697 | wpb 510.9 | bsz 1 | num_updates 8955 | best_loss 6.936
2022-03-06 20:44:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8955 updates
2022-03-06 20:44:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:44:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:44:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 46 @ 8955 updates, score 8.062) (writing took 3.5960474649909884 seconds)
2022-03-06 20:44:15 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 20:44:15 | INFO | train | epoch 046 | loss 3.705 | nll_loss 3.458 | ppl 10.99 | wps 20340.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8955 | lr 0.00033417 | gnorm 0.878 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 28964
2022-03-06 20:44:15 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 20:44:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:46:37 | INFO | train_inner | epoch 047:     45 / 196 loss=3.684, nll_loss=3.437, ppl=10.83, wps=20155.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.866, loss_scale=32, train_wall=293, gb_free=19.9, wall=29106
2022-03-06 20:49:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:51:56 | INFO | train_inner | epoch 047:    146 / 196 loss=3.664, nll_loss=3.416, ppl=10.68, wps=20512.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.888, loss_scale=16, train_wall=297, gb_free=19.9, wall=29425
2022-03-06 20:54:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:54:39 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.11 | nll_loss 7.909 | ppl 240.28 | wps 39801.5 | wpb 510.9 | bsz 1 | num_updates 9150 | best_loss 6.936
2022-03-06 20:54:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9150 updates
2022-03-06 20:54:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:54:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 20:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 47 @ 9150 updates, score 8.11) (writing took 3.7238521759863943 seconds)
2022-03-06 20:54:43 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 20:54:43 | INFO | train | epoch 047 | loss 3.664 | nll_loss 3.416 | ppl 10.68 | wps 20320.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9150 | lr 0.00033059 | gnorm 0.891 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 29592
2022-03-06 20:54:43 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 20:54:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:56:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:57:24 | INFO | train_inner | epoch 048:     51 / 196 loss=3.637, nll_loss=3.389, ppl=10.47, wps=19957.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.896, loss_scale=16, train_wall=296, gb_free=19.9, wall=29753
2022-03-06 21:02:40 | INFO | train_inner | epoch 048:    151 / 196 loss=3.64, nll_loss=3.391, ppl=10.49, wps=20734.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.895, loss_scale=16, train_wall=294, gb_free=19.9, wall=30069
2022-03-06 21:04:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:05:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:05:07 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.185 | nll_loss 7.981 | ppl 252.71 | wps 39445.7 | wpb 510.9 | bsz 1 | num_updates 9344 | best_loss 6.936
2022-03-06 21:05:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9344 updates
2022-03-06 21:05:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:05:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 48 @ 9344 updates, score 8.185) (writing took 3.616244793869555 seconds)
2022-03-06 21:05:11 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 21:05:11 | INFO | train | epoch 048 | loss 3.625 | nll_loss 3.376 | ppl 10.38 | wps 20218.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.889 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 30220
2022-03-06 21:05:11 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 21:05:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:08:08 | INFO | train_inner | epoch 049:     56 / 196 loss=3.59, nll_loss=3.34, ppl=10.12, wps=19953.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.884, loss_scale=16, train_wall=296, gb_free=19.9, wall=30396
2022-03-06 21:11:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:13:26 | INFO | train_inner | epoch 049:    157 / 196 loss=3.604, nll_loss=3.353, ppl=10.22, wps=20599.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.886, loss_scale=16, train_wall=296, gb_free=19.9, wall=30715
2022-03-06 21:15:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:15:33 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.25 | nll_loss 8.048 | ppl 264.59 | wps 41223 | wpb 510.9 | bsz 1 | num_updates 9539 | best_loss 6.936
2022-03-06 21:15:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9539 updates
2022-03-06 21:15:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 49 @ 9539 updates, score 8.25) (writing took 2.975162152899429 seconds)
2022-03-06 21:15:36 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 21:15:36 | INFO | train | epoch 049 | loss 3.588 | nll_loss 3.338 | ppl 10.11 | wps 20418.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9539 | lr 0.000323779 | gnorm 0.885 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 30845
2022-03-06 21:15:36 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 21:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:18:49 | INFO | train_inner | epoch 050:     61 / 196 loss=3.551, nll_loss=3.3, ppl=9.85, wps=20217.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.888, loss_scale=32, train_wall=292, gb_free=19.9, wall=31038
2022-03-06 21:19:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:24:08 | INFO | train_inner | epoch 050:    162 / 196 loss=3.57, nll_loss=3.318, ppl=9.98, wps=20545.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.897, loss_scale=16, train_wall=296, gb_free=19.9, wall=31357
2022-03-06 21:25:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:26:00 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.249 | nll_loss 8.047 | ppl 264.5 | wps 40686.6 | wpb 510.9 | bsz 1 | num_updates 9734 | best_loss 6.936
2022-03-06 21:26:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9734 updates
2022-03-06 21:26:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:26:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:26:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 50 @ 9734 updates, score 8.249) (writing took 3.1415171860717237 seconds)
2022-03-06 21:26:03 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 21:26:03 | INFO | train | epoch 050 | loss 3.552 | nll_loss 3.3 | ppl 9.85 | wps 20345.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9734 | lr 0.000320519 | gnorm 0.89 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 31472
2022-03-06 21:26:03 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 21:26:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:28:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:29:35 | INFO | train_inner | epoch 051:     67 / 196 loss=3.501, nll_loss=3.248, ppl=9.5, wps=19988.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.891, loss_scale=16, train_wall=296, gb_free=19.9, wall=31684
2022-03-06 21:34:51 | INFO | train_inner | epoch 051:    167 / 196 loss=3.544, nll_loss=3.291, ppl=9.79, wps=20731.4, ups=0.32, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.916, loss_scale=32, train_wall=294, gb_free=19.9, wall=32000
2022-03-06 21:34:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:36:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:36:28 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.307 | nll_loss 8.106 | ppl 275.46 | wps 39407.4 | wpb 510.9 | bsz 1 | num_updates 9928 | best_loss 6.936
2022-03-06 21:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9928 updates
2022-03-06 21:36:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 51 @ 9928 updates, score 8.307) (writing took 3.5831887149251997 seconds)
2022-03-06 21:36:31 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 21:36:31 | INFO | train | epoch 051 | loss 3.516 | nll_loss 3.263 | ppl 9.6 | wps 20221.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9928 | lr 0.000317372 | gnorm 0.902 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 32100
2022-03-06 21:36:31 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 21:36:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:40:18 | INFO | train_inner | epoch 052:     72 / 196 loss=3.472, nll_loss=3.218, ppl=9.31, wps=19968.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.894, loss_scale=16, train_wall=295, gb_free=19.9, wall=32327
2022-03-06 21:42:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:45:37 | INFO | train_inner | epoch 052:    173 / 196 loss=3.516, nll_loss=3.263, ppl=9.6, wps=20538.5, ups=0.31, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.914, loss_scale=16, train_wall=296, gb_free=19.9, wall=32646
2022-03-06 21:46:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:46:55 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.398 | nll_loss 8.196 | ppl 293.33 | wps 39418.1 | wpb 510.9 | bsz 1 | num_updates 10123 | best_loss 6.936
2022-03-06 21:46:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10123 updates
2022-03-06 21:46:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:46:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:46:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 52 @ 10123 updates, score 8.398) (writing took 3.8840931421145797 seconds)
2022-03-06 21:46:59 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 21:46:59 | INFO | train | epoch 052 | loss 3.485 | nll_loss 3.231 | ppl 9.39 | wps 20329.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10123 | lr 0.000314301 | gnorm 0.908 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 32728
2022-03-06 21:46:59 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 21:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:51:02 | INFO | train_inner | epoch 053:     77 / 196 loss=3.429, nll_loss=3.174, ppl=9.03, wps=20120.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.912, loss_scale=32, train_wall=293, gb_free=19.9, wall=32971
2022-03-06 21:51:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:56:21 | INFO | train_inner | epoch 053:    178 / 196 loss=3.488, nll_loss=3.234, ppl=9.41, wps=20546.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.925, loss_scale=16, train_wall=296, gb_free=19.9, wall=33290
2022-03-06 21:57:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:57:22 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.399 | nll_loss 8.197 | ppl 293.45 | wps 40499.5 | wpb 510.9 | bsz 1 | num_updates 10318 | best_loss 6.936
2022-03-06 21:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10318 updates
2022-03-06 21:57:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:57:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 21:57:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 53 @ 10318 updates, score 8.399) (writing took 3.3996386979706585 seconds)
2022-03-06 21:57:26 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 21:57:26 | INFO | train | epoch 053 | loss 3.451 | nll_loss 3.196 | ppl 9.17 | wps 20358.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10318 | lr 0.000311317 | gnorm 0.917 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 33355
2022-03-06 21:57:26 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 21:57:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:59:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:01:47 | INFO | train_inner | epoch 054:     83 / 196 loss=3.38, nll_loss=3.124, ppl=8.72, wps=20058.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.911, loss_scale=16, train_wall=294, gb_free=19.9, wall=33616
2022-03-06 22:07:03 | INFO | train_inner | epoch 054:    183 / 196 loss=3.462, nll_loss=3.207, ppl=9.24, wps=20729.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.948, loss_scale=32, train_wall=294, gb_free=19.9, wall=33932
2022-03-06 22:07:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:07:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:07:49 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.451 | nll_loss 8.248 | ppl 304.04 | wps 41317.5 | wpb 510.9 | bsz 1 | num_updates 10512 | best_loss 6.936
2022-03-06 22:07:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10512 updates
2022-03-06 22:07:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:07:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:07:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 54 @ 10512 updates, score 8.451) (writing took 3.158804640173912 seconds)
2022-03-06 22:07:52 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 22:07:52 | INFO | train | epoch 054 | loss 3.419 | nll_loss 3.163 | ppl 8.96 | wps 20283.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10512 | lr 0.000308431 | gnorm 0.931 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 33981
2022-03-06 22:07:52 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 22:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:12:30 | INFO | train_inner | epoch 055:     88 / 196 loss=3.352, nll_loss=3.094, ppl=8.54, wps=19992.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.909, loss_scale=16, train_wall=296, gb_free=19.9, wall=34259
2022-03-06 22:14:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:17:50 | INFO | train_inner | epoch 055:    189 / 196 loss=3.437, nll_loss=3.181, ppl=9.07, wps=20527.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.917, loss_scale=16, train_wall=296, gb_free=19.9, wall=34578
2022-03-06 22:18:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:18:16 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.486 | nll_loss 8.286 | ppl 312.08 | wps 41064.6 | wpb 510.9 | bsz 1 | num_updates 10707 | best_loss 6.936
2022-03-06 22:18:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10707 updates
2022-03-06 22:18:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:18:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:18:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 55 @ 10707 updates, score 8.486) (writing took 2.9871578209567815 seconds)
2022-03-06 22:18:19 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 22:18:19 | INFO | train | epoch 055 | loss 3.39 | nll_loss 3.133 | ppl 8.77 | wps 20349.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10707 | lr 0.000305609 | gnorm 0.913 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 34608
2022-03-06 22:18:19 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 22:18:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:21:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:23:16 | INFO | train_inner | epoch 056:     94 / 196 loss=3.317, nll_loss=3.059, ppl=8.33, wps=20007.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.922, loss_scale=16, train_wall=295, gb_free=19.9, wall=34905
2022-03-06 22:28:32 | INFO | train_inner | epoch 056:    194 / 196 loss=3.406, nll_loss=3.149, ppl=8.87, wps=20739.1, ups=0.32, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.934, loss_scale=32, train_wall=293, gb_free=19.9, wall=35221
2022-03-06 22:28:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:28:43 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.547 | nll_loss 8.343 | ppl 324.73 | wps 41161.6 | wpb 510.9 | bsz 1 | num_updates 10901 | best_loss 6.936
2022-03-06 22:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10901 updates
2022-03-06 22:28:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:28:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 56 @ 10901 updates, score 8.547) (writing took 3.138273910153657 seconds)
2022-03-06 22:28:46 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 22:28:46 | INFO | train | epoch 056 | loss 3.36 | nll_loss 3.102 | ppl 8.59 | wps 20244.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10901 | lr 0.000302877 | gnorm 0.929 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 35235
2022-03-06 22:28:46 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 22:28:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:33:59 | INFO | train_inner | epoch 057:     99 / 196 loss=3.285, nll_loss=3.026, ppl=8.14, wps=19980.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.917, loss_scale=16, train_wall=296, gb_free=19.9, wall=35548
2022-03-06 22:35:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:39:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:39:10 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.593 | nll_loss 8.393 | ppl 336.26 | wps 41065.9 | wpb 510.9 | bsz 1 | num_updates 11096 | best_loss 6.936
2022-03-06 22:39:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11096 updates
2022-03-06 22:39:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:39:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 57 @ 11096 updates, score 8.593) (writing took 3.0479429340921342 seconds)
2022-03-06 22:39:14 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 22:39:14 | INFO | train | epoch 057 | loss 3.33 | nll_loss 3.071 | ppl 8.41 | wps 20344.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11096 | lr 0.000300204 | gnorm 0.925 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 35862
2022-03-06 22:39:14 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 22:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:39:26 | INFO | train_inner | epoch 058:      4 / 196 loss=3.373, nll_loss=3.115, ppl=8.66, wps=20001.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.936, loss_scale=16, train_wall=296, gb_free=19.9, wall=35875
2022-03-06 22:43:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:44:44 | INFO | train_inner | epoch 058:    105 / 196 loss=3.262, nll_loss=3.002, ppl=8.01, wps=20591.2, ups=0.31, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.93, loss_scale=16, train_wall=296, gb_free=19.9, wall=36193
2022-03-06 22:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:49:36 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.662 | nll_loss 8.462 | ppl 352.71 | wps 40028.7 | wpb 510.9 | bsz 1 | num_updates 11291 | best_loss 6.936
2022-03-06 22:49:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11291 updates
2022-03-06 22:49:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:49:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 22:49:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 58 @ 11291 updates, score 8.662) (writing took 3.727367199026048 seconds)
2022-03-06 22:49:40 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 22:49:40 | INFO | train | epoch 058 | loss 3.303 | nll_loss 3.043 | ppl 8.24 | wps 20386.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11291 | lr 0.000297601 | gnorm 0.935 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 36488
2022-03-06 22:49:40 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 22:49:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:50:08 | INFO | train_inner | epoch 059:      9 / 196 loss=3.333, nll_loss=3.074, ppl=8.42, wps=20190.1, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.939, loss_scale=32, train_wall=292, gb_free=19.9, wall=36517
2022-03-06 22:50:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:55:27 | INFO | train_inner | epoch 059:    110 / 196 loss=3.235, nll_loss=2.974, ppl=7.85, wps=20539.6, ups=0.31, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.942, loss_scale=16, train_wall=296, gb_free=19.9, wall=36836
2022-03-06 22:57:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:00:04 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.688 | nll_loss 8.487 | ppl 358.67 | wps 39540.4 | wpb 510.9 | bsz 1 | num_updates 11485 | best_loss 6.936
2022-03-06 23:00:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11485 updates
2022-03-06 23:00:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:00:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:00:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 59 @ 11485 updates, score 8.688) (writing took 4.1771687341388315 seconds)
2022-03-06 23:00:08 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 23:00:08 | INFO | train | epoch 059 | loss 3.274 | nll_loss 3.014 | ppl 8.08 | wps 20206.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11485 | lr 0.000295076 | gnorm 0.946 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 37117
2022-03-06 23:00:08 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 23:00:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:00:56 | INFO | train_inner | epoch 060:     15 / 196 loss=3.306, nll_loss=3.047, ppl=8.26, wps=19907, ups=0.3, wpb=65367, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.948, loss_scale=16, train_wall=296, gb_free=19.9, wall=37165
2022-03-06 23:06:11 | INFO | train_inner | epoch 060:    115 / 196 loss=3.22, nll_loss=2.959, ppl=7.77, wps=20752.3, ups=0.32, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.932, loss_scale=32, train_wall=293, gb_free=19.9, wall=37480
2022-03-06 23:06:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:10:32 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.727 | nll_loss 8.525 | ppl 368.37 | wps 39514 | wpb 510.9 | bsz 1 | num_updates 11680 | best_loss 6.936
2022-03-06 23:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11680 updates
2022-03-06 23:10:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 60 @ 11680 updates, score 8.727) (writing took 3.6295390748418868 seconds)
2022-03-06 23:10:36 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 23:10:36 | INFO | train | epoch 060 | loss 3.249 | nll_loss 2.988 | ppl 7.93 | wps 20327.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11680 | lr 0.000292603 | gnorm 0.943 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 37745
2022-03-06 23:10:36 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 23:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:11:39 | INFO | train_inner | epoch 061:     20 / 196 loss=3.267, nll_loss=3.006, ppl=8.03, wps=19945.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.951, loss_scale=16, train_wall=296, gb_free=19.9, wall=37808
2022-03-06 23:13:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:16:58 | INFO | train_inner | epoch 061:    121 / 196 loss=3.199, nll_loss=2.936, ppl=7.65, wps=20534.6, ups=0.31, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.941, loss_scale=16, train_wall=296, gb_free=19.9, wall=38127
2022-03-06 23:20:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:20:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:21:00 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.773 | nll_loss 8.571 | ppl 380.37 | wps 39831.7 | wpb 510.9 | bsz 1 | num_updates 11874 | best_loss 6.936
2022-03-06 23:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11874 updates
2022-03-06 23:21:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:21:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 61 @ 11874 updates, score 8.773) (writing took 3.5891587678343058 seconds)
2022-03-06 23:21:03 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 23:21:03 | INFO | train | epoch 061 | loss 3.223 | nll_loss 2.961 | ppl 7.79 | wps 20226.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11874 | lr 0.000290203 | gnorm 0.941 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 38372
2022-03-06 23:21:03 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 23:21:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:22:26 | INFO | train_inner | epoch 062:     26 / 196 loss=3.236, nll_loss=2.975, ppl=7.86, wps=19957.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.941, loss_scale=16, train_wall=296, gb_free=19.9, wall=38455
2022-03-06 23:27:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:27:45 | INFO | train_inner | epoch 062:    127 / 196 loss=3.177, nll_loss=2.914, ppl=7.54, wps=20542.6, ups=0.31, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.952, loss_scale=16, train_wall=296, gb_free=19.9, wall=38774
2022-03-06 23:31:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:31:26 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.843 | nll_loss 8.64 | ppl 398.93 | wps 41105.5 | wpb 510.9 | bsz 1 | num_updates 12069 | best_loss 6.936
2022-03-06 23:31:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12069 updates
2022-03-06 23:31:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:31:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:31:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 62 @ 12069 updates, score 8.843) (writing took 3.2138492700178176 seconds)
2022-03-06 23:31:29 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 23:31:29 | INFO | train | epoch 062 | loss 3.199 | nll_loss 2.936 | ppl 7.65 | wps 20392.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12069 | lr 0.000287849 | gnorm 0.946 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 38998
2022-03-06 23:31:29 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 23:31:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:33:07 | INFO | train_inner | epoch 063:     31 / 196 loss=3.212, nll_loss=2.95, ppl=7.73, wps=20266.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.941, loss_scale=16, train_wall=292, gb_free=19.9, wall=39096
2022-03-06 23:34:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:38:27 | INFO | train_inner | epoch 063:    132 / 196 loss=3.156, nll_loss=2.892, ppl=7.42, wps=20531.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.956, loss_scale=16, train_wall=296, gb_free=19.9, wall=39415
2022-03-06 23:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:41:53 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.887 | nll_loss 8.684 | ppl 411.34 | wps 39968.8 | wpb 510.9 | bsz 1 | num_updates 12263 | best_loss 6.936
2022-03-06 23:41:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12263 updates
2022-03-06 23:41:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:41:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 63 @ 12263 updates, score 8.887) (writing took 3.840336407069117 seconds)
2022-03-06 23:41:57 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 23:41:57 | INFO | train | epoch 063 | loss 3.173 | nll_loss 2.91 | ppl 7.52 | wps 20227.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12263 | lr 0.000285563 | gnorm 0.951 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 39626
2022-03-06 23:41:57 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 23:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:43:54 | INFO | train_inner | epoch 064:     37 / 196 loss=3.178, nll_loss=2.915, ppl=7.54, wps=19950, ups=0.31, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.947, loss_scale=16, train_wall=295, gb_free=19.9, wall=39743
2022-03-06 23:49:11 | INFO | train_inner | epoch 064:    137 / 196 loss=3.139, nll_loss=2.874, ppl=7.33, wps=20709.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.949, loss_scale=32, train_wall=294, gb_free=19.9, wall=40060
2022-03-06 23:49:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:52:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:52:21 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.89 | nll_loss 8.688 | ppl 412.43 | wps 39493.7 | wpb 510.9 | bsz 1 | num_updates 12458 | best_loss 6.936
2022-03-06 23:52:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12458 updates
2022-03-06 23:52:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:52:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-06 23:52:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 64 @ 12458 updates, score 8.89) (writing took 3.772314616944641 seconds)
2022-03-06 23:52:25 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 23:52:25 | INFO | train | epoch 064 | loss 3.151 | nll_loss 2.886 | ppl 7.39 | wps 20316.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12458 | lr 0.000283319 | gnorm 0.952 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 40254
2022-03-06 23:52:25 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 23:52:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:54:38 | INFO | train_inner | epoch 065:     42 / 196 loss=3.146, nll_loss=2.882, ppl=7.37, wps=19943.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.966, loss_scale=16, train_wall=296, gb_free=19.9, wall=40387
2022-03-06 23:57:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:59:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:00:01 | INFO | train_inner | epoch 065:    144 / 196 loss=3.124, nll_loss=2.859, ppl=7.25, wps=20339.5, ups=0.31, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.968, loss_scale=8, train_wall=299, gb_free=19.9, wall=40710
2022-03-07 00:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:02:49 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.93 | nll_loss 8.726 | ppl 423.58 | wps 39671.5 | wpb 510.9 | bsz 1 | num_updates 12652 | best_loss 6.936
2022-03-07 00:02:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12652 updates
2022-03-07 00:02:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:02:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:02:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 65 @ 12652 updates, score 8.93) (writing took 3.554820867953822 seconds)
2022-03-07 00:02:53 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-07 00:02:53 | INFO | train | epoch 065 | loss 3.127 | nll_loss 2.862 | ppl 7.27 | wps 20235.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12652 | lr 0.000281139 | gnorm 0.962 | loss_scale 8 | train_wall 574 | gb_free 19.9 | wall 40882
2022-03-07 00:02:53 | INFO | fairseq.trainer | begin training epoch 66
2022-03-07 00:02:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:05:25 | INFO | train_inner | epoch 066:     48 / 196 loss=3.12, nll_loss=2.855, ppl=7.24, wps=20172.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.947, loss_scale=8, train_wall=292, gb_free=19.9, wall=41034
2022-03-07 00:10:41 | INFO | train_inner | epoch 066:    148 / 196 loss=3.11, nll_loss=2.844, ppl=7.18, wps=20736.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.955, loss_scale=16, train_wall=293, gb_free=19.9, wall=41350
2022-03-07 00:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:13:16 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 8.997 | nll_loss 8.793 | ppl 443.65 | wps 39521.1 | wpb 510.9 | bsz 1 | num_updates 12848 | best_loss 6.936
2022-03-07 00:13:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12848 updates
2022-03-07 00:13:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:13:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 66 @ 12848 updates, score 8.997) (writing took 3.5607400690205395 seconds)
2022-03-07 00:13:20 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-07 00:13:20 | INFO | train | epoch 066 | loss 3.106 | nll_loss 2.841 | ppl 7.16 | wps 20442.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 12848 | lr 0.000278986 | gnorm 0.958 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 41509
2022-03-07 00:13:20 | INFO | fairseq.trainer | begin training epoch 67
2022-03-07 00:13:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:13:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:16:07 | INFO | train_inner | epoch 067:     53 / 196 loss=3.089, nll_loss=2.823, ppl=7.08, wps=20035.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.963, loss_scale=16, train_wall=294, gb_free=19.9, wall=41676
2022-03-07 00:20:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:21:25 | INFO | train_inner | epoch 067:    154 / 196 loss=3.091, nll_loss=2.825, ppl=7.08, wps=20609.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.96, loss_scale=16, train_wall=295, gb_free=19.9, wall=41994
2022-03-07 00:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:23:43 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.007 | nll_loss 8.802 | ppl 446.22 | wps 39537.8 | wpb 510.9 | bsz 1 | num_updates 13042 | best_loss 6.936
2022-03-07 00:23:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13042 updates
2022-03-07 00:23:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:23:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:23:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 67 @ 13042 updates, score 9.007) (writing took 3.8812996041961014 seconds)
2022-03-07 00:23:47 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-07 00:23:47 | INFO | train | epoch 067 | loss 3.082 | nll_loss 2.816 | ppl 7.04 | wps 20262.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13042 | lr 0.000276903 | gnorm 0.958 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 42136
2022-03-07 00:23:47 | INFO | fairseq.trainer | begin training epoch 68
2022-03-07 00:23:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:26:49 | INFO | train_inner | epoch 068:     58 / 196 loss=3.058, nll_loss=2.791, ppl=6.92, wps=20139.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.97, loss_scale=16, train_wall=293, gb_free=19.9, wall=42318
2022-03-07 00:27:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:32:08 | INFO | train_inner | epoch 068:    159 / 196 loss=3.074, nll_loss=2.807, ppl=7, wps=20548.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.967, loss_scale=16, train_wall=296, gb_free=19.9, wall=42637
2022-03-07 00:33:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:34:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:34:10 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.055 | nll_loss 8.853 | ppl 462.36 | wps 39452.8 | wpb 510.9 | bsz 1 | num_updates 13236 | best_loss 6.936
2022-03-07 00:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13236 updates
2022-03-07 00:34:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:34:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 68 @ 13236 updates, score 9.055) (writing took 3.528505334863439 seconds)
2022-03-07 00:34:14 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-07 00:34:14 | INFO | train | epoch 068 | loss 3.06 | nll_loss 2.793 | ppl 6.93 | wps 20248 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13236 | lr 0.000274866 | gnorm 0.968 | loss_scale 8 | train_wall 574 | gb_free 19.9 | wall 42763
2022-03-07 00:34:14 | INFO | fairseq.trainer | begin training epoch 69
2022-03-07 00:34:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:37:36 | INFO | train_inner | epoch 069:     64 / 196 loss=3.032, nll_loss=2.764, ppl=6.79, wps=19977, ups=0.31, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=0.957, loss_scale=8, train_wall=295, gb_free=19.9, wall=42965
2022-03-07 00:42:51 | INFO | train_inner | epoch 069:    164 / 196 loss=3.061, nll_loss=2.794, ppl=6.94, wps=20764.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.967, loss_scale=16, train_wall=293, gb_free=19.9, wall=43280
2022-03-07 00:44:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:44:37 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.09 | nll_loss 8.886 | ppl 473.01 | wps 39947.3 | wpb 510.9 | bsz 1 | num_updates 13432 | best_loss 6.936
2022-03-07 00:44:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13432 updates
2022-03-07 00:44:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:44:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:44:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 69 @ 13432 updates, score 9.09) (writing took 3.593621702864766 seconds)
2022-03-07 00:44:41 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-07 00:44:41 | INFO | train | epoch 069 | loss 3.042 | nll_loss 2.775 | ppl 6.84 | wps 20450 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 13432 | lr 0.000272854 | gnorm 0.964 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 43390
2022-03-07 00:44:41 | INFO | fairseq.trainer | begin training epoch 70
2022-03-07 00:44:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:48:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:48:19 | INFO | train_inner | epoch 070:     69 / 196 loss=3.01, nll_loss=2.742, ppl=6.69, wps=19950.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.972, loss_scale=16, train_wall=296, gb_free=19.9, wall=43608
2022-03-07 00:53:34 | INFO | train_inner | epoch 070:    169 / 196 loss=3.042, nll_loss=2.774, ppl=6.84, wps=20765.6, ups=0.32, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.976, loss_scale=16, train_wall=293, gb_free=19.9, wall=43923
2022-03-07 00:54:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:55:05 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.124 | nll_loss 8.921 | ppl 484.65 | wps 39640.2 | wpb 510.9 | bsz 1 | num_updates 13627 | best_loss 6.936
2022-03-07 00:55:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13627 updates
2022-03-07 00:55:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:55:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 00:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 70 @ 13627 updates, score 9.124) (writing took 3.5498011759482324 seconds)
2022-03-07 00:55:08 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-07 00:55:08 | INFO | train | epoch 070 | loss 3.021 | nll_loss 2.752 | ppl 6.74 | wps 20348.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13627 | lr 0.000270894 | gnorm 0.973 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 44017
2022-03-07 00:55:08 | INFO | fairseq.trainer | begin training epoch 71
2022-03-07 00:55:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:55:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:59:02 | INFO | train_inner | epoch 071:     74 / 196 loss=2.985, nll_loss=2.716, ppl=6.57, wps=19959.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=0.977, loss_scale=16, train_wall=296, gb_free=19.9, wall=44251
2022-03-07 01:02:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:04:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:04:22 | INFO | train_inner | epoch 071:    176 / 196 loss=3.022, nll_loss=2.754, ppl=6.75, wps=20471.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.984, loss_scale=8, train_wall=297, gb_free=19.9, wall=44571
2022-03-07 01:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:05:29 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.174 | nll_loss 8.971 | ppl 501.93 | wps 41118.6 | wpb 510.9 | bsz 1 | num_updates 13820 | best_loss 6.936
2022-03-07 01:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13820 updates
2022-03-07 01:05:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:05:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 71 @ 13820 updates, score 9.174) (writing took 3.6052805930376053 seconds)
2022-03-07 01:05:33 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-07 01:05:33 | INFO | train | epoch 071 | loss 3 | nll_loss 2.731 | ppl 6.64 | wps 20221.7 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 13820 | lr 0.000268996 | gnorm 0.983 | loss_scale 8 | train_wall 572 | gb_free 19.9 | wall 44642
2022-03-07 01:05:33 | INFO | fairseq.trainer | begin training epoch 72
2022-03-07 01:05:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:09:43 | INFO | train_inner | epoch 072:     80 / 196 loss=2.955, nll_loss=2.685, ppl=6.43, wps=20338.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.982, loss_scale=8, train_wall=290, gb_free=19.9, wall=44892
2022-03-07 01:12:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:15:00 | INFO | train_inner | epoch 072:    181 / 196 loss=3.015, nll_loss=2.747, ppl=6.71, wps=20728.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.997, loss_scale=8, train_wall=294, gb_free=19.9, wall=45208
2022-03-07 01:15:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:15:51 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.228 | nll_loss 9.029 | ppl 522.31 | wps 41292.3 | wpb 510.9 | bsz 1 | num_updates 14015 | best_loss 6.936
2022-03-07 01:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14015 updates
2022-03-07 01:15:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:15:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 72 @ 14015 updates, score 9.228) (writing took 3.594691603910178 seconds)
2022-03-07 01:15:55 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-07 01:15:55 | INFO | train | epoch 072 | loss 2.983 | nll_loss 2.714 | ppl 6.56 | wps 20523.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14015 | lr 0.000267118 | gnorm 0.988 | loss_scale 8 | train_wall 569 | gb_free 19.9 | wall 45264
2022-03-07 01:15:55 | INFO | fairseq.trainer | begin training epoch 73
2022-03-07 01:15:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:20:21 | INFO | train_inner | epoch 073:     85 / 196 loss=2.936, nll_loss=2.666, ppl=6.35, wps=20334.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=0.976, loss_scale=16, train_wall=290, gb_free=19.9, wall=45530
2022-03-07 01:25:34 | INFO | train_inner | epoch 073:    185 / 196 loss=2.998, nll_loss=2.729, ppl=6.63, wps=20917.5, ups=0.32, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.981, loss_scale=32, train_wall=291, gb_free=19.9, wall=45843
2022-03-07 01:25:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:26:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:26:13 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.251 | nll_loss 9.046 | ppl 528.59 | wps 41154.3 | wpb 510.9 | bsz 1 | num_updates 14210 | best_loss 6.936
2022-03-07 01:26:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14210 updates
2022-03-07 01:26:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:26:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:26:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 73 @ 14210 updates, score 9.251) (writing took 3.555359700927511 seconds)
2022-03-07 01:26:17 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-07 01:26:17 | INFO | train | epoch 073 | loss 2.963 | nll_loss 2.693 | ppl 6.47 | wps 20512.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14210 | lr 0.000265279 | gnorm 0.981 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 45886
2022-03-07 01:26:17 | INFO | fairseq.trainer | begin training epoch 74
2022-03-07 01:26:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:30:59 | INFO | train_inner | epoch 074:     90 / 196 loss=2.908, nll_loss=2.637, ppl=6.22, wps=20144.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.989, loss_scale=16, train_wall=293, gb_free=19.9, wall=46168
2022-03-07 01:32:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:36:15 | INFO | train_inner | epoch 074:    191 / 196 loss=2.99, nll_loss=2.72, ppl=6.59, wps=20721.2, ups=0.32, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.995, loss_scale=16, train_wall=294, gb_free=19.9, wall=46484
2022-03-07 01:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:36:35 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.328 | nll_loss 9.125 | ppl 558.44 | wps 41167.9 | wpb 510.9 | bsz 1 | num_updates 14405 | best_loss 6.936
2022-03-07 01:36:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14405 updates
2022-03-07 01:36:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:36:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:36:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 74 @ 14405 updates, score 9.328) (writing took 3.556799567071721 seconds)
2022-03-07 01:36:39 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-07 01:36:39 | INFO | train | epoch 074 | loss 2.945 | nll_loss 2.674 | ppl 6.38 | wps 20518.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14405 | lr 0.000263477 | gnorm 0.989 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 46508
2022-03-07 01:36:39 | INFO | fairseq.trainer | begin training epoch 75
2022-03-07 01:36:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:39:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:41:39 | INFO | train_inner | epoch 075:     96 / 196 loss=2.884, nll_loss=2.612, ppl=6.11, wps=20151.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=0.994, loss_scale=16, train_wall=293, gb_free=19.9, wall=46808
2022-03-07 01:44:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:46:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:46:57 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.333 | nll_loss 9.132 | ppl 560.93 | wps 41127.5 | wpb 510.9 | bsz 1 | num_updates 14599 | best_loss 6.936
2022-03-07 01:46:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14599 updates
2022-03-07 01:46:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:47:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:47:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 75 @ 14599 updates, score 9.333) (writing took 3.5204659081064165 seconds)
2022-03-07 01:47:00 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-07 01:47:00 | INFO | train | epoch 075 | loss 2.927 | nll_loss 2.656 | ppl 6.3 | wps 20425.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14599 | lr 0.000261721 | gnorm 0.997 | loss_scale 8 | train_wall 569 | gb_free 19.9 | wall 47129
2022-03-07 01:47:00 | INFO | fairseq.trainer | begin training epoch 76
2022-03-07 01:47:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:47:04 | INFO | train_inner | epoch 076:      1 / 196 loss=2.973, nll_loss=2.703, ppl=6.51, wps=20156.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=1, loss_scale=8, train_wall=293, gb_free=19.9, wall=47133
2022-03-07 01:52:17 | INFO | train_inner | epoch 076:    101 / 196 loss=2.873, nll_loss=2.6, ppl=6.06, wps=20932.4, ups=0.32, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.986, loss_scale=16, train_wall=291, gb_free=19.9, wall=47446
2022-03-07 01:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:57:19 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.391 | nll_loss 9.19 | ppl 584.13 | wps 41124 | wpb 510.9 | bsz 1 | num_updates 14795 | best_loss 6.936
2022-03-07 01:57:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14795 updates
2022-03-07 01:57:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:57:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 01:57:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 76 @ 14795 updates, score 9.391) (writing took 3.548459712881595 seconds)
2022-03-07 01:57:22 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-07 01:57:22 | INFO | train | epoch 076 | loss 2.911 | nll_loss 2.639 | ppl 6.23 | wps 20625.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 14795 | lr 0.000259982 | gnorm 0.996 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 47751
2022-03-07 01:57:22 | INFO | fairseq.trainer | begin training epoch 77
2022-03-07 01:57:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:57:38 | INFO | train_inner | epoch 077:      5 / 196 loss=2.941, nll_loss=2.671, ppl=6.37, wps=20340.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.005, loss_scale=16, train_wall=290, gb_free=19.9, wall=47767
2022-03-07 01:58:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:02:54 | INFO | train_inner | epoch 077:    106 / 196 loss=2.857, nll_loss=2.584, ppl=5.99, wps=20715.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.981, loss_scale=16, train_wall=294, gb_free=19.9, wall=48083
2022-03-07 02:05:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:07:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:07:41 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.416 | nll_loss 9.215 | ppl 594.12 | wps 41089.1 | wpb 510.9 | bsz 1 | num_updates 14989 | best_loss 6.936
2022-03-07 02:07:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14989 updates
2022-03-07 02:07:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:07:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 77 @ 14989 updates, score 9.416) (writing took 3.4912854328285903 seconds)
2022-03-07 02:07:44 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-07 02:07:44 | INFO | train | epoch 077 | loss 2.892 | nll_loss 2.62 | ppl 6.15 | wps 20414.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14989 | lr 0.000258294 | gnorm 0.995 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 48373
2022-03-07 02:07:44 | INFO | fairseq.trainer | begin training epoch 78
2022-03-07 02:07:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:08:19 | INFO | train_inner | epoch 078:     11 / 196 loss=2.924, nll_loss=2.653, ppl=6.29, wps=20147.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.01, loss_scale=16, train_wall=293, gb_free=19.9, wall=48408
2022-03-07 02:12:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:13:35 | INFO | train_inner | epoch 078:    112 / 196 loss=2.838, nll_loss=2.564, ppl=5.91, wps=20718.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.986, loss_scale=16, train_wall=294, gb_free=19.9, wall=48724
2022-03-07 02:17:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:18:03 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.437 | nll_loss 9.234 | ppl 602.3 | wps 40745.4 | wpb 510.9 | bsz 1 | num_updates 15184 | best_loss 6.936
2022-03-07 02:18:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15184 updates
2022-03-07 02:18:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:18:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:18:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 78 @ 15184 updates, score 9.437) (writing took 3.5771446200087667 seconds)
2022-03-07 02:18:06 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-07 02:18:06 | INFO | train | epoch 078 | loss 2.877 | nll_loss 2.604 | ppl 6.08 | wps 20513.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15184 | lr 0.00025663 | gnorm 0.993 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 48995
2022-03-07 02:18:06 | INFO | fairseq.trainer | begin training epoch 79
2022-03-07 02:18:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:18:57 | INFO | train_inner | epoch 079:     16 / 196 loss=2.906, nll_loss=2.634, ppl=6.21, wps=20337.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=0.997, loss_scale=16, train_wall=290, gb_free=19.9, wall=49046
2022-03-07 02:19:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:24:13 | INFO | train_inner | epoch 079:    117 / 196 loss=2.831, nll_loss=2.557, ppl=5.89, wps=20721.1, ups=0.32, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.01, loss_scale=16, train_wall=294, gb_free=19.9, wall=49362
2022-03-07 02:28:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:28:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:28:25 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.443 | nll_loss 9.24 | ppl 604.5 | wps 41306.9 | wpb 510.9 | bsz 1 | num_updates 15378 | best_loss 6.936
2022-03-07 02:28:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15378 updates
2022-03-07 02:28:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:28:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:28:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 79 @ 15378 updates, score 9.443) (writing took 3.4906128079164773 seconds)
2022-03-07 02:28:28 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-07 02:28:28 | INFO | train | epoch 079 | loss 2.86 | nll_loss 2.587 | ppl 6.01 | wps 20417 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15378 | lr 0.000255006 | gnorm 1.007 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 49617
2022-03-07 02:28:28 | INFO | fairseq.trainer | begin training epoch 80
2022-03-07 02:28:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:29:37 | INFO | train_inner | epoch 080:     22 / 196 loss=2.882, nll_loss=2.609, ppl=6.1, wps=20151.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.009, loss_scale=16, train_wall=293, gb_free=19.9, wall=49686
2022-03-07 02:34:50 | INFO | train_inner | epoch 080:    122 / 196 loss=2.819, nll_loss=2.545, ppl=5.83, wps=20928.7, ups=0.32, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.989, loss_scale=16, train_wall=291, gb_free=19.9, wall=49999
2022-03-07 02:36:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:36:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:38:47 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.49 | nll_loss 9.288 | ppl 624.98 | wps 41168.8 | wpb 510.9 | bsz 1 | num_updates 15572 | best_loss 6.936
2022-03-07 02:38:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15572 updates
2022-03-07 02:38:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:38:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:38:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 80 @ 15572 updates, score 9.49) (writing took 3.3630224189255387 seconds)
2022-03-07 02:38:50 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-07 02:38:50 | INFO | train | epoch 080 | loss 2.844 | nll_loss 2.571 | ppl 5.94 | wps 20420.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15572 | lr 0.000253412 | gnorm 1 | loss_scale 8 | train_wall 570 | gb_free 19.9 | wall 50239
2022-03-07 02:38:50 | INFO | fairseq.trainer | begin training epoch 81
2022-03-07 02:38:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:40:18 | INFO | train_inner | epoch 081:     28 / 196 loss=2.869, nll_loss=2.596, ppl=6.05, wps=19963.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.016, loss_scale=8, train_wall=296, gb_free=19.9, wall=50327
2022-03-07 02:45:31 | INFO | train_inner | epoch 081:    128 / 196 loss=2.809, nll_loss=2.534, ppl=5.79, wps=20925, ups=0.32, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.995, loss_scale=16, train_wall=291, gb_free=19.9, wall=50640
2022-03-07 02:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:49:08 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.554 | nll_loss 9.353 | ppl 653.91 | wps 41434.4 | wpb 510.9 | bsz 1 | num_updates 15768 | best_loss 6.936
2022-03-07 02:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15768 updates
2022-03-07 02:49:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:49:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:49:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 81 @ 15768 updates, score 9.554) (writing took 3.383662353036925 seconds)
2022-03-07 02:49:12 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-07 02:49:12 | INFO | train | epoch 081 | loss 2.829 | nll_loss 2.555 | ppl 5.88 | wps 20631.8 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 15768 | lr 0.000251832 | gnorm 1.006 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 50861
2022-03-07 02:49:12 | INFO | fairseq.trainer | begin training epoch 82
2022-03-07 02:49:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:50:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:50:55 | INFO | train_inner | epoch 082:     33 / 196 loss=2.838, nll_loss=2.564, ppl=5.91, wps=20160.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.017, loss_scale=16, train_wall=293, gb_free=19.9, wall=50964
2022-03-07 02:56:08 | INFO | train_inner | epoch 082:    133 / 196 loss=2.801, nll_loss=2.526, ppl=5.76, wps=20925, ups=0.32, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.005, loss_scale=16, train_wall=291, gb_free=19.9, wall=51277
2022-03-07 02:57:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:59:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:59:30 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.578 | nll_loss 9.374 | ppl 663.54 | wps 41372.9 | wpb 510.9 | bsz 1 | num_updates 15962 | best_loss 6.936
2022-03-07 02:59:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15962 updates
2022-03-07 02:59:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 02:59:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 82 @ 15962 updates, score 9.578) (writing took 3.3478728958871216 seconds)
2022-03-07 02:59:34 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-07 02:59:34 | INFO | train | epoch 082 | loss 2.813 | nll_loss 2.538 | ppl 5.81 | wps 20423.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15962 | lr 0.000250297 | gnorm 1.009 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 51482
2022-03-07 02:59:34 | INFO | fairseq.trainer | begin training epoch 83
2022-03-07 02:59:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:01:33 | INFO | train_inner | epoch 083:     38 / 196 loss=2.811, nll_loss=2.536, ppl=5.8, wps=20161.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.005, loss_scale=16, train_wall=293, gb_free=19.9, wall=51602
2022-03-07 03:05:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:06:49 | INFO | train_inner | epoch 083:    139 / 196 loss=2.795, nll_loss=2.52, ppl=5.74, wps=20728.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.022, loss_scale=16, train_wall=294, gb_free=19.9, wall=51918
2022-03-07 03:09:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:09:52 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.605 | nll_loss 9.405 | ppl 678 | wps 41510.6 | wpb 510.9 | bsz 1 | num_updates 16157 | best_loss 6.936
2022-03-07 03:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16157 updates
2022-03-07 03:09:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:09:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 83 @ 16157 updates, score 9.605) (writing took 3.3197194719687104 seconds)
2022-03-07 03:09:55 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-07 03:09:55 | INFO | train | epoch 083 | loss 2.799 | nll_loss 2.524 | ppl 5.75 | wps 20532.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16157 | lr 0.000248782 | gnorm 1.018 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 52104
2022-03-07 03:09:55 | INFO | fairseq.trainer | begin training epoch 84
2022-03-07 03:09:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:12:10 | INFO | train_inner | epoch 084:     43 / 196 loss=2.789, nll_loss=2.514, ppl=5.71, wps=20359.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.009, loss_scale=32, train_wall=290, gb_free=19.9, wall=52239
2022-03-07 03:12:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:17:26 | INFO | train_inner | epoch 084:    144 / 196 loss=2.79, nll_loss=2.514, ppl=5.71, wps=20727.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.001, loss_scale=16, train_wall=294, gb_free=19.9, wall=52555
2022-03-07 03:18:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:20:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:20:13 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.676 | nll_loss 9.474 | ppl 711.34 | wps 41383.2 | wpb 510.9 | bsz 1 | num_updates 16351 | best_loss 6.936
2022-03-07 03:20:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16351 updates
2022-03-07 03:20:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:20:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:20:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 84 @ 16351 updates, score 9.676) (writing took 3.3779859081842005 seconds)
2022-03-07 03:20:17 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-07 03:20:17 | INFO | train | epoch 084 | loss 2.783 | nll_loss 2.508 | ppl 5.69 | wps 20428.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16351 | lr 0.000247302 | gnorm 1.007 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 52726
2022-03-07 03:20:17 | INFO | fairseq.trainer | begin training epoch 85
2022-03-07 03:20:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:22:50 | INFO | train_inner | epoch 085:     49 / 196 loss=2.776, nll_loss=2.5, ppl=5.66, wps=20173.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.02, loss_scale=16, train_wall=293, gb_free=19.9, wall=52879
2022-03-07 03:24:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:28:06 | INFO | train_inner | epoch 085:    150 / 196 loss=2.777, nll_loss=2.501, ppl=5.66, wps=20731.8, ups=0.32, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.021, loss_scale=8, train_wall=294, gb_free=19.9, wall=53195
2022-03-07 03:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:30:35 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.682 | nll_loss 9.48 | ppl 714.22 | wps 41359.1 | wpb 510.9 | bsz 1 | num_updates 16546 | best_loss 6.936
2022-03-07 03:30:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16546 updates
2022-03-07 03:30:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:30:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 85 @ 16546 updates, score 9.682) (writing took 3.3791741027962416 seconds)
2022-03-07 03:30:38 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-07 03:30:38 | INFO | train | epoch 085 | loss 2.77 | nll_loss 2.494 | ppl 5.63 | wps 20536.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16546 | lr 0.000245841 | gnorm 1.012 | loss_scale 8 | train_wall 569 | gb_free 19.9 | wall 53347
2022-03-07 03:30:38 | INFO | fairseq.trainer | begin training epoch 86
2022-03-07 03:30:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:33:27 | INFO | train_inner | epoch 086:     54 / 196 loss=2.75, nll_loss=2.474, ppl=5.56, wps=20359.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=0.999, loss_scale=16, train_wall=290, gb_free=19.9, wall=53516
2022-03-07 03:38:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:38:43 | INFO | train_inner | epoch 086:    155 / 196 loss=2.765, nll_loss=2.489, ppl=5.62, wps=20724.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.02, loss_scale=16, train_wall=294, gb_free=19.9, wall=53832
2022-03-07 03:40:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:40:56 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.725 | nll_loss 9.524 | ppl 736.18 | wps 41514.1 | wpb 510.9 | bsz 1 | num_updates 16741 | best_loss 6.936
2022-03-07 03:40:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16741 updates
2022-03-07 03:40:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:41:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 86 @ 16741 updates, score 9.725) (writing took 3.357987156137824 seconds)
2022-03-07 03:41:00 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-07 03:41:00 | INFO | train | epoch 086 | loss 2.755 | nll_loss 2.479 | ppl 5.57 | wps 20528.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16741 | lr 0.000244405 | gnorm 1.01 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 53969
2022-03-07 03:41:00 | INFO | fairseq.trainer | begin training epoch 87
2022-03-07 03:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:44:04 | INFO | train_inner | epoch 087:     59 / 196 loss=2.741, nll_loss=2.464, ppl=5.52, wps=20359.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.009, loss_scale=16, train_wall=290, gb_free=19.9, wall=54153
2022-03-07 03:45:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:49:21 | INFO | train_inner | epoch 087:    160 / 196 loss=2.749, nll_loss=2.473, ppl=5.55, wps=20729.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.029, loss_scale=16, train_wall=294, gb_free=19.9, wall=54470
2022-03-07 03:51:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:51:18 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.722 | nll_loss 9.522 | ppl 735.44 | wps 41417.9 | wpb 510.9 | bsz 1 | num_updates 16936 | best_loss 6.936
2022-03-07 03:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16936 updates
2022-03-07 03:51:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 03:51:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 87 @ 16936 updates, score 9.722) (writing took 3.3352761429268867 seconds)
2022-03-07 03:51:21 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-07 03:51:21 | INFO | train | epoch 087 | loss 2.742 | nll_loss 2.465 | ppl 5.52 | wps 20537.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16936 | lr 0.000242993 | gnorm 1.022 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 54590
2022-03-07 03:51:21 | INFO | fairseq.trainer | begin training epoch 88
2022-03-07 03:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:54:45 | INFO | train_inner | epoch 088:     65 / 196 loss=2.722, nll_loss=2.444, ppl=5.44, wps=20178.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.023, loss_scale=16, train_wall=293, gb_free=19.9, wall=54793
2022-03-07 03:59:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:00:01 | INFO | train_inner | epoch 088:    166 / 196 loss=2.741, nll_loss=2.464, ppl=5.52, wps=20737.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.021, loss_scale=16, train_wall=294, gb_free=19.9, wall=55110
2022-03-07 04:01:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:01:39 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.774 | nll_loss 9.571 | ppl 760.63 | wps 41252.2 | wpb 510.9 | bsz 1 | num_updates 17130 | best_loss 6.936
2022-03-07 04:01:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17130 updates
2022-03-07 04:01:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:01:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:01:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 88 @ 17130 updates, score 9.774) (writing took 3.3331870809197426 seconds)
2022-03-07 04:01:42 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-07 04:01:42 | INFO | train | epoch 088 | loss 2.727 | nll_loss 2.45 | ppl 5.46 | wps 20439.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17130 | lr 0.000241614 | gnorm 1.022 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 55211
2022-03-07 04:01:42 | INFO | fairseq.trainer | begin training epoch 89
2022-03-07 04:01:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:05:21 | INFO | train_inner | epoch 089:     70 / 196 loss=2.702, nll_loss=2.425, ppl=5.37, wps=20367.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.016, loss_scale=16, train_wall=290, gb_free=19.9, wall=55430
2022-03-07 04:07:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:10:38 | INFO | train_inner | epoch 089:    171 / 196 loss=2.73, nll_loss=2.453, ppl=5.48, wps=20737, ups=0.32, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.03, loss_scale=16, train_wall=294, gb_free=19.9, wall=55746
2022-03-07 04:11:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:11:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:12:00 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.799 | nll_loss 9.597 | ppl 774.24 | wps 41149.5 | wpb 510.9 | bsz 1 | num_updates 17324 | best_loss 6.936
2022-03-07 04:12:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17324 updates
2022-03-07 04:12:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:12:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 89 @ 17324 updates, score 9.799) (writing took 3.3739599650725722 seconds)
2022-03-07 04:12:04 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-07 04:12:04 | INFO | train | epoch 089 | loss 2.714 | nll_loss 2.436 | ppl 5.41 | wps 20434.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17324 | lr 0.000240257 | gnorm 1.027 | loss_scale 8 | train_wall 569 | gb_free 19.9 | wall 55833
2022-03-07 04:12:04 | INFO | fairseq.trainer | begin training epoch 90
2022-03-07 04:12:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:16:02 | INFO | train_inner | epoch 090:     76 / 196 loss=2.688, nll_loss=2.41, ppl=5.31, wps=20168.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.031, loss_scale=8, train_wall=293, gb_free=19.9, wall=56071
2022-03-07 04:21:15 | INFO | train_inner | epoch 090:    176 / 196 loss=2.728, nll_loss=2.451, ppl=5.47, wps=20939.6, ups=0.32, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.024, loss_scale=16, train_wall=291, gb_free=19.9, wall=56384
2022-03-07 04:22:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:22:22 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.797 | nll_loss 9.595 | ppl 773.42 | wps 41055.6 | wpb 510.9 | bsz 1 | num_updates 17520 | best_loss 6.936
2022-03-07 04:22:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17520 updates
2022-03-07 04:22:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:22:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:22:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 90 @ 17520 updates, score 9.797) (writing took 3.4601926130708307 seconds)
2022-03-07 04:22:25 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-07 04:22:25 | INFO | train | epoch 090 | loss 2.703 | nll_loss 2.425 | ppl 5.37 | wps 20637.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 17520 | lr 0.000238909 | gnorm 1.022 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 56454
2022-03-07 04:22:25 | INFO | fairseq.trainer | begin training epoch 91
2022-03-07 04:22:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:24:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:26:39 | INFO | train_inner | epoch 091:     81 / 196 loss=2.667, nll_loss=2.388, ppl=5.23, wps=20162.1, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.023, loss_scale=16, train_wall=293, gb_free=19.9, wall=56708
2022-03-07 04:31:52 | INFO | train_inner | epoch 091:    181 / 196 loss=2.719, nll_loss=2.442, ppl=5.43, wps=20937.9, ups=0.32, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.033, loss_scale=32, train_wall=291, gb_free=19.9, wall=57021
2022-03-07 04:32:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:32:43 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.842 | nll_loss 9.641 | ppl 798.14 | wps 41270.5 | wpb 510.9 | bsz 1 | num_updates 17714 | best_loss 6.936
2022-03-07 04:32:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17714 updates
2022-03-07 04:32:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:32:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 91 @ 17714 updates, score 9.842) (writing took 3.42066811514087 seconds)
2022-03-07 04:32:47 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-07 04:32:47 | INFO | train | epoch 091 | loss 2.69 | nll_loss 2.411 | ppl 5.32 | wps 20431.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17714 | lr 0.000237597 | gnorm 1.03 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 57076
2022-03-07 04:32:47 | INFO | fairseq.trainer | begin training epoch 92
2022-03-07 04:32:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:37:16 | INFO | train_inner | epoch 092:     86 / 196 loss=2.646, nll_loss=2.367, ppl=5.16, wps=20163.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.029, loss_scale=16, train_wall=293, gb_free=19.9, wall=57345
2022-03-07 04:39:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:42:32 | INFO | train_inner | epoch 092:    187 / 196 loss=2.712, nll_loss=2.434, ppl=5.41, wps=20735.1, ups=0.32, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.029, loss_scale=16, train_wall=294, gb_free=19.9, wall=57661
2022-03-07 04:42:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:43:05 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.878 | nll_loss 9.676 | ppl 817.77 | wps 41070.2 | wpb 510.9 | bsz 1 | num_updates 17909 | best_loss 6.936
2022-03-07 04:43:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17909 updates
2022-03-07 04:43:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:43:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:43:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 92 @ 17909 updates, score 9.878) (writing took 3.5097760770004243 seconds)
2022-03-07 04:43:08 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-07 04:43:08 | INFO | train | epoch 092 | loss 2.677 | nll_loss 2.398 | ppl 5.27 | wps 20531.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17909 | lr 0.0002363 | gnorm 1.03 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 57697
2022-03-07 04:43:08 | INFO | fairseq.trainer | begin training epoch 93
2022-03-07 04:43:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:46:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:47:56 | INFO | train_inner | epoch 093:     92 / 196 loss=2.629, nll_loss=2.349, ppl=5.09, wps=20162.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.018, loss_scale=16, train_wall=293, gb_free=19.9, wall=57985
2022-03-07 04:53:09 | INFO | train_inner | epoch 093:    192 / 196 loss=2.706, nll_loss=2.428, ppl=5.38, wps=20945.4, ups=0.32, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.051, loss_scale=16, train_wall=291, gb_free=19.9, wall=58298
2022-03-07 04:53:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:53:26 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.914 | nll_loss 9.712 | ppl 838.8 | wps 41039.8 | wpb 510.9 | bsz 1 | num_updates 18104 | best_loss 6.936
2022-03-07 04:53:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18104 updates
2022-03-07 04:53:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:53:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 04:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 93 @ 18104 updates, score 9.914) (writing took 3.541703494032845 seconds)
2022-03-07 04:53:30 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-07 04:53:30 | INFO | train | epoch 093 | loss 2.666 | nll_loss 2.387 | ppl 5.23 | wps 20533.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18104 | lr 0.000235024 | gnorm 1.035 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 58319
2022-03-07 04:53:30 | INFO | fairseq.trainer | begin training epoch 94
2022-03-07 04:53:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:53:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:58:33 | INFO | train_inner | epoch 094:     97 / 196 loss=2.615, nll_loss=2.335, ppl=5.04, wps=20151.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.018, loss_scale=16, train_wall=293, gb_free=19.9, wall=58622
2022-03-07 05:01:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:03:48 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.944 | nll_loss 9.742 | ppl 856.17 | wps 41105.6 | wpb 510.9 | bsz 1 | num_updates 18298 | best_loss 6.936
2022-03-07 05:03:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18298 updates
2022-03-07 05:03:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:03:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:03:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 94 @ 18298 updates, score 9.944) (writing took 3.5595337089616805 seconds)
2022-03-07 05:03:51 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-07 05:03:51 | INFO | train | epoch 094 | loss 2.653 | nll_loss 2.373 | ppl 5.18 | wps 20425.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18298 | lr 0.000233775 | gnorm 1.036 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 58940
2022-03-07 05:03:51 | INFO | fairseq.trainer | begin training epoch 95
2022-03-07 05:03:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:03:58 | INFO | train_inner | epoch 095:      2 / 196 loss=2.693, nll_loss=2.415, ppl=5.33, wps=20152.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=18300, lr=0.000233762, gnorm=1.056, loss_scale=16, train_wall=293, gb_free=19.9, wall=58947
2022-03-07 05:08:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:09:14 | INFO | train_inner | epoch 095:    103 / 196 loss=2.61, nll_loss=2.329, ppl=5.02, wps=20745.3, ups=0.32, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=1.029, loss_scale=16, train_wall=294, gb_free=19.9, wall=59263
2022-03-07 05:14:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:14:09 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.965 | nll_loss 9.765 | ppl 870.02 | wps 41116.1 | wpb 510.9 | bsz 1 | num_updates 18493 | best_loss 6.936
2022-03-07 05:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18493 updates
2022-03-07 05:14:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:14:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:14:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 95 @ 18493 updates, score 9.965) (writing took 3.5322693181224167 seconds)
2022-03-07 05:14:13 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-07 05:14:13 | INFO | train | epoch 095 | loss 2.642 | nll_loss 2.362 | ppl 5.14 | wps 20537.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18493 | lr 0.000232539 | gnorm 1.038 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 59562
2022-03-07 05:14:13 | INFO | fairseq.trainer | begin training epoch 96
2022-03-07 05:14:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:14:35 | INFO | train_inner | epoch 096:      7 / 196 loss=2.667, nll_loss=2.388, ppl=5.23, wps=20352.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.046, loss_scale=16, train_wall=290, gb_free=19.9, wall=59584
2022-03-07 05:15:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:19:51 | INFO | train_inner | epoch 096:    108 / 196 loss=2.6, nll_loss=2.319, ppl=4.99, wps=20736.8, ups=0.32, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.026, loss_scale=16, train_wall=294, gb_free=19.9, wall=59900
2022-03-07 05:22:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:24:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:24:31 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.011 | nll_loss 9.811 | ppl 898 | wps 41284.8 | wpb 510.9 | bsz 1 | num_updates 18687 | best_loss 6.936
2022-03-07 05:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18687 updates
2022-03-07 05:24:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:24:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 96 @ 18687 updates, score 10.011) (writing took 3.5456019178964198 seconds)
2022-03-07 05:24:34 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-07 05:24:34 | INFO | train | epoch 096 | loss 2.629 | nll_loss 2.349 | ppl 5.09 | wps 20425.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18687 | lr 0.000231329 | gnorm 1.04 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 60183
2022-03-07 05:24:34 | INFO | fairseq.trainer | begin training epoch 97
2022-03-07 05:24:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:25:15 | INFO | train_inner | epoch 097:     13 / 196 loss=2.654, nll_loss=2.374, ppl=5.19, wps=20158, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.053, loss_scale=16, train_wall=293, gb_free=19.9, wall=60224
2022-03-07 05:30:28 | INFO | train_inner | epoch 097:    113 / 196 loss=2.594, nll_loss=2.313, ppl=4.97, wps=20940.4, ups=0.32, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.035, loss_scale=32, train_wall=291, gb_free=19.9, wall=60537
2022-03-07 05:30:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:34:52 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.025 | nll_loss 9.823 | ppl 905.76 | wps 41200.5 | wpb 510.9 | bsz 1 | num_updates 18882 | best_loss 6.936
2022-03-07 05:34:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18882 updates
2022-03-07 05:34:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:34:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:34:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 97 @ 18882 updates, score 10.025) (writing took 3.5236693769693375 seconds)
2022-03-07 05:34:56 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-07 05:34:56 | INFO | train | epoch 097 | loss 2.62 | nll_loss 2.339 | ppl 5.06 | wps 20535.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18882 | lr 0.000230131 | gnorm 1.046 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 60805
2022-03-07 05:34:56 | INFO | fairseq.trainer | begin training epoch 98
2022-03-07 05:34:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:35:52 | INFO | train_inner | epoch 098:     18 / 196 loss=2.637, nll_loss=2.357, ppl=5.12, wps=20162.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.054, loss_scale=16, train_wall=293, gb_free=19.9, wall=60861
2022-03-07 05:37:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:41:08 | INFO | train_inner | epoch 098:    119 / 196 loss=2.582, nll_loss=2.301, ppl=4.93, wps=20735.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.052, loss_scale=16, train_wall=294, gb_free=19.9, wall=61177
2022-03-07 05:45:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:45:14 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.031 | nll_loss 9.83 | ppl 909.97 | wps 41255.3 | wpb 510.9 | bsz 1 | num_updates 19077 | best_loss 6.936
2022-03-07 05:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19077 updates
2022-03-07 05:45:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:45:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:45:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 98 @ 19077 updates, score 10.031) (writing took 3.536654338007793 seconds)
2022-03-07 05:45:18 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-07 05:45:18 | INFO | train | epoch 098 | loss 2.607 | nll_loss 2.327 | ppl 5.02 | wps 20531 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19077 | lr 0.000228952 | gnorm 1.046 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 61426
2022-03-07 05:45:18 | INFO | fairseq.trainer | begin training epoch 99
2022-03-07 05:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:46:30 | INFO | train_inner | epoch 099:     23 / 196 loss=2.632, nll_loss=2.352, ppl=5.1, wps=20348.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.041, loss_scale=32, train_wall=290, gb_free=19.9, wall=61499
2022-03-07 05:46:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:51:46 | INFO | train_inner | epoch 099:    124 / 196 loss=2.575, nll_loss=2.293, ppl=4.9, wps=20736, ups=0.32, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.04, loss_scale=16, train_wall=294, gb_free=19.9, wall=61815
2022-03-07 05:53:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:55:35 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.084 | nll_loss 9.886 | ppl 946.04 | wps 41011 | wpb 510.9 | bsz 1 | num_updates 19271 | best_loss 6.936
2022-03-07 05:55:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19271 updates
2022-03-07 05:55:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 05:55:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 99 @ 19271 updates, score 10.084) (writing took 3.4931830710265785 seconds)
2022-03-07 05:55:39 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-07 05:55:39 | INFO | train | epoch 099 | loss 2.597 | nll_loss 2.316 | ppl 4.98 | wps 20430.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19271 | lr 0.000227797 | gnorm 1.042 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 62048
2022-03-07 05:55:39 | INFO | fairseq.trainer | begin training epoch 100
2022-03-07 05:55:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:57:10 | INFO | train_inner | epoch 100:     29 / 196 loss=2.613, nll_loss=2.333, ppl=5.04, wps=20162.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.047, loss_scale=16, train_wall=293, gb_free=19.9, wall=62139
2022-03-07 06:01:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:02:26 | INFO | train_inner | epoch 100:    130 / 196 loss=2.571, nll_loss=2.289, ppl=4.89, wps=20725.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.043, loss_scale=16, train_wall=294, gb_free=19.9, wall=62455
2022-03-07 06:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:05:57 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.071 | nll_loss 9.87 | ppl 935.9 | wps 41070.1 | wpb 510.9 | bsz 1 | num_updates 19466 | best_loss 6.936
2022-03-07 06:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19466 updates
2022-03-07 06:05:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:06:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:06:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 100 @ 19466 updates, score 10.071) (writing took 3.5189393339678645 seconds)
2022-03-07 06:06:01 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-07 06:06:01 | INFO | train | epoch 100 | loss 2.586 | nll_loss 2.305 | ppl 4.94 | wps 20529.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19466 | lr 0.000226653 | gnorm 1.044 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 62670
2022-03-07 06:06:01 | INFO | fairseq.trainer | begin training epoch 101
2022-03-07 06:06:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:07:47 | INFO | train_inner | epoch 101:     34 / 196 loss=2.594, nll_loss=2.313, ppl=4.97, wps=20355.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.055, loss_scale=16, train_wall=290, gb_free=19.9, wall=62776
2022-03-07 06:09:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:13:03 | INFO | train_inner | epoch 101:    135 / 196 loss=2.569, nll_loss=2.287, ppl=4.88, wps=20734.7, ups=0.32, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.049, loss_scale=16, train_wall=294, gb_free=19.9, wall=63092
2022-03-07 06:16:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:16:19 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.119 | nll_loss 9.919 | ppl 967.9 | wps 41048.8 | wpb 510.9 | bsz 1 | num_updates 19661 | best_loss 6.936
2022-03-07 06:16:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19661 updates
2022-03-07 06:16:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 101 @ 19661 updates, score 10.119) (writing took 3.5475571528077126 seconds)
2022-03-07 06:16:22 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-07 06:16:22 | INFO | train | epoch 101 | loss 2.576 | nll_loss 2.294 | ppl 4.9 | wps 20531.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19661 | lr 0.000225526 | gnorm 1.051 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 63291
2022-03-07 06:16:22 | INFO | fairseq.trainer | begin training epoch 102
2022-03-07 06:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:17:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:18:27 | INFO | train_inner | epoch 102:     40 / 196 loss=2.57, nll_loss=2.288, ppl=4.88, wps=20160.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.046, loss_scale=16, train_wall=293, gb_free=19.9, wall=63416
2022-03-07 06:23:40 | INFO | train_inner | epoch 102:    140 / 196 loss=2.564, nll_loss=2.282, ppl=4.86, wps=20941, ups=0.32, wpb=65532.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.057, loss_scale=16, train_wall=291, gb_free=19.9, wall=63729
2022-03-07 06:23:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:26:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:26:40 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.133 | nll_loss 9.929 | ppl 974.94 | wps 41240.1 | wpb 510.9 | bsz 1 | num_updates 19855 | best_loss 6.936
2022-03-07 06:26:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19855 updates
2022-03-07 06:26:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:26:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 102 @ 19855 updates, score 10.133) (writing took 3.508289312943816 seconds)
2022-03-07 06:26:44 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-07 06:26:44 | INFO | train | epoch 102 | loss 2.566 | nll_loss 2.284 | ppl 4.87 | wps 20432.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19855 | lr 0.000224422 | gnorm 1.053 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 63913
2022-03-07 06:26:44 | INFO | fairseq.trainer | begin training epoch 103
2022-03-07 06:26:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:29:04 | INFO | train_inner | epoch 103:     45 / 196 loss=2.569, nll_loss=2.287, ppl=4.88, wps=20168, ups=0.31, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.046, loss_scale=16, train_wall=293, gb_free=19.9, wall=64053
2022-03-07 06:30:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:34:21 | INFO | train_inner | epoch 103:    146 / 196 loss=2.557, nll_loss=2.275, ppl=4.84, wps=20733.4, ups=0.32, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.051, loss_scale=16, train_wall=294, gb_free=19.9, wall=64369
2022-03-07 06:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:37:02 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.144 | nll_loss 9.943 | ppl 984.15 | wps 41339.7 | wpb 510.9 | bsz 1 | num_updates 20050 | best_loss 6.936
2022-03-07 06:37:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20050 updates
2022-03-07 06:37:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:37:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 103 @ 20050 updates, score 10.144) (writing took 3.50582433398813 seconds)
2022-03-07 06:37:05 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-07 06:37:05 | INFO | train | epoch 103 | loss 2.555 | nll_loss 2.273 | ppl 4.83 | wps 20536.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20050 | lr 0.000223328 | gnorm 1.051 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 64534
2022-03-07 06:37:05 | INFO | fairseq.trainer | begin training epoch 104
2022-03-07 06:37:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:39:45 | INFO | train_inner | epoch 104:     51 / 196 loss=2.537, nll_loss=2.254, ppl=4.77, wps=20159.7, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.047, loss_scale=16, train_wall=293, gb_free=19.9, wall=64694
2022-03-07 06:44:58 | INFO | train_inner | epoch 104:    151 / 196 loss=2.551, nll_loss=2.268, ppl=4.82, wps=20941.1, ups=0.32, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.06, loss_scale=16, train_wall=291, gb_free=19.9, wall=65007
2022-03-07 06:45:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:47:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:47:23 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.209 | nll_loss 10.01 | ppl 1031.23 | wps 41249.7 | wpb 510.9 | bsz 1 | num_updates 20244 | best_loss 6.936
2022-03-07 06:47:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20244 updates
2022-03-07 06:47:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:47:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt
2022-03-07 06:47:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#2/checkpoint_last.pt (epoch 104 @ 20244 updates, score 10.209) (writing took 3.5036020069383085 seconds)
2022-03-07 06:47:27 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-07 06:47:27 | INFO | train | epoch 104 | loss 2.546 | nll_loss 2.263 | ppl 4.8 | wps 20428.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20244 | lr 0.000222255 | gnorm 1.053 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 65156
2022-03-07 06:47:27 | INFO | fairseq.trainer | begin training epoch 105
2022-03-07 06:47:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:50:22 | INFO | train_inner | epoch 105:     56 / 196 loss=2.536, nll_loss=2.253, ppl=4.77, wps=20163.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.058, loss_scale=16, train_wall=293, gb_free=19.9, wall=65331
2022-03-07 06:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
