Sender: LSF System <lsfadmin@eu-g3-072>
Subject: Job 207346227: <w103_fp16_size_0.03125_jelinek_0.02_0.06_0.92_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.03125_jelinek_0.02_0.06_0.92_#1> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 13:28:37 2022
Job was executed on host(s) <eu-g3-072>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Sun Mar  6 16:18:26 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 16:18:26 2022
Terminated at Sun Mar  6 20:38:52 2022
Results reported at Sun Mar  6 20:38:52 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.03125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.02, 0.06, 0.92)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --no-epoch-checkpoints --no-last-checkpoints --seed 66575621 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   15614.39 sec.
    Max Memory :                                 5778 MB
    Average Memory :                             2995.47 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14222.00 MB
    Max Swap :                                   842 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   15626 sec.
    Turnaround time :                            25815 sec.

The output (if any) follows:

2022-03-06 16:18:33 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575621, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.03125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575621, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02, 0.06, 0.92)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 16:18:33 | INFO | fairseq.tasks.language_modeling | dictionary: 96056 types
2022-03-06 16:18:34 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
Calculating frequency stats:
  0%|          | 0/56292 [00:00<?, ?it/s]  1%|          | 697/56292 [00:00<00:07, 6966.44it/s]  2%|▏         | 1394/56292 [00:00<00:08, 6242.20it/s]  4%|▎         | 2024/56292 [00:00<00:09, 5847.22it/s]  5%|▍         | 2649/56292 [00:00<00:08, 5991.81it/s]  6%|▌         | 3307/56292 [00:00<00:08, 6190.97it/s]  7%|▋         | 3980/56292 [00:00<00:08, 6356.35it/s]  8%|▊         | 4704/56292 [00:00<00:07, 6636.46it/s] 10%|▉         | 5479/56292 [00:00<00:07, 6982.73it/s] 11%|█         | 6180/56292 [00:00<00:07, 6930.94it/s] 12%|█▏        | 6875/56292 [00:01<00:07, 6291.67it/s] 13%|█▎        | 7542/56292 [00:01<00:07, 6396.32it/s] 15%|█▍        | 8191/56292 [00:01<00:07, 6208.87it/s] 16%|█▌        | 8819/56292 [00:01<00:07, 6136.43it/s] 17%|█▋        | 9460/56292 [00:01<00:07, 6209.87it/s] 18%|█▊        | 10123/56292 [00:01<00:07, 6328.76it/s] 19%|█▉        | 10759/56292 [00:01<00:07, 6210.96it/s] 20%|██        | 11383/56292 [00:01<00:07, 6145.32it/s] 21%|██▏       | 12084/56292 [00:01<00:06, 6394.41it/s] 23%|██▎       | 12726/56292 [00:02<00:07, 6176.65it/s] 24%|██▎       | 13368/56292 [00:02<00:06, 6244.99it/s] 25%|██▍       | 14038/56292 [00:02<00:06, 6377.23it/s] 26%|██▌       | 14686/56292 [00:02<00:06, 6403.40it/s] 27%|██▋       | 15351/56292 [00:02<00:06, 6476.00it/s] 28%|██▊       | 16000/56292 [00:02<00:06, 6186.04it/s] 30%|██▉       | 16622/56292 [00:02<00:06, 6130.98it/s] 31%|███       | 17285/56292 [00:02<00:06, 6265.36it/s] 32%|███▏      | 17936/56292 [00:02<00:06, 6334.86it/s] 33%|███▎      | 18572/56292 [00:02<00:06, 6153.18it/s] 35%|███▍      | 19432/56292 [00:03<00:05, 6859.56it/s] 36%|███▌      | 20122/56292 [00:03<00:05, 6514.58it/s] 37%|███▋      | 20780/56292 [00:03<00:05, 6373.56it/s] 38%|███▊      | 21422/56292 [00:03<00:05, 6094.36it/s] 39%|███▉      | 22078/56292 [00:03<00:05, 6222.52it/s] 40%|████      | 22705/56292 [00:03<00:05, 6233.71it/s] 41%|████▏     | 23332/56292 [00:03<00:05, 6226.08it/s] 43%|████▎     | 24137/56292 [00:03<00:04, 6754.46it/s] 44%|████▍     | 24873/56292 [00:03<00:04, 6931.79it/s] 45%|████▌     | 25569/56292 [00:03<00:04, 6927.57it/s] 47%|████▋     | 26264/56292 [00:04<00:04, 6506.55it/s] 48%|████▊     | 26921/56292 [00:04<00:04, 6284.23it/s] 49%|████▉     | 27555/56292 [00:04<00:04, 6073.84it/s] 50%|█████     | 28211/56292 [00:04<00:04, 6207.13it/s] 51%|█████▏    | 28904/56292 [00:04<00:04, 6410.67it/s] 52%|█████▏    | 29549/56292 [00:04<00:04, 6217.22it/s] 54%|█████▎    | 30184/56292 [00:04<00:04, 6250.87it/s] 55%|█████▍    | 30812/56292 [00:04<00:04, 5941.13it/s] 56%|█████▌    | 31437/56292 [00:04<00:04, 6022.33it/s] 57%|█████▋    | 32043/56292 [00:05<00:04, 5896.48it/s] 58%|█████▊    | 32665/56292 [00:05<00:03, 5986.08it/s] 59%|█████▉    | 33266/56292 [00:05<00:03, 5856.11it/s] 60%|██████    | 33919/56292 [00:05<00:03, 6049.16it/s] 62%|██████▏   | 34626/56292 [00:05<00:03, 6344.21it/s] 63%|██████▎   | 35263/56292 [00:05<00:03, 6210.67it/s] 64%|██████▍   | 35887/56292 [00:05<00:03, 6123.49it/s] 65%|██████▍   | 36533/56292 [00:05<00:03, 6220.68it/s] 66%|██████▌   | 37157/56292 [00:05<00:03, 6040.47it/s] 67%|██████▋   | 37763/56292 [00:06<00:03, 5894.32it/s] 68%|██████▊   | 38354/56292 [00:06<00:03, 5844.58it/s] 69%|██████▉   | 39011/56292 [00:06<00:02, 6053.42it/s] 70%|███████   | 39618/56292 [00:06<00:02, 6027.04it/s] 72%|███████▏  | 40285/56292 [00:06<00:02, 6206.99it/s] 73%|███████▎  | 40976/56292 [00:06<00:02, 6409.56it/s] 74%|███████▍  | 41618/56292 [00:06<00:02, 6235.26it/s] 75%|███████▌  | 42244/56292 [00:06<00:02, 5932.33it/s] 76%|███████▌  | 42841/56292 [00:06<00:02, 5928.40it/s] 77%|███████▋  | 43437/56292 [00:06<00:02, 5866.60it/s] 78%|███████▊  | 44114/56292 [00:07<00:01, 6127.30it/s] 80%|███████▉  | 44791/56292 [00:07<00:01, 6311.72it/s] 81%|████████  | 45425/56292 [00:07<00:01, 6283.88it/s] 82%|████████▏ | 46153/56292 [00:07<00:01, 6575.18it/s] 83%|████████▎ | 46812/56292 [00:07<00:01, 6499.44it/s] 85%|████████▍ | 47825/56292 [00:07<00:01, 7560.12it/s] 86%|████████▋ | 48584/56292 [00:07<00:01, 7297.26it/s] 88%|████████▊ | 49323/56292 [00:07<00:00, 7323.63it/s] 89%|████████▉ | 50058/56292 [00:07<00:00, 6996.57it/s] 90%|█████████ | 50762/56292 [00:08<00:00, 6539.43it/s] 91%|█████████▏| 51424/56292 [00:08<00:00, 6517.83it/s] 93%|█████████▎| 52157/56292 [00:08<00:00, 6745.93it/s] 94%|█████████▍| 52950/56292 [00:08<00:00, 7076.89it/s] 95%|█████████▌| 53663/56292 [00:08<00:00, 6704.46it/s] 97%|█████████▋| 54341/56292 [00:08<00:00, 6328.40it/s] 98%|█████████▊| 54982/56292 [00:08<00:00, 6229.58it/s] 99%|█████████▉| 55610/56292 [00:08<00:00, 6220.49it/s]100%|█████████▉| 56282/56292 [00:08<00:00, 6360.13it/s]100%|██████████| 56292/56292 [00:08<00:00, 6341.03it/s]

gathering stats for n=1
  0%|          | 0/56292 [00:00<?, ?it/s]  3%|▎         | 1891/56292 [00:00<00:02, 18885.39it/s]  7%|▋         | 3944/56292 [00:00<00:02, 19835.78it/s] 11%|█         | 6165/56292 [00:00<00:02, 20913.01it/s] 15%|█▍        | 8257/56292 [00:00<00:02, 19991.57it/s] 18%|█▊        | 10269/56292 [00:00<00:02, 20027.83it/s] 22%|██▏       | 12276/56292 [00:00<00:02, 19679.91it/s] 25%|██▌       | 14323/56292 [00:00<00:02, 19931.08it/s] 29%|██▉       | 16319/56292 [00:00<00:02, 19635.39it/s] 32%|███▏      | 18285/56292 [00:00<00:01, 19575.25it/s] 36%|███▌      | 20317/56292 [00:01<00:01, 19795.57it/s] 40%|███▉      | 22298/56292 [00:01<00:01, 19661.27it/s] 43%|████▎     | 24458/56292 [00:01<00:01, 20242.13it/s] 47%|████▋     | 26484/56292 [00:01<00:01, 20117.20it/s] 51%|█████     | 28497/56292 [00:01<00:01, 19955.41it/s] 54%|█████▍    | 30494/56292 [00:01<00:01, 19857.95it/s] 58%|█████▊    | 32481/56292 [00:01<00:01, 19407.73it/s] 61%|██████    | 34424/56292 [00:01<00:01, 19193.95it/s] 65%|██████▍   | 36405/56292 [00:01<00:01, 19369.25it/s] 68%|██████▊   | 38344/56292 [00:01<00:00, 18824.71it/s] 72%|███████▏  | 40300/56292 [00:02<00:00, 19028.55it/s] 75%|███████▍  | 42207/56292 [00:02<00:00, 18810.87it/s] 78%|███████▊  | 44117/56292 [00:02<00:00, 18892.55it/s] 82%|████████▏ | 46280/56292 [00:02<00:00, 19694.72it/s] 86%|████████▌ | 48541/56292 [00:02<00:00, 20553.04it/s] 90%|████████▉ | 50600/56292 [00:02<00:00, 20444.49it/s] 94%|█████████▍| 52792/56292 [00:02<00:00, 20881.27it/s] 97%|█████████▋| 54883/56292 [00:02<00:00, 20102.82it/s]100%|██████████| 56292/56292 [00:02<00:00, 19771.27it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 236.53it/s]2022-03-06 16:18:50 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(96056, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=96056, bias=False)
  )
)
2022-03-06 16:18:50 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 16:18:50 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 16:18:50 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-06 16:18:50 | INFO | fairseq_cli.train | num. shared model params: 68,094,976 (num. trained: 68,094,976)
2022-03-06 16:18:50 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 16:18:50 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.03125/valid
2022-03-06 16:18:51 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 16:18:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 16:18:51 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-06 16:18:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 16:18:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 16:18:51 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 16:18:51 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_last.pt
2022-03-06 16:18:51 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_last.pt
2022-03-06 16:18:51 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 16:18:51 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
2022-03-06 16:18:51 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 16:18:51 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-06 16:18:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 16:19:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:19:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:19:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 16:19:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 16:21:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:21:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.547 | ppl 47879.8 | wps 38776.3 | wpb 510.9 | bsz 1 | num_updates 44
2022-03-06 16:21:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 44 updates
2022-03-06 16:21:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:21:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:21:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 1 @ 44 updates, score 15.547) (writing took 1.9347104392945766 seconds)
2022-03-06 16:21:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 16:21:33 | INFO | train | epoch 001 | loss 16.771 | ppl 111822 | wps 21699.5 | ups 0.34 | wpb 64791.3 | bsz 126.5 | num_updates 44 | lr 5.5989e-06 | gnorm 5.344 | loss_scale 4 | train_wall 144 | gb_free 21.5 | wall 163
2022-03-06 16:21:33 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 16:21:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:23:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:23:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.952 | ppl 15852.9 | wps 40394.6 | wpb 510.9 | bsz 1 | num_updates 93 | best_loss 13.952
2022-03-06 16:23:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 93 updates
2022-03-06 16:23:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:23:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:23:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 2 @ 93 updates, score 13.952) (writing took 1.971434230916202 seconds)
2022-03-06 16:23:58 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 16:23:58 | INFO | train | epoch 002 | loss 14.674 | ppl 26136.4 | wps 21915.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 93 | lr 1.17227e-05 | gnorm 2.323 | loss_scale 4 | train_wall 127 | gb_free 21.5 | wall 308
2022-03-06 16:23:58 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 16:23:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:24:19 | INFO | train_inner | epoch 003:      7 / 49 loss=15.547, ppl=47875, wps=21885.5, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=100, lr=1.25975e-05, gnorm=3.601, loss_scale=4, train_wall=289, gb_free=21.5, wall=328
2022-03-06 16:26:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:26:22 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.293 | ppl 10037 | wps 38964 | wpb 510.9 | bsz 1 | num_updates 142 | best_loss 13.293
2022-03-06 16:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 142 updates
2022-03-06 16:26:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:26:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 3 @ 142 updates, score 13.293) (writing took 1.9471256826072931 seconds)
2022-03-06 16:26:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 16:26:24 | INFO | train | epoch 003 | loss 13.71 | ppl 13397.1 | wps 21803.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 142 | lr 1.78465e-05 | gnorm 1.507 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 454
2022-03-06 16:26:24 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 16:26:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:28:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:28:47 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.485 | ppl 5732.91 | wps 40462.9 | wpb 510.9 | bsz 1 | num_updates 191 | best_loss 12.485
2022-03-06 16:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 191 updates
2022-03-06 16:28:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:28:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:28:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 4 @ 191 updates, score 12.485) (writing took 1.9334904300048947 seconds)
2022-03-06 16:28:49 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 16:28:49 | INFO | train | epoch 004 | loss 12.981 | ppl 8087.09 | wps 22004.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 191 | lr 2.39702e-05 | gnorm 1.322 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 598
2022-03-06 16:28:49 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 16:28:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:29:14 | INFO | train_inner | epoch 005:      9 / 49 loss=13.219, ppl=9531.82, wps=21943.3, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=200, lr=2.5095e-05, gnorm=1.377, loss_scale=8, train_wall=258, gb_free=21.5, wall=624
2022-03-06 16:31:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:31:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.663 | ppl 3243.14 | wps 39069.5 | wpb 510.9 | bsz 1 | num_updates 240 | best_loss 11.663
2022-03-06 16:31:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 240 updates
2022-03-06 16:31:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:31:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 5 @ 240 updates, score 11.663) (writing took 1.9938129214569926 seconds)
2022-03-06 16:31:14 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 16:31:14 | INFO | train | epoch 005 | loss 12.102 | ppl 4396.18 | wps 21825.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 240 | lr 3.0094e-05 | gnorm 1.028 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 744
2022-03-06 16:31:14 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 16:31:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:33:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:33:38 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.045 | ppl 2112.36 | wps 39019.4 | wpb 510.9 | bsz 1 | num_updates 289 | best_loss 11.045
2022-03-06 16:33:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 289 updates
2022-03-06 16:33:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:33:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:33:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 6 @ 289 updates, score 11.045) (writing took 1.9615873452275991 seconds)
2022-03-06 16:33:39 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 16:33:39 | INFO | train | epoch 006 | loss 11.338 | ppl 2587.84 | wps 21888.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 289 | lr 3.62178e-05 | gnorm 0.756 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 889
2022-03-06 16:33:39 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 16:33:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:34:11 | INFO | train_inner | epoch 007:     11 / 49 loss=11.57, ppl=3041.25, wps=21858.7, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=300, lr=3.75925e-05, gnorm=0.845, loss_scale=16, train_wall=259, gb_free=21.5, wall=920
2022-03-06 16:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:36:03 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.672 | ppl 1631.77 | wps 40410.3 | wpb 510.9 | bsz 1 | num_updates 338 | best_loss 10.672
2022-03-06 16:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 338 updates
2022-03-06 16:36:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:36:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 7 @ 338 updates, score 10.672) (writing took 1.9779329234734178 seconds)
2022-03-06 16:36:05 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 16:36:05 | INFO | train | epoch 007 | loss 10.802 | ppl 1785.38 | wps 21851.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 338 | lr 4.23416e-05 | gnorm 0.561 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 1034
2022-03-06 16:36:05 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 16:36:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:38:28 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.453 | ppl 1402.01 | wps 39019.9 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.453
2022-03-06 16:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 387 updates
2022-03-06 16:38:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:38:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 8 @ 387 updates, score 10.453) (writing took 1.9460816094651818 seconds)
2022-03-06 16:38:30 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 16:38:30 | INFO | train | epoch 008 | loss 10.491 | ppl 1438.88 | wps 21859 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 387 | lr 4.84653e-05 | gnorm 0.474 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1180
2022-03-06 16:38:30 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 16:38:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:39:07 | INFO | train_inner | epoch 009:     13 / 49 loss=10.573, ppl=1523.8, wps=21891.3, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=400, lr=5.009e-05, gnorm=0.492, loss_scale=32, train_wall=259, gb_free=21.5, wall=1217
2022-03-06 16:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:40:53 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.303 | ppl 1263.21 | wps 39271.1 | wpb 510.9 | bsz 1 | num_updates 436 | best_loss 10.303
2022-03-06 16:40:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 436 updates
2022-03-06 16:40:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:40:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:40:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 9 @ 436 updates, score 10.303) (writing took 1.9677084255963564 seconds)
2022-03-06 16:40:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 16:40:55 | INFO | train | epoch 009 | loss 10.297 | ppl 1258.48 | wps 21938.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 436 | lr 5.45891e-05 | gnorm 0.435 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 1325
2022-03-06 16:40:55 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 16:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:43:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:43:20 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.165 | ppl 1148.33 | wps 37992.7 | wpb 510.9 | bsz 1 | num_updates 485 | best_loss 10.165
2022-03-06 16:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 485 updates
2022-03-06 16:43:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 10 @ 485 updates, score 10.165) (writing took 1.973771221935749 seconds)
2022-03-06 16:43:22 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 16:43:22 | INFO | train | epoch 010 | loss 10.142 | ppl 1129.96 | wps 21679.7 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 485 | lr 6.07129e-05 | gnorm 0.478 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1471
2022-03-06 16:43:22 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 16:43:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:44:05 | INFO | train_inner | epoch 011:     15 / 49 loss=10.173, ppl=1154.49, wps=21799.9, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=500, lr=6.25875e-05, gnorm=0.454, loss_scale=32, train_wall=260, gb_free=21.5, wall=1514
2022-03-06 16:44:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:45:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:45:46 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.038 | ppl 1051.21 | wps 38622.2 | wpb 510.9 | bsz 1 | num_updates 533 | best_loss 10.038
2022-03-06 16:45:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 533 updates
2022-03-06 16:45:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:45:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:45:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 11 @ 533 updates, score 10.038) (writing took 2.1880702124908566 seconds)
2022-03-06 16:45:49 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 16:45:49 | INFO | train | epoch 011 | loss 9.994 | ppl 1019.47 | wps 21185.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 533 | lr 6.67117e-05 | gnorm 0.509 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1618
2022-03-06 16:45:49 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 16:45:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:48:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:48:13 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.909 | ppl 961.63 | wps 38547.2 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.909
2022-03-06 16:48:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 582 updates
2022-03-06 16:48:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:48:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:48:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 12 @ 582 updates, score 9.909) (writing took 1.9231804255396128 seconds)
2022-03-06 16:48:15 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 16:48:15 | INFO | train | epoch 012 | loss 9.851 | ppl 923.76 | wps 21737.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 582 | lr 7.28355e-05 | gnorm 0.573 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1764
2022-03-06 16:48:15 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 16:48:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:49:06 | INFO | train_inner | epoch 013:     18 / 49 loss=9.871, ppl=936.62, wps=21522.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=600, lr=7.5085e-05, gnorm=0.557, loss_scale=32, train_wall=263, gb_free=21.5, wall=1816
2022-03-06 16:50:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:50:38 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.794 | ppl 887.48 | wps 40658.2 | wpb 510.9 | bsz 1 | num_updates 631 | best_loss 9.794
2022-03-06 16:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 631 updates
2022-03-06 16:50:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:50:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:50:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 13 @ 631 updates, score 9.794) (writing took 1.9314003512263298 seconds)
2022-03-06 16:50:40 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 16:50:40 | INFO | train | epoch 013 | loss 9.711 | ppl 837.97 | wps 21874.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 631 | lr 7.89592e-05 | gnorm 0.581 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1910
2022-03-06 16:50:40 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 16:50:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:51:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:52:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:53:03 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.679 | ppl 819.66 | wps 39837.2 | wpb 510.9 | bsz 1 | num_updates 679 | best_loss 9.679
2022-03-06 16:53:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 679 updates
2022-03-06 16:53:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:53:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 14 @ 679 updates, score 9.679) (writing took 1.8465143600478768 seconds)
2022-03-06 16:53:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 16:53:05 | INFO | train | epoch 014 | loss 9.578 | ppl 764.45 | wps 21505.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 679 | lr 8.4958e-05 | gnorm 0.61 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 2054
2022-03-06 16:53:05 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 16:53:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:54:05 | INFO | train_inner | epoch 015:     21 / 49 loss=9.593, ppl=772.26, wps=21739.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=700, lr=8.75825e-05, gnorm=0.641, loss_scale=32, train_wall=261, gb_free=21.5, wall=2114
2022-03-06 16:55:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:55:28 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.586 | ppl 768.3 | wps 40503.6 | wpb 510.9 | bsz 1 | num_updates 728 | best_loss 9.586
2022-03-06 16:55:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 728 updates
2022-03-06 16:55:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:55:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:55:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 15 @ 728 updates, score 9.586) (writing took 1.8673044862225652 seconds)
2022-03-06 16:55:29 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 16:55:29 | INFO | train | epoch 015 | loss 9.454 | ppl 701.39 | wps 21979.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 728 | lr 9.10818e-05 | gnorm 0.696 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 2199
2022-03-06 16:55:30 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 16:55:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:57:52 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.484 | ppl 716.29 | wps 40327.1 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.484
2022-03-06 16:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 777 updates
2022-03-06 16:57:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:57:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 16:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 16 @ 777 updates, score 9.484) (writing took 1.8959321500733495 seconds)
2022-03-06 16:57:54 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 16:57:54 | INFO | train | epoch 016 | loss 9.329 | ppl 643.34 | wps 22031.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 777 | lr 9.72056e-05 | gnorm 0.688 | loss_scale 64 | train_wall 126 | gb_free 21.5 | wall 2343
2022-03-06 16:57:54 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 16:57:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:58:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:59:02 | INFO | train_inner | epoch 017:     24 / 49 loss=9.336, ppl=646.29, wps=21845.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=800, lr=0.00010008, gnorm=0.699, loss_scale=32, train_wall=260, gb_free=21.5, wall=2411
2022-03-06 17:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:00:17 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.414 | ppl 682.12 | wps 38887.9 | wpb 510.9 | bsz 1 | num_updates 825 | best_loss 9.414
2022-03-06 17:00:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 825 updates
2022-03-06 17:00:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 17 @ 825 updates, score 9.414) (writing took 1.948059012182057 seconds)
2022-03-06 17:00:19 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 17:00:19 | INFO | train | epoch 017 | loss 9.212 | ppl 593.25 | wps 21458.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 825 | lr 0.000103204 | gnorm 0.8 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 2488
2022-03-06 17:00:19 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 17:00:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:02:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:02:43 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.328 | ppl 642.91 | wps 38941.2 | wpb 510.9 | bsz 1 | num_updates 874 | best_loss 9.328
2022-03-06 17:02:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 874 updates
2022-03-06 17:02:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:02:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:02:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 18 @ 874 updates, score 9.328) (writing took 1.9273642720654607 seconds)
2022-03-06 17:02:44 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 17:02:44 | INFO | train | epoch 018 | loss 9.097 | ppl 547.56 | wps 21814.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 874 | lr 0.000109328 | gnorm 0.813 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 2634
2022-03-06 17:02:44 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 17:02:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:03:59 | INFO | train_inner | epoch 019:     26 / 49 loss=9.095, ppl=546.78, wps=21843.2, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=900, lr=0.000112578, gnorm=0.815, loss_scale=32, train_wall=259, gb_free=21.5, wall=2708
2022-03-06 17:05:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:05:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:05:08 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.276 | ppl 619.76 | wps 40257.7 | wpb 510.9 | bsz 1 | num_updates 922 | best_loss 9.276
2022-03-06 17:05:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 922 updates
2022-03-06 17:05:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:05:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:05:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 19 @ 922 updates, score 9.276) (writing took 1.9225607011467218 seconds)
2022-03-06 17:05:10 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 17:05:10 | INFO | train | epoch 019 | loss 8.986 | ppl 507.08 | wps 21440.1 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 922 | lr 0.000115327 | gnorm 0.852 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 2779
2022-03-06 17:05:10 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 17:05:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:07:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:07:33 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.21 | ppl 592.09 | wps 38671.8 | wpb 510.9 | bsz 1 | num_updates 971 | best_loss 9.21
2022-03-06 17:07:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 971 updates
2022-03-06 17:07:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:07:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 20 @ 971 updates, score 9.21) (writing took 1.9472647728398442 seconds)
2022-03-06 17:07:35 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 17:07:35 | INFO | train | epoch 020 | loss 8.879 | ppl 470.85 | wps 21907.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 971 | lr 0.000121451 | gnorm 0.778 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 2924
2022-03-06 17:07:35 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 17:07:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:08:57 | INFO | train_inner | epoch 021:     29 / 49 loss=8.87, ppl=467.96, wps=21732.6, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1000, lr=0.000125075, gnorm=0.83, loss_scale=32, train_wall=261, gb_free=21.5, wall=3007
2022-03-06 17:09:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:09:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:09:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.147 | ppl 566.85 | wps 40485.1 | wpb 510.9 | bsz 1 | num_updates 1019 | best_loss 9.147
2022-03-06 17:09:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1019 updates
2022-03-06 17:09:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:10:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:10:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 21 @ 1019 updates, score 9.147) (writing took 1.8892783550545573 seconds)
2022-03-06 17:10:00 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 17:10:00 | INFO | train | epoch 021 | loss 8.779 | ppl 439.17 | wps 21461.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1019 | lr 0.00012745 | gnorm 0.891 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 3069
2022-03-06 17:10:00 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 17:10:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:12:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:12:22 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.089 | ppl 544.41 | wps 40366.6 | wpb 510.9 | bsz 1 | num_updates 1068 | best_loss 9.089
2022-03-06 17:12:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1068 updates
2022-03-06 17:12:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:12:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:12:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 22 @ 1068 updates, score 9.089) (writing took 1.904635850340128 seconds)
2022-03-06 17:12:24 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 17:12:24 | INFO | train | epoch 022 | loss 8.678 | ppl 409.59 | wps 22028 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1068 | lr 0.000133573 | gnorm 0.828 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 3213
2022-03-06 17:12:24 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 17:12:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:13:54 | INFO | train_inner | epoch 023:     32 / 49 loss=8.667, ppl=406.38, wps=21833.7, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=1100, lr=0.000137573, gnorm=0.851, loss_scale=16, train_wall=260, gb_free=21.5, wall=3304
2022-03-06 17:14:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:14:47 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.046 | ppl 528.45 | wps 40299.8 | wpb 510.9 | bsz 1 | num_updates 1117 | best_loss 9.046
2022-03-06 17:14:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1117 updates
2022-03-06 17:14:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:14:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:14:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 23 @ 1117 updates, score 9.046) (writing took 1.9112359015271068 seconds)
2022-03-06 17:14:49 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 17:14:49 | INFO | train | epoch 023 | loss 8.58 | ppl 382.7 | wps 21974.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1117 | lr 0.000139697 | gnorm 0.835 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 3358
2022-03-06 17:14:49 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 17:14:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:17:11 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.99 | ppl 508.29 | wps 40313 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.99
2022-03-06 17:17:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1166 updates
2022-03-06 17:17:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:17:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:17:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 24 @ 1166 updates, score 8.99) (writing took 1.8651969665661454 seconds)
2022-03-06 17:17:13 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 17:17:13 | INFO | train | epoch 024 | loss 8.485 | ppl 358.4 | wps 21967.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1166 | lr 0.000145821 | gnorm 0.869 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 3503
2022-03-06 17:17:13 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 17:17:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:18:50 | INFO | train_inner | epoch 025:     34 / 49 loss=8.466, ppl=353.68, wps=21964.3, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=1200, lr=0.00015007, gnorm=0.837, loss_scale=32, train_wall=258, gb_free=21.5, wall=3599
2022-03-06 17:19:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:19:36 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.941 | ppl 491.35 | wps 40446.3 | wpb 510.9 | bsz 1 | num_updates 1215 | best_loss 8.941
2022-03-06 17:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1215 updates
2022-03-06 17:19:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:19:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:19:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 25 @ 1215 updates, score 8.941) (writing took 1.9163952404633164 seconds)
2022-03-06 17:19:38 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:19:38 | INFO | train | epoch 025 | loss 8.389 | ppl 335.27 | wps 21954.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1215 | lr 0.000151945 | gnorm 0.803 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 3647
2022-03-06 17:19:38 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:19:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:21:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:22:01 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.878 | ppl 470.46 | wps 39046.9 | wpb 510.9 | bsz 1 | num_updates 1264 | best_loss 8.878
2022-03-06 17:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1264 updates
2022-03-06 17:22:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 26 @ 1264 updates, score 8.878) (writing took 1.9406753703951836 seconds)
2022-03-06 17:22:03 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:22:03 | INFO | train | epoch 026 | loss 8.298 | ppl 314.77 | wps 21912.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1264 | lr 0.000158068 | gnorm 0.877 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 3793
2022-03-06 17:22:03 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:23:45 | INFO | train_inner | epoch 027:     36 / 49 loss=8.279, ppl=310.62, wps=21970.3, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1300, lr=0.000162568, gnorm=0.877, loss_scale=64, train_wall=258, gb_free=21.5, wall=3894
2022-03-06 17:24:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:24:26 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.849 | ppl 461.16 | wps 39868.7 | wpb 510.9 | bsz 1 | num_updates 1313 | best_loss 8.849
2022-03-06 17:24:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1313 updates
2022-03-06 17:24:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:24:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:24:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 27 @ 1313 updates, score 8.849) (writing took 1.9070584028959274 seconds)
2022-03-06 17:24:28 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:24:28 | INFO | train | epoch 027 | loss 8.204 | ppl 294.86 | wps 21939.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1313 | lr 0.000164192 | gnorm 0.883 | loss_scale 64 | train_wall 127 | gb_free 21.5 | wall 3937
2022-03-06 17:24:28 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:24:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:24:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:26:51 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.817 | ppl 451.13 | wps 40507.9 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.817
2022-03-06 17:26:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1361 updates
2022-03-06 17:26:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:26:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:26:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 28 @ 1361 updates, score 8.817) (writing took 1.8851641844958067 seconds)
2022-03-06 17:26:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:26:53 | INFO | train | epoch 028 | loss 8.112 | ppl 276.62 | wps 21506.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1361 | lr 0.000170191 | gnorm 0.894 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 4082
2022-03-06 17:26:53 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:26:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:28:43 | INFO | train_inner | epoch 029:     39 / 49 loss=8.086, ppl=271.75, wps=21727.5, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=1400, lr=0.000175065, gnorm=0.888, loss_scale=32, train_wall=261, gb_free=21.5, wall=4193
2022-03-06 17:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:29:16 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.782 | ppl 440.24 | wps 38946.7 | wpb 510.9 | bsz 1 | num_updates 1410 | best_loss 8.782
2022-03-06 17:29:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1410 updates
2022-03-06 17:29:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:29:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:29:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 29 @ 1410 updates, score 8.782) (writing took 1.9058254370465875 seconds)
2022-03-06 17:29:18 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:29:18 | INFO | train | epoch 029 | loss 8.02 | ppl 259.58 | wps 21852.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1410 | lr 0.000176315 | gnorm 0.918 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4228
2022-03-06 17:29:18 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:29:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:31:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:31:41 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.748 | ppl 430.06 | wps 38736 | wpb 510.9 | bsz 1 | num_updates 1458 | best_loss 8.748
2022-03-06 17:31:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1458 updates
2022-03-06 17:31:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:31:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:31:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 30 @ 1458 updates, score 8.748) (writing took 1.8430624641478062 seconds)
2022-03-06 17:31:43 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 17:31:43 | INFO | train | epoch 030 | loss 7.923 | ppl 242.68 | wps 21468.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1458 | lr 0.000182314 | gnorm 0.892 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 4373
2022-03-06 17:31:43 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 17:31:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:33:44 | INFO | train_inner | epoch 031:     42 / 49 loss=7.894, ppl=237.81, wps=21603.2, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=1500, lr=0.000187563, gnorm=0.878, loss_scale=32, train_wall=262, gb_free=21.5, wall=4493
2022-03-06 17:34:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:34:08 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.724 | ppl 422.78 | wps 38555.7 | wpb 510.9 | bsz 1 | num_updates 1507 | best_loss 8.724
2022-03-06 17:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1507 updates
2022-03-06 17:34:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:34:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:34:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 31 @ 1507 updates, score 8.724) (writing took 2.0799842393025756 seconds)
2022-03-06 17:34:10 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 17:34:10 | INFO | train | epoch 031 | loss 7.833 | ppl 228 | wps 21600.1 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1507 | lr 0.000188437 | gnorm 0.874 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 4520
2022-03-06 17:34:10 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 17:34:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:36:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:36:35 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.686 | ppl 411.75 | wps 38537.8 | wpb 510.9 | bsz 1 | num_updates 1556 | best_loss 8.686
2022-03-06 17:36:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1556 updates
2022-03-06 17:36:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:36:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:36:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 32 @ 1556 updates, score 8.686) (writing took 2.1182051794603467 seconds)
2022-03-06 17:36:37 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 17:36:37 | INFO | train | epoch 032 | loss 7.742 | ppl 214.02 | wps 21655.6 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1556 | lr 0.000194561 | gnorm 0.922 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 4666
2022-03-06 17:36:37 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 17:36:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:38:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:38:46 | INFO | train_inner | epoch 033:     45 / 49 loss=7.706, ppl=208.75, wps=21494.2, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=1600, lr=0.00020006, gnorm=0.94, loss_scale=32, train_wall=263, gb_free=21.5, wall=4795
2022-03-06 17:38:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:39:01 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.673 | ppl 408.3 | wps 40355.5 | wpb 510.9 | bsz 1 | num_updates 1604 | best_loss 8.673
2022-03-06 17:39:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1604 updates
2022-03-06 17:39:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:39:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:39:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 33 @ 1604 updates, score 8.673) (writing took 1.9801186602562666 seconds)
2022-03-06 17:39:03 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 17:39:03 | INFO | train | epoch 033 | loss 7.65 | ppl 200.79 | wps 21334.4 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1604 | lr 0.00020056 | gnorm 0.932 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4812
2022-03-06 17:39:03 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 17:39:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:41:26 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.659 | ppl 404.33 | wps 38882.5 | wpb 510.9 | bsz 1 | num_updates 1653 | best_loss 8.659
2022-03-06 17:41:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1653 updates
2022-03-06 17:41:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:41:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 34 @ 1653 updates, score 8.659) (writing took 1.8343162387609482 seconds)
2022-03-06 17:41:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 17:41:28 | INFO | train | epoch 034 | loss 7.559 | ppl 188.55 | wps 21884.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1653 | lr 0.000206684 | gnorm 0.891 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4958
2022-03-06 17:41:28 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 17:41:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:43:41 | INFO | train_inner | epoch 035:     47 / 49 loss=7.522, ppl=183.76, wps=21942.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1700, lr=0.000212558, gnorm=0.92, loss_scale=32, train_wall=258, gb_free=21.5, wall=5091
2022-03-06 17:43:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:43:51 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.64 | ppl 398.9 | wps 40409.1 | wpb 510.9 | bsz 1 | num_updates 1702 | best_loss 8.64
2022-03-06 17:43:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1702 updates
2022-03-06 17:43:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:43:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:43:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 35 @ 1702 updates, score 8.64) (writing took 1.8479904364794493 seconds)
2022-03-06 17:43:53 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 17:43:53 | INFO | train | epoch 035 | loss 7.472 | ppl 177.49 | wps 21969.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1702 | lr 0.000212807 | gnorm 0.966 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 5102
2022-03-06 17:43:53 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 17:43:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:45:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:46:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:46:16 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.624 | ppl 394.47 | wps 39130.1 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 8.624
2022-03-06 17:46:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1750 updates
2022-03-06 17:46:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:46:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 36 @ 1750 updates, score 8.624) (writing took 1.8686109827831388 seconds)
2022-03-06 17:46:17 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 17:46:17 | INFO | train | epoch 036 | loss 7.381 | ppl 166.72 | wps 21508.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1750 | lr 0.000218806 | gnorm 0.971 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 5247
2022-03-06 17:46:17 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 17:46:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:48:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:48:41 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.608 | ppl 390.09 | wps 40483.8 | wpb 510.9 | bsz 1 | num_updates 1799 | best_loss 8.608
2022-03-06 17:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1799 updates
2022-03-06 17:48:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:48:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt
2022-03-06 17:48:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.02_0.06_0.92_#1/checkpoint_best.pt (epoch 37 @ 1799 updates, score 8.608) (writing took 1.8702123472467065 seconds)
2022-03-06 17:48:43 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 17:48:43 | INFO | train | epoch 037 | loss 7.295 | ppl 157.01 | wps 21888.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1799 | lr 0.00022493 | gnorm 0.93 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5392
2022-03-06 17:48:43 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 17:48:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:48:46 | INFO | train_inner | epoch 038:      1 / 49 loss=7.338, ppl=161.77, wps=21203.5, ups=0.33, wpb=64544.1, bsz=126.1, num_updates=1800, lr=0.000225055, gnorm=0.954, loss_scale=32, train_wall=260, gb_free=21.5, wall=5395
2022-03-06 17:50:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:51:05 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.612 | ppl 391.27 | wps 40352.7 | wpb 510.9 | bsz 1 | num_updates 1848 | best_loss 8.608
2022-03-06 17:51:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1848 updates
2022-03-06 17:51:05 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 17:51:05 | INFO | train | epoch 038 | loss 7.209 | ppl 147.95 | wps 22335.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1848 | lr 0.000231054 | gnorm 0.999 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 5534
2022-03-06 17:51:05 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 17:51:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:52:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:53:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:53:28 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.631 | ppl 396.32 | wps 39058.9 | wpb 510.9 | bsz 1 | num_updates 1896 | best_loss 8.608
2022-03-06 17:53:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1896 updates
2022-03-06 17:53:28 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 17:53:28 | INFO | train | epoch 039 | loss 7.122 | ppl 139.32 | wps 21715.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1896 | lr 0.000237053 | gnorm 0.926 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5678
2022-03-06 17:53:28 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 17:53:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:53:40 | INFO | train_inner | epoch 040:      4 / 49 loss=7.161, ppl=143.09, wps=22061.2, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1900, lr=0.000237553, gnorm=0.96, loss_scale=32, train_wall=260, gb_free=21.5, wall=5689
2022-03-06 17:55:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:55:51 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.633 | ppl 397.06 | wps 39260.4 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 8.608
2022-03-06 17:55:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1945 updates
2022-03-06 17:55:51 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 17:55:51 | INFO | train | epoch 040 | loss 7.038 | ppl 131.42 | wps 22213 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1945 | lr 0.000243176 | gnorm 0.973 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5821
2022-03-06 17:55:51 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 17:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:58:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:58:15 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.642 | ppl 399.42 | wps 40456.2 | wpb 510.9 | bsz 1 | num_updates 1994 | best_loss 8.608
2022-03-06 17:58:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1994 updates
2022-03-06 17:58:15 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 17:58:15 | INFO | train | epoch 041 | loss 6.952 | ppl 123.82 | wps 22187.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1994 | lr 0.0002493 | gnorm 0.976 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5964
2022-03-06 17:58:15 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 17:58:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:58:32 | INFO | train_inner | epoch 042:      6 / 49 loss=6.984, ppl=126.58, wps=22217.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2000, lr=0.00025005, gnorm=0.972, loss_scale=32, train_wall=258, gb_free=21.5, wall=5981
2022-03-06 17:58:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 18:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:00:37 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.646 | ppl 400.64 | wps 40333.3 | wpb 510.9 | bsz 1 | num_updates 2042 | best_loss 8.608
2022-03-06 18:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2042 updates
2022-03-06 18:00:37 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 18:00:37 | INFO | train | epoch 042 | loss 6.87 | ppl 116.93 | wps 21819.5 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2042 | lr 0.000255299 | gnorm 0.986 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 6107
2022-03-06 18:00:37 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 18:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:02:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:03:01 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.671 | ppl 407.46 | wps 39223.1 | wpb 510.9 | bsz 1 | num_updates 2091 | best_loss 8.608
2022-03-06 18:03:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2091 updates
2022-03-06 18:03:01 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 18:03:01 | INFO | train | epoch 043 | loss 6.789 | ppl 110.59 | wps 22118.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2091 | lr 0.000261423 | gnorm 1.016 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 6250
2022-03-06 18:03:01 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 18:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:03:27 | INFO | train_inner | epoch 044:      9 / 49 loss=6.816, ppl=112.64, wps=21989.7, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=2100, lr=0.000262548, gnorm=0.996, loss_scale=32, train_wall=261, gb_free=21.5, wall=6276
2022-03-06 18:03:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:05:24 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.668 | ppl 406.62 | wps 40379.6 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 8.608
2022-03-06 18:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2139 updates
2022-03-06 18:05:24 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 18:05:24 | INFO | train | epoch 044 | loss 6.707 | ppl 104.5 | wps 21767.9 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2139 | lr 0.000267422 | gnorm 1.016 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 6393
2022-03-06 18:05:24 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 18:05:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:07:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:07:47 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.716 | ppl 420.53 | wps 40487.3 | wpb 510.9 | bsz 1 | num_updates 2188 | best_loss 8.608
2022-03-06 18:07:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2188 updates
2022-03-06 18:07:47 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 18:07:47 | INFO | train | epoch 045 | loss 6.623 | ppl 98.59 | wps 22172.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2188 | lr 0.000273545 | gnorm 1.025 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 6537
2022-03-06 18:07:47 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 18:07:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:08:21 | INFO | train_inner | epoch 046:     12 / 49 loss=6.648, ppl=100.31, wps=22025.2, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=2200, lr=0.000275045, gnorm=1.034, loss_scale=16, train_wall=261, gb_free=21.5, wall=6571
2022-03-06 18:10:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:10:10 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.78 | ppl 439.61 | wps 40460.8 | wpb 510.9 | bsz 1 | num_updates 2237 | best_loss 8.608
2022-03-06 18:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2237 updates
2022-03-06 18:10:10 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 18:10:10 | INFO | train | epoch 046 | loss 6.544 | ppl 93.31 | wps 22301.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2237 | lr 0.000279669 | gnorm 1.041 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 6679
2022-03-06 18:10:10 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 18:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:12:32 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.768 | ppl 435.8 | wps 40406.7 | wpb 510.9 | bsz 1 | num_updates 2286 | best_loss 8.608
2022-03-06 18:12:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2286 updates
2022-03-06 18:12:32 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 18:12:32 | INFO | train | epoch 047 | loss 6.463 | ppl 88.23 | wps 22304.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2286 | lr 0.000285793 | gnorm 1.076 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 6822
2022-03-06 18:12:32 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 18:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:13:12 | INFO | train_inner | epoch 048:     14 / 49 loss=6.478, ppl=89.15, wps=22288.9, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=2300, lr=0.000287543, gnorm=1.057, loss_scale=32, train_wall=258, gb_free=21.5, wall=6862
2022-03-06 18:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:14:55 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.828 | ppl 454.49 | wps 40562.6 | wpb 510.9 | bsz 1 | num_updates 2335 | best_loss 8.608
2022-03-06 18:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2335 updates
2022-03-06 18:14:55 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 18:14:55 | INFO | train | epoch 048 | loss 6.379 | ppl 83.25 | wps 22235.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2335 | lr 0.000291917 | gnorm 1.043 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 6965
2022-03-06 18:14:55 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 18:14:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:16:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 18:17:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:17:18 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.878 | ppl 470.33 | wps 40472.7 | wpb 510.9 | bsz 1 | num_updates 2383 | best_loss 8.608
2022-03-06 18:17:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2383 updates
2022-03-06 18:17:18 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 18:17:18 | INFO | train | epoch 049 | loss 6.3 | ppl 78.78 | wps 21786.7 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2383 | lr 0.000297915 | gnorm 1.089 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 7107
2022-03-06 18:17:18 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 18:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:18:06 | INFO | train_inner | epoch 050:     17 / 49 loss=6.315, ppl=79.61, wps=22074.5, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=2400, lr=0.00030004, gnorm=1.09, loss_scale=32, train_wall=260, gb_free=21.5, wall=7156
2022-03-06 18:19:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:19:41 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.889 | ppl 473.98 | wps 40376.5 | wpb 510.9 | bsz 1 | num_updates 2432 | best_loss 8.608
2022-03-06 18:19:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2432 updates
2022-03-06 18:19:41 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 18:19:41 | INFO | train | epoch 050 | loss 6.223 | ppl 74.7 | wps 22297.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2432 | lr 0.000304039 | gnorm 1.109 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 7250
2022-03-06 18:19:41 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 18:19:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:21:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:22:04 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.905 | ppl 479.36 | wps 38852.5 | wpb 510.9 | bsz 1 | num_updates 2481 | best_loss 8.608
2022-03-06 18:22:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2481 updates
2022-03-06 18:22:04 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 18:22:04 | INFO | train | epoch 051 | loss 6.139 | ppl 70.45 | wps 22084.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2481 | lr 0.000310163 | gnorm 1.06 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 7394
2022-03-06 18:22:04 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 18:22:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:22:59 | INFO | train_inner | epoch 052:     19 / 49 loss=6.149, ppl=70.97, wps=22125.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2500, lr=0.000312538, gnorm=1.084, loss_scale=64, train_wall=259, gb_free=21.5, wall=7449
2022-03-06 18:23:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 18:24:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:24:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:24:30 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.001 | ppl 512.34 | wps 38218.3 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 8.608
2022-03-06 18:24:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2528 updates
2022-03-06 18:24:30 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 18:24:30 | INFO | train | epoch 052 | loss 6.06 | ppl 66.74 | wps 20978.2 | ups 0.32 | wpb 64829.4 | bsz 126.6 | num_updates 2528 | lr 0.000316037 | gnorm 1.156 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 7539
2022-03-06 18:24:30 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 18:24:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:26:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:26:54 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.991 | ppl 508.83 | wps 38668.3 | wpb 510.9 | bsz 1 | num_updates 2577 | best_loss 8.608
2022-03-06 18:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2577 updates
2022-03-06 18:26:54 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 18:26:54 | INFO | train | epoch 053 | loss 5.993 | ppl 63.69 | wps 22012.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2577 | lr 0.000322161 | gnorm 1.195 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 7683
2022-03-06 18:26:54 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 18:26:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:28:00 | INFO | train_inner | epoch 054:     23 / 49 loss=5.989, ppl=63.52, wps=21594.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=2600, lr=0.000325035, gnorm=1.131, loss_scale=16, train_wall=266, gb_free=21.5, wall=7749
2022-03-06 18:29:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:29:17 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.012 | ppl 516.13 | wps 40432 | wpb 510.9 | bsz 1 | num_updates 2626 | best_loss 8.608
2022-03-06 18:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2626 updates
2022-03-06 18:29:17 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 18:29:17 | INFO | train | epoch 054 | loss 5.912 | ppl 60.19 | wps 22233.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2626 | lr 0.000328284 | gnorm 1.138 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 7826
2022-03-06 18:29:17 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 18:29:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:31:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:31:40 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.128 | ppl 559.65 | wps 40528.1 | wpb 510.9 | bsz 1 | num_updates 2674 | best_loss 8.608
2022-03-06 18:31:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2674 updates
2022-03-06 18:31:40 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 18:31:40 | INFO | train | epoch 055 | loss 5.831 | ppl 56.93 | wps 21741.6 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2674 | lr 0.000334283 | gnorm 1.192 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 7970
2022-03-06 18:31:40 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 18:31:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:32:54 | INFO | train_inner | epoch 056:     26 / 49 loss=5.831, ppl=56.92, wps=22077.6, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2700, lr=0.000337533, gnorm=1.187, loss_scale=16, train_wall=260, gb_free=21.5, wall=8043
2022-03-06 18:33:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:34:02 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.052 | ppl 530.86 | wps 40557.4 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 8.608
2022-03-06 18:34:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2723 updates
2022-03-06 18:34:02 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 18:34:02 | INFO | train | epoch 056 | loss 5.754 | ppl 53.97 | wps 22335.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2723 | lr 0.000340407 | gnorm 1.117 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 8112
2022-03-06 18:34:02 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 18:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:36:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:36:25 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.14 | ppl 564.35 | wps 40337.2 | wpb 510.9 | bsz 1 | num_updates 2772 | best_loss 8.608
2022-03-06 18:36:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2772 updates
2022-03-06 18:36:25 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 18:36:25 | INFO | train | epoch 057 | loss 5.679 | ppl 51.23 | wps 22325.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2772 | lr 0.000346531 | gnorm 1.195 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 8254
2022-03-06 18:36:25 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 18:36:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:37:44 | INFO | train_inner | epoch 058:     28 / 49 loss=5.678, ppl=51.2, wps=22333.6, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=2800, lr=0.00035003, gnorm=1.213, loss_scale=32, train_wall=257, gb_free=21.5, wall=8333
2022-03-06 18:38:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:38:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:38:47 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.149 | ppl 567.8 | wps 40440.7 | wpb 510.9 | bsz 1 | num_updates 2820 | best_loss 8.608
2022-03-06 18:38:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2820 updates
2022-03-06 18:38:47 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 18:38:47 | INFO | train | epoch 058 | loss 5.604 | ppl 48.64 | wps 21813.9 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2820 | lr 0.00035253 | gnorm 1.247 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 8397
2022-03-06 18:38:48 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 18:38:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:41:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:41:11 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.189 | ppl 583.68 | wps 38927.7 | wpb 510.9 | bsz 1 | num_updates 2869 | best_loss 8.608
2022-03-06 18:41:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2869 updates
2022-03-06 18:41:11 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 18:41:11 | INFO | train | epoch 059 | loss 5.528 | ppl 46.13 | wps 22095.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2869 | lr 0.000358653 | gnorm 1.201 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8541
2022-03-06 18:41:11 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 18:41:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:42:39 | INFO | train_inner | epoch 060:     31 / 49 loss=5.52, ppl=45.88, wps=21963.5, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=2900, lr=0.000362528, gnorm=1.234, loss_scale=16, train_wall=262, gb_free=21.5, wall=8629
2022-03-06 18:43:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:43:34 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.206 | ppl 590.43 | wps 40504.9 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 8.608
2022-03-06 18:43:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2918 updates
2022-03-06 18:43:34 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 18:43:34 | INFO | train | epoch 060 | loss 5.449 | ppl 43.68 | wps 22224 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2918 | lr 0.000364777 | gnorm 1.221 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8684
2022-03-06 18:43:34 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 18:43:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:45:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:45:57 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.249 | ppl 608.56 | wps 39122.6 | wpb 510.9 | bsz 1 | num_updates 2967 | best_loss 8.608
2022-03-06 18:45:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 2967 updates
2022-03-06 18:45:57 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 18:45:57 | INFO | train | epoch 061 | loss 5.375 | ppl 41.51 | wps 22267.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2967 | lr 0.000370901 | gnorm 1.272 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 8826
2022-03-06 18:45:57 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 18:45:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:46:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:47:33 | INFO | train_inner | epoch 062:     34 / 49 loss=5.361, ppl=41.09, wps=22087.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=3000, lr=0.000375025, gnorm=1.236, loss_scale=16, train_wall=260, gb_free=21.5, wall=8922
2022-03-06 18:48:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:48:20 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.34 | ppl 648.19 | wps 40170.9 | wpb 510.9 | bsz 1 | num_updates 3015 | best_loss 8.608
2022-03-06 18:48:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3015 updates
2022-03-06 18:48:20 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 18:48:20 | INFO | train | epoch 062 | loss 5.296 | ppl 39.3 | wps 21827.8 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3015 | lr 0.0003769 | gnorm 1.198 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 8969
2022-03-06 18:48:20 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 18:48:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:50:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:50:42 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.398 | ppl 674.51 | wps 40321.6 | wpb 510.9 | bsz 1 | num_updates 3064 | best_loss 8.608
2022-03-06 18:50:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3064 updates
2022-03-06 18:50:42 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 18:50:42 | INFO | train | epoch 063 | loss 5.237 | ppl 37.71 | wps 22284.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3064 | lr 0.000383023 | gnorm 1.296 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 9112
2022-03-06 18:50:42 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 18:50:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:52:25 | INFO | train_inner | epoch 064:     36 / 49 loss=5.22, ppl=37.27, wps=22220.8, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=3100, lr=0.000387523, gnorm=1.304, loss_scale=16, train_wall=259, gb_free=21.5, wall=9214
2022-03-06 18:53:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:53:06 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.396 | ppl 673.68 | wps 40437.1 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 8.608
2022-03-06 18:53:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3113 updates
2022-03-06 18:53:06 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 18:53:06 | INFO | train | epoch 064 | loss 5.163 | ppl 35.83 | wps 22134.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3113 | lr 0.000389147 | gnorm 1.328 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 9255
2022-03-06 18:53:06 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 18:53:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:53:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:55:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:55:29 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.487 | ppl 717.35 | wps 40475.9 | wpb 510.9 | bsz 1 | num_updates 3161 | best_loss 8.608
2022-03-06 18:55:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3161 updates
2022-03-06 18:55:29 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-06 18:55:29 | INFO | train | epoch 065 | loss 5.079 | ppl 33.8 | wps 21749.2 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3161 | lr 0.000395146 | gnorm 1.297 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 9398
2022-03-06 18:55:29 | INFO | fairseq.trainer | begin training epoch 66
2022-03-06 18:55:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:57:20 | INFO | train_inner | epoch 066:     39 / 49 loss=5.064, ppl=33.45, wps=22021.3, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=3200, lr=0.00040002, gnorm=1.273, loss_scale=16, train_wall=261, gb_free=21.5, wall=9509
2022-03-06 18:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:57:52 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.459 | ppl 703.69 | wps 39224 | wpb 510.9 | bsz 1 | num_updates 3210 | best_loss 8.608
2022-03-06 18:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3210 updates
2022-03-06 18:57:52 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-06 18:57:52 | INFO | train | epoch 066 | loss 5.014 | ppl 32.32 | wps 22170.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3210 | lr 0.00040127 | gnorm 1.307 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 9542
2022-03-06 18:57:52 | INFO | fairseq.trainer | begin training epoch 67
2022-03-06 18:57:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:00:15 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.514 | ppl 731.05 | wps 40381.2 | wpb 510.9 | bsz 1 | num_updates 3259 | best_loss 8.608
2022-03-06 19:00:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3259 updates
2022-03-06 19:00:15 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-06 19:00:15 | INFO | train | epoch 067 | loss 4.947 | ppl 30.84 | wps 22224.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3259 | lr 0.000407394 | gnorm 1.406 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 9685
2022-03-06 19:00:15 | INFO | fairseq.trainer | begin training epoch 68
2022-03-06 19:00:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:01:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:02:14 | INFO | train_inner | epoch 068:     42 / 49 loss=4.921, ppl=30.29, wps=22052.9, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3300, lr=0.000412518, gnorm=1.314, loss_scale=16, train_wall=261, gb_free=21.5, wall=9803
2022-03-06 19:02:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:02:38 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.566 | ppl 757.73 | wps 40423.7 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 8.608
2022-03-06 19:02:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3307 updates
2022-03-06 19:02:38 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-06 19:02:38 | INFO | train | epoch 068 | loss 4.866 | ppl 29.17 | wps 21877.9 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3307 | lr 0.000413392 | gnorm 1.194 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 9827
2022-03-06 19:02:38 | INFO | fairseq.trainer | begin training epoch 69
2022-03-06 19:02:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:04:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:05:01 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.652 | ppl 804.77 | wps 40447 | wpb 510.9 | bsz 1 | num_updates 3356 | best_loss 8.608
2022-03-06 19:05:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3356 updates
2022-03-06 19:05:01 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-06 19:05:01 | INFO | train | epoch 069 | loss 4.811 | ppl 28.07 | wps 22178.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3356 | lr 0.000419516 | gnorm 1.339 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 9970
2022-03-06 19:05:01 | INFO | fairseq.trainer | begin training epoch 70
2022-03-06 19:05:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:07:05 | INFO | train_inner | epoch 070:     44 / 49 loss=4.779, ppl=27.45, wps=22239.6, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3400, lr=0.000425015, gnorm=1.306, loss_scale=16, train_wall=258, gb_free=21.5, wall=10095
2022-03-06 19:07:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:07:24 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.626 | ppl 789.94 | wps 39235.5 | wpb 510.9 | bsz 1 | num_updates 3405 | best_loss 8.608
2022-03-06 19:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3405 updates
2022-03-06 19:07:24 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-06 19:07:24 | INFO | train | epoch 070 | loss 4.733 | ppl 26.58 | wps 22214.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3405 | lr 0.00042564 | gnorm 1.283 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 10113
2022-03-06 19:07:24 | INFO | fairseq.trainer | begin training epoch 71
2022-03-06 19:07:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:08:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:09:48 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.705 | ppl 834.87 | wps 38127.8 | wpb 510.9 | bsz 1 | num_updates 3453 | best_loss 8.608
2022-03-06 19:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3453 updates
2022-03-06 19:09:48 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-06 19:09:48 | INFO | train | epoch 071 | loss 4.683 | ppl 25.68 | wps 21609 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3453 | lr 0.000431639 | gnorm 1.434 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 10257
2022-03-06 19:09:48 | INFO | fairseq.trainer | begin training epoch 72
2022-03-06 19:09:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:12:03 | INFO | train_inner | epoch 072:     47 / 49 loss=4.65, ppl=25.11, wps=21834.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3500, lr=0.000437513, gnorm=1.397, loss_scale=16, train_wall=263, gb_free=21.5, wall=10392
2022-03-06 19:12:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:12:13 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.769 | ppl 872.73 | wps 38677.3 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 8.608
2022-03-06 19:12:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 3502 updates
2022-03-06 19:12:13 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-06 19:12:13 | INFO | train | epoch 072 | loss 4.604 | ppl 24.32 | wps 21968.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3502 | lr 0.000437762 | gnorm 1.388 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 10402
2022-03-06 19:12:13 | INFO | fairseq.trainer | begin training epoch 73
2022-03-06 19:12:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:14:37 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.771 | ppl 873.45 | wps 38938.8 | wpb 510.9 | bsz 1 | num_updates 3551 | best_loss 8.608
2022-03-06 19:14:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 3551 updates
2022-03-06 19:14:37 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-06 19:14:37 | INFO | train | epoch 073 | loss 4.528 | ppl 23.08 | wps 21989.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3551 | lr 0.000443886 | gnorm 1.26 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 10547
2022-03-06 19:14:37 | INFO | fairseq.trainer | begin training epoch 74
2022-03-06 19:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:14:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:17:01 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.865 | ppl 932.76 | wps 40418.5 | wpb 510.9 | bsz 1 | num_updates 3599 | best_loss 8.608
2022-03-06 19:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 3599 updates
2022-03-06 19:17:01 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-06 19:17:01 | INFO | train | epoch 074 | loss 4.467 | ppl 22.12 | wps 21679.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3599 | lr 0.000449885 | gnorm 1.332 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 10690
2022-03-06 19:17:01 | INFO | fairseq.trainer | begin training epoch 75
2022-03-06 19:17:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:17:04 | INFO | train_inner | epoch 075:      1 / 49 loss=4.499, ppl=22.61, wps=21439.8, ups=0.33, wpb=64544.1, bsz=126.1, num_updates=3600, lr=0.00045001, gnorm=1.298, loss_scale=16, train_wall=261, gb_free=21.5, wall=10693
2022-03-06 19:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:19:23 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.887 | ppl 947.09 | wps 40468 | wpb 510.9 | bsz 1 | num_updates 3648 | best_loss 8.608
2022-03-06 19:19:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 3648 updates
2022-03-06 19:19:23 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-06 19:19:23 | INFO | train | epoch 075 | loss 4.411 | ppl 21.28 | wps 22327.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3648 | lr 0.000456009 | gnorm 1.403 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 10832
2022-03-06 19:19:23 | INFO | fairseq.trainer | begin training epoch 76
2022-03-06 19:19:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:21:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:21:45 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.949 | ppl 988.73 | wps 40298.7 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 8.608
2022-03-06 19:21:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 3697 updates
2022-03-06 19:21:45 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-06 19:21:45 | INFO | train | epoch 076 | loss 4.381 | ppl 20.84 | wps 22323.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3697 | lr 0.000462133 | gnorm 1.527 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 10975
2022-03-06 19:21:45 | INFO | fairseq.trainer | begin training epoch 77
2022-03-06 19:21:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:21:54 | INFO | train_inner | epoch 077:      3 / 49 loss=4.391, ppl=20.98, wps=22341, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3700, lr=0.000462508, gnorm=1.449, loss_scale=32, train_wall=257, gb_free=21.5, wall=10983
2022-03-06 19:23:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:24:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:24:08 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.017 | ppl 1036.29 | wps 40388.2 | wpb 510.9 | bsz 1 | num_updates 3745 | best_loss 8.608
2022-03-06 19:24:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 3745 updates
2022-03-06 19:24:08 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-06 19:24:08 | INFO | train | epoch 077 | loss 4.272 | ppl 19.32 | wps 21779.9 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3745 | lr 0.000468131 | gnorm 1.238 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 11118
2022-03-06 19:24:08 | INFO | fairseq.trainer | begin training epoch 78
2022-03-06 19:24:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:26:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:26:31 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.081 | ppl 1083.34 | wps 40320 | wpb 510.9 | bsz 1 | num_updates 3794 | best_loss 8.608
2022-03-06 19:26:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 3794 updates
2022-03-06 19:26:31 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-06 19:26:31 | INFO | train | epoch 078 | loss 4.222 | ppl 18.66 | wps 22213.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3794 | lr 0.000474255 | gnorm 1.291 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 11261
2022-03-06 19:26:31 | INFO | fairseq.trainer | begin training epoch 79
2022-03-06 19:26:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:26:48 | INFO | train_inner | epoch 079:      6 / 49 loss=4.238, ppl=18.87, wps=22033.9, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=3800, lr=0.000475005, gnorm=1.28, loss_scale=16, train_wall=261, gb_free=21.5, wall=11278
2022-03-06 19:28:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:28:54 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.098 | ppl 1095.79 | wps 40251.4 | wpb 510.9 | bsz 1 | num_updates 3843 | best_loss 8.608
2022-03-06 19:28:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 3843 updates
2022-03-06 19:28:54 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-06 19:28:54 | INFO | train | epoch 079 | loss 4.165 | ppl 17.94 | wps 22235.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3843 | lr 0.000480379 | gnorm 1.368 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 11404
2022-03-06 19:28:54 | INFO | fairseq.trainer | begin training epoch 80
2022-03-06 19:28:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:30:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:30:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:31:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:31:18 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.142 | ppl 1129.65 | wps 39123.7 | wpb 510.9 | bsz 1 | num_updates 3890 | best_loss 8.608
2022-03-06 19:31:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 3890 updates
2022-03-06 19:31:18 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-06 19:31:18 | INFO | train | epoch 080 | loss 4.103 | ppl 17.18 | wps 21238.9 | ups 0.33 | wpb 64829.4 | bsz 126.6 | num_updates 3890 | lr 0.000486253 | gnorm 1.394 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 11547
2022-03-06 19:31:18 | INFO | fairseq.trainer | begin training epoch 81
2022-03-06 19:31:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:31:46 | INFO | train_inner | epoch 081:     10 / 49 loss=4.124, ppl=17.43, wps=21774.3, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=3900, lr=0.000487503, gnorm=1.363, loss_scale=8, train_wall=264, gb_free=21.5, wall=11576
2022-03-06 19:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:33:41 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.188 | ppl 1166.31 | wps 40558.4 | wpb 510.9 | bsz 1 | num_updates 3939 | best_loss 8.608
2022-03-06 19:33:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 3939 updates
2022-03-06 19:33:41 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-06 19:33:41 | INFO | train | epoch 081 | loss 4.054 | ppl 16.61 | wps 22170.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3939 | lr 0.000492377 | gnorm 1.39 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 11691
2022-03-06 19:33:41 | INFO | fairseq.trainer | begin training epoch 82
2022-03-06 19:33:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:35:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:36:04 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.243 | ppl 1211.83 | wps 40498 | wpb 510.9 | bsz 1 | num_updates 3988 | best_loss 8.608
2022-03-06 19:36:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 3988 updates
2022-03-06 19:36:04 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-06 19:36:04 | INFO | train | epoch 082 | loss 3.987 | ppl 15.86 | wps 22172.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3988 | lr 0.0004985 | gnorm 1.292 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 11834
2022-03-06 19:36:04 | INFO | fairseq.trainer | begin training epoch 83
2022-03-06 19:36:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:36:38 | INFO | train_inner | epoch 083:     12 / 49 loss=4.007, ppl=16.08, wps=22211, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4000, lr=0.0005, gnorm=1.342, loss_scale=8, train_wall=259, gb_free=21.5, wall=11868
2022-03-06 19:38:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:38:28 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.28 | ppl 1243.66 | wps 39046.9 | wpb 510.9 | bsz 1 | num_updates 4037 | best_loss 8.608
2022-03-06 19:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 4037 updates
2022-03-06 19:38:28 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-06 19:38:28 | INFO | train | epoch 083 | loss 3.936 | ppl 15.3 | wps 22158.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4037 | lr 0.000497703 | gnorm 1.317 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 11977
2022-03-06 19:38:28 | INFO | fairseq.trainer | begin training epoch 84
2022-03-06 19:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:39:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:40:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:40:52 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.328 | ppl 1285.3 | wps 40521.9 | wpb 510.9 | bsz 1 | num_updates 4085 | best_loss 8.608
2022-03-06 19:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 4085 updates
2022-03-06 19:40:52 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-06 19:40:52 | INFO | train | epoch 084 | loss 3.879 | ppl 14.71 | wps 21653.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4085 | lr 0.000494771 | gnorm 1.379 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 12121
2022-03-06 19:40:52 | INFO | fairseq.trainer | begin training epoch 85
2022-03-06 19:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:41:34 | INFO | train_inner | epoch 085:     15 / 49 loss=3.888, ppl=14.81, wps=21949.3, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4100, lr=0.000493865, gnorm=1.32, loss_scale=8, train_wall=262, gb_free=21.5, wall=12163
2022-03-06 19:43:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:43:15 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.367 | ppl 1320.94 | wps 39180.5 | wpb 510.9 | bsz 1 | num_updates 4134 | best_loss 8.608
2022-03-06 19:43:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 4134 updates
2022-03-06 19:43:15 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-06 19:43:15 | INFO | train | epoch 085 | loss 3.818 | ppl 14.1 | wps 22214.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4134 | lr 0.00049183 | gnorm 1.262 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 12264
2022-03-06 19:43:15 | INFO | fairseq.trainer | begin training epoch 86
2022-03-06 19:43:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:45:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:45:38 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.39 | ppl 1341.74 | wps 40520.5 | wpb 510.9 | bsz 1 | num_updates 4183 | best_loss 8.608
2022-03-06 19:45:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 4183 updates
2022-03-06 19:45:38 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-06 19:45:38 | INFO | train | epoch 086 | loss 3.76 | ppl 13.55 | wps 22141.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4183 | lr 0.000488941 | gnorm 1.277 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 12408
2022-03-06 19:45:38 | INFO | fairseq.trainer | begin training epoch 87
2022-03-06 19:45:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:46:26 | INFO | train_inner | epoch 087:     17 / 49 loss=3.768, ppl=13.63, wps=22201.1, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=4200, lr=0.00048795, gnorm=1.275, loss_scale=16, train_wall=259, gb_free=21.5, wall=12456
2022-03-06 19:47:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:48:01 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.428 | ppl 1377.79 | wps 40066.2 | wpb 510.9 | bsz 1 | num_updates 4232 | best_loss 8.608
2022-03-06 19:48:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 4232 updates
2022-03-06 19:48:01 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-06 19:48:01 | INFO | train | epoch 087 | loss 3.705 | ppl 13.04 | wps 22300.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4232 | lr 0.000486102 | gnorm 1.231 | loss_scale 16 | train_wall 126 | gb_free 21.5 | wall 12550
2022-03-06 19:48:01 | INFO | fairseq.trainer | begin training epoch 88
2022-03-06 19:48:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:49:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:50:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:50:24 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.525 | ppl 1473.48 | wps 40386.1 | wpb 510.9 | bsz 1 | num_updates 4280 | best_loss 8.608
2022-03-06 19:50:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 4280 updates
2022-03-06 19:50:24 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-06 19:50:24 | INFO | train | epoch 088 | loss 3.656 | ppl 12.6 | wps 21668.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4280 | lr 0.000483368 | gnorm 1.292 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 12694
2022-03-06 19:50:24 | INFO | fairseq.trainer | begin training epoch 89
2022-03-06 19:50:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:51:21 | INFO | train_inner | epoch 089:     20 / 49 loss=3.66, ppl=12.64, wps=21993.2, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=4300, lr=0.000482243, gnorm=1.242, loss_scale=8, train_wall=261, gb_free=21.5, wall=12751
2022-03-06 19:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:52:48 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.538 | ppl 1487.19 | wps 39272 | wpb 510.9 | bsz 1 | num_updates 4329 | best_loss 8.608
2022-03-06 19:52:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 4329 updates
2022-03-06 19:52:48 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-06 19:52:48 | INFO | train | epoch 089 | loss 3.603 | ppl 12.15 | wps 22176.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4329 | lr 0.000480625 | gnorm 1.199 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 12837
2022-03-06 19:52:48 | INFO | fairseq.trainer | begin training epoch 90
2022-03-06 19:52:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:55:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:55:10 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.623 | ppl 1577.06 | wps 40385.7 | wpb 510.9 | bsz 1 | num_updates 4378 | best_loss 8.608
2022-03-06 19:55:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 4378 updates
2022-03-06 19:55:10 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-06 19:55:10 | INFO | train | epoch 090 | loss 3.555 | ppl 11.75 | wps 22297.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4378 | lr 0.000477928 | gnorm 1.222 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 12980
2022-03-06 19:55:10 | INFO | fairseq.trainer | begin training epoch 91
2022-03-06 19:55:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:56:12 | INFO | train_inner | epoch 091:     22 / 49 loss=3.561, ppl=11.8, wps=22270.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=4400, lr=0.000476731, gnorm=1.218, loss_scale=16, train_wall=258, gb_free=21.5, wall=13042
2022-03-06 19:57:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:57:33 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.681 | ppl 1641.78 | wps 38704.8 | wpb 510.9 | bsz 1 | num_updates 4426 | best_loss 8.608
2022-03-06 19:57:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 4426 updates
2022-03-06 19:57:33 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-06 19:57:33 | INFO | train | epoch 091 | loss 3.504 | ppl 11.34 | wps 21782.1 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4426 | lr 0.000475329 | gnorm 1.182 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 13123
2022-03-06 19:57:33 | INFO | fairseq.trainer | begin training epoch 92
2022-03-06 19:57:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:59:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:59:58 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.679 | ppl 1638.9 | wps 39127.5 | wpb 510.9 | bsz 1 | num_updates 4475 | best_loss 8.608
2022-03-06 19:59:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 4475 updates
2022-03-06 19:59:58 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-06 19:59:58 | INFO | train | epoch 092 | loss 3.464 | ppl 11.03 | wps 21949.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4475 | lr 0.000472719 | gnorm 1.206 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13267
2022-03-06 19:59:58 | INFO | fairseq.trainer | begin training epoch 93
2022-03-06 19:59:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:01:09 | INFO | train_inner | epoch 093:     25 / 49 loss=3.46, ppl=11.01, wps=21842, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4500, lr=0.000471405, gnorm=1.192, loss_scale=8, train_wall=263, gb_free=21.5, wall=13339
2022-03-06 20:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:02:22 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.699 | ppl 1661.98 | wps 38577.2 | wpb 510.9 | bsz 1 | num_updates 4524 | best_loss 8.608
2022-03-06 20:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 4524 updates
2022-03-06 20:02:22 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-06 20:02:22 | INFO | train | epoch 093 | loss 3.419 | ppl 10.7 | wps 21986.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4524 | lr 0.000470152 | gnorm 1.204 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13412
2022-03-06 20:02:22 | INFO | fairseq.trainer | begin training epoch 94
2022-03-06 20:02:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:04:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:04:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:04:46 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.772 | ppl 1748.34 | wps 39131.5 | wpb 510.9 | bsz 1 | num_updates 4572 | best_loss 8.608
2022-03-06 20:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 4572 updates
2022-03-06 20:04:46 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-06 20:04:46 | INFO | train | epoch 094 | loss 3.375 | ppl 10.37 | wps 21607.1 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4572 | lr 0.000467678 | gnorm 1.173 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 13556
2022-03-06 20:04:46 | INFO | fairseq.trainer | begin training epoch 95
2022-03-06 20:04:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:06:06 | INFO | train_inner | epoch 095:     28 / 49 loss=3.377, ppl=10.39, wps=21887.2, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4600, lr=0.000466252, gnorm=1.191, loss_scale=8, train_wall=262, gb_free=21.5, wall=13635
2022-03-06 20:07:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:07:09 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.798 | ppl 1780.08 | wps 40469.7 | wpb 510.9 | bsz 1 | num_updates 4621 | best_loss 8.608
2022-03-06 20:07:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 4621 updates
2022-03-06 20:07:09 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-06 20:07:09 | INFO | train | epoch 095 | loss 3.334 | ppl 10.09 | wps 22240.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4621 | lr 0.000465192 | gnorm 1.161 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 13699
2022-03-06 20:07:09 | INFO | fairseq.trainer | begin training epoch 96
2022-03-06 20:07:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:09:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:09:32 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.928 | ppl 1947.69 | wps 40491.3 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 8.608
2022-03-06 20:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 4670 updates
2022-03-06 20:09:32 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-06 20:09:32 | INFO | train | epoch 096 | loss 3.294 | ppl 9.81 | wps 22356.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4670 | lr 0.000462745 | gnorm 1.117 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 13841
2022-03-06 20:09:32 | INFO | fairseq.trainer | begin training epoch 97
2022-03-06 20:09:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:10:56 | INFO | train_inner | epoch 097:     30 / 49 loss=3.29, ppl=9.78, wps=22340.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4700, lr=0.000461266, gnorm=1.145, loss_scale=16, train_wall=257, gb_free=21.5, wall=13926
2022-03-06 20:11:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:11:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:11:54 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.878 | ppl 1881.86 | wps 40533.5 | wpb 510.9 | bsz 1 | num_updates 4718 | best_loss 8.608
2022-03-06 20:11:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 4718 updates
2022-03-06 20:11:54 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-06 20:11:54 | INFO | train | epoch 097 | loss 3.259 | ppl 9.57 | wps 21882.6 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4718 | lr 0.000460385 | gnorm 1.172 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 13983
2022-03-06 20:11:54 | INFO | fairseq.trainer | begin training epoch 98
2022-03-06 20:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:14:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:14:17 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.93 | ppl 1951.29 | wps 40785.7 | wpb 510.9 | bsz 1 | num_updates 4767 | best_loss 8.608
2022-03-06 20:14:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 4767 updates
2022-03-06 20:14:17 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-06 20:14:17 | INFO | train | epoch 098 | loss 3.219 | ppl 9.31 | wps 22187.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4767 | lr 0.000458013 | gnorm 1.107 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14126
2022-03-06 20:14:17 | INFO | fairseq.trainer | begin training epoch 99
2022-03-06 20:14:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:15:50 | INFO | train_inner | epoch 099:     33 / 49 loss=3.217, ppl=9.3, wps=22057.3, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4800, lr=0.000456435, gnorm=1.13, loss_scale=8, train_wall=261, gb_free=21.5, wall=14220
2022-03-06 20:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:16:40 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 11.002 | ppl 2051.19 | wps 39145.6 | wpb 510.9 | bsz 1 | num_updates 4816 | best_loss 8.608
2022-03-06 20:16:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 4816 updates
2022-03-06 20:16:40 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-06 20:16:40 | INFO | train | epoch 099 | loss 3.188 | ppl 9.11 | wps 22224.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4816 | lr 0.000455677 | gnorm 1.128 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 14269
2022-03-06 20:16:40 | INFO | fairseq.trainer | begin training epoch 100
2022-03-06 20:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:18:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:19:04 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.991 | ppl 2035.85 | wps 40381 | wpb 510.9 | bsz 1 | num_updates 4865 | best_loss 8.608
2022-03-06 20:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 4865 updates
2022-03-06 20:19:04 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-06 20:19:04 | INFO | train | epoch 100 | loss 3.15 | ppl 8.88 | wps 22132.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4865 | lr 0.000453376 | gnorm 1.111 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 14413
2022-03-06 20:19:04 | INFO | fairseq.trainer | begin training epoch 101
2022-03-06 20:19:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:20:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:20:45 | INFO | train_inner | epoch 101:     36 / 49 loss=3.143, ppl=8.84, wps=21992.5, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4900, lr=0.000451754, gnorm=1.103, loss_scale=8, train_wall=261, gb_free=21.5, wall=14515
2022-03-06 20:21:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:21:27 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 11.013 | ppl 2066.45 | wps 39074.4 | wpb 510.9 | bsz 1 | num_updates 4913 | best_loss 8.608
2022-03-06 20:21:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 4913 updates
2022-03-06 20:21:27 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-06 20:21:27 | INFO | train | epoch 101 | loss 3.117 | ppl 8.67 | wps 21772.9 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4913 | lr 0.000451156 | gnorm 1.117 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 14556
2022-03-06 20:21:27 | INFO | fairseq.trainer | begin training epoch 102
2022-03-06 20:21:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:23:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:23:50 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 11.052 | ppl 2122.8 | wps 40407.2 | wpb 510.9 | bsz 1 | num_updates 4962 | best_loss 8.608
2022-03-06 20:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 4962 updates
2022-03-06 20:23:50 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-06 20:23:50 | INFO | train | epoch 102 | loss 3.085 | ppl 8.49 | wps 22112.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4962 | lr 0.000448923 | gnorm 1.049 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14700
2022-03-06 20:23:50 | INFO | fairseq.trainer | begin training epoch 103
2022-03-06 20:23:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:25:38 | INFO | train_inner | epoch 103:     38 / 49 loss=3.081, ppl=8.46, wps=22192.2, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=5000, lr=0.000447214, gnorm=1.096, loss_scale=8, train_wall=259, gb_free=21.5, wall=14807
2022-03-06 20:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:26:13 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 11.063 | ppl 2139.36 | wps 38834.1 | wpb 510.9 | bsz 1 | num_updates 5011 | best_loss 8.608
2022-03-06 20:26:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 5011 updates
2022-03-06 20:26:13 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-06 20:26:13 | INFO | train | epoch 103 | loss 3.058 | ppl 8.33 | wps 22254.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5011 | lr 0.000446722 | gnorm 1.101 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 14842
2022-03-06 20:26:13 | INFO | fairseq.trainer | begin training epoch 104
2022-03-06 20:26:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:27:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:28:37 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 11.122 | ppl 2228.39 | wps 39219.4 | wpb 510.9 | bsz 1 | num_updates 5059 | best_loss 8.608
2022-03-06 20:28:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 5059 updates
2022-03-06 20:28:37 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-06 20:28:37 | INFO | train | epoch 104 | loss 3.026 | ppl 8.15 | wps 21621.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5059 | lr 0.000444598 | gnorm 1.046 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14986
2022-03-06 20:28:37 | INFO | fairseq.trainer | begin training epoch 105
2022-03-06 20:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:30:34 | INFO | train_inner | epoch 105:     41 / 49 loss=3.019, ppl=8.1, wps=21882.9, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=5100, lr=0.000442807, gnorm=1.044, loss_scale=8, train_wall=262, gb_free=21.5, wall=15103
2022-03-06 20:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:31:01 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 11.131 | ppl 2243.26 | wps 40417.3 | wpb 510.9 | bsz 1 | num_updates 5108 | best_loss 8.608
2022-03-06 20:31:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 5108 updates
2022-03-06 20:31:01 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-06 20:31:01 | INFO | train | epoch 105 | loss 3 | ppl 8 | wps 22118.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5108 | lr 0.000442461 | gnorm 1.055 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15130
2022-03-06 20:31:01 | INFO | fairseq.trainer | begin training epoch 106
2022-03-06 20:31:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:33:24 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 11.153 | ppl 2277.43 | wps 39164 | wpb 510.9 | bsz 1 | num_updates 5157 | best_loss 8.608
2022-03-06 20:33:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 5157 updates
2022-03-06 20:33:24 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-06 20:33:24 | INFO | train | epoch 106 | loss 2.975 | ppl 7.86 | wps 22256.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5157 | lr 0.000440353 | gnorm 1.082 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 15273
2022-03-06 20:33:24 | INFO | fairseq.trainer | begin training epoch 107
2022-03-06 20:33:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:34:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:35:28 | INFO | train_inner | epoch 107:     44 / 49 loss=2.965, ppl=7.81, wps=22032.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=5200, lr=0.000438529, gnorm=1.048, loss_scale=8, train_wall=261, gb_free=21.5, wall=15398
2022-03-06 20:35:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:35:47 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 11.174 | ppl 2310.97 | wps 40445.7 | wpb 510.9 | bsz 1 | num_updates 5205 | best_loss 8.608
2022-03-06 20:35:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 5205 updates
2022-03-06 20:35:47 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-06 20:35:47 | INFO | train | epoch 107 | loss 2.942 | ppl 7.69 | wps 21753.5 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 5205 | lr 0.000438318 | gnorm 0.995 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15416
2022-03-06 20:35:47 | INFO | fairseq.trainer | begin training epoch 108
2022-03-06 20:35:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:38:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:38:09 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 11.212 | ppl 2371.72 | wps 39137.9 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 8.608
2022-03-06 20:38:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 5254 updates
2022-03-06 20:38:09 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-06 20:38:09 | INFO | train | epoch 108 | loss 2.922 | ppl 7.58 | wps 22288 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5254 | lr 0.00043627 | gnorm 1.022 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 15559
2022-03-06 20:38:09 | INFO | fairseq.trainer | begin training epoch 109
2022-03-06 20:38:09 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
