Sender: LSF System <lsfadmin@eu-g3-072>
Subject: Job 206448861: <w103_fp16_size_0.5_jelinek_0.038_0.002_0.96_#5> in cluster <euler> Exited

Job <w103_fp16_size_0.5_jelinek_0.038_0.002_0.96_#5> was submitted from host <eu-login-45> by user <andriusb> in cluster <euler> at Fri Feb 25 06:43:58 2022
Job was executed on host(s) <eu-g3-072>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 25 06:44:27 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 25 06:44:27 2022
Terminated at Sun Feb 27 15:16:44 2022
Results reported at Sun Feb 27 15:16:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.038, 0.002, 0.96)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321675 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   203440.39 sec.
    Max Memory :                                 11967 MB
    Average Memory :                             4134.27 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               8033.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   203536 sec.
    Turnaround time :                            203566 sec.

The output (if any) follows:

2022-02-25 06:44:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321675, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321675, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.038, 0.002, 0.96)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-25 06:44:39 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-25 06:44:46 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
Calculating frequency stats:
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 683/900675 [00:00<02:11, 6827.94it/s]  0%|          | 1366/900675 [00:00<02:27, 6109.37it/s]  0%|          | 1982/900675 [00:00<02:32, 5884.94it/s]  0%|          | 2606/900675 [00:00<02:29, 6013.93it/s]  0%|          | 3284/900675 [00:00<02:22, 6277.58it/s]  0%|          | 3967/900675 [00:00<02:18, 6457.31it/s]  1%|          | 4679/900675 [00:00<02:14, 6665.45it/s]  1%|          | 5464/900675 [00:00<02:07, 7036.93it/s]  1%|          | 6170/900675 [00:00<02:09, 6931.75it/s]  1%|          | 6865/900675 [00:01<02:22, 6287.82it/s]  1%|          | 7536/900675 [00:01<02:19, 6405.57it/s]  1%|          | 8186/900675 [00:01<02:22, 6251.84it/s]  1%|          | 8818/900675 [00:01<02:23, 6201.56it/s]  1%|          | 9466/900675 [00:01<02:22, 6274.03it/s]  1%|          | 10139/900675 [00:01<02:19, 6399.23it/s]  1%|          | 10782/900675 [00:01<02:21, 6275.43it/s]  1%|▏         | 11412/900675 [00:01<02:22, 6237.20it/s]  1%|▏         | 12106/900675 [00:01<02:17, 6440.69it/s]  1%|▏         | 12752/900675 [00:02<02:22, 6237.86it/s]  1%|▏         | 13422/900675 [00:02<02:19, 6371.12it/s]  2%|▏         | 14091/900675 [00:02<02:17, 6453.63it/s]  2%|▏         | 14755/900675 [00:02<02:16, 6507.85it/s]  2%|▏         | 15408/900675 [00:02<02:16, 6492.73it/s]  2%|▏         | 16059/900675 [00:02<02:22, 6219.17it/s]  2%|▏         | 16684/900675 [00:02<02:23, 6168.06it/s]  2%|▏         | 17348/900675 [00:02<02:20, 6304.62it/s]  2%|▏         | 17989/900675 [00:02<02:19, 6335.38it/s]  2%|▏         | 18624/900675 [00:02<02:23, 6158.98it/s]  2%|▏         | 19480/900675 [00:03<02:08, 6835.25it/s]  2%|▏         | 20167/900675 [00:03<02:15, 6505.40it/s]  2%|▏         | 20823/900675 [00:03<02:17, 6379.67it/s]  2%|▏         | 21465/900675 [00:03<02:23, 6106.42it/s]  2%|▏         | 22121/900675 [00:03<02:21, 6222.41it/s]  3%|▎         | 22823/900675 [00:03<02:16, 6448.26it/s]  3%|▎         | 23494/900675 [00:03<02:14, 6523.92it/s]  3%|▎         | 24283/900675 [00:03<02:06, 6914.83it/s]  3%|▎         | 25027/900675 [00:03<02:04, 7061.27it/s]  3%|▎         | 25736/900675 [00:03<02:06, 6943.47it/s]  3%|▎         | 26433/900675 [00:04<02:13, 6526.85it/s]  3%|▎         | 27092/900675 [00:04<02:17, 6337.17it/s]  3%|▎         | 27731/900675 [00:04<02:21, 6179.07it/s]  3%|▎         | 28417/900675 [00:04<02:17, 6366.62it/s]  3%|▎         | 29139/900675 [00:04<02:11, 6609.15it/s]  3%|▎         | 29804/900675 [00:04<02:15, 6429.84it/s]  3%|▎         | 30451/900675 [00:04<02:17, 6351.45it/s]  3%|▎         | 31089/900675 [00:04<02:27, 5907.59it/s]  4%|▎         | 31794/900675 [00:04<02:19, 6221.95it/s]  4%|▎         | 32424/900675 [00:05<02:24, 6025.52it/s]  4%|▎         | 33033/900675 [00:05<02:25, 5953.08it/s]  4%|▎         | 33647/900675 [00:05<02:24, 6005.33it/s]  4%|▍         | 34251/900675 [00:05<02:24, 5990.76it/s]  4%|▍         | 35001/900675 [00:05<02:14, 6424.02it/s]  4%|▍         | 35647/900675 [00:05<02:19, 6221.48it/s]  4%|▍         | 36277/900675 [00:05<02:18, 6241.68it/s]  4%|▍         | 36904/900675 [00:05<02:21, 6118.00it/s]  4%|▍         | 37518/900675 [00:05<02:27, 5863.27it/s]  4%|▍         | 38129/900675 [00:06<02:25, 5931.23it/s]  4%|▍         | 38752/900675 [00:06<02:23, 6016.81it/s]  4%|▍         | 39356/900675 [00:06<02:23, 6006.20it/s]  4%|▍         | 40009/900675 [00:06<02:19, 6152.57it/s]  5%|▍         | 40726/900675 [00:06<02:13, 6451.42it/s]  5%|▍         | 41373/900675 [00:06<02:14, 6391.68it/s]  5%|▍         | 42014/900675 [00:06<02:22, 6015.40it/s]  5%|▍         | 42621/900675 [00:06<02:23, 5996.85it/s]  5%|▍         | 43225/900675 [00:06<02:25, 5900.33it/s]  5%|▍         | 43924/900675 [00:06<02:18, 6206.43it/s]  5%|▍         | 44611/900675 [00:07<02:13, 6389.94it/s]  5%|▌         | 45253/900675 [00:07<02:17, 6241.02it/s]  5%|▌         | 45972/900675 [00:07<02:11, 6515.76it/s]  5%|▌         | 46661/900675 [00:07<02:09, 6619.41it/s]  5%|▌         | 47592/900675 [00:07<01:55, 7410.43it/s]  5%|▌         | 48339/900675 [00:07<01:54, 7419.25it/s]  5%|▌         | 49083/900675 [00:07<01:56, 7324.11it/s]  6%|▌         | 49817/900675 [00:07<01:58, 7198.55it/s]  6%|▌         | 50539/900675 [00:07<02:06, 6705.79it/s]  6%|▌         | 51217/900675 [00:08<02:09, 6569.17it/s]  6%|▌         | 51884/900675 [00:08<02:08, 6595.73it/s]  6%|▌         | 52696/900675 [00:08<02:00, 7028.41it/s]  6%|▌         | 53404/900675 [00:08<02:07, 6641.28it/s]  6%|▌         | 54075/900675 [00:08<02:08, 6587.15it/s]  6%|▌         | 54739/900675 [00:08<02:14, 6272.26it/s]  6%|▌         | 55372/900675 [00:08<02:14, 6262.73it/s]  6%|▌         | 56136/900675 [00:08<02:07, 6646.52it/s]  6%|▋         | 56806/900675 [00:08<02:14, 6252.81it/s]  6%|▋         | 57439/900675 [00:09<02:20, 6021.98it/s]  6%|▋         | 58096/900675 [00:09<02:16, 6171.86it/s]  7%|▋         | 58861/900675 [00:09<02:07, 6587.89it/s]  7%|▋         | 59526/900675 [00:09<02:14, 6250.42it/s]  7%|▋         | 60158/900675 [00:09<02:15, 6210.29it/s]  7%|▋         | 60901/900675 [00:09<02:08, 6554.84it/s]  7%|▋         | 61591/900675 [00:09<02:06, 6654.23it/s]  7%|▋         | 62261/900675 [00:09<02:09, 6477.36it/s]  7%|▋         | 62928/900675 [00:09<02:08, 6531.23it/s]  7%|▋         | 63584/900675 [00:09<02:11, 6384.05it/s]  7%|▋         | 64225/900675 [00:10<02:13, 6246.93it/s]  7%|▋         | 64921/900675 [00:10<02:09, 6448.23it/s]  7%|▋         | 65568/900675 [00:10<02:13, 6244.05it/s]  7%|▋         | 66229/900675 [00:10<02:11, 6340.25it/s]  7%|▋         | 66957/900675 [00:10<02:06, 6613.10it/s]  8%|▊         | 67621/900675 [00:10<02:13, 6251.91it/s]  8%|▊         | 68252/900675 [00:10<02:16, 6096.13it/s]  8%|▊         | 69014/900675 [00:10<02:07, 6522.80it/s]  8%|▊         | 69680/900675 [00:10<02:06, 6561.68it/s]  8%|▊         | 70344/900675 [00:10<02:06, 6579.77it/s]  8%|▊         | 71005/900675 [00:11<02:10, 6379.30it/s]  8%|▊         | 71650/900675 [00:11<02:09, 6390.40it/s]  8%|▊         | 72300/900675 [00:11<02:09, 6421.12it/s]  8%|▊         | 73095/900675 [00:11<02:00, 6870.75it/s]  8%|▊         | 73785/900675 [00:11<02:00, 6855.14it/s]  8%|▊         | 74585/900675 [00:11<01:54, 7194.14it/s]  8%|▊         | 75306/900675 [00:11<01:59, 6898.63it/s]  8%|▊         | 76000/900675 [00:11<02:02, 6741.75it/s]  9%|▊         | 76677/900675 [00:11<02:02, 6742.31it/s]  9%|▊         | 77354/900675 [00:12<02:03, 6642.72it/s]  9%|▊         | 78029/900675 [00:12<02:03, 6671.02it/s]  9%|▊         | 78698/900675 [00:12<02:09, 6324.82it/s]  9%|▉         | 79335/900675 [00:12<02:18, 5918.37it/s]  9%|▉         | 79967/900675 [00:12<02:16, 6021.54it/s]  9%|▉         | 80575/900675 [00:12<02:17, 5978.63it/s]  9%|▉         | 81332/900675 [00:12<02:07, 6424.61it/s]  9%|▉         | 81980/900675 [00:12<02:10, 6260.93it/s]  9%|▉         | 82641/900675 [00:12<02:08, 6355.62it/s]  9%|▉         | 83358/900675 [00:12<02:04, 6587.84it/s]  9%|▉         | 84047/900675 [00:13<02:02, 6674.00it/s]  9%|▉         | 84717/900675 [00:13<02:02, 6648.00it/s]  9%|▉         | 85384/900675 [00:13<02:08, 6351.29it/s] 10%|▉         | 86058/900675 [00:13<02:06, 6457.11it/s] 10%|▉         | 86732/900675 [00:13<02:04, 6539.21it/s] 10%|▉         | 87436/900675 [00:13<02:01, 6684.21it/s] 10%|▉         | 88115/900675 [00:13<02:01, 6713.98it/s] 10%|▉         | 88788/900675 [00:13<02:10, 6243.33it/s] 10%|▉         | 89420/900675 [00:13<02:12, 6138.35it/s] 10%|▉         | 90039/900675 [00:14<02:13, 6071.39it/s] 10%|█         | 90656/900675 [00:14<02:12, 6095.61it/s] 10%|█         | 91331/900675 [00:14<02:08, 6276.22it/s] 10%|█         | 91988/900675 [00:14<02:07, 6351.35it/s] 10%|█         | 92710/900675 [00:14<02:02, 6599.31it/s] 10%|█         | 93372/900675 [00:14<02:07, 6331.37it/s] 10%|█         | 94009/900675 [00:14<02:08, 6272.24it/s] 11%|█         | 94747/900675 [00:14<02:02, 6590.04it/s] 11%|█         | 95409/900675 [00:14<02:04, 6475.82it/s] 11%|█         | 96059/900675 [00:14<02:04, 6448.19it/s] 11%|█         | 96916/900675 [00:15<01:53, 7062.91it/s] 11%|█         | 97625/900675 [00:15<02:01, 6632.33it/s] 11%|█         | 98318/900675 [00:15<01:59, 6714.90it/s] 11%|█         | 98995/900675 [00:15<02:03, 6498.26it/s] 11%|█         | 99650/900675 [00:15<02:06, 6308.87it/s] 11%|█         | 100285/900675 [00:15<02:08, 6218.08it/s] 11%|█         | 100919/900675 [00:15<02:07, 6251.58it/s] 11%|█▏        | 101546/900675 [00:15<02:08, 6243.06it/s] 11%|█▏        | 102226/900675 [00:15<02:04, 6404.42it/s] 11%|█▏        | 102868/900675 [00:16<02:05, 6377.57it/s] 11%|█▏        | 103507/900675 [00:16<02:12, 6025.44it/s] 12%|█▏        | 104114/900675 [00:16<02:15, 5858.75it/s] 12%|█▏        | 104811/900675 [00:16<02:08, 6169.72it/s] 12%|█▏        | 105433/900675 [00:16<02:08, 6177.07it/s] 12%|█▏        | 106076/900675 [00:16<02:07, 6242.12it/s] 12%|█▏        | 106704/900675 [00:16<02:07, 6249.22it/s] 12%|█▏        | 107331/900675 [00:16<02:10, 6085.91it/s] 12%|█▏        | 108006/900675 [00:16<02:06, 6274.50it/s] 12%|█▏        | 108636/900675 [00:16<02:10, 6064.51it/s] 12%|█▏        | 109245/900675 [00:17<02:12, 5973.80it/s] 12%|█▏        | 109902/900675 [00:17<02:08, 6143.37it/s] 12%|█▏        | 110566/900675 [00:17<02:05, 6283.31it/s] 12%|█▏        | 111268/900675 [00:17<02:01, 6497.51it/s] 12%|█▏        | 111920/900675 [00:17<02:02, 6435.19it/s] 13%|█▎        | 112606/900675 [00:17<02:00, 6555.26it/s] 13%|█▎        | 113263/900675 [00:17<02:03, 6362.07it/s] 13%|█▎        | 113901/900675 [00:17<02:05, 6249.60it/s] 13%|█▎        | 114528/900675 [00:17<02:05, 6252.32it/s] 13%|█▎        | 115166/900675 [00:18<02:04, 6286.20it/s] 13%|█▎        | 115796/900675 [00:18<02:10, 6014.78it/s] 13%|█▎        | 116401/900675 [00:18<02:11, 5979.82it/s] 13%|█▎        | 117013/900675 [00:18<02:10, 6014.48it/s] 13%|█▎        | 117621/900675 [00:18<02:09, 6031.21it/s] 13%|█▎        | 118242/900675 [00:18<02:08, 6077.38it/s] 13%|█▎        | 118906/900675 [00:18<02:05, 6236.31it/s] 13%|█▎        | 119531/900675 [00:18<02:07, 6136.33it/s] 13%|█▎        | 120315/900675 [00:18<01:57, 6632.52it/s] 13%|█▎        | 120980/900675 [00:18<02:00, 6496.09it/s] 14%|█▎        | 121632/900675 [00:19<02:05, 6225.75it/s] 14%|█▎        | 122286/900675 [00:19<02:03, 6310.34it/s] 14%|█▎        | 122920/900675 [00:19<02:04, 6247.26it/s] 14%|█▎        | 123547/900675 [00:19<02:04, 6223.76it/s] 14%|█▍        | 124171/900675 [00:19<02:05, 6192.03it/s] 14%|█▍        | 124791/900675 [00:19<02:06, 6132.41it/s] 14%|█▍        | 125432/900675 [00:19<02:04, 6213.28it/s] 14%|█▍        | 126172/900675 [00:19<01:58, 6563.22it/s] 14%|█▍        | 126872/900675 [00:19<01:55, 6687.71it/s] 14%|█▍        | 127542/900675 [00:19<02:01, 6387.16it/s] 14%|█▍        | 128196/900675 [00:20<02:00, 6422.66it/s] 14%|█▍        | 128841/900675 [00:20<02:01, 6349.27it/s] 14%|█▍        | 129525/900675 [00:20<01:58, 6486.93it/s] 14%|█▍        | 130176/900675 [00:20<02:06, 6103.47it/s] 15%|█▍        | 130792/900675 [00:20<02:06, 6095.69it/s] 15%|█▍        | 131431/900675 [00:20<02:04, 6179.18it/s] 15%|█▍        | 132052/900675 [00:20<02:14, 5721.44it/s] 15%|█▍        | 132778/900675 [00:20<02:04, 6147.99it/s] 15%|█▍        | 133402/900675 [00:20<02:04, 6144.05it/s] 15%|█▍        | 134123/900675 [00:21<01:58, 6448.73it/s] 15%|█▍        | 134862/900675 [00:21<01:53, 6718.34it/s] 15%|█▌        | 135610/900675 [00:21<01:50, 6939.43it/s] 15%|█▌        | 136308/900675 [00:21<01:53, 6728.41it/s] 15%|█▌        | 137150/900675 [00:21<01:45, 7217.94it/s] 15%|█▌        | 137877/900675 [00:21<01:51, 6862.09it/s] 15%|█▌        | 138570/900675 [00:21<01:51, 6813.17it/s] 15%|█▌        | 139256/900675 [00:21<01:55, 6580.38it/s] 16%|█▌        | 139918/900675 [00:21<01:55, 6581.46it/s] 16%|█▌        | 140579/900675 [00:22<01:58, 6434.68it/s] 16%|█▌        | 141226/900675 [00:22<01:57, 6440.19it/s] 16%|█▌        | 141872/900675 [00:22<02:00, 6322.15it/s] 16%|█▌        | 142506/900675 [00:22<02:00, 6310.73it/s] 16%|█▌        | 143138/900675 [00:22<02:06, 5995.10it/s] 16%|█▌        | 143964/900675 [00:22<01:54, 6636.99it/s] 16%|█▌        | 144634/900675 [00:22<01:53, 6640.21it/s] 16%|█▌        | 145348/900675 [00:22<01:51, 6785.56it/s] 16%|█▌        | 146038/900675 [00:22<01:50, 6818.97it/s] 16%|█▋        | 146723/900675 [00:22<01:56, 6466.74it/s] 16%|█▋        | 147375/900675 [00:23<02:00, 6236.19it/s] 16%|█▋        | 148004/900675 [00:23<02:03, 6111.94it/s] 17%|█▋        | 148639/900675 [00:23<02:01, 6177.96it/s] 17%|█▋        | 149271/900675 [00:23<02:00, 6215.19it/s] 17%|█▋        | 149895/900675 [00:23<02:01, 6184.40it/s] 17%|█▋        | 150515/900675 [00:23<02:08, 5834.09it/s] 17%|█▋        | 151103/900675 [00:23<02:08, 5837.18it/s] 17%|█▋        | 151725/900675 [00:23<02:06, 5940.64it/s] 17%|█▋        | 152322/900675 [00:23<02:07, 5858.79it/s] 17%|█▋        | 153015/900675 [00:24<02:01, 6166.40it/s] 17%|█▋        | 153640/900675 [00:24<02:00, 6186.58it/s] 17%|█▋        | 154261/900675 [00:24<02:00, 6190.67it/s] 17%|█▋        | 154892/900675 [00:24<01:59, 6225.07it/s] 17%|█▋        | 155561/900675 [00:24<01:57, 6363.10it/s] 17%|█▋        | 156261/900675 [00:24<01:53, 6550.86it/s] 17%|█▋        | 156917/900675 [00:24<01:56, 6381.74it/s] 18%|█▊        | 157642/900675 [00:24<01:51, 6636.35it/s] 18%|█▊        | 158362/900675 [00:24<01:49, 6800.35it/s] 18%|█▊        | 159044/900675 [00:24<01:55, 6393.96it/s] 18%|█▊        | 159690/900675 [00:25<01:55, 6392.92it/s] 18%|█▊        | 160334/900675 [00:25<01:55, 6387.99it/s] 18%|█▊        | 161074/900675 [00:25<01:50, 6682.08it/s] 18%|█▊        | 161745/900675 [00:25<01:57, 6265.09it/s] 18%|█▊        | 162430/900675 [00:25<01:54, 6427.38it/s] 18%|█▊        | 163150/900675 [00:25<01:50, 6648.72it/s] 18%|█▊        | 163820/900675 [00:25<01:57, 6276.17it/s] 18%|█▊        | 164455/900675 [00:25<01:56, 6292.66it/s] 18%|█▊        | 165160/900675 [00:25<01:53, 6505.89it/s] 18%|█▊        | 165876/900675 [00:25<01:49, 6692.67it/s] 18%|█▊        | 166550/900675 [00:26<01:49, 6689.46it/s] 19%|█▊        | 167258/900675 [00:26<01:47, 6793.31it/s] 19%|█▊        | 167940/900675 [00:26<01:54, 6393.08it/s] 19%|█▊        | 168613/900675 [00:26<01:52, 6480.40it/s] 19%|█▉        | 169266/900675 [00:26<01:53, 6433.17it/s] 19%|█▉        | 169913/900675 [00:26<01:56, 6286.84it/s] 19%|█▉        | 170545/900675 [00:26<01:59, 6129.60it/s] 19%|█▉        | 171161/900675 [00:26<02:01, 6016.04it/s] 19%|█▉        | 171848/900675 [00:26<01:56, 6259.83it/s] 19%|█▉        | 172477/900675 [00:27<01:57, 6174.02it/s] 19%|█▉        | 173099/900675 [00:27<01:57, 6181.20it/s] 19%|█▉        | 173748/900675 [00:27<01:55, 6267.42it/s] 19%|█▉        | 174376/900675 [00:27<01:58, 6125.81it/s] 19%|█▉        | 175105/900675 [00:27<01:52, 6463.62it/s] 20%|█▉        | 175754/900675 [00:27<01:57, 6166.19it/s] 20%|█▉        | 176391/900675 [00:27<01:56, 6215.48it/s] 20%|█▉        | 177016/900675 [00:27<01:59, 6072.11it/s] 20%|█▉        | 177626/900675 [00:27<02:08, 5622.62it/s] 20%|█▉        | 178288/900675 [00:28<02:02, 5892.74it/s] 20%|█▉        | 178955/900675 [00:28<01:58, 6111.94it/s] 20%|█▉        | 179613/900675 [00:28<01:55, 6241.25it/s] 20%|██        | 180243/900675 [00:28<01:58, 6066.11it/s] 20%|██        | 180875/900675 [00:28<01:57, 6137.76it/s] 20%|██        | 181493/900675 [00:28<02:00, 5992.81it/s] 20%|██        | 182118/900675 [00:28<01:58, 6063.26it/s] 20%|██        | 182727/900675 [00:28<01:58, 6052.10it/s] 20%|██        | 183355/900675 [00:28<01:57, 6115.68it/s] 20%|██        | 183968/900675 [00:28<02:04, 5760.08it/s] 20%|██        | 184562/900675 [00:29<02:03, 5806.01it/s] 21%|██        | 185208/900675 [00:29<01:59, 5984.55it/s] 21%|██        | 185810/900675 [00:29<02:02, 5824.30it/s] 21%|██        | 186396/900675 [00:29<02:03, 5784.95it/s] 21%|██        | 187097/900675 [00:29<01:56, 6136.47it/s] 21%|██        | 187731/900675 [00:29<01:55, 6194.17it/s] 21%|██        | 188358/900675 [00:29<01:54, 6210.78it/s] 21%|██        | 189026/900675 [00:29<01:52, 6347.29it/s] 21%|██        | 189689/900675 [00:29<01:50, 6423.03it/s] 21%|██        | 190333/900675 [00:29<01:51, 6383.85it/s] 21%|██        | 190987/900675 [00:30<01:50, 6425.84it/s] 21%|██▏       | 191631/900675 [00:30<01:52, 6318.38it/s] 21%|██▏       | 192306/900675 [00:30<01:49, 6443.44it/s] 21%|██▏       | 192952/900675 [00:30<01:50, 6423.19it/s] 21%|██▏       | 193604/900675 [00:30<01:49, 6449.13it/s] 22%|██▏       | 194259/900675 [00:30<01:49, 6478.63it/s] 22%|██▏       | 194908/900675 [00:30<01:50, 6398.86it/s] 22%|██▏       | 195566/900675 [00:30<01:49, 6450.79it/s] 22%|██▏       | 196212/900675 [00:30<01:52, 6270.28it/s] 22%|██▏       | 196841/900675 [00:30<01:52, 6256.87it/s] 22%|██▏       | 197815/900675 [00:31<01:36, 7278.95it/s] 22%|██▏       | 198546/900675 [00:31<01:45, 6652.74it/s] 22%|██▏       | 199224/900675 [00:31<01:46, 6578.53it/s] 22%|██▏       | 199890/900675 [00:31<01:51, 6265.14it/s] 22%|██▏       | 200524/900675 [00:31<01:56, 6001.45it/s] 22%|██▏       | 201164/900675 [00:31<01:54, 6106.97it/s] 22%|██▏       | 201780/900675 [00:31<01:55, 6061.64it/s] 22%|██▏       | 202399/900675 [00:31<01:54, 6094.21it/s] 23%|██▎       | 203061/900675 [00:31<01:51, 6242.95it/s] 23%|██▎       | 203795/900675 [00:32<01:46, 6556.08it/s] 23%|██▎       | 204454/900675 [00:32<01:50, 6275.13it/s] 23%|██▎       | 205086/900675 [00:32<01:52, 6194.88it/s] 23%|██▎       | 205709/900675 [00:32<01:52, 6159.53it/s] 23%|██▎       | 206399/900675 [00:32<01:48, 6373.48it/s] 23%|██▎       | 207039/900675 [00:32<01:52, 6178.39it/s] 23%|██▎       | 207660/900675 [00:32<01:53, 6105.48it/s] 23%|██▎       | 208273/900675 [00:32<01:55, 6002.21it/s] 23%|██▎       | 208875/900675 [00:32<01:59, 5810.43it/s] 23%|██▎       | 209459/900675 [00:33<01:58, 5817.81it/s] 23%|██▎       | 210190/900675 [00:33<01:50, 6248.89it/s] 23%|██▎       | 210848/900675 [00:33<01:48, 6338.64it/s] 23%|██▎       | 211500/900675 [00:33<01:47, 6389.63it/s] 24%|██▎       | 212141/900675 [00:33<01:50, 6208.17it/s] 24%|██▎       | 212899/900675 [00:33<01:44, 6607.09it/s] 24%|██▎       | 213563/900675 [00:33<01:49, 6256.82it/s] 24%|██▍       | 214194/900675 [00:33<01:50, 6200.00it/s] 24%|██▍       | 214818/900675 [00:33<01:54, 5990.35it/s] 24%|██▍       | 215443/900675 [00:33<01:53, 6060.71it/s] 24%|██▍       | 216055/900675 [00:34<01:52, 6075.62it/s] 24%|██▍       | 216665/900675 [00:34<01:53, 6034.46it/s] 24%|██▍       | 217271/900675 [00:34<01:53, 6040.40it/s] 24%|██▍       | 217966/900675 [00:34<01:48, 6307.94it/s] 24%|██▍       | 218613/900675 [00:34<01:47, 6355.53it/s] 24%|██▍       | 219299/900675 [00:34<01:44, 6503.70it/s] 24%|██▍       | 219951/900675 [00:34<01:48, 6278.50it/s] 24%|██▍       | 220581/900675 [00:34<01:51, 6088.60it/s] 25%|██▍       | 221293/900675 [00:34<01:46, 6383.95it/s] 25%|██▍       | 221935/900675 [00:35<01:50, 6120.07it/s] 25%|██▍       | 222551/900675 [00:35<01:56, 5844.24it/s] 25%|██▍       | 223197/900675 [00:35<01:52, 6013.37it/s] 25%|██▍       | 223803/900675 [00:35<01:57, 5757.36it/s] 25%|██▍       | 224544/900675 [00:35<01:48, 6217.66it/s] 25%|██▌       | 225198/900675 [00:35<01:47, 6308.51it/s] 25%|██▌       | 225988/900675 [00:35<01:39, 6765.63it/s] 25%|██▌       | 226670/900675 [00:35<01:42, 6584.60it/s] 25%|██▌       | 227333/900675 [00:35<01:43, 6506.06it/s] 25%|██▌       | 227987/900675 [00:35<01:47, 6276.40it/s] 25%|██▌       | 228618/900675 [00:36<01:49, 6124.65it/s] 25%|██▌       | 229233/900675 [00:36<01:49, 6123.35it/s] 26%|██▌       | 229847/900675 [00:36<01:56, 5758.03it/s] 26%|██▌       | 230501/900675 [00:36<01:52, 5973.26it/s] 26%|██▌       | 231146/900675 [00:36<01:49, 6102.91it/s] 26%|██▌       | 231761/900675 [00:36<01:52, 5969.47it/s] 26%|██▌       | 232381/900675 [00:36<01:50, 6034.84it/s] 26%|██▌       | 233123/900675 [00:36<01:43, 6431.47it/s] 26%|██▌       | 233832/900675 [00:36<01:40, 6613.24it/s] 26%|██▌       | 234565/900675 [00:37<01:37, 6821.44it/s] 26%|██▌       | 235271/900675 [00:37<01:36, 6890.14it/s] 26%|██▌       | 235962/900675 [00:37<01:39, 6681.67it/s] 26%|██▋       | 236633/900675 [00:37<01:46, 6241.26it/s] 26%|██▋       | 237264/900675 [00:37<01:46, 6219.69it/s] 26%|██▋       | 237891/900675 [00:37<01:48, 6107.43it/s] 26%|██▋       | 238671/900675 [00:37<01:40, 6585.28it/s] 27%|██▋       | 239335/900675 [00:37<01:40, 6571.45it/s] 27%|██▋       | 239996/900675 [00:37<01:46, 6180.17it/s] 27%|██▋       | 240808/900675 [00:37<01:38, 6722.00it/s] 27%|██▋       | 241489/900675 [00:38<01:38, 6690.89it/s] 27%|██▋       | 242167/900675 [00:38<01:38, 6708.02it/s] 27%|██▋       | 242842/900675 [00:38<01:40, 6514.28it/s] 27%|██▋       | 243498/900675 [00:38<01:40, 6525.93it/s] 27%|██▋       | 244154/900675 [00:38<01:41, 6438.56it/s] 27%|██▋       | 244800/900675 [00:38<01:41, 6436.67it/s] 27%|██▋       | 245445/900675 [00:38<01:42, 6382.71it/s] 27%|██▋       | 246159/900675 [00:38<01:39, 6594.72it/s] 27%|██▋       | 246993/900675 [00:38<01:31, 7108.74it/s] 28%|██▊       | 247706/900675 [00:39<01:35, 6819.39it/s] 28%|██▊       | 248392/900675 [00:39<01:37, 6670.32it/s] 28%|██▊       | 249097/900675 [00:39<01:36, 6779.20it/s] 28%|██▊       | 249778/900675 [00:39<01:38, 6608.63it/s] 28%|██▊       | 250442/900675 [00:39<01:41, 6420.31it/s] 28%|██▊       | 251122/900675 [00:39<01:39, 6519.09it/s] 28%|██▊       | 251776/900675 [00:39<01:39, 6490.05it/s] 28%|██▊       | 252427/900675 [00:39<01:39, 6495.29it/s] 28%|██▊       | 253078/900675 [00:39<01:43, 6281.32it/s] 28%|██▊       | 253779/900675 [00:39<01:39, 6485.66it/s] 28%|██▊       | 254430/900675 [00:40<01:41, 6341.13it/s] 28%|██▊       | 255084/900675 [00:40<01:40, 6398.13it/s] 28%|██▊       | 255733/900675 [00:40<01:40, 6422.68it/s] 28%|██▊       | 256402/900675 [00:40<01:39, 6488.27it/s] 29%|██▊       | 257069/900675 [00:40<01:38, 6540.75it/s] 29%|██▊       | 257790/900675 [00:40<01:35, 6738.79it/s] 29%|██▊       | 258465/900675 [00:40<01:47, 5997.86it/s] 29%|██▉       | 259081/900675 [00:40<01:47, 5956.36it/s] 29%|██▉       | 259706/900675 [00:40<01:46, 6037.57it/s] 29%|██▉       | 260416/900675 [00:41<01:40, 6339.77it/s] 29%|██▉       | 261117/900675 [00:41<01:37, 6526.70it/s] 29%|██▉       | 261776/900675 [00:41<01:39, 6441.11it/s] 29%|██▉       | 262425/900675 [00:41<01:43, 6164.01it/s] 29%|██▉       | 263047/900675 [00:41<01:43, 6135.44it/s] 29%|██▉       | 263701/900675 [00:41<01:41, 6245.08it/s] 29%|██▉       | 264329/900675 [00:41<01:46, 5989.79it/s] 29%|██▉       | 265010/900675 [00:41<01:42, 6223.10it/s] 29%|██▉       | 265637/900675 [00:41<01:43, 6154.19it/s] 30%|██▉       | 266256/900675 [00:41<01:45, 6027.18it/s] 30%|██▉       | 266861/900675 [00:42<01:45, 6009.55it/s] 30%|██▉       | 267485/900675 [00:42<01:44, 6073.15it/s] 30%|██▉       | 268117/900675 [00:42<01:42, 6141.69it/s] 30%|██▉       | 268764/900675 [00:42<01:41, 6226.74it/s] 30%|██▉       | 269445/900675 [00:42<01:38, 6388.82it/s] 30%|██▉       | 270085/900675 [00:42<01:40, 6271.77it/s] 30%|███       | 270738/900675 [00:42<01:39, 6345.59it/s] 30%|███       | 271374/900675 [00:42<01:41, 6223.10it/s] 30%|███       | 272077/900675 [00:42<01:37, 6451.24it/s] 30%|███       | 272908/900675 [00:42<01:29, 6991.05it/s] 30%|███       | 273609/900675 [00:43<01:30, 6921.38it/s] 30%|███       | 274303/900675 [00:43<01:32, 6746.68it/s] 31%|███       | 274980/900675 [00:43<01:38, 6358.72it/s] 31%|███       | 275621/900675 [00:43<01:41, 6174.89it/s] 31%|███       | 276243/900675 [00:43<01:42, 6116.91it/s] 31%|███       | 276858/900675 [00:43<01:43, 6017.48it/s] 31%|███       | 277462/900675 [00:43<01:43, 6021.20it/s] 31%|███       | 278138/900675 [00:43<01:39, 6230.07it/s] 31%|███       | 278840/900675 [00:43<01:36, 6459.63it/s] 31%|███       | 279488/900675 [00:44<01:38, 6287.92it/s] 31%|███       | 280161/900675 [00:44<01:36, 6410.97it/s] 31%|███       | 280804/900675 [00:44<01:41, 6120.24it/s] 31%|███       | 281447/900675 [00:44<01:39, 6208.47it/s] 31%|███▏      | 282119/900675 [00:44<01:37, 6345.41it/s] 31%|███▏      | 282757/900675 [00:44<01:41, 6093.96it/s] 31%|███▏      | 283479/900675 [00:44<01:36, 6410.59it/s] 32%|███▏      | 284141/900675 [00:44<01:35, 6471.05it/s] 32%|███▏      | 284792/900675 [00:44<01:36, 6354.27it/s] 32%|███▏      | 285430/900675 [00:45<01:38, 6258.54it/s] 32%|███▏      | 286101/900675 [00:45<01:36, 6379.98it/s] 32%|███▏      | 286741/900675 [00:45<01:39, 6153.82it/s] 32%|███▏      | 287359/900675 [00:45<01:40, 6083.16it/s] 32%|███▏      | 288096/900675 [00:45<01:34, 6449.83it/s] 32%|███▏      | 288744/900675 [00:45<01:36, 6369.75it/s] 32%|███▏      | 289383/900675 [00:45<01:35, 6367.92it/s] 32%|███▏      | 290022/900675 [00:45<01:40, 6048.12it/s] 32%|███▏      | 290657/900675 [00:45<01:39, 6133.93it/s] 32%|███▏      | 291282/900675 [00:45<01:38, 6164.15it/s] 32%|███▏      | 291920/900675 [00:46<01:37, 6222.81it/s] 32%|███▏      | 292582/900675 [00:46<01:36, 6331.37it/s] 33%|███▎      | 293217/900675 [00:46<01:37, 6243.26it/s] 33%|███▎      | 293843/900675 [00:46<01:39, 6088.08it/s] 33%|███▎      | 294544/900675 [00:46<01:35, 6349.24it/s] 33%|███▎      | 295226/900675 [00:46<01:33, 6479.97it/s] 33%|███▎      | 295876/900675 [00:46<01:33, 6448.44it/s] 33%|███▎      | 296522/900675 [00:46<01:35, 6328.47it/s] 33%|███▎      | 297178/900675 [00:46<01:34, 6394.49it/s] 33%|███▎      | 297878/900675 [00:46<01:31, 6571.36it/s] 33%|███▎      | 298537/900675 [00:47<01:34, 6367.24it/s] 33%|███▎      | 299176/900675 [00:47<01:39, 6040.93it/s] 33%|███▎      | 299915/900675 [00:47<01:33, 6420.77it/s] 33%|███▎      | 300563/900675 [00:47<01:35, 6316.51it/s] 33%|███▎      | 301199/900675 [00:47<01:37, 6133.07it/s] 34%|███▎      | 301816/900675 [00:47<01:40, 5942.57it/s] 34%|███▎      | 302499/900675 [00:47<01:36, 6192.75it/s] 34%|███▎      | 303185/900675 [00:47<01:33, 6383.84it/s] 34%|███▎      | 303851/900675 [00:47<01:32, 6463.02it/s] 34%|███▍      | 304500/900675 [00:48<01:34, 6291.64it/s] 34%|███▍      | 305149/900675 [00:48<01:33, 6348.93it/s] 34%|███▍      | 305786/900675 [00:48<01:36, 6179.07it/s] 34%|███▍      | 306431/900675 [00:48<01:35, 6232.91it/s] 34%|███▍      | 307056/900675 [00:48<01:36, 6173.39it/s] 34%|███▍      | 307777/900675 [00:48<01:31, 6474.94it/s] 34%|███▍      | 308427/900675 [00:48<01:41, 5821.85it/s] 34%|███▍      | 309260/900675 [00:48<01:30, 6501.19it/s] 34%|███▍      | 310022/900675 [00:48<01:26, 6812.58it/s] 34%|███▍      | 310716/900675 [00:49<01:29, 6589.40it/s] 35%|███▍      | 311385/900675 [00:49<01:31, 6450.20it/s] 35%|███▍      | 312037/900675 [00:49<01:34, 6258.03it/s] 35%|███▍      | 312681/900675 [00:49<01:33, 6308.40it/s] 35%|███▍      | 313341/900675 [00:49<01:31, 6388.16it/s] 35%|███▍      | 314086/900675 [00:49<01:27, 6691.17it/s] 35%|███▍      | 314759/900675 [00:49<01:31, 6418.23it/s] 35%|███▌      | 315405/900675 [00:49<01:35, 6150.19it/s] 35%|███▌      | 316045/900675 [00:49<01:33, 6220.06it/s] 35%|███▌      | 316671/900675 [00:49<01:35, 6116.76it/s] 35%|███▌      | 317286/900675 [00:50<01:35, 6083.95it/s] 35%|███▌      | 317897/900675 [00:50<01:39, 5867.43it/s] 35%|███▌      | 318536/900675 [00:50<01:36, 6013.72it/s] 35%|███▌      | 319203/900675 [00:50<01:33, 6201.74it/s] 36%|███▌      | 319843/900675 [00:50<01:32, 6256.58it/s] 36%|███▌      | 320471/900675 [00:50<01:36, 6032.04it/s] 36%|███▌      | 321211/900675 [00:50<01:30, 6424.45it/s] 36%|███▌      | 321857/900675 [00:50<01:33, 6222.22it/s] 36%|███▌      | 322483/900675 [00:50<01:35, 6063.64it/s] 36%|███▌      | 323112/900675 [00:51<01:34, 6127.63it/s] 36%|███▌      | 323753/900675 [00:51<01:32, 6205.08it/s] 36%|███▌      | 324407/900675 [00:51<01:31, 6301.34it/s] 36%|███▌      | 325039/900675 [00:51<01:31, 6288.35it/s] 36%|███▌      | 325669/900675 [00:51<01:36, 5965.72it/s] 36%|███▌      | 326272/900675 [00:51<01:36, 5978.36it/s] 36%|███▋      | 327019/900675 [00:51<01:29, 6403.22it/s] 36%|███▋      | 327763/900675 [00:51<01:25, 6705.59it/s] 36%|███▋      | 328528/900675 [00:51<01:21, 6982.48it/s] 37%|███▋      | 329229/900675 [00:51<01:25, 6682.20it/s] 37%|███▋      | 329902/900675 [00:52<01:27, 6514.53it/s] 37%|███▋      | 330557/900675 [00:52<01:35, 5992.37it/s] 37%|███▋      | 331236/900675 [00:52<01:31, 6208.41it/s] 37%|███▋      | 331866/900675 [00:52<01:33, 6110.64it/s] 37%|███▋      | 332565/900675 [00:52<01:29, 6357.46it/s] 37%|███▋      | 333207/900675 [00:52<01:31, 6186.79it/s] 37%|███▋      | 333890/900675 [00:52<01:29, 6367.08it/s] 37%|███▋      | 334531/900675 [00:52<01:30, 6281.60it/s] 37%|███▋      | 335184/900675 [00:52<01:29, 6349.31it/s] 37%|███▋      | 335840/900675 [00:53<01:28, 6407.03it/s] 37%|███▋      | 336483/900675 [00:53<01:28, 6380.86it/s] 37%|███▋      | 337176/900675 [00:53<01:26, 6542.77it/s] 38%|███▊      | 337873/900675 [00:53<01:24, 6667.02it/s] 38%|███▊      | 338541/900675 [00:53<01:25, 6582.41it/s] 38%|███▊      | 339201/900675 [00:53<01:26, 6485.88it/s] 38%|███▊      | 339851/900675 [00:53<01:29, 6274.79it/s] 38%|███▊      | 340481/900675 [00:53<01:33, 5990.22it/s] 38%|███▊      | 341098/900675 [00:53<01:32, 6037.58it/s] 38%|███▊      | 341768/900675 [00:53<01:29, 6220.71it/s] 38%|███▊      | 342449/900675 [00:54<01:27, 6391.82it/s] 38%|███▊      | 343096/900675 [00:54<01:26, 6413.60it/s] 38%|███▊      | 343844/900675 [00:54<01:22, 6724.59it/s] 38%|███▊      | 344519/900675 [00:54<01:29, 6223.69it/s] 38%|███▊      | 345233/900675 [00:54<01:25, 6473.82it/s] 38%|███▊      | 345888/900675 [00:54<01:33, 5965.01it/s] 38%|███▊      | 346618/900675 [00:54<01:27, 6327.77it/s] 39%|███▊      | 347263/900675 [00:54<01:28, 6275.14it/s] 39%|███▊      | 347905/900675 [00:54<01:27, 6311.38it/s] 39%|███▊      | 348542/900675 [00:55<01:30, 6119.36it/s] 39%|███▉      | 349176/900675 [00:55<01:29, 6171.22it/s] 39%|███▉      | 349797/900675 [00:55<01:29, 6140.86it/s] 39%|███▉      | 350680/900675 [00:55<01:19, 6916.16it/s] 39%|███▉      | 351376/900675 [00:55<01:22, 6667.24it/s] 39%|███▉      | 352048/900675 [00:55<01:26, 6322.14it/s] 39%|███▉      | 352687/900675 [00:55<01:33, 5838.62it/s] 39%|███▉      | 353343/900675 [00:55<01:30, 6029.07it/s] 39%|███▉      | 353955/900675 [00:55<01:32, 5890.30it/s] 39%|███▉      | 354741/900675 [00:56<01:24, 6438.27it/s] 39%|███▉      | 355394/900675 [00:56<01:29, 6090.17it/s] 40%|███▉      | 356097/900675 [00:56<01:25, 6344.01it/s] 40%|███▉      | 356740/900675 [00:56<01:28, 6175.31it/s] 40%|███▉      | 357364/900675 [00:56<01:33, 5795.64it/s] 40%|███▉      | 358062/900675 [00:56<01:28, 6115.39it/s] 40%|███▉      | 358688/900675 [00:56<01:28, 6154.91it/s] 40%|███▉      | 359328/900675 [00:56<01:26, 6222.49it/s] 40%|███▉      | 359955/900675 [00:56<01:26, 6216.41it/s] 40%|████      | 360675/900675 [00:56<01:23, 6496.72it/s] 40%|████      | 361360/900675 [00:57<01:21, 6592.72it/s] 40%|████      | 362103/900675 [00:57<01:18, 6839.79it/s] 40%|████      | 362789/900675 [00:57<01:18, 6829.49it/s] 40%|████      | 363474/900675 [00:57<01:20, 6698.05it/s] 40%|████      | 364149/900675 [00:57<01:19, 6710.41it/s] 41%|████      | 364843/900675 [00:57<01:19, 6772.85it/s] 41%|████      | 365528/900675 [00:57<01:18, 6792.79it/s] 41%|████      | 366222/900675 [00:57<01:18, 6832.32it/s] 41%|████      | 366906/900675 [00:57<01:23, 6426.86it/s] 41%|████      | 367580/900675 [00:58<01:21, 6515.79it/s] 41%|████      | 368236/900675 [00:58<01:28, 6032.91it/s] 41%|████      | 368937/900675 [00:58<01:24, 6303.19it/s] 41%|████      | 369592/900675 [00:58<01:23, 6366.63it/s] 41%|████      | 370254/900675 [00:58<01:22, 6439.54it/s] 41%|████      | 370903/900675 [00:58<01:24, 6236.11it/s] 41%|████▏     | 371531/900675 [00:58<01:24, 6242.64it/s] 41%|████▏     | 372159/900675 [00:58<01:26, 6093.01it/s] 41%|████▏     | 372803/900675 [00:58<01:25, 6190.92it/s] 41%|████▏     | 373455/900675 [00:58<01:23, 6281.46it/s] 42%|████▏     | 374085/900675 [00:59<01:24, 6217.28it/s] 42%|████▏     | 374819/900675 [00:59<01:20, 6537.57it/s] 42%|████▏     | 375475/900675 [00:59<01:23, 6254.38it/s] 42%|████▏     | 376104/900675 [00:59<01:25, 6100.09it/s] 42%|████▏     | 376717/900675 [00:59<01:28, 5909.92it/s] 42%|████▏     | 377352/900675 [00:59<01:26, 6033.80it/s] 42%|████▏     | 377995/900675 [00:59<01:25, 6139.30it/s] 42%|████▏     | 378649/900675 [00:59<01:23, 6252.67it/s] 42%|████▏     | 379305/900675 [00:59<01:22, 6341.96it/s] 42%|████▏     | 380040/900675 [00:59<01:18, 6639.44it/s] 42%|████▏     | 380706/900675 [01:00<01:19, 6565.19it/s] 42%|████▏     | 381431/900675 [01:00<01:16, 6762.16it/s] 42%|████▏     | 382109/900675 [01:00<01:23, 6231.68it/s] 42%|████▏     | 382741/900675 [01:00<01:25, 6037.71it/s] 43%|████▎     | 383397/900675 [01:00<01:23, 6176.41it/s] 43%|████▎     | 384021/900675 [01:00<01:23, 6159.18it/s] 43%|████▎     | 384749/900675 [01:00<01:19, 6481.49it/s] 43%|████▎     | 385402/900675 [01:00<01:20, 6398.07it/s] 43%|████▎     | 386045/900675 [01:00<01:22, 6271.58it/s] 43%|████▎     | 386675/900675 [01:01<01:22, 6263.11it/s] 43%|████▎     | 387478/900675 [01:01<01:15, 6778.02it/s] 43%|████▎     | 388159/900675 [01:01<01:17, 6627.69it/s] 43%|████▎     | 388825/900675 [01:01<01:20, 6389.95it/s] 43%|████▎     | 389527/900675 [01:01<01:17, 6567.32it/s] 43%|████▎     | 390223/900675 [01:01<01:16, 6675.38it/s] 43%|████▎     | 390893/900675 [01:01<01:18, 6470.02it/s] 43%|████▎     | 391543/900675 [01:01<01:22, 6194.60it/s] 44%|████▎     | 392166/900675 [01:01<01:24, 5995.50it/s] 44%|████▎     | 392796/900675 [01:02<01:23, 6074.86it/s] 44%|████▎     | 393407/900675 [01:02<01:25, 5941.09it/s] 44%|████▍     | 394066/900675 [01:02<01:22, 6124.13it/s] 44%|████▍     | 394710/900675 [01:02<01:21, 6213.14it/s] 44%|████▍     | 395341/900675 [01:02<01:20, 6239.84it/s] 44%|████▍     | 396021/900675 [01:02<01:18, 6401.06it/s] 44%|████▍     | 396663/900675 [01:02<01:24, 5974.08it/s] 44%|████▍     | 397267/900675 [01:02<01:26, 5850.29it/s] 44%|████▍     | 397892/900675 [01:02<01:24, 5963.07it/s] 44%|████▍     | 398633/900675 [01:02<01:18, 6372.51it/s] 44%|████▍     | 399275/900675 [01:03<01:19, 6323.15it/s] 44%|████▍     | 400051/900675 [01:03<01:14, 6740.89it/s] 44%|████▍     | 400729/900675 [01:03<01:15, 6647.39it/s] 45%|████▍     | 401397/900675 [01:03<01:17, 6449.22it/s] 45%|████▍     | 402045/900675 [01:03<01:23, 5974.51it/s] 45%|████▍     | 402651/900675 [01:03<01:25, 5791.53it/s] 45%|████▍     | 403284/900675 [01:03<01:23, 5937.12it/s] 45%|████▍     | 403891/900675 [01:03<01:23, 5971.59it/s] 45%|████▍     | 404588/900675 [01:03<01:19, 6257.33it/s] 45%|████▍     | 405218/900675 [01:04<01:22, 5988.12it/s] 45%|████▌     | 405838/900675 [01:04<01:21, 6045.95it/s] 45%|████▌     | 406447/900675 [01:04<01:23, 5944.77it/s] 45%|████▌     | 407101/900675 [01:04<01:20, 6116.01it/s] 45%|████▌     | 407716/900675 [01:04<01:22, 5946.05it/s] 45%|████▌     | 408395/900675 [01:04<01:19, 6180.49it/s] 45%|████▌     | 409103/900675 [01:04<01:16, 6442.19it/s] 45%|████▌     | 409750/900675 [01:04<01:19, 6181.56it/s] 46%|████▌     | 410372/900675 [01:04<01:20, 6104.34it/s] 46%|████▌     | 411031/900675 [01:04<01:18, 6241.57it/s] 46%|████▌     | 411658/900675 [01:05<01:19, 6185.58it/s] 46%|████▌     | 412279/900675 [01:05<01:23, 5873.44it/s] 46%|████▌     | 412871/900675 [01:05<01:26, 5658.83it/s] 46%|████▌     | 413485/900675 [01:05<01:24, 5792.93it/s] 46%|████▌     | 414068/900675 [01:05<01:27, 5581.43it/s] 46%|████▌     | 414630/900675 [01:05<01:27, 5585.92it/s] 46%|████▌     | 415236/900675 [01:05<01:24, 5714.11it/s] 46%|████▌     | 415856/900675 [01:05<01:22, 5855.13it/s] 46%|████▌     | 416444/900675 [01:05<01:23, 5787.89it/s] 46%|████▋     | 417045/900675 [01:06<01:22, 5849.94it/s] 46%|████▋     | 417713/900675 [01:06<01:19, 6094.23it/s] 46%|████▋     | 418331/900675 [01:06<01:18, 6114.75it/s] 47%|████▋     | 418949/900675 [01:06<01:18, 6133.61it/s] 47%|████▋     | 419599/900675 [01:06<01:17, 6226.65it/s] 47%|████▋     | 420259/900675 [01:06<01:15, 6333.22it/s] 47%|████▋     | 420893/900675 [01:06<01:17, 6219.75it/s] 47%|████▋     | 421516/900675 [01:06<01:20, 5963.56it/s] 47%|████▋     | 422115/900675 [01:06<01:20, 5967.54it/s] 47%|████▋     | 422823/900675 [01:06<01:16, 6278.21it/s] 47%|████▋     | 423453/900675 [01:07<01:16, 6257.17it/s] 47%|████▋     | 424081/900675 [01:07<01:17, 6150.02it/s] 47%|████▋     | 424698/900675 [01:07<01:18, 6079.01it/s] 47%|████▋     | 425307/900675 [01:07<01:18, 6027.60it/s] 47%|████▋     | 425984/900675 [01:07<01:16, 6243.69it/s] 47%|████▋     | 426756/900675 [01:07<01:10, 6677.78it/s] 47%|████▋     | 427426/900675 [01:07<01:12, 6521.82it/s] 48%|████▊     | 428080/900675 [01:07<01:14, 6334.10it/s] 48%|████▊     | 428836/900675 [01:07<01:10, 6677.65it/s] 48%|████▊     | 429507/900675 [01:07<01:11, 6631.27it/s] 48%|████▊     | 430172/900675 [01:08<01:10, 6627.95it/s] 48%|████▊     | 430837/900675 [01:08<01:13, 6382.00it/s] 48%|████▊     | 431478/900675 [01:08<01:16, 6100.93it/s] 48%|████▊     | 432092/900675 [01:08<01:17, 6023.13it/s] 48%|████▊     | 433017/900675 [01:08<01:07, 6939.66it/s] 48%|████▊     | 433718/900675 [01:08<01:10, 6611.00it/s] 48%|████▊     | 434387/900675 [01:08<01:14, 6249.06it/s] 48%|████▊     | 435020/900675 [01:08<01:16, 6124.78it/s] 48%|████▊     | 435734/900675 [01:08<01:12, 6403.41it/s] 48%|████▊     | 436381/900675 [01:09<01:15, 6125.00it/s] 49%|████▊     | 437036/900675 [01:09<01:14, 6236.49it/s] 49%|████▊     | 437713/900675 [01:09<01:12, 6380.40it/s] 49%|████▊     | 438356/900675 [01:09<01:14, 6227.94it/s] 49%|████▊     | 438982/900675 [01:09<01:15, 6150.55it/s] 49%|████▉     | 439600/900675 [01:09<01:17, 5981.49it/s] 49%|████▉     | 440201/900675 [01:09<01:17, 5916.96it/s] 49%|████▉     | 440837/900675 [01:09<01:16, 6040.57it/s] 49%|████▉     | 441504/900675 [01:09<01:13, 6220.82it/s] 49%|████▉     | 442144/900675 [01:10<01:13, 6271.44it/s] 49%|████▉     | 442773/900675 [01:10<01:14, 6167.33it/s] 49%|████▉     | 443485/900675 [01:10<01:11, 6434.66it/s] 49%|████▉     | 444172/900675 [01:10<01:09, 6559.50it/s] 49%|████▉     | 444830/900675 [01:10<01:17, 5918.31it/s] 49%|████▉     | 445435/900675 [01:10<01:16, 5939.21it/s] 50%|████▉     | 446040/900675 [01:10<01:16, 5968.84it/s] 50%|████▉     | 446644/900675 [01:10<01:16, 5912.30it/s] 50%|████▉     | 447344/900675 [01:10<01:12, 6225.76it/s] 50%|████▉     | 447971/900675 [01:10<01:13, 6164.65it/s] 50%|████▉     | 448675/900675 [01:11<01:10, 6405.53it/s] 50%|████▉     | 449319/900675 [01:11<01:11, 6332.97it/s] 50%|████▉     | 449955/900675 [01:11<01:14, 6040.17it/s] 50%|█████     | 450679/900675 [01:11<01:10, 6381.71it/s] 50%|█████     | 451327/900675 [01:11<01:10, 6405.79it/s] 50%|█████     | 451971/900675 [01:11<01:11, 6237.52it/s] 50%|█████     | 452598/900675 [01:11<01:14, 6009.02it/s] 50%|█████     | 453288/900675 [01:11<01:11, 6259.31it/s] 50%|█████     | 453918/900675 [01:11<01:13, 6067.67it/s] 50%|█████     | 454528/900675 [01:12<01:14, 5988.52it/s] 51%|█████     | 455129/900675 [01:12<01:14, 5984.32it/s] 51%|█████     | 455747/900675 [01:12<01:13, 6037.55it/s] 51%|█████     | 456433/900675 [01:12<01:10, 6278.00it/s] 51%|█████     | 457174/900675 [01:12<01:07, 6606.53it/s] 51%|█████     | 457837/900675 [01:12<01:07, 6514.12it/s] 51%|█████     | 458490/900675 [01:12<01:11, 6202.80it/s] 51%|█████     | 459178/900675 [01:12<01:09, 6395.84it/s] 51%|█████     | 459855/900675 [01:12<01:07, 6492.91it/s] 51%|█████     | 460508/900675 [01:12<01:11, 6193.67it/s] 51%|█████     | 461132/900675 [01:13<01:11, 6175.89it/s] 51%|█████▏    | 461790/900675 [01:13<01:09, 6292.01it/s] 51%|█████▏    | 462422/900675 [01:13<01:12, 6028.22it/s] 51%|█████▏    | 463075/900675 [01:13<01:10, 6168.95it/s] 51%|█████▏    | 463696/900675 [01:13<01:11, 6135.72it/s] 52%|█████▏    | 464345/900675 [01:13<01:09, 6235.71it/s] 52%|█████▏    | 464990/900675 [01:13<01:09, 6295.40it/s] 52%|█████▏    | 465621/900675 [01:13<01:10, 6174.41it/s] 52%|█████▏    | 466332/900675 [01:13<01:07, 6443.38it/s] 52%|█████▏    | 466978/900675 [01:14<01:08, 6322.59it/s] 52%|█████▏    | 467621/900675 [01:14<01:08, 6353.15it/s] 52%|█████▏    | 468258/900675 [01:14<01:09, 6195.20it/s] 52%|█████▏    | 468929/900675 [01:14<01:08, 6340.33it/s] 52%|█████▏    | 469565/900675 [01:14<01:09, 6173.96it/s] 52%|█████▏    | 470185/900675 [01:14<01:11, 6007.81it/s] 52%|█████▏    | 470926/900675 [01:14<01:07, 6407.74it/s] 52%|█████▏    | 471595/900675 [01:14<01:06, 6476.38it/s] 52%|█████▏    | 472245/900675 [01:14<01:09, 6130.35it/s] 53%|█████▎    | 472911/900675 [01:14<01:08, 6275.10it/s] 53%|█████▎    | 473562/900675 [01:15<01:07, 6341.52it/s] 53%|█████▎    | 474200/900675 [01:15<01:09, 6103.42it/s] 53%|█████▎    | 474936/900675 [01:15<01:05, 6456.80it/s] 53%|█████▎    | 475653/900675 [01:15<01:03, 6663.27it/s] 53%|█████▎    | 476324/900675 [01:15<01:06, 6362.92it/s] 53%|█████▎    | 476966/900675 [01:15<01:08, 6200.40it/s] 53%|█████▎    | 477666/900675 [01:15<01:05, 6418.71it/s] 53%|█████▎    | 478334/900675 [01:15<01:05, 6490.31it/s] 53%|█████▎    | 479084/900675 [01:15<01:02, 6783.86it/s] 53%|█████▎    | 479766/900675 [01:16<01:03, 6606.84it/s] 53%|█████▎    | 480430/900675 [01:16<01:06, 6332.29it/s] 53%|█████▎    | 481067/900675 [01:16<01:07, 6206.01it/s] 53%|█████▎    | 481691/900675 [01:16<01:10, 5957.51it/s] 54%|█████▎    | 482300/900675 [01:16<01:09, 5994.21it/s] 54%|█████▎    | 482974/900675 [01:16<01:07, 6203.47it/s] 54%|█████▎    | 483597/900675 [01:16<01:09, 6019.93it/s] 54%|█████▍    | 484275/900675 [01:16<01:06, 6228.98it/s] 54%|█████▍    | 485030/900675 [01:16<01:02, 6601.85it/s] 54%|█████▍    | 485725/900675 [01:16<01:01, 6702.99it/s] 54%|█████▍    | 486398/900675 [01:17<01:04, 6383.78it/s] 54%|█████▍    | 487041/900675 [01:17<01:08, 6051.73it/s] 54%|█████▍    | 487761/900675 [01:17<01:04, 6367.85it/s] 54%|█████▍    | 488415/900675 [01:17<01:04, 6415.72it/s] 54%|█████▍    | 489062/900675 [01:17<01:05, 6248.64it/s] 54%|█████▍    | 489691/900675 [01:17<01:06, 6197.92it/s] 54%|█████▍    | 490314/900675 [01:17<01:08, 6018.96it/s] 55%|█████▍    | 490928/900675 [01:17<01:07, 6052.63it/s] 55%|█████▍    | 491596/900675 [01:17<01:05, 6233.12it/s] 55%|█████▍    | 492273/900675 [01:18<01:03, 6388.75it/s] 55%|█████▍    | 492914/900675 [01:18<01:04, 6288.45it/s] 55%|█████▍    | 493545/900675 [01:18<01:06, 6077.17it/s] 55%|█████▍    | 494214/900675 [01:18<01:05, 6248.08it/s] 55%|█████▍    | 494956/900675 [01:18<01:01, 6586.05it/s] 55%|█████▌    | 495618/900675 [01:18<01:03, 6426.77it/s] 55%|█████▌    | 496270/900675 [01:18<01:02, 6448.89it/s] 55%|█████▌    | 496917/900675 [01:18<01:03, 6338.68it/s] 55%|█████▌    | 497603/900675 [01:18<01:02, 6489.34it/s] 55%|█████▌    | 498254/900675 [01:18<01:04, 6205.86it/s] 55%|█████▌    | 498978/900675 [01:19<01:01, 6497.33it/s] 55%|█████▌    | 499677/900675 [01:19<01:00, 6637.68it/s] 56%|█████▌    | 500347/900675 [01:19<01:00, 6655.59it/s] 56%|█████▌    | 501028/900675 [01:19<00:59, 6694.62it/s] 56%|█████▌    | 501700/900675 [01:19<01:03, 6325.34it/s] 56%|█████▌    | 502338/900675 [01:19<01:03, 6247.92it/s] 56%|█████▌    | 502967/900675 [01:19<01:03, 6222.72it/s] 56%|█████▌    | 503633/900675 [01:19<01:02, 6348.14it/s] 56%|█████▌    | 504270/900675 [01:19<01:05, 6054.70it/s] 56%|█████▌    | 504905/900675 [01:20<01:04, 6129.90it/s] 56%|█████▌    | 505595/900675 [01:20<01:02, 6346.79it/s] 56%|█████▌    | 506265/900675 [01:20<01:01, 6449.69it/s] 56%|█████▋    | 506925/900675 [01:20<01:00, 6488.17it/s] 56%|█████▋    | 507576/900675 [01:20<01:03, 6159.21it/s] 56%|█████▋    | 508197/900675 [01:20<01:06, 5936.23it/s] 56%|█████▋    | 508795/900675 [01:20<01:06, 5924.09it/s] 57%|█████▋    | 509536/900675 [01:20<01:01, 6345.93it/s] 57%|█████▋    | 510193/900675 [01:20<01:00, 6406.80it/s] 57%|█████▋    | 510837/900675 [01:20<01:01, 6356.77it/s] 57%|█████▋    | 511678/900675 [01:21<00:55, 6957.95it/s] 57%|█████▋    | 512377/900675 [01:21<01:00, 6412.51it/s] 57%|█████▋    | 513117/900675 [01:21<00:57, 6688.38it/s] 57%|█████▋    | 513795/900675 [01:21<00:59, 6501.37it/s] 57%|█████▋    | 514452/900675 [01:21<01:00, 6380.16it/s] 57%|█████▋    | 515095/900675 [01:21<01:01, 6315.73it/s] 57%|█████▋    | 515730/900675 [01:21<01:01, 6278.83it/s] 57%|█████▋    | 516414/900675 [01:21<00:59, 6440.02it/s] 57%|█████▋    | 517061/900675 [01:21<01:02, 6145.98it/s] 57%|█████▋    | 517729/900675 [01:22<01:00, 6294.83it/s] 58%|█████▊    | 518431/900675 [01:22<00:58, 6500.33it/s] 58%|█████▊    | 519085/900675 [01:22<01:01, 6231.26it/s] 58%|█████▊    | 519821/900675 [01:22<00:58, 6550.04it/s] 58%|█████▊    | 520481/900675 [01:22<01:00, 6238.55it/s] 58%|█████▊    | 521199/900675 [01:22<00:58, 6500.15it/s] 58%|█████▊    | 521855/900675 [01:22<00:58, 6470.89it/s] 58%|█████▊    | 522506/900675 [01:22<00:59, 6407.27it/s] 58%|█████▊    | 523150/900675 [01:22<01:00, 6192.46it/s] 58%|█████▊    | 523779/900675 [01:22<01:00, 6206.68it/s] 58%|█████▊    | 524402/900675 [01:23<01:01, 6158.99it/s] 58%|█████▊    | 525020/900675 [01:23<01:02, 5978.03it/s] 58%|█████▊    | 525620/900675 [01:23<01:06, 5630.45it/s] 58%|█████▊    | 526349/900675 [01:23<01:01, 6089.95it/s] 59%|█████▊    | 527133/900675 [01:23<00:56, 6586.93it/s] 59%|█████▊    | 527799/900675 [01:23<01:01, 6055.13it/s] 59%|█████▊    | 528492/900675 [01:23<00:59, 6288.72it/s] 59%|█████▉    | 529177/900675 [01:23<00:57, 6445.76it/s] 59%|█████▉    | 529831/900675 [01:23<00:58, 6292.91it/s] 59%|█████▉    | 530467/900675 [01:24<01:01, 6002.41it/s] 59%|█████▉    | 531096/900675 [01:24<01:00, 6082.13it/s] 59%|█████▉    | 531737/900675 [01:24<00:59, 6172.59it/s] 59%|█████▉    | 532359/900675 [01:24<01:00, 6130.21it/s] 59%|█████▉    | 533053/900675 [01:24<00:57, 6360.23it/s] 59%|█████▉    | 533692/900675 [01:24<00:59, 6142.02it/s] 59%|█████▉    | 534310/900675 [01:24<00:59, 6148.29it/s] 59%|█████▉    | 534928/900675 [01:24<00:59, 6146.90it/s] 59%|█████▉    | 535562/900675 [01:24<00:58, 6198.62it/s] 60%|█████▉    | 536184/900675 [01:25<00:59, 6123.87it/s] 60%|█████▉    | 536847/900675 [01:25<00:58, 6270.06it/s] 60%|█████▉    | 537475/900675 [01:25<01:00, 6035.52it/s] 60%|█████▉    | 538156/900675 [01:25<00:57, 6255.80it/s] 60%|█████▉    | 538899/900675 [01:25<00:54, 6595.20it/s] 60%|█████▉    | 539562/900675 [01:25<00:57, 6318.48it/s] 60%|█████▉    | 540199/900675 [01:25<00:56, 6330.16it/s] 60%|██████    | 540845/900675 [01:25<00:56, 6354.49it/s] 60%|██████    | 541483/900675 [01:25<00:59, 6021.41it/s] 60%|██████    | 542130/900675 [01:25<00:58, 6147.65it/s] 60%|██████    | 542831/900675 [01:26<00:55, 6395.91it/s] 60%|██████    | 543496/900675 [01:26<00:55, 6466.55it/s] 60%|██████    | 544146/900675 [01:26<00:55, 6418.88it/s] 60%|██████    | 544790/900675 [01:26<00:57, 6186.03it/s] 61%|██████    | 545438/900675 [01:26<00:56, 6267.70it/s] 61%|██████    | 546068/900675 [01:26<00:56, 6266.31it/s] 61%|██████    | 546730/900675 [01:26<00:55, 6366.86it/s] 61%|██████    | 547369/900675 [01:26<00:57, 6132.95it/s] 61%|██████    | 548107/900675 [01:26<00:54, 6487.61it/s] 61%|██████    | 548801/900675 [01:26<00:53, 6618.67it/s] 61%|██████    | 549466/900675 [01:27<00:55, 6298.86it/s] 61%|██████    | 550145/900675 [01:27<00:54, 6434.68it/s] 61%|██████    | 550793/900675 [01:27<00:55, 6346.93it/s] 61%|██████    | 551462/900675 [01:27<00:54, 6436.55it/s] 61%|██████▏   | 552108/900675 [01:27<00:54, 6360.61it/s] 61%|██████▏   | 552746/900675 [01:27<00:56, 6162.66it/s] 61%|██████▏   | 553381/900675 [01:27<00:55, 6212.06it/s] 62%|██████▏   | 554013/900675 [01:27<00:55, 6242.48it/s] 62%|██████▏   | 554639/900675 [01:27<00:56, 6072.78it/s] 62%|██████▏   | 555248/900675 [01:28<00:56, 6071.67it/s] 62%|██████▏   | 555948/900675 [01:28<00:54, 6340.96it/s] 62%|██████▏   | 556615/900675 [01:28<00:53, 6436.50it/s] 62%|██████▏   | 557260/900675 [01:28<00:55, 6149.73it/s] 62%|██████▏   | 557896/900675 [01:28<00:55, 6209.99it/s] 62%|██████▏   | 558520/900675 [01:28<00:56, 6076.72it/s] 62%|██████▏   | 559130/900675 [01:28<00:58, 5801.20it/s] 62%|██████▏   | 559758/900675 [01:28<00:57, 5933.63it/s] 62%|██████▏   | 560365/900675 [01:28<00:56, 5971.33it/s] 62%|██████▏   | 560983/900675 [01:28<00:56, 6027.68it/s] 62%|██████▏   | 561702/900675 [01:29<00:53, 6367.02it/s] 62%|██████▏   | 562364/900675 [01:29<00:52, 6436.70it/s] 63%|██████▎   | 563073/900675 [01:29<00:50, 6627.53it/s] 63%|██████▎   | 563737/900675 [01:29<00:54, 6164.25it/s] 63%|██████▎   | 564383/900675 [01:29<00:53, 6244.53it/s] 63%|██████▎   | 565013/900675 [01:29<00:55, 6017.81it/s] 63%|██████▎   | 565727/900675 [01:29<00:52, 6329.15it/s] 63%|██████▎   | 566366/900675 [01:29<00:56, 5965.63it/s] 63%|██████▎   | 566970/900675 [01:29<00:56, 5929.36it/s] 63%|██████▎   | 567744/900675 [01:30<00:51, 6436.85it/s] 63%|██████▎   | 568394/900675 [01:30<00:51, 6402.10it/s] 63%|██████▎   | 569039/900675 [01:30<00:53, 6227.18it/s] 63%|██████▎   | 569666/900675 [01:30<00:53, 6169.28it/s] 63%|██████▎   | 570401/900675 [01:30<00:50, 6501.92it/s] 63%|██████▎   | 571199/900675 [01:30<00:47, 6932.18it/s] 63%|██████▎   | 571896/900675 [01:30<00:47, 6889.88it/s] 64%|██████▎   | 572588/900675 [01:30<00:49, 6596.86it/s] 64%|██████▎   | 573252/900675 [01:30<00:51, 6333.58it/s] 64%|██████▎   | 573907/900675 [01:31<00:51, 6392.79it/s] 64%|██████▍   | 574550/900675 [01:31<00:53, 6116.11it/s] 64%|██████▍   | 575166/900675 [01:31<00:54, 6012.80it/s] 64%|██████▍   | 575808/900675 [01:31<00:53, 6123.30it/s] 64%|██████▍   | 576423/900675 [01:31<00:53, 6073.05it/s] 64%|██████▍   | 577114/900675 [01:31<00:51, 6304.93it/s] 64%|██████▍   | 577747/900675 [01:31<00:51, 6254.15it/s] 64%|██████▍   | 578374/900675 [01:31<00:55, 5845.75it/s] 64%|██████▍   | 579019/900675 [01:31<00:53, 6014.77it/s] 64%|██████▍   | 579659/900675 [01:31<00:52, 6118.30it/s] 64%|██████▍   | 580356/900675 [01:32<00:50, 6360.28it/s] 65%|██████▍   | 580996/900675 [01:32<00:50, 6275.47it/s] 65%|██████▍   | 581689/900675 [01:32<00:49, 6463.60it/s] 65%|██████▍   | 582349/900675 [01:32<00:48, 6497.36it/s] 65%|██████▍   | 583036/900675 [01:32<00:48, 6597.01it/s] 65%|██████▍   | 583795/900675 [01:32<00:46, 6885.75it/s] 65%|██████▍   | 584570/900675 [01:32<00:44, 7131.57it/s] 65%|██████▍   | 585285/900675 [01:32<00:44, 7086.05it/s] 65%|██████▌   | 585995/900675 [01:32<00:46, 6696.39it/s] 65%|██████▌   | 586670/900675 [01:32<00:47, 6542.26it/s] 65%|██████▌   | 587328/900675 [01:33<00:52, 5979.99it/s] 65%|██████▌   | 588007/900675 [01:33<00:50, 6197.54it/s] 65%|██████▌   | 588637/900675 [01:33<00:50, 6129.26it/s] 65%|██████▌   | 589296/900675 [01:33<00:49, 6258.86it/s] 65%|██████▌   | 589935/900675 [01:33<00:49, 6296.19it/s] 66%|██████▌   | 590635/900675 [01:33<00:47, 6500.73it/s] 66%|██████▌   | 591289/900675 [01:33<00:49, 6304.26it/s] 66%|██████▌   | 591983/900675 [01:33<00:47, 6485.50it/s] 66%|██████▌   | 592635/900675 [01:33<00:50, 6146.03it/s] 66%|██████▌   | 593255/900675 [01:34<00:50, 6078.75it/s] 66%|██████▌   | 593881/900675 [01:34<00:50, 6127.62it/s] 66%|██████▌   | 594519/900675 [01:34<00:49, 6199.25it/s] 66%|██████▌   | 595179/900675 [01:34<00:48, 6311.96it/s] 66%|██████▌   | 595812/900675 [01:34<00:50, 6048.22it/s] 66%|██████▌   | 596471/900675 [01:34<00:49, 6202.93it/s] 66%|██████▋   | 597149/900675 [01:34<00:47, 6369.14it/s] 66%|██████▋   | 597789/900675 [01:34<00:48, 6209.75it/s] 66%|██████▋   | 598443/900675 [01:34<00:47, 6305.07it/s] 67%|██████▋   | 599076/900675 [01:35<00:48, 6186.01it/s] 67%|██████▋   | 599818/900675 [01:35<00:46, 6533.05it/s] 67%|██████▋   | 600474/900675 [01:35<00:47, 6345.39it/s] 67%|██████▋   | 601111/900675 [01:35<00:48, 6229.81it/s] 67%|██████▋   | 601736/900675 [01:35<00:51, 5860.17it/s] 67%|██████▋   | 602327/900675 [01:35<00:50, 5873.71it/s] 67%|██████▋   | 602980/900675 [01:35<00:49, 6056.35it/s] 67%|██████▋   | 603694/900675 [01:35<00:46, 6369.32it/s] 67%|██████▋   | 604335/900675 [01:35<00:47, 6267.70it/s] 67%|██████▋   | 604966/900675 [01:35<00:47, 6279.81it/s] 67%|██████▋   | 605596/900675 [01:36<00:48, 6051.98it/s] 67%|██████▋   | 606283/900675 [01:36<00:46, 6285.48it/s] 67%|██████▋   | 606937/900675 [01:36<00:46, 6356.10it/s] 67%|██████▋   | 607575/900675 [01:36<00:47, 6214.28it/s] 68%|██████▊   | 608219/900675 [01:36<00:46, 6271.18it/s] 68%|██████▊   | 608848/900675 [01:36<00:48, 5999.33it/s] 68%|██████▊   | 609452/900675 [01:36<00:48, 5982.43it/s] 68%|██████▊   | 610172/900675 [01:36<00:45, 6326.44it/s] 68%|██████▊   | 610892/900675 [01:36<00:44, 6573.50it/s] 68%|██████▊   | 611552/900675 [01:36<00:45, 6363.32it/s] 68%|██████▊   | 612192/900675 [01:37<00:45, 6371.21it/s] 68%|██████▊   | 612832/900675 [01:37<00:46, 6187.62it/s] 68%|██████▊   | 613453/900675 [01:37<00:47, 6064.14it/s] 68%|██████▊   | 614062/900675 [01:37<00:47, 6040.72it/s] 68%|██████▊   | 614668/900675 [01:37<00:49, 5812.90it/s] 68%|██████▊   | 615313/900675 [01:37<00:47, 5990.79it/s] 68%|██████▊   | 615915/900675 [01:37<00:48, 5868.20it/s] 68%|██████▊   | 616624/900675 [01:37<00:45, 6217.12it/s] 69%|██████▊   | 617432/900675 [01:37<00:41, 6757.40it/s] 69%|██████▊   | 618133/900675 [01:38<00:41, 6831.32it/s] 69%|██████▊   | 618825/900675 [01:38<00:41, 6856.81it/s] 69%|██████▉   | 619618/900675 [01:38<00:39, 7170.54it/s] 69%|██████▉   | 620407/900675 [01:38<00:37, 7383.85it/s] 69%|██████▉   | 621147/900675 [01:38<00:39, 7080.32it/s] 69%|██████▉   | 621859/900675 [01:38<00:42, 6618.03it/s] 69%|██████▉   | 622529/900675 [01:38<00:42, 6592.86it/s] 69%|██████▉   | 623282/900675 [01:38<00:40, 6851.42it/s] 69%|██████▉   | 623973/900675 [01:38<00:42, 6481.89it/s] 69%|██████▉   | 624628/900675 [01:39<00:43, 6274.84it/s] 69%|██████▉   | 625485/900675 [01:39<00:39, 6906.83it/s] 70%|██████▉   | 626184/900675 [01:39<00:41, 6686.58it/s] 70%|██████▉   | 626860/900675 [01:39<00:42, 6446.16it/s] 70%|██████▉   | 627542/900675 [01:39<00:41, 6548.81it/s] 70%|██████▉   | 628202/900675 [01:39<00:43, 6298.25it/s] 70%|██████▉   | 628856/900675 [01:39<00:42, 6364.34it/s] 70%|██████▉   | 629550/900675 [01:39<00:41, 6519.74it/s] 70%|██████▉   | 630206/900675 [01:39<00:44, 6142.43it/s] 70%|███████   | 630950/900675 [01:39<00:41, 6502.66it/s] 70%|███████   | 631691/900675 [01:40<00:39, 6761.65it/s] 70%|███████   | 632374/900675 [01:40<00:40, 6624.06it/s] 70%|███████   | 633113/900675 [01:40<00:39, 6843.98it/s] 70%|███████   | 633830/900675 [01:40<00:38, 6929.70it/s] 70%|███████   | 634526/900675 [01:40<00:39, 6708.65it/s] 71%|███████   | 635207/900675 [01:40<00:39, 6724.81it/s] 71%|███████   | 635882/900675 [01:40<00:40, 6608.38it/s] 71%|███████   | 636545/900675 [01:40<00:41, 6374.51it/s] 71%|███████   | 637187/900675 [01:40<00:41, 6385.29it/s] 71%|███████   | 637828/900675 [01:41<00:41, 6391.46it/s] 71%|███████   | 638477/900675 [01:41<00:40, 6418.67it/s] 71%|███████   | 639171/900675 [01:41<00:39, 6571.71it/s] 71%|███████   | 639830/900675 [01:41<00:41, 6234.93it/s] 71%|███████   | 640524/900675 [01:41<00:40, 6434.75it/s] 71%|███████   | 641393/900675 [01:41<00:36, 7085.34it/s] 71%|███████▏  | 642107/900675 [01:41<00:38, 6688.23it/s] 71%|███████▏  | 642784/900675 [01:41<00:38, 6689.18it/s] 71%|███████▏  | 643459/900675 [01:41<00:41, 6174.49it/s] 72%|███████▏  | 644089/900675 [01:41<00:41, 6204.85it/s] 72%|███████▏  | 644717/900675 [01:42<00:41, 6107.59it/s] 72%|███████▏  | 645354/900675 [01:42<00:41, 6177.12it/s] 72%|███████▏  | 646083/900675 [01:42<00:39, 6496.80it/s] 72%|███████▏  | 646737/900675 [01:42<00:40, 6239.41it/s] 72%|███████▏  | 647366/900675 [01:42<00:41, 6157.97it/s] 72%|███████▏  | 647985/900675 [01:42<00:44, 5727.89it/s] 72%|███████▏  | 648565/900675 [01:42<00:44, 5645.91it/s] 72%|███████▏  | 649190/900675 [01:42<00:43, 5811.72it/s] 72%|███████▏  | 649776/900675 [01:42<00:43, 5708.19it/s] 72%|███████▏  | 650372/900675 [01:43<00:43, 5777.06it/s] 72%|███████▏  | 650985/900675 [01:43<00:42, 5877.49it/s] 72%|███████▏  | 651679/900675 [01:43<00:40, 6187.38it/s] 72%|███████▏  | 652301/900675 [01:43<00:40, 6167.04it/s] 72%|███████▏  | 652920/900675 [01:43<00:40, 6154.75it/s] 73%|███████▎  | 653651/900675 [01:43<00:38, 6494.08it/s] 73%|███████▎  | 654322/900675 [01:43<00:37, 6557.23it/s] 73%|███████▎  | 654979/900675 [01:43<00:38, 6453.71it/s] 73%|███████▎  | 655626/900675 [01:43<00:38, 6445.77it/s] 73%|███████▎  | 656272/900675 [01:43<00:38, 6276.82it/s] 73%|███████▎  | 656929/900675 [01:44<00:38, 6358.46it/s] 73%|███████▎  | 657566/900675 [01:44<00:38, 6355.82it/s] 73%|███████▎  | 658203/900675 [01:44<00:39, 6178.63it/s] 73%|███████▎  | 658823/900675 [01:44<00:39, 6081.61it/s] 73%|███████▎  | 659546/900675 [01:44<00:37, 6413.80it/s] 73%|███████▎  | 660190/900675 [01:44<00:37, 6359.95it/s] 73%|███████▎  | 660828/900675 [01:44<00:37, 6350.32it/s] 73%|███████▎  | 661464/900675 [01:44<00:38, 6182.64it/s] 74%|███████▎  | 662175/900675 [01:44<00:36, 6451.78it/s] 74%|███████▎  | 662822/900675 [01:45<00:37, 6339.54it/s] 74%|███████▎  | 663458/900675 [01:45<00:38, 6099.10it/s] 74%|███████▎  | 664168/900675 [01:45<00:37, 6381.75it/s] 74%|███████▍  | 664810/900675 [01:45<00:37, 6280.95it/s] 74%|███████▍  | 665482/900675 [01:45<00:36, 6399.34it/s] 74%|███████▍  | 666214/900675 [01:45<00:35, 6655.90it/s] 74%|███████▍  | 666882/900675 [01:45<00:35, 6521.60it/s] 74%|███████▍  | 667536/900675 [01:45<00:36, 6313.15it/s] 74%|███████▍  | 668188/900675 [01:45<00:36, 6372.29it/s] 74%|███████▍  | 668903/900675 [01:45<00:35, 6588.32it/s] 74%|███████▍  | 669622/900675 [01:46<00:34, 6763.58it/s] 74%|███████▍  | 670301/900675 [01:46<00:35, 6551.18it/s] 74%|███████▍  | 670968/900675 [01:46<00:34, 6584.54it/s] 75%|███████▍  | 671653/900675 [01:46<00:34, 6647.18it/s] 75%|███████▍  | 672320/900675 [01:46<00:34, 6543.77it/s] 75%|███████▍  | 673004/900675 [01:46<00:34, 6625.79it/s] 75%|███████▍  | 673668/900675 [01:46<00:34, 6542.91it/s] 75%|███████▍  | 674324/900675 [01:46<00:35, 6455.73it/s] 75%|███████▍  | 674978/900675 [01:46<00:34, 6480.32it/s] 75%|███████▌  | 675643/900675 [01:46<00:34, 6529.81it/s] 75%|███████▌  | 676297/900675 [01:47<00:35, 6379.62it/s] 75%|███████▌  | 676936/900675 [01:47<00:35, 6343.73it/s] 75%|███████▌  | 677571/900675 [01:47<00:35, 6209.76it/s] 75%|███████▌  | 678217/900675 [01:47<00:35, 6278.96it/s] 75%|███████▌  | 678855/900675 [01:47<00:35, 6301.71it/s] 75%|███████▌  | 679486/900675 [01:47<00:36, 6036.92it/s] 76%|███████▌  | 680271/900675 [01:47<00:33, 6553.30it/s] 76%|███████▌  | 680935/900675 [01:47<00:33, 6577.82it/s] 76%|███████▌  | 681596/900675 [01:47<00:33, 6469.33it/s] 76%|███████▌  | 682246/900675 [01:48<00:35, 6206.83it/s] 76%|███████▌  | 682870/900675 [01:48<00:35, 6164.23it/s] 76%|███████▌  | 683567/900675 [01:48<00:33, 6395.89it/s] 76%|███████▌  | 684297/900675 [01:48<00:32, 6655.45it/s] 76%|███████▌  | 684976/900675 [01:48<00:32, 6692.79it/s] 76%|███████▌  | 685648/900675 [01:48<00:33, 6416.93it/s] 76%|███████▌  | 686294/900675 [01:48<00:33, 6419.87it/s] 76%|███████▋  | 686939/900675 [01:48<00:35, 6092.01it/s] 76%|███████▋  | 687602/900675 [01:48<00:34, 6230.15it/s] 76%|███████▋  | 688229/900675 [01:48<00:34, 6221.50it/s] 76%|███████▋  | 688854/900675 [01:49<00:34, 6182.53it/s] 77%|███████▋  | 689475/900675 [01:49<00:34, 6158.77it/s] 77%|███████▋  | 690094/900675 [01:49<00:34, 6166.46it/s] 77%|███████▋  | 690712/900675 [01:49<00:35, 5969.19it/s] 77%|███████▋  | 691311/900675 [01:49<00:35, 5954.54it/s] 77%|███████▋  | 691908/900675 [01:49<00:35, 5886.03it/s] 77%|███████▋  | 692498/900675 [01:49<00:35, 5862.54it/s] 77%|███████▋  | 693104/900675 [01:49<00:35, 5918.59it/s] 77%|███████▋  | 693717/900675 [01:49<00:34, 5979.91it/s] 77%|███████▋  | 694373/900675 [01:50<00:33, 6151.22it/s] 77%|███████▋  | 694989/900675 [01:50<00:33, 6122.67it/s] 77%|███████▋  | 695602/900675 [01:50<00:34, 6020.64it/s] 77%|███████▋  | 696205/900675 [01:50<00:34, 5977.73it/s] 77%|███████▋  | 696832/900675 [01:50<00:33, 6060.06it/s] 77%|███████▋  | 697488/900675 [01:50<00:32, 6201.87it/s] 78%|███████▊  | 698189/900675 [01:50<00:31, 6441.15it/s] 78%|███████▊  | 698834/900675 [01:50<00:33, 6103.59it/s] 78%|███████▊  | 699457/900675 [01:50<00:32, 6130.13it/s] 78%|███████▊  | 700112/900675 [01:50<00:32, 6239.45it/s] 78%|███████▊  | 700749/900675 [01:51<00:31, 6274.29it/s] 78%|███████▊  | 701415/900675 [01:51<00:31, 6382.75it/s] 78%|███████▊  | 702081/900675 [01:51<00:30, 6464.41it/s] 78%|███████▊  | 702729/900675 [01:51<00:30, 6436.52it/s] 78%|███████▊  | 703383/900675 [01:51<00:30, 6465.16it/s] 78%|███████▊  | 704031/900675 [01:51<00:30, 6370.41it/s] 78%|███████▊  | 704669/900675 [01:51<00:30, 6357.02it/s] 78%|███████▊  | 705306/900675 [01:51<00:32, 6012.27it/s] 78%|███████▊  | 705912/900675 [01:51<00:33, 5831.62it/s] 78%|███████▊  | 706529/900675 [01:51<00:32, 5926.16it/s] 79%|███████▊  | 707126/900675 [01:52<00:32, 5935.02it/s] 79%|███████▊  | 707722/900675 [01:52<00:32, 5917.62it/s] 79%|███████▊  | 708436/900675 [01:52<00:30, 6275.45it/s] 79%|███████▊  | 709094/900675 [01:52<00:30, 6362.10it/s] 79%|███████▉  | 709769/900675 [01:52<00:29, 6472.46it/s] 79%|███████▉  | 710446/900675 [01:52<00:29, 6555.42it/s] 79%|███████▉  | 711104/900675 [01:52<00:28, 6562.49it/s] 79%|███████▉  | 711761/900675 [01:52<00:28, 6560.75it/s] 79%|███████▉  | 712418/900675 [01:52<00:28, 6503.20it/s] 79%|███████▉  | 713126/900675 [01:52<00:28, 6669.45it/s] 79%|███████▉  | 713794/900675 [01:53<00:28, 6655.46it/s] 79%|███████▉  | 714460/900675 [01:53<00:28, 6450.00it/s] 79%|███████▉  | 715132/900675 [01:53<00:28, 6527.69it/s] 79%|███████▉  | 715787/900675 [01:53<00:29, 6371.65it/s] 80%|███████▉  | 716480/900675 [01:53<00:28, 6530.55it/s] 80%|███████▉  | 717135/900675 [01:53<00:28, 6482.20it/s] 80%|███████▉  | 717785/900675 [01:53<00:29, 6209.71it/s] 80%|███████▉  | 718409/900675 [01:53<00:29, 6129.60it/s] 80%|███████▉  | 719024/900675 [01:53<00:29, 6096.95it/s] 80%|███████▉  | 719635/900675 [01:54<00:30, 5900.12it/s] 80%|███████▉  | 720242/900675 [01:54<00:30, 5946.83it/s] 80%|████████  | 720852/900675 [01:54<00:30, 5984.61it/s] 80%|████████  | 721550/900675 [01:54<00:28, 6275.05it/s] 80%|████████  | 722266/900675 [01:54<00:27, 6535.65it/s] 80%|████████  | 722922/900675 [01:54<00:28, 6131.47it/s] 80%|████████  | 723601/900675 [01:54<00:28, 6314.97it/s] 80%|████████  | 724238/900675 [01:54<00:28, 6286.89it/s] 80%|████████  | 724871/900675 [01:54<00:28, 6191.81it/s] 81%|████████  | 725493/900675 [01:54<00:29, 5925.47it/s] 81%|████████  | 726151/900675 [01:55<00:28, 6106.45it/s] 81%|████████  | 726766/900675 [01:55<00:28, 6100.35it/s] 81%|████████  | 727444/900675 [01:55<00:27, 6292.92it/s] 81%|████████  | 728130/900675 [01:55<00:26, 6448.86it/s] 81%|████████  | 728777/900675 [01:55<00:28, 6131.38it/s] 81%|████████  | 729395/900675 [01:55<00:28, 5967.65it/s] 81%|████████  | 730030/900675 [01:55<00:28, 6072.88it/s] 81%|████████  | 730650/900675 [01:55<00:27, 6109.15it/s] 81%|████████  | 731264/900675 [01:55<00:28, 6013.80it/s] 81%|████████▏ | 731928/900675 [01:56<00:27, 6193.15it/s] 81%|████████▏ | 732583/900675 [01:56<00:26, 6297.63it/s] 81%|████████▏ | 733215/900675 [01:56<00:26, 6211.64it/s] 82%|████████▏ | 734076/900675 [01:56<00:24, 6914.21it/s] 82%|████████▏ | 734770/900675 [01:56<00:24, 6904.84it/s] 82%|████████▏ | 735463/900675 [01:56<00:27, 6049.43it/s] 82%|████████▏ | 736171/900675 [01:56<00:26, 6325.93it/s] 82%|████████▏ | 736999/900675 [01:56<00:23, 6862.21it/s] 82%|████████▏ | 737702/900675 [01:56<00:23, 6894.70it/s] 82%|████████▏ | 738403/900675 [01:56<00:24, 6550.11it/s] 82%|████████▏ | 739075/900675 [01:57<00:24, 6591.71it/s] 82%|████████▏ | 739742/900675 [01:57<00:25, 6357.49it/s] 82%|████████▏ | 740385/900675 [01:57<00:25, 6291.92it/s] 82%|████████▏ | 741075/900675 [01:57<00:24, 6464.42it/s] 82%|████████▏ | 741726/900675 [01:57<00:25, 6133.52it/s] 82%|████████▏ | 742345/900675 [01:57<00:26, 6026.96it/s] 82%|████████▏ | 742989/900675 [01:57<00:25, 6143.47it/s] 83%|████████▎ | 743671/900675 [01:57<00:24, 6323.90it/s] 83%|████████▎ | 744307/900675 [01:57<00:25, 6068.14it/s] 83%|████████▎ | 744959/900675 [01:58<00:25, 6196.16it/s] 83%|████████▎ | 745582/900675 [01:58<00:25, 6023.58it/s] 83%|████████▎ | 746188/900675 [01:58<00:25, 5957.12it/s] 83%|████████▎ | 746818/900675 [01:58<00:25, 6054.13it/s] 83%|████████▎ | 747426/900675 [01:58<00:25, 5897.05it/s] 83%|████████▎ | 748056/900675 [01:58<00:25, 6009.65it/s] 83%|████████▎ | 748659/900675 [01:58<00:25, 5856.48it/s] 83%|████████▎ | 749371/900675 [01:58<00:24, 6215.51it/s] 83%|████████▎ | 749996/900675 [01:58<00:24, 6081.97it/s] 83%|████████▎ | 750665/900675 [01:59<00:23, 6255.27it/s] 83%|████████▎ | 751293/900675 [01:59<00:24, 6143.21it/s] 83%|████████▎ | 751926/900675 [01:59<00:24, 6197.00it/s] 84%|████████▎ | 752548/900675 [01:59<00:24, 6027.03it/s] 84%|████████▎ | 753195/900675 [01:59<00:23, 6147.36it/s] 84%|████████▎ | 753837/900675 [01:59<00:23, 6224.96it/s] 84%|████████▍ | 754462/900675 [01:59<00:23, 6231.37it/s] 84%|████████▍ | 755087/900675 [01:59<00:23, 6088.78it/s] 84%|████████▍ | 755698/900675 [01:59<00:24, 5957.13it/s] 84%|████████▍ | 756295/900675 [01:59<00:25, 5732.84it/s] 84%|████████▍ | 756965/900675 [02:00<00:23, 6008.09it/s] 84%|████████▍ | 757615/900675 [02:00<00:23, 6147.70it/s] 84%|████████▍ | 758283/900675 [02:00<00:22, 6297.94it/s] 84%|████████▍ | 758915/900675 [02:00<00:22, 6279.70it/s] 84%|████████▍ | 759601/900675 [02:00<00:21, 6446.75it/s] 84%|████████▍ | 760264/900675 [02:00<00:21, 6499.27it/s] 84%|████████▍ | 760926/900675 [02:00<00:21, 6534.39it/s] 85%|████████▍ | 761581/900675 [02:00<00:21, 6513.44it/s] 85%|████████▍ | 762233/900675 [02:00<00:21, 6381.43it/s] 85%|████████▍ | 762873/900675 [02:00<00:22, 6090.28it/s] 85%|████████▍ | 763486/900675 [02:01<00:22, 6038.16it/s] 85%|████████▍ | 764092/900675 [02:01<00:22, 5997.24it/s] 85%|████████▍ | 764735/900675 [02:01<00:22, 6113.95it/s] 85%|████████▍ | 765348/900675 [02:01<00:22, 6050.86it/s] 85%|████████▌ | 765954/900675 [02:01<00:22, 6029.00it/s] 85%|████████▌ | 766703/900675 [02:01<00:20, 6453.68it/s] 85%|████████▌ | 767350/900675 [02:01<00:20, 6391.64it/s] 85%|████████▌ | 768023/900675 [02:01<00:20, 6486.05it/s] 85%|████████▌ | 768673/900675 [02:01<00:21, 6200.08it/s] 85%|████████▌ | 769296/900675 [02:02<00:22, 5924.34it/s] 85%|████████▌ | 769893/900675 [02:02<00:22, 5924.70it/s] 86%|████████▌ | 770512/900675 [02:02<00:21, 5997.87it/s] 86%|████████▌ | 771114/900675 [02:02<00:21, 5925.32it/s] 86%|████████▌ | 771802/900675 [02:02<00:20, 6202.53it/s] 86%|████████▌ | 772425/900675 [02:02<00:21, 6020.89it/s] 86%|████████▌ | 773154/900675 [02:02<00:19, 6382.36it/s] 86%|████████▌ | 773801/900675 [02:02<00:19, 6406.79it/s] 86%|████████▌ | 774477/900675 [02:02<00:19, 6506.91it/s] 86%|████████▌ | 775130/900675 [02:02<00:20, 6127.55it/s] 86%|████████▌ | 775760/900675 [02:03<00:20, 6174.96it/s] 86%|████████▌ | 776382/900675 [02:03<00:20, 5981.59it/s] 86%|████████▋ | 776984/900675 [02:03<00:20, 5895.19it/s] 86%|████████▋ | 777678/900675 [02:03<00:19, 6192.88it/s] 86%|████████▋ | 778308/900675 [02:03<00:19, 6219.44it/s] 86%|████████▋ | 778957/900675 [02:03<00:19, 6297.90it/s] 87%|████████▋ | 779589/900675 [02:03<00:19, 6062.68it/s] 87%|████████▋ | 780207/900675 [02:03<00:19, 6093.30it/s] 87%|████████▋ | 780819/900675 [02:03<00:20, 5981.76it/s] 87%|████████▋ | 781637/900675 [02:04<00:18, 6609.62it/s] 87%|████████▋ | 782302/900675 [02:04<00:18, 6373.27it/s] 87%|████████▋ | 782943/900675 [02:04<00:20, 5832.16it/s] 87%|████████▋ | 783585/900675 [02:04<00:19, 5985.93it/s] 87%|████████▋ | 784193/900675 [02:04<00:19, 5928.71it/s] 87%|████████▋ | 784792/900675 [02:04<00:19, 5913.79it/s] 87%|████████▋ | 785453/900675 [02:04<00:18, 6105.02it/s] 87%|████████▋ | 786152/900675 [02:04<00:18, 6360.45it/s] 87%|████████▋ | 786792/900675 [02:04<00:18, 6304.42it/s] 87%|████████▋ | 787425/900675 [02:04<00:17, 6305.97it/s] 88%|████████▊ | 788102/900675 [02:05<00:17, 6441.60it/s] 88%|████████▊ | 788748/900675 [02:05<00:18, 6168.63it/s] 88%|████████▊ | 789369/900675 [02:05<00:19, 5833.72it/s] 88%|████████▊ | 790041/900675 [02:05<00:18, 6079.60it/s] 88%|████████▊ | 790725/900675 [02:05<00:17, 6294.83it/s] 88%|████████▊ | 791360/900675 [02:05<00:17, 6181.95it/s] 88%|████████▊ | 791982/900675 [02:05<00:18, 5772.77it/s] 88%|████████▊ | 792684/900675 [02:05<00:17, 6114.42it/s] 88%|████████▊ | 793379/900675 [02:05<00:16, 6349.10it/s] 88%|████████▊ | 794021/900675 [02:06<00:18, 5899.61it/s] 88%|████████▊ | 794668/900675 [02:06<00:17, 6051.89it/s] 88%|████████▊ | 795341/900675 [02:06<00:16, 6235.58it/s] 88%|████████▊ | 795998/900675 [02:06<00:16, 6325.86it/s] 88%|████████▊ | 796636/900675 [02:06<00:16, 6295.26it/s] 89%|████████▊ | 797275/900675 [02:06<00:16, 6320.90it/s] 89%|████████▊ | 797910/900675 [02:06<00:16, 6212.15it/s] 89%|████████▊ | 798541/900675 [02:06<00:16, 6239.30it/s] 89%|████████▊ | 799201/900675 [02:06<00:15, 6345.13it/s] 89%|████████▉ | 799849/900675 [02:06<00:15, 6377.08it/s] 89%|████████▉ | 800488/900675 [02:07<00:16, 6192.89it/s] 89%|████████▉ | 801109/900675 [02:07<00:16, 6153.99it/s] 89%|████████▉ | 801806/900675 [02:07<00:15, 6391.92it/s] 89%|████████▉ | 802447/900675 [02:07<00:16, 5965.41it/s] 89%|████████▉ | 803105/900675 [02:07<00:15, 6135.18it/s] 89%|████████▉ | 803735/900675 [02:07<00:15, 6181.70it/s] 89%|████████▉ | 804373/900675 [02:07<00:15, 6232.31it/s] 89%|████████▉ | 805000/900675 [02:07<00:15, 6219.39it/s] 89%|████████▉ | 805624/900675 [02:07<00:15, 6116.53it/s] 90%|████████▉ | 806276/900675 [02:08<00:15, 6228.61it/s] 90%|████████▉ | 806925/900675 [02:08<00:14, 6304.12it/s] 90%|████████▉ | 807557/900675 [02:08<00:15, 6074.10it/s] 90%|████████▉ | 808167/900675 [02:08<00:16, 5611.83it/s] 90%|████████▉ | 808811/900675 [02:08<00:15, 5840.45it/s] 90%|████████▉ | 809468/900675 [02:08<00:15, 6044.51it/s] 90%|████████▉ | 810106/900675 [02:08<00:14, 6135.21it/s] 90%|█████████ | 810725/900675 [02:08<00:15, 5683.80it/s] 90%|█████████ | 811303/900675 [02:08<00:15, 5694.19it/s] 90%|█████████ | 811984/900675 [02:08<00:14, 6007.95it/s] 90%|█████████ | 812592/900675 [02:09<00:14, 5927.38it/s] 90%|█████████ | 813308/900675 [02:09<00:13, 6276.49it/s] 90%|█████████ | 813988/900675 [02:09<00:13, 6427.30it/s] 90%|█████████ | 814692/900675 [02:09<00:13, 6603.00it/s] 91%|█████████ | 815440/900675 [02:09<00:12, 6859.94it/s] 91%|█████████ | 816152/900675 [02:09<00:12, 6936.36it/s] 91%|█████████ | 816914/900675 [02:09<00:11, 7134.02it/s] 91%|█████████ | 817629/900675 [02:09<00:11, 7105.59it/s] 91%|█████████ | 818363/900675 [02:09<00:11, 7165.41it/s] 91%|█████████ | 819081/900675 [02:10<00:12, 6495.74it/s] 91%|█████████ | 819743/900675 [02:10<00:12, 6462.77it/s] 91%|█████████ | 820475/900675 [02:10<00:11, 6702.48it/s] 91%|█████████ | 821231/900675 [02:10<00:11, 6943.53it/s] 91%|█████████▏| 821932/900675 [02:10<00:12, 6221.85it/s] 91%|█████████▏| 822572/900675 [02:10<00:12, 6201.83it/s] 91%|█████████▏| 823262/900675 [02:10<00:12, 6384.84it/s] 91%|█████████▏| 823911/900675 [02:10<00:11, 6397.48it/s] 92%|█████████▏| 824558/900675 [02:10<00:12, 6154.72it/s] 92%|█████████▏| 825180/900675 [02:11<00:12, 5941.29it/s] 92%|█████████▏| 825780/900675 [02:11<00:12, 5853.65it/s] 92%|█████████▏| 826387/900675 [02:11<00:12, 5909.24it/s] 92%|█████████▏| 827065/900675 [02:11<00:11, 6156.90it/s] 92%|█████████▏| 827732/900675 [02:11<00:11, 6304.74it/s] 92%|█████████▏| 828365/900675 [02:11<00:11, 6245.24it/s] 92%|█████████▏| 828992/900675 [02:11<00:12, 5678.59it/s] 92%|█████████▏| 829584/900675 [02:11<00:12, 5739.79it/s] 92%|█████████▏| 830219/900675 [02:11<00:11, 5908.86it/s] 92%|█████████▏| 830817/900675 [02:11<00:11, 5922.65it/s] 92%|█████████▏| 831414/900675 [02:12<00:11, 5801.35it/s] 92%|█████████▏| 832066/900675 [02:12<00:11, 6003.70it/s] 92%|█████████▏| 832670/900675 [02:12<00:11, 5760.33it/s] 93%|█████████▎| 833375/900675 [02:12<00:10, 6128.01it/s] 93%|█████████▎| 834011/900675 [02:12<00:10, 6194.51it/s] 93%|█████████▎| 834634/900675 [02:12<00:10, 6074.97it/s] 93%|█████████▎| 835298/900675 [02:12<00:10, 6238.42it/s] 93%|█████████▎| 835943/900675 [02:12<00:10, 6300.31it/s] 93%|█████████▎| 836603/900675 [02:12<00:10, 6381.23it/s] 93%|█████████▎| 837274/900675 [02:12<00:09, 6478.24it/s] 93%|█████████▎| 837923/900675 [02:13<00:09, 6450.16it/s] 93%|█████████▎| 838569/900675 [02:13<00:09, 6421.44it/s] 93%|█████████▎| 839212/900675 [02:13<00:10, 6062.75it/s] 93%|█████████▎| 839855/900675 [02:13<00:09, 6164.34it/s] 93%|█████████▎| 840616/900675 [02:13<00:09, 6573.98it/s] 93%|█████████▎| 841278/900675 [02:13<00:09, 6417.20it/s] 93%|█████████▎| 841923/900675 [02:13<00:09, 6324.12it/s] 94%|█████████▎| 842558/900675 [02:13<00:09, 6136.86it/s] 94%|█████████▎| 843174/900675 [02:13<00:09, 5801.80it/s] 94%|█████████▎| 843759/900675 [02:14<00:10, 5611.61it/s] 94%|█████████▎| 844324/900675 [02:14<00:10, 5423.83it/s] 94%|█████████▍| 844899/900675 [02:14<00:10, 5513.16it/s] 94%|█████████▍| 845521/900675 [02:14<00:09, 5711.42it/s] 94%|█████████▍| 846109/900675 [02:14<00:09, 5753.47it/s] 94%|█████████▍| 846730/900675 [02:14<00:09, 5885.70it/s] 94%|█████████▍| 847386/900675 [02:14<00:08, 6080.56it/s] 94%|█████████▍| 848010/900675 [02:14<00:08, 6123.95it/s] 94%|█████████▍| 848646/900675 [02:14<00:08, 6192.34it/s] 94%|█████████▍| 849267/900675 [02:14<00:08, 6168.81it/s] 94%|█████████▍| 849902/900675 [02:15<00:08, 6218.95it/s] 94%|█████████▍| 850525/900675 [02:15<00:08, 6135.16it/s] 95%|█████████▍| 851140/900675 [02:15<00:08, 5936.72it/s] 95%|█████████▍| 851788/900675 [02:15<00:08, 6080.79it/s] 95%|█████████▍| 852398/900675 [02:15<00:08, 6025.37it/s] 95%|█████████▍| 853002/900675 [02:15<00:08, 5928.83it/s] 95%|█████████▍| 853608/900675 [02:15<00:07, 5957.38it/s] 95%|█████████▍| 854214/900675 [02:15<00:07, 5987.45it/s] 95%|█████████▍| 854855/900675 [02:15<00:07, 6111.96it/s] 95%|█████████▍| 855467/900675 [02:16<00:07, 6050.75it/s] 95%|█████████▌| 856114/900675 [02:16<00:07, 6171.19it/s] 95%|█████████▌| 856762/900675 [02:16<00:07, 6262.55it/s] 95%|█████████▌| 857420/900675 [02:16<00:06, 6351.97it/s] 95%|█████████▌| 858060/900675 [02:16<00:06, 6361.08it/s] 95%|█████████▌| 858697/900675 [02:16<00:06, 6108.46it/s] 95%|█████████▌| 859311/900675 [02:16<00:06, 6030.78it/s] 95%|█████████▌| 859951/900675 [02:16<00:06, 6131.57it/s] 96%|█████████▌| 860801/900675 [02:16<00:05, 6821.92it/s] 96%|█████████▌| 861486/900675 [02:16<00:06, 6504.89it/s] 96%|█████████▌| 862142/900675 [02:17<00:06, 6408.51it/s] 96%|█████████▌| 862848/900675 [02:17<00:05, 6594.98it/s] 96%|█████████▌| 863511/900675 [02:17<00:05, 6502.55it/s] 96%|█████████▌| 864164/900675 [02:17<00:05, 6425.64it/s] 96%|█████████▌| 864809/900675 [02:17<00:05, 6356.91it/s] 96%|█████████▌| 865446/900675 [02:17<00:05, 6336.54it/s] 96%|█████████▌| 866081/900675 [02:17<00:05, 5968.00it/s] 96%|█████████▌| 866773/900675 [02:17<00:05, 6231.81it/s] 96%|█████████▋| 867401/900675 [02:17<00:05, 6133.59it/s] 96%|█████████▋| 868037/900675 [02:18<00:05, 6197.01it/s] 96%|█████████▋| 868660/900675 [02:18<00:05, 6103.60it/s] 97%|█████████▋| 869282/900675 [02:18<00:05, 6135.24it/s] 97%|█████████▋| 869980/900675 [02:18<00:04, 6380.90it/s] 97%|█████████▋| 870629/900675 [02:18<00:04, 6412.08it/s] 97%|█████████▋| 871412/900675 [02:18<00:04, 6831.99it/s] 97%|█████████▋| 872097/900675 [02:18<00:04, 6356.50it/s] 97%|█████████▋| 872740/900675 [02:18<00:04, 6178.23it/s] 97%|█████████▋| 873426/900675 [02:18<00:04, 6369.76it/s] 97%|█████████▋| 874123/900675 [02:18<00:04, 6539.55it/s] 97%|█████████▋| 874782/900675 [02:19<00:04, 6370.85it/s] 97%|█████████▋| 875449/900675 [02:19<00:03, 6456.67it/s] 97%|█████████▋| 876098/900675 [02:19<00:03, 6291.07it/s] 97%|█████████▋| 876752/900675 [02:19<00:03, 6356.23it/s] 97%|█████████▋| 877390/900675 [02:19<00:03, 6103.49it/s] 97%|█████████▋| 878004/900675 [02:19<00:03, 6052.28it/s] 98%|█████████▊| 878618/900675 [02:19<00:03, 6077.28it/s] 98%|█████████▊| 879266/900675 [02:19<00:03, 6190.81it/s] 98%|█████████▊| 879887/900675 [02:19<00:03, 5921.39it/s] 98%|█████████▊| 880530/900675 [02:20<00:03, 6062.36it/s] 98%|█████████▊| 881140/900675 [02:20<00:03, 5934.49it/s] 98%|█████████▊| 881736/900675 [02:20<00:03, 5749.66it/s] 98%|█████████▊| 882314/900675 [02:20<00:03, 5564.98it/s] 98%|█████████▊| 882995/900675 [02:20<00:02, 5915.11it/s] 98%|█████████▊| 883645/900675 [02:20<00:02, 6077.79it/s] 98%|█████████▊| 884257/900675 [02:20<00:02, 6062.80it/s] 98%|█████████▊| 884866/900675 [02:20<00:02, 5906.83it/s] 98%|█████████▊| 885549/900675 [02:20<00:02, 6172.50it/s] 98%|█████████▊| 886293/900675 [02:20<00:02, 6538.22it/s] 98%|█████████▊| 886978/900675 [02:21<00:02, 6626.20it/s] 99%|█████████▊| 887643/900675 [02:21<00:02, 6252.61it/s] 99%|█████████▊| 888274/900675 [02:21<00:01, 6218.39it/s] 99%|█████████▊| 888900/900675 [02:21<00:01, 6009.17it/s] 99%|█████████▉| 889612/900675 [02:21<00:01, 6317.30it/s] 99%|█████████▉| 890248/900675 [02:21<00:01, 6225.62it/s] 99%|█████████▉| 890970/900675 [02:21<00:01, 6509.75it/s] 99%|█████████▉| 891625/900675 [02:21<00:01, 6265.80it/s] 99%|█████████▉| 892256/900675 [02:21<00:01, 6119.23it/s] 99%|█████████▉| 892871/900675 [02:22<00:01, 6069.75it/s] 99%|█████████▉| 893480/900675 [02:22<00:01, 6015.84it/s] 99%|█████████▉| 894083/900675 [02:22<00:01, 5785.60it/s] 99%|█████████▉| 894664/900675 [02:22<00:01, 5600.25it/s] 99%|█████████▉| 895325/900675 [02:22<00:00, 5879.54it/s] 99%|█████████▉| 895975/900675 [02:22<00:00, 6046.75it/s]100%|█████████▉| 896631/900675 [02:22<00:00, 6180.41it/s]100%|█████████▉| 897300/900675 [02:22<00:00, 6324.01it/s]100%|█████████▉| 897992/900675 [02:22<00:00, 6482.62it/s]100%|█████████▉| 898642/900675 [02:22<00:00, 6449.10it/s]100%|█████████▉| 899289/900675 [02:23<00:00, 6426.03it/s]100%|█████████▉| 899994/900675 [02:23<00:00, 6601.48it/s]100%|█████████▉| 900655/900675 [02:23<00:00, 6387.17it/s]100%|██████████| 900675/900675 [02:23<00:00, 6286.80it/s]

gathering stats for n=1
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 1913/900675 [00:00<00:47, 19122.04it/s]  0%|          | 3995/900675 [00:00<00:44, 20102.78it/s]  1%|          | 6240/900675 [00:00<00:42, 21167.72it/s]  1%|          | 8357/900675 [00:00<00:43, 20360.25it/s]  1%|          | 10398/900675 [00:00<00:43, 20249.92it/s]  1%|▏         | 12426/900675 [00:00<00:44, 20152.74it/s]  2%|▏         | 14497/900675 [00:00<00:43, 20329.80it/s]  2%|▏         | 16532/900675 [00:00<00:43, 20138.59it/s]  2%|▏         | 18548/900675 [00:00<00:43, 20050.18it/s]  2%|▏         | 20709/900675 [00:01<00:42, 20520.03it/s]  3%|▎         | 22763/900675 [00:01<00:42, 20434.77it/s]  3%|▎         | 25052/900675 [00:01<00:41, 21170.60it/s]  3%|▎         | 27171/900675 [00:01<00:42, 20453.21it/s]  3%|▎         | 29274/900675 [00:01<00:42, 20614.38it/s]  3%|▎         | 31340/900675 [00:01<00:43, 20120.59it/s]  4%|▎         | 33357/900675 [00:01<00:43, 19793.41it/s]  4%|▍         | 35408/900675 [00:01<00:43, 19998.96it/s]  4%|▍         | 37412/900675 [00:01<00:43, 19681.71it/s]  4%|▍         | 39383/900675 [00:01<00:43, 19686.52it/s]  5%|▍         | 41496/900675 [00:02<00:42, 20110.64it/s]  5%|▍         | 43510/900675 [00:02<00:43, 19603.54it/s]  5%|▌         | 45603/900675 [00:02<00:42, 19986.97it/s]  5%|▌         | 48007/900675 [00:02<00:40, 21173.75it/s]  6%|▌         | 50149/900675 [00:02<00:40, 21241.85it/s]  6%|▌         | 52321/900675 [00:02<00:39, 21382.82it/s]  6%|▌         | 54462/900675 [00:02<00:40, 20760.04it/s]  6%|▋         | 56566/900675 [00:02<00:40, 20836.27it/s]  7%|▋         | 58654/900675 [00:02<00:40, 20687.45it/s]  7%|▋         | 60726/900675 [00:02<00:41, 20406.40it/s]  7%|▋         | 62893/900675 [00:03<00:40, 20771.20it/s]  7%|▋         | 64973/900675 [00:03<00:40, 20551.85it/s]  7%|▋         | 67031/900675 [00:03<00:40, 20508.97it/s]  8%|▊         | 69084/900675 [00:03<00:40, 20388.07it/s]  8%|▊         | 71155/900675 [00:03<00:40, 20482.55it/s]  8%|▊         | 73422/900675 [00:03<00:39, 21126.60it/s]  8%|▊         | 75592/900675 [00:03<00:38, 21293.94it/s]  9%|▊         | 77723/900675 [00:03<00:38, 21267.48it/s]  9%|▉         | 79851/900675 [00:03<00:40, 20276.21it/s]  9%|▉         | 81944/900675 [00:04<00:40, 20463.92it/s]  9%|▉         | 84091/900675 [00:04<00:39, 20754.11it/s] 10%|▉         | 86173/900675 [00:04<00:39, 20555.24it/s] 10%|▉         | 88359/900675 [00:04<00:38, 20938.02it/s] 10%|█         | 90457/900675 [00:04<00:39, 20286.60it/s] 10%|█         | 92587/900675 [00:04<00:39, 20575.55it/s] 11%|█         | 94650/900675 [00:04<00:39, 20490.08it/s] 11%|█         | 96902/900675 [00:04<00:38, 21086.82it/s] 11%|█         | 99015/900675 [00:04<00:38, 20699.04it/s] 11%|█         | 101089/900675 [00:04<00:39, 20325.07it/s] 11%|█▏        | 103132/900675 [00:05<00:39, 20354.42it/s] 12%|█▏        | 105170/900675 [00:05<00:39, 20081.92it/s] 12%|█▏        | 107181/900675 [00:05<00:39, 19997.84it/s] 12%|█▏        | 109183/900675 [00:05<00:39, 19866.93it/s] 12%|█▏        | 111317/900675 [00:05<00:38, 20296.41it/s] 13%|█▎        | 113350/900675 [00:05<00:38, 20298.69it/s] 13%|█▎        | 115381/900675 [00:05<00:38, 20193.91it/s] 13%|█▎        | 117402/900675 [00:05<00:40, 19568.60it/s] 13%|█▎        | 119407/900675 [00:05<00:39, 19706.79it/s] 13%|█▎        | 121500/900675 [00:05<00:38, 20063.76it/s] 14%|█▎        | 123510/900675 [00:06<00:38, 20025.75it/s] 14%|█▍        | 125515/900675 [00:06<00:38, 19998.47it/s] 14%|█▍        | 127634/900675 [00:06<00:37, 20351.90it/s] 14%|█▍        | 129685/900675 [00:06<00:37, 20394.83it/s] 15%|█▍        | 131726/900675 [00:06<00:39, 19621.14it/s] 15%|█▍        | 133849/900675 [00:06<00:38, 20078.59it/s] 15%|█▌        | 136137/900675 [00:06<00:36, 20886.47it/s] 15%|█▌        | 138313/900675 [00:06<00:36, 21142.83it/s] 16%|█▌        | 140432/900675 [00:06<00:36, 20816.76it/s] 16%|█▌        | 142518/900675 [00:06<00:36, 20615.92it/s] 16%|█▌        | 144648/900675 [00:07<00:36, 20815.90it/s] 16%|█▋        | 146739/900675 [00:07<00:36, 20836.31it/s] 17%|█▋        | 148825/900675 [00:07<00:36, 20424.76it/s] 17%|█▋        | 150871/900675 [00:07<00:37, 20075.58it/s] 17%|█▋        | 152882/900675 [00:07<00:37, 19993.34it/s] 17%|█▋        | 154933/900675 [00:07<00:37, 20138.45it/s] 17%|█▋        | 157079/900675 [00:07<00:36, 20528.27it/s] 18%|█▊        | 159134/900675 [00:07<00:36, 20448.96it/s] 18%|█▊        | 161298/900675 [00:07<00:35, 20802.14it/s] 18%|█▊        | 163380/900675 [00:07<00:35, 20788.74it/s] 18%|█▊        | 165460/900675 [00:08<00:35, 20751.10it/s] 19%|█▊        | 167611/900675 [00:08<00:34, 20976.49it/s] 19%|█▉        | 169710/900675 [00:08<00:35, 20526.70it/s] 19%|█▉        | 171766/900675 [00:08<00:36, 20195.81it/s] 19%|█▉        | 173788/900675 [00:08<00:36, 20107.48it/s] 20%|█▉        | 175801/900675 [00:08<00:36, 19880.17it/s] 20%|█▉        | 177791/900675 [00:08<00:37, 19290.26it/s] 20%|█▉        | 179870/900675 [00:08<00:36, 19722.58it/s] 20%|██        | 181847/900675 [00:08<00:36, 19595.99it/s] 20%|██        | 183810/900675 [00:09<00:37, 19265.89it/s] 21%|██        | 185740/900675 [00:09<00:37, 19274.31it/s] 21%|██        | 187744/900675 [00:09<00:36, 19498.77it/s] 21%|██        | 189797/900675 [00:09<00:35, 19798.07it/s] 21%|██▏       | 191848/900675 [00:09<00:35, 20006.01it/s] 22%|██▏       | 193910/900675 [00:09<00:35, 20185.32it/s] 22%|██▏       | 195999/900675 [00:09<00:34, 20395.11it/s] 22%|██▏       | 198182/900675 [00:09<00:33, 20820.61it/s] 22%|██▏       | 200265/900675 [00:09<00:34, 20471.92it/s] 22%|██▏       | 202314/900675 [00:09<00:34, 20242.80it/s] 23%|██▎       | 204387/900675 [00:10<00:34, 20385.54it/s] 23%|██▎       | 206432/900675 [00:10<00:34, 20404.27it/s] 23%|██▎       | 208474/900675 [00:10<00:34, 19894.56it/s] 23%|██▎       | 210503/900675 [00:10<00:34, 20004.09it/s] 24%|██▎       | 212570/900675 [00:10<00:34, 20194.05it/s] 24%|██▍       | 214592/900675 [00:10<00:34, 19886.58it/s] 24%|██▍       | 216583/900675 [00:10<00:34, 19594.32it/s] 24%|██▍       | 218661/900675 [00:10<00:34, 19940.93it/s] 24%|██▍       | 220658/900675 [00:10<00:34, 19945.10it/s] 25%|██▍       | 222655/900675 [00:10<00:34, 19802.25it/s] 25%|██▍       | 224642/900675 [00:11<00:34, 19815.02it/s] 25%|██▌       | 226818/900675 [00:11<00:33, 20392.34it/s] 25%|██▌       | 228859/900675 [00:11<00:33, 19947.64it/s] 26%|██▌       | 230857/900675 [00:11<00:33, 19756.59it/s] 26%|██▌       | 232896/900675 [00:11<00:33, 19940.10it/s] 26%|██▌       | 235100/900675 [00:11<00:32, 20556.66it/s] 26%|██▋       | 237158/900675 [00:11<00:32, 20318.62it/s] 27%|██▋       | 239267/900675 [00:11<00:32, 20545.00it/s] 27%|██▋       | 241420/900675 [00:11<00:31, 20836.35it/s] 27%|██▋       | 243506/900675 [00:11<00:31, 20819.05it/s] 27%|██▋       | 245589/900675 [00:12<00:31, 20717.73it/s] 28%|██▊       | 247833/900675 [00:12<00:30, 21228.38it/s] 28%|██▊       | 249973/900675 [00:12<00:30, 21278.95it/s] 28%|██▊       | 252102/900675 [00:12<00:30, 21108.54it/s] 28%|██▊       | 254214/900675 [00:12<00:30, 20874.48it/s] 28%|██▊       | 256303/900675 [00:12<00:31, 20775.66it/s] 29%|██▊       | 258382/900675 [00:12<00:31, 20458.26it/s] 29%|██▉       | 260429/900675 [00:12<00:31, 20437.25it/s] 29%|██▉       | 262474/900675 [00:12<00:31, 20257.75it/s] 29%|██▉       | 264501/900675 [00:13<00:31, 20057.90it/s] 30%|██▉       | 266508/900675 [00:13<00:31, 19940.92it/s] 30%|██▉       | 268503/900675 [00:13<00:31, 19876.37it/s] 30%|███       | 270589/900675 [00:13<00:31, 20164.99it/s] 30%|███       | 272809/900675 [00:13<00:30, 20764.39it/s] 31%|███       | 274887/900675 [00:13<00:30, 20623.31it/s] 31%|███       | 276951/900675 [00:13<00:31, 19989.60it/s] 31%|███       | 279042/900675 [00:13<00:30, 20256.36it/s] 31%|███       | 281072/900675 [00:13<00:30, 20045.14it/s] 31%|███▏      | 283112/900675 [00:13<00:30, 20146.80it/s] 32%|███▏      | 285194/900675 [00:14<00:30, 20342.88it/s] 32%|███▏      | 287231/900675 [00:14<00:30, 20040.77it/s] 32%|███▏      | 289362/900675 [00:14<00:29, 20412.82it/s] 32%|███▏      | 291406/900675 [00:14<00:30, 19964.17it/s] 33%|███▎      | 293411/900675 [00:14<00:30, 19987.46it/s] 33%|███▎      | 295510/900675 [00:14<00:29, 20281.03it/s] 33%|███▎      | 297541/900675 [00:14<00:29, 20274.57it/s] 33%|███▎      | 299670/900675 [00:14<00:29, 20576.06it/s] 34%|███▎      | 301729/900675 [00:14<00:29, 20271.84it/s] 34%|███▎      | 303845/900675 [00:14<00:29, 20529.80it/s] 34%|███▍      | 305900/900675 [00:15<00:29, 20269.90it/s] 34%|███▍      | 307929/900675 [00:15<00:29, 20213.82it/s] 34%|███▍      | 310075/900675 [00:15<00:28, 20578.36it/s] 35%|███▍      | 312150/900675 [00:15<00:28, 20628.62it/s] 35%|███▍      | 314327/900675 [00:15<00:27, 20965.94it/s] 35%|███▌      | 316425/900675 [00:15<00:29, 20064.55it/s] 35%|███▌      | 318440/900675 [00:15<00:29, 19931.92it/s] 36%|███▌      | 320439/900675 [00:15<00:29, 19786.43it/s] 36%|███▌      | 322442/900675 [00:15<00:29, 19857.30it/s] 36%|███▌      | 324555/900675 [00:15<00:28, 20230.47it/s] 36%|███▋      | 326581/900675 [00:16<00:28, 19853.17it/s] 37%|███▋      | 328947/900675 [00:16<00:27, 20966.25it/s] 37%|███▋      | 331049/900675 [00:16<00:28, 20290.29it/s] 37%|███▋      | 333086/900675 [00:16<00:28, 20047.16it/s] 37%|███▋      | 335177/900675 [00:16<00:27, 20292.72it/s] 37%|███▋      | 337258/900675 [00:16<00:27, 20436.25it/s] 38%|███▊      | 339414/900675 [00:16<00:27, 20764.49it/s] 38%|███▊      | 341494/900675 [00:16<00:27, 20183.73it/s] 38%|███▊      | 343560/900675 [00:16<00:27, 20321.82it/s] 38%|███▊      | 345597/900675 [00:17<00:27, 20213.68it/s] 39%|███▊      | 347622/900675 [00:17<00:27, 20114.43it/s] 39%|███▉      | 349636/900675 [00:17<00:27, 20059.98it/s] 39%|███▉      | 351796/900675 [00:17<00:26, 20514.02it/s] 39%|███▉      | 353849/900675 [00:17<00:27, 19754.15it/s] 40%|███▉      | 355936/900675 [00:17<00:27, 20074.74it/s] 40%|███▉      | 357950/900675 [00:17<00:27, 19732.52it/s] 40%|███▉      | 359964/900675 [00:17<00:27, 19845.66it/s] 40%|████      | 362240/900675 [00:17<00:26, 20693.03it/s] 40%|████      | 364380/900675 [00:17<00:25, 20891.01it/s] 41%|████      | 366498/900675 [00:18<00:25, 20972.19it/s] 41%|████      | 368598/900675 [00:18<00:26, 20419.83it/s] 41%|████      | 370651/900675 [00:18<00:25, 20448.51it/s] 41%|████▏     | 372700/900675 [00:18<00:26, 20279.44it/s] 42%|████▏     | 374799/900675 [00:18<00:25, 20488.41it/s] 42%|████▏     | 376850/900675 [00:18<00:26, 19743.17it/s] 42%|████▏     | 378956/900675 [00:18<00:25, 20119.96it/s] 42%|████▏     | 381162/900675 [00:18<00:25, 20686.61it/s] 43%|████▎     | 383237/900675 [00:18<00:25, 20055.51it/s] 43%|████▎     | 385349/900675 [00:18<00:25, 20363.71it/s] 43%|████▎     | 387425/900675 [00:19<00:25, 20477.33it/s] 43%|████▎     | 389479/900675 [00:19<00:24, 20492.37it/s] 43%|████▎     | 391532/900675 [00:19<00:25, 20072.33it/s] 44%|████▎     | 393544/900675 [00:19<00:25, 19606.72it/s] 44%|████▍     | 395661/900675 [00:19<00:25, 20055.61it/s] 44%|████▍     | 397671/900675 [00:19<00:25, 19692.42it/s] 44%|████▍     | 399960/900675 [00:19<00:24, 20615.71it/s] 45%|████▍     | 402027/900675 [00:19<00:25, 19888.37it/s] 45%|████▍     | 404024/900675 [00:19<00:25, 19631.05it/s] 45%|████▌     | 406016/900675 [00:20<00:25, 19710.90it/s] 45%|████▌     | 407992/900675 [00:20<00:25, 19491.78it/s] 46%|████▌     | 410062/900675 [00:20<00:24, 19839.81it/s] 46%|████▌     | 412050/900675 [00:20<00:24, 19811.46it/s] 46%|████▌     | 414034/900675 [00:20<00:25, 18989.77it/s] 46%|████▌     | 415942/900675 [00:20<00:25, 19009.87it/s] 46%|████▋     | 417951/900675 [00:20<00:24, 19320.01it/s] 47%|████▋     | 419944/900675 [00:20<00:24, 19499.10it/s] 47%|████▋     | 421898/900675 [00:20<00:24, 19222.06it/s] 47%|████▋     | 423963/900675 [00:20<00:24, 19638.92it/s] 47%|████▋     | 425935/900675 [00:21<00:24, 19657.70it/s] 48%|████▊     | 428033/900675 [00:21<00:23, 20046.07it/s] 48%|████▊     | 430236/900675 [00:21<00:22, 20635.98it/s] 48%|████▊     | 432302/900675 [00:21<00:23, 20091.52it/s] 48%|████▊     | 434424/900675 [00:21<00:22, 20420.51it/s] 48%|████▊     | 436470/900675 [00:21<00:22, 20208.71it/s] 49%|████▊     | 438503/900675 [00:21<00:22, 20243.76it/s] 49%|████▉     | 440530/900675 [00:21<00:23, 19774.59it/s] 49%|████▉     | 442579/900675 [00:21<00:22, 19973.25it/s] 49%|████▉     | 444589/900675 [00:21<00:22, 20005.61it/s] 50%|████▉     | 446592/900675 [00:22<00:23, 19620.48it/s] 50%|████▉     | 448736/900675 [00:22<00:22, 20152.35it/s] 50%|█████     | 450755/900675 [00:22<00:22, 20119.91it/s] 50%|█████     | 452770/900675 [00:22<00:22, 19750.09it/s] 50%|█████     | 454750/900675 [00:22<00:22, 19764.48it/s] 51%|█████     | 456846/900675 [00:22<00:22, 20116.52it/s] 51%|█████     | 458860/900675 [00:22<00:22, 20070.47it/s] 51%|█████     | 460869/900675 [00:22<00:21, 20034.03it/s] 51%|█████▏    | 462901/900675 [00:22<00:21, 20113.75it/s] 52%|█████▏    | 464914/900675 [00:22<00:21, 19996.60it/s] 52%|█████▏    | 466965/900675 [00:23<00:21, 20148.89it/s] 52%|█████▏    | 468991/900675 [00:23<00:21, 20181.20it/s] 52%|█████▏    | 471015/900675 [00:23<00:21, 20197.42it/s] 53%|█████▎    | 473036/900675 [00:23<00:21, 20084.73it/s] 53%|█████▎    | 475087/900675 [00:23<00:21, 20211.12it/s] 53%|█████▎    | 477144/900675 [00:23<00:20, 20316.67it/s] 53%|█████▎    | 479349/900675 [00:23<00:20, 20834.08it/s] 53%|█████▎    | 481433/900675 [00:23<00:20, 20381.01it/s] 54%|█████▎    | 483474/900675 [00:23<00:20, 20252.70it/s] 54%|█████▍    | 485670/900675 [00:23<00:19, 20751.29it/s] 54%|█████▍    | 487748/900675 [00:24<00:20, 20313.07it/s] 54%|█████▍    | 489783/900675 [00:24<00:20, 20088.42it/s] 55%|█████▍    | 491795/900675 [00:24<00:20, 20002.25it/s] 55%|█████▍    | 493797/900675 [00:24<00:20, 19885.20it/s] 55%|█████▌    | 495955/900675 [00:24<00:19, 20383.41it/s] 55%|█████▌    | 498013/900675 [00:24<00:19, 20440.61it/s] 56%|█████▌    | 500112/900675 [00:24<00:19, 20600.23it/s] 56%|█████▌    | 502173/900675 [00:24<00:19, 20300.73it/s] 56%|█████▌    | 504205/900675 [00:24<00:19, 20124.83it/s] 56%|█████▌    | 506329/900675 [00:25<00:19, 20451.22it/s] 56%|█████▋    | 508376/900675 [00:25<00:19, 20076.58it/s] 57%|█████▋    | 510429/900675 [00:25<00:19, 20206.25it/s] 57%|█████▋    | 512521/900675 [00:25<00:19, 20413.44it/s] 57%|█████▋    | 514581/900675 [00:25<00:18, 20466.95it/s] 57%|█████▋    | 516629/900675 [00:25<00:18, 20338.05it/s] 58%|█████▊    | 518671/900675 [00:25<00:18, 20360.06it/s] 58%|█████▊    | 520708/900675 [00:25<00:18, 20185.80it/s] 58%|█████▊    | 522765/900675 [00:25<00:18, 20292.80it/s] 58%|█████▊    | 524795/900675 [00:25<00:18, 20189.17it/s] 58%|█████▊    | 526815/900675 [00:26<00:18, 20001.46it/s] 59%|█████▊    | 528820/900675 [00:26<00:18, 20009.14it/s] 59%|█████▉    | 530822/900675 [00:26<00:18, 19775.48it/s] 59%|█████▉    | 532919/900675 [00:26<00:18, 20127.49it/s] 59%|█████▉    | 534933/900675 [00:26<00:18, 19782.95it/s] 60%|█████▉    | 536964/900675 [00:26<00:18, 19926.96it/s] 60%|█████▉    | 539039/900675 [00:26<00:17, 20169.64it/s] 60%|██████    | 541058/900675 [00:26<00:18, 19785.17it/s] 60%|██████    | 543156/900675 [00:26<00:17, 20134.65it/s] 61%|██████    | 545172/900675 [00:26<00:17, 19895.43it/s] 61%|██████    | 547189/900675 [00:27<00:17, 19976.09it/s] 61%|██████    | 549294/900675 [00:27<00:17, 20290.62it/s] 61%|██████    | 551327/900675 [00:27<00:17, 20299.36it/s] 61%|██████▏   | 553358/900675 [00:27<00:17, 20164.71it/s] 62%|██████▏   | 555376/900675 [00:27<00:17, 19864.61it/s] 62%|██████▏   | 557442/900675 [00:27<00:17, 20094.18it/s] 62%|██████▏   | 559453/900675 [00:27<00:17, 19717.50it/s] 62%|██████▏   | 561441/900675 [00:27<00:17, 19761.83it/s] 63%|██████▎   | 563531/900675 [00:27<00:16, 20090.32it/s] 63%|██████▎   | 565542/900675 [00:27<00:16, 20050.80it/s] 63%|██████▎   | 567549/900675 [00:28<00:16, 19948.70it/s] 63%|██████▎   | 569610/900675 [00:28<00:16, 20144.65it/s] 64%|██████▎   | 571931/900675 [00:28<00:15, 21056.32it/s] 64%|██████▎   | 574038/900675 [00:28<00:15, 20705.60it/s] 64%|██████▍   | 576111/900675 [00:28<00:16, 20159.73it/s] 64%|██████▍   | 578131/900675 [00:28<00:16, 19953.07it/s] 64%|██████▍   | 580196/900675 [00:28<00:15, 20154.39it/s] 65%|██████▍   | 582266/900675 [00:28<00:15, 20312.15it/s] 65%|██████▍   | 584598/900675 [00:28<00:14, 21201.33it/s] 65%|██████▌   | 586721/900675 [00:28<00:15, 20818.58it/s] 65%|██████▌   | 588806/900675 [00:29<00:15, 20203.15it/s] 66%|██████▌   | 590900/900675 [00:29<00:15, 20411.82it/s] 66%|██████▌   | 592946/900675 [00:29<00:15, 19950.50it/s] 66%|██████▌   | 595035/900675 [00:29<00:15, 20213.01it/s] 66%|██████▋   | 597061/900675 [00:29<00:15, 20039.91it/s] 67%|██████▋   | 599068/900675 [00:29<00:15, 19817.01it/s] 67%|██████▋   | 601106/900675 [00:29<00:14, 19975.70it/s] 67%|██████▋   | 603106/900675 [00:29<00:15, 19413.70it/s] 67%|██████▋   | 605187/900675 [00:29<00:14, 19817.16it/s] 67%|██████▋   | 607221/900675 [00:30<00:14, 19969.18it/s] 68%|██████▊   | 609222/900675 [00:30<00:14, 19603.57it/s] 68%|██████▊   | 611370/900675 [00:30<00:14, 20151.80it/s] 68%|██████▊   | 613389/900675 [00:30<00:14, 20141.11it/s] 68%|██████▊   | 615406/900675 [00:30<00:14, 19727.06it/s] 69%|██████▊   | 617601/900675 [00:30<00:13, 20375.75it/s] 69%|██████▉   | 619906/900675 [00:30<00:13, 21162.56it/s] 69%|██████▉   | 622027/900675 [00:30<00:13, 21080.24it/s] 69%|██████▉   | 624138/900675 [00:30<00:13, 20595.57it/s] 70%|██████▉   | 626306/900675 [00:30<00:13, 20912.72it/s] 70%|██████▉   | 628401/900675 [00:31<00:13, 20457.59it/s] 70%|██████▉   | 630451/900675 [00:31<00:13, 20368.92it/s] 70%|███████   | 632674/900675 [00:31<00:12, 20909.60it/s] 70%|███████   | 634793/900675 [00:31<00:12, 20991.79it/s] 71%|███████   | 636895/900675 [00:31<00:12, 20706.30it/s] 71%|███████   | 639040/900675 [00:31<00:12, 20923.24it/s] 71%|███████   | 641220/900675 [00:31<00:12, 21182.52it/s] 71%|███████▏  | 643340/900675 [00:31<00:12, 20482.28it/s] 72%|███████▏  | 645394/900675 [00:31<00:12, 20285.88it/s] 72%|███████▏  | 647427/900675 [00:31<00:12, 20078.63it/s] 72%|███████▏  | 649438/900675 [00:32<00:13, 19272.25it/s] 72%|███████▏  | 651411/900675 [00:32<00:12, 19402.75it/s] 73%|███████▎  | 653486/900675 [00:32<00:12, 19792.39it/s] 73%|███████▎  | 655512/900675 [00:32<00:12, 19918.92it/s] 73%|███████▎  | 657606/900675 [00:32<00:12, 20219.63it/s] 73%|███████▎  | 659632/900675 [00:32<00:11, 20105.94it/s] 73%|███████▎  | 661645/900675 [00:32<00:11, 20013.54it/s] 74%|███████▎  | 663648/900675 [00:32<00:11, 19925.48it/s] 74%|███████▍  | 665772/900675 [00:32<00:11, 20313.89it/s] 74%|███████▍  | 667805/900675 [00:32<00:11, 20224.97it/s] 74%|███████▍  | 669953/900675 [00:33<00:11, 20595.59it/s] 75%|███████▍  | 672059/900675 [00:33<00:11, 20727.25it/s] 75%|███████▍  | 674133/900675 [00:33<00:11, 20514.98it/s] 75%|███████▌  | 676204/900675 [00:33<00:10, 20568.31it/s] 75%|███████▌  | 678262/900675 [00:33<00:11, 20169.86it/s] 76%|███████▌  | 680308/900675 [00:33<00:10, 20253.44it/s] 76%|███████▌  | 682335/900675 [00:33<00:10, 20115.09it/s] 76%|███████▌  | 684473/900675 [00:33<00:10, 20488.51it/s] 76%|███████▌  | 686524/900675 [00:33<00:10, 20386.49it/s] 76%|███████▋  | 688564/900675 [00:34<00:10, 19975.45it/s] 77%|███████▋  | 690564/900675 [00:34<00:10, 19680.19it/s] 77%|███████▋  | 692534/900675 [00:34<00:10, 19279.36it/s] 77%|███████▋  | 694503/900675 [00:34<00:10, 19392.00it/s] 77%|███████▋  | 696445/900675 [00:34<00:10, 19330.18it/s] 78%|███████▊  | 698486/900675 [00:34<00:10, 19639.13it/s] 78%|███████▊  | 700452/900675 [00:34<00:10, 19592.21it/s] 78%|███████▊  | 702636/900675 [00:34<00:09, 20257.67it/s] 78%|███████▊  | 704664/900675 [00:34<00:09, 20240.87it/s] 78%|███████▊  | 706690/900675 [00:34<00:09, 19704.10it/s] 79%|███████▊  | 708706/900675 [00:35<00:09, 19835.35it/s] 79%|███████▉  | 710742/900675 [00:35<00:09, 19984.39it/s] 79%|███████▉  | 712869/900675 [00:35<00:09, 20362.25it/s] 79%|███████▉  | 714908/900675 [00:35<00:09, 20161.02it/s] 80%|███████▉  | 717037/900675 [00:35<00:08, 20494.64it/s] 80%|███████▉  | 719089/900675 [00:35<00:09, 19925.40it/s] 80%|████████  | 721086/900675 [00:35<00:09, 19727.56it/s] 80%|████████  | 723102/900675 [00:35<00:08, 19849.57it/s] 81%|████████  | 725136/900675 [00:35<00:08, 19993.17it/s] 81%|████████  | 727138/900675 [00:35<00:08, 19761.01it/s] 81%|████████  | 729116/900675 [00:36<00:08, 19753.79it/s] 81%|████████  | 731093/900675 [00:36<00:08, 19675.17it/s] 81%|████████▏ | 733098/900675 [00:36<00:08, 19777.80it/s] 82%|████████▏ | 735225/900675 [00:36<00:08, 20217.55it/s] 82%|████████▏ | 737419/900675 [00:36<00:07, 20728.31it/s] 82%|████████▏ | 739493/900675 [00:36<00:07, 20449.95it/s] 82%|████████▏ | 741540/900675 [00:36<00:07, 20336.33it/s] 83%|████████▎ | 743575/900675 [00:36<00:07, 20247.18it/s] 83%|████████▎ | 745601/900675 [00:36<00:07, 19701.60it/s] 83%|████████▎ | 747631/900675 [00:36<00:07, 19875.59it/s] 83%|████████▎ | 749622/900675 [00:37<00:07, 19760.71it/s] 83%|████████▎ | 751605/900675 [00:37<00:07, 19778.94it/s] 84%|████████▎ | 753671/900675 [00:37<00:07, 20035.44it/s] 84%|████████▍ | 755676/900675 [00:37<00:07, 19634.96it/s] 84%|████████▍ | 757644/900675 [00:37<00:07, 19645.69it/s] 84%|████████▍ | 759710/900675 [00:37<00:07, 19945.05it/s] 85%|████████▍ | 761835/900675 [00:37<00:06, 20326.76it/s] 85%|████████▍ | 763870/900675 [00:37<00:06, 19791.76it/s] 85%|████████▌ | 765854/900675 [00:37<00:06, 19631.60it/s] 85%|████████▌ | 767999/900675 [00:38<00:06, 20157.67it/s] 85%|████████▌ | 770018/900675 [00:38<00:06, 19448.46it/s] 86%|████████▌ | 772039/900675 [00:38<00:06, 19668.16it/s] 86%|████████▌ | 774147/900675 [00:38<00:06, 20074.55it/s] 86%|████████▌ | 776160/900675 [00:38<00:06, 19623.92it/s] 86%|████████▋ | 778172/900675 [00:38<00:06, 19764.42it/s] 87%|████████▋ | 780153/900675 [00:38<00:06, 19664.27it/s] 87%|████████▋ | 782236/900675 [00:38<00:05, 20006.62it/s] 87%|████████▋ | 784240/900675 [00:38<00:06, 19339.18it/s] 87%|████████▋ | 786258/900675 [00:38<00:05, 19580.43it/s] 88%|████████▊ | 788293/900675 [00:39<00:05, 19805.75it/s] 88%|████████▊ | 790278/900675 [00:39<00:05, 19816.55it/s] 88%|████████▊ | 792263/900675 [00:39<00:05, 19567.01it/s] 88%|████████▊ | 794225/900675 [00:39<00:05, 19581.65it/s] 88%|████████▊ | 796260/900675 [00:39<00:05, 19808.32it/s] 89%|████████▊ | 798316/900675 [00:39<00:05, 20031.47it/s] 89%|████████▉ | 800323/900675 [00:39<00:05, 20042.78it/s] 89%|████████▉ | 802329/900675 [00:39<00:04, 19825.04it/s] 89%|████████▉ | 804352/900675 [00:39<00:04, 19944.12it/s] 90%|████████▉ | 806348/900675 [00:39<00:04, 19920.12it/s] 90%|████████▉ | 808341/900675 [00:40<00:04, 19416.19it/s] 90%|████████▉ | 810304/900675 [00:40<00:04, 19478.70it/s] 90%|█████████ | 812255/900675 [00:40<00:04, 19269.20it/s] 90%|█████████ | 814373/900675 [00:40<00:04, 19831.39it/s] 91%|█████████ | 816717/900675 [00:40<00:04, 20893.85it/s] 91%|█████████ | 818859/900675 [00:40<00:03, 21043.59it/s] 91%|█████████ | 820977/900675 [00:40<00:03, 21082.62it/s] 91%|█████████▏| 823087/900675 [00:40<00:03, 20556.40it/s] 92%|█████████▏| 825147/900675 [00:40<00:03, 20071.43it/s] 92%|█████████▏| 827159/900675 [00:40<00:03, 19939.83it/s] 92%|█████████▏| 829156/900675 [00:41<00:03, 19322.47it/s] 92%|█████████▏| 831125/900675 [00:41<00:03, 19423.41it/s] 93%|█████████▎| 833141/900675 [00:41<00:03, 19637.25it/s] 93%|█████████▎| 835167/900675 [00:41<00:03, 19813.29it/s] 93%|█████████▎| 837260/900675 [00:41<00:03, 20139.12it/s] 93%|█████████▎| 839277/900675 [00:41<00:03, 19911.91it/s] 93%|█████████▎| 841392/900675 [00:41<00:02, 20277.08it/s] 94%|█████████▎| 843422/900675 [00:41<00:02, 19504.93it/s] 94%|█████████▍| 845380/900675 [00:41<00:02, 18987.68it/s] 94%|█████████▍| 847378/900675 [00:42<00:02, 19271.99it/s] 94%|█████████▍| 849359/900675 [00:42<00:02, 19425.87it/s] 95%|█████████▍| 851307/900675 [00:42<00:02, 19351.97it/s] 95%|█████████▍| 853246/900675 [00:42<00:02, 19275.55it/s] 95%|█████████▍| 855224/900675 [00:42<00:02, 19423.54it/s] 95%|█████████▌| 857310/900675 [00:42<00:02, 19843.02it/s] 95%|█████████▌| 859296/900675 [00:42<00:02, 19599.04it/s] 96%|█████████▌| 861488/900675 [00:42<00:01, 20284.63it/s] 96%|█████████▌| 863548/900675 [00:42<00:01, 20372.94it/s] 96%|█████████▌| 865587/900675 [00:42<00:01, 20150.14it/s] 96%|█████████▋| 867619/900675 [00:43<00:01, 20198.57it/s] 97%|█████████▋| 869640/900675 [00:43<00:01, 19885.91it/s] 97%|█████████▋| 871859/900675 [00:43<00:01, 20561.52it/s] 97%|█████████▋| 873918/900675 [00:43<00:01, 20314.90it/s] 97%|█████████▋| 875992/900675 [00:43<00:01, 20439.55it/s] 97%|█████████▋| 878038/900675 [00:43<00:01, 20237.75it/s] 98%|█████████▊| 880064/900675 [00:43<00:01, 20006.01it/s] 98%|█████████▊| 882066/900675 [00:43<00:00, 19281.94it/s] 98%|█████████▊| 884153/900675 [00:43<00:00, 19737.53it/s] 98%|█████████▊| 886230/900675 [00:43<00:00, 20038.33it/s] 99%|█████████▊| 888239/900675 [00:44<00:00, 19990.74it/s] 99%|█████████▉| 890242/900675 [00:44<00:00, 19886.56it/s] 99%|█████████▉| 892234/900675 [00:44<00:00, 19890.59it/s] 99%|█████████▉| 894225/900675 [00:44<00:00, 19373.24it/s]100%|█████████▉| 896280/900675 [00:44<00:00, 19713.06it/s]100%|█████████▉| 898349/900675 [00:44<00:00, 19999.64it/s]100%|█████████▉| 900386/900675 [00:44<00:00, 20100.97it/s]100%|██████████| 900675/900675 [00:44<00:00, 20154.51it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 15.92it/s]2022-02-25 06:48:07 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-25 06:48:07 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-25 06:48:07 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-25 06:48:07 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-02-25 06:48:07 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-25 06:48:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-25 06:48:07 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-25 06:48:08 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-25 06:48:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 06:48:08 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-25 06:48:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 06:48:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-25 06:48:08 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-02-25 06:48:08 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_last.pt
2022-02-25 06:48:08 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_last.pt
2022-02-25 06:48:08 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-25 06:48:08 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-25 06:48:08 | INFO | fairseq.trainer | begin training epoch 1
2022-02-25 06:48:08 | INFO | fairseq_cli.train | Start iterating over samples

2022-02-25 06:48:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-25 06:48:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 06:48:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 06:49:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 06:50:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-25 06:59:59 | INFO | train_inner | epoch 001:    105 / 788 loss=17.487, ppl=183740, wps=10107.8, ups=0.15, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.438, loss_scale=4, train_wall=705, gb_free=4, wall=711
2022-02-25 07:10:41 | INFO | train_inner | epoch 001:    205 / 788 loss=15.106, ppl=35272.7, wps=10208.3, ups=0.16, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.697, loss_scale=4, train_wall=637, gb_free=4, wall=1353
2022-02-25 07:21:22 | INFO | train_inner | epoch 001:    305 / 788 loss=12.893, ppl=7603.96, wps=10212.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.187, loss_scale=4, train_wall=637, gb_free=4, wall=1994
2022-02-25 07:32:04 | INFO | train_inner | epoch 001:    405 / 788 loss=11.269, ppl=2467.16, wps=10217.7, ups=0.16, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.657, loss_scale=4, train_wall=637, gb_free=4, wall=2636
2022-02-25 07:42:45 | INFO | train_inner | epoch 001:    505 / 788 loss=10.62, ppl=1573.42, wps=10218.9, ups=0.16, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.491, loss_scale=4, train_wall=636, gb_free=4, wall=3277
2022-02-25 07:53:26 | INFO | train_inner | epoch 001:    605 / 788 loss=10.29, ppl=1252.2, wps=10217.7, ups=0.16, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.581, loss_scale=8, train_wall=637, gb_free=4, wall=3919
2022-02-25 08:04:08 | INFO | train_inner | epoch 001:    705 / 788 loss=10.022, ppl=1040.09, wps=10220.6, ups=0.16, wpb=65534.7, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.671, loss_scale=8, train_wall=636, gb_free=4, wall=4560
2022-02-25 08:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 08:13:06 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.628 | ppl 791.06 | wps 23699.7 | wpb 2034.1 | bsz 4 | num_updates 783
2022-02-25 08:13:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 783 updates
2022-02-25 08:13:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 08:13:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 08:13:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 1 @ 783 updates, score 9.628) (writing took 6.623449483886361 seconds)
2022-02-25 08:13:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-25 08:13:13 | INFO | train | epoch 001 | loss 12.239 | ppl 4834.02 | wps 10170.5 | ups 0.16 | wpb 65497.3 | bsz 127.9 | num_updates 783 | lr 9.79554e-05 | gnorm 1.189 | loss_scale 8 | train_wall 5050 | gb_free 4 | wall 5105
2022-02-25 08:13:13 | INFO | fairseq.trainer | begin training epoch 2
2022-02-25 08:13:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 08:15:02 | INFO | train_inner | epoch 002:     17 / 788 loss=9.78, ppl=879.35, wps=9971.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.726, loss_scale=8, train_wall=634, gb_free=4, wall=5214
2022-02-25 08:25:43 | INFO | train_inner | epoch 002:    117 / 788 loss=9.559, ppl=754.12, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.847, loss_scale=8, train_wall=636, gb_free=4, wall=5855
2022-02-25 08:36:25 | INFO | train_inner | epoch 002:    217 / 788 loss=9.356, ppl=655.43, wps=10219.3, ups=0.16, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.855, loss_scale=8, train_wall=636, gb_free=4, wall=6497
2022-02-25 08:47:06 | INFO | train_inner | epoch 002:    317 / 788 loss=9.187, ppl=582.79, wps=10212.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.89, loss_scale=16, train_wall=637, gb_free=4, wall=7138
2022-02-25 08:57:48 | INFO | train_inner | epoch 002:    417 / 788 loss=9.047, ppl=528.93, wps=10210.6, ups=0.16, wpb=65534.7, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.895, loss_scale=16, train_wall=637, gb_free=4, wall=7780
2022-02-25 09:08:30 | INFO | train_inner | epoch 002:    517 / 788 loss=8.912, ppl=481.84, wps=10212.4, ups=0.16, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.882, loss_scale=16, train_wall=637, gb_free=4, wall=8422
2022-02-25 09:19:12 | INFO | train_inner | epoch 002:    617 / 788 loss=8.783, ppl=440.5, wps=10213.5, ups=0.16, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.885, loss_scale=16, train_wall=637, gb_free=4, wall=9064
2022-02-25 09:29:53 | INFO | train_inner | epoch 002:    717 / 788 loss=8.652, ppl=402.25, wps=10215.2, ups=0.16, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.929, loss_scale=16, train_wall=637, gb_free=4, wall=9705
2022-02-25 09:37:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 09:37:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.363 | ppl 329.35 | wps 23675.8 | wpb 2034.1 | bsz 4 | num_updates 1571 | best_loss 8.363
2022-02-25 09:37:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1571 updates
2022-02-25 09:37:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 09:37:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 09:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 2 @ 1571 updates, score 8.363) (writing took 6.394646652042866 seconds)
2022-02-25 09:37:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-25 09:37:41 | INFO | train | epoch 002 | loss 9.038 | ppl 525.63 | wps 10183 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 1571 | lr 0.000196436 | gnorm 0.883 | loss_scale 32 | train_wall 5014 | gb_free 4 | wall 10173
2022-02-25 09:37:41 | INFO | fairseq.trainer | begin training epoch 3
2022-02-25 09:37:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 09:40:48 | INFO | train_inner | epoch 003:     29 / 788 loss=8.524, ppl=368.08, wps=9969.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.892, loss_scale=32, train_wall=634, gb_free=4, wall=10360
2022-02-25 09:51:29 | INFO | train_inner | epoch 003:    129 / 788 loss=8.388, ppl=334.91, wps=10212, ups=0.16, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.905, loss_scale=32, train_wall=637, gb_free=4, wall=11001
2022-02-25 10:02:11 | INFO | train_inner | epoch 003:    229 / 788 loss=8.29, ppl=312.93, wps=10212.2, ups=0.16, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.908, loss_scale=32, train_wall=637, gb_free=4, wall=11643
2022-02-25 10:12:53 | INFO | train_inner | epoch 003:    329 / 788 loss=8.208, ppl=295.67, wps=10210.8, ups=0.16, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.874, loss_scale=32, train_wall=637, gb_free=4, wall=12285
2022-02-25 10:23:35 | INFO | train_inner | epoch 003:    429 / 788 loss=8.109, ppl=276.19, wps=10213.7, ups=0.16, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.864, loss_scale=32, train_wall=637, gb_free=4, wall=12927
2022-02-25 10:32:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 10:34:23 | INFO | train_inner | epoch 003:    530 / 788 loss=8.031, ppl=261.54, wps=10112.1, ups=0.15, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.847, loss_scale=32, train_wall=643, gb_free=4, wall=13575
2022-02-25 10:45:04 | INFO | train_inner | epoch 003:    630 / 788 loss=7.937, ppl=245.03, wps=10211.6, ups=0.16, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.842, loss_scale=32, train_wall=637, gb_free=4, wall=14216
2022-02-25 10:55:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 10:55:53 | INFO | train_inner | epoch 003:    731 / 788 loss=7.868, ppl=233.57, wps=10111.8, ups=0.15, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.821, loss_scale=16, train_wall=643, gb_free=4, wall=14865
2022-02-25 11:01:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 11:02:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.647 | ppl 200.49 | wps 23666.7 | wpb 2034.1 | bsz 4 | num_updates 2357 | best_loss 7.647
2022-02-25 11:02:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2357 updates
2022-02-25 11:02:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 11:02:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 11:02:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 3 @ 2357 updates, score 7.647) (writing took 6.766252690926194 seconds)
2022-02-25 11:02:11 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-25 11:02:11 | INFO | train | epoch 003 | loss 8.109 | ppl 276.07 | wps 10154.7 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 2357 | lr 0.000294666 | gnorm 0.862 | loss_scale 16 | train_wall 5015 | gb_free 4 | wall 15243
2022-02-25 11:02:11 | INFO | fairseq.trainer | begin training epoch 4
2022-02-25 11:02:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 11:06:47 | INFO | train_inner | epoch 004:     43 / 788 loss=7.77, ppl=218.29, wps=9967.4, ups=0.15, wpb=65232.6, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.805, loss_scale=16, train_wall=634, gb_free=4, wall=15519
2022-02-25 11:17:29 | INFO | train_inner | epoch 004:    143 / 788 loss=7.668, ppl=203.31, wps=10214.8, ups=0.16, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.81, loss_scale=16, train_wall=637, gb_free=4, wall=16161
2022-02-25 11:28:10 | INFO | train_inner | epoch 004:    243 / 788 loss=7.607, ppl=194.95, wps=10213.1, ups=0.16, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.781, loss_scale=16, train_wall=637, gb_free=4, wall=16802
2022-02-25 11:38:52 | INFO | train_inner | epoch 004:    343 / 788 loss=7.551, ppl=187.48, wps=10214.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.791, loss_scale=16, train_wall=637, gb_free=4, wall=17444
2022-02-25 11:49:33 | INFO | train_inner | epoch 004:    443 / 788 loss=7.495, ppl=180.38, wps=10214.7, ups=0.16, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.755, loss_scale=16, train_wall=637, gb_free=4, wall=18085
2022-02-25 11:56:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 12:00:21 | INFO | train_inner | epoch 004:    544 / 788 loss=7.441, ppl=173.73, wps=10112.1, ups=0.15, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.767, loss_scale=16, train_wall=643, gb_free=4, wall=18734
2022-02-25 12:11:03 | INFO | train_inner | epoch 004:    644 / 788 loss=7.396, ppl=168.49, wps=10214.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.752, loss_scale=16, train_wall=637, gb_free=4, wall=19375
2022-02-25 12:21:44 | INFO | train_inner | epoch 004:    744 / 788 loss=7.343, ppl=162.34, wps=10215.4, ups=0.16, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.727, loss_scale=16, train_wall=637, gb_free=4, wall=20017
2022-02-25 12:26:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 12:26:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.214 | ppl 148.5 | wps 23640.4 | wpb 2034.1 | bsz 4 | num_updates 3144 | best_loss 7.214
2022-02-25 12:26:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3144 updates
2022-02-25 12:26:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 12:26:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 12:26:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 4 @ 3144 updates, score 7.214) (writing took 7.070698992349207 seconds)
2022-02-25 12:26:40 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-25 12:26:40 | INFO | train | epoch 004 | loss 7.5 | ppl 181.04 | wps 10168.6 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 3144 | lr 0.000393021 | gnorm 0.768 | loss_scale 16 | train_wall 5014 | gb_free 4 | wall 20312
2022-02-25 12:26:40 | INFO | fairseq.trainer | begin training epoch 5
2022-02-25 12:26:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 12:32:39 | INFO | train_inner | epoch 005:     56 / 788 loss=7.238, ppl=150.92, wps=9960.5, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.74, loss_scale=16, train_wall=634, gb_free=4, wall=20671
2022-02-25 12:43:21 | INFO | train_inner | epoch 005:    156 / 788 loss=7.163, ppl=143.33, wps=10212.4, ups=0.16, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.731, loss_scale=16, train_wall=637, gb_free=4, wall=21313
2022-02-25 12:54:03 | INFO | train_inner | epoch 005:    256 / 788 loss=7.138, ppl=140.82, wps=10214.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.731, loss_scale=32, train_wall=637, gb_free=4, wall=21955
2022-02-25 13:04:45 | INFO | train_inner | epoch 005:    356 / 788 loss=7.09, ppl=136.23, wps=10211.1, ups=0.16, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.708, loss_scale=32, train_wall=637, gb_free=4, wall=22597
2022-02-25 13:10:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 13:15:33 | INFO | train_inner | epoch 005:    457 / 788 loss=7.078, ppl=135.15, wps=10113.1, ups=0.15, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.712, loss_scale=16, train_wall=643, gb_free=4, wall=23245
2022-02-25 13:26:14 | INFO | train_inner | epoch 005:    557 / 788 loss=7.035, ppl=131.14, wps=10215.5, ups=0.16, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.694, loss_scale=16, train_wall=637, gb_free=4, wall=23886
2022-02-25 13:36:56 | INFO | train_inner | epoch 005:    657 / 788 loss=7.024, ppl=130.14, wps=10216.2, ups=0.16, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.703, loss_scale=16, train_wall=637, gb_free=4, wall=24528
2022-02-25 13:47:37 | INFO | train_inner | epoch 005:    757 / 788 loss=6.98, ppl=126.27, wps=10215.5, ups=0.16, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.678, loss_scale=16, train_wall=637, gb_free=4, wall=25169
2022-02-25 13:50:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 13:51:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.891 | ppl 118.69 | wps 23700.7 | wpb 2034.1 | bsz 4 | num_updates 3931 | best_loss 6.891
2022-02-25 13:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3931 updates
2022-02-25 13:51:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 13:51:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 13:51:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 5 @ 3931 updates, score 6.891) (writing took 7.0648430390283465 seconds)
2022-02-25 13:51:09 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-25 13:51:09 | INFO | train | epoch 005 | loss 7.078 | ppl 135.07 | wps 10168.5 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 3931 | lr 0.000491377 | gnorm 0.711 | loss_scale 16 | train_wall 5014 | gb_free 4 | wall 25381
2022-02-25 13:51:09 | INFO | fairseq.trainer | begin training epoch 6
2022-02-25 13:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 13:58:32 | INFO | train_inner | epoch 006:     69 / 788 loss=6.864, ppl=116.49, wps=9960.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.68, loss_scale=16, train_wall=634, gb_free=4, wall=25824
2022-02-25 14:05:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 14:09:20 | INFO | train_inner | epoch 006:    170 / 788 loss=6.811, ppl=112.3, wps=10115.6, ups=0.15, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.653, loss_scale=16, train_wall=643, gb_free=4, wall=26472
2022-02-25 14:20:02 | INFO | train_inner | epoch 006:    270 / 788 loss=6.807, ppl=111.94, wps=10216.6, ups=0.16, wpb=65534.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.649, loss_scale=16, train_wall=637, gb_free=4, wall=27114
2022-02-25 14:30:43 | INFO | train_inner | epoch 006:    370 / 788 loss=6.784, ppl=110.18, wps=10216.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.661, loss_scale=16, train_wall=636, gb_free=4, wall=27755
2022-02-25 14:41:24 | INFO | train_inner | epoch 006:    470 / 788 loss=6.765, ppl=108.79, wps=10217.2, ups=0.16, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.612, loss_scale=16, train_wall=637, gb_free=4, wall=28396
2022-02-25 14:52:06 | INFO | train_inner | epoch 006:    570 / 788 loss=6.746, ppl=107.35, wps=10218.3, ups=0.16, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.636, loss_scale=16, train_wall=636, gb_free=4, wall=29038
2022-02-25 15:01:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 15:02:54 | INFO | train_inner | epoch 006:    671 / 788 loss=6.725, ppl=105.79, wps=10114.1, ups=0.15, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.594, loss_scale=16, train_wall=643, gb_free=4, wall=29686
2022-02-25 15:13:35 | INFO | train_inner | epoch 006:    771 / 788 loss=6.715, ppl=105.08, wps=10214.3, ups=0.16, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.594, loss_scale=16, train_wall=637, gb_free=4, wall=30327
2022-02-25 15:15:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 15:15:31 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.679 | ppl 102.43 | wps 23714.4 | wpb 2034.1 | bsz 4 | num_updates 4717 | best_loss 6.679
2022-02-25 15:15:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4717 updates
2022-02-25 15:15:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 15:15:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 15:15:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 6 @ 4717 updates, score 6.679) (writing took 7.110042339190841 seconds)
2022-02-25 15:15:38 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-25 15:15:38 | INFO | train | epoch 006 | loss 6.768 | ppl 108.99 | wps 10157.3 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 4717 | lr 0.000460434 | gnorm 0.632 | loss_scale 16 | train_wall 5013 | gb_free 4 | wall 30450
2022-02-25 15:15:38 | INFO | fairseq.trainer | begin training epoch 7
2022-02-25 15:15:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 15:24:31 | INFO | train_inner | epoch 007:     83 / 788 loss=6.572, ppl=95.17, wps=9957.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.597, loss_scale=16, train_wall=634, gb_free=4, wall=30983
2022-02-25 15:35:12 | INFO | train_inner | epoch 007:    183 / 788 loss=6.541, ppl=93.11, wps=10212.6, ups=0.16, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.592, loss_scale=16, train_wall=637, gb_free=4, wall=31624
2022-02-25 15:45:54 | INFO | train_inner | epoch 007:    283 / 788 loss=6.545, ppl=93.39, wps=10213.4, ups=0.16, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.59, loss_scale=16, train_wall=637, gb_free=4, wall=32266
2022-02-25 15:56:36 | INFO | train_inner | epoch 007:    383 / 788 loss=6.546, ppl=93.43, wps=10213.4, ups=0.16, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.563, loss_scale=32, train_wall=637, gb_free=4, wall=32908
2022-02-25 15:58:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 16:07:24 | INFO | train_inner | epoch 007:    484 / 788 loss=6.532, ppl=92.52, wps=10107.2, ups=0.15, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.576, loss_scale=16, train_wall=643, gb_free=4, wall=33556
2022-02-25 16:18:06 | INFO | train_inner | epoch 007:    584 / 788 loss=6.527, ppl=92.23, wps=10215.2, ups=0.16, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.592, loss_scale=16, train_wall=637, gb_free=4, wall=34198
2022-02-25 16:28:47 | INFO | train_inner | epoch 007:    684 / 788 loss=6.53, ppl=92.41, wps=10215.8, ups=0.16, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.565, loss_scale=16, train_wall=637, gb_free=4, wall=34839
2022-02-25 16:39:29 | INFO | train_inner | epoch 007:    784 / 788 loss=6.511, ppl=91.19, wps=10214.2, ups=0.16, wpb=65519.3, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.557, loss_scale=16, train_wall=637, gb_free=4, wall=35481
2022-02-25 16:39:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 16:40:01 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.543 | ppl 93.24 | wps 23686.9 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 6.543
2022-02-25 16:40:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5504 updates
2022-02-25 16:40:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 16:40:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 16:40:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 7 @ 5504 updates, score 6.543) (writing took 7.425602934323251 seconds)
2022-02-25 16:40:08 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-25 16:40:08 | INFO | train | epoch 007 | loss 6.534 | ppl 92.69 | wps 10166.6 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 5504 | lr 0.000426246 | gnorm 0.578 | loss_scale 16 | train_wall 5015 | gb_free 4 | wall 35520
2022-02-25 16:40:08 | INFO | fairseq.trainer | begin training epoch 8
2022-02-25 16:40:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 16:50:24 | INFO | train_inner | epoch 008:     96 / 788 loss=6.362, ppl=82.27, wps=9955.6, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=5600, lr=0.000422577, gnorm=0.568, loss_scale=16, train_wall=634, gb_free=4, wall=36136
2022-02-25 16:55:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 17:01:12 | INFO | train_inner | epoch 008:    197 / 788 loss=6.367, ppl=82.53, wps=10114.9, ups=0.15, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.571, loss_scale=16, train_wall=643, gb_free=4, wall=36784
2022-02-25 17:11:53 | INFO | train_inner | epoch 008:    297 / 788 loss=6.37, ppl=82.71, wps=10218.6, ups=0.16, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.554, loss_scale=16, train_wall=636, gb_free=4, wall=37425
2022-02-25 17:22:35 | INFO | train_inner | epoch 008:    397 / 788 loss=6.373, ppl=82.86, wps=10215.7, ups=0.16, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.569, loss_scale=16, train_wall=637, gb_free=4, wall=38067
2022-02-25 17:33:16 | INFO | train_inner | epoch 008:    497 / 788 loss=6.366, ppl=82.46, wps=10217.7, ups=0.16, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.562, loss_scale=16, train_wall=636, gb_free=4, wall=38708
2022-02-25 17:43:57 | INFO | train_inner | epoch 008:    597 / 788 loss=6.388, ppl=83.76, wps=10215.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.575, loss_scale=16, train_wall=637, gb_free=4, wall=39349
2022-02-25 17:54:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 17:54:45 | INFO | train_inner | epoch 008:    698 / 788 loss=6.372, ppl=82.81, wps=10113.7, ups=0.15, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.553, loss_scale=16, train_wall=643, gb_free=4, wall=39997
2022-02-25 18:04:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 18:04:29 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.45 | ppl 87.43 | wps 23695.3 | wpb 2034.1 | bsz 4 | num_updates 6290 | best_loss 6.45
2022-02-25 18:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6290 updates
2022-02-25 18:04:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 18:04:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 18:04:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 8 @ 6290 updates, score 6.45) (writing took 7.538696979172528 seconds)
2022-02-25 18:04:37 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-25 18:04:37 | INFO | train | epoch 008 | loss 6.37 | ppl 82.72 | wps 10156.7 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 6290 | lr 0.000398726 | gnorm 0.562 | loss_scale 16 | train_wall 5013 | gb_free 4 | wall 40589
2022-02-25 18:04:37 | INFO | fairseq.trainer | begin training epoch 9
2022-02-25 18:04:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 18:05:41 | INFO | train_inner | epoch 009:     10 / 788 loss=6.352, ppl=81.7, wps=9954.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.543, loss_scale=16, train_wall=634, gb_free=4, wall=40653
2022-02-25 18:16:22 | INFO | train_inner | epoch 009:    110 / 788 loss=6.22, ppl=74.53, wps=10216.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.555, loss_scale=16, train_wall=637, gb_free=4, wall=41294
2022-02-25 18:27:04 | INFO | train_inner | epoch 009:    210 / 788 loss=6.231, ppl=75.11, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.565, loss_scale=16, train_wall=636, gb_free=4, wall=41936
2022-02-25 18:37:45 | INFO | train_inner | epoch 009:    310 / 788 loss=6.231, ppl=75.14, wps=10215.7, ups=0.16, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.551, loss_scale=16, train_wall=637, gb_free=4, wall=42577
2022-02-25 18:48:26 | INFO | train_inner | epoch 009:    410 / 788 loss=6.262, ppl=76.76, wps=10219.2, ups=0.16, wpb=65520.6, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.551, loss_scale=16, train_wall=636, gb_free=4, wall=43218
2022-02-25 18:49:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 18:59:14 | INFO | train_inner | epoch 009:    511 / 788 loss=6.261, ppl=76.71, wps=10120, ups=0.15, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.556, loss_scale=16, train_wall=643, gb_free=4, wall=43866
2022-02-25 19:09:55 | INFO | train_inner | epoch 009:    611 / 788 loss=6.254, ppl=76.3, wps=10221, ups=0.16, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.534, loss_scale=16, train_wall=636, gb_free=4, wall=44507
2022-02-25 19:20:37 | INFO | train_inner | epoch 009:    711 / 788 loss=6.255, ppl=76.36, wps=10218, ups=0.16, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.56, loss_scale=16, train_wall=636, gb_free=4, wall=45149
2022-02-25 19:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 19:28:57 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.388 | ppl 83.74 | wps 23657.5 | wpb 2034.1 | bsz 4 | num_updates 7077 | best_loss 6.388
2022-02-25 19:28:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7077 updates
2022-02-25 19:28:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 19:29:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 19:29:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 9 @ 7077 updates, score 6.388) (writing took 7.3517157370224595 seconds)
2022-02-25 19:29:04 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-25 19:29:04 | INFO | train | epoch 009 | loss 6.246 | ppl 75.9 | wps 10172.2 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 7077 | lr 0.000375903 | gnorm 0.552 | loss_scale 16 | train_wall 5012 | gb_free 4 | wall 45656
2022-02-25 19:29:04 | INFO | fairseq.trainer | begin training epoch 10
2022-02-25 19:29:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 19:31:32 | INFO | train_inner | epoch 010:     23 / 788 loss=6.224, ppl=74.73, wps=9959.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.561, loss_scale=16, train_wall=634, gb_free=4, wall=45804
2022-02-25 19:42:13 | INFO | train_inner | epoch 010:    123 / 788 loss=6.107, ppl=68.91, wps=10218.4, ups=0.16, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.537, loss_scale=16, train_wall=636, gb_free=4, wall=46445
2022-02-25 19:44:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 19:53:01 | INFO | train_inner | epoch 010:    224 / 788 loss=6.133, ppl=70.17, wps=10117.2, ups=0.15, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.56, loss_scale=16, train_wall=643, gb_free=4, wall=47093
2022-02-25 20:03:42 | INFO | train_inner | epoch 010:    324 / 788 loss=6.142, ppl=70.64, wps=10217.4, ups=0.16, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.551, loss_scale=16, train_wall=636, gb_free=4, wall=47734
2022-02-25 20:14:23 | INFO | train_inner | epoch 010:    424 / 788 loss=6.157, ppl=71.38, wps=10220, ups=0.16, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.55, loss_scale=16, train_wall=636, gb_free=4, wall=48376
2022-02-25 20:25:05 | INFO | train_inner | epoch 010:    524 / 788 loss=6.159, ppl=71.44, wps=10219.4, ups=0.16, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.536, loss_scale=16, train_wall=636, gb_free=4, wall=49017
2022-02-25 20:35:46 | INFO | train_inner | epoch 010:    624 / 788 loss=6.169, ppl=71.96, wps=10217.4, ups=0.16, wpb=65519.3, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.568, loss_scale=16, train_wall=636, gb_free=4, wall=49658
2022-02-25 20:40:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 20:46:34 | INFO | train_inner | epoch 010:    725 / 788 loss=6.169, ppl=71.95, wps=10116.9, ups=0.15, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.533, loss_scale=16, train_wall=643, gb_free=4, wall=50306
2022-02-25 20:53:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 20:53:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.342 | ppl 81.11 | wps 23664.8 | wpb 2034.1 | bsz 4 | num_updates 7863 | best_loss 6.342
2022-02-25 20:53:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7863 updates
2022-02-25 20:53:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 20:53:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 20:53:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 10 @ 7863 updates, score 6.342) (writing took 7.1023018676787615 seconds)
2022-02-25 20:53:31 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-25 20:53:31 | INFO | train | epoch 010 | loss 6.148 | ppl 70.92 | wps 10159.4 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 7863 | lr 0.00035662 | gnorm 0.551 | loss_scale 16 | train_wall 5012 | gb_free 4 | wall 50723
2022-02-25 20:53:31 | INFO | fairseq.trainer | begin training epoch 11
2022-02-25 20:53:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 20:57:29 | INFO | train_inner | epoch 011:     37 / 788 loss=6.11, ppl=69.09, wps=9957.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.562, loss_scale=16, train_wall=634, gb_free=4, wall=50961
2022-02-25 21:08:11 | INFO | train_inner | epoch 011:    137 / 788 loss=6.03, ppl=65.33, wps=10204.4, ups=0.16, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.555, loss_scale=16, train_wall=637, gb_free=4, wall=51603
2022-02-25 21:18:53 | INFO | train_inner | epoch 011:    237 / 788 loss=6.044, ppl=65.98, wps=10209.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.547, loss_scale=16, train_wall=637, gb_free=4, wall=52245
2022-02-25 21:29:35 | INFO | train_inner | epoch 011:    337 / 788 loss=6.059, ppl=66.67, wps=10212.1, ups=0.16, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.55, loss_scale=16, train_wall=637, gb_free=4, wall=52887
2022-02-25 21:35:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 21:40:23 | INFO | train_inner | epoch 011:    438 / 788 loss=6.08, ppl=67.67, wps=10115.3, ups=0.15, wpb=65520.6, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.554, loss_scale=16, train_wall=643, gb_free=4, wall=53535
2022-02-25 21:51:04 | INFO | train_inner | epoch 011:    538 / 788 loss=6.085, ppl=67.89, wps=10214.9, ups=0.16, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.545, loss_scale=16, train_wall=637, gb_free=4, wall=54176
2022-02-25 21:54:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 22:01:52 | INFO | train_inner | epoch 011:    639 / 788 loss=6.092, ppl=68.2, wps=10117, ups=0.15, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.586, loss_scale=8, train_wall=643, gb_free=4, wall=54824
2022-02-25 22:12:33 | INFO | train_inner | epoch 011:    739 / 788 loss=6.097, ppl=68.43, wps=10218.4, ups=0.16, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.554, loss_scale=8, train_wall=636, gb_free=4, wall=55466
2022-02-25 22:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 22:17:54 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.303 | ppl 78.97 | wps 23698.3 | wpb 2034.1 | bsz 4 | num_updates 8649 | best_loss 6.303
2022-02-25 22:17:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8649 updates
2022-02-25 22:17:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 22:18:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 22:18:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 11 @ 8649 updates, score 6.303) (writing took 7.013053854927421 seconds)
2022-02-25 22:18:01 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-25 22:18:01 | INFO | train | epoch 011 | loss 6.068 | ppl 67.08 | wps 10154.5 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 8649 | lr 0.00034003 | gnorm 0.558 | loss_scale 8 | train_wall 5015 | gb_free 4 | wall 55793
2022-02-25 22:18:01 | INFO | fairseq.trainer | begin training epoch 12
2022-02-25 22:18:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 22:23:28 | INFO | train_inner | epoch 012:     51 / 788 loss=6.012, ppl=64.54, wps=9964.7, ups=0.15, wpb=65248, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.557, loss_scale=8, train_wall=634, gb_free=4, wall=56120
2022-02-25 22:34:10 | INFO | train_inner | epoch 012:    151 / 788 loss=5.961, ppl=62.3, wps=10216.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.553, loss_scale=8, train_wall=636, gb_free=4, wall=56762
2022-02-25 22:44:51 | INFO | train_inner | epoch 012:    251 / 788 loss=5.984, ppl=63.27, wps=10220.2, ups=0.16, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.596, loss_scale=8, train_wall=636, gb_free=4, wall=57403
2022-02-25 22:51:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 22:55:38 | INFO | train_inner | epoch 012:    352 / 788 loss=5.989, ppl=63.51, wps=10119.3, ups=0.15, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.563, loss_scale=8, train_wall=643, gb_free=4, wall=58050
2022-02-25 23:06:20 | INFO | train_inner | epoch 012:    452 / 788 loss=6.006, ppl=64.29, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.56, loss_scale=8, train_wall=636, gb_free=4, wall=58692
2022-02-25 23:17:01 | INFO | train_inner | epoch 012:    552 / 788 loss=6.017, ppl=64.75, wps=10220.7, ups=0.16, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.56, loss_scale=8, train_wall=636, gb_free=4, wall=59333
2022-02-25 23:27:42 | INFO | train_inner | epoch 012:    652 / 788 loss=6.033, ppl=65.47, wps=10216.7, ups=0.16, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.59, loss_scale=8, train_wall=637, gb_free=4, wall=59974
2022-02-25 23:38:24 | INFO | train_inner | epoch 012:    752 / 788 loss=6.031, ppl=65.37, wps=10217.7, ups=0.16, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.531, loss_scale=8, train_wall=637, gb_free=4, wall=60616
2022-02-25 23:42:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 23:42:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.271 | ppl 77.24 | wps 23694.6 | wpb 2034.1 | bsz 4 | num_updates 9436 | best_loss 6.271
2022-02-25 23:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9436 updates
2022-02-25 23:42:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 23:42:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-25 23:42:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 12 @ 9436 updates, score 6.271) (writing took 6.936902333050966 seconds)
2022-02-25 23:42:28 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-25 23:42:28 | INFO | train | epoch 012 | loss 6 | ppl 64 | wps 10173.3 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 9436 | lr 0.000325541 | gnorm 0.561 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 60860
2022-02-25 23:42:28 | INFO | fairseq.trainer | begin training epoch 13
2022-02-25 23:42:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 23:49:19 | INFO | train_inner | epoch 013:     64 / 788 loss=5.935, ppl=61.17, wps=9965.2, ups=0.15, wpb=65248, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.553, loss_scale=16, train_wall=634, gb_free=4, wall=61271
2022-02-25 23:53:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 00:00:07 | INFO | train_inner | epoch 013:    165 / 788 loss=5.902, ppl=59.8, wps=10109.4, ups=0.15, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.565, loss_scale=8, train_wall=643, gb_free=4, wall=61919
2022-02-26 00:10:48 | INFO | train_inner | epoch 013:    265 / 788 loss=5.918, ppl=60.48, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.561, loss_scale=8, train_wall=636, gb_free=4, wall=62560
2022-02-26 00:21:30 | INFO | train_inner | epoch 013:    365 / 788 loss=5.936, ppl=61.22, wps=10217.7, ups=0.16, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.549, loss_scale=8, train_wall=636, gb_free=4, wall=63202
2022-02-26 00:32:11 | INFO | train_inner | epoch 013:    465 / 788 loss=5.954, ppl=61.98, wps=10219, ups=0.16, wpb=65520.6, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.55, loss_scale=8, train_wall=636, gb_free=4, wall=63843
2022-02-26 00:42:52 | INFO | train_inner | epoch 013:    565 / 788 loss=5.961, ppl=62.3, wps=10218.8, ups=0.16, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.553, loss_scale=8, train_wall=636, gb_free=4, wall=64484
2022-02-26 00:53:34 | INFO | train_inner | epoch 013:    665 / 788 loss=5.972, ppl=62.78, wps=10206.3, ups=0.16, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.554, loss_scale=16, train_wall=637, gb_free=4, wall=65126
2022-02-26 01:02:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 01:04:22 | INFO | train_inner | epoch 013:    766 / 788 loss=5.981, ppl=63.15, wps=10116.2, ups=0.15, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.558, loss_scale=8, train_wall=643, gb_free=4, wall=65774
2022-02-26 01:06:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 01:06:49 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.253 | ppl 76.25 | wps 23713.1 | wpb 2034.1 | bsz 4 | num_updates 10222 | best_loss 6.253
2022-02-26 01:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10222 updates
2022-02-26 01:06:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 01:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 01:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 13 @ 10222 updates, score 6.253) (writing took 6.7352776834741235 seconds)
2022-02-26 01:06:56 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 01:06:56 | INFO | train | epoch 013 | loss 5.942 | ppl 61.49 | wps 10157.5 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 10222 | lr 0.000312775 | gnorm 0.554 | loss_scale 8 | train_wall 5014 | gb_free 4 | wall 65928
2022-02-26 01:06:56 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 01:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 01:15:17 | INFO | train_inner | epoch 014:     78 / 788 loss=5.864, ppl=58.24, wps=9968.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=10300, lr=0.000311588, gnorm=0.541, loss_scale=8, train_wall=634, gb_free=4, wall=66429
2022-02-26 01:25:58 | INFO | train_inner | epoch 014:    178 / 788 loss=5.856, ppl=57.9, wps=10213, ups=0.16, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.583, loss_scale=8, train_wall=637, gb_free=4, wall=67070
2022-02-26 01:36:39 | INFO | train_inner | epoch 014:    278 / 788 loss=5.872, ppl=58.58, wps=10220.3, ups=0.16, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.572, loss_scale=8, train_wall=636, gb_free=4, wall=67712
2022-02-26 01:47:21 | INFO | train_inner | epoch 014:    378 / 788 loss=5.887, ppl=59.17, wps=10219, ups=0.16, wpb=65534.7, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.549, loss_scale=8, train_wall=636, gb_free=4, wall=68353
2022-02-26 01:58:02 | INFO | train_inner | epoch 014:    478 / 788 loss=5.901, ppl=59.76, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.548, loss_scale=16, train_wall=636, gb_free=4, wall=68994
2022-02-26 02:03:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:08:50 | INFO | train_inner | epoch 014:    579 / 788 loss=5.911, ppl=60.15, wps=10114, ups=0.15, wpb=65520.6, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.582, loss_scale=8, train_wall=643, gb_free=4, wall=69642
2022-02-26 02:19:32 | INFO | train_inner | epoch 014:    679 / 788 loss=5.925, ppl=60.78, wps=10211, ups=0.16, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.57, loss_scale=8, train_wall=637, gb_free=4, wall=70284
2022-02-26 02:30:13 | INFO | train_inner | epoch 014:    779 / 788 loss=5.945, ppl=61.61, wps=10220.7, ups=0.16, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.581, loss_scale=8, train_wall=636, gb_free=4, wall=70925
2022-02-26 02:31:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 02:31:17 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.236 | ppl 75.36 | wps 23700.2 | wpb 2034.1 | bsz 4 | num_updates 11009 | best_loss 6.236
2022-02-26 02:31:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 11009 updates
2022-02-26 02:31:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 02:31:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 02:31:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 14 @ 11009 updates, score 6.236) (writing took 6.6525805201381445 seconds)
2022-02-26 02:31:24 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 02:31:24 | INFO | train | epoch 014 | loss 5.893 | ppl 59.4 | wps 10171.9 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 11009 | lr 0.000301388 | gnorm 0.567 | loss_scale 8 | train_wall 5013 | gb_free 4 | wall 70996
2022-02-26 02:31:24 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 02:31:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 02:41:07 | INFO | train_inner | epoch 015:     91 / 788 loss=5.79, ppl=55.33, wps=9970.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11100, lr=0.00030015, gnorm=0.566, loss_scale=8, train_wall=634, gb_free=4, wall=71579
2022-02-26 02:51:49 | INFO | train_inner | epoch 015:    191 / 788 loss=5.81, ppl=56.11, wps=10216.7, ups=0.16, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.587, loss_scale=8, train_wall=637, gb_free=4, wall=72221
2022-02-26 03:02:30 | INFO | train_inner | epoch 015:    291 / 788 loss=5.823, ppl=56.61, wps=10218.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.547, loss_scale=16, train_wall=636, gb_free=4, wall=72862
2022-02-26 03:03:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 03:13:18 | INFO | train_inner | epoch 015:    392 / 788 loss=5.844, ppl=57.43, wps=10114.4, ups=0.15, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.586, loss_scale=8, train_wall=643, gb_free=4, wall=73510
2022-02-26 03:23:59 | INFO | train_inner | epoch 015:    492 / 788 loss=5.86, ppl=58.06, wps=10219.6, ups=0.16, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.587, loss_scale=8, train_wall=636, gb_free=4, wall=74151
2022-02-26 03:34:41 | INFO | train_inner | epoch 015:    592 / 788 loss=5.875, ppl=58.69, wps=10218.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.548, loss_scale=8, train_wall=636, gb_free=4, wall=74793
2022-02-26 03:45:22 | INFO | train_inner | epoch 015:    692 / 788 loss=5.886, ppl=59.16, wps=10218, ups=0.16, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.569, loss_scale=8, train_wall=636, gb_free=4, wall=75434
2022-02-26 03:55:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 03:55:44 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.219 | ppl 74.51 | wps 23651.4 | wpb 2034.1 | bsz 4 | num_updates 11796 | best_loss 6.219
2022-02-26 03:55:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11796 updates
2022-02-26 03:55:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 03:55:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 03:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 15 @ 11796 updates, score 6.219) (writing took 6.536410305649042 seconds)
2022-02-26 03:55:51 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 03:55:51 | INFO | train | epoch 015 | loss 5.848 | ppl 57.59 | wps 10173.3 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 11796 | lr 0.000291161 | gnorm 0.572 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 76063
2022-02-26 03:55:51 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 03:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 03:56:16 | INFO | train_inner | epoch 016:      4 / 788 loss=5.892, ppl=59.39, wps=9971.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.589, loss_scale=8, train_wall=634, gb_free=4, wall=76088
2022-02-26 04:06:58 | INFO | train_inner | epoch 016:    104 / 788 loss=5.745, ppl=53.64, wps=10216.6, ups=0.16, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.565, loss_scale=16, train_wall=637, gb_free=4, wall=76730
2022-02-26 04:16:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:17:46 | INFO | train_inner | epoch 016:    205 / 788 loss=5.774, ppl=54.71, wps=10117, ups=0.15, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.577, loss_scale=8, train_wall=643, gb_free=4, wall=77378
2022-02-26 04:28:27 | INFO | train_inner | epoch 016:    305 / 788 loss=5.789, ppl=55.31, wps=10218, ups=0.16, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.582, loss_scale=8, train_wall=636, gb_free=4, wall=78019
2022-02-26 04:39:08 | INFO | train_inner | epoch 016:    405 / 788 loss=5.809, ppl=56.06, wps=10217.2, ups=0.16, wpb=65519.3, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.576, loss_scale=8, train_wall=636, gb_free=4, wall=78660
2022-02-26 04:49:50 | INFO | train_inner | epoch 016:    505 / 788 loss=5.815, ppl=56.29, wps=10219.8, ups=0.16, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.587, loss_scale=8, train_wall=636, gb_free=4, wall=79302
2022-02-26 05:00:31 | INFO | train_inner | epoch 016:    605 / 788 loss=5.831, ppl=56.93, wps=10218.9, ups=0.16, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.586, loss_scale=8, train_wall=636, gb_free=4, wall=79943
2022-02-26 05:11:12 | INFO | train_inner | epoch 016:    705 / 788 loss=5.863, ppl=58.19, wps=10217.6, ups=0.16, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.582, loss_scale=16, train_wall=636, gb_free=4, wall=80584
2022-02-26 05:18:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 05:20:11 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.219 | ppl 74.52 | wps 23682.5 | wpb 2034.1 | bsz 4 | num_updates 12582 | best_loss 6.219
2022-02-26 05:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12582 updates
2022-02-26 05:20:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 05:20:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 05:20:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 16 @ 12582 updates, score 6.219) (writing took 6.572010359726846 seconds)
2022-02-26 05:20:17 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 05:20:17 | INFO | train | epoch 016 | loss 5.808 | ppl 56.04 | wps 10160.3 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 12582 | lr 0.00028192 | gnorm 0.579 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 81130
2022-02-26 05:20:18 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 05:20:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 05:22:13 | INFO | train_inner | epoch 017:     18 / 788 loss=5.823, ppl=56.63, wps=9874.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.594, loss_scale=8, train_wall=640, gb_free=4, wall=81245
2022-02-26 05:32:54 | INFO | train_inner | epoch 017:    118 / 788 loss=5.707, ppl=52.25, wps=10218, ups=0.16, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.574, loss_scale=8, train_wall=636, gb_free=4, wall=81886
2022-02-26 05:43:36 | INFO | train_inner | epoch 017:    218 / 788 loss=5.746, ppl=53.65, wps=10219.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.593, loss_scale=8, train_wall=636, gb_free=4, wall=82528
2022-02-26 05:54:17 | INFO | train_inner | epoch 017:    318 / 788 loss=5.756, ppl=54.05, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.616, loss_scale=8, train_wall=636, gb_free=4, wall=83169
2022-02-26 06:04:58 | INFO | train_inner | epoch 017:    418 / 788 loss=5.782, ppl=55.02, wps=10220.4, ups=0.16, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.572, loss_scale=8, train_wall=636, gb_free=4, wall=83810
2022-02-26 06:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 06:15:46 | INFO | train_inner | epoch 017:    519 / 788 loss=5.782, ppl=55.02, wps=10117.6, ups=0.15, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.579, loss_scale=8, train_wall=643, gb_free=4, wall=84458
2022-02-26 06:26:27 | INFO | train_inner | epoch 017:    619 / 788 loss=5.799, ppl=55.67, wps=10215.7, ups=0.16, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.559, loss_scale=8, train_wall=637, gb_free=4, wall=85099
2022-02-26 06:37:09 | INFO | train_inner | epoch 017:    719 / 788 loss=5.814, ppl=56.27, wps=10216.1, ups=0.16, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.586, loss_scale=8, train_wall=637, gb_free=4, wall=85741
2022-02-26 06:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 06:44:38 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.199 | ppl 73.47 | wps 23709 | wpb 2034.1 | bsz 4 | num_updates 13369 | best_loss 6.199
2022-02-26 06:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13369 updates
2022-02-26 06:44:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 06:44:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 06:44:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 17 @ 13369 updates, score 6.199) (writing took 6.559707369655371 seconds)
2022-02-26 06:44:45 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 06:44:45 | INFO | train | epoch 017 | loss 5.773 | ppl 54.68 | wps 10172.8 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 13369 | lr 0.000273496 | gnorm 0.583 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 86197
2022-02-26 06:44:45 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 06:44:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 06:48:04 | INFO | train_inner | epoch 018:     31 / 788 loss=5.778, ppl=54.87, wps=9967.3, ups=0.15, wpb=65248, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.567, loss_scale=8, train_wall=634, gb_free=4, wall=86396
2022-02-26 06:58:45 | INFO | train_inner | epoch 018:    131 / 788 loss=5.677, ppl=51.17, wps=10219.4, ups=0.16, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.569, loss_scale=8, train_wall=636, gb_free=4, wall=87037
2022-02-26 07:09:26 | INFO | train_inner | epoch 018:    231 / 788 loss=5.708, ppl=52.28, wps=10218.1, ups=0.16, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.573, loss_scale=16, train_wall=636, gb_free=4, wall=87678
2022-02-26 07:10:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:20:14 | INFO | train_inner | epoch 018:    332 / 788 loss=5.735, ppl=53.25, wps=10113.8, ups=0.15, wpb=65534.7, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.605, loss_scale=8, train_wall=643, gb_free=4, wall=88326
2022-02-26 07:30:56 | INFO | train_inner | epoch 018:    432 / 788 loss=5.736, ppl=53.29, wps=10217, ups=0.16, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.56, loss_scale=8, train_wall=636, gb_free=4, wall=88968
2022-02-26 07:41:37 | INFO | train_inner | epoch 018:    532 / 788 loss=5.76, ppl=54.21, wps=10220.3, ups=0.16, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.593, loss_scale=8, train_wall=636, gb_free=4, wall=89609
2022-02-26 07:52:18 | INFO | train_inner | epoch 018:    632 / 788 loss=5.776, ppl=54.8, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.6, loss_scale=8, train_wall=636, gb_free=4, wall=90250
2022-02-26 08:02:59 | INFO | train_inner | epoch 018:    732 / 788 loss=5.786, ppl=55.19, wps=10217.8, ups=0.16, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.577, loss_scale=8, train_wall=636, gb_free=4, wall=90892
2022-02-26 08:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 08:09:05 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.197 | ppl 73.35 | wps 23655 | wpb 2034.1 | bsz 4 | num_updates 14156 | best_loss 6.197
2022-02-26 08:09:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14156 updates
2022-02-26 08:09:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 08:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 08:09:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 18 @ 14156 updates, score 6.197) (writing took 6.549443257972598 seconds)
2022-02-26 08:09:12 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 08:09:12 | INFO | train | epoch 018 | loss 5.741 | ppl 53.48 | wps 10173.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 14156 | lr 0.000265785 | gnorm 0.58 | loss_scale 16 | train_wall 5012 | gb_free 4 | wall 91264
2022-02-26 08:09:12 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 08:09:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 08:10:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:14:00 | INFO | train_inner | epoch 019:     45 / 788 loss=5.734, ppl=53.21, wps=9873.6, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.594, loss_scale=8, train_wall=640, gb_free=4, wall=91552
2022-02-26 08:24:42 | INFO | train_inner | epoch 019:    145 / 788 loss=5.664, ppl=50.69, wps=10209, ups=0.16, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.577, loss_scale=8, train_wall=637, gb_free=4, wall=92194
2022-02-26 08:35:24 | INFO | train_inner | epoch 019:    245 / 788 loss=5.676, ppl=51.14, wps=10213.4, ups=0.16, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.597, loss_scale=8, train_wall=637, gb_free=4, wall=92836
2022-02-26 08:46:05 | INFO | train_inner | epoch 019:    345 / 788 loss=5.703, ppl=52.11, wps=10217.7, ups=0.16, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.568, loss_scale=8, train_wall=636, gb_free=4, wall=93477
2022-02-26 08:56:47 | INFO | train_inner | epoch 019:    445 / 788 loss=5.709, ppl=52.3, wps=10216.8, ups=0.16, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.604, loss_scale=8, train_wall=637, gb_free=4, wall=94119
2022-02-26 09:06:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:07:34 | INFO | train_inner | epoch 019:    546 / 788 loss=5.731, ppl=53.13, wps=10117.8, ups=0.15, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.57, loss_scale=8, train_wall=643, gb_free=4, wall=94766
2022-02-26 09:18:16 | INFO | train_inner | epoch 019:    646 / 788 loss=5.744, ppl=53.61, wps=10220.3, ups=0.16, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.588, loss_scale=8, train_wall=636, gb_free=4, wall=95408
2022-02-26 09:28:57 | INFO | train_inner | epoch 019:    746 / 788 loss=5.756, ppl=54.04, wps=10220.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.6, loss_scale=8, train_wall=636, gb_free=4, wall=96049
2022-02-26 09:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 09:33:32 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.197 | ppl 73.34 | wps 23667 | wpb 2034.1 | bsz 4 | num_updates 14942 | best_loss 6.197
2022-02-26 09:33:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14942 updates
2022-02-26 09:33:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 09:33:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 09:33:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 19 @ 14942 updates, score 6.197) (writing took 6.466757323592901 seconds)
2022-02-26 09:33:39 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 09:33:39 | INFO | train | epoch 019 | loss 5.712 | ppl 52.42 | wps 10159.3 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 14942 | lr 0.0002587 | gnorm 0.588 | loss_scale 8 | train_wall 5013 | gb_free 4 | wall 96331
2022-02-26 09:33:39 | INFO | fairseq.trainer | begin training epoch 20
2022-02-26 09:33:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 09:39:51 | INFO | train_inner | epoch 020:     58 / 788 loss=5.676, ppl=51.12, wps=9971.9, ups=0.15, wpb=65248, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.577, loss_scale=8, train_wall=634, gb_free=4, wall=96703
2022-02-26 09:50:32 | INFO | train_inner | epoch 020:    158 / 788 loss=5.62, ppl=49.16, wps=10220.3, ups=0.16, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.577, loss_scale=8, train_wall=636, gb_free=4, wall=97344
2022-02-26 10:01:13 | INFO | train_inner | epoch 020:    258 / 788 loss=5.659, ppl=50.52, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.626, loss_scale=8, train_wall=636, gb_free=4, wall=97986
2022-02-26 10:01:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 10:12:01 | INFO | train_inner | epoch 020:    359 / 788 loss=5.68, ppl=51.26, wps=10118.5, ups=0.15, wpb=65534.7, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.593, loss_scale=8, train_wall=643, gb_free=4, wall=98633
2022-02-26 10:22:42 | INFO | train_inner | epoch 020:    459 / 788 loss=5.693, ppl=51.73, wps=10220.2, ups=0.16, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.582, loss_scale=8, train_wall=636, gb_free=4, wall=99274
2022-02-26 10:33:24 | INFO | train_inner | epoch 020:    559 / 788 loss=5.722, ppl=52.79, wps=10219.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.589, loss_scale=8, train_wall=636, gb_free=4, wall=99916
2022-02-26 10:44:05 | INFO | train_inner | epoch 020:    659 / 788 loss=5.719, ppl=52.66, wps=10220.6, ups=0.16, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.612, loss_scale=8, train_wall=636, gb_free=4, wall=100557
2022-02-26 10:54:46 | INFO | train_inner | epoch 020:    759 / 788 loss=5.735, ppl=53.27, wps=10220.4, ups=0.16, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.588, loss_scale=8, train_wall=636, gb_free=4, wall=101198
2022-02-26 10:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 10:57:58 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.187 | ppl 72.85 | wps 23657.5 | wpb 2034.1 | bsz 4 | num_updates 15729 | best_loss 6.187
2022-02-26 10:57:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15729 updates
2022-02-26 10:57:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 10:58:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 10:58:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 20 @ 15729 updates, score 6.187) (writing took 6.5561626413837075 seconds)
2022-02-26 10:58:05 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-26 10:58:05 | INFO | train | epoch 020 | loss 5.685 | ppl 51.46 | wps 10175.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 15729 | lr 0.000252144 | gnorm 0.594 | loss_scale 16 | train_wall 5011 | gb_free 4 | wall 101397
2022-02-26 10:58:05 | INFO | fairseq.trainer | begin training epoch 21
2022-02-26 10:58:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 11:04:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 11:05:47 | INFO | train_inner | epoch 021:     72 / 788 loss=5.63, ppl=49.53, wps=9874, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=15800, lr=0.000251577, gnorm=0.588, loss_scale=8, train_wall=640, gb_free=4, wall=101859
2022-02-26 11:16:28 | INFO | train_inner | epoch 021:    172 / 788 loss=5.605, ppl=48.66, wps=10219, ups=0.16, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.583, loss_scale=8, train_wall=636, gb_free=4, wall=102500
2022-02-26 11:27:09 | INFO | train_inner | epoch 021:    272 / 788 loss=5.634, ppl=49.64, wps=10221, ups=0.16, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.616, loss_scale=8, train_wall=636, gb_free=4, wall=103141
2022-02-26 11:37:50 | INFO | train_inner | epoch 021:    372 / 788 loss=5.645, ppl=50.03, wps=10222.4, ups=0.16, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.585, loss_scale=8, train_wall=636, gb_free=4, wall=103782
2022-02-26 11:48:32 | INFO | train_inner | epoch 021:    472 / 788 loss=5.673, ppl=51.03, wps=10219.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.588, loss_scale=8, train_wall=636, gb_free=4, wall=104424
2022-02-26 11:59:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 11:59:19 | INFO | train_inner | epoch 021:    573 / 788 loss=5.691, ppl=51.65, wps=10121, ups=0.15, wpb=65534.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.599, loss_scale=8, train_wall=643, gb_free=4, wall=105071
2022-02-26 12:10:00 | INFO | train_inner | epoch 021:    673 / 788 loss=5.7, ppl=52, wps=10220.8, ups=0.16, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.588, loss_scale=8, train_wall=636, gb_free=4, wall=105712
2022-02-26 12:20:42 | INFO | train_inner | epoch 021:    773 / 788 loss=5.727, ppl=52.95, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.608, loss_scale=8, train_wall=636, gb_free=4, wall=106354
2022-02-26 12:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 12:22:24 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.181 | ppl 72.55 | wps 23706 | wpb 2034.1 | bsz 4 | num_updates 16515 | best_loss 6.181
2022-02-26 12:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16515 updates
2022-02-26 12:22:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 12:22:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 12:22:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 21 @ 16515 updates, score 6.181) (writing took 6.601133749820292 seconds)
2022-02-26 12:22:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-26 12:22:31 | INFO | train | epoch 021 | loss 5.661 | ppl 50.59 | wps 10162.4 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 16515 | lr 0.000246071 | gnorm 0.594 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 106463
2022-02-26 12:22:31 | INFO | fairseq.trainer | begin training epoch 22
2022-02-26 12:22:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 12:31:36 | INFO | train_inner | epoch 022:     85 / 788 loss=5.589, ppl=48.15, wps=9974.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=16600, lr=0.00024544, gnorm=0.612, loss_scale=8, train_wall=633, gb_free=4, wall=107008
2022-02-26 12:42:17 | INFO | train_inner | epoch 022:    185 / 788 loss=5.591, ppl=48.19, wps=10223.9, ups=0.16, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.586, loss_scale=8, train_wall=636, gb_free=4, wall=107649
2022-02-26 12:52:58 | INFO | train_inner | epoch 022:    285 / 788 loss=5.608, ppl=48.77, wps=10224, ups=0.16, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.618, loss_scale=8, train_wall=636, gb_free=4, wall=108290
2022-02-26 13:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 13:03:45 | INFO | train_inner | epoch 022:    386 / 788 loss=5.639, ppl=49.81, wps=10120.2, ups=0.15, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.599, loss_scale=8, train_wall=643, gb_free=4, wall=108937
2022-02-26 13:14:26 | INFO | train_inner | epoch 022:    486 / 788 loss=5.651, ppl=50.24, wps=10221.3, ups=0.16, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.575, loss_scale=8, train_wall=636, gb_free=4, wall=109578
2022-02-26 13:25:08 | INFO | train_inner | epoch 022:    586 / 788 loss=5.667, ppl=50.81, wps=10221.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.619, loss_scale=8, train_wall=636, gb_free=4, wall=110220
2022-02-26 13:35:49 | INFO | train_inner | epoch 022:    686 / 788 loss=5.685, ppl=51.45, wps=10221.1, ups=0.16, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.601, loss_scale=8, train_wall=636, gb_free=4, wall=110861
2022-02-26 13:46:30 | INFO | train_inner | epoch 022:    786 / 788 loss=5.688, ppl=51.56, wps=10219.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.582, loss_scale=8, train_wall=636, gb_free=4, wall=111502
2022-02-26 13:46:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 13:46:49 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.181 | ppl 72.53 | wps 23694.3 | wpb 2034.1 | bsz 4 | num_updates 17302 | best_loss 6.181
2022-02-26 13:46:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17302 updates
2022-02-26 13:46:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 13:46:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 13:46:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 22 @ 17302 updates, score 6.181) (writing took 6.512243536300957 seconds)
2022-02-26 13:46:56 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-26 13:46:56 | INFO | train | epoch 022 | loss 5.638 | ppl 49.81 | wps 10177.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 17302 | lr 0.00024041 | gnorm 0.6 | loss_scale 8 | train_wall 5010 | gb_free 4 | wall 111528
2022-02-26 13:46:56 | INFO | fairseq.trainer | begin training epoch 23
2022-02-26 13:46:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 13:57:24 | INFO | train_inner | epoch 023:     98 / 788 loss=5.544, ppl=46.66, wps=9974, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=17400, lr=0.000239732, gnorm=0.606, loss_scale=16, train_wall=633, gb_free=4, wall=112156
2022-02-26 14:07:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 14:08:12 | INFO | train_inner | epoch 023:    199 / 788 loss=5.577, ppl=47.73, wps=10118.1, ups=0.15, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.598, loss_scale=8, train_wall=643, gb_free=4, wall=112804
2022-02-26 14:18:53 | INFO | train_inner | epoch 023:    299 / 788 loss=5.599, ppl=48.46, wps=10222.6, ups=0.16, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.642, loss_scale=8, train_wall=636, gb_free=4, wall=113445
2022-02-26 14:29:34 | INFO | train_inner | epoch 023:    399 / 788 loss=5.616, ppl=49.05, wps=10221.4, ups=0.16, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.606, loss_scale=8, train_wall=636, gb_free=4, wall=114086
2022-02-26 14:40:15 | INFO | train_inner | epoch 023:    499 / 788 loss=5.632, ppl=49.6, wps=10221, ups=0.16, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.603, loss_scale=8, train_wall=636, gb_free=4, wall=114727
2022-02-26 14:50:56 | INFO | train_inner | epoch 023:    599 / 788 loss=5.645, ppl=50.05, wps=10221.9, ups=0.16, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.595, loss_scale=8, train_wall=636, gb_free=4, wall=115368
2022-02-26 15:01:37 | INFO | train_inner | epoch 023:    699 / 788 loss=5.662, ppl=50.62, wps=10220.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.619, loss_scale=8, train_wall=636, gb_free=4, wall=116010
2022-02-26 15:07:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 15:11:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 15:11:15 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.184 | ppl 72.7 | wps 23688.1 | wpb 2034.1 | bsz 4 | num_updates 18088 | best_loss 6.181
2022-02-26 15:11:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18088 updates
2022-02-26 15:11:15 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-26 15:11:15 | INFO | train | epoch 023 | loss 5.618 | ppl 49.11 | wps 10176.2 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 18088 | lr 0.000235128 | gnorm 0.609 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 116587
2022-02-26 15:11:15 | INFO | fairseq.trainer | begin training epoch 24
2022-02-26 15:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 15:12:32 | INFO | train_inner | epoch 024:     12 / 788 loss=5.656, ppl=50.44, wps=9973.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.602, loss_scale=8, train_wall=640, gb_free=4, wall=116664
2022-02-26 15:23:13 | INFO | train_inner | epoch 024:    112 / 788 loss=5.53, ppl=46.21, wps=10220.4, ups=0.16, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.619, loss_scale=8, train_wall=636, gb_free=4, wall=117305
2022-02-26 15:33:54 | INFO | train_inner | epoch 024:    212 / 788 loss=5.563, ppl=47.27, wps=10222.1, ups=0.16, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.593, loss_scale=8, train_wall=636, gb_free=4, wall=117946
2022-02-26 15:44:35 | INFO | train_inner | epoch 024:    312 / 788 loss=5.582, ppl=47.92, wps=10221.9, ups=0.16, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.622, loss_scale=8, train_wall=636, gb_free=4, wall=118587
2022-02-26 15:55:16 | INFO | train_inner | epoch 024:    412 / 788 loss=5.598, ppl=48.45, wps=10220.6, ups=0.16, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.612, loss_scale=8, train_wall=636, gb_free=4, wall=119228
2022-02-26 16:02:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:06:04 | INFO | train_inner | epoch 024:    513 / 788 loss=5.619, ppl=49.14, wps=10119.6, ups=0.15, wpb=65520.6, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.596, loss_scale=8, train_wall=643, gb_free=4, wall=119876
2022-02-26 16:16:45 | INFO | train_inner | epoch 024:    613 / 788 loss=5.622, ppl=49.24, wps=10218.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.616, loss_scale=8, train_wall=636, gb_free=4, wall=120517
2022-02-26 16:27:26 | INFO | train_inner | epoch 024:    713 / 788 loss=5.643, ppl=49.98, wps=10220.4, ups=0.16, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.6, loss_scale=8, train_wall=636, gb_free=4, wall=121158
2022-02-26 16:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 16:35:34 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.178 | ppl 72.4 | wps 23672.9 | wpb 2034.1 | bsz 4 | num_updates 18875 | best_loss 6.178
2022-02-26 16:35:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18875 updates
2022-02-26 16:35:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 16:35:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 16:35:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 24 @ 18875 updates, score 6.178) (writing took 6.477550712414086 seconds)
2022-02-26 16:35:40 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-26 16:35:40 | INFO | train | epoch 024 | loss 5.598 | ppl 48.43 | wps 10175.7 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 18875 | lr 0.000230174 | gnorm 0.608 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 121652
2022-02-26 16:35:40 | INFO | fairseq.trainer | begin training epoch 25
2022-02-26 16:35:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 16:38:21 | INFO | train_inner | epoch 025:     25 / 788 loss=5.61, ppl=48.83, wps=9974.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.616, loss_scale=8, train_wall=634, gb_free=4, wall=121813
2022-02-26 16:49:02 | INFO | train_inner | epoch 025:    125 / 788 loss=5.514, ppl=45.7, wps=10221.6, ups=0.16, wpb=65519.3, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.598, loss_scale=8, train_wall=636, gb_free=4, wall=122454
2022-02-26 16:58:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:59:49 | INFO | train_inner | epoch 025:    226 / 788 loss=5.541, ppl=46.58, wps=10121.3, ups=0.15, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.615, loss_scale=8, train_wall=643, gb_free=4, wall=123101
2022-02-26 17:10:30 | INFO | train_inner | epoch 025:    326 / 788 loss=5.559, ppl=47.13, wps=10222.5, ups=0.16, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.603, loss_scale=8, train_wall=636, gb_free=4, wall=123742
2022-02-26 17:21:11 | INFO | train_inner | epoch 025:    426 / 788 loss=5.586, ppl=48.02, wps=10220.9, ups=0.16, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.612, loss_scale=8, train_wall=636, gb_free=4, wall=124383
2022-02-26 17:31:53 | INFO | train_inner | epoch 025:    526 / 788 loss=5.601, ppl=48.52, wps=10220.5, ups=0.16, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.606, loss_scale=8, train_wall=636, gb_free=4, wall=125025
2022-02-26 17:42:34 | INFO | train_inner | epoch 025:    626 / 788 loss=5.609, ppl=48.81, wps=10221.2, ups=0.16, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.62, loss_scale=8, train_wall=636, gb_free=4, wall=125666
2022-02-26 17:53:15 | INFO | train_inner | epoch 025:    726 / 788 loss=5.634, ppl=49.66, wps=10223.4, ups=0.16, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.591, loss_scale=8, train_wall=636, gb_free=4, wall=126307
2022-02-26 17:53:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 17:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 17:59:59 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.17 | ppl 72 | wps 23672.9 | wpb 2034.1 | bsz 4 | num_updates 19661 | best_loss 6.17
2022-02-26 17:59:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19661 updates
2022-02-26 17:59:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 18:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 18:00:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 25 @ 19661 updates, score 6.17) (writing took 6.509016712196171 seconds)
2022-02-26 18:00:05 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-26 18:00:05 | INFO | train | epoch 025 | loss 5.58 | ppl 47.85 | wps 10164.1 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 19661 | lr 0.000225526 | gnorm 0.607 | loss_scale 8 | train_wall 5010 | gb_free 4 | wall 126717
2022-02-26 18:00:05 | INFO | fairseq.trainer | begin training epoch 26
2022-02-26 18:00:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 18:04:15 | INFO | train_inner | epoch 026:     39 / 788 loss=5.585, ppl=48, wps=9878.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.607, loss_scale=8, train_wall=640, gb_free=4, wall=126967
2022-02-26 18:14:57 | INFO | train_inner | epoch 026:    139 / 788 loss=5.501, ppl=45.3, wps=10220.1, ups=0.16, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.604, loss_scale=8, train_wall=636, gb_free=4, wall=127609
2022-02-26 18:25:38 | INFO | train_inner | epoch 026:    239 / 788 loss=5.527, ppl=46.12, wps=10222.2, ups=0.16, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.609, loss_scale=8, train_wall=636, gb_free=4, wall=128250
2022-02-26 18:36:19 | INFO | train_inner | epoch 026:    339 / 788 loss=5.551, ppl=46.87, wps=10221.8, ups=0.16, wpb=65519.3, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.629, loss_scale=8, train_wall=636, gb_free=4, wall=128891
2022-02-26 18:47:00 | INFO | train_inner | epoch 026:    439 / 788 loss=5.568, ppl=47.45, wps=10221.1, ups=0.16, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.615, loss_scale=8, train_wall=636, gb_free=4, wall=129532
2022-02-26 18:49:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 18:57:47 | INFO | train_inner | epoch 026:    540 / 788 loss=5.588, ppl=48.1, wps=10122.1, ups=0.15, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.631, loss_scale=8, train_wall=643, gb_free=4, wall=130179
2022-02-26 19:08:28 | INFO | train_inner | epoch 026:    640 / 788 loss=5.596, ppl=48.37, wps=10223.2, ups=0.16, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.598, loss_scale=8, train_wall=636, gb_free=4, wall=130820
2022-02-26 19:19:09 | INFO | train_inner | epoch 026:    740 / 788 loss=5.614, ppl=48.96, wps=10221.7, ups=0.16, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.617, loss_scale=8, train_wall=636, gb_free=4, wall=131461
2022-02-26 19:24:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:24:24 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.167 | ppl 71.88 | wps 23708.9 | wpb 2034.1 | bsz 4 | num_updates 20448 | best_loss 6.167
2022-02-26 19:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20448 updates
2022-02-26 19:24:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 19:24:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt
2022-02-26 19:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#5/checkpoint_best.pt (epoch 26 @ 20448 updates, score 6.167) (writing took 6.515988475643098 seconds)
2022-02-26 19:24:30 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-26 19:24:30 | INFO | train | epoch 026 | loss 5.564 | ppl 47.3 | wps 10177.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 20448 | lr 0.000221144 | gnorm 0.616 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 131782
2022-02-26 19:24:30 | INFO | fairseq.trainer | begin training epoch 27
2022-02-26 19:24:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:30:04 | INFO | train_inner | epoch 027:     52 / 788 loss=5.543, ppl=46.61, wps=9974.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=20500, lr=0.000220863, gnorm=0.613, loss_scale=8, train_wall=634, gb_free=4, wall=132116
2022-02-26 19:40:45 | INFO | train_inner | epoch 027:    152 / 788 loss=5.499, ppl=45.22, wps=10222, ups=0.16, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.631, loss_scale=8, train_wall=636, gb_free=4, wall=132757
2022-02-26 19:45:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 19:51:32 | INFO | train_inner | epoch 027:    253 / 788 loss=5.512, ppl=45.65, wps=10121.8, ups=0.15, wpb=65519.3, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.621, loss_scale=8, train_wall=642, gb_free=4, wall=133404
2022-02-26 20:02:13 | INFO | train_inner | epoch 027:    353 / 788 loss=5.542, ppl=46.58, wps=10222.7, ups=0.16, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.606, loss_scale=8, train_wall=636, gb_free=4, wall=134045
2022-02-26 20:12:54 | INFO | train_inner | epoch 027:    453 / 788 loss=5.548, ppl=46.78, wps=10224, ups=0.16, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.621, loss_scale=8, train_wall=636, gb_free=4, wall=134686
2022-02-26 20:23:35 | INFO | train_inner | epoch 027:    553 / 788 loss=5.575, ppl=47.68, wps=10223.8, ups=0.16, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.635, loss_scale=8, train_wall=636, gb_free=4, wall=135327
2022-02-26 20:34:16 | INFO | train_inner | epoch 027:    653 / 788 loss=5.584, ppl=47.97, wps=10222.7, ups=0.16, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.629, loss_scale=8, train_wall=636, gb_free=4, wall=135968
2022-02-26 20:44:57 | INFO | train_inner | epoch 027:    753 / 788 loss=5.597, ppl=48.41, wps=10223, ups=0.16, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.631, loss_scale=16, train_wall=636, gb_free=4, wall=136609
2022-02-26 20:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 20:48:48 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.175 | ppl 72.23 | wps 23658.1 | wpb 2034.1 | bsz 4 | num_updates 21235 | best_loss 6.167
2022-02-26 20:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21235 updates
2022-02-26 20:48:48 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-26 20:48:48 | INFO | train | epoch 027 | loss 5.548 | ppl 46.77 | wps 10191 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 21235 | lr 0.000217007 | gnorm 0.622 | loss_scale 16 | train_wall 5010 | gb_free 4 | wall 136840
2022-02-26 20:48:48 | INFO | fairseq.trainer | begin training epoch 28
2022-02-26 20:48:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 20:50:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 20:55:51 | INFO | train_inner | epoch 028:     66 / 788 loss=5.505, ppl=45.42, wps=9977, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=21300, lr=0.000216676, gnorm=0.614, loss_scale=8, train_wall=640, gb_free=4, wall=137263
2022-02-26 21:06:32 | INFO | train_inner | epoch 028:    166 / 788 loss=5.483, ppl=44.74, wps=10221.2, ups=0.16, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.624, loss_scale=8, train_wall=636, gb_free=4, wall=137905
2022-02-26 21:17:14 | INFO | train_inner | epoch 028:    266 / 788 loss=5.498, ppl=45.19, wps=10221, ups=0.16, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.635, loss_scale=8, train_wall=636, gb_free=4, wall=138546
2022-02-26 21:27:55 | INFO | train_inner | epoch 028:    366 / 788 loss=5.518, ppl=45.82, wps=10222, ups=0.16, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.626, loss_scale=8, train_wall=636, gb_free=4, wall=139187
2022-02-26 21:38:36 | INFO | train_inner | epoch 028:    466 / 788 loss=5.545, ppl=46.68, wps=10222.5, ups=0.16, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.633, loss_scale=8, train_wall=636, gb_free=4, wall=139828
2022-02-26 21:49:17 | INFO | train_inner | epoch 028:    566 / 788 loss=5.565, ppl=47.33, wps=10219.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.621, loss_scale=16, train_wall=636, gb_free=4, wall=140469
2022-02-26 21:59:58 | INFO | train_inner | epoch 028:    666 / 788 loss=5.575, ppl=47.66, wps=10218.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.615, loss_scale=16, train_wall=636, gb_free=4, wall=141110
2022-02-26 22:06:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 22:10:46 | INFO | train_inner | epoch 028:    767 / 788 loss=5.586, ppl=48.03, wps=10119.4, ups=0.15, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.616, loss_scale=8, train_wall=643, gb_free=4, wall=141758
2022-02-26 22:12:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 22:13:07 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.174 | ppl 72.18 | wps 23653.7 | wpb 2034.1 | bsz 4 | num_updates 22021 | best_loss 6.167
2022-02-26 22:13:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 22021 updates
2022-02-26 22:13:07 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-26 22:13:07 | INFO | train | epoch 028 | loss 5.533 | ppl 46.29 | wps 10176.3 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 22021 | lr 0.000213099 | gnorm 0.624 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 141899
2022-02-26 22:13:07 | INFO | fairseq.trainer | begin training epoch 29
2022-02-26 22:13:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 22:21:34 | INFO | train_inner | epoch 029:     79 / 788 loss=5.48, ppl=44.62, wps=10073.8, ups=0.15, wpb=65248, bsz=127.4, num_updates=22100, lr=0.000212718, gnorm=0.631, loss_scale=8, train_wall=633, gb_free=4, wall=142406
2022-02-26 22:32:15 | INFO | train_inner | epoch 029:    179 / 788 loss=5.461, ppl=44.06, wps=10221.7, ups=0.16, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.626, loss_scale=8, train_wall=636, gb_free=4, wall=143047
2022-02-26 22:42:56 | INFO | train_inner | epoch 029:    279 / 788 loss=5.49, ppl=44.94, wps=10221.9, ups=0.16, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.624, loss_scale=8, train_wall=636, gb_free=4, wall=143688
2022-02-26 22:53:37 | INFO | train_inner | epoch 029:    379 / 788 loss=5.511, ppl=45.6, wps=10220.6, ups=0.16, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.632, loss_scale=8, train_wall=636, gb_free=4, wall=144329
2022-02-26 23:01:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 23:04:25 | INFO | train_inner | epoch 029:    480 / 788 loss=5.533, ppl=46.31, wps=10119.2, ups=0.15, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.622, loss_scale=8, train_wall=643, gb_free=4, wall=144977
2022-02-26 23:15:06 | INFO | train_inner | epoch 029:    580 / 788 loss=5.538, ppl=46.47, wps=10222.4, ups=0.16, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.631, loss_scale=8, train_wall=636, gb_free=4, wall=145618
2022-02-26 23:25:48 | INFO | train_inner | epoch 029:    680 / 788 loss=5.569, ppl=47.47, wps=10210.4, ups=0.16, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.641, loss_scale=8, train_wall=637, gb_free=4, wall=146260
2022-02-26 23:36:30 | INFO | train_inner | epoch 029:    780 / 788 loss=5.575, ppl=47.66, wps=10209.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.618, loss_scale=8, train_wall=637, gb_free=4, wall=146902
2022-02-26 23:37:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 23:37:27 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.18 | ppl 72.52 | wps 23685.7 | wpb 2034.1 | bsz 4 | num_updates 22808 | best_loss 6.167
2022-02-26 23:37:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22808 updates
2022-02-26 23:37:27 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-26 23:37:27 | INFO | train | epoch 029 | loss 5.519 | ppl 45.84 | wps 10186.7 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 22808 | lr 0.00020939 | gnorm 0.629 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 146959
2022-02-26 23:37:27 | INFO | fairseq.trainer | begin training epoch 30
2022-02-26 23:37:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 23:47:17 | INFO | train_inner | epoch 030:     92 / 788 loss=5.438, ppl=43.37, wps=10072.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22900, lr=0.000208969, gnorm=0.629, loss_scale=8, train_wall=634, gb_free=4, wall=147549
2022-02-26 23:57:59 | INFO | train_inner | epoch 030:    192 / 788 loss=5.458, ppl=43.94, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.626, loss_scale=16, train_wall=636, gb_free=4, wall=148191
2022-02-27 00:08:40 | INFO | train_inner | epoch 030:    292 / 788 loss=5.481, ppl=44.67, wps=10212.5, ups=0.16, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.621, loss_scale=16, train_wall=637, gb_free=4, wall=148832
2022-02-27 00:09:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 00:19:28 | INFO | train_inner | epoch 030:    393 / 788 loss=5.499, ppl=45.22, wps=10117.4, ups=0.15, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.629, loss_scale=8, train_wall=643, gb_free=4, wall=149480
2022-02-27 00:30:09 | INFO | train_inner | epoch 030:    493 / 788 loss=5.525, ppl=46.04, wps=10218.8, ups=0.16, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.64, loss_scale=8, train_wall=636, gb_free=4, wall=150121
2022-02-27 00:40:51 | INFO | train_inner | epoch 030:    593 / 788 loss=5.537, ppl=46.42, wps=10220.2, ups=0.16, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.623, loss_scale=8, train_wall=636, gb_free=4, wall=150763
2022-02-27 00:51:32 | INFO | train_inner | epoch 030:    693 / 788 loss=5.549, ppl=46.82, wps=10217.5, ups=0.16, wpb=65519.3, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.636, loss_scale=8, train_wall=636, gb_free=4, wall=151404
2022-02-27 01:01:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 01:01:48 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.172 | ppl 72.13 | wps 23518 | wpb 2034.1 | bsz 4 | num_updates 23595 | best_loss 6.167
2022-02-27 01:01:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23595 updates
2022-02-27 01:01:48 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-27 01:01:48 | INFO | train | epoch 030 | loss 5.505 | ppl 45.42 | wps 10186.2 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 23595 | lr 0.000205869 | gnorm 0.631 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 152020
2022-02-27 01:01:48 | INFO | fairseq.trainer | begin training epoch 31
2022-02-27 01:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 01:02:20 | INFO | train_inner | epoch 031:      5 / 788 loss=5.56, ppl=47.16, wps=10069.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.643, loss_scale=8, train_wall=634, gb_free=4, wall=152052
2022-02-27 01:06:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 01:13:08 | INFO | train_inner | epoch 031:    106 / 788 loss=5.415, ppl=42.68, wps=10119.1, ups=0.15, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.635, loss_scale=8, train_wall=643, gb_free=4, wall=152700
2022-02-27 01:23:49 | INFO | train_inner | epoch 031:    206 / 788 loss=5.445, ppl=43.56, wps=10219.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.614, loss_scale=8, train_wall=636, gb_free=4, wall=153341
2022-02-27 01:34:30 | INFO | train_inner | epoch 031:    306 / 788 loss=5.477, ppl=44.55, wps=10218.3, ups=0.16, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.652, loss_scale=8, train_wall=636, gb_free=4, wall=153982
2022-02-27 01:45:11 | INFO | train_inner | epoch 031:    406 / 788 loss=5.489, ppl=44.92, wps=10220.2, ups=0.16, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.641, loss_scale=8, train_wall=636, gb_free=4, wall=154623
2022-02-27 01:55:53 | INFO | train_inner | epoch 031:    506 / 788 loss=5.508, ppl=45.51, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.648, loss_scale=8, train_wall=636, gb_free=4, wall=155265
2022-02-27 02:06:34 | INFO | train_inner | epoch 031:    606 / 788 loss=5.53, ppl=46.21, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.617, loss_scale=16, train_wall=636, gb_free=4, wall=155906
2022-02-27 02:07:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 02:17:22 | INFO | train_inner | epoch 031:    707 / 788 loss=5.539, ppl=46.49, wps=10118.3, ups=0.15, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.638, loss_scale=8, train_wall=643, gb_free=4, wall=156554
2022-02-27 02:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 02:26:08 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.178 | ppl 72.42 | wps 23648.8 | wpb 2034.1 | bsz 4 | num_updates 24381 | best_loss 6.167
2022-02-27 02:26:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24381 updates
2022-02-27 02:26:08 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-27 02:26:08 | INFO | train | epoch 031 | loss 5.492 | ppl 45.01 | wps 10174 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 24381 | lr 0.000202523 | gnorm 0.638 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 157080
2022-02-27 02:26:08 | INFO | fairseq.trainer | begin training epoch 32
2022-02-27 02:26:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 02:28:10 | INFO | train_inner | epoch 032:     19 / 788 loss=5.518, ppl=45.82, wps=10068.8, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.663, loss_scale=8, train_wall=634, gb_free=4, wall=157202
2022-02-27 02:38:51 | INFO | train_inner | epoch 032:    119 / 788 loss=5.414, ppl=42.65, wps=10222.3, ups=0.16, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.639, loss_scale=8, train_wall=636, gb_free=4, wall=157843
2022-02-27 02:49:32 | INFO | train_inner | epoch 032:    219 / 788 loss=5.444, ppl=43.54, wps=10218.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.648, loss_scale=8, train_wall=636, gb_free=4, wall=158484
2022-02-27 03:00:14 | INFO | train_inner | epoch 032:    319 / 788 loss=5.461, ppl=44.04, wps=10215.9, ups=0.16, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.628, loss_scale=8, train_wall=637, gb_free=4, wall=159126
2022-02-27 03:10:55 | INFO | train_inner | epoch 032:    419 / 788 loss=5.484, ppl=44.75, wps=10218.6, ups=0.16, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.657, loss_scale=16, train_wall=636, gb_free=4, wall=159767
2022-02-27 03:11:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 03:21:43 | INFO | train_inner | epoch 032:    520 / 788 loss=5.487, ppl=44.84, wps=10116.7, ups=0.15, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.627, loss_scale=8, train_wall=643, gb_free=4, wall=160415
2022-02-27 03:32:24 | INFO | train_inner | epoch 032:    620 / 788 loss=5.513, ppl=45.65, wps=10218.1, ups=0.16, wpb=65520.6, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.638, loss_scale=8, train_wall=636, gb_free=4, wall=161056
2022-02-27 03:43:05 | INFO | train_inner | epoch 032:    720 / 788 loss=5.535, ppl=46.37, wps=10219.3, ups=0.16, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.629, loss_scale=8, train_wall=636, gb_free=4, wall=161697
2022-02-27 03:50:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 03:50:28 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.172 | ppl 72.1 | wps 23653.1 | wpb 2034.1 | bsz 4 | num_updates 25168 | best_loss 6.167
2022-02-27 03:50:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25168 updates
2022-02-27 03:50:28 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-27 03:50:28 | INFO | train | epoch 032 | loss 5.481 | ppl 44.65 | wps 10187.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 25168 | lr 0.000199331 | gnorm 0.638 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 162140
2022-02-27 03:50:28 | INFO | fairseq.trainer | begin training epoch 33
2022-02-27 03:50:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 03:53:53 | INFO | train_inner | epoch 033:     32 / 788 loss=5.494, ppl=45.08, wps=10073.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.633, loss_scale=8, train_wall=634, gb_free=4, wall=162345
2022-02-27 04:04:34 | INFO | train_inner | epoch 033:    132 / 788 loss=5.405, ppl=42.38, wps=10219.9, ups=0.16, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.639, loss_scale=8, train_wall=636, gb_free=4, wall=162986
2022-02-27 04:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 04:15:22 | INFO | train_inner | epoch 033:    233 / 788 loss=5.433, ppl=43.2, wps=10116.6, ups=0.15, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.632, loss_scale=8, train_wall=643, gb_free=4, wall=163634
2022-02-27 04:26:03 | INFO | train_inner | epoch 033:    333 / 788 loss=5.454, ppl=43.85, wps=10219.1, ups=0.16, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.642, loss_scale=8, train_wall=636, gb_free=4, wall=164275
2022-02-27 04:36:45 | INFO | train_inner | epoch 033:    433 / 788 loss=5.474, ppl=44.45, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.66, loss_scale=8, train_wall=636, gb_free=4, wall=164917
2022-02-27 04:47:26 | INFO | train_inner | epoch 033:    533 / 788 loss=5.488, ppl=44.89, wps=10219.3, ups=0.16, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.641, loss_scale=8, train_wall=636, gb_free=4, wall=165558
2022-02-27 04:58:07 | INFO | train_inner | epoch 033:    633 / 788 loss=5.509, ppl=45.53, wps=10219.4, ups=0.16, wpb=65519.3, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.642, loss_scale=8, train_wall=636, gb_free=4, wall=166199
2022-02-27 05:08:49 | INFO | train_inner | epoch 033:    733 / 788 loss=5.512, ppl=45.62, wps=10217, ups=0.16, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.653, loss_scale=16, train_wall=637, gb_free=4, wall=166841
2022-02-27 05:14:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 05:14:48 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.177 | ppl 72.34 | wps 23624.8 | wpb 2034.1 | bsz 4 | num_updates 25955 | best_loss 6.167
2022-02-27 05:14:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25955 updates
2022-02-27 05:14:48 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-27 05:14:48 | INFO | train | epoch 033 | loss 5.469 | ppl 44.3 | wps 10186.8 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 25955 | lr 0.000196286 | gnorm 0.643 | loss_scale 16 | train_wall 5012 | gb_free 4 | wall 167200
2022-02-27 05:14:48 | INFO | fairseq.trainer | begin training epoch 34
2022-02-27 05:14:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 05:15:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 05:19:43 | INFO | train_inner | epoch 034:     46 / 788 loss=5.461, ppl=44.05, wps=9972.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=26000, lr=0.000196116, gnorm=0.646, loss_scale=8, train_wall=640, gb_free=4, wall=167495
2022-02-27 05:30:24 | INFO | train_inner | epoch 034:    146 / 788 loss=5.405, ppl=42.37, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.647, loss_scale=8, train_wall=636, gb_free=4, wall=168136
2022-02-27 05:41:05 | INFO | train_inner | epoch 034:    246 / 788 loss=5.423, ppl=42.91, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.642, loss_scale=8, train_wall=636, gb_free=4, wall=168778
2022-02-27 05:51:47 | INFO | train_inner | epoch 034:    346 / 788 loss=5.441, ppl=43.43, wps=10217.2, ups=0.16, wpb=65520.6, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.656, loss_scale=8, train_wall=636, gb_free=4, wall=169419
2022-02-27 06:02:28 | INFO | train_inner | epoch 034:    446 / 788 loss=5.459, ppl=43.98, wps=10219.2, ups=0.16, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.642, loss_scale=8, train_wall=636, gb_free=4, wall=170060
2022-02-27 06:13:09 | INFO | train_inner | epoch 034:    546 / 788 loss=5.476, ppl=44.51, wps=10217.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.654, loss_scale=16, train_wall=636, gb_free=4, wall=170701
2022-02-27 06:18:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 06:23:57 | INFO | train_inner | epoch 034:    647 / 788 loss=5.5, ppl=45.27, wps=10116.1, ups=0.15, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.637, loss_scale=8, train_wall=643, gb_free=4, wall=171349
2022-02-27 06:34:39 | INFO | train_inner | epoch 034:    747 / 788 loss=5.511, ppl=45.6, wps=10218.3, ups=0.16, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.642, loss_scale=8, train_wall=636, gb_free=4, wall=171991
2022-02-27 06:38:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 06:39:08 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.182 | ppl 72.59 | wps 23718.9 | wpb 2034.1 | bsz 4 | num_updates 26741 | best_loss 6.167
2022-02-27 06:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 26741 updates
2022-02-27 06:39:08 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-27 06:39:08 | INFO | train | epoch 034 | loss 5.458 | ppl 43.96 | wps 10173.8 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 26741 | lr 0.00019338 | gnorm 0.647 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 172260
2022-02-27 06:39:08 | INFO | fairseq.trainer | begin training epoch 35
2022-02-27 06:39:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 06:45:27 | INFO | train_inner | epoch 035:     59 / 788 loss=5.435, ppl=43.26, wps=10071.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=26800, lr=0.000193167, gnorm=0.641, loss_scale=8, train_wall=634, gb_free=4, wall=172639
2022-02-27 06:56:08 | INFO | train_inner | epoch 035:    159 / 788 loss=5.391, ppl=41.97, wps=10217, ups=0.16, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.656, loss_scale=8, train_wall=636, gb_free=4, wall=173280
2022-02-27 07:06:49 | INFO | train_inner | epoch 035:    259 / 788 loss=5.424, ppl=42.92, wps=10220.1, ups=0.16, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.666, loss_scale=8, train_wall=636, gb_free=4, wall=173921
2022-02-27 07:17:30 | INFO | train_inner | epoch 035:    359 / 788 loss=5.437, ppl=43.31, wps=10217.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.659, loss_scale=16, train_wall=636, gb_free=4, wall=174563
2022-02-27 07:27:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 07:28:18 | INFO | train_inner | epoch 035:    460 / 788 loss=5.449, ppl=43.7, wps=10116.6, ups=0.15, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.647, loss_scale=8, train_wall=643, gb_free=4, wall=175210
2022-02-27 07:39:00 | INFO | train_inner | epoch 035:    560 / 788 loss=5.479, ppl=44.61, wps=10220.2, ups=0.16, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.648, loss_scale=8, train_wall=636, gb_free=4, wall=175852
2022-02-27 07:49:41 | INFO | train_inner | epoch 035:    660 / 788 loss=5.485, ppl=44.78, wps=10220, ups=0.16, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.639, loss_scale=8, train_wall=636, gb_free=4, wall=176493
2022-02-27 08:00:22 | INFO | train_inner | epoch 035:    760 / 788 loss=5.496, ppl=45.13, wps=10218.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.645, loss_scale=8, train_wall=636, gb_free=4, wall=177134
2022-02-27 08:03:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 08:03:28 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.183 | ppl 72.64 | wps 23627.4 | wpb 2034.1 | bsz 4 | num_updates 27528 | best_loss 6.167
2022-02-27 08:03:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 27528 updates
2022-02-27 08:03:28 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-27 08:03:28 | INFO | train | epoch 035 | loss 5.448 | ppl 43.65 | wps 10186.7 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 27528 | lr 0.000190596 | gnorm 0.65 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 177320
2022-02-27 08:03:28 | INFO | fairseq.trainer | begin training epoch 36
2022-02-27 08:03:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 08:11:10 | INFO | train_inner | epoch 036:     72 / 788 loss=5.407, ppl=42.43, wps=10072.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=27600, lr=0.000190347, gnorm=0.653, loss_scale=8, train_wall=634, gb_free=4, wall=177782
2022-02-27 08:21:51 | INFO | train_inner | epoch 036:    172 / 788 loss=5.379, ppl=41.62, wps=10218.3, ups=0.16, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.65, loss_scale=8, train_wall=636, gb_free=4, wall=178423
2022-02-27 08:30:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 08:32:39 | INFO | train_inner | epoch 036:    273 / 788 loss=5.405, ppl=42.37, wps=10115.3, ups=0.15, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.654, loss_scale=8, train_wall=643, gb_free=4, wall=179071
2022-02-27 08:43:20 | INFO | train_inner | epoch 036:    373 / 788 loss=5.423, ppl=42.92, wps=10220.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.636, loss_scale=8, train_wall=636, gb_free=4, wall=179712
2022-02-27 08:54:02 | INFO | train_inner | epoch 036:    473 / 788 loss=5.446, ppl=43.59, wps=10218.4, ups=0.16, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.647, loss_scale=8, train_wall=636, gb_free=4, wall=180354
2022-02-27 09:04:43 | INFO | train_inner | epoch 036:    573 / 788 loss=5.471, ppl=44.34, wps=10216.3, ups=0.16, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.662, loss_scale=8, train_wall=637, gb_free=4, wall=180995
2022-02-27 09:15:24 | INFO | train_inner | epoch 036:    673 / 788 loss=5.484, ppl=44.74, wps=10217.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.667, loss_scale=8, train_wall=636, gb_free=4, wall=181637
2022-02-27 09:26:06 | INFO | train_inner | epoch 036:    773 / 788 loss=5.501, ppl=45.29, wps=10217.9, ups=0.16, wpb=65536, bsz=128, num_updates=28300, lr=0.000187978, gnorm=0.652, loss_scale=16, train_wall=636, gb_free=4, wall=182278
2022-02-27 09:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 09:27:48 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.185 | ppl 72.76 | wps 23724.3 | wpb 2034.1 | bsz 4 | num_updates 28315 | best_loss 6.167
2022-02-27 09:27:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 28315 updates
2022-02-27 09:27:48 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-27 09:27:48 | INFO | train | epoch 036 | loss 5.438 | ppl 43.35 | wps 10186.5 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 28315 | lr 0.000187928 | gnorm 0.653 | loss_scale 16 | train_wall 5012 | gb_free 4 | wall 182380
2022-02-27 09:27:48 | INFO | fairseq.trainer | begin training epoch 37
2022-02-27 09:27:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 09:36:54 | INFO | train_inner | epoch 037:     85 / 788 loss=5.38, ppl=41.65, wps=10069.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=28400, lr=0.000187647, gnorm=0.651, loss_scale=16, train_wall=634, gb_free=4, wall=182926
2022-02-27 09:41:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 09:47:42 | INFO | train_inner | epoch 037:    186 / 788 loss=5.38, ppl=41.65, wps=10113.6, ups=0.15, wpb=65534.7, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.679, loss_scale=8, train_wall=643, gb_free=4, wall=183574
2022-02-27 09:58:23 | INFO | train_inner | epoch 037:    286 / 788 loss=5.399, ppl=42.2, wps=10217.2, ups=0.16, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.658, loss_scale=8, train_wall=636, gb_free=4, wall=184215
2022-02-27 10:09:05 | INFO | train_inner | epoch 037:    386 / 788 loss=5.422, ppl=42.88, wps=10217, ups=0.16, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.655, loss_scale=8, train_wall=636, gb_free=4, wall=184857
2022-02-27 10:19:46 | INFO | train_inner | epoch 037:    486 / 788 loss=5.44, ppl=43.41, wps=10214, ups=0.16, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.652, loss_scale=8, train_wall=637, gb_free=4, wall=185498
2022-02-27 10:30:28 | INFO | train_inner | epoch 037:    586 / 788 loss=5.458, ppl=43.95, wps=10218.4, ups=0.16, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.658, loss_scale=8, train_wall=636, gb_free=4, wall=186140
2022-02-27 10:41:09 | INFO | train_inner | epoch 037:    686 / 788 loss=5.474, ppl=44.44, wps=10216.7, ups=0.16, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.673, loss_scale=16, train_wall=637, gb_free=4, wall=186781
2022-02-27 10:51:51 | INFO | train_inner | epoch 037:    786 / 788 loss=5.487, ppl=44.84, wps=10213.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.647, loss_scale=16, train_wall=637, gb_free=4, wall=187423
2022-02-27 10:52:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 10:52:10 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.181 | ppl 72.58 | wps 23672 | wpb 2034.1 | bsz 4 | num_updates 29102 | best_loss 6.167
2022-02-27 10:52:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 29102 updates
2022-02-27 10:52:10 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-27 10:52:10 | INFO | train | epoch 037 | loss 5.428 | ppl 43.06 | wps 10184.2 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 29102 | lr 0.00018537 | gnorm 0.66 | loss_scale 16 | train_wall 5013 | gb_free 4 | wall 187442
2022-02-27 10:52:10 | INFO | fairseq.trainer | begin training epoch 38
2022-02-27 10:52:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 10:53:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 11:02:45 | INFO | train_inner | epoch 038:     99 / 788 loss=5.349, ppl=40.76, wps=9974.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=29200, lr=0.000185058, gnorm=0.683, loss_scale=8, train_wall=640, gb_free=4, wall=188077
2022-02-27 11:13:26 | INFO | train_inner | epoch 038:    199 / 788 loss=5.386, ppl=41.8, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.648, loss_scale=8, train_wall=636, gb_free=4, wall=188718
2022-02-27 11:24:08 | INFO | train_inner | epoch 038:    299 / 788 loss=5.393, ppl=42.03, wps=10217.3, ups=0.16, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.674, loss_scale=8, train_wall=636, gb_free=4, wall=189360
2022-02-27 11:34:49 | INFO | train_inner | epoch 038:    399 / 788 loss=5.416, ppl=42.71, wps=10219.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.646, loss_scale=8, train_wall=636, gb_free=4, wall=190001
2022-02-27 11:45:30 | INFO | train_inner | epoch 038:    499 / 788 loss=5.434, ppl=43.22, wps=10219, ups=0.16, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.648, loss_scale=8, train_wall=636, gb_free=4, wall=190642
2022-02-27 11:56:11 | INFO | train_inner | epoch 038:    599 / 788 loss=5.451, ppl=43.73, wps=10218.4, ups=0.16, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.661, loss_scale=16, train_wall=636, gb_free=4, wall=191283
2022-02-27 12:06:53 | INFO | train_inner | epoch 038:    699 / 788 loss=5.459, ppl=43.99, wps=10219.4, ups=0.16, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.658, loss_scale=16, train_wall=636, gb_free=4, wall=191925
2022-02-27 12:11:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 12:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 12:16:30 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.184 | ppl 72.69 | wps 23660.6 | wpb 2034.1 | bsz 4 | num_updates 29888 | best_loss 6.167
2022-02-27 12:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 29888 updates
2022-02-27 12:16:30 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-27 12:16:30 | INFO | train | epoch 038 | loss 5.419 | ppl 42.8 | wps 10174.3 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 29888 | lr 0.000182916 | gnorm 0.661 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 192502
2022-02-27 12:16:30 | INFO | fairseq.trainer | begin training epoch 39
2022-02-27 12:16:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 12:17:47 | INFO | train_inner | epoch 039:     12 / 788 loss=5.458, ppl=43.94, wps=9974.2, ups=0.15, wpb=65248, bsz=127.4, num_updates=29900, lr=0.000182879, gnorm=0.675, loss_scale=8, train_wall=640, gb_free=4, wall=192579
2022-02-27 12:28:28 | INFO | train_inner | epoch 039:    112 / 788 loss=5.348, ppl=40.73, wps=10219.8, ups=0.16, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.66, loss_scale=8, train_wall=636, gb_free=4, wall=193220
2022-02-27 12:39:09 | INFO | train_inner | epoch 039:    212 / 788 loss=5.366, ppl=41.23, wps=10219.6, ups=0.16, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.646, loss_scale=8, train_wall=636, gb_free=4, wall=193861
2022-02-27 12:49:51 | INFO | train_inner | epoch 039:    312 / 788 loss=5.399, ppl=42.19, wps=10219.2, ups=0.16, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.658, loss_scale=8, train_wall=636, gb_free=4, wall=194503
2022-02-27 13:00:32 | INFO | train_inner | epoch 039:    412 / 788 loss=5.408, ppl=42.47, wps=10219.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.656, loss_scale=8, train_wall=636, gb_free=4, wall=195144
2022-02-27 13:11:13 | INFO | train_inner | epoch 039:    512 / 788 loss=5.433, ppl=43.21, wps=10218.7, ups=0.16, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.665, loss_scale=16, train_wall=636, gb_free=4, wall=195785
2022-02-27 13:16:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 13:22:01 | INFO | train_inner | epoch 039:    613 / 788 loss=5.438, ppl=43.34, wps=10117.1, ups=0.15, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.69, loss_scale=8, train_wall=643, gb_free=4, wall=196433
2022-02-27 13:32:42 | INFO | train_inner | epoch 039:    713 / 788 loss=5.461, ppl=44.04, wps=10218.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=30600, lr=0.000180775, gnorm=0.666, loss_scale=8, train_wall=636, gb_free=4, wall=197074
2022-02-27 13:40:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 13:40:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.188 | ppl 72.93 | wps 23633.6 | wpb 2034.1 | bsz 4 | num_updates 30675 | best_loss 6.167
2022-02-27 13:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 30675 updates
2022-02-27 13:40:50 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-27 13:40:50 | INFO | train | epoch 039 | loss 5.411 | ppl 42.55 | wps 10187.3 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 30675 | lr 0.000180554 | gnorm 0.662 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 197562
2022-02-27 13:40:50 | INFO | fairseq.trainer | begin training epoch 40
2022-02-27 13:40:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 13:43:30 | INFO | train_inner | epoch 040:     25 / 788 loss=5.431, ppl=43.13, wps=10072.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=30700, lr=0.000180481, gnorm=0.65, loss_scale=8, train_wall=634, gb_free=4, wall=197722
2022-02-27 13:54:11 | INFO | train_inner | epoch 040:    125 / 788 loss=5.335, ppl=40.37, wps=10217.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.672, loss_scale=8, train_wall=636, gb_free=4, wall=198363
2022-02-27 14:04:53 | INFO | train_inner | epoch 040:    225 / 788 loss=5.363, ppl=41.16, wps=10218.3, ups=0.16, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.66, loss_scale=8, train_wall=636, gb_free=4, wall=199005
2022-02-27 14:15:34 | INFO | train_inner | epoch 040:    325 / 788 loss=5.382, ppl=41.7, wps=10218.3, ups=0.16, wpb=65536, bsz=128, num_updates=31000, lr=0.000179605, gnorm=0.666, loss_scale=16, train_wall=636, gb_free=4, wall=199646
2022-02-27 14:21:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 14:26:22 | INFO | train_inner | epoch 040:    426 / 788 loss=5.393, ppl=42.03, wps=10114.6, ups=0.15, wpb=65520.6, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.668, loss_scale=8, train_wall=643, gb_free=4, wall=200294
2022-02-27 14:37:03 | INFO | train_inner | epoch 040:    526 / 788 loss=5.419, ppl=42.78, wps=10219.9, ups=0.16, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.655, loss_scale=8, train_wall=636, gb_free=4, wall=200935
2022-02-27 14:47:44 | INFO | train_inner | epoch 040:    626 / 788 loss=5.457, ppl=43.94, wps=10220.1, ups=0.16, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.68, loss_scale=8, train_wall=636, gb_free=4, wall=201576
2022-02-27 14:58:26 | INFO | train_inner | epoch 040:    726 / 788 loss=5.453, ppl=43.81, wps=10218.2, ups=0.16, wpb=65536, bsz=128, num_updates=31400, lr=0.000178458, gnorm=0.658, loss_scale=8, train_wall=636, gb_free=4, wall=202218
2022-02-27 15:05:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 15:05:10 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.191 | ppl 73.08 | wps 23560.3 | wpb 2034.1 | bsz 4 | num_updates 31462 | best_loss 6.167
2022-02-27 15:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 31462 updates
2022-02-27 15:05:10 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-27 15:05:10 | INFO | train | epoch 040 | loss 5.404 | ppl 42.33 | wps 10185.8 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 31462 | lr 0.000178282 | gnorm 0.664 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 202622
2022-02-27 15:05:10 | INFO | fairseq.trainer | begin training epoch 41
2022-02-27 15:05:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 15:09:14 | INFO | train_inner | epoch 041:     38 / 788 loss=5.408, ppl=42.45, wps=10066.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=31500, lr=0.000178174, gnorm=0.663, loss_scale=8, train_wall=634, gb_free=4, wall=202866
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 98, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 433, in forward
    x = self.final_layer_norm(x)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/normalization.py", line 169, in forward
    return F.layer_norm(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 2094, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps,
KeyboardInterrupt
