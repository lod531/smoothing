Sender: LSF System <lsfadmin@eu-g3-021>
Subject: Job 210480316: <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4> in cluster <euler> Done

Job <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4> was submitted from host <eu-login-43> by user <andriusb> in cluster <euler> at Tue Mar 22 19:44:50 2022
Job was executed on host(s) <eu-g3-021>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Tue Mar 22 20:04:34 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 22 20:04:34 2022
Terminated at Tue Mar 22 23:29:45 2022
Results reported at Tue Mar 22 23:29:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-cleaned-bpe-size0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.45 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 32 --seed 66575611 --fp16 --no-epoch-checkpoints --no-last-checkpoints --patience 3 --seed 66575614 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   12293.44 sec.
    Max Memory :                                 4373 MB
    Average Memory :                             3130.26 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15627.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   12310 sec.
    Turnaround time :                            13495 sec.

The output (if any) follows:

2022-03-22 20:04:42 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575614, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.45, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-cleaned-bpe-size0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575614, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-22 20:04:42 | INFO | fairseq.tasks.language_modeling | dictionary: 39136 types
2022-03-22 20:04:43 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39136, bias=False)
  )
)
2022-03-22 20:04:43 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-22 20:04:43 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-22 20:04:43 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-22 20:04:43 | INFO | fairseq_cli.train | num. shared model params: 38,951,936 (num. trained: 38,951,936)
2022-03-22 20:04:43 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-22 20:04:43 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/valid
2022-03-22 20:04:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-22 20:04:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-22 20:04:46 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-22 20:04:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-22 20:04:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-22 20:04:46 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-22 20:04:46 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_last.pt
2022-03-22 20:04:46 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_last.pt
2022-03-22 20:04:46 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-22 20:04:46 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
2022-03-22 20:04:46 | INFO | fairseq.trainer | begin training epoch 1
2022-03-22 20:04:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:04:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:04:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 20:06:10 | INFO | train_inner | epoch 001:    102 / 411 loss=14.559, ppl=24138.6, wps=20162.8, ups=1.23, wpb=16384, bsz=32, num_updates=100, lr=1.25975e-05, gnorm=2.793, loss_scale=32, train_wall=78, gb_free=9.7, wall=85
2022-03-22 20:07:32 | INFO | train_inner | epoch 001:    202 / 411 loss=12.703, ppl=6666.81, wps=20187.3, ups=1.23, wpb=16384, bsz=32, num_updates=200, lr=2.5095e-05, gnorm=1.282, loss_scale=32, train_wall=75, gb_free=9.7, wall=166
2022-03-22 20:08:53 | INFO | train_inner | epoch 001:    302 / 411 loss=11.454, ppl=2804.94, wps=20177.4, ups=1.23, wpb=16384, bsz=32, num_updates=300, lr=3.75925e-05, gnorm=0.926, loss_scale=32, train_wall=75, gb_free=9.7, wall=247
2022-03-22 20:10:14 | INFO | train_inner | epoch 001:    402 / 411 loss=10.714, ppl=1679.72, wps=20200.7, ups=1.23, wpb=16378.9, bsz=32, num_updates=400, lr=5.009e-05, gnorm=0.788, loss_scale=32, train_wall=75, gb_free=9.7, wall=328
2022-03-22 20:10:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:10:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.434 | ppl 1383.25 | wps 35692.3 | wpb 511.2 | bsz 1 | num_updates 409
2022-03-22 20:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 409 updates
2022-03-22 20:10:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 1 @ 409 updates, score 10.434) (writing took 1.1628342755138874 seconds)
2022-03-22 20:10:29 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-22 20:10:29 | INFO | train | epoch 001 | loss 12.319 | ppl 5110.56 | wps 19724.4 | ups 1.21 | wpb 16367.7 | bsz 32 | num_updates 409 | lr 5.12148e-05 | gnorm 1.431 | loss_scale 32 | train_wall 310 | gb_free 9.7 | wall 343
2022-03-22 20:10:29 | INFO | fairseq.trainer | begin training epoch 2
2022-03-22 20:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:11:42 | INFO | train_inner | epoch 002:     91 / 411 loss=10.438, ppl=1387.04, wps=18429.1, ups=1.13, wpb=16322.6, bsz=31.9, num_updates=500, lr=6.25875e-05, gnorm=0.716, loss_scale=32, train_wall=75, gb_free=9.7, wall=417
2022-03-22 20:13:04 | INFO | train_inner | epoch 002:    191 / 411 loss=10.281, ppl=1244.03, wps=20149.6, ups=1.23, wpb=16378.9, bsz=32, num_updates=600, lr=7.5085e-05, gnorm=0.662, loss_scale=64, train_wall=75, gb_free=9.7, wall=498
2022-03-22 20:14:25 | INFO | train_inner | epoch 002:    291 / 411 loss=10.119, ppl=1111.7, wps=20231.6, ups=1.23, wpb=16384, bsz=32, num_updates=700, lr=8.75825e-05, gnorm=0.646, loss_scale=64, train_wall=75, gb_free=9.7, wall=579
2022-03-22 20:15:46 | INFO | train_inner | epoch 002:    391 / 411 loss=9.961, ppl=996.85, wps=20184.8, ups=1.23, wpb=16384, bsz=32, num_updates=800, lr=0.00010008, gnorm=0.629, loss_scale=64, train_wall=75, gb_free=9.7, wall=660
2022-03-22 20:16:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:16:08 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.693 | ppl 827.97 | wps 35734.7 | wpb 511.2 | bsz 1 | num_updates 820 | best_loss 9.693
2022-03-22 20:16:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 820 updates
2022-03-22 20:16:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:16:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:16:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 2 @ 820 updates, score 9.693) (writing took 1.1613147612661123 seconds)
2022-03-22 20:16:10 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-22 20:16:10 | INFO | train | epoch 002 | loss 10.177 | ppl 1157.48 | wps 19725.2 | ups 1.21 | wpb 16367.8 | bsz 32 | num_updates 820 | lr 0.00010258 | gnorm 0.66 | loss_scale 64 | train_wall 308 | gb_free 9.7 | wall 684
2022-03-22 20:16:10 | INFO | fairseq.trainer | begin training epoch 3
2022-03-22 20:16:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:17:15 | INFO | train_inner | epoch 003:     80 / 411 loss=9.789, ppl=884.65, wps=18365.6, ups=1.13, wpb=16317.5, bsz=31.9, num_updates=900, lr=0.000112578, gnorm=0.622, loss_scale=64, train_wall=75, gb_free=9.7, wall=749
2022-03-22 20:18:36 | INFO | train_inner | epoch 003:    180 / 411 loss=9.642, ppl=799.04, wps=20189.9, ups=1.23, wpb=16384, bsz=32, num_updates=1000, lr=0.000125075, gnorm=0.661, loss_scale=64, train_wall=75, gb_free=9.7, wall=830
2022-03-22 20:19:57 | INFO | train_inner | epoch 003:    280 / 411 loss=9.482, ppl=715.18, wps=20159.2, ups=1.23, wpb=16384, bsz=32, num_updates=1100, lr=0.000137573, gnorm=0.631, loss_scale=128, train_wall=75, gb_free=9.7, wall=911
2022-03-22 20:20:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:21:20 | INFO | train_inner | epoch 003:    381 / 411 loss=9.354, ppl=654.17, wps=19873.1, ups=1.21, wpb=16384, bsz=32, num_updates=1200, lr=0.00015007, gnorm=0.661, loss_scale=64, train_wall=76, gb_free=9.7, wall=994
2022-03-22 20:21:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:21:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.056 | ppl 532.29 | wps 35313.4 | wpb 511.2 | bsz 1 | num_updates 1230 | best_loss 9.056
2022-03-22 20:21:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1230 updates
2022-03-22 20:21:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:21:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:21:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 3 @ 1230 updates, score 9.056) (writing took 1.220823671668768 seconds)
2022-03-22 20:21:52 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-22 20:21:52 | INFO | train | epoch 003 | loss 9.53 | ppl 739.04 | wps 19623.5 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 1230 | lr 0.000153819 | gnorm 0.651 | loss_scale 64 | train_wall 309 | gb_free 9.7 | wall 1026
2022-03-22 20:21:52 | INFO | fairseq.trainer | begin training epoch 4
2022-03-22 20:21:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:22:49 | INFO | train_inner | epoch 004:     70 / 411 loss=9.168, ppl=575.13, wps=18314.8, ups=1.12, wpb=16322.6, bsz=31.9, num_updates=1300, lr=0.000162568, gnorm=0.696, loss_scale=64, train_wall=75, gb_free=9.7, wall=1083
2022-03-22 20:24:10 | INFO | train_inner | epoch 004:    170 / 411 loss=9.049, ppl=529.59, wps=20078, ups=1.23, wpb=16384, bsz=32, num_updates=1400, lr=0.000175065, gnorm=0.687, loss_scale=64, train_wall=75, gb_free=9.7, wall=1164
2022-03-22 20:25:32 | INFO | train_inner | epoch 004:    270 / 411 loss=8.935, ppl=489.34, wps=20160.1, ups=1.23, wpb=16384, bsz=32, num_updates=1500, lr=0.000187563, gnorm=0.693, loss_scale=64, train_wall=75, gb_free=9.7, wall=1246
2022-03-22 20:26:53 | INFO | train_inner | epoch 004:    370 / 411 loss=8.826, ppl=453.85, wps=20128.7, ups=1.23, wpb=16378.9, bsz=32, num_updates=1600, lr=0.00020006, gnorm=0.662, loss_scale=64, train_wall=75, gb_free=9.7, wall=1327
2022-03-22 20:27:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:27:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.571 | ppl 380.27 | wps 35448.7 | wpb 511.2 | bsz 1 | num_updates 1641 | best_loss 8.571
2022-03-22 20:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1641 updates
2022-03-22 20:27:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:27:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 4 @ 1641 updates, score 8.571) (writing took 1.2685723528265953 seconds)
2022-03-22 20:27:34 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-22 20:27:34 | INFO | train | epoch 004 | loss 8.953 | ppl 495.67 | wps 19648.9 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 1641 | lr 0.000205184 | gnorm 0.684 | loss_scale 64 | train_wall 309 | gb_free 9.7 | wall 1368
2022-03-22 20:27:34 | INFO | fairseq.trainer | begin training epoch 5
2022-03-22 20:27:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:28:22 | INFO | train_inner | epoch 005:     59 / 411 loss=8.704, ppl=417.05, wps=18349.3, ups=1.12, wpb=16317.5, bsz=31.9, num_updates=1700, lr=0.000212558, gnorm=0.689, loss_scale=128, train_wall=75, gb_free=9.7, wall=1416
2022-03-22 20:29:43 | INFO | train_inner | epoch 005:    159 / 411 loss=8.594, ppl=386.32, wps=20116.4, ups=1.23, wpb=16384, bsz=32, num_updates=1800, lr=0.000225055, gnorm=0.671, loss_scale=128, train_wall=75, gb_free=9.7, wall=1498
2022-03-22 20:31:05 | INFO | train_inner | epoch 005:    259 / 411 loss=8.521, ppl=367.23, wps=20168.6, ups=1.23, wpb=16384, bsz=32, num_updates=1900, lr=0.000237553, gnorm=0.679, loss_scale=128, train_wall=75, gb_free=9.7, wall=1579
2022-03-22 20:32:26 | INFO | train_inner | epoch 005:    359 / 411 loss=8.433, ppl=345.69, wps=20197.3, ups=1.23, wpb=16384, bsz=32, num_updates=2000, lr=0.00025005, gnorm=0.678, loss_scale=128, train_wall=75, gb_free=9.7, wall=1660
2022-03-22 20:33:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:33:14 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.244 | ppl 303.23 | wps 35341 | wpb 511.2 | bsz 1 | num_updates 2052 | best_loss 8.244
2022-03-22 20:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2052 updates
2022-03-22 20:33:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:33:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:33:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 5 @ 2052 updates, score 8.244) (writing took 1.2335899230092764 seconds)
2022-03-22 20:33:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-22 20:33:16 | INFO | train | epoch 005 | loss 8.52 | ppl 367.12 | wps 19690.6 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 2052 | lr 0.000256549 | gnorm 0.677 | loss_scale 128 | train_wall 308 | gb_free 9.7 | wall 1710
2022-03-22 20:33:16 | INFO | fairseq.trainer | begin training epoch 6
2022-03-22 20:33:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:33:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:33:55 | INFO | train_inner | epoch 006:     49 / 411 loss=8.325, ppl=320.77, wps=18174.5, ups=1.11, wpb=16322.6, bsz=31.9, num_updates=2100, lr=0.000262548, gnorm=0.655, loss_scale=64, train_wall=76, gb_free=9.7, wall=1750
2022-03-22 20:35:17 | INFO | train_inner | epoch 006:    149 / 411 loss=8.237, ppl=301.7, wps=20178.9, ups=1.23, wpb=16384, bsz=32, num_updates=2200, lr=0.000275045, gnorm=0.657, loss_scale=64, train_wall=75, gb_free=9.7, wall=1831
2022-03-22 20:36:38 | INFO | train_inner | epoch 006:    249 / 411 loss=8.183, ppl=290.61, wps=20170.6, ups=1.23, wpb=16384, bsz=32, num_updates=2300, lr=0.000287543, gnorm=0.666, loss_scale=64, train_wall=75, gb_free=9.7, wall=1912
2022-03-22 20:37:59 | INFO | train_inner | epoch 006:    349 / 411 loss=8.135, ppl=281.08, wps=20130.7, ups=1.23, wpb=16378.9, bsz=32, num_updates=2400, lr=0.00030004, gnorm=0.655, loss_scale=64, train_wall=75, gb_free=9.7, wall=1993
2022-03-22 20:38:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:38:56 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.938 | ppl 245.21 | wps 35861.9 | wpb 511.2 | bsz 1 | num_updates 2462 | best_loss 7.938
2022-03-22 20:38:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2462 updates
2022-03-22 20:38:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:38:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:38:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 6 @ 2462 updates, score 7.938) (writing took 1.2677392940968275 seconds)
2022-03-22 20:38:57 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-22 20:38:57 | INFO | train | epoch 006 | loss 8.179 | ppl 289.8 | wps 19639.3 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 2462 | lr 0.000307788 | gnorm 0.653 | loss_scale 64 | train_wall 308 | gb_free 9.7 | wall 2051
2022-03-22 20:38:57 | INFO | fairseq.trainer | begin training epoch 7
2022-03-22 20:38:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:39:28 | INFO | train_inner | epoch 007:     38 / 411 loss=8.048, ppl=264.62, wps=18344.3, ups=1.12, wpb=16322.6, bsz=31.9, num_updates=2500, lr=0.000312538, gnorm=0.641, loss_scale=64, train_wall=75, gb_free=9.7, wall=2082
2022-03-22 20:40:50 | INFO | train_inner | epoch 007:    138 / 411 loss=7.939, ppl=245.36, wps=20146.7, ups=1.23, wpb=16384, bsz=32, num_updates=2600, lr=0.000325035, gnorm=0.644, loss_scale=128, train_wall=75, gb_free=9.7, wall=2164
2022-03-22 20:42:11 | INFO | train_inner | epoch 007:    238 / 411 loss=7.861, ppl=232.56, wps=20113.6, ups=1.23, wpb=16384, bsz=32, num_updates=2700, lr=0.000337533, gnorm=0.65, loss_scale=128, train_wall=75, gb_free=9.7, wall=2245
2022-03-22 20:43:32 | INFO | train_inner | epoch 007:    338 / 411 loss=7.864, ppl=233.02, wps=20203.7, ups=1.23, wpb=16384, bsz=32, num_updates=2800, lr=0.00035003, gnorm=0.652, loss_scale=128, train_wall=75, gb_free=9.7, wall=2326
2022-03-22 20:44:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:44:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:44:38 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.719 | ppl 210.72 | wps 35532 | wpb 511.2 | bsz 1 | num_updates 2872 | best_loss 7.719
2022-03-22 20:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2872 updates
2022-03-22 20:44:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:44:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 7 @ 2872 updates, score 7.719) (writing took 1.2301091253757477 seconds)
2022-03-22 20:44:39 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-22 20:44:39 | INFO | train | epoch 007 | loss 7.879 | ppl 235.45 | wps 19623.8 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 2872 | lr 0.000359028 | gnorm 0.65 | loss_scale 64 | train_wall 309 | gb_free 9.7 | wall 2393
2022-03-22 20:44:39 | INFO | fairseq.trainer | begin training epoch 8
2022-03-22 20:44:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:45:02 | INFO | train_inner | epoch 008:     28 / 411 loss=7.755, ppl=216.02, wps=18142, ups=1.11, wpb=16317.5, bsz=31.9, num_updates=2900, lr=0.000362528, gnorm=0.645, loss_scale=64, train_wall=76, gb_free=9.7, wall=2416
2022-03-22 20:46:23 | INFO | train_inner | epoch 008:    128 / 411 loss=7.642, ppl=199.73, wps=20162.5, ups=1.23, wpb=16378.9, bsz=32, num_updates=3000, lr=0.000375025, gnorm=0.649, loss_scale=64, train_wall=75, gb_free=9.7, wall=2498
2022-03-22 20:47:45 | INFO | train_inner | epoch 008:    228 / 411 loss=7.606, ppl=194.76, wps=20160.1, ups=1.23, wpb=16384, bsz=32, num_updates=3100, lr=0.000387523, gnorm=0.66, loss_scale=64, train_wall=75, gb_free=9.7, wall=2579
2022-03-22 20:49:06 | INFO | train_inner | epoch 008:    328 / 411 loss=7.584, ppl=191.91, wps=20170.3, ups=1.23, wpb=16384, bsz=32, num_updates=3200, lr=0.00040002, gnorm=0.644, loss_scale=64, train_wall=75, gb_free=9.7, wall=2660
2022-03-22 20:50:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:50:20 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.485 | ppl 179.19 | wps 35456.4 | wpb 511.2 | bsz 1 | num_updates 3283 | best_loss 7.485
2022-03-22 20:50:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3283 updates
2022-03-22 20:50:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:50:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 8 @ 3283 updates, score 7.485) (writing took 1.2546892333775759 seconds)
2022-03-22 20:50:21 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-22 20:50:21 | INFO | train | epoch 008 | loss 7.601 | ppl 194.21 | wps 19696.7 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 3283 | lr 0.000410393 | gnorm 0.65 | loss_scale 64 | train_wall 308 | gb_free 9.7 | wall 2735
2022-03-22 20:50:21 | INFO | fairseq.trainer | begin training epoch 9
2022-03-22 20:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:50:35 | INFO | train_inner | epoch 009:     17 / 411 loss=7.528, ppl=184.59, wps=18351.2, ups=1.12, wpb=16322.6, bsz=31.9, num_updates=3300, lr=0.000412518, gnorm=0.658, loss_scale=64, train_wall=75, gb_free=9.7, wall=2749
2022-03-22 20:51:56 | INFO | train_inner | epoch 009:    117 / 411 loss=7.379, ppl=166.41, wps=20189.1, ups=1.23, wpb=16384, bsz=32, num_updates=3400, lr=0.000425015, gnorm=0.649, loss_scale=128, train_wall=75, gb_free=9.7, wall=2830
2022-03-22 20:51:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:53:18 | INFO | train_inner | epoch 009:    218 / 411 loss=7.367, ppl=165.04, wps=19963.9, ups=1.22, wpb=16384, bsz=32, num_updates=3500, lr=0.000437513, gnorm=0.647, loss_scale=64, train_wall=76, gb_free=9.7, wall=2912
2022-03-22 20:54:40 | INFO | train_inner | epoch 009:    318 / 411 loss=7.336, ppl=161.61, wps=20069.7, ups=1.22, wpb=16384, bsz=32, num_updates=3600, lr=0.00045001, gnorm=0.654, loss_scale=64, train_wall=75, gb_free=9.7, wall=2994
2022-03-22 20:55:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:56:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.3 | ppl 157.56 | wps 35985.6 | wpb 511.2 | bsz 1 | num_updates 3693 | best_loss 7.3
2022-03-22 20:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3693 updates
2022-03-22 20:56:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 20:56:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 9 @ 3693 updates, score 7.3) (writing took 1.2907933816313744 seconds)
2022-03-22 20:56:02 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-22 20:56:02 | INFO | train | epoch 009 | loss 7.349 | ppl 163.01 | wps 19663.9 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 3693 | lr 0.000461633 | gnorm 0.653 | loss_scale 64 | train_wall 308 | gb_free 9.7 | wall 3076
2022-03-22 20:56:02 | INFO | fairseq.trainer | begin training epoch 10
2022-03-22 20:56:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:56:08 | INFO | train_inner | epoch 010:      7 / 411 loss=7.282, ppl=155.67, wps=18504.9, ups=1.13, wpb=16317.5, bsz=31.9, num_updates=3700, lr=0.000462508, gnorm=0.659, loss_scale=64, train_wall=74, gb_free=9.7, wall=3082
2022-03-22 20:57:28 | INFO | train_inner | epoch 010:    107 / 411 loss=7.133, ppl=140.32, wps=20402.2, ups=1.25, wpb=16378.9, bsz=32, num_updates=3800, lr=0.000475005, gnorm=0.652, loss_scale=64, train_wall=74, gb_free=9.7, wall=3162
2022-03-22 20:58:48 | INFO | train_inner | epoch 010:    207 / 411 loss=7.143, ppl=141.3, wps=20395.6, ups=1.24, wpb=16384, bsz=32, num_updates=3900, lr=0.000487503, gnorm=0.648, loss_scale=64, train_wall=74, gb_free=9.7, wall=3243
2022-03-22 20:59:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:00:09 | INFO | train_inner | epoch 010:    308 / 411 loss=7.116, ppl=138.69, wps=20203.9, ups=1.23, wpb=16384, bsz=32, num_updates=4000, lr=0.0005, gnorm=0.657, loss_scale=64, train_wall=75, gb_free=9.7, wall=3324
2022-03-22 21:01:30 | INFO | train_inner | epoch 010:    408 / 411 loss=7.093, ppl=136.51, wps=20441.2, ups=1.25, wpb=16384, bsz=32, num_updates=4100, lr=0.000493865, gnorm=0.652, loss_scale=64, train_wall=74, gb_free=9.7, wall=3404
2022-03-22 21:01:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:01:38 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.156 | ppl 142.66 | wps 35746.2 | wpb 511.2 | bsz 1 | num_updates 4103 | best_loss 7.156
2022-03-22 21:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4103 updates
2022-03-22 21:01:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:01:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 10 @ 4103 updates, score 7.156) (writing took 1.454683506861329 seconds)
2022-03-22 21:01:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-22 21:01:40 | INFO | train | epoch 010 | loss 7.121 | ppl 139.18 | wps 19867.1 | ups 1.21 | wpb 16367.8 | bsz 32 | num_updates 4103 | lr 0.000493684 | gnorm 0.653 | loss_scale 64 | train_wall 305 | gb_free 9.7 | wall 3414
2022-03-22 21:01:40 | INFO | fairseq.trainer | begin training epoch 11
2022-03-22 21:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:02:58 | INFO | train_inner | epoch 011:     97 / 411 loss=6.926, ppl=121.56, wps=18493.9, ups=1.13, wpb=16322.6, bsz=31.9, num_updates=4200, lr=0.00048795, gnorm=0.655, loss_scale=64, train_wall=74, gb_free=9.7, wall=3492
2022-03-22 21:04:18 | INFO | train_inner | epoch 011:    197 / 411 loss=6.918, ppl=120.9, wps=20371.4, ups=1.24, wpb=16384, bsz=32, num_updates=4300, lr=0.000482243, gnorm=0.647, loss_scale=64, train_wall=74, gb_free=9.7, wall=3573
2022-03-22 21:05:39 | INFO | train_inner | epoch 011:    297 / 411 loss=6.899, ppl=119.32, wps=20384.1, ups=1.24, wpb=16378.9, bsz=32, num_updates=4400, lr=0.000476731, gnorm=0.655, loss_scale=64, train_wall=74, gb_free=9.7, wall=3653
2022-03-22 21:06:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 21:07:00 | INFO | train_inner | epoch 011:    398 / 411 loss=6.877, ppl=117.52, wps=20136.2, ups=1.23, wpb=16384, bsz=32, num_updates=4500, lr=0.000471405, gnorm=0.647, loss_scale=32, train_wall=75, gb_free=9.7, wall=3734
2022-03-22 21:07:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:07:17 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.013 | ppl 129.19 | wps 36592.2 | wpb 511.2 | bsz 1 | num_updates 4513 | best_loss 7.013
2022-03-22 21:07:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4513 updates
2022-03-22 21:07:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:07:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:07:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 11 @ 4513 updates, score 7.013) (writing took 1.3955661412328482 seconds)
2022-03-22 21:07:18 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-22 21:07:18 | INFO | train | epoch 011 | loss 6.902 | ppl 119.56 | wps 19839.1 | ups 1.21 | wpb 16367.8 | bsz 32 | num_updates 4513 | lr 0.000470725 | gnorm 0.651 | loss_scale 32 | train_wall 305 | gb_free 9.7 | wall 3752
2022-03-22 21:07:18 | INFO | fairseq.trainer | begin training epoch 12
2022-03-22 21:07:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:08:28 | INFO | train_inner | epoch 012:     87 / 411 loss=6.721, ppl=105.49, wps=18545.3, ups=1.14, wpb=16322.6, bsz=31.9, num_updates=4600, lr=0.000466252, gnorm=0.644, loss_scale=32, train_wall=74, gb_free=9.7, wall=3822
2022-03-22 21:09:50 | INFO | train_inner | epoch 012:    187 / 411 loss=6.703, ppl=104.17, wps=19933, ups=1.22, wpb=16378.9, bsz=32, num_updates=4700, lr=0.000461266, gnorm=0.647, loss_scale=32, train_wall=76, gb_free=9.7, wall=3904
2022-03-22 21:11:12 | INFO | train_inner | epoch 012:    287 / 411 loss=6.708, ppl=104.55, wps=20088.5, ups=1.23, wpb=16384, bsz=32, num_updates=4800, lr=0.000456435, gnorm=0.651, loss_scale=32, train_wall=75, gb_free=9.7, wall=3986
2022-03-22 21:12:33 | INFO | train_inner | epoch 012:    387 / 411 loss=6.716, ppl=105.12, wps=20200.7, ups=1.23, wpb=16384, bsz=32, num_updates=4900, lr=0.000451754, gnorm=0.649, loss_scale=32, train_wall=75, gb_free=9.7, wall=4067
2022-03-22 21:12:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:12:59 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.917 | ppl 120.87 | wps 35860.3 | wpb 511.2 | bsz 1 | num_updates 4924 | best_loss 6.917
2022-03-22 21:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4924 updates
2022-03-22 21:12:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:13:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:13:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 12 @ 4924 updates, score 6.917) (writing took 1.4507582914084196 seconds)
2022-03-22 21:13:00 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-22 21:13:00 | INFO | train | epoch 012 | loss 6.708 | ppl 104.55 | wps 19675.5 | ups 1.2 | wpb 16367.8 | bsz 32 | num_updates 4924 | lr 0.000450652 | gnorm 0.648 | loss_scale 32 | train_wall 309 | gb_free 9.7 | wall 4094
2022-03-22 21:13:00 | INFO | fairseq.trainer | begin training epoch 13
2022-03-22 21:13:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:14:01 | INFO | train_inner | epoch 013:     76 / 411 loss=6.563, ppl=94.55, wps=18484.2, ups=1.13, wpb=16322.6, bsz=31.9, num_updates=5000, lr=0.000447214, gnorm=0.648, loss_scale=64, train_wall=74, gb_free=9.7, wall=4155
2022-03-22 21:15:20 | INFO | train_inner | epoch 013:    176 / 411 loss=6.555, ppl=94, wps=20730.8, ups=1.27, wpb=16378.9, bsz=32, num_updates=5100, lr=0.000442807, gnorm=0.649, loss_scale=64, train_wall=73, gb_free=9.7, wall=4234
2022-03-22 21:16:38 | INFO | train_inner | epoch 013:    276 / 411 loss=6.561, ppl=94.42, wps=20975.9, ups=1.28, wpb=16384, bsz=32, num_updates=5200, lr=0.000438529, gnorm=0.651, loss_scale=64, train_wall=72, gb_free=9.7, wall=4313
2022-03-22 21:17:56 | INFO | train_inner | epoch 013:    376 / 411 loss=6.549, ppl=93.65, wps=21023.6, ups=1.28, wpb=16384, bsz=32, num_updates=5300, lr=0.000434372, gnorm=0.658, loss_scale=64, train_wall=72, gb_free=9.7, wall=4390
2022-03-22 21:18:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:18:30 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.832 | ppl 113.92 | wps 36798.3 | wpb 511.2 | bsz 1 | num_updates 5335 | best_loss 6.832
2022-03-22 21:18:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5335 updates
2022-03-22 21:18:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:18:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 13 @ 5335 updates, score 6.832) (writing took 1.1779356319457293 seconds)
2022-03-22 21:18:31 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-22 21:18:31 | INFO | train | epoch 013 | loss 6.549 | ppl 93.63 | wps 20308.7 | ups 1.24 | wpb 16367.8 | bsz 32 | num_updates 5335 | lr 0.000432945 | gnorm 0.651 | loss_scale 64 | train_wall 299 | gb_free 9.7 | wall 4425
2022-03-22 21:18:31 | INFO | fairseq.trainer | begin training epoch 14
2022-03-22 21:18:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:19:22 | INFO | train_inner | epoch 014:     65 / 411 loss=6.462, ppl=88.17, wps=19010.5, ups=1.16, wpb=16322.6, bsz=31.9, num_updates=5400, lr=0.000430331, gnorm=0.649, loss_scale=64, train_wall=72, gb_free=9.7, wall=4476
2022-03-22 21:20:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:20:41 | INFO | train_inner | epoch 014:    166 / 411 loss=6.404, ppl=84.69, wps=20759.7, ups=1.27, wpb=16384, bsz=32, num_updates=5500, lr=0.000426401, gnorm=0.652, loss_scale=64, train_wall=73, gb_free=9.7, wall=4555
2022-03-22 21:21:59 | INFO | train_inner | epoch 014:    266 / 411 loss=6.428, ppl=86.12, wps=20982.5, ups=1.28, wpb=16378.9, bsz=32, num_updates=5600, lr=0.000422577, gnorm=0.659, loss_scale=64, train_wall=72, gb_free=9.7, wall=4633
2022-03-22 21:23:17 | INFO | train_inner | epoch 014:    366 / 411 loss=6.418, ppl=85.5, wps=20980.1, ups=1.28, wpb=16384, bsz=32, num_updates=5700, lr=0.000418854, gnorm=0.655, loss_scale=64, train_wall=72, gb_free=9.7, wall=4711
2022-03-22 21:23:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:23:59 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.768 | ppl 109.02 | wps 37120.9 | wpb 511.2 | bsz 1 | num_updates 5745 | best_loss 6.768
2022-03-22 21:23:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5745 updates
2022-03-22 21:23:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:24:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:24:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 14 @ 5745 updates, score 6.768) (writing took 1.2929553259164095 seconds)
2022-03-22 21:24:00 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-22 21:24:00 | INFO | train | epoch 014 | loss 6.414 | ppl 85.28 | wps 20420.6 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 5745 | lr 0.00041721 | gnorm 0.655 | loss_scale 64 | train_wall 297 | gb_free 9.7 | wall 4754
2022-03-22 21:24:00 | INFO | fairseq.trainer | begin training epoch 15
2022-03-22 21:24:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:24:43 | INFO | train_inner | epoch 015:     55 / 411 loss=6.329, ppl=80.4, wps=19015.1, ups=1.17, wpb=16317.5, bsz=31.9, num_updates=5800, lr=0.000415227, gnorm=0.662, loss_scale=64, train_wall=72, gb_free=9.7, wall=4797
2022-03-22 21:26:02 | INFO | train_inner | epoch 015:    155 / 411 loss=6.273, ppl=77.33, wps=20874.8, ups=1.27, wpb=16384, bsz=32, num_updates=5900, lr=0.000411693, gnorm=0.664, loss_scale=64, train_wall=73, gb_free=9.7, wall=4876
2022-03-22 21:27:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:27:21 | INFO | train_inner | epoch 015:    256 / 411 loss=6.314, ppl=79.54, wps=20639.6, ups=1.26, wpb=16384, bsz=32, num_updates=6000, lr=0.000408248, gnorm=0.667, loss_scale=64, train_wall=73, gb_free=9.7, wall=4955
2022-03-22 21:28:39 | INFO | train_inner | epoch 015:    356 / 411 loss=6.315, ppl=79.6, wps=20922.7, ups=1.28, wpb=16384, bsz=32, num_updates=6100, lr=0.000404888, gnorm=0.66, loss_scale=64, train_wall=72, gb_free=9.7, wall=5033
2022-03-22 21:29:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:29:28 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.726 | ppl 105.82 | wps 37188.4 | wpb 511.2 | bsz 1 | num_updates 6155 | best_loss 6.726
2022-03-22 21:29:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 6155 updates
2022-03-22 21:29:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:29:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 15 @ 6155 updates, score 6.726) (writing took 1.3561511617153883 seconds)
2022-03-22 21:29:30 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-22 21:29:30 | INFO | train | epoch 015 | loss 6.298 | ppl 78.7 | wps 20351.7 | ups 1.24 | wpb 16367.8 | bsz 32 | num_updates 6155 | lr 0.000403075 | gnorm 0.663 | loss_scale 64 | train_wall 298 | gb_free 9.7 | wall 5084
2022-03-22 21:29:30 | INFO | fairseq.trainer | begin training epoch 16
2022-03-22 21:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:30:05 | INFO | train_inner | epoch 016:     45 / 411 loss=6.244, ppl=75.78, wps=19029, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=6200, lr=0.00040161, gnorm=0.661, loss_scale=64, train_wall=72, gb_free=9.7, wall=5119
2022-03-22 21:31:23 | INFO | train_inner | epoch 016:    145 / 411 loss=6.17, ppl=72, wps=20925, ups=1.28, wpb=16378.9, bsz=32, num_updates=6300, lr=0.00039841, gnorm=0.667, loss_scale=64, train_wall=72, gb_free=9.7, wall=5197
2022-03-22 21:32:41 | INFO | train_inner | epoch 016:    245 / 411 loss=6.208, ppl=73.92, wps=20961.6, ups=1.28, wpb=16384, bsz=32, num_updates=6400, lr=0.000395285, gnorm=0.674, loss_scale=64, train_wall=72, gb_free=9.7, wall=5276
2022-03-22 21:34:00 | INFO | train_inner | epoch 016:    345 / 411 loss=6.236, ppl=75.37, wps=20954.4, ups=1.28, wpb=16384, bsz=32, num_updates=6500, lr=0.000392232, gnorm=0.679, loss_scale=64, train_wall=72, gb_free=9.7, wall=5354
2022-03-22 21:34:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:34:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:34:57 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.682 | ppl 102.68 | wps 37114.9 | wpb 511.2 | bsz 1 | num_updates 6565 | best_loss 6.682
2022-03-22 21:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6565 updates
2022-03-22 21:34:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:34:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:34:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 16 @ 6565 updates, score 6.682) (writing took 1.2934619523584843 seconds)
2022-03-22 21:34:59 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-22 21:34:59 | INFO | train | epoch 016 | loss 6.2 | ppl 73.5 | wps 20397.5 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 6565 | lr 0.000390286 | gnorm 0.672 | loss_scale 64 | train_wall 297 | gb_free 9.7 | wall 5413
2022-03-22 21:34:59 | INFO | fairseq.trainer | begin training epoch 17
2022-03-22 21:34:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:35:26 | INFO | train_inner | epoch 017:     35 / 411 loss=6.159, ppl=71.48, wps=18849.1, ups=1.15, wpb=16322.6, bsz=31.9, num_updates=6600, lr=0.000389249, gnorm=0.671, loss_scale=64, train_wall=73, gb_free=9.7, wall=5440
2022-03-22 21:36:45 | INFO | train_inner | epoch 017:    135 / 411 loss=6.089, ppl=68.05, wps=20915.7, ups=1.28, wpb=16378.9, bsz=32, num_updates=6700, lr=0.000386334, gnorm=0.677, loss_scale=64, train_wall=72, gb_free=9.7, wall=5519
2022-03-22 21:38:03 | INFO | train_inner | epoch 017:    235 / 411 loss=6.11, ppl=69.08, wps=20909.5, ups=1.28, wpb=16384, bsz=32, num_updates=6800, lr=0.000383482, gnorm=0.674, loss_scale=64, train_wall=72, gb_free=9.7, wall=5597
2022-03-22 21:39:21 | INFO | train_inner | epoch 017:    335 / 411 loss=6.125, ppl=69.81, wps=20951.8, ups=1.28, wpb=16384, bsz=32, num_updates=6900, lr=0.000380693, gnorm=0.684, loss_scale=64, train_wall=72, gb_free=9.7, wall=5675
2022-03-22 21:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:40:27 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.655 | ppl 100.8 | wps 37299.1 | wpb 511.2 | bsz 1 | num_updates 6976 | best_loss 6.655
2022-03-22 21:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6976 updates
2022-03-22 21:40:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:40:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:40:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 17 @ 6976 updates, score 6.655) (writing took 1.3181438371539116 seconds)
2022-03-22 21:40:28 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-22 21:40:28 | INFO | train | epoch 017 | loss 6.111 | ppl 69.1 | wps 20431.7 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 6976 | lr 0.000378614 | gnorm 0.679 | loss_scale 64 | train_wall 297 | gb_free 9.7 | wall 5742
2022-03-22 21:40:28 | INFO | fairseq.trainer | begin training epoch 18
2022-03-22 21:40:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:40:47 | INFO | train_inner | epoch 018:     24 / 411 loss=6.103, ppl=68.74, wps=19057.5, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=7000, lr=0.000377964, gnorm=0.683, loss_scale=64, train_wall=72, gb_free=9.7, wall=5761
2022-03-22 21:41:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:42:06 | INFO | train_inner | epoch 018:    125 / 411 loss=5.984, ppl=63.3, wps=20755.3, ups=1.27, wpb=16384, bsz=32, num_updates=7100, lr=0.000375293, gnorm=0.681, loss_scale=64, train_wall=73, gb_free=9.7, wall=5840
2022-03-22 21:43:24 | INFO | train_inner | epoch 018:    225 / 411 loss=6.023, ppl=65.04, wps=20953.5, ups=1.28, wpb=16384, bsz=32, num_updates=7200, lr=0.000372678, gnorm=0.688, loss_scale=64, train_wall=72, gb_free=9.7, wall=5918
2022-03-22 21:44:42 | INFO | train_inner | epoch 018:    325 / 411 loss=6.05, ppl=66.27, wps=20976.6, ups=1.28, wpb=16384, bsz=32, num_updates=7300, lr=0.000370117, gnorm=0.691, loss_scale=64, train_wall=72, gb_free=9.7, wall=5996
2022-03-22 21:45:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 21:45:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:45:55 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.614 | ppl 97.93 | wps 37394.5 | wpb 511.2 | bsz 1 | num_updates 7385 | best_loss 6.614
2022-03-22 21:45:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7385 updates
2022-03-22 21:45:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:45:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:45:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 18 @ 7385 updates, score 6.614) (writing took 1.3116943873465061 seconds)
2022-03-22 21:45:56 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-22 21:45:56 | INFO | train | epoch 018 | loss 6.029 | ppl 65.32 | wps 20371.1 | ups 1.24 | wpb 16367.7 | bsz 32 | num_updates 7385 | lr 0.00036798 | gnorm 0.688 | loss_scale 32 | train_wall 297 | gb_free 9.7 | wall 6071
2022-03-22 21:45:57 | INFO | fairseq.trainer | begin training epoch 19
2022-03-22 21:45:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:46:08 | INFO | train_inner | epoch 019:     15 / 411 loss=6.054, ppl=66.46, wps=18893.1, ups=1.16, wpb=16317.5, bsz=31.9, num_updates=7400, lr=0.000367607, gnorm=0.698, loss_scale=32, train_wall=73, gb_free=9.7, wall=6083
2022-03-22 21:47:26 | INFO | train_inner | epoch 019:    115 / 411 loss=5.914, ppl=60.28, wps=20975.3, ups=1.28, wpb=16378.9, bsz=32, num_updates=7500, lr=0.000365148, gnorm=0.691, loss_scale=32, train_wall=72, gb_free=9.7, wall=6161
2022-03-22 21:48:44 | INFO | train_inner | epoch 019:    215 / 411 loss=5.961, ppl=62.28, wps=20982.7, ups=1.28, wpb=16384, bsz=32, num_updates=7600, lr=0.000362738, gnorm=0.694, loss_scale=32, train_wall=72, gb_free=9.7, wall=6239
2022-03-22 21:50:03 | INFO | train_inner | epoch 019:    315 / 411 loss=5.988, ppl=63.48, wps=20991.3, ups=1.28, wpb=16384, bsz=32, num_updates=7700, lr=0.000360375, gnorm=0.698, loss_scale=32, train_wall=72, gb_free=9.7, wall=6317
2022-03-22 21:51:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:51:24 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.582 | ppl 95.82 | wps 37328.1 | wpb 511.2 | bsz 1 | num_updates 7796 | best_loss 6.582
2022-03-22 21:51:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7796 updates
2022-03-22 21:51:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:51:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:51:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 19 @ 7796 updates, score 6.582) (writing took 1.2351885251700878 seconds)
2022-03-22 21:51:25 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-22 21:51:25 | INFO | train | epoch 019 | loss 5.96 | ppl 62.26 | wps 20490.4 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 7796 | lr 0.000358149 | gnorm 0.697 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 6399
2022-03-22 21:51:25 | INFO | fairseq.trainer | begin training epoch 20
2022-03-22 21:51:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:51:28 | INFO | train_inner | epoch 020:      4 / 411 loss=5.983, ppl=63.24, wps=19094.9, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=7800, lr=0.000358057, gnorm=0.706, loss_scale=32, train_wall=72, gb_free=9.7, wall=6402
2022-03-22 21:52:46 | INFO | train_inner | epoch 020:    104 / 411 loss=5.847, ppl=57.56, wps=20923.7, ups=1.28, wpb=16378.9, bsz=32, num_updates=7900, lr=0.000355784, gnorm=0.69, loss_scale=64, train_wall=72, gb_free=9.7, wall=6481
2022-03-22 21:54:05 | INFO | train_inner | epoch 020:    204 / 411 loss=5.877, ppl=58.75, wps=20942.8, ups=1.28, wpb=16384, bsz=32, num_updates=8000, lr=0.000353553, gnorm=0.702, loss_scale=64, train_wall=72, gb_free=9.7, wall=6559
2022-03-22 21:55:23 | INFO | train_inner | epoch 020:    304 / 411 loss=5.913, ppl=60.27, wps=20970.2, ups=1.28, wpb=16384, bsz=32, num_updates=8100, lr=0.000351364, gnorm=0.706, loss_scale=64, train_wall=72, gb_free=9.7, wall=6637
2022-03-22 21:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 21:56:41 | INFO | train_inner | epoch 020:    405 / 411 loss=5.93, ppl=60.98, wps=20825.5, ups=1.27, wpb=16384, bsz=32, num_updates=8200, lr=0.000349215, gnorm=0.71, loss_scale=32, train_wall=73, gb_free=9.7, wall=6716
2022-03-22 21:56:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:56:52 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.571 | ppl 95.07 | wps 37426.7 | wpb 511.2 | bsz 1 | num_updates 8206 | best_loss 6.571
2022-03-22 21:56:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 8206 updates
2022-03-22 21:56:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:56:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 21:56:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 20 @ 8206 updates, score 6.571) (writing took 1.3721482437103987 seconds)
2022-03-22 21:56:53 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-22 21:56:53 | INFO | train | epoch 020 | loss 5.893 | ppl 59.44 | wps 20420.6 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 8206 | lr 0.000349087 | gnorm 0.702 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 6728
2022-03-22 21:56:53 | INFO | fairseq.trainer | begin training epoch 21
2022-03-22 21:56:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:58:07 | INFO | train_inner | epoch 021:     94 / 411 loss=5.799, ppl=55.69, wps=19126.2, ups=1.17, wpb=16317.5, bsz=31.9, num_updates=8300, lr=0.000347105, gnorm=0.709, loss_scale=32, train_wall=72, gb_free=9.7, wall=6801
2022-03-22 21:59:24 | INFO | train_inner | epoch 021:    194 / 411 loss=5.827, ppl=56.79, wps=21065.6, ups=1.29, wpb=16384, bsz=32, num_updates=8400, lr=0.000345033, gnorm=0.706, loss_scale=32, train_wall=72, gb_free=9.7, wall=6879
2022-03-22 22:00:42 | INFO | train_inner | epoch 021:    294 / 411 loss=5.841, ppl=57.33, wps=21078.3, ups=1.29, wpb=16384, bsz=32, num_updates=8500, lr=0.000342997, gnorm=0.714, loss_scale=32, train_wall=72, gb_free=9.7, wall=6956
2022-03-22 22:02:00 | INFO | train_inner | epoch 021:    394 / 411 loss=5.861, ppl=58.13, wps=21073.4, ups=1.29, wpb=16384, bsz=32, num_updates=8600, lr=0.000340997, gnorm=0.717, loss_scale=32, train_wall=72, gb_free=9.7, wall=7034
2022-03-22 22:02:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:02:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.534 | ppl 92.65 | wps 37421 | wpb 511.2 | bsz 1 | num_updates 8617 | best_loss 6.534
2022-03-22 22:02:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8617 updates
2022-03-22 22:02:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:02:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:02:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 21 @ 8617 updates, score 6.534) (writing took 1.2936953641474247 seconds)
2022-03-22 22:02:20 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-22 22:02:20 | INFO | train | epoch 021 | loss 5.833 | ppl 57 | wps 20570.6 | ups 1.26 | wpb 16367.8 | bsz 32 | num_updates 8617 | lr 0.000340661 | gnorm 0.712 | loss_scale 32 | train_wall 295 | gb_free 9.7 | wall 7055
2022-03-22 22:02:20 | INFO | fairseq.trainer | begin training epoch 22
2022-03-22 22:02:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:02:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:03:26 | INFO | train_inner | epoch 022:     84 / 411 loss=5.737, ppl=53.33, wps=18950.4, ups=1.16, wpb=16322.6, bsz=31.9, num_updates=8700, lr=0.000339032, gnorm=0.717, loss_scale=32, train_wall=73, gb_free=9.7, wall=7120
2022-03-22 22:04:44 | INFO | train_inner | epoch 022:    184 / 411 loss=5.766, ppl=54.43, wps=21029.7, ups=1.28, wpb=16384, bsz=32, num_updates=8800, lr=0.0003371, gnorm=0.72, loss_scale=32, train_wall=72, gb_free=9.7, wall=7198
2022-03-22 22:06:02 | INFO | train_inner | epoch 022:    284 / 411 loss=5.796, ppl=55.55, wps=21027.5, ups=1.28, wpb=16384, bsz=32, num_updates=8900, lr=0.000335201, gnorm=0.72, loss_scale=32, train_wall=72, gb_free=9.7, wall=7276
2022-03-22 22:07:20 | INFO | train_inner | epoch 022:    384 / 411 loss=5.825, ppl=56.7, wps=21016.2, ups=1.28, wpb=16378.9, bsz=32, num_updates=9000, lr=0.000333333, gnorm=0.717, loss_scale=32, train_wall=72, gb_free=9.7, wall=7354
2022-03-22 22:07:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:07:47 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.518 | ppl 91.65 | wps 37493.3 | wpb 511.2 | bsz 1 | num_updates 9027 | best_loss 6.518
2022-03-22 22:07:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 9027 updates
2022-03-22 22:07:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:07:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 22 @ 9027 updates, score 6.518) (writing took 1.3577462434768677 seconds)
2022-03-22 22:07:48 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-22 22:07:48 | INFO | train | epoch 022 | loss 5.777 | ppl 54.85 | wps 20472.4 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 9027 | lr 0.000332834 | gnorm 0.719 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 7382
2022-03-22 22:07:48 | INFO | fairseq.trainer | begin training epoch 23
2022-03-22 22:07:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:08:45 | INFO | train_inner | epoch 023:     73 / 411 loss=5.696, ppl=51.85, wps=19117.1, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=9100, lr=0.000331497, gnorm=0.718, loss_scale=32, train_wall=72, gb_free=9.7, wall=7439
2022-03-22 22:10:03 | INFO | train_inner | epoch 023:    173 / 411 loss=5.699, ppl=51.94, wps=20947.5, ups=1.28, wpb=16384, bsz=32, num_updates=9200, lr=0.00032969, gnorm=0.72, loss_scale=64, train_wall=72, gb_free=9.7, wall=7518
2022-03-22 22:11:22 | INFO | train_inner | epoch 023:    273 / 411 loss=5.743, ppl=53.56, wps=20883.6, ups=1.27, wpb=16384, bsz=32, num_updates=9300, lr=0.000327913, gnorm=0.727, loss_scale=64, train_wall=72, gb_free=9.7, wall=7596
2022-03-22 22:12:40 | INFO | train_inner | epoch 023:    373 / 411 loss=5.77, ppl=54.58, wps=20963.3, ups=1.28, wpb=16378.9, bsz=32, num_updates=9400, lr=0.000326164, gnorm=0.733, loss_scale=64, train_wall=72, gb_free=9.7, wall=7674
2022-03-22 22:13:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:13:16 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.512 | ppl 91.25 | wps 35777.9 | wpb 511.2 | bsz 1 | num_updates 9438 | best_loss 6.512
2022-03-22 22:13:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9438 updates
2022-03-22 22:13:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:13:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:13:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 23 @ 9438 updates, score 6.512) (writing took 1.3157077133655548 seconds)
2022-03-22 22:13:17 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-22 22:13:17 | INFO | train | epoch 023 | loss 5.726 | ppl 52.93 | wps 20447.9 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 9438 | lr 0.000325507 | gnorm 0.725 | loss_scale 64 | train_wall 297 | gb_free 9.7 | wall 7711
2022-03-22 22:13:17 | INFO | fairseq.trainer | begin training epoch 24
2022-03-22 22:13:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:14:06 | INFO | train_inner | epoch 024:     62 / 411 loss=5.67, ppl=50.9, wps=19055.7, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=9500, lr=0.000324443, gnorm=0.729, loss_scale=64, train_wall=72, gb_free=9.7, wall=7760
2022-03-22 22:15:24 | INFO | train_inner | epoch 024:    162 / 411 loss=5.661, ppl=50.6, wps=21020.7, ups=1.28, wpb=16378.9, bsz=32, num_updates=9600, lr=0.000322749, gnorm=0.739, loss_scale=64, train_wall=72, gb_free=9.7, wall=7838
2022-03-22 22:16:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:16:42 | INFO | train_inner | epoch 024:    263 / 411 loss=5.685, ppl=51.46, wps=20811.7, ups=1.27, wpb=16384, bsz=32, num_updates=9700, lr=0.000321081, gnorm=0.741, loss_scale=64, train_wall=73, gb_free=9.7, wall=7917
2022-03-22 22:18:00 | INFO | train_inner | epoch 024:    363 / 411 loss=5.722, ppl=52.79, wps=20997.1, ups=1.28, wpb=16384, bsz=32, num_updates=9800, lr=0.000319438, gnorm=0.738, loss_scale=64, train_wall=72, gb_free=9.7, wall=7995
2022-03-22 22:18:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:18:44 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.489 | ppl 89.82 | wps 37537.4 | wpb 511.2 | bsz 1 | num_updates 9848 | best_loss 6.489
2022-03-22 22:18:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9848 updates
2022-03-22 22:18:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:18:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 24 @ 9848 updates, score 6.489) (writing took 1.275573220103979 seconds)
2022-03-22 22:18:45 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-22 22:18:45 | INFO | train | epoch 024 | loss 5.679 | ppl 51.23 | wps 20472.1 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 9848 | lr 0.000318659 | gnorm 0.738 | loss_scale 64 | train_wall 296 | gb_free 9.7 | wall 8039
2022-03-22 22:18:45 | INFO | fairseq.trainer | begin training epoch 25
2022-03-22 22:18:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:19:26 | INFO | train_inner | epoch 025:     52 / 411 loss=5.622, ppl=49.26, wps=19122.3, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=9900, lr=0.000317821, gnorm=0.733, loss_scale=64, train_wall=72, gb_free=9.7, wall=8080
2022-03-22 22:20:44 | INFO | train_inner | epoch 025:    152 / 411 loss=5.594, ppl=48.31, wps=20995.4, ups=1.28, wpb=16378.9, bsz=32, num_updates=10000, lr=0.000316228, gnorm=0.74, loss_scale=64, train_wall=72, gb_free=9.7, wall=8158
2022-03-22 22:22:02 | INFO | train_inner | epoch 025:    252 / 411 loss=5.637, ppl=49.76, wps=21018.9, ups=1.28, wpb=16384, bsz=32, num_updates=10100, lr=0.000314658, gnorm=0.743, loss_scale=64, train_wall=72, gb_free=9.7, wall=8236
2022-03-22 22:23:20 | INFO | train_inner | epoch 025:    352 / 411 loss=5.68, ppl=51.29, wps=21019.6, ups=1.28, wpb=16384, bsz=32, num_updates=10200, lr=0.000313112, gnorm=0.744, loss_scale=64, train_wall=72, gb_free=9.7, wall=8314
2022-03-22 22:23:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:24:12 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.491 | ppl 89.94 | wps 37434.6 | wpb 511.2 | bsz 1 | num_updates 10258 | best_loss 6.489
2022-03-22 22:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 10258 updates
2022-03-22 22:24:12 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-22 22:24:12 | INFO | train | epoch 025 | loss 5.633 | ppl 49.63 | wps 20550.1 | ups 1.26 | wpb 16367.8 | bsz 32 | num_updates 10258 | lr 0.000312226 | gnorm 0.741 | loss_scale 64 | train_wall 296 | gb_free 9.7 | wall 8366
2022-03-22 22:24:12 | INFO | fairseq.trainer | begin training epoch 26
2022-03-22 22:24:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:24:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:24:45 | INFO | train_inner | epoch 026:     43 / 411 loss=5.61, ppl=48.83, wps=19063.2, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=10300, lr=0.000311588, gnorm=0.75, loss_scale=32, train_wall=73, gb_free=9.7, wall=8399
2022-03-22 22:26:03 | INFO | train_inner | epoch 026:    143 / 411 loss=5.557, ppl=47.08, wps=20996.8, ups=1.28, wpb=16378.9, bsz=32, num_updates=10400, lr=0.000310087, gnorm=0.744, loss_scale=32, train_wall=72, gb_free=9.7, wall=8477
2022-03-22 22:27:21 | INFO | train_inner | epoch 026:    243 / 411 loss=5.587, ppl=48.06, wps=21012.4, ups=1.28, wpb=16384, bsz=32, num_updates=10500, lr=0.000308607, gnorm=0.753, loss_scale=32, train_wall=72, gb_free=9.7, wall=8555
2022-03-22 22:28:39 | INFO | train_inner | epoch 026:    343 / 411 loss=5.625, ppl=49.36, wps=21008.2, ups=1.28, wpb=16384, bsz=32, num_updates=10600, lr=0.000307148, gnorm=0.75, loss_scale=32, train_wall=72, gb_free=9.7, wall=8633
2022-03-22 22:29:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:29:38 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.466 | ppl 88.41 | wps 37390.2 | wpb 511.2 | bsz 1 | num_updates 10668 | best_loss 6.466
2022-03-22 22:29:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10668 updates
2022-03-22 22:29:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:29:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:29:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 26 @ 10668 updates, score 6.466) (writing took 1.295311814174056 seconds)
2022-03-22 22:29:40 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-22 22:29:40 | INFO | train | epoch 026 | loss 5.591 | ppl 48.2 | wps 20461.1 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 10668 | lr 0.000306167 | gnorm 0.75 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 8694
2022-03-22 22:29:40 | INFO | fairseq.trainer | begin training epoch 27
2022-03-22 22:29:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:30:05 | INFO | train_inner | epoch 027:     32 / 411 loss=5.591, ppl=48.21, wps=19098.2, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=10700, lr=0.000305709, gnorm=0.751, loss_scale=32, train_wall=72, gb_free=9.7, wall=8719
2022-03-22 22:31:23 | INFO | train_inner | epoch 027:    132 / 411 loss=5.516, ppl=45.77, wps=20979.7, ups=1.28, wpb=16384, bsz=32, num_updates=10800, lr=0.00030429, gnorm=0.75, loss_scale=64, train_wall=72, gb_free=9.7, wall=8797
2022-03-22 22:32:41 | INFO | train_inner | epoch 027:    232 / 411 loss=5.556, ppl=47.04, wps=20997.3, ups=1.28, wpb=16378.9, bsz=32, num_updates=10900, lr=0.000302891, gnorm=0.756, loss_scale=64, train_wall=72, gb_free=9.7, wall=8875
2022-03-22 22:33:59 | INFO | train_inner | epoch 027:    332 / 411 loss=5.573, ppl=47.6, wps=21011.4, ups=1.28, wpb=16384, bsz=32, num_updates=11000, lr=0.000301511, gnorm=0.764, loss_scale=64, train_wall=72, gb_free=9.7, wall=8953
2022-03-22 22:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:35:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:35:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.459 | ppl 88 | wps 37431.1 | wpb 511.2 | bsz 1 | num_updates 11078 | best_loss 6.459
2022-03-22 22:35:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 11078 updates
2022-03-22 22:35:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:35:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:35:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 27 @ 11078 updates, score 6.459) (writing took 1.1984697207808495 seconds)
2022-03-22 22:35:08 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-22 22:35:08 | INFO | train | epoch 027 | loss 5.552 | ppl 46.92 | wps 20459.9 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 11078 | lr 0.000300448 | gnorm 0.757 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 9022
2022-03-22 22:35:08 | INFO | fairseq.trainer | begin training epoch 28
2022-03-22 22:35:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:35:25 | INFO | train_inner | epoch 028:     22 / 411 loss=5.571, ppl=47.55, wps=18959.7, ups=1.16, wpb=16322.6, bsz=31.9, num_updates=11100, lr=0.00030015, gnorm=0.762, loss_scale=32, train_wall=73, gb_free=9.7, wall=9039
2022-03-22 22:36:43 | INFO | train_inner | epoch 028:    122 / 411 loss=5.465, ppl=44.16, wps=21003.6, ups=1.28, wpb=16378.9, bsz=32, num_updates=11200, lr=0.000298807, gnorm=0.764, loss_scale=32, train_wall=72, gb_free=9.7, wall=9117
2022-03-22 22:38:01 | INFO | train_inner | epoch 028:    222 / 411 loss=5.509, ppl=45.55, wps=21015.3, ups=1.28, wpb=16384, bsz=32, num_updates=11300, lr=0.000297482, gnorm=0.765, loss_scale=32, train_wall=72, gb_free=9.7, wall=9195
2022-03-22 22:39:19 | INFO | train_inner | epoch 028:    322 / 411 loss=5.542, ppl=46.59, wps=21021.3, ups=1.28, wpb=16384, bsz=32, num_updates=11400, lr=0.000296174, gnorm=0.766, loss_scale=32, train_wall=72, gb_free=9.7, wall=9273
2022-03-22 22:40:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:40:34 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.453 | ppl 87.58 | wps 37327.4 | wpb 511.2 | bsz 1 | num_updates 11489 | best_loss 6.453
2022-03-22 22:40:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11489 updates
2022-03-22 22:40:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:40:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:40:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 28 @ 11489 updates, score 6.453) (writing took 1.2383846305310726 seconds)
2022-03-22 22:40:35 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-22 22:40:35 | INFO | train | epoch 028 | loss 5.515 | ppl 45.71 | wps 20524.6 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 11489 | lr 0.000295025 | gnorm 0.765 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 9350
2022-03-22 22:40:35 | INFO | fairseq.trainer | begin training epoch 29
2022-03-22 22:40:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:40:44 | INFO | train_inner | epoch 029:     11 / 411 loss=5.543, ppl=46.62, wps=19132.9, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=11500, lr=0.000294884, gnorm=0.77, loss_scale=32, train_wall=72, gb_free=9.7, wall=9358
2022-03-22 22:41:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:42:03 | INFO | train_inner | epoch 029:    112 / 411 loss=5.416, ppl=42.69, wps=20785.4, ups=1.27, wpb=16378.9, bsz=32, num_updates=11600, lr=0.00029361, gnorm=0.771, loss_scale=32, train_wall=73, gb_free=9.7, wall=9437
2022-03-22 22:43:21 | INFO | train_inner | epoch 029:    212 / 411 loss=5.476, ppl=44.51, wps=21000.5, ups=1.28, wpb=16384, bsz=32, num_updates=11700, lr=0.000292353, gnorm=0.769, loss_scale=32, train_wall=72, gb_free=9.7, wall=9515
2022-03-22 22:44:39 | INFO | train_inner | epoch 029:    312 / 411 loss=5.497, ppl=45.15, wps=21006.2, ups=1.28, wpb=16384, bsz=32, num_updates=11800, lr=0.000291111, gnorm=0.769, loss_scale=32, train_wall=72, gb_free=9.7, wall=9593
2022-03-22 22:45:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:46:02 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.448 | ppl 87.32 | wps 37450.6 | wpb 511.2 | bsz 1 | num_updates 11899 | best_loss 6.448
2022-03-22 22:46:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11899 updates
2022-03-22 22:46:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:46:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 29 @ 11899 updates, score 6.448) (writing took 1.2282165493816137 seconds)
2022-03-22 22:46:03 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-22 22:46:03 | INFO | train | epoch 029 | loss 5.478 | ppl 44.56 | wps 20458.1 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 11899 | lr 0.000289898 | gnorm 0.771 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 9678
2022-03-22 22:46:03 | INFO | fairseq.trainer | begin training epoch 30
2022-03-22 22:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:46:04 | INFO | train_inner | epoch 030:      1 / 411 loss=5.527, ppl=46.11, wps=19111.9, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=11900, lr=0.000289886, gnorm=0.775, loss_scale=32, train_wall=72, gb_free=9.7, wall=9678
2022-03-22 22:47:22 | INFO | train_inner | epoch 030:    101 / 411 loss=5.405, ppl=42.38, wps=20988.4, ups=1.28, wpb=16384, bsz=32, num_updates=12000, lr=0.000288675, gnorm=0.775, loss_scale=32, train_wall=72, gb_free=9.7, wall=9757
2022-03-22 22:48:40 | INFO | train_inner | epoch 030:    201 / 411 loss=5.428, ppl=43.05, wps=20988.5, ups=1.28, wpb=16384, bsz=32, num_updates=12100, lr=0.00028748, gnorm=0.777, loss_scale=64, train_wall=72, gb_free=9.7, wall=9835
2022-03-22 22:49:58 | INFO | train_inner | epoch 030:    301 / 411 loss=5.437, ppl=43.31, wps=20992.3, ups=1.28, wpb=16384, bsz=32, num_updates=12200, lr=0.000286299, gnorm=0.78, loss_scale=64, train_wall=72, gb_free=9.7, wall=9913
2022-03-22 22:51:16 | INFO | train_inner | epoch 030:    401 / 411 loss=5.507, ppl=45.48, wps=20988, ups=1.28, wpb=16378.9, bsz=32, num_updates=12300, lr=0.000285133, gnorm=0.776, loss_scale=64, train_wall=72, gb_free=9.7, wall=9991
2022-03-22 22:51:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:51:30 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.442 | ppl 86.94 | wps 37402.1 | wpb 511.2 | bsz 1 | num_updates 12310 | best_loss 6.442
2022-03-22 22:51:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 12310 updates
2022-03-22 22:51:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:51:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 30 @ 12310 updates, score 6.442) (writing took 1.2508078645914793 seconds)
2022-03-22 22:51:32 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-22 22:51:32 | INFO | train | epoch 030 | loss 5.445 | ppl 43.55 | wps 20497.2 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 12310 | lr 0.000285017 | gnorm 0.778 | loss_scale 64 | train_wall 296 | gb_free 9.7 | wall 10006
2022-03-22 22:51:32 | INFO | fairseq.trainer | begin training epoch 31
2022-03-22 22:51:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:52:42 | INFO | train_inner | epoch 031:     90 / 411 loss=5.366, ppl=41.23, wps=19105.9, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=12400, lr=0.000283981, gnorm=0.781, loss_scale=64, train_wall=72, gb_free=9.7, wall=10076
2022-03-22 22:54:00 | INFO | train_inner | epoch 031:    190 / 411 loss=5.39, ppl=41.94, wps=20994.9, ups=1.28, wpb=16384, bsz=32, num_updates=12500, lr=0.000282843, gnorm=0.786, loss_scale=64, train_wall=72, gb_free=9.7, wall=10154
2022-03-22 22:54:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:55:19 | INFO | train_inner | epoch 031:    291 / 411 loss=5.439, ppl=43.39, wps=20790.3, ups=1.27, wpb=16378.9, bsz=32, num_updates=12600, lr=0.000281718, gnorm=0.782, loss_scale=64, train_wall=73, gb_free=9.7, wall=10233
2022-03-22 22:56:37 | INFO | train_inner | epoch 031:    391 / 411 loss=5.45, ppl=43.73, wps=21007, ups=1.28, wpb=16384, bsz=32, num_updates=12700, lr=0.000280607, gnorm=0.788, loss_scale=64, train_wall=72, gb_free=9.7, wall=10311
2022-03-22 22:56:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:56:58 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.44 | ppl 86.79 | wps 37402.2 | wpb 511.2 | bsz 1 | num_updates 12720 | best_loss 6.44
2022-03-22 22:56:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12720 updates
2022-03-22 22:56:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:57:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 22:57:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 31 @ 12720 updates, score 6.44) (writing took 1.221056867390871 seconds)
2022-03-22 22:57:00 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-22 22:57:00 | INFO | train | epoch 031 | loss 5.413 | ppl 42.62 | wps 20460.5 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 12720 | lr 0.000280386 | gnorm 0.785 | loss_scale 64 | train_wall 296 | gb_free 9.7 | wall 10334
2022-03-22 22:57:00 | INFO | fairseq.trainer | begin training epoch 32
2022-03-22 22:57:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:57:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:58:03 | INFO | train_inner | epoch 032:     81 / 411 loss=5.35, ppl=40.77, wps=18949.6, ups=1.16, wpb=16322.6, bsz=31.9, num_updates=12800, lr=0.000279508, gnorm=0.787, loss_scale=32, train_wall=73, gb_free=9.7, wall=10397
2022-03-22 22:59:21 | INFO | train_inner | epoch 032:    181 / 411 loss=5.356, ppl=40.95, wps=20981.1, ups=1.28, wpb=16384, bsz=32, num_updates=12900, lr=0.000278423, gnorm=0.793, loss_scale=32, train_wall=72, gb_free=9.7, wall=10475
2022-03-22 23:00:39 | INFO | train_inner | epoch 032:    281 / 411 loss=5.412, ppl=42.57, wps=21006.7, ups=1.28, wpb=16384, bsz=32, num_updates=13000, lr=0.00027735, gnorm=0.794, loss_scale=32, train_wall=72, gb_free=9.7, wall=10553
2022-03-22 23:01:57 | INFO | train_inner | epoch 032:    381 / 411 loss=5.424, ppl=42.94, wps=21001, ups=1.28, wpb=16378.9, bsz=32, num_updates=13100, lr=0.000276289, gnorm=0.794, loss_scale=32, train_wall=72, gb_free=9.7, wall=10631
2022-03-22 23:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 23:02:26 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.423 | ppl 85.78 | wps 37391.3 | wpb 511.2 | bsz 1 | num_updates 13130 | best_loss 6.423
2022-03-22 23:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 13130 updates
2022-03-22 23:02:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 23:02:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 23:02:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 32 @ 13130 updates, score 6.423) (writing took 1.27915901504457 seconds)
2022-03-22 23:02:28 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-22 23:02:28 | INFO | train | epoch 032 | loss 5.384 | ppl 41.76 | wps 20454.1 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 13130 | lr 0.000275974 | gnorm 0.792 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 10662
2022-03-22 23:02:28 | INFO | fairseq.trainer | begin training epoch 33
2022-03-22 23:02:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 23:03:22 | INFO | train_inner | epoch 033:     70 / 411 loss=5.322, ppl=40, wps=19103.7, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=13200, lr=0.000275241, gnorm=0.794, loss_scale=32, train_wall=72, gb_free=9.7, wall=10717
2022-03-22 23:04:40 | INFO | train_inner | epoch 033:    170 / 411 loss=5.325, ppl=40.09, wps=20996.5, ups=1.28, wpb=16378.9, bsz=32, num_updates=13300, lr=0.000274204, gnorm=0.8, loss_scale=64, train_wall=72, gb_free=9.7, wall=10795
2022-03-22 23:05:58 | INFO | train_inner | epoch 033:    270 / 411 loss=5.376, ppl=41.54, wps=21008.7, ups=1.28, wpb=16384, bsz=32, num_updates=13400, lr=0.000273179, gnorm=0.798, loss_scale=64, train_wall=72, gb_free=9.7, wall=10873
2022-03-22 23:07:16 | INFO | train_inner | epoch 033:    370 / 411 loss=5.396, ppl=42.12, wps=21009.9, ups=1.28, wpb=16384, bsz=32, num_updates=13500, lr=0.000272166, gnorm=0.804, loss_scale=64, train_wall=72, gb_free=9.7, wall=10951
2022-03-22 23:07:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 23:07:54 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.428 | ppl 86.1 | wps 37450.1 | wpb 511.2 | bsz 1 | num_updates 13541 | best_loss 6.423
2022-03-22 23:07:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13541 updates
2022-03-22 23:07:54 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-22 23:07:54 | INFO | train | epoch 033 | loss 5.355 | ppl 40.92 | wps 20592.2 | ups 1.26 | wpb 16367.8 | bsz 32 | num_updates 13541 | lr 0.000271753 | gnorm 0.799 | loss_scale 64 | train_wall 296 | gb_free 9.7 | wall 10989
2022-03-22 23:07:54 | INFO | fairseq.trainer | begin training epoch 34
2022-03-22 23:07:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 23:08:40 | INFO | train_inner | epoch 034:     59 / 411 loss=5.32, ppl=39.95, wps=19403.8, ups=1.19, wpb=16322.6, bsz=31.9, num_updates=13600, lr=0.000271163, gnorm=0.796, loss_scale=64, train_wall=72, gb_free=9.7, wall=11035
2022-03-22 23:09:59 | INFO | train_inner | epoch 034:    159 / 411 loss=5.288, ppl=39.08, wps=20989, ups=1.28, wpb=16384, bsz=32, num_updates=13700, lr=0.000270172, gnorm=0.801, loss_scale=64, train_wall=72, gb_free=9.7, wall=11113
2022-03-22 23:11:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 23:11:17 | INFO | train_inner | epoch 034:    260 / 411 loss=5.341, ppl=40.54, wps=20781.7, ups=1.27, wpb=16378.9, bsz=32, num_updates=13800, lr=0.000269191, gnorm=0.804, loss_scale=64, train_wall=73, gb_free=9.7, wall=11192
2022-03-22 23:12:35 | INFO | train_inner | epoch 034:    360 / 411 loss=5.363, ppl=41.14, wps=20996.9, ups=1.28, wpb=16384, bsz=32, num_updates=13900, lr=0.000268221, gnorm=0.809, loss_scale=64, train_wall=72, gb_free=9.7, wall=11270
2022-03-22 23:13:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 23:13:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.421 | ppl 85.69 | wps 37056 | wpb 511.2 | bsz 1 | num_updates 13951 | best_loss 6.421
2022-03-22 23:13:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13951 updates
2022-03-22 23:13:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 23:13:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-22 23:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 34 @ 13951 updates, score 6.421) (writing took 1.2750897072255611 seconds)
2022-03-22 23:13:23 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-22 23:13:23 | INFO | train | epoch 034 | loss 5.327 | ppl 40.14 | wps 20447.3 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 13951 | lr 0.00026773 | gnorm 0.804 | loss_scale 64 | train_wall 296 | gb_free 9.7 | wall 11317
2022-03-22 23:13:23 | INFO | fairseq.trainer | begin training epoch 35
2022-03-22 23:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 23:14:01 | INFO | train_inner | epoch 035:     49 / 411 loss=5.301, ppl=39.42, wps=19078.2, ups=1.17, wpb=16322.6, bsz=31.9, num_updates=14000, lr=0.000267261, gnorm=0.804, loss_scale=64, train_wall=72, gb_free=9.7, wall=11355
2022-03-22 23:15:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 23:15:20 | INFO | train_inner | epoch 035:    150 / 411 loss=5.266, ppl=38.49, wps=20777.7, ups=1.27, wpb=16384, bsz=32, num_updates=14100, lr=0.000266312, gnorm=0.808, loss_scale=32, train_wall=73, gb_free=9.7, wall=11434
2022-03-22 23:16:38 | INFO | train_inner | epoch 035:    250 / 411 loss=5.319, ppl=39.93, wps=20989.8, ups=1.28, wpb=16384, bsz=32, num_updates=14200, lr=0.000265372, gnorm=0.817, loss_scale=32, train_wall=72, gb_free=9.7, wall=11512
2022-03-22 23:17:56 | INFO | train_inner | epoch 035:    350 / 411 loss=5.325, ppl=40.07, wps=20995.5, ups=1.28, wpb=16378.9, bsz=32, num_updates=14300, lr=0.000264443, gnorm=0.812, loss_scale=32, train_wall=72, gb_free=9.7, wall=11590
2022-03-22 23:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 23:18:49 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.423 | ppl 85.82 | wps 37469.9 | wpb 511.2 | bsz 1 | num_updates 14361 | best_loss 6.421
2022-03-22 23:18:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 14361 updates
2022-03-22 23:18:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-22 23:18:49 | INFO | train | epoch 035 | loss 5.301 | ppl 39.42 | wps 20524.6 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 14361 | lr 0.000263881 | gnorm 0.812 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 11644
2022-03-22 23:18:50 | INFO | fairseq.trainer | begin training epoch 36
2022-03-22 23:18:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 23:19:20 | INFO | train_inner | epoch 036:     39 / 411 loss=5.286, ppl=39.02, wps=19392.7, ups=1.19, wpb=16322.6, bsz=31.9, num_updates=14400, lr=0.000263523, gnorm=0.816, loss_scale=32, train_wall=72, gb_free=9.7, wall=11674
2022-03-22 23:20:38 | INFO | train_inner | epoch 036:    139 / 411 loss=5.223, ppl=37.35, wps=21006.8, ups=1.28, wpb=16384, bsz=32, num_updates=14500, lr=0.000262613, gnorm=0.813, loss_scale=32, train_wall=72, gb_free=9.7, wall=11752
2022-03-22 23:21:56 | INFO | train_inner | epoch 036:    239 / 411 loss=5.296, ppl=39.28, wps=21005.7, ups=1.28, wpb=16378.9, bsz=32, num_updates=14600, lr=0.000261712, gnorm=0.817, loss_scale=32, train_wall=72, gb_free=9.7, wall=11830
2022-03-22 23:23:14 | INFO | train_inner | epoch 036:    339 / 411 loss=5.311, ppl=39.7, wps=21018.4, ups=1.28, wpb=16384, bsz=32, num_updates=14700, lr=0.00026082, gnorm=0.819, loss_scale=64, train_wall=72, gb_free=9.7, wall=11908
2022-03-22 23:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 23:24:16 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.426 | ppl 85.96 | wps 37344.1 | wpb 511.2 | bsz 1 | num_updates 14772 | best_loss 6.421
2022-03-22 23:24:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14772 updates
2022-03-22 23:24:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-22 23:24:16 | INFO | train | epoch 036 | loss 5.275 | ppl 38.72 | wps 20595.5 | ups 1.26 | wpb 16367.8 | bsz 32 | num_updates 14772 | lr 0.000260184 | gnorm 0.816 | loss_scale 64 | train_wall 296 | gb_free 9.7 | wall 11970
2022-03-22 23:24:16 | INFO | fairseq.trainer | begin training epoch 37
2022-03-22 23:24:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 23:24:38 | INFO | train_inner | epoch 037:     28 / 411 loss=5.278, ppl=38.79, wps=19403.5, ups=1.19, wpb=16322.6, bsz=31.9, num_updates=14800, lr=0.000259938, gnorm=0.817, loss_scale=64, train_wall=72, gb_free=9.7, wall=11992
2022-03-22 23:25:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 23:25:57 | INFO | train_inner | epoch 037:    129 / 411 loss=5.206, ppl=36.91, wps=20796, ups=1.27, wpb=16378.9, bsz=32, num_updates=14900, lr=0.000259064, gnorm=0.82, loss_scale=32, train_wall=73, gb_free=9.7, wall=12071
2022-03-22 23:27:15 | INFO | train_inner | epoch 037:    229 / 411 loss=5.244, ppl=37.91, wps=20992.1, ups=1.28, wpb=16384, bsz=32, num_updates=15000, lr=0.000258199, gnorm=0.826, loss_scale=32, train_wall=72, gb_free=9.7, wall=12149
2022-03-22 23:28:33 | INFO | train_inner | epoch 037:    329 / 411 loss=5.291, ppl=39.15, wps=21009.7, ups=1.28, wpb=16384, bsz=32, num_updates=15100, lr=0.000257343, gnorm=0.825, loss_scale=32, train_wall=72, gb_free=9.7, wall=12227
2022-03-22 23:29:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 23:29:43 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.427 | ppl 86.06 | wps 37418.7 | wpb 511.2 | bsz 1 | num_updates 15182 | best_loss 6.421
2022-03-22 23:29:43 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-22 23:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 15182 updates
2022-03-22 23:29:43 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-22 23:29:43 | INFO | train | epoch 037 | loss 5.252 | ppl 38.1 | wps 20539.4 | ups 1.25 | wpb 16367.8 | bsz 32 | num_updates 15182 | lr 0.000256647 | gnorm 0.824 | loss_scale 32 | train_wall 296 | gb_free 9.7 | wall 12297
2022-03-22 23:29:43 | INFO | fairseq_cli.train | done training in 12297.0 seconds
Sender: LSF System <lsfadmin@eu-g3-054>
Subject: Job 210566210: <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4> in cluster <euler> Done

Job <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4> was submitted from host <eu-login-06> by user <andriusb> in cluster <euler> at Wed Mar 23 06:37:06 2022
Job was executed on host(s) <eu-g3-054>, in queue <gpuhe.4h>, as user <andriusb> in cluster <euler> at Wed Mar 23 06:37:31 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 06:37:31 2022
Terminated at Wed Mar 23 08:11:46 2022
Results reported at Wed Mar 23 08:11:46 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-cleaned-bpe-size0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.45 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 66575611 --fp16 --no-epoch-checkpoints --no-last-checkpoints --patience 3 --seed 66575614 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5643.59 sec.
    Max Memory :                                 4489 MB
    Average Memory :                             3351.75 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15511.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   5656 sec.
    Turnaround time :                            5680 sec.

The output (if any) follows:

2022-03-23 06:37:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575614, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.45, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-cleaned-bpe-size0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575614, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 06:37:40 | INFO | fairseq.tasks.language_modeling | dictionary: 39136 types
2022-03-23 06:37:40 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39136, bias=False)
  )
)
2022-03-23 06:37:40 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-23 06:37:40 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-23 06:37:40 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-23 06:37:40 | INFO | fairseq_cli.train | num. shared model params: 38,951,936 (num. trained: 38,951,936)
2022-03-23 06:37:40 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 06:37:40 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/valid
2022-03-23 06:37:43 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 06:37:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 06:37:43 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 06:37:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 06:37:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 06:37:43 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-23 06:37:43 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_last.pt
2022-03-23 06:37:43 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_last.pt
2022-03-23 06:37:43 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 06:37:43 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
2022-03-23 06:37:43 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 06:37:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:37:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-23 06:37:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 06:37:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 06:37:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-23 06:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:39:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.97 | ppl 8024.76 | wps 171026 | wpb 2040.3 | bsz 4 | num_updates 99
2022-03-23 06:39:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 99 updates
2022-03-23 06:39:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:39:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 1 @ 99 updates, score 12.97) (writing took 0.9268011590465903 seconds)
2022-03-23 06:39:14 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-23 06:39:14 | INFO | train | epoch 001 | loss 14.547 | ppl 23940.8 | wps 80061 | ups 1.23 | wpb 65303.3 | bsz 127.6 | num_updates 99 | lr 1.24725e-05 | gnorm 2.717 | loss_scale 8 | train_wall 83 | gb_free 21.6 | wall 91
2022-03-23 06:39:14 | INFO | fairseq.trainer | begin training epoch 2
2022-03-23 06:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:39:15 | INFO | train_inner | epoch 002:      1 / 103 loss=14.535, ppl=23734.4, wps=80012.4, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=100, lr=1.25975e-05, gnorm=2.704, loss_scale=8, train_wall=84, gb_free=21.6, wall=91
2022-03-23 06:40:34 | INFO | train_inner | epoch 002:    101 / 103 loss=12.586, ppl=6146.38, wps=82166.9, ups=1.25, wpb=65530.9, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.189, loss_scale=8, train_wall=75, gb_free=21.6, wall=171
2022-03-23 06:40:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:40:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.628 | ppl 3164.81 | wps 168939 | wpb 2040.3 | bsz 4 | num_updates 202 | best_loss 11.628
2022-03-23 06:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 202 updates
2022-03-23 06:40:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:40:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:40:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 2 @ 202 updates, score 11.628) (writing took 0.9018878200440668 seconds)
2022-03-23 06:40:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-23 06:40:38 | INFO | train | epoch 002 | loss 12.581 | ppl 6127.2 | wps 79817.5 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 202 | lr 2.5345e-05 | gnorm 1.187 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 175
2022-03-23 06:40:38 | INFO | fairseq.trainer | begin training epoch 3
2022-03-23 06:40:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:41:56 | INFO | train_inner | epoch 003:     98 / 103 loss=11.257, ppl=2446.59, wps=79700.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=300, lr=3.75925e-05, gnorm=0.732, loss_scale=8, train_wall=75, gb_free=21.6, wall=253
2022-03-23 06:42:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:42:01 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.655 | ppl 1612.74 | wps 166348 | wpb 2040.3 | bsz 4 | num_updates 305 | best_loss 10.655
2022-03-23 06:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 305 updates
2022-03-23 06:42:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:42:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 3 @ 305 updates, score 10.655) (writing took 0.8952619000338018 seconds)
2022-03-23 06:42:02 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-23 06:42:02 | INFO | train | epoch 003 | loss 11.225 | ppl 2393.74 | wps 79759.3 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 305 | lr 3.82174e-05 | gnorm 0.72 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 259
2022-03-23 06:42:02 | INFO | fairseq.trainer | begin training epoch 4
2022-03-23 06:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:43:18 | INFO | train_inner | epoch 004:     95 / 103 loss=10.563, ppl=1512.94, wps=79802.6, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=400, lr=5.009e-05, gnorm=0.473, loss_scale=8, train_wall=75, gb_free=21.6, wall=335
2022-03-23 06:43:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:43:26 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.287 | ppl 1249.76 | wps 164390 | wpb 2040.3 | bsz 4 | num_updates 408 | best_loss 10.287
2022-03-23 06:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 408 updates
2022-03-23 06:43:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:43:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 4 @ 408 updates, score 10.287) (writing took 0.8977597350021824 seconds)
2022-03-23 06:43:27 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-23 06:43:27 | INFO | train | epoch 004 | loss 10.542 | ppl 1491.04 | wps 79862.8 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 408 | lr 5.10898e-05 | gnorm 0.463 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 343
2022-03-23 06:43:27 | INFO | fairseq.trainer | begin training epoch 5
2022-03-23 06:43:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:44:40 | INFO | train_inner | epoch 005:     92 / 103 loss=10.285, ppl=1248.04, wps=79775, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=500, lr=6.25875e-05, gnorm=0.449, loss_scale=8, train_wall=75, gb_free=21.6, wall=417
2022-03-23 06:44:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:44:50 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.034 | ppl 1048.18 | wps 163669 | wpb 2040.3 | bsz 4 | num_updates 511 | best_loss 10.034
2022-03-23 06:44:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 511 updates
2022-03-23 06:44:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:44:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:44:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 5 @ 511 updates, score 10.034) (writing took 0.892834584985394 seconds)
2022-03-23 06:44:51 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-23 06:44:51 | INFO | train | epoch 005 | loss 10.264 | ppl 1230 | wps 79822.8 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 511 | lr 6.39622e-05 | gnorm 0.452 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 428
2022-03-23 06:44:51 | INFO | fairseq.trainer | begin training epoch 6
2022-03-23 06:44:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:46:02 | INFO | train_inner | epoch 006:     89 / 103 loss=10.057, ppl=1065.1, wps=79710.5, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=600, lr=7.5085e-05, gnorm=0.486, loss_scale=16, train_wall=75, gb_free=21.6, wall=499
2022-03-23 06:46:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:46:14 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.775 | ppl 876.22 | wps 164191 | wpb 2040.3 | bsz 4 | num_updates 614 | best_loss 9.775
2022-03-23 06:46:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 614 updates
2022-03-23 06:46:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 6 @ 614 updates, score 9.775) (writing took 0.8985711499699391 seconds)
2022-03-23 06:46:15 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-23 06:46:15 | INFO | train | epoch 006 | loss 10.027 | ppl 1043.57 | wps 79776 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 614 | lr 7.68347e-05 | gnorm 0.486 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 512
2022-03-23 06:46:15 | INFO | fairseq.trainer | begin training epoch 7
2022-03-23 06:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:47:24 | INFO | train_inner | epoch 007:     86 / 103 loss=9.825, ppl=907.13, wps=79679.1, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=700, lr=8.75825e-05, gnorm=0.449, loss_scale=16, train_wall=75, gb_free=21.6, wall=581
2022-03-23 06:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:47:39 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.534 | ppl 741.13 | wps 164367 | wpb 2040.3 | bsz 4 | num_updates 717 | best_loss 9.534
2022-03-23 06:47:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 717 updates
2022-03-23 06:47:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:47:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:47:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 7 @ 717 updates, score 9.534) (writing took 0.8791771869873628 seconds)
2022-03-23 06:47:40 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-23 06:47:40 | INFO | train | epoch 007 | loss 9.79 | ppl 885.04 | wps 79774.1 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 717 | lr 8.97071e-05 | gnorm 0.462 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 596
2022-03-23 06:47:40 | INFO | fairseq.trainer | begin training epoch 8
2022-03-23 06:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:48:46 | INFO | train_inner | epoch 008:     83 / 103 loss=9.594, ppl=772.84, wps=79693.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=800, lr=0.00010008, gnorm=0.503, loss_scale=16, train_wall=75, gb_free=21.6, wall=663
2022-03-23 06:49:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:49:03 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.29 | ppl 625.79 | wps 164838 | wpb 2040.3 | bsz 4 | num_updates 820 | best_loss 9.29
2022-03-23 06:49:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 820 updates
2022-03-23 06:49:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:49:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:49:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 8 @ 820 updates, score 9.29) (writing took 0.885475298971869 seconds)
2022-03-23 06:49:04 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-23 06:49:04 | INFO | train | epoch 008 | loss 9.557 | ppl 753.26 | wps 79738.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 820 | lr 0.00010258 | gnorm 0.495 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 681
2022-03-23 06:49:04 | INFO | fairseq.trainer | begin training epoch 9
2022-03-23 06:49:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:50:08 | INFO | train_inner | epoch 009:     80 / 103 loss=9.375, ppl=663.95, wps=79674.9, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=900, lr=0.000112578, gnorm=0.562, loss_scale=16, train_wall=75, gb_free=21.6, wall=745
2022-03-23 06:50:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:50:27 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.058 | ppl 533.13 | wps 167278 | wpb 2040.3 | bsz 4 | num_updates 923 | best_loss 9.058
2022-03-23 06:50:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 923 updates
2022-03-23 06:50:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:50:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:50:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 9 @ 923 updates, score 9.058) (writing took 0.9331864319974557 seconds)
2022-03-23 06:50:28 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-23 06:50:28 | INFO | train | epoch 009 | loss 9.324 | ppl 640.84 | wps 79710.2 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 923 | lr 0.000115452 | gnorm 0.57 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 765
2022-03-23 06:50:28 | INFO | fairseq.trainer | begin training epoch 10
2022-03-23 06:50:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:51:30 | INFO | train_inner | epoch 010:     77 / 103 loss=9.149, ppl=567.87, wps=79598, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=1000, lr=0.000125075, gnorm=0.559, loss_scale=16, train_wall=75, gb_free=21.6, wall=827
2022-03-23 06:51:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:51:52 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.849 | ppl 461.12 | wps 165928 | wpb 2040.3 | bsz 4 | num_updates 1026 | best_loss 8.849
2022-03-23 06:51:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1026 updates
2022-03-23 06:51:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:51:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:51:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 10 @ 1026 updates, score 8.849) (writing took 0.8474607190000825 seconds)
2022-03-23 06:51:53 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-23 06:51:53 | INFO | train | epoch 010 | loss 9.097 | ppl 547.79 | wps 79772.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1026 | lr 0.000128324 | gnorm 0.566 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 849
2022-03-23 06:51:53 | INFO | fairseq.trainer | begin training epoch 11
2022-03-23 06:51:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:52:52 | INFO | train_inner | epoch 011:     74 / 103 loss=8.947, ppl=493.45, wps=79724.1, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1100, lr=0.000137573, gnorm=0.624, loss_scale=32, train_wall=75, gb_free=21.6, wall=909
2022-03-23 06:53:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:53:16 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.679 | ppl 409.78 | wps 166441 | wpb 2040.3 | bsz 4 | num_updates 1129 | best_loss 8.679
2022-03-23 06:53:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1129 updates
2022-03-23 06:53:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:53:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 11 @ 1129 updates, score 8.679) (writing took 0.8753411610377952 seconds)
2022-03-23 06:53:17 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-23 06:53:17 | INFO | train | epoch 011 | loss 8.898 | ppl 477.06 | wps 79697.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1129 | lr 0.000141197 | gnorm 0.626 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 934
2022-03-23 06:53:17 | INFO | fairseq.trainer | begin training epoch 12
2022-03-23 06:53:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:54:14 | INFO | train_inner | epoch 012:     71 / 103 loss=8.771, ppl=436.98, wps=79542.1, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1200, lr=0.00015007, gnorm=0.611, loss_scale=32, train_wall=75, gb_free=21.6, wall=991
2022-03-23 06:54:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:54:41 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.528 | ppl 369.26 | wps 165876 | wpb 2040.3 | bsz 4 | num_updates 1232 | best_loss 8.528
2022-03-23 06:54:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1232 updates
2022-03-23 06:54:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:54:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 12 @ 1232 updates, score 8.528) (writing took 0.8703083210275508 seconds)
2022-03-23 06:54:42 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-23 06:54:42 | INFO | train | epoch 012 | loss 8.721 | ppl 422.01 | wps 79615.2 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1232 | lr 0.000154069 | gnorm 0.615 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1018
2022-03-23 06:54:42 | INFO | fairseq.trainer | begin training epoch 13
2022-03-23 06:54:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:55:36 | INFO | train_inner | epoch 013:     68 / 103 loss=8.606, ppl=389.56, wps=79666.2, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1300, lr=0.000162568, gnorm=0.593, loss_scale=32, train_wall=75, gb_free=21.6, wall=1073
2022-03-23 06:56:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:56:05 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.399 | ppl 337.66 | wps 167032 | wpb 2040.3 | bsz 4 | num_updates 1335 | best_loss 8.399
2022-03-23 06:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1335 updates
2022-03-23 06:56:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:56:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:56:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 13 @ 1335 updates, score 8.399) (writing took 0.8529628660180606 seconds)
2022-03-23 06:56:06 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-23 06:56:06 | INFO | train | epoch 013 | loss 8.562 | ppl 378 | wps 79795.3 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1335 | lr 0.000166942 | gnorm 0.576 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1103
2022-03-23 06:56:06 | INFO | fairseq.trainer | begin training epoch 14
2022-03-23 06:56:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:56:58 | INFO | train_inner | epoch 014:     65 / 103 loss=8.469, ppl=354.35, wps=79716.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1400, lr=0.000175065, gnorm=0.598, loss_scale=32, train_wall=75, gb_free=21.6, wall=1155
2022-03-23 06:57:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:57:29 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.271 | ppl 308.91 | wps 166547 | wpb 2040.3 | bsz 4 | num_updates 1438 | best_loss 8.271
2022-03-23 06:57:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1438 updates
2022-03-23 06:57:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:57:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:57:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 14 @ 1438 updates, score 8.271) (writing took 0.8896568839554675 seconds)
2022-03-23 06:57:30 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-23 06:57:30 | INFO | train | epoch 014 | loss 8.418 | ppl 341.92 | wps 79761.6 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1438 | lr 0.000179814 | gnorm 0.62 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1187
2022-03-23 06:57:30 | INFO | fairseq.trainer | begin training epoch 15
2022-03-23 06:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:58:20 | INFO | train_inner | epoch 015:     62 / 103 loss=8.334, ppl=322.77, wps=79665.8, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1500, lr=0.000187563, gnorm=0.627, loss_scale=32, train_wall=75, gb_free=21.6, wall=1237
2022-03-23 06:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:58:54 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.17 | ppl 287.96 | wps 166974 | wpb 2040.3 | bsz 4 | num_updates 1541 | best_loss 8.17
2022-03-23 06:58:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1541 updates
2022-03-23 06:58:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 06:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 15 @ 1541 updates, score 8.17) (writing took 0.8533561039948836 seconds)
2022-03-23 06:58:55 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-23 06:58:55 | INFO | train | epoch 015 | loss 8.285 | ppl 311.83 | wps 79750.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1541 | lr 0.000192686 | gnorm 0.632 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 1271
2022-03-23 06:58:55 | INFO | fairseq.trainer | begin training epoch 16
2022-03-23 06:58:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:58:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 06:59:43 | INFO | train_inner | epoch 016:     60 / 103 loss=8.206, ppl=295.35, wps=78925.7, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=1600, lr=0.00020006, gnorm=0.626, loss_scale=32, train_wall=75, gb_free=21.6, wall=1319
2022-03-23 07:00:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:00:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.065 | ppl 267.73 | wps 166649 | wpb 2040.3 | bsz 4 | num_updates 1643 | best_loss 8.065
2022-03-23 07:00:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1643 updates
2022-03-23 07:00:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 16 @ 1643 updates, score 8.065) (writing took 0.9820935470052063 seconds)
2022-03-23 07:00:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-23 07:00:19 | INFO | train | epoch 016 | loss 8.157 | ppl 285.47 | wps 78897.3 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 1643 | lr 0.000205434 | gnorm 0.616 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1356
2022-03-23 07:00:19 | INFO | fairseq.trainer | begin training epoch 17
2022-03-23 07:00:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:01:05 | INFO | train_inner | epoch 017:     57 / 103 loss=8.088, ppl=272.08, wps=79614.8, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=1700, lr=0.000212558, gnorm=0.608, loss_scale=32, train_wall=75, gb_free=21.6, wall=1401
2022-03-23 07:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:01:42 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.964 | ppl 249.75 | wps 167332 | wpb 2040.3 | bsz 4 | num_updates 1746 | best_loss 7.964
2022-03-23 07:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1746 updates
2022-03-23 07:01:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:01:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:01:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 17 @ 1746 updates, score 7.964) (writing took 0.91143362398725 seconds)
2022-03-23 07:01:43 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-23 07:01:43 | INFO | train | epoch 017 | loss 8.036 | ppl 262.44 | wps 79716.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1746 | lr 0.000218306 | gnorm 0.618 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1440
2022-03-23 07:01:43 | INFO | fairseq.trainer | begin training epoch 18
2022-03-23 07:01:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:02:27 | INFO | train_inner | epoch 018:     54 / 103 loss=7.974, ppl=251.48, wps=79632.8, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1800, lr=0.000225055, gnorm=0.601, loss_scale=32, train_wall=75, gb_free=21.6, wall=1483
2022-03-23 07:03:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:03:07 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.871 | ppl 234.11 | wps 169302 | wpb 2040.3 | bsz 4 | num_updates 1849 | best_loss 7.871
2022-03-23 07:03:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1849 updates
2022-03-23 07:03:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:03:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:03:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 18 @ 1849 updates, score 7.871) (writing took 0.859491879993584 seconds)
2022-03-23 07:03:08 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-23 07:03:08 | INFO | train | epoch 018 | loss 7.918 | ppl 241.88 | wps 79760.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1849 | lr 0.000231179 | gnorm 0.609 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1525
2022-03-23 07:03:08 | INFO | fairseq.trainer | begin training epoch 19
2022-03-23 07:03:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:03:49 | INFO | train_inner | epoch 019:     51 / 103 loss=7.86, ppl=232.26, wps=79655.7, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=1900, lr=0.000237553, gnorm=0.627, loss_scale=32, train_wall=75, gb_free=21.6, wall=1565
2022-03-23 07:04:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:04:31 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.764 | ppl 217.34 | wps 169307 | wpb 2040.3 | bsz 4 | num_updates 1952 | best_loss 7.764
2022-03-23 07:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1952 updates
2022-03-23 07:04:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:04:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:04:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 19 @ 1952 updates, score 7.764) (writing took 0.8965228270390071 seconds)
2022-03-23 07:04:32 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-23 07:04:32 | INFO | train | epoch 019 | loss 7.801 | ppl 223.07 | wps 79723.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1952 | lr 0.000244051 | gnorm 0.616 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1609
2022-03-23 07:04:32 | INFO | fairseq.trainer | begin training epoch 20
2022-03-23 07:04:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:05:10 | INFO | train_inner | epoch 020:     48 / 103 loss=7.74, ppl=213.85, wps=79746.6, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2000, lr=0.00025005, gnorm=0.607, loss_scale=32, train_wall=75, gb_free=21.6, wall=1647
2022-03-23 07:05:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:05:56 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.683 | ppl 205.47 | wps 170732 | wpb 2040.3 | bsz 4 | num_updates 2055 | best_loss 7.683
2022-03-23 07:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2055 updates
2022-03-23 07:05:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:05:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:05:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 20 @ 2055 updates, score 7.683) (writing took 0.8738887159852311 seconds)
2022-03-23 07:05:56 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-23 07:05:56 | INFO | train | epoch 020 | loss 7.687 | ppl 206.05 | wps 79813.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2055 | lr 0.000256924 | gnorm 0.606 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1693
2022-03-23 07:05:56 | INFO | fairseq.trainer | begin training epoch 21
2022-03-23 07:05:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:06:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:06:33 | INFO | train_inner | epoch 021:     46 / 103 loss=7.639, ppl=199.34, wps=78860.2, ups=1.21, wpb=65310.7, bsz=127.6, num_updates=2100, lr=0.000262548, gnorm=0.603, loss_scale=32, train_wall=75, gb_free=21.6, wall=1730
2022-03-23 07:07:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:07:20 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.598 | ppl 193.68 | wps 170484 | wpb 2040.3 | bsz 4 | num_updates 2157 | best_loss 7.598
2022-03-23 07:07:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2157 updates
2022-03-23 07:07:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:07:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 21 @ 2157 updates, score 7.598) (writing took 0.8845947820227593 seconds)
2022-03-23 07:07:21 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-23 07:07:21 | INFO | train | epoch 021 | loss 7.575 | ppl 190.71 | wps 78909.4 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 2157 | lr 0.000269671 | gnorm 0.6 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1778
2022-03-23 07:07:21 | INFO | fairseq.trainer | begin training epoch 22
2022-03-23 07:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:07:55 | INFO | train_inner | epoch 022:     43 / 103 loss=7.525, ppl=184.14, wps=79718.4, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2200, lr=0.000275045, gnorm=0.608, loss_scale=32, train_wall=75, gb_free=21.6, wall=1812
2022-03-23 07:08:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:08:44 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.527 | ppl 184.49 | wps 176040 | wpb 2040.3 | bsz 4 | num_updates 2260 | best_loss 7.527
2022-03-23 07:08:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2260 updates
2022-03-23 07:08:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:08:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 22 @ 2260 updates, score 7.527) (writing took 0.8411300329607911 seconds)
2022-03-23 07:08:45 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-23 07:08:45 | INFO | train | epoch 022 | loss 7.468 | ppl 176.99 | wps 79857.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2260 | lr 0.000282544 | gnorm 0.612 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1862
2022-03-23 07:08:45 | INFO | fairseq.trainer | begin training epoch 23
2022-03-23 07:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:09:17 | INFO | train_inner | epoch 023:     40 / 103 loss=7.424, ppl=171.74, wps=79781.5, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=2300, lr=0.000287543, gnorm=0.599, loss_scale=32, train_wall=75, gb_free=21.6, wall=1894
2022-03-23 07:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:10:08 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.448 | ppl 174.64 | wps 173064 | wpb 2040.3 | bsz 4 | num_updates 2363 | best_loss 7.448
2022-03-23 07:10:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2363 updates
2022-03-23 07:10:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 23 @ 2363 updates, score 7.448) (writing took 0.8515209099859931 seconds)
2022-03-23 07:10:09 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-23 07:10:09 | INFO | train | epoch 023 | loss 7.36 | ppl 164.26 | wps 79876.6 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2363 | lr 0.000295416 | gnorm 0.592 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1946
2022-03-23 07:10:09 | INFO | fairseq.trainer | begin training epoch 24
2022-03-23 07:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:10:39 | INFO | train_inner | epoch 024:     37 / 103 loss=7.32, ppl=159.79, wps=79782.1, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2400, lr=0.00030004, gnorm=0.607, loss_scale=32, train_wall=75, gb_free=21.6, wall=1976
2022-03-23 07:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:11:33 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.377 | ppl 166.17 | wps 172237 | wpb 2040.3 | bsz 4 | num_updates 2466 | best_loss 7.377
2022-03-23 07:11:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2466 updates
2022-03-23 07:11:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:11:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 24 @ 2466 updates, score 7.377) (writing took 0.8513336150208488 seconds)
2022-03-23 07:11:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-23 07:11:34 | INFO | train | epoch 024 | loss 7.256 | ppl 152.85 | wps 79763.5 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2466 | lr 0.000308288 | gnorm 0.596 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2030
2022-03-23 07:11:34 | INFO | fairseq.trainer | begin training epoch 25
2022-03-23 07:11:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:12:01 | INFO | train_inner | epoch 025:     34 / 103 loss=7.219, ppl=149.01, wps=79724.2, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=2500, lr=0.000312538, gnorm=0.587, loss_scale=32, train_wall=75, gb_free=21.6, wall=2058
2022-03-23 07:12:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:12:57 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.295 | ppl 157.08 | wps 172872 | wpb 2040.3 | bsz 4 | num_updates 2569 | best_loss 7.295
2022-03-23 07:12:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2569 updates
2022-03-23 07:12:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:12:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:12:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 25 @ 2569 updates, score 7.295) (writing took 0.8702783170156181 seconds)
2022-03-23 07:12:58 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-23 07:12:58 | INFO | train | epoch 025 | loss 7.155 | ppl 142.56 | wps 79800.3 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2569 | lr 0.000321161 | gnorm 0.597 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2115
2022-03-23 07:12:58 | INFO | fairseq.trainer | begin training epoch 26
2022-03-23 07:12:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:13:23 | INFO | train_inner | epoch 026:     31 / 103 loss=7.123, ppl=139.42, wps=79712.7, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=2600, lr=0.000325035, gnorm=0.596, loss_scale=64, train_wall=75, gb_free=21.6, wall=2139
2022-03-23 07:13:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:14:21 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.231 | ppl 150.23 | wps 171163 | wpb 2040.3 | bsz 4 | num_updates 2671 | best_loss 7.231
2022-03-23 07:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2671 updates
2022-03-23 07:14:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:14:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:14:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 26 @ 2671 updates, score 7.231) (writing took 0.8956892190035433 seconds)
2022-03-23 07:14:22 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-23 07:14:22 | INFO | train | epoch 026 | loss 7.057 | ppl 133.2 | wps 78969.1 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 2671 | lr 0.000333908 | gnorm 0.593 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2199
2022-03-23 07:14:22 | INFO | fairseq.trainer | begin training epoch 27
2022-03-23 07:14:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:14:45 | INFO | train_inner | epoch 027:     29 / 103 loss=7.034, ppl=131.03, wps=78918.1, ups=1.21, wpb=65310.7, bsz=127.6, num_updates=2700, lr=0.000337533, gnorm=0.585, loss_scale=32, train_wall=75, gb_free=21.6, wall=2222
2022-03-23 07:15:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:15:46 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.18 | ppl 145.01 | wps 170856 | wpb 2040.3 | bsz 4 | num_updates 2774 | best_loss 7.18
2022-03-23 07:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2774 updates
2022-03-23 07:15:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:15:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 27 @ 2774 updates, score 7.18) (writing took 0.8641124030109495 seconds)
2022-03-23 07:15:47 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-23 07:15:47 | INFO | train | epoch 027 | loss 6.962 | ppl 124.7 | wps 79780.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2774 | lr 0.000346781 | gnorm 0.586 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2283
2022-03-23 07:15:47 | INFO | fairseq.trainer | begin training epoch 28
2022-03-23 07:15:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:16:07 | INFO | train_inner | epoch 028:     26 / 103 loss=6.933, ppl=122.16, wps=79675.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2800, lr=0.00035003, gnorm=0.594, loss_scale=32, train_wall=75, gb_free=21.6, wall=2304
2022-03-23 07:17:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:17:10 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.122 | ppl 139.3 | wps 171181 | wpb 2040.3 | bsz 4 | num_updates 2877 | best_loss 7.122
2022-03-23 07:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2877 updates
2022-03-23 07:17:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:17:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 28 @ 2877 updates, score 7.122) (writing took 0.858177924004849 seconds)
2022-03-23 07:17:11 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-23 07:17:11 | INFO | train | epoch 028 | loss 6.871 | ppl 117.06 | wps 79727.2 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2877 | lr 0.000359653 | gnorm 0.581 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2368
2022-03-23 07:17:11 | INFO | fairseq.trainer | begin training epoch 29
2022-03-23 07:17:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:17:29 | INFO | train_inner | epoch 029:     23 / 103 loss=6.85, ppl=115.35, wps=79683.8, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2900, lr=0.000362528, gnorm=0.583, loss_scale=32, train_wall=75, gb_free=21.6, wall=2386
2022-03-23 07:18:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:18:34 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.066 | ppl 134.03 | wps 171746 | wpb 2040.3 | bsz 4 | num_updates 2980 | best_loss 7.066
2022-03-23 07:18:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2980 updates
2022-03-23 07:18:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:18:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:18:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 29 @ 2980 updates, score 7.066) (writing took 0.9492188299773261 seconds)
2022-03-23 07:18:35 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-23 07:18:35 | INFO | train | epoch 029 | loss 6.783 | ppl 110.11 | wps 79705.8 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2980 | lr 0.000372526 | gnorm 0.578 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2452
2022-03-23 07:18:35 | INFO | fairseq.trainer | begin training epoch 30
2022-03-23 07:18:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:18:51 | INFO | train_inner | epoch 030:     20 / 103 loss=6.766, ppl=108.82, wps=79643.7, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3000, lr=0.000375025, gnorm=0.578, loss_scale=32, train_wall=75, gb_free=21.6, wall=2468
2022-03-23 07:19:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:19:59 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.021 | ppl 129.84 | wps 169443 | wpb 2040.3 | bsz 4 | num_updates 3083 | best_loss 7.021
2022-03-23 07:19:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 3083 updates
2022-03-23 07:19:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:20:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:20:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 30 @ 3083 updates, score 7.021) (writing took 0.8954911439795978 seconds)
2022-03-23 07:20:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-23 07:20:00 | INFO | train | epoch 030 | loss 6.699 | ppl 103.87 | wps 79724.8 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3083 | lr 0.000385398 | gnorm 0.567 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2537
2022-03-23 07:20:00 | INFO | fairseq.trainer | begin training epoch 31
2022-03-23 07:20:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:20:13 | INFO | train_inner | epoch 031:     17 / 103 loss=6.687, ppl=103.07, wps=79648.5, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3100, lr=0.000387523, gnorm=0.563, loss_scale=32, train_wall=75, gb_free=21.6, wall=2550
2022-03-23 07:21:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:21:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:21:23 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.976 | ppl 125.91 | wps 171261 | wpb 2040.3 | bsz 4 | num_updates 3185 | best_loss 6.976
2022-03-23 07:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3185 updates
2022-03-23 07:21:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:21:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:21:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 31 @ 3185 updates, score 6.976) (writing took 0.9297282610205002 seconds)
2022-03-23 07:21:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-23 07:21:24 | INFO | train | epoch 031 | loss 6.62 | ppl 98.36 | wps 78956.2 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 3185 | lr 0.000398145 | gnorm 0.57 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2621
2022-03-23 07:21:24 | INFO | fairseq.trainer | begin training epoch 32
2022-03-23 07:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:21:36 | INFO | train_inner | epoch 032:     15 / 103 loss=6.606, ppl=97.39, wps=78920.3, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=3200, lr=0.00040002, gnorm=0.568, loss_scale=32, train_wall=75, gb_free=21.6, wall=2633
2022-03-23 07:22:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:22:47 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.944 | ppl 123.12 | wps 171328 | wpb 2040.3 | bsz 4 | num_updates 3288 | best_loss 6.944
2022-03-23 07:22:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3288 updates
2022-03-23 07:22:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:22:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:22:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 32 @ 3288 updates, score 6.944) (writing took 0.9066742190043442 seconds)
2022-03-23 07:22:48 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-23 07:22:48 | INFO | train | epoch 032 | loss 6.544 | ppl 93.32 | wps 79856.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3288 | lr 0.000411018 | gnorm 0.557 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2705
2022-03-23 07:22:48 | INFO | fairseq.trainer | begin training epoch 33
2022-03-23 07:22:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:22:58 | INFO | train_inner | epoch 033:     12 / 103 loss=6.538, ppl=92.92, wps=79769, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3300, lr=0.000412518, gnorm=0.556, loss_scale=32, train_wall=75, gb_free=21.6, wall=2715
2022-03-23 07:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:24:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.918 | ppl 120.92 | wps 171544 | wpb 2040.3 | bsz 4 | num_updates 3391 | best_loss 6.918
2022-03-23 07:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3391 updates
2022-03-23 07:24:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:24:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 33 @ 3391 updates, score 6.918) (writing took 0.8593802620307542 seconds)
2022-03-23 07:24:13 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-23 07:24:13 | INFO | train | epoch 033 | loss 6.472 | ppl 88.76 | wps 79853.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3391 | lr 0.00042389 | gnorm 0.551 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2789
2022-03-23 07:24:13 | INFO | fairseq.trainer | begin training epoch 34
2022-03-23 07:24:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:24:20 | INFO | train_inner | epoch 034:      9 / 103 loss=6.469, ppl=88.57, wps=79804.3, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3400, lr=0.000425015, gnorm=0.557, loss_scale=32, train_wall=75, gb_free=21.6, wall=2797
2022-03-23 07:25:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:25:36 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.867 | ppl 116.73 | wps 171710 | wpb 2040.3 | bsz 4 | num_updates 3494 | best_loss 6.867
2022-03-23 07:25:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3494 updates
2022-03-23 07:25:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:25:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:25:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 34 @ 3494 updates, score 6.867) (writing took 0.9222055579884909 seconds)
2022-03-23 07:25:37 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-23 07:25:37 | INFO | train | epoch 034 | loss 6.404 | ppl 84.67 | wps 79786.2 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3494 | lr 0.000436763 | gnorm 0.56 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2874
2022-03-23 07:25:37 | INFO | fairseq.trainer | begin training epoch 35
2022-03-23 07:25:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:25:42 | INFO | train_inner | epoch 035:      6 / 103 loss=6.397, ppl=84.26, wps=79714.7, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3500, lr=0.000437513, gnorm=0.555, loss_scale=32, train_wall=75, gb_free=21.6, wall=2879
2022-03-23 07:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:27:00 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.837 | ppl 114.31 | wps 171290 | wpb 2040.3 | bsz 4 | num_updates 3597 | best_loss 6.837
2022-03-23 07:27:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3597 updates
2022-03-23 07:27:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:27:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 35 @ 3597 updates, score 6.837) (writing took 0.8642082870355807 seconds)
2022-03-23 07:27:01 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-23 07:27:01 | INFO | train | epoch 035 | loss 6.337 | ppl 80.84 | wps 79826.6 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3597 | lr 0.000449635 | gnorm 0.559 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2958
2022-03-23 07:27:01 | INFO | fairseq.trainer | begin training epoch 36
2022-03-23 07:27:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:27:04 | INFO | train_inner | epoch 036:      3 / 103 loss=6.337, ppl=80.84, wps=79749.3, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3600, lr=0.00045001, gnorm=0.558, loss_scale=32, train_wall=75, gb_free=21.6, wall=2960
2022-03-23 07:28:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:28:24 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.812 | ppl 112.32 | wps 171546 | wpb 2040.3 | bsz 4 | num_updates 3699 | best_loss 6.812
2022-03-23 07:28:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3699 updates
2022-03-23 07:28:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:28:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:28:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 36 @ 3699 updates, score 6.812) (writing took 0.8730305539793335 seconds)
2022-03-23 07:28:25 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-23 07:28:25 | INFO | train | epoch 036 | loss 6.274 | ppl 77.38 | wps 79138.2 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 3699 | lr 0.000462383 | gnorm 0.552 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3042
2022-03-23 07:28:25 | INFO | fairseq.trainer | begin training epoch 37
2022-03-23 07:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:28:26 | INFO | train_inner | epoch 037:      1 / 103 loss=6.273, ppl=77.34, wps=79071.5, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=3700, lr=0.000462508, gnorm=0.551, loss_scale=32, train_wall=75, gb_free=21.6, wall=3043
2022-03-23 07:29:46 | INFO | train_inner | epoch 037:    101 / 103 loss=6.216, ppl=74.32, wps=82130.9, ups=1.25, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.553, loss_scale=32, train_wall=75, gb_free=21.6, wall=3123
2022-03-23 07:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:29:49 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.79 | ppl 110.67 | wps 172343 | wpb 2040.3 | bsz 4 | num_updates 3802 | best_loss 6.79
2022-03-23 07:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3802 updates
2022-03-23 07:29:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:29:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 37 @ 3802 updates, score 6.79) (writing took 0.8648733479785733 seconds)
2022-03-23 07:29:50 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-23 07:29:50 | INFO | train | epoch 037 | loss 6.215 | ppl 74.3 | wps 79840.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3802 | lr 0.000475255 | gnorm 0.552 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3126
2022-03-23 07:29:50 | INFO | fairseq.trainer | begin training epoch 38
2022-03-23 07:29:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:31:08 | INFO | train_inner | epoch 038:     98 / 103 loss=6.157, ppl=71.33, wps=79717, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3900, lr=0.000487503, gnorm=0.543, loss_scale=32, train_wall=75, gb_free=21.6, wall=3205
2022-03-23 07:31:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:31:13 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.754 | ppl 107.94 | wps 171493 | wpb 2040.3 | bsz 4 | num_updates 3905 | best_loss 6.754
2022-03-23 07:31:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3905 updates
2022-03-23 07:31:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:31:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 38 @ 3905 updates, score 6.754) (writing took 0.8929699229774997 seconds)
2022-03-23 07:31:14 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-23 07:31:14 | INFO | train | epoch 038 | loss 6.158 | ppl 71.4 | wps 79735.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3905 | lr 0.000488127 | gnorm 0.543 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3211
2022-03-23 07:31:14 | INFO | fairseq.trainer | begin training epoch 39
2022-03-23 07:31:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:32:30 | INFO | train_inner | epoch 039:     95 / 103 loss=6.102, ppl=68.69, wps=79752.8, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4000, lr=0.0005, gnorm=0.546, loss_scale=32, train_wall=75, gb_free=21.6, wall=3287
2022-03-23 07:32:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:32:37 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.727 | ppl 105.93 | wps 170470 | wpb 2040.3 | bsz 4 | num_updates 4008 | best_loss 6.727
2022-03-23 07:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 4008 updates
2022-03-23 07:32:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:32:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:32:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 39 @ 4008 updates, score 6.727) (writing took 0.854822357010562 seconds)
2022-03-23 07:32:38 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-23 07:32:38 | INFO | train | epoch 039 | loss 6.102 | ppl 68.67 | wps 79804.3 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4008 | lr 0.000499501 | gnorm 0.544 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3295
2022-03-23 07:32:38 | INFO | fairseq.trainer | begin training epoch 40
2022-03-23 07:32:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:33:52 | INFO | train_inner | epoch 040:     92 / 103 loss=6.053, ppl=66.39, wps=79706.7, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4100, lr=0.000493865, gnorm=0.527, loss_scale=32, train_wall=75, gb_free=21.6, wall=3369
2022-03-23 07:34:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:34:02 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.706 | ppl 104.38 | wps 171392 | wpb 2040.3 | bsz 4 | num_updates 4111 | best_loss 6.706
2022-03-23 07:34:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 4111 updates
2022-03-23 07:34:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:34:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 40 @ 4111 updates, score 6.706) (writing took 0.8520833470392972 seconds)
2022-03-23 07:34:03 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-23 07:34:03 | INFO | train | epoch 040 | loss 6.047 | ppl 66.11 | wps 79806.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4111 | lr 0.000493204 | gnorm 0.526 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3379
2022-03-23 07:34:03 | INFO | fairseq.trainer | begin training epoch 41
2022-03-23 07:34:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:35:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:35:14 | INFO | train_inner | epoch 041:     90 / 103 loss=5.993, ppl=63.68, wps=78989.2, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=4200, lr=0.00048795, gnorm=0.533, loss_scale=32, train_wall=75, gb_free=21.6, wall=3451
2022-03-23 07:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:35:26 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.696 | ppl 103.68 | wps 171050 | wpb 2040.3 | bsz 4 | num_updates 4213 | best_loss 6.696
2022-03-23 07:35:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 4213 updates
2022-03-23 07:35:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:35:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:35:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 41 @ 4213 updates, score 6.696) (writing took 0.8736292329849675 seconds)
2022-03-23 07:35:27 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-23 07:35:27 | INFO | train | epoch 041 | loss 5.991 | ppl 63.61 | wps 79013.1 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 4213 | lr 0.000487197 | gnorm 0.539 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3464
2022-03-23 07:35:27 | INFO | fairseq.trainer | begin training epoch 42
2022-03-23 07:35:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:36:36 | INFO | train_inner | epoch 042:     87 / 103 loss=5.947, ppl=61.7, wps=79708.6, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=4300, lr=0.000482243, gnorm=0.522, loss_scale=32, train_wall=75, gb_free=21.6, wall=3533
2022-03-23 07:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:36:50 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.67 | ppl 101.8 | wps 171764 | wpb 2040.3 | bsz 4 | num_updates 4316 | best_loss 6.67
2022-03-23 07:36:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4316 updates
2022-03-23 07:36:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 42 @ 4316 updates, score 6.67) (writing took 0.8823794519994408 seconds)
2022-03-23 07:36:51 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-23 07:36:51 | INFO | train | epoch 042 | loss 5.939 | ppl 61.34 | wps 79767.6 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4316 | lr 0.000481348 | gnorm 0.52 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3548
2022-03-23 07:36:51 | INFO | fairseq.trainer | begin training epoch 43
2022-03-23 07:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:37:58 | INFO | train_inner | epoch 043:     84 / 103 loss=5.899, ppl=59.67, wps=79731.6, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=4400, lr=0.000476731, gnorm=0.529, loss_scale=32, train_wall=75, gb_free=21.6, wall=3615
2022-03-23 07:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:38:15 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.656 | ppl 100.86 | wps 171360 | wpb 2040.3 | bsz 4 | num_updates 4419 | best_loss 6.656
2022-03-23 07:38:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4419 updates
2022-03-23 07:38:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:38:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:38:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 43 @ 4419 updates, score 6.656) (writing took 0.8534371120040305 seconds)
2022-03-23 07:38:15 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-23 07:38:15 | INFO | train | epoch 043 | loss 5.891 | ppl 59.32 | wps 79837.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4419 | lr 0.000475705 | gnorm 0.529 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3632
2022-03-23 07:38:15 | INFO | fairseq.trainer | begin training epoch 44
2022-03-23 07:38:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:39:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 07:39:21 | INFO | train_inner | epoch 044:     82 / 103 loss=5.851, ppl=57.73, wps=79001.2, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=4500, lr=0.000471405, gnorm=0.528, loss_scale=16, train_wall=75, gb_free=21.6, wall=3698
2022-03-23 07:39:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:39:39 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.631 | ppl 99.14 | wps 172066 | wpb 2040.3 | bsz 4 | num_updates 4521 | best_loss 6.631
2022-03-23 07:39:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4521 updates
2022-03-23 07:39:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:39:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:39:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 44 @ 4521 updates, score 6.631) (writing took 0.9104410749860108 seconds)
2022-03-23 07:39:40 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-23 07:39:40 | INFO | train | epoch 044 | loss 5.842 | ppl 57.36 | wps 79048.8 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 4521 | lr 0.000470308 | gnorm 0.522 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 3717
2022-03-23 07:39:40 | INFO | fairseq.trainer | begin training epoch 45
2022-03-23 07:39:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:40:43 | INFO | train_inner | epoch 045:     79 / 103 loss=5.805, ppl=55.91, wps=79816.1, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4600, lr=0.000466252, gnorm=0.516, loss_scale=16, train_wall=75, gb_free=21.6, wall=3780
2022-03-23 07:41:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:41:03 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.624 | ppl 98.63 | wps 172517 | wpb 2040.3 | bsz 4 | num_updates 4624 | best_loss 6.624
2022-03-23 07:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4624 updates
2022-03-23 07:41:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:41:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:41:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 45 @ 4624 updates, score 6.624) (writing took 0.8609720370150171 seconds)
2022-03-23 07:41:04 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-23 07:41:04 | INFO | train | epoch 045 | loss 5.799 | ppl 55.67 | wps 79889.3 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4624 | lr 0.000465041 | gnorm 0.522 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 3801
2022-03-23 07:41:04 | INFO | fairseq.trainer | begin training epoch 46
2022-03-23 07:41:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:42:05 | INFO | train_inner | epoch 046:     76 / 103 loss=5.762, ppl=54.27, wps=79845.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4700, lr=0.000461266, gnorm=0.514, loss_scale=16, train_wall=75, gb_free=21.6, wall=3861
2022-03-23 07:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:42:27 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.612 | ppl 97.83 | wps 171523 | wpb 2040.3 | bsz 4 | num_updates 4727 | best_loss 6.612
2022-03-23 07:42:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4727 updates
2022-03-23 07:42:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:42:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:42:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 46 @ 4727 updates, score 6.612) (writing took 0.8551962749916129 seconds)
2022-03-23 07:42:28 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-23 07:42:28 | INFO | train | epoch 046 | loss 5.755 | ppl 53.99 | wps 79935.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4727 | lr 0.000459946 | gnorm 0.511 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 3885
2022-03-23 07:42:28 | INFO | fairseq.trainer | begin training epoch 47
2022-03-23 07:42:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:43:26 | INFO | train_inner | epoch 047:     73 / 103 loss=5.726, ppl=52.93, wps=79821.5, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4800, lr=0.000456435, gnorm=0.512, loss_scale=16, train_wall=75, gb_free=21.6, wall=3943
2022-03-23 07:43:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:43:52 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.59 | ppl 96.32 | wps 171502 | wpb 2040.3 | bsz 4 | num_updates 4830 | best_loss 6.59
2022-03-23 07:43:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4830 updates
2022-03-23 07:43:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:43:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:43:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 47 @ 4830 updates, score 6.59) (writing took 0.8842659929650836 seconds)
2022-03-23 07:43:52 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-23 07:43:52 | INFO | train | epoch 047 | loss 5.713 | ppl 52.47 | wps 79812.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4830 | lr 0.000455016 | gnorm 0.513 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 3969
2022-03-23 07:43:52 | INFO | fairseq.trainer | begin training epoch 48
2022-03-23 07:43:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:44:48 | INFO | train_inner | epoch 048:     70 / 103 loss=5.691, ppl=51.67, wps=79764.6, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=4900, lr=0.000451754, gnorm=0.512, loss_scale=16, train_wall=75, gb_free=21.6, wall=4025
2022-03-23 07:45:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:45:16 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.587 | ppl 96.12 | wps 171975 | wpb 2040.3 | bsz 4 | num_updates 4933 | best_loss 6.587
2022-03-23 07:45:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4933 updates
2022-03-23 07:45:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:45:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:45:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 48 @ 4933 updates, score 6.587) (writing took 0.9381507399957627 seconds)
2022-03-23 07:45:17 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-23 07:45:17 | INFO | train | epoch 048 | loss 5.677 | ppl 51.17 | wps 79837.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4933 | lr 0.00045024 | gnorm 0.515 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 4053
2022-03-23 07:45:17 | INFO | fairseq.trainer | begin training epoch 49
2022-03-23 07:45:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:46:10 | INFO | train_inner | epoch 049:     67 / 103 loss=5.645, ppl=50.05, wps=79724.2, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=5000, lr=0.000447214, gnorm=0.523, loss_scale=16, train_wall=75, gb_free=21.6, wall=4107
2022-03-23 07:46:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:46:40 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.583 | ppl 95.86 | wps 171800 | wpb 2040.3 | bsz 4 | num_updates 5036 | best_loss 6.583
2022-03-23 07:46:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 5036 updates
2022-03-23 07:46:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:46:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 49 @ 5036 updates, score 6.583) (writing took 0.8791482839733362 seconds)
2022-03-23 07:46:41 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-23 07:46:41 | INFO | train | epoch 049 | loss 5.638 | ppl 49.81 | wps 79823.8 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5036 | lr 0.000445612 | gnorm 0.518 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4138
2022-03-23 07:46:41 | INFO | fairseq.trainer | begin training epoch 50
2022-03-23 07:46:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:47:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 07:47:33 | INFO | train_inner | epoch 050:     65 / 103 loss=5.615, ppl=49, wps=79025.3, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=5100, lr=0.000442807, gnorm=0.518, loss_scale=16, train_wall=75, gb_free=21.6, wall=4190
2022-03-23 07:48:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:48:04 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.57 | ppl 94.98 | wps 170542 | wpb 2040.3 | bsz 4 | num_updates 5138 | best_loss 6.57
2022-03-23 07:48:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 5138 updates
2022-03-23 07:48:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:48:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:48:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 50 @ 5138 updates, score 6.57) (writing took 0.8831477250205353 seconds)
2022-03-23 07:48:05 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-23 07:48:05 | INFO | train | epoch 050 | loss 5.603 | ppl 48.62 | wps 79101.5 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 5138 | lr 0.000441167 | gnorm 0.517 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 4222
2022-03-23 07:48:05 | INFO | fairseq.trainer | begin training epoch 51
2022-03-23 07:48:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:48:55 | INFO | train_inner | epoch 051:     62 / 103 loss=5.58, ppl=47.82, wps=79816.8, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=5200, lr=0.000438529, gnorm=0.517, loss_scale=16, train_wall=75, gb_free=21.6, wall=4271
2022-03-23 07:49:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:49:28 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.555 | ppl 94.06 | wps 171607 | wpb 2040.3 | bsz 4 | num_updates 5241 | best_loss 6.555
2022-03-23 07:49:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 5241 updates
2022-03-23 07:49:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:49:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:49:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 51 @ 5241 updates, score 6.555) (writing took 0.8704150110133924 seconds)
2022-03-23 07:49:29 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-23 07:49:29 | INFO | train | epoch 051 | loss 5.569 | ppl 47.46 | wps 79908.1 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5241 | lr 0.00043681 | gnorm 0.517 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 4306
2022-03-23 07:49:29 | INFO | fairseq.trainer | begin training epoch 52
2022-03-23 07:49:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:50:16 | INFO | train_inner | epoch 052:     59 / 103 loss=5.549, ppl=46.8, wps=79824.7, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=5300, lr=0.000434372, gnorm=0.517, loss_scale=16, train_wall=75, gb_free=21.6, wall=4353
2022-03-23 07:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:50:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.56 | ppl 94.35 | wps 171632 | wpb 2040.3 | bsz 4 | num_updates 5344 | best_loss 6.555
2022-03-23 07:50:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5344 updates
2022-03-23 07:50:53 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-23 07:50:53 | INFO | train | epoch 052 | loss 5.537 | ppl 46.44 | wps 80689.2 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 5344 | lr 0.00043258 | gnorm 0.521 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 4390
2022-03-23 07:50:53 | INFO | fairseq.trainer | begin training epoch 53
2022-03-23 07:50:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:51:37 | INFO | train_inner | epoch 053:     56 / 103 loss=5.522, ppl=45.94, wps=80649.2, ups=1.24, wpb=65300.5, bsz=127.6, num_updates=5400, lr=0.000430331, gnorm=0.519, loss_scale=16, train_wall=75, gb_free=21.6, wall=4434
2022-03-23 07:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:52:16 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.547 | ppl 93.49 | wps 171498 | wpb 2040.3 | bsz 4 | num_updates 5447 | best_loss 6.547
2022-03-23 07:52:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5447 updates
2022-03-23 07:52:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:52:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:52:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 53 @ 5447 updates, score 6.547) (writing took 0.8987287330091931 seconds)
2022-03-23 07:52:17 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-23 07:52:17 | INFO | train | epoch 053 | loss 5.505 | ppl 45.4 | wps 79852.5 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5447 | lr 0.000428471 | gnorm 0.522 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 4474
2022-03-23 07:52:17 | INFO | fairseq.trainer | begin training epoch 54
2022-03-23 07:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:52:59 | INFO | train_inner | epoch 054:     53 / 103 loss=5.485, ppl=44.79, wps=79760.1, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=5500, lr=0.000426401, gnorm=0.524, loss_scale=16, train_wall=75, gb_free=21.6, wall=4516
2022-03-23 07:53:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:53:40 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.541 | ppl 93.1 | wps 171075 | wpb 2040.3 | bsz 4 | num_updates 5550 | best_loss 6.541
2022-03-23 07:53:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5550 updates
2022-03-23 07:53:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:53:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 54 @ 5550 updates, score 6.541) (writing took 0.90250481601106 seconds)
2022-03-23 07:53:41 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-23 07:53:41 | INFO | train | epoch 054 | loss 5.475 | ppl 44.48 | wps 79825 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5550 | lr 0.000424476 | gnorm 0.522 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 4558
2022-03-23 07:53:41 | INFO | fairseq.trainer | begin training epoch 55
2022-03-23 07:53:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:54:21 | INFO | train_inner | epoch 055:     50 / 103 loss=5.464, ppl=44.13, wps=79770.5, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=5600, lr=0.000422577, gnorm=0.52, loss_scale=32, train_wall=75, gb_free=21.6, wall=4598
2022-03-23 07:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:55:05 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.538 | ppl 92.94 | wps 172896 | wpb 2040.3 | bsz 4 | num_updates 5653 | best_loss 6.538
2022-03-23 07:55:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5653 updates
2022-03-23 07:55:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:55:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:55:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 55 @ 5653 updates, score 6.538) (writing took 0.8647134689963423 seconds)
2022-03-23 07:55:05 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-23 07:55:05 | INFO | train | epoch 055 | loss 5.446 | ppl 43.59 | wps 79908.6 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5653 | lr 0.000420592 | gnorm 0.522 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4642
2022-03-23 07:55:05 | INFO | fairseq.trainer | begin training epoch 56
2022-03-23 07:55:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:55:43 | INFO | train_inner | epoch 056:     47 / 103 loss=5.435, ppl=43.27, wps=79823.6, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=5700, lr=0.000418854, gnorm=0.522, loss_scale=32, train_wall=75, gb_free=21.6, wall=4680
2022-03-23 07:56:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:56:29 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.535 | ppl 92.73 | wps 170951 | wpb 2040.3 | bsz 4 | num_updates 5756 | best_loss 6.535
2022-03-23 07:56:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5756 updates
2022-03-23 07:56:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:56:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:56:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 56 @ 5756 updates, score 6.535) (writing took 0.8822084169951268 seconds)
2022-03-23 07:56:30 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-23 07:56:30 | INFO | train | epoch 056 | loss 5.418 | ppl 42.76 | wps 79845.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5756 | lr 0.000416811 | gnorm 0.522 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4726
2022-03-23 07:56:30 | INFO | fairseq.trainer | begin training epoch 57
2022-03-23 07:56:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:57:05 | INFO | train_inner | epoch 057:     44 / 103 loss=5.399, ppl=42.2, wps=79807.4, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=5800, lr=0.000415227, gnorm=0.523, loss_scale=32, train_wall=75, gb_free=21.6, wall=4762
2022-03-23 07:57:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:57:53 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.541 | ppl 93.14 | wps 172087 | wpb 2040.3 | bsz 4 | num_updates 5859 | best_loss 6.535
2022-03-23 07:57:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5859 updates
2022-03-23 07:57:53 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-23 07:57:53 | INFO | train | epoch 057 | loss 5.392 | ppl 41.99 | wps 80714.7 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 5859 | lr 0.000413131 | gnorm 0.528 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4810
2022-03-23 07:57:53 | INFO | fairseq.trainer | begin training epoch 58
2022-03-23 07:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:58:26 | INFO | train_inner | epoch 058:     41 / 103 loss=5.384, ppl=41.76, wps=80661.6, ups=1.24, wpb=65305.6, bsz=127.6, num_updates=5900, lr=0.000411693, gnorm=0.523, loss_scale=32, train_wall=75, gb_free=21.6, wall=4843
2022-03-23 07:59:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:59:16 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.523 | ppl 91.95 | wps 171010 | wpb 2040.3 | bsz 4 | num_updates 5962 | best_loss 6.523
2022-03-23 07:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5962 updates
2022-03-23 07:59:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:59:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 07:59:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 58 @ 5962 updates, score 6.523) (writing took 0.8523991419933736 seconds)
2022-03-23 07:59:17 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-23 07:59:17 | INFO | train | epoch 058 | loss 5.363 | ppl 41.17 | wps 79887.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5962 | lr 0.000409547 | gnorm 0.518 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4894
2022-03-23 07:59:17 | INFO | fairseq.trainer | begin training epoch 59
2022-03-23 07:59:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:59:48 | INFO | train_inner | epoch 059:     38 / 103 loss=5.354, ppl=40.9, wps=79849.7, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=6000, lr=0.000408248, gnorm=0.526, loss_scale=32, train_wall=75, gb_free=21.6, wall=4924
2022-03-23 08:00:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:00:40 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.53 | ppl 92.41 | wps 172057 | wpb 2040.3 | bsz 4 | num_updates 6065 | best_loss 6.523
2022-03-23 08:00:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 6065 updates
2022-03-23 08:00:40 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-23 08:00:40 | INFO | train | epoch 059 | loss 5.34 | ppl 40.51 | wps 80800.9 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 6065 | lr 0.000406055 | gnorm 0.525 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4977
2022-03-23 08:00:41 | INFO | fairseq.trainer | begin training epoch 60
2022-03-23 08:00:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:01:08 | INFO | train_inner | epoch 060:     35 / 103 loss=5.335, ppl=40.35, wps=80754.2, ups=1.24, wpb=65300.5, bsz=127.6, num_updates=6100, lr=0.000404888, gnorm=0.527, loss_scale=32, train_wall=74, gb_free=21.6, wall=5005
2022-03-23 08:01:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 08:02:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:02:04 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.52 | ppl 91.79 | wps 173156 | wpb 2040.3 | bsz 4 | num_updates 6167 | best_loss 6.52
2022-03-23 08:02:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 6167 updates
2022-03-23 08:02:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:02:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:02:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 60 @ 6167 updates, score 6.52) (writing took 0.9374827099964023 seconds)
2022-03-23 08:02:05 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-23 08:02:05 | INFO | train | epoch 060 | loss 5.316 | ppl 39.83 | wps 78571.4 | ups 1.2 | wpb 65310.1 | bsz 127.6 | num_updates 6167 | lr 0.000402683 | gnorm 0.539 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 5062
2022-03-23 08:02:05 | INFO | fairseq.trainer | begin training epoch 61
2022-03-23 08:02:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:02:31 | INFO | train_inner | epoch 061:     33 / 103 loss=5.308, ppl=39.61, wps=78713.6, ups=1.21, wpb=65310.7, bsz=127.6, num_updates=6200, lr=0.00040161, gnorm=0.538, loss_scale=32, train_wall=76, gb_free=21.6, wall=5088
2022-03-23 08:02:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 08:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:03:28 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.523 | ppl 91.96 | wps 171871 | wpb 2040.3 | bsz 4 | num_updates 6269 | best_loss 6.52
2022-03-23 08:03:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 6269 updates
2022-03-23 08:03:28 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-23 08:03:28 | INFO | train | epoch 061 | loss 5.291 | ppl 39.15 | wps 80792.3 | ups 1.24 | wpb 65310.1 | bsz 127.6 | num_updates 6269 | lr 0.000399393 | gnorm 0.532 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 5145
2022-03-23 08:03:28 | INFO | fairseq.trainer | begin training epoch 62
2022-03-23 08:03:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:03:52 | INFO | train_inner | epoch 062:     31 / 103 loss=5.282, ppl=38.92, wps=80811.7, ups=1.24, wpb=65305.6, bsz=127.6, num_updates=6300, lr=0.00039841, gnorm=0.536, loss_scale=16, train_wall=74, gb_free=21.6, wall=5169
2022-03-23 08:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:04:50 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.518 | ppl 91.63 | wps 173269 | wpb 2040.3 | bsz 4 | num_updates 6372 | best_loss 6.518
2022-03-23 08:04:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 6372 updates
2022-03-23 08:04:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 62 @ 6372 updates, score 6.518) (writing took 0.9300296690198593 seconds)
2022-03-23 08:04:51 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-23 08:04:51 | INFO | train | epoch 062 | loss 5.27 | ppl 38.58 | wps 80792.8 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 6372 | lr 0.000396152 | gnorm 0.538 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 5228
2022-03-23 08:04:51 | INFO | fairseq.trainer | begin training epoch 63
2022-03-23 08:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:05:13 | INFO | train_inner | epoch 063:     28 / 103 loss=5.263, ppl=38.41, wps=80702.8, ups=1.24, wpb=65305.6, bsz=127.6, num_updates=6400, lr=0.000395285, gnorm=0.535, loss_scale=16, train_wall=74, gb_free=21.6, wall=5250
2022-03-23 08:06:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:06:13 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.515 | ppl 91.47 | wps 172101 | wpb 2040.3 | bsz 4 | num_updates 6475 | best_loss 6.515
2022-03-23 08:06:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6475 updates
2022-03-23 08:06:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:06:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 63 @ 6475 updates, score 6.515) (writing took 0.9842096540378407 seconds)
2022-03-23 08:06:14 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-23 08:06:14 | INFO | train | epoch 063 | loss 5.247 | ppl 37.98 | wps 80585 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 6475 | lr 0.000392989 | gnorm 0.537 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 5311
2022-03-23 08:06:14 | INFO | fairseq.trainer | begin training epoch 64
2022-03-23 08:06:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:06:34 | INFO | train_inner | epoch 064:     25 / 103 loss=5.243, ppl=37.87, wps=80502.8, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=6500, lr=0.000392232, gnorm=0.537, loss_scale=16, train_wall=74, gb_free=21.6, wall=5331
2022-03-23 08:07:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:07:37 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.514 | ppl 91.38 | wps 173504 | wpb 2040.3 | bsz 4 | num_updates 6578 | best_loss 6.514
2022-03-23 08:07:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6578 updates
2022-03-23 08:07:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:07:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:07:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 64 @ 6578 updates, score 6.514) (writing took 0.9370362170157023 seconds)
2022-03-23 08:07:38 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-23 08:07:38 | INFO | train | epoch 064 | loss 5.225 | ppl 37.41 | wps 80737 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 6578 | lr 0.0003899 | gnorm 0.536 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 5395
2022-03-23 08:07:38 | INFO | fairseq.trainer | begin training epoch 65
2022-03-23 08:07:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:07:55 | INFO | train_inner | epoch 065:     22 / 103 loss=5.222, ppl=37.33, wps=80675, ups=1.24, wpb=65300.5, bsz=127.6, num_updates=6600, lr=0.000389249, gnorm=0.538, loss_scale=16, train_wall=74, gb_free=21.6, wall=5412
2022-03-23 08:08:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:09:00 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.517 | ppl 91.57 | wps 173599 | wpb 2040.3 | bsz 4 | num_updates 6681 | best_loss 6.514
2022-03-23 08:09:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6681 updates
2022-03-23 08:09:00 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-23 08:09:00 | INFO | train | epoch 065 | loss 5.205 | ppl 36.88 | wps 81618 | ups 1.25 | wpb 65312.3 | bsz 127.6 | num_updates 6681 | lr 0.000386883 | gnorm 0.54 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 5477
2022-03-23 08:09:00 | INFO | fairseq.trainer | begin training epoch 66
2022-03-23 08:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:09:15 | INFO | train_inner | epoch 066:     19 / 103 loss=5.203, ppl=36.82, wps=81547.7, ups=1.25, wpb=65310.7, bsz=127.6, num_updates=6700, lr=0.000386334, gnorm=0.544, loss_scale=16, train_wall=74, gb_free=21.6, wall=5492
2022-03-23 08:10:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 08:10:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:10:23 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.516 | ppl 91.55 | wps 175885 | wpb 2040.3 | bsz 4 | num_updates 6783 | best_loss 6.514
2022-03-23 08:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6783 updates
2022-03-23 08:10:23 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-23 08:10:23 | INFO | train | epoch 066 | loss 5.185 | ppl 36.39 | wps 80888.5 | ups 1.24 | wpb 65310.1 | bsz 127.6 | num_updates 6783 | lr 0.000383963 | gnorm 0.545 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 5559
2022-03-23 08:10:23 | INFO | fairseq.trainer | begin training epoch 67
2022-03-23 08:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:10:36 | INFO | train_inner | epoch 067:     17 / 103 loss=5.182, ppl=36.3, wps=80982.9, ups=1.24, wpb=65305.6, bsz=127.6, num_updates=6800, lr=0.000383482, gnorm=0.544, loss_scale=16, train_wall=74, gb_free=21.6, wall=5573
2022-03-23 08:11:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:11:44 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.514 | ppl 91.4 | wps 175960 | wpb 2040.3 | bsz 4 | num_updates 6886 | best_loss 6.514
2022-03-23 08:11:44 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-23 08:11:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6886 updates
2022-03-23 08:11:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:11:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt
2022-03-23 08:11:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#4/checkpoint_best.pt (epoch 67 @ 6886 updates, score 6.514) (writing took 0.9600631030043587 seconds)
2022-03-23 08:11:45 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-23 08:11:45 | INFO | train | epoch 067 | loss 5.165 | ppl 35.88 | wps 81417 | ups 1.25 | wpb 65312.3 | bsz 127.6 | num_updates 6886 | lr 0.00038108 | gnorm 0.542 | loss_scale 16 | train_wall 75 | gb_free 21.6 | wall 5642
2022-03-23 08:11:45 | INFO | fairseq_cli.train | done training in 5641.9 seconds
