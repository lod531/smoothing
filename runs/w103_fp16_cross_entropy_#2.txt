Sender: LSF System <lsfadmin@eu-g3-075>
Subject: Job 204473307: <w103_fp16_cross_entropy_#2> in cluster <euler> Exited

Job <w103_fp16_cross_entropy_#2> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Feb 10 20:26:51 2022
Job was executed on host(s) <eu-g3-075>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Feb 10 20:39:51 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Feb 10 20:39:51 2022
Terminated at Fri Feb 11 15:15:29 2022
Results reported at Fri Feb 11 15:15:29 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 6658482 --fp16 --max-update 5000000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   66912.64 sec.
    Max Memory :                                 18289 MB
    Average Memory :                             3486.94 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               1711.00 MB
    Max Swap :                                   5 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   66936 sec.
    Turnaround time :                            67718 sec.

The output (if any) follows:

2022-02-10 20:39:58 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6658482, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6658482, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-10 20:39:59 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-10 20:40:05 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-10 20:40:05 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-10 20:40:05 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-10 20:40:05 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-10 20:40:05 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-10 20:40:05 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-10 20:40:05 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-10 20:40:08 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-10 20:40:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-10 20:40:08 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-10 20:40:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-10 20:40:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-10 20:40:08 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-10 20:40:08 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint_last.pt
2022-02-10 20:40:08 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint_last.pt
2022-02-10 20:40:08 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-10 20:40:08 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-10 20:40:09 | INFO | fairseq.trainer | begin training epoch 1
2022-02-10 20:40:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-10 20:40:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-10 20:40:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 20:40:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-10 20:40:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-10 20:48:48 | INFO | train_inner | epoch 001:    104 / 1576 loss=18.216, ppl=304545, wps=13944.1, ups=0.21, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.994, loss_scale=8, train_wall=504, gb_free=10, wall=520
2022-02-10 20:56:39 | INFO | train_inner | epoch 001:    204 / 1576 loss=15.749, ppl=55075.8, wps=13923.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.741, loss_scale=8, train_wall=460, gb_free=10, wall=990
2022-02-10 21:04:29 | INFO | train_inner | epoch 001:    304 / 1576 loss=13.306, ppl=10131, wps=13945.1, ups=0.21, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.366, loss_scale=16, train_wall=459, gb_free=10, wall=1460
2022-02-10 21:12:19 | INFO | train_inner | epoch 001:    404 / 1576 loss=11.32, ppl=2557, wps=13935.9, ups=0.21, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.824, loss_scale=16, train_wall=460, gb_free=10, wall=1931
2022-02-10 21:20:09 | INFO | train_inner | epoch 001:    504 / 1576 loss=10.535, ppl=1484.06, wps=13943, ups=0.21, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.508, loss_scale=16, train_wall=460, gb_free=10, wall=2401
2022-02-10 21:27:59 | INFO | train_inner | epoch 001:    604 / 1576 loss=10.268, ppl=1232.67, wps=13940.3, ups=0.21, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.48, loss_scale=32, train_wall=460, gb_free=10, wall=2871
2022-02-10 21:35:49 | INFO | train_inner | epoch 001:    704 / 1576 loss=10.037, ppl=1050.56, wps=13936.5, ups=0.21, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.461, loss_scale=32, train_wall=460, gb_free=10, wall=3341
2022-02-10 21:41:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 21:43:44 | INFO | train_inner | epoch 001:    805 / 1576 loss=9.814, ppl=900.32, wps=13805.5, ups=0.21, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.446, loss_scale=32, train_wall=464, gb_free=10, wall=3816
2022-02-10 21:51:34 | INFO | train_inner | epoch 001:    905 / 1576 loss=9.615, ppl=783.92, wps=13953.1, ups=0.21, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.485, loss_scale=32, train_wall=459, gb_free=10, wall=4286
2022-02-10 21:59:23 | INFO | train_inner | epoch 001:   1005 / 1576 loss=9.411, ppl=680.99, wps=13956.2, ups=0.21, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.525, loss_scale=32, train_wall=459, gb_free=10, wall=4755
2022-02-10 22:01:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 22:07:18 | INFO | train_inner | epoch 001:   1106 / 1576 loss=9.238, ppl=603.86, wps=13816.2, ups=0.21, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.563, loss_scale=32, train_wall=464, gb_free=10, wall=5229
2022-02-10 22:15:07 | INFO | train_inner | epoch 001:   1206 / 1576 loss=9.059, ppl=533.33, wps=13961.2, ups=0.21, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.556, loss_scale=32, train_wall=459, gb_free=10, wall=5699
2022-02-10 22:22:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 22:23:01 | INFO | train_inner | epoch 001:   1307 / 1576 loss=8.903, ppl=478.74, wps=13818.1, ups=0.21, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.568, loss_scale=32, train_wall=464, gb_free=10, wall=6173
2022-02-10 22:30:51 | INFO | train_inner | epoch 001:   1407 / 1576 loss=8.772, ppl=437.03, wps=13960.3, ups=0.21, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.56, loss_scale=32, train_wall=459, gb_free=10, wall=6643
2022-02-10 22:38:40 | INFO | train_inner | epoch 001:   1507 / 1576 loss=8.654, ppl=402.74, wps=13956.9, ups=0.21, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.59, loss_scale=32, train_wall=459, gb_free=10, wall=7112
2022-02-10 22:42:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 22:44:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-10 22:44:06 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.3 | ppl 315.16 | wps 37584.2 | wpb 1021.8 | bsz 2 | num_updates 1568
2022-02-10 22:44:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1568 updates
2022-02-10 22:44:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint1.pt
2022-02-10 22:44:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint1.pt
2022-02-10 22:44:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint1.pt (epoch 1 @ 1568 updates, score 8.3) (writing took 28.05904029775411 seconds)
2022-02-10 22:44:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-10 22:44:34 | INFO | train | epoch 001 | loss 10.761 | ppl 1734.8 | wps 13848.5 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 1568 | lr 0.000196061 | gnorm 0.833 | loss_scale 32 | train_wall 7262 | gb_free 10 | wall 7466
2022-02-10 22:44:34 | INFO | fairseq.trainer | begin training epoch 2
2022-02-10 22:44:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-10 22:47:04 | INFO | train_inner | epoch 002:     32 / 1576 loss=8.519, ppl=366.83, wps=12889.7, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=0.583, loss_scale=32, train_wall=459, gb_free=10, wall=7616
2022-02-10 22:54:53 | INFO | train_inner | epoch 002:    132 / 1576 loss=8.406, ppl=339.1, wps=13967, ups=0.21, wpb=65532.3, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.566, loss_scale=32, train_wall=459, gb_free=10, wall=8085
2022-02-10 23:02:43 | INFO | train_inner | epoch 002:    232 / 1576 loss=8.303, ppl=315.79, wps=13955.5, ups=0.21, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.577, loss_scale=32, train_wall=459, gb_free=10, wall=8555
2022-02-10 23:06:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 23:10:37 | INFO | train_inner | epoch 002:    333 / 1576 loss=8.215, ppl=297.05, wps=13816.2, ups=0.21, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.534, loss_scale=32, train_wall=464, gb_free=10, wall=9029
2022-02-10 23:18:27 | INFO | train_inner | epoch 002:    433 / 1576 loss=8.125, ppl=279.26, wps=13962.4, ups=0.21, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.545, loss_scale=32, train_wall=459, gb_free=10, wall=9499
2022-02-10 23:26:16 | INFO | train_inner | epoch 002:    533 / 1576 loss=8.034, ppl=262.09, wps=13963.4, ups=0.21, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.543, loss_scale=32, train_wall=459, gb_free=10, wall=9968
2022-02-10 23:34:06 | INFO | train_inner | epoch 002:    633 / 1576 loss=7.939, ppl=245.49, wps=13954.1, ups=0.21, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.537, loss_scale=64, train_wall=459, gb_free=10, wall=10438
2022-02-10 23:41:55 | INFO | train_inner | epoch 002:    733 / 1576 loss=7.866, ppl=233.26, wps=13955.7, ups=0.21, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.544, loss_scale=64, train_wall=459, gb_free=10, wall=10907
2022-02-10 23:47:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-10 23:47:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-10 23:49:54 | INFO | train_inner | epoch 002:    835 / 1576 loss=7.803, ppl=223.31, wps=13685.3, ups=0.21, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.539, loss_scale=32, train_wall=468, gb_free=10, wall=11386
2022-02-10 23:57:44 | INFO | train_inner | epoch 002:    935 / 1576 loss=7.714, ppl=209.99, wps=13964.9, ups=0.21, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.51, loss_scale=32, train_wall=459, gb_free=10, wall=11855
2022-02-11 00:05:33 | INFO | train_inner | epoch 002:   1035 / 1576 loss=7.645, ppl=200.12, wps=13960.4, ups=0.21, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.522, loss_scale=32, train_wall=459, gb_free=10, wall=12325
2022-02-11 00:09:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 00:13:27 | INFO | train_inner | epoch 002:   1136 / 1576 loss=7.564, ppl=189.27, wps=13823.4, ups=0.21, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.515, loss_scale=32, train_wall=463, gb_free=10, wall=12799
2022-02-11 00:21:17 | INFO | train_inner | epoch 002:   1236 / 1576 loss=7.506, ppl=181.76, wps=13960.7, ups=0.21, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.535, loss_scale=32, train_wall=459, gb_free=10, wall=13268
2022-02-11 00:29:06 | INFO | train_inner | epoch 002:   1336 / 1576 loss=7.438, ppl=173.35, wps=13960.7, ups=0.21, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.525, loss_scale=64, train_wall=459, gb_free=10, wall=13738
2022-02-11 00:33:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 00:37:00 | INFO | train_inner | epoch 002:   1437 / 1576 loss=7.384, ppl=166.98, wps=13820.4, ups=0.21, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.521, loss_scale=32, train_wall=464, gb_free=10, wall=14212
2022-02-11 00:41:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 00:44:54 | INFO | train_inner | epoch 002:   1538 / 1576 loss=7.336, ppl=161.53, wps=13827, ups=0.21, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.516, loss_scale=16, train_wall=463, gb_free=10, wall=14686
2022-02-11 00:47:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 00:47:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.059 | ppl 133.37 | wps 37485.5 | wpb 1021.8 | bsz 2 | num_updates 3138 | best_loss 7.059
2022-02-11 00:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3138 updates
2022-02-11 00:47:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint2.pt
2022-02-11 00:48:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint2.pt
2022-02-11 00:48:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint2.pt (epoch 2 @ 3138 updates, score 7.059) (writing took 28.360795255750418 seconds)
2022-02-11 00:48:23 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-11 00:48:23 | INFO | train | epoch 002 | loss 7.819 | ppl 225.82 | wps 13843 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 3138 | lr 0.000392272 | gnorm 0.536 | loss_scale 16 | train_wall 7229 | gb_free 10 | wall 14895
2022-02-11 00:48:23 | INFO | fairseq.trainer | begin training epoch 3
2022-02-11 00:48:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 00:53:13 | INFO | train_inner | epoch 003:     62 / 1576 loss=7.259, ppl=153.19, wps=13011.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.516, loss_scale=16, train_wall=454, gb_free=10, wall=15185
2022-02-11 01:01:02 | INFO | train_inner | epoch 003:    162 / 1576 loss=7.203, ppl=147.36, wps=13985.5, ups=0.21, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.504, loss_scale=16, train_wall=458, gb_free=10, wall=15654
2022-02-11 01:08:51 | INFO | train_inner | epoch 003:    262 / 1576 loss=7.156, ppl=142.64, wps=13984.9, ups=0.21, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.503, loss_scale=32, train_wall=458, gb_free=10, wall=16122
2022-02-11 01:16:39 | INFO | train_inner | epoch 003:    362 / 1576 loss=7.116, ppl=138.7, wps=13984.5, ups=0.21, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.505, loss_scale=32, train_wall=458, gb_free=10, wall=16591
2022-02-11 01:22:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 01:24:33 | INFO | train_inner | epoch 003:    463 / 1576 loss=7.077, ppl=135.05, wps=13843.8, ups=0.21, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.51, loss_scale=32, train_wall=463, gb_free=10, wall=17065
2022-02-11 01:32:21 | INFO | train_inner | epoch 003:    563 / 1576 loss=7.034, ppl=131.08, wps=13993, ups=0.21, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.5, loss_scale=32, train_wall=458, gb_free=10, wall=17533
2022-02-11 01:32:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 01:40:14 | INFO | train_inner | epoch 003:    664 / 1576 loss=7.007, ppl=128.65, wps=13851.3, ups=0.21, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.507, loss_scale=16, train_wall=463, gb_free=10, wall=18006
2022-02-11 01:48:03 | INFO | train_inner | epoch 003:    764 / 1576 loss=6.958, ppl=124.29, wps=13987.6, ups=0.21, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.511, loss_scale=16, train_wall=458, gb_free=10, wall=18475
2022-02-11 01:55:51 | INFO | train_inner | epoch 003:    864 / 1576 loss=6.932, ppl=122.09, wps=13987.3, ups=0.21, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.513, loss_scale=32, train_wall=458, gb_free=10, wall=18943
2022-02-11 01:59:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 02:03:44 | INFO | train_inner | epoch 003:    965 / 1576 loss=6.904, ppl=119.76, wps=13850.9, ups=0.21, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.49, loss_scale=16, train_wall=463, gb_free=10, wall=19416
2022-02-11 02:11:33 | INFO | train_inner | epoch 003:   1065 / 1576 loss=6.866, ppl=116.65, wps=13992.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.502, loss_scale=16, train_wall=458, gb_free=10, wall=19885
2022-02-11 02:19:21 | INFO | train_inner | epoch 003:   1165 / 1576 loss=6.828, ppl=113.65, wps=13999.3, ups=0.21, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.491, loss_scale=16, train_wall=458, gb_free=10, wall=20353
2022-02-11 02:24:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 02:27:14 | INFO | train_inner | epoch 003:   1266 / 1576 loss=6.798, ppl=111.28, wps=13855.8, ups=0.21, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.484, loss_scale=16, train_wall=462, gb_free=10, wall=20826
2022-02-11 02:35:02 | INFO | train_inner | epoch 003:   1366 / 1576 loss=6.777, ppl=109.65, wps=13990.5, ups=0.21, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.485, loss_scale=16, train_wall=458, gb_free=10, wall=21294
2022-02-11 02:39:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 02:42:55 | INFO | train_inner | epoch 003:   1467 / 1576 loss=6.748, ppl=107.48, wps=13867, ups=0.21, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.481, loss_scale=8, train_wall=462, gb_free=10, wall=21767
2022-02-11 02:50:43 | INFO | train_inner | epoch 003:   1567 / 1576 loss=6.711, ppl=104.76, wps=14005.3, ups=0.21, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.493, loss_scale=8, train_wall=457, gb_free=10, wall=22235
2022-02-11 02:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 02:51:27 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.46 | ppl 88.05 | wps 37615.1 | wpb 1021.8 | bsz 2 | num_updates 4709 | best_loss 6.46
2022-02-11 02:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4709 updates
2022-02-11 02:51:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint3.pt
2022-02-11 02:51:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint3.pt
2022-02-11 02:51:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint3.pt (epoch 3 @ 4709 updates, score 6.46) (writing took 29.671382799744606 seconds)
2022-02-11 02:51:56 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-11 02:51:56 | INFO | train | epoch 003 | loss 6.952 | ppl 123.79 | wps 13879.5 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 4709 | lr 0.000460825 | gnorm 0.5 | loss_scale 8 | train_wall 7212 | gb_free 10 | wall 22308
2022-02-11 02:51:57 | INFO | fairseq.trainer | begin training epoch 4
2022-02-11 02:51:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 02:59:02 | INFO | train_inner | epoch 004:     91 / 1576 loss=6.635, ppl=99.36, wps=13003.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.481, loss_scale=8, train_wall=453, gb_free=10, wall=22734
2022-02-11 03:06:51 | INFO | train_inner | epoch 004:    191 / 1576 loss=6.629, ppl=98.96, wps=13995.5, ups=0.21, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.473, loss_scale=16, train_wall=458, gb_free=10, wall=23203
2022-02-11 03:14:39 | INFO | train_inner | epoch 004:    291 / 1576 loss=6.613, ppl=97.89, wps=13992.4, ups=0.21, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.479, loss_scale=16, train_wall=458, gb_free=10, wall=23671
2022-02-11 03:21:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 03:22:32 | INFO | train_inner | epoch 004:    392 / 1576 loss=6.588, ppl=96.21, wps=13855.7, ups=0.21, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.471, loss_scale=16, train_wall=462, gb_free=10, wall=24144
2022-02-11 03:30:20 | INFO | train_inner | epoch 004:    492 / 1576 loss=6.578, ppl=95.53, wps=13995.3, ups=0.21, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.479, loss_scale=16, train_wall=458, gb_free=10, wall=24612
2022-02-11 03:38:08 | INFO | train_inner | epoch 004:    592 / 1576 loss=6.561, ppl=94.43, wps=13997.6, ups=0.21, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.479, loss_scale=16, train_wall=458, gb_free=10, wall=25080
2022-02-11 03:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 03:46:02 | INFO | train_inner | epoch 004:    693 / 1576 loss=6.549, ppl=93.65, wps=13854.3, ups=0.21, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.487, loss_scale=16, train_wall=462, gb_free=10, wall=25553
2022-02-11 03:53:50 | INFO | train_inner | epoch 004:    793 / 1576 loss=6.531, ppl=92.48, wps=13994.4, ups=0.21, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.48, loss_scale=16, train_wall=458, gb_free=10, wall=26022
2022-02-11 04:01:38 | INFO | train_inner | epoch 004:    893 / 1576 loss=6.533, ppl=92.6, wps=13991.9, ups=0.21, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.48, loss_scale=16, train_wall=458, gb_free=10, wall=26490
2022-02-11 04:05:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 04:09:31 | INFO | train_inner | epoch 004:    994 / 1576 loss=6.5, ppl=90.52, wps=13860, ups=0.21, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.482, loss_scale=16, train_wall=462, gb_free=10, wall=26963
2022-02-11 04:17:19 | INFO | train_inner | epoch 004:   1094 / 1576 loss=6.498, ppl=90.38, wps=13991.1, ups=0.21, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.477, loss_scale=16, train_wall=458, gb_free=10, wall=27431
2022-02-11 04:25:08 | INFO | train_inner | epoch 004:   1194 / 1576 loss=6.493, ppl=90.1, wps=13994.9, ups=0.21, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.476, loss_scale=16, train_wall=458, gb_free=10, wall=27900
2022-02-11 04:27:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 04:33:01 | INFO | train_inner | epoch 004:   1295 / 1576 loss=6.484, ppl=89.51, wps=13855.4, ups=0.21, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.488, loss_scale=16, train_wall=462, gb_free=10, wall=28373
2022-02-11 04:40:49 | INFO | train_inner | epoch 004:   1395 / 1576 loss=6.474, ppl=88.88, wps=13992.4, ups=0.21, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.483, loss_scale=16, train_wall=458, gb_free=10, wall=28841
2022-02-11 04:48:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 04:48:42 | INFO | train_inner | epoch 004:   1496 / 1576 loss=6.448, ppl=87.29, wps=13853.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.479, loss_scale=16, train_wall=462, gb_free=10, wall=29314
2022-02-11 04:54:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 04:54:59 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.189 | ppl 72.94 | wps 37355.9 | wpb 1021.8 | bsz 2 | num_updates 6280 | best_loss 6.189
2022-02-11 04:54:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6280 updates
2022-02-11 04:54:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint4.pt
2022-02-11 04:55:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint4.pt
2022-02-11 04:55:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint4.pt (epoch 4 @ 6280 updates, score 6.189) (writing took 29.207684352062643 seconds)
2022-02-11 04:55:28 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-11 04:55:28 | INFO | train | epoch 004 | loss 6.535 | ppl 92.74 | wps 13883.7 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 6280 | lr 0.000399043 | gnorm 0.479 | loss_scale 16 | train_wall 7211 | gb_free 10 | wall 29720
2022-02-11 04:55:28 | INFO | fairseq.trainer | begin training epoch 5
2022-02-11 04:55:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 04:57:02 | INFO | train_inner | epoch 005:     20 / 1576 loss=6.432, ppl=86.34, wps=13002.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.485, loss_scale=16, train_wall=454, gb_free=10, wall=29814
2022-02-11 05:04:51 | INFO | train_inner | epoch 005:    120 / 1576 loss=6.366, ppl=82.47, wps=13957.4, ups=0.21, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.473, loss_scale=16, train_wall=459, gb_free=10, wall=30283
2022-02-11 05:09:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 05:12:44 | INFO | train_inner | epoch 005:    221 / 1576 loss=6.355, ppl=81.88, wps=13851.7, ups=0.21, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.483, loss_scale=16, train_wall=462, gb_free=10, wall=30756
2022-02-11 05:20:35 | INFO | train_inner | epoch 005:    321 / 1576 loss=6.349, ppl=81.52, wps=13937.5, ups=0.21, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.478, loss_scale=16, train_wall=459, gb_free=10, wall=31227
2022-02-11 05:28:26 | INFO | train_inner | epoch 005:    421 / 1576 loss=6.354, ppl=81.77, wps=13909.4, ups=0.21, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.493, loss_scale=16, train_wall=460, gb_free=10, wall=31698
2022-02-11 05:29:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 05:36:22 | INFO | train_inner | epoch 005:    522 / 1576 loss=6.341, ppl=81.07, wps=13760.1, ups=0.21, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.482, loss_scale=16, train_wall=465, gb_free=10, wall=32174
2022-02-11 05:44:13 | INFO | train_inner | epoch 005:    622 / 1576 loss=6.347, ppl=81.39, wps=13913.4, ups=0.21, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.499, loss_scale=16, train_wall=460, gb_free=10, wall=32645
2022-02-11 05:50:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 05:52:09 | INFO | train_inner | epoch 005:    723 / 1576 loss=6.344, ppl=81.23, wps=13757, ups=0.21, wpb=65532.3, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.507, loss_scale=16, train_wall=465, gb_free=10, wall=33121
2022-02-11 06:00:01 | INFO | train_inner | epoch 005:    823 / 1576 loss=6.34, ppl=81.01, wps=13896.7, ups=0.21, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.486, loss_scale=16, train_wall=461, gb_free=10, wall=33593
2022-02-11 06:07:53 | INFO | train_inner | epoch 005:    923 / 1576 loss=6.314, ppl=79.53, wps=13892.7, ups=0.21, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.498, loss_scale=16, train_wall=461, gb_free=10, wall=34065
2022-02-11 06:11:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 06:15:49 | INFO | train_inner | epoch 005:   1024 / 1576 loss=6.313, ppl=79.52, wps=13762.8, ups=0.21, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.485, loss_scale=16, train_wall=465, gb_free=10, wall=34541
2022-02-11 06:23:40 | INFO | train_inner | epoch 005:   1124 / 1576 loss=6.306, ppl=79.15, wps=13900, ups=0.21, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.505, loss_scale=16, train_wall=461, gb_free=10, wall=35012
2022-02-11 06:31:32 | INFO | train_inner | epoch 005:   1224 / 1576 loss=6.308, ppl=79.21, wps=13900.2, ups=0.21, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.499, loss_scale=32, train_wall=461, gb_free=10, wall=35484
2022-02-11 06:35:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 06:39:28 | INFO | train_inner | epoch 005:   1325 / 1576 loss=6.304, ppl=78.99, wps=13756.8, ups=0.21, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.51, loss_scale=16, train_wall=465, gb_free=10, wall=35960
2022-02-11 06:47:20 | INFO | train_inner | epoch 005:   1425 / 1576 loss=6.302, ppl=78.9, wps=13906.7, ups=0.21, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.493, loss_scale=16, train_wall=460, gb_free=10, wall=36431
2022-02-11 06:55:11 | INFO | train_inner | epoch 005:   1525 / 1576 loss=6.293, ppl=78.39, wps=13906, ups=0.21, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.509, loss_scale=16, train_wall=460, gb_free=10, wall=36903
2022-02-11 06:55:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 06:59:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 06:59:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.017 | ppl 64.75 | wps 36495.5 | wpb 1021.8 | bsz 2 | num_updates 7850 | best_loss 6.017
2022-02-11 06:59:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7850 updates
2022-02-11 06:59:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint5.pt
2022-02-11 06:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint5.pt
2022-02-11 06:59:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint5.pt (epoch 5 @ 7850 updates, score 6.017) (writing took 29.324120617471635 seconds)
2022-02-11 06:59:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-11 06:59:43 | INFO | train | epoch 005 | loss 6.329 | ppl 80.37 | wps 13794.6 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 7850 | lr 0.000356915 | gnorm 0.494 | loss_scale 16 | train_wall 7249 | gb_free 10 | wall 37175
2022-02-11 06:59:43 | INFO | fairseq.trainer | begin training epoch 6
2022-02-11 06:59:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 07:03:38 | INFO | train_inner | epoch 006:     50 / 1576 loss=6.26, ppl=76.62, wps=12804.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.514, loss_scale=16, train_wall=461, gb_free=10, wall=37410
2022-02-11 07:11:30 | INFO | train_inner | epoch 006:    150 / 1576 loss=6.219, ppl=74.51, wps=13899.3, ups=0.21, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.515, loss_scale=16, train_wall=461, gb_free=10, wall=37882
2022-02-11 07:17:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 07:19:26 | INFO | train_inner | epoch 006:    251 / 1576 loss=6.21, ppl=74.04, wps=13774.3, ups=0.21, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.515, loss_scale=16, train_wall=465, gb_free=10, wall=38357
2022-02-11 07:27:17 | INFO | train_inner | epoch 006:    351 / 1576 loss=6.21, ppl=74.03, wps=13899, ups=0.21, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.519, loss_scale=16, train_wall=461, gb_free=10, wall=38829
2022-02-11 07:35:09 | INFO | train_inner | epoch 006:    451 / 1576 loss=6.211, ppl=74.09, wps=13895.2, ups=0.21, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.513, loss_scale=16, train_wall=461, gb_free=10, wall=39301
2022-02-11 07:37:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 07:43:05 | INFO | train_inner | epoch 006:    552 / 1576 loss=6.207, ppl=73.89, wps=13767.2, ups=0.21, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.51, loss_scale=16, train_wall=465, gb_free=10, wall=39777
2022-02-11 07:50:56 | INFO | train_inner | epoch 006:    652 / 1576 loss=6.209, ppl=73.99, wps=13895.4, ups=0.21, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.522, loss_scale=16, train_wall=461, gb_free=10, wall=40248
2022-02-11 07:57:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 07:58:52 | INFO | train_inner | epoch 006:    753 / 1576 loss=6.203, ppl=73.66, wps=13768.3, ups=0.21, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.527, loss_scale=16, train_wall=465, gb_free=10, wall=40724
2022-02-11 08:02:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 08:06:48 | INFO | train_inner | epoch 006:    854 / 1576 loss=6.201, ppl=73.56, wps=13771.3, ups=0.21, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.531, loss_scale=8, train_wall=465, gb_free=10, wall=41200
2022-02-11 08:14:39 | INFO | train_inner | epoch 006:    954 / 1576 loss=6.209, ppl=74, wps=13912, ups=0.21, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.526, loss_scale=8, train_wall=460, gb_free=10, wall=41671
2022-02-11 08:22:30 | INFO | train_inner | epoch 006:   1054 / 1576 loss=6.192, ppl=73.11, wps=13914.7, ups=0.21, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.529, loss_scale=8, train_wall=460, gb_free=10, wall=42142
2022-02-11 08:30:22 | INFO | train_inner | epoch 006:   1154 / 1576 loss=6.196, ppl=73.3, wps=13906.8, ups=0.21, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.538, loss_scale=16, train_wall=460, gb_free=10, wall=42613
2022-02-11 08:38:13 | INFO | train_inner | epoch 006:   1254 / 1576 loss=6.198, ppl=73.4, wps=13903.1, ups=0.21, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.53, loss_scale=16, train_wall=461, gb_free=10, wall=43085
2022-02-11 08:43:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 08:46:09 | INFO | train_inner | epoch 006:   1355 / 1576 loss=6.2, ppl=73.51, wps=13762.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.543, loss_scale=16, train_wall=465, gb_free=10, wall=43561
2022-02-11 08:48:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 08:54:05 | INFO | train_inner | epoch 006:   1456 / 1576 loss=6.188, ppl=72.91, wps=13777.8, ups=0.21, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.56, loss_scale=8, train_wall=465, gb_free=10, wall=44037
2022-02-11 09:01:56 | INFO | train_inner | epoch 006:   1556 / 1576 loss=6.182, ppl=72.61, wps=13899.5, ups=0.21, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.55, loss_scale=8, train_wall=461, gb_free=10, wall=44508
2022-02-11 09:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 09:03:32 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.916 | ppl 60.38 | wps 36775.2 | wpb 1021.8 | bsz 2 | num_updates 9420 | best_loss 5.916
2022-02-11 09:03:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9420 updates
2022-02-11 09:03:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint6.pt
2022-02-11 09:03:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint6.pt
2022-02-11 09:04:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint6.pt (epoch 6 @ 9420 updates, score 5.916) (writing took 28.43600504565984 seconds)
2022-02-11 09:04:01 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-11 09:04:01 | INFO | train | epoch 006 | loss 6.203 | ppl 73.65 | wps 13787.9 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 9420 | lr 0.000325818 | gnorm 0.529 | loss_scale 8 | train_wall 7252 | gb_free 10 | wall 44633
2022-02-11 09:04:01 | INFO | fairseq.trainer | begin training epoch 7
2022-02-11 09:04:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 09:10:17 | INFO | train_inner | epoch 007:     80 / 1576 loss=6.127, ppl=69.91, wps=12963.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.571, loss_scale=16, train_wall=456, gb_free=10, wall=45009
2022-02-11 09:18:09 | INFO | train_inner | epoch 007:    180 / 1576 loss=6.108, ppl=68.97, wps=13900.8, ups=0.21, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.555, loss_scale=16, train_wall=461, gb_free=10, wall=45481
2022-02-11 09:26:00 | INFO | train_inner | epoch 007:    280 / 1576 loss=6.134, ppl=70.21, wps=13903.3, ups=0.21, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.56, loss_scale=16, train_wall=460, gb_free=10, wall=45952
2022-02-11 09:29:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 09:33:57 | INFO | train_inner | epoch 007:    381 / 1576 loss=6.122, ppl=69.64, wps=13756.6, ups=0.21, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.564, loss_scale=16, train_wall=465, gb_free=10, wall=46428
2022-02-11 09:41:48 | INFO | train_inner | epoch 007:    481 / 1576 loss=6.111, ppl=69.11, wps=13889.2, ups=0.21, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.561, loss_scale=16, train_wall=461, gb_free=10, wall=46900
2022-02-11 09:49:40 | INFO | train_inner | epoch 007:    581 / 1576 loss=6.135, ppl=70.3, wps=13897.9, ups=0.21, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.572, loss_scale=16, train_wall=461, gb_free=10, wall=47372
2022-02-11 09:49:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 09:51:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 09:57:41 | INFO | train_inner | epoch 007:    683 / 1576 loss=6.115, ppl=69.31, wps=13634.3, ups=0.21, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.579, loss_scale=8, train_wall=470, gb_free=10, wall=47853
2022-02-11 10:05:32 | INFO | train_inner | epoch 007:    783 / 1576 loss=6.116, ppl=69.33, wps=13906.2, ups=0.21, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.572, loss_scale=8, train_wall=460, gb_free=10, wall=48324
2022-02-11 10:13:23 | INFO | train_inner | epoch 007:    883 / 1576 loss=6.121, ppl=69.58, wps=13906.7, ups=0.21, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.575, loss_scale=16, train_wall=460, gb_free=10, wall=48795
2022-02-11 10:13:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 10:21:19 | INFO | train_inner | epoch 007:    984 / 1576 loss=6.114, ppl=69.25, wps=13768.2, ups=0.21, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.581, loss_scale=8, train_wall=465, gb_free=10, wall=49271
2022-02-11 10:29:11 | INFO | train_inner | epoch 007:   1084 / 1576 loss=6.114, ppl=69.24, wps=13901.2, ups=0.21, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.575, loss_scale=8, train_wall=461, gb_free=10, wall=49742
2022-02-11 10:35:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 10:37:07 | INFO | train_inner | epoch 007:   1185 / 1576 loss=6.13, ppl=70.04, wps=13770.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.604, loss_scale=8, train_wall=465, gb_free=10, wall=50218
2022-02-11 10:44:57 | INFO | train_inner | epoch 007:   1285 / 1576 loss=6.115, ppl=69.32, wps=13915.1, ups=0.21, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.6, loss_scale=8, train_wall=460, gb_free=10, wall=50689
2022-02-11 10:52:49 | INFO | train_inner | epoch 007:   1385 / 1576 loss=6.106, ppl=68.89, wps=13910.8, ups=0.21, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.589, loss_scale=8, train_wall=460, gb_free=10, wall=51160
2022-02-11 10:57:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 11:00:44 | INFO | train_inner | epoch 007:   1486 / 1576 loss=6.108, ppl=68.97, wps=13777.6, ups=0.21, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.608, loss_scale=8, train_wall=465, gb_free=10, wall=51636
2022-02-11 11:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 11:07:50 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.841 | ppl 57.32 | wps 36501 | wpb 1021.8 | bsz 2 | num_updates 10990 | best_loss 5.841
2022-02-11 11:07:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10990 updates
2022-02-11 11:07:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint7.pt
2022-02-11 11:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint7.pt
2022-02-11 11:08:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint7.pt (epoch 7 @ 10990 updates, score 5.841) (writing took 28.907275710254908 seconds)
2022-02-11 11:08:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-11 11:08:19 | INFO | train | epoch 007 | loss 6.117 | ppl 69.41 | wps 13787.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 10990 | lr 0.000301648 | gnorm 0.579 | loss_scale 8 | train_wall 7252 | gb_free 10 | wall 52091
2022-02-11 11:08:19 | INFO | fairseq.trainer | begin training epoch 8
2022-02-11 11:08:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 11:09:06 | INFO | train_inner | epoch 008:     10 / 1576 loss=6.098, ppl=68.5, wps=12937.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.622, loss_scale=8, train_wall=456, gb_free=10, wall=52138
2022-02-11 11:16:58 | INFO | train_inner | epoch 008:    110 / 1576 loss=6.042, ppl=65.91, wps=13907.2, ups=0.21, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.602, loss_scale=8, train_wall=460, gb_free=10, wall=52610
2022-02-11 11:18:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 11:24:54 | INFO | train_inner | epoch 008:    211 / 1576 loss=6.04, ppl=65.8, wps=13771.1, ups=0.21, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.616, loss_scale=8, train_wall=465, gb_free=10, wall=53085
2022-02-11 11:32:44 | INFO | train_inner | epoch 008:    311 / 1576 loss=6.049, ppl=66.21, wps=13917.1, ups=0.21, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.626, loss_scale=8, train_wall=460, gb_free=10, wall=53556
2022-02-11 11:39:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 11:40:41 | INFO | train_inner | epoch 008:    412 / 1576 loss=6.051, ppl=66.32, wps=13753.8, ups=0.21, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.618, loss_scale=8, train_wall=465, gb_free=10, wall=54033
2022-02-11 11:48:32 | INFO | train_inner | epoch 008:    512 / 1576 loss=6.052, ppl=66.35, wps=13904.9, ups=0.21, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.627, loss_scale=8, train_wall=460, gb_free=10, wall=54504
2022-02-11 11:56:23 | INFO | train_inner | epoch 008:    612 / 1576 loss=6.057, ppl=66.59, wps=13925.5, ups=0.21, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.628, loss_scale=8, train_wall=460, gb_free=10, wall=54975
2022-02-11 12:04:13 | INFO | train_inner | epoch 008:    712 / 1576 loss=6.057, ppl=66.58, wps=13929, ups=0.21, wpb=65532.3, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.637, loss_scale=16, train_wall=460, gb_free=10, wall=55445
2022-02-11 12:05:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 12:12:09 | INFO | train_inner | epoch 008:    813 / 1576 loss=6.061, ppl=66.78, wps=13787.4, ups=0.21, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.651, loss_scale=8, train_wall=464, gb_free=10, wall=55921
2022-02-11 12:20:00 | INFO | train_inner | epoch 008:    913 / 1576 loss=6.059, ppl=66.66, wps=13915.8, ups=0.21, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.653, loss_scale=8, train_wall=460, gb_free=10, wall=56391
2022-02-11 12:25:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 12:27:55 | INFO | train_inner | epoch 008:   1014 / 1576 loss=6.059, ppl=66.69, wps=13784.2, ups=0.21, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.661, loss_scale=8, train_wall=465, gb_free=10, wall=56867
2022-02-11 12:35:46 | INFO | train_inner | epoch 008:   1114 / 1576 loss=6.06, ppl=66.74, wps=13918.3, ups=0.21, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.664, loss_scale=8, train_wall=460, gb_free=10, wall=57338
2022-02-11 12:43:37 | INFO | train_inner | epoch 008:   1214 / 1576 loss=6.057, ppl=66.56, wps=13916.5, ups=0.21, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.664, loss_scale=8, train_wall=460, gb_free=10, wall=57809
2022-02-11 12:45:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 12:51:32 | INFO | train_inner | epoch 008:   1315 / 1576 loss=6.062, ppl=66.79, wps=13780.9, ups=0.21, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.657, loss_scale=8, train_wall=465, gb_free=10, wall=58284
2022-02-11 12:59:23 | INFO | train_inner | epoch 008:   1415 / 1576 loss=6.069, ppl=67.14, wps=13932.7, ups=0.21, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.676, loss_scale=8, train_wall=460, gb_free=10, wall=58755
2022-02-11 13:07:13 | INFO | train_inner | epoch 008:   1515 / 1576 loss=6.062, ppl=66.83, wps=13928.4, ups=0.21, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.675, loss_scale=16, train_wall=460, gb_free=10, wall=59225
2022-02-11 13:07:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 13:11:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 13:12:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.8 | ppl 55.73 | wps 37323.7 | wpb 1021.8 | bsz 2 | num_updates 12560 | best_loss 5.8
2022-02-11 13:12:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12560 updates
2022-02-11 13:12:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint8.pt
2022-02-11 13:12:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint8.pt
2022-02-11 13:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint8.pt (epoch 8 @ 12560 updates, score 5.8) (writing took 29.192904951050878 seconds)
2022-02-11 13:12:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-11 13:12:32 | INFO | train | epoch 008 | loss 6.055 | ppl 66.5 | wps 13799 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 12560 | lr 0.000282166 | gnorm 0.646 | loss_scale 8 | train_wall 7246 | gb_free 10 | wall 59543
2022-02-11 13:12:32 | INFO | fairseq.trainer | begin training epoch 9
2022-02-11 13:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 13:15:39 | INFO | train_inner | epoch 009:     40 / 1576 loss=6.022, ppl=64.99, wps=12849.1, ups=0.2, wpb=64958.8, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.703, loss_scale=8, train_wall=460, gb_free=10, wall=59731
2022-02-11 13:23:30 | INFO | train_inner | epoch 009:    140 / 1576 loss=5.991, ppl=63.62, wps=13924.1, ups=0.21, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.686, loss_scale=8, train_wall=460, gb_free=10, wall=60201
2022-02-11 13:28:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 13:31:25 | INFO | train_inner | epoch 009:    241 / 1576 loss=5.984, ppl=63.3, wps=13786.6, ups=0.21, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.682, loss_scale=8, train_wall=464, gb_free=10, wall=60677
2022-02-11 13:39:15 | INFO | train_inner | epoch 009:    341 / 1576 loss=5.999, ppl=63.94, wps=13944.8, ups=0.21, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.699, loss_scale=8, train_wall=459, gb_free=10, wall=61147
2022-02-11 13:47:05 | INFO | train_inner | epoch 009:    441 / 1576 loss=6.007, ppl=64.31, wps=13936, ups=0.21, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.714, loss_scale=8, train_wall=459, gb_free=10, wall=61617
2022-02-11 13:48:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 13:55:01 | INFO | train_inner | epoch 009:    542 / 1576 loss=5.992, ppl=63.64, wps=13786.3, ups=0.21, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.702, loss_scale=8, train_wall=464, gb_free=10, wall=62092
2022-02-11 14:02:51 | INFO | train_inner | epoch 009:    642 / 1576 loss=6.01, ppl=64.43, wps=13922.8, ups=0.21, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.722, loss_scale=8, train_wall=460, gb_free=10, wall=62563
2022-02-11 14:08:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 14:10:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 14:10:52 | INFO | train_inner | epoch 009:    744 / 1576 loss=6.013, ppl=64.56, wps=13642.1, ups=0.21, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.714, loss_scale=4, train_wall=469, gb_free=10, wall=63043
2022-02-11 14:18:43 | INFO | train_inner | epoch 009:    844 / 1576 loss=6.012, ppl=64.52, wps=13914.9, ups=0.21, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.743, loss_scale=4, train_wall=460, gb_free=10, wall=63514
2022-02-11 14:26:33 | INFO | train_inner | epoch 009:    944 / 1576 loss=6.015, ppl=64.67, wps=13919.1, ups=0.21, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.724, loss_scale=4, train_wall=460, gb_free=10, wall=63985
2022-02-11 14:34:24 | INFO | train_inner | epoch 009:   1044 / 1576 loss=6.021, ppl=64.93, wps=13923, ups=0.21, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.729, loss_scale=8, train_wall=460, gb_free=10, wall=64456
2022-02-11 14:42:15 | INFO | train_inner | epoch 009:   1144 / 1576 loss=6.022, ppl=64.98, wps=13912.7, ups=0.21, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.759, loss_scale=8, train_wall=460, gb_free=10, wall=64927
2022-02-11 14:43:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 14:50:10 | INFO | train_inner | epoch 009:   1245 / 1576 loss=6.01, ppl=64.45, wps=13788.2, ups=0.21, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.762, loss_scale=4, train_wall=464, gb_free=10, wall=65402
2022-02-11 14:58:00 | INFO | train_inner | epoch 009:   1345 / 1576 loss=6.016, ppl=64.7, wps=13955.1, ups=0.21, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.745, loss_scale=4, train_wall=459, gb_free=10, wall=65872
2022-02-11 15:05:48 | INFO | train_inner | epoch 009:   1445 / 1576 loss=6.024, ppl=65.09, wps=14009.4, ups=0.21, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.764, loss_scale=8, train_wall=457, gb_free=10, wall=66340
2022-02-11 15:13:36 | INFO | train_inner | epoch 009:   1545 / 1576 loss=6.02, ppl=64.89, wps=13998.6, ups=0.21, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.755, loss_scale=8, train_wall=458, gb_free=10, wall=66808
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/cross_entropy.py", line 35, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 328, in extract_features_scriptable
    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 204616362: <w103_fp16_cross_entropy_#2> in cluster <euler> Done

Job <w103_fp16_cross_entropy_#2> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 16:42:00 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 17:39:14 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 17:39:14 2022
Terminated at Mon Feb 14 11:34:59 2022
Results reported at Mon Feb 14 11:34:59 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 6658482 --fp16 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   237225.56 sec.
    Max Memory :                                 17974 MB
    Average Memory :                             3049.13 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               2026.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   237346 sec.
    Turnaround time :                            240779 sec.

The output (if any) follows:

2022-02-11 17:39:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6658482, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6658482, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 17:39:21 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 17:39:26 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 17:39:26 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 17:39:26 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 17:39:26 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-11 17:39:26 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 17:39:26 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 17:39:26 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 17:39:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 17:39:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:30 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 17:39:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 17:39:30 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 17:39:30 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint_last.pt
2022-02-11 17:39:30 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint_last.pt
2022-02-11 17:39:30 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 17:39:31 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 17:39:31 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 17:39:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 17:39:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 17:39:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 17:40:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 17:40:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 17:40:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 17:48:27 | INFO | train_inner | epoch 001:    105 / 1576 loss=17.845, ppl=235414, wps=13738.6, ups=0.21, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.646, loss_scale=4, train_wall=522, gb_free=10, wall=536
2022-02-11 17:56:19 | INFO | train_inner | epoch 001:    205 / 1576 loss=15.348, ppl=41694.2, wps=13860.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.755, loss_scale=4, train_wall=462, gb_free=10, wall=1009
2022-02-11 18:04:12 | INFO | train_inner | epoch 001:    305 / 1576 loss=12.949, ppl=7906.47, wps=13872.6, ups=0.21, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.268, loss_scale=8, train_wall=462, gb_free=10, wall=1482
2022-02-11 18:12:04 | INFO | train_inner | epoch 001:    405 / 1576 loss=11.036, ppl=2099.28, wps=13872.1, ups=0.21, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.707, loss_scale=8, train_wall=462, gb_free=10, wall=1954
2022-02-11 18:19:56 | INFO | train_inner | epoch 001:    505 / 1576 loss=10.221, ppl=1193.64, wps=13885.3, ups=0.21, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.529, loss_scale=8, train_wall=461, gb_free=10, wall=2426
2022-02-11 18:27:48 | INFO | train_inner | epoch 001:    605 / 1576 loss=9.843, ppl=918.38, wps=13886.6, ups=0.21, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.545, loss_scale=16, train_wall=461, gb_free=10, wall=2898
2022-02-11 18:35:40 | INFO | train_inner | epoch 001:    705 / 1576 loss=9.54, ppl=744.4, wps=13881.3, ups=0.21, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.693, loss_scale=16, train_wall=462, gb_free=10, wall=3370
2022-02-11 18:43:32 | INFO | train_inner | epoch 001:    805 / 1576 loss=9.281, ppl=622.09, wps=13888.6, ups=0.21, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.705, loss_scale=32, train_wall=461, gb_free=10, wall=3842
2022-02-11 18:51:25 | INFO | train_inner | epoch 001:    905 / 1576 loss=9.056, ppl=532.16, wps=13870.7, ups=0.21, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.829, loss_scale=32, train_wall=462, gb_free=10, wall=4314
2022-02-11 18:59:17 | INFO | train_inner | epoch 001:   1005 / 1576 loss=8.838, ppl=457.54, wps=13881.5, ups=0.21, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.895, loss_scale=32, train_wall=462, gb_free=10, wall=4787
2022-02-11 19:01:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:02:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 19:07:18 | INFO | train_inner | epoch 001:   1107 / 1576 loss=8.669, ppl=407.13, wps=13625.4, ups=0.21, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.932, loss_scale=16, train_wall=470, gb_free=10, wall=5267
2022-02-11 19:15:09 | INFO | train_inner | epoch 001:   1207 / 1576 loss=8.499, ppl=361.81, wps=13894.8, ups=0.21, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.929, loss_scale=16, train_wall=461, gb_free=10, wall=5739
2022-02-11 19:23:01 | INFO | train_inner | epoch 001:   1307 / 1576 loss=8.342, ppl=324.52, wps=13892.9, ups=0.21, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.974, loss_scale=32, train_wall=461, gb_free=10, wall=6211
2022-02-11 19:29:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 19:30:58 | INFO | train_inner | epoch 001:   1408 / 1576 loss=8.212, ppl=296.54, wps=13754.7, ups=0.21, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.966, loss_scale=16, train_wall=466, gb_free=10, wall=6687
2022-02-11 19:38:49 | INFO | train_inner | epoch 001:   1508 / 1576 loss=8.085, ppl=271.56, wps=13903.5, ups=0.21, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.98, loss_scale=16, train_wall=461, gb_free=10, wall=7159
2022-02-11 19:44:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 19:44:11 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.772 | ppl 218.63 | wps 37251.2 | wpb 1021.8 | bsz 2 | num_updates 1568
2022-02-11 19:44:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1568 updates
2022-02-11 19:44:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint1.pt
2022-02-11 19:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint1.pt
2022-02-11 19:44:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint1.pt (epoch 1 @ 1568 updates, score 7.772) (writing took 29.267453578300774 seconds)
2022-02-11 19:44:41 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-11 19:44:41 | INFO | train | epoch 001 | loss 10.281 | ppl 1243.91 | wps 13783.6 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 1568 | lr 0.000196061 | gnorm 1.087 | loss_scale 16 | train_wall 7305 | gb_free 10 | wall 7510
2022-02-11 19:44:41 | INFO | fairseq.trainer | begin training epoch 2
2022-02-11 19:44:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 19:47:12 | INFO | train_inner | epoch 002:     32 / 1576 loss=7.939, ppl=245.37, wps=12926.7, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=1.004, loss_scale=16, train_wall=457, gb_free=10, wall=7661
2022-02-11 19:55:03 | INFO | train_inner | epoch 002:    132 / 1576 loss=7.811, ppl=224.6, wps=13895.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.956, loss_scale=32, train_wall=461, gb_free=10, wall=8133
2022-02-11 20:02:55 | INFO | train_inner | epoch 002:    232 / 1576 loss=7.704, ppl=208.49, wps=13882.6, ups=0.21, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.981, loss_scale=32, train_wall=461, gb_free=10, wall=8605
2022-02-11 20:10:47 | INFO | train_inner | epoch 002:    332 / 1576 loss=7.611, ppl=195.48, wps=13883.7, ups=0.21, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.933, loss_scale=64, train_wall=461, gb_free=10, wall=9077
2022-02-11 20:10:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:18:44 | INFO | train_inner | epoch 002:    433 / 1576 loss=7.522, ppl=183.78, wps=13752.9, ups=0.21, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.938, loss_scale=32, train_wall=466, gb_free=10, wall=9553
2022-02-11 20:26:35 | INFO | train_inner | epoch 002:    533 / 1576 loss=7.426, ppl=172, wps=13894, ups=0.21, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.917, loss_scale=32, train_wall=461, gb_free=10, wall=10025
2022-02-11 20:31:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:34:32 | INFO | train_inner | epoch 002:    634 / 1576 loss=7.335, ppl=161.44, wps=13758.7, ups=0.21, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.907, loss_scale=32, train_wall=466, gb_free=10, wall=10502
2022-02-11 20:42:24 | INFO | train_inner | epoch 002:    734 / 1576 loss=7.263, ppl=153.62, wps=13889, ups=0.21, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.901, loss_scale=32, train_wall=461, gb_free=10, wall=10973
2022-02-11 20:50:15 | INFO | train_inner | epoch 002:    834 / 1576 loss=7.196, ppl=146.66, wps=13890.8, ups=0.21, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.855, loss_scale=32, train_wall=461, gb_free=10, wall=11445
2022-02-11 20:51:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:58:12 | INFO | train_inner | epoch 002:    935 / 1576 loss=7.114, ppl=138.51, wps=13750.3, ups=0.21, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.864, loss_scale=32, train_wall=466, gb_free=10, wall=11922
2022-02-11 21:06:04 | INFO | train_inner | epoch 002:   1035 / 1576 loss=7.047, ppl=132.28, wps=13891.4, ups=0.21, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.853, loss_scale=32, train_wall=461, gb_free=10, wall=12394
2022-02-11 21:12:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 21:14:00 | INFO | train_inner | epoch 002:   1136 / 1576 loss=6.968, ppl=125.18, wps=13752.4, ups=0.21, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.843, loss_scale=32, train_wall=466, gb_free=10, wall=12870
2022-02-11 21:21:52 | INFO | train_inner | epoch 002:   1236 / 1576 loss=6.91, ppl=120.25, wps=13891.8, ups=0.21, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.82, loss_scale=32, train_wall=461, gb_free=10, wall=13342
2022-02-11 21:22:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 21:29:48 | INFO | train_inner | epoch 002:   1337 / 1576 loss=6.845, ppl=114.96, wps=13768.5, ups=0.21, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.836, loss_scale=16, train_wall=465, gb_free=10, wall=13818
2022-02-11 21:37:39 | INFO | train_inner | epoch 002:   1437 / 1576 loss=6.793, ppl=110.85, wps=13904.1, ups=0.21, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.807, loss_scale=16, train_wall=461, gb_free=10, wall=14289
2022-02-11 21:45:31 | INFO | train_inner | epoch 002:   1537 / 1576 loss=6.747, ppl=107.42, wps=13900.6, ups=0.21, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.797, loss_scale=32, train_wall=461, gb_free=10, wall=14761
2022-02-11 21:45:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 21:48:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 21:48:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.521 | ppl 91.83 | wps 37161.5 | wpb 1021.8 | bsz 2 | num_updates 3138 | best_loss 6.521
2022-02-11 21:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3138 updates
2022-02-11 21:48:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint2.pt
2022-02-11 21:48:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint2.pt
2022-02-11 21:49:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint2.pt (epoch 2 @ 3138 updates, score 6.521) (writing took 29.434367108158767 seconds)
2022-02-11 21:49:06 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-11 21:49:06 | INFO | train | epoch 002 | loss 7.221 | ppl 149.14 | wps 13774.9 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 3138 | lr 0.000392272 | gnorm 0.881 | loss_scale 16 | train_wall 7263 | gb_free 10 | wall 14976
2022-02-11 21:49:06 | INFO | fairseq.trainer | begin training epoch 3
2022-02-11 21:49:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 21:53:58 | INFO | train_inner | epoch 003:     62 / 1576 loss=6.654, ppl=100.73, wps=12805.7, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.819, loss_scale=16, train_wall=461, gb_free=10, wall=15268
2022-02-11 22:01:50 | INFO | train_inner | epoch 003:    162 / 1576 loss=6.589, ppl=96.26, wps=13897.4, ups=0.21, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.751, loss_scale=16, train_wall=461, gb_free=10, wall=15740
2022-02-11 22:06:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:09:46 | INFO | train_inner | epoch 003:    263 / 1576 loss=6.547, ppl=93.53, wps=13760, ups=0.21, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.78, loss_scale=16, train_wall=466, gb_free=10, wall=16216
2022-02-11 22:17:38 | INFO | train_inner | epoch 003:    363 / 1576 loss=6.51, ppl=91.16, wps=13900.3, ups=0.21, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.774, loss_scale=16, train_wall=461, gb_free=10, wall=16687
2022-02-11 22:25:29 | INFO | train_inner | epoch 003:    463 / 1576 loss=6.472, ppl=88.75, wps=13896.7, ups=0.21, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.758, loss_scale=16, train_wall=461, gb_free=10, wall=17159
2022-02-11 22:27:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:33:25 | INFO | train_inner | epoch 003:    564 / 1576 loss=6.431, ppl=86.27, wps=13766.3, ups=0.21, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.763, loss_scale=16, train_wall=465, gb_free=10, wall=17635
2022-02-11 22:41:17 | INFO | train_inner | epoch 003:    664 / 1576 loss=6.408, ppl=84.93, wps=13895.1, ups=0.21, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.776, loss_scale=16, train_wall=461, gb_free=10, wall=18107
2022-02-11 22:49:08 | INFO | train_inner | epoch 003:    764 / 1576 loss=6.355, ppl=81.83, wps=13900.5, ups=0.21, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.719, loss_scale=32, train_wall=461, gb_free=10, wall=18578
2022-02-11 22:54:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:57:05 | INFO | train_inner | epoch 003:    865 / 1576 loss=6.331, ppl=80.53, wps=13763.3, ups=0.21, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.748, loss_scale=16, train_wall=465, gb_free=10, wall=19054
2022-02-11 23:04:56 | INFO | train_inner | epoch 003:    965 / 1576 loss=6.304, ppl=79.01, wps=13899.2, ups=0.21, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.71, loss_scale=16, train_wall=461, gb_free=10, wall=19526
2022-02-11 23:12:47 | INFO | train_inner | epoch 003:   1065 / 1576 loss=6.263, ppl=76.77, wps=13900.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.695, loss_scale=16, train_wall=461, gb_free=10, wall=19997
2022-02-11 23:17:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:20:44 | INFO | train_inner | epoch 003:   1166 / 1576 loss=6.224, ppl=74.75, wps=13758.6, ups=0.21, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.69, loss_scale=16, train_wall=466, gb_free=10, wall=20473
2022-02-11 23:28:35 | INFO | train_inner | epoch 003:   1266 / 1576 loss=6.195, ppl=73.29, wps=13898.2, ups=0.21, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.674, loss_scale=16, train_wall=461, gb_free=10, wall=20945
2022-02-11 23:36:27 | INFO | train_inner | epoch 003:   1366 / 1576 loss=6.172, ppl=72.09, wps=13901, ups=0.21, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.637, loss_scale=16, train_wall=461, gb_free=10, wall=21416
2022-02-11 23:38:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:44:23 | INFO | train_inner | epoch 003:   1467 / 1576 loss=6.14, ppl=70.51, wps=13765.8, ups=0.21, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.638, loss_scale=16, train_wall=465, gb_free=10, wall=21893
2022-02-11 23:52:14 | INFO | train_inner | epoch 003:   1567 / 1576 loss=6.101, ppl=68.63, wps=13900, ups=0.21, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.646, loss_scale=16, train_wall=461, gb_free=10, wall=22364
2022-02-11 23:52:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 23:52:59 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.946 | ppl 61.66 | wps 37121 | wpb 1021.8 | bsz 2 | num_updates 4709 | best_loss 5.946
2022-02-11 23:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4709 updates
2022-02-11 23:52:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint3.pt
2022-02-11 23:53:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint3.pt
2022-02-11 23:53:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint3.pt (epoch 3 @ 4709 updates, score 5.946) (writing took 29.81818732060492 seconds)
2022-02-11 23:53:28 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-11 23:53:28 | INFO | train | epoch 003 | loss 6.346 | ppl 81.37 | wps 13789 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 4709 | lr 0.000460825 | gnorm 0.722 | loss_scale 16 | train_wall 7260 | gb_free 10 | wall 22438
2022-02-11 23:53:29 | INFO | fairseq.trainer | begin training epoch 4
2022-02-11 23:53:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 00:00:37 | INFO | train_inner | epoch 004:     91 / 1576 loss=5.979, ppl=63.05, wps=12914.7, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.643, loss_scale=32, train_wall=457, gb_free=10, wall=22867
2022-02-12 00:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:08:34 | INFO | train_inner | epoch 004:    192 / 1576 loss=5.976, ppl=62.94, wps=13763.6, ups=0.21, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.642, loss_scale=16, train_wall=465, gb_free=10, wall=23343
2022-02-12 00:16:25 | INFO | train_inner | epoch 004:    292 / 1576 loss=5.958, ppl=62.15, wps=13898.5, ups=0.21, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.605, loss_scale=16, train_wall=461, gb_free=10, wall=23815
2022-02-12 00:24:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:24:21 | INFO | train_inner | epoch 004:    393 / 1576 loss=5.935, ppl=61.17, wps=13764.9, ups=0.21, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.625, loss_scale=16, train_wall=465, gb_free=10, wall=24291
2022-02-12 00:32:13 | INFO | train_inner | epoch 004:    493 / 1576 loss=5.925, ppl=60.74, wps=13902.1, ups=0.21, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.612, loss_scale=16, train_wall=461, gb_free=10, wall=24762
2022-02-12 00:40:04 | INFO | train_inner | epoch 004:    593 / 1576 loss=5.913, ppl=60.23, wps=13899.3, ups=0.21, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.609, loss_scale=16, train_wall=461, gb_free=10, wall=25234
2022-02-12 00:44:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:48:00 | INFO | train_inner | epoch 004:    694 / 1576 loss=5.902, ppl=59.78, wps=13764, ups=0.21, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.628, loss_scale=16, train_wall=465, gb_free=10, wall=25710
2022-02-12 00:55:52 | INFO | train_inner | epoch 004:    794 / 1576 loss=5.884, ppl=59.04, wps=13894.2, ups=0.21, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.614, loss_scale=16, train_wall=461, gb_free=10, wall=26182
2022-02-12 01:03:44 | INFO | train_inner | epoch 004:    894 / 1576 loss=5.885, ppl=59.08, wps=13895.1, ups=0.21, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.604, loss_scale=16, train_wall=461, gb_free=10, wall=26653
2022-02-12 01:10:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:11:40 | INFO | train_inner | epoch 004:    995 / 1576 loss=5.851, ppl=57.74, wps=13763.5, ups=0.21, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.595, loss_scale=16, train_wall=466, gb_free=10, wall=27129
2022-02-12 01:19:31 | INFO | train_inner | epoch 004:   1095 / 1576 loss=5.849, ppl=57.63, wps=13898.5, ups=0.21, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.592, loss_scale=16, train_wall=461, gb_free=10, wall=27601
2022-02-12 01:27:23 | INFO | train_inner | epoch 004:   1195 / 1576 loss=5.846, ppl=57.51, wps=13900.8, ups=0.21, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.581, loss_scale=16, train_wall=461, gb_free=10, wall=28072
2022-02-12 01:35:14 | INFO | train_inner | epoch 004:   1295 / 1576 loss=5.833, ppl=57, wps=13897.2, ups=0.21, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.586, loss_scale=32, train_wall=461, gb_free=10, wall=28544
2022-02-12 01:40:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:43:11 | INFO | train_inner | epoch 004:   1396 / 1576 loss=5.825, ppl=56.69, wps=13759.8, ups=0.21, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.61, loss_scale=16, train_wall=466, gb_free=10, wall=29020
2022-02-12 01:51:02 | INFO | train_inner | epoch 004:   1496 / 1576 loss=5.797, ppl=55.61, wps=13896.7, ups=0.21, wpb=65532.3, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.575, loss_scale=16, train_wall=461, gb_free=10, wall=29492
2022-02-12 01:57:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 01:57:21 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.652 | ppl 50.3 | wps 37188 | wpb 1021.8 | bsz 2 | num_updates 6280 | best_loss 5.652
2022-02-12 01:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6280 updates
2022-02-12 01:57:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint4.pt
2022-02-12 01:57:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint4.pt
2022-02-12 01:57:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint4.pt (epoch 4 @ 6280 updates, score 5.652) (writing took 28.899291614070535 seconds)
2022-02-12 01:57:50 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-12 01:57:50 | INFO | train | epoch 004 | loss 5.884 | ppl 59.06 | wps 13790.7 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 6280 | lr 0.000399043 | gnorm 0.605 | loss_scale 16 | train_wall 7260 | gb_free 10 | wall 29900
2022-02-12 01:57:50 | INFO | fairseq.trainer | begin training epoch 5
2022-02-12 01:57:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 01:59:24 | INFO | train_inner | epoch 005:     20 / 1576 loss=5.769, ppl=54.52, wps=12938.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.579, loss_scale=16, train_wall=457, gb_free=10, wall=29994
2022-02-12 02:01:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:07:20 | INFO | train_inner | epoch 005:    121 / 1576 loss=5.658, ppl=50.48, wps=13769.9, ups=0.21, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.564, loss_scale=16, train_wall=465, gb_free=10, wall=30470
2022-02-12 02:15:11 | INFO | train_inner | epoch 005:    221 / 1576 loss=5.655, ppl=50.39, wps=13909.3, ups=0.21, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.569, loss_scale=16, train_wall=461, gb_free=10, wall=30941
2022-02-12 02:22:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:23:07 | INFO | train_inner | epoch 005:    322 / 1576 loss=5.651, ppl=50.26, wps=13770.4, ups=0.21, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.583, loss_scale=16, train_wall=465, gb_free=10, wall=31417
2022-02-12 02:30:59 | INFO | train_inner | epoch 005:    422 / 1576 loss=5.66, ppl=50.55, wps=13905.6, ups=0.21, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.577, loss_scale=16, train_wall=461, gb_free=10, wall=31888
2022-02-12 02:38:50 | INFO | train_inner | epoch 005:    522 / 1576 loss=5.647, ppl=50.12, wps=13913, ups=0.21, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.566, loss_scale=16, train_wall=460, gb_free=10, wall=32359
2022-02-12 02:42:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:46:45 | INFO | train_inner | epoch 005:    623 / 1576 loss=5.653, ppl=50.32, wps=13773.3, ups=0.21, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.587, loss_scale=16, train_wall=465, gb_free=10, wall=32835
2022-02-12 02:54:37 | INFO | train_inner | epoch 005:    723 / 1576 loss=5.653, ppl=50.31, wps=13908.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.571, loss_scale=16, train_wall=461, gb_free=10, wall=33306
2022-02-12 03:02:28 | INFO | train_inner | epoch 005:    823 / 1576 loss=5.651, ppl=50.24, wps=13902.7, ups=0.21, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.558, loss_scale=16, train_wall=461, gb_free=10, wall=33778
2022-02-12 03:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:10:24 | INFO | train_inner | epoch 005:    924 / 1576 loss=5.625, ppl=49.33, wps=13772.4, ups=0.21, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.563, loss_scale=16, train_wall=465, gb_free=10, wall=34253
2022-02-12 03:18:15 | INFO | train_inner | epoch 005:   1024 / 1576 loss=5.626, ppl=49.39, wps=13907.5, ups=0.21, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.582, loss_scale=16, train_wall=461, gb_free=10, wall=34725
2022-02-12 03:23:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:26:11 | INFO | train_inner | epoch 005:   1125 / 1576 loss=5.62, ppl=49.18, wps=13768.2, ups=0.21, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.551, loss_scale=16, train_wall=465, gb_free=10, wall=35201
2022-02-12 03:34:02 | INFO | train_inner | epoch 005:   1225 / 1576 loss=5.622, ppl=49.26, wps=13907.1, ups=0.21, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.572, loss_scale=16, train_wall=461, gb_free=10, wall=35672
2022-02-12 03:41:53 | INFO | train_inner | epoch 005:   1325 / 1576 loss=5.618, ppl=49.11, wps=13910.1, ups=0.21, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.566, loss_scale=16, train_wall=461, gb_free=10, wall=36143
2022-02-12 03:45:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:49:49 | INFO | train_inner | epoch 005:   1426 / 1576 loss=5.618, ppl=49.11, wps=13773.1, ups=0.21, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.542, loss_scale=16, train_wall=465, gb_free=10, wall=36619
2022-02-12 03:57:40 | INFO | train_inner | epoch 005:   1526 / 1576 loss=5.609, ppl=48.82, wps=13915.1, ups=0.21, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.585, loss_scale=16, train_wall=460, gb_free=10, wall=37090
2022-02-12 04:01:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 04:01:38 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.509 | ppl 45.53 | wps 37146.1 | wpb 1021.8 | bsz 2 | num_updates 7850 | best_loss 5.509
2022-02-12 04:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7850 updates
2022-02-12 04:01:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint5.pt
2022-02-12 04:01:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint5.pt
2022-02-12 04:02:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint5.pt (epoch 5 @ 7850 updates, score 5.509) (writing took 28.864639412611723 seconds)
2022-02-12 04:02:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-12 04:02:07 | INFO | train | epoch 005 | loss 5.637 | ppl 49.78 | wps 13790.9 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 7850 | lr 0.000356915 | gnorm 0.569 | loss_scale 16 | train_wall 7255 | gb_free 10 | wall 37356
2022-02-12 04:02:07 | INFO | fairseq.trainer | begin training epoch 6
2022-02-12 04:02:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 04:06:02 | INFO | train_inner | epoch 006:     50 / 1576 loss=5.549, ppl=46.83, wps=12939, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.554, loss_scale=32, train_wall=457, gb_free=10, wall=37592
2022-02-12 04:06:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:13:58 | INFO | train_inner | epoch 006:    151 / 1576 loss=5.483, ppl=44.71, wps=13769.4, ups=0.21, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.59, loss_scale=16, train_wall=465, gb_free=10, wall=38068
2022-02-12 04:21:50 | INFO | train_inner | epoch 006:    251 / 1576 loss=5.475, ppl=44.47, wps=13903.4, ups=0.21, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.551, loss_scale=16, train_wall=461, gb_free=10, wall=38539
2022-02-12 04:29:41 | INFO | train_inner | epoch 006:    351 / 1576 loss=5.481, ppl=44.67, wps=13909.6, ups=0.21, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.545, loss_scale=32, train_wall=461, gb_free=10, wall=39010
2022-02-12 04:30:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:37:37 | INFO | train_inner | epoch 006:    452 / 1576 loss=5.489, ppl=44.9, wps=13774.7, ups=0.21, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.576, loss_scale=16, train_wall=465, gb_free=10, wall=39486
2022-02-12 04:45:28 | INFO | train_inner | epoch 006:    552 / 1576 loss=5.486, ppl=44.82, wps=13912.3, ups=0.21, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.55, loss_scale=16, train_wall=460, gb_free=10, wall=39957
2022-02-12 04:50:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:53:23 | INFO | train_inner | epoch 006:    653 / 1576 loss=5.493, ppl=45.02, wps=13775.8, ups=0.21, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.586, loss_scale=16, train_wall=465, gb_free=10, wall=40433
2022-02-12 05:01:15 | INFO | train_inner | epoch 006:    753 / 1576 loss=5.486, ppl=44.83, wps=13908, ups=0.21, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.569, loss_scale=16, train_wall=461, gb_free=10, wall=40904
2022-02-12 05:09:06 | INFO | train_inner | epoch 006:    853 / 1576 loss=5.485, ppl=44.79, wps=13909.8, ups=0.21, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.55, loss_scale=16, train_wall=461, gb_free=10, wall=41375
2022-02-12 05:11:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:17:02 | INFO | train_inner | epoch 006:    954 / 1576 loss=5.493, ppl=45.04, wps=13771.9, ups=0.21, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.564, loss_scale=16, train_wall=465, gb_free=10, wall=41851
2022-02-12 05:24:53 | INFO | train_inner | epoch 006:   1054 / 1576 loss=5.48, ppl=44.63, wps=13912.8, ups=0.21, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.544, loss_scale=16, train_wall=460, gb_free=10, wall=42322
2022-02-12 05:31:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:32:48 | INFO | train_inner | epoch 006:   1155 / 1576 loss=5.484, ppl=44.76, wps=13774.2, ups=0.21, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.558, loss_scale=16, train_wall=465, gb_free=10, wall=42798
2022-02-12 05:40:40 | INFO | train_inner | epoch 006:   1255 / 1576 loss=5.491, ppl=44.97, wps=13907.3, ups=0.21, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.547, loss_scale=16, train_wall=461, gb_free=10, wall=43269
2022-02-12 05:48:31 | INFO | train_inner | epoch 006:   1355 / 1576 loss=5.492, ppl=45.01, wps=13911, ups=0.21, wpb=65532.3, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.579, loss_scale=16, train_wall=460, gb_free=10, wall=43740
2022-02-12 05:53:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:56:27 | INFO | train_inner | epoch 006:   1456 / 1576 loss=5.479, ppl=44.6, wps=13769, ups=0.21, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.576, loss_scale=16, train_wall=465, gb_free=10, wall=44216
2022-02-12 06:04:18 | INFO | train_inner | epoch 006:   1556 / 1576 loss=5.474, ppl=44.43, wps=13904.4, ups=0.21, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.549, loss_scale=16, train_wall=461, gb_free=10, wall=44688
2022-02-12 06:05:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 06:05:54 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.404 | ppl 42.35 | wps 37207.1 | wpb 1021.8 | bsz 2 | num_updates 9420 | best_loss 5.404
2022-02-12 06:05:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9420 updates
2022-02-12 06:05:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint6.pt
2022-02-12 06:06:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint6.pt
2022-02-12 06:06:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint6.pt (epoch 6 @ 9420 updates, score 5.404) (writing took 28.684282695874572 seconds)
2022-02-12 06:06:23 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-12 06:06:23 | INFO | train | epoch 006 | loss 5.484 | ppl 44.77 | wps 13791.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 9420 | lr 0.000325818 | gnorm 0.563 | loss_scale 16 | train_wall 7254 | gb_free 10 | wall 44812
2022-02-12 06:06:23 | INFO | fairseq.trainer | begin training epoch 7
2022-02-12 06:06:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 06:12:40 | INFO | train_inner | epoch 007:     80 / 1576 loss=5.376, ppl=41.52, wps=12945.1, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.591, loss_scale=16, train_wall=456, gb_free=10, wall=45190
2022-02-12 06:18:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 06:20:36 | INFO | train_inner | epoch 007:    181 / 1576 loss=5.349, ppl=40.75, wps=13767.6, ups=0.21, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.569, loss_scale=16, train_wall=465, gb_free=10, wall=45666
2022-02-12 06:28:27 | INFO | train_inner | epoch 007:    281 / 1576 loss=5.38, ppl=41.65, wps=13912.5, ups=0.21, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.575, loss_scale=16, train_wall=460, gb_free=10, wall=46137
2022-02-12 06:36:18 | INFO | train_inner | epoch 007:    381 / 1576 loss=5.37, ppl=41.35, wps=13901.2, ups=0.21, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.549, loss_scale=16, train_wall=461, gb_free=10, wall=46608
2022-02-12 06:41:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 06:44:14 | INFO | train_inner | epoch 007:    482 / 1576 loss=5.362, ppl=41.12, wps=13766.6, ups=0.21, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.55, loss_scale=16, train_wall=465, gb_free=10, wall=47084
2022-02-12 06:52:06 | INFO | train_inner | epoch 007:    582 / 1576 loss=5.393, ppl=42.02, wps=13907.6, ups=0.21, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.587, loss_scale=16, train_wall=461, gb_free=10, wall=47555
2022-02-12 06:59:57 | INFO | train_inner | epoch 007:    682 / 1576 loss=5.374, ppl=41.48, wps=13906.8, ups=0.21, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.57, loss_scale=16, train_wall=461, gb_free=10, wall=48027
2022-02-12 07:02:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:07:53 | INFO | train_inner | epoch 007:    783 / 1576 loss=5.376, ppl=41.52, wps=13766, ups=0.21, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.556, loss_scale=16, train_wall=465, gb_free=10, wall=48503
2022-02-12 07:15:44 | INFO | train_inner | epoch 007:    883 / 1576 loss=5.383, ppl=41.73, wps=13911.1, ups=0.21, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.574, loss_scale=16, train_wall=461, gb_free=10, wall=48974
2022-02-12 07:23:35 | INFO | train_inner | epoch 007:    983 / 1576 loss=5.378, ppl=41.58, wps=13909.8, ups=0.21, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.561, loss_scale=32, train_wall=461, gb_free=10, wall=49445
2022-02-12 07:24:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:31:31 | INFO | train_inner | epoch 007:   1084 / 1576 loss=5.381, ppl=41.67, wps=13776.7, ups=0.21, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.593, loss_scale=16, train_wall=465, gb_free=10, wall=49921
2022-02-12 07:39:22 | INFO | train_inner | epoch 007:   1184 / 1576 loss=5.397, ppl=42.14, wps=13909.7, ups=0.21, wpb=65532.3, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.563, loss_scale=16, train_wall=461, gb_free=10, wall=50392
2022-02-12 07:44:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:47:18 | INFO | train_inner | epoch 007:   1285 / 1576 loss=5.387, ppl=41.83, wps=13774.8, ups=0.21, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.557, loss_scale=16, train_wall=465, gb_free=10, wall=50868
2022-02-12 07:55:09 | INFO | train_inner | epoch 007:   1385 / 1576 loss=5.375, ppl=41.51, wps=13903.1, ups=0.21, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.559, loss_scale=16, train_wall=461, gb_free=10, wall=51339
2022-02-12 08:03:00 | INFO | train_inner | epoch 007:   1485 / 1576 loss=5.38, ppl=41.66, wps=13906.8, ups=0.21, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.547, loss_scale=16, train_wall=461, gb_free=10, wall=51810
2022-02-12 08:05:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 08:10:11 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.339 | ppl 40.48 | wps 37365.4 | wpb 1021.8 | bsz 2 | num_updates 10990 | best_loss 5.339
2022-02-12 08:10:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10990 updates
2022-02-12 08:10:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint7.pt
2022-02-12 08:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint7.pt
2022-02-12 08:10:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint7.pt (epoch 7 @ 10990 updates, score 5.339) (writing took 31.320386829786003 seconds)
2022-02-12 08:10:42 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-12 08:10:42 | INFO | train | epoch 007 | loss 5.377 | ppl 41.55 | wps 13785.2 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 10990 | lr 0.000301648 | gnorm 0.567 | loss_scale 16 | train_wall 7255 | gb_free 10 | wall 52272
2022-02-12 08:10:43 | INFO | fairseq.trainer | begin training epoch 8
2022-02-12 08:10:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 08:11:30 | INFO | train_inner | epoch 008:     10 / 1576 loss=5.366, ppl=41.24, wps=12756.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.584, loss_scale=16, train_wall=461, gb_free=10, wall=52319
2022-02-12 08:19:21 | INFO | train_inner | epoch 008:    110 / 1576 loss=5.263, ppl=38.4, wps=13904.2, ups=0.21, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.602, loss_scale=16, train_wall=461, gb_free=10, wall=52791
2022-02-12 08:26:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:27:17 | INFO | train_inner | epoch 008:    211 / 1576 loss=5.264, ppl=38.44, wps=13770.9, ups=0.21, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.574, loss_scale=16, train_wall=465, gb_free=10, wall=53267
2022-02-12 08:35:08 | INFO | train_inner | epoch 008:    311 / 1576 loss=5.278, ppl=38.8, wps=13911.3, ups=0.21, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.564, loss_scale=16, train_wall=460, gb_free=10, wall=53738
2022-02-12 08:42:59 | INFO | train_inner | epoch 008:    411 / 1576 loss=5.283, ppl=38.95, wps=13910.8, ups=0.21, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.553, loss_scale=16, train_wall=460, gb_free=10, wall=54209
2022-02-12 08:47:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:50:55 | INFO | train_inner | epoch 008:    512 / 1576 loss=5.288, ppl=39.06, wps=13773.7, ups=0.21, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.586, loss_scale=16, train_wall=465, gb_free=10, wall=54685
2022-02-12 08:58:46 | INFO | train_inner | epoch 008:    612 / 1576 loss=5.294, ppl=39.22, wps=13900.5, ups=0.21, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.575, loss_scale=16, train_wall=461, gb_free=10, wall=55156
2022-02-12 09:06:38 | INFO | train_inner | epoch 008:    712 / 1576 loss=5.298, ppl=39.34, wps=13905.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.577, loss_scale=16, train_wall=461, gb_free=10, wall=55627
2022-02-12 09:08:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 09:14:34 | INFO | train_inner | epoch 008:    813 / 1576 loss=5.303, ppl=39.48, wps=13769.5, ups=0.21, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.561, loss_scale=16, train_wall=465, gb_free=10, wall=56103
2022-02-12 09:22:25 | INFO | train_inner | epoch 008:    913 / 1576 loss=5.304, ppl=39.5, wps=13905.2, ups=0.21, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.587, loss_scale=16, train_wall=461, gb_free=10, wall=56575
2022-02-12 09:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 09:30:21 | INFO | train_inner | epoch 008:   1014 / 1576 loss=5.306, ppl=39.55, wps=13765.9, ups=0.21, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.569, loss_scale=16, train_wall=465, gb_free=10, wall=57051
2022-02-12 09:38:12 | INFO | train_inner | epoch 008:   1114 / 1576 loss=5.307, ppl=39.58, wps=13901.2, ups=0.21, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.579, loss_scale=16, train_wall=461, gb_free=10, wall=57522
2022-02-12 09:46:04 | INFO | train_inner | epoch 008:   1214 / 1576 loss=5.304, ppl=39.52, wps=13903.7, ups=0.21, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.575, loss_scale=16, train_wall=461, gb_free=10, wall=57994
2022-02-12 09:50:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 09:54:00 | INFO | train_inner | epoch 008:   1315 / 1576 loss=5.312, ppl=39.73, wps=13767.4, ups=0.21, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.586, loss_scale=16, train_wall=465, gb_free=10, wall=58470
2022-02-12 10:01:51 | INFO | train_inner | epoch 008:   1415 / 1576 loss=5.321, ppl=39.97, wps=13909.5, ups=0.21, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.564, loss_scale=16, train_wall=460, gb_free=10, wall=58941
2022-02-12 10:09:42 | INFO | train_inner | epoch 008:   1515 / 1576 loss=5.315, ppl=39.81, wps=13910, ups=0.21, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.548, loss_scale=16, train_wall=461, gb_free=10, wall=59412
2022-02-12 10:11:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 10:14:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 10:14:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.302 | ppl 39.46 | wps 37321.4 | wpb 1021.8 | bsz 2 | num_updates 12560 | best_loss 5.302
2022-02-12 10:14:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12560 updates
2022-02-12 10:14:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint8.pt
2022-02-12 10:14:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint8.pt
2022-02-12 10:15:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint8.pt (epoch 8 @ 12560 updates, score 5.302) (writing took 29.496144372969866 seconds)
2022-02-12 10:15:01 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-12 10:15:01 | INFO | train | epoch 008 | loss 5.296 | ppl 39.28 | wps 13787.3 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 12560 | lr 0.000282166 | gnorm 0.574 | loss_scale 16 | train_wall 7255 | gb_free 10 | wall 59731
2022-02-12 10:15:01 | INFO | fairseq.trainer | begin training epoch 9
2022-02-12 10:15:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 10:18:10 | INFO | train_inner | epoch 009:     40 / 1576 loss=5.253, ppl=38.13, wps=12797.3, ups=0.2, wpb=64958.8, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.614, loss_scale=16, train_wall=461, gb_free=10, wall=59919
2022-02-12 10:24:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 10:26:06 | INFO | train_inner | epoch 009:    141 / 1576 loss=5.193, ppl=36.58, wps=13764, ups=0.21, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.562, loss_scale=8, train_wall=465, gb_free=10, wall=60396
2022-02-12 10:33:57 | INFO | train_inner | epoch 009:    241 / 1576 loss=5.193, ppl=36.58, wps=13914.4, ups=0.21, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.574, loss_scale=8, train_wall=460, gb_free=10, wall=60867
2022-02-12 10:41:48 | INFO | train_inner | epoch 009:    341 / 1576 loss=5.21, ppl=37.02, wps=13917.8, ups=0.21, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.573, loss_scale=8, train_wall=460, gb_free=10, wall=61337
2022-02-12 10:49:39 | INFO | train_inner | epoch 009:    441 / 1576 loss=5.222, ppl=37.33, wps=13911.2, ups=0.21, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.567, loss_scale=16, train_wall=460, gb_free=10, wall=61809
2022-02-12 10:57:30 | INFO | train_inner | epoch 009:    541 / 1576 loss=5.211, ppl=37.05, wps=13899, ups=0.21, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.565, loss_scale=16, train_wall=461, gb_free=10, wall=62280
2022-02-12 11:05:22 | INFO | train_inner | epoch 009:    641 / 1576 loss=5.232, ppl=37.57, wps=13906.8, ups=0.21, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.587, loss_scale=32, train_wall=461, gb_free=10, wall=62751
2022-02-12 11:09:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 11:13:18 | INFO | train_inner | epoch 009:    742 / 1576 loss=5.237, ppl=37.7, wps=13768.3, ups=0.21, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.574, loss_scale=16, train_wall=465, gb_free=10, wall=63227
2022-02-12 11:21:09 | INFO | train_inner | epoch 009:    842 / 1576 loss=5.239, ppl=37.77, wps=13909.2, ups=0.21, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.567, loss_scale=16, train_wall=461, gb_free=10, wall=63699
2022-02-12 11:29:00 | INFO | train_inner | epoch 009:    942 / 1576 loss=5.244, ppl=37.9, wps=13909.3, ups=0.21, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.569, loss_scale=16, train_wall=461, gb_free=10, wall=64170
2022-02-12 11:30:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 11:36:56 | INFO | train_inner | epoch 009:   1043 / 1576 loss=5.25, ppl=38.06, wps=13772.6, ups=0.21, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.584, loss_scale=16, train_wall=465, gb_free=10, wall=64646
2022-02-12 11:44:47 | INFO | train_inner | epoch 009:   1143 / 1576 loss=5.252, ppl=38.12, wps=13909.1, ups=0.21, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.579, loss_scale=16, train_wall=461, gb_free=10, wall=65117
2022-02-12 11:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 11:52:43 | INFO | train_inner | epoch 009:   1244 / 1576 loss=5.243, ppl=37.86, wps=13773.6, ups=0.21, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.574, loss_scale=16, train_wall=465, gb_free=10, wall=65593
2022-02-12 12:00:34 | INFO | train_inner | epoch 009:   1344 / 1576 loss=5.252, ppl=38.12, wps=13912.6, ups=0.21, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.571, loss_scale=16, train_wall=460, gb_free=10, wall=66064
2022-02-12 12:08:25 | INFO | train_inner | epoch 009:   1444 / 1576 loss=5.261, ppl=38.34, wps=13901.4, ups=0.21, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.552, loss_scale=16, train_wall=461, gb_free=10, wall=66535
2022-02-12 12:11:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 12:16:21 | INFO | train_inner | epoch 009:   1545 / 1576 loss=5.26, ppl=38.33, wps=13777.1, ups=0.21, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.598, loss_scale=16, train_wall=465, gb_free=10, wall=67011
2022-02-12 12:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 12:18:49 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.262 | ppl 38.36 | wps 37298.2 | wpb 1021.8 | bsz 2 | num_updates 14131 | best_loss 5.262
2022-02-12 12:18:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14131 updates
2022-02-12 12:18:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint9.pt
2022-02-12 12:18:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint9.pt
2022-02-12 12:19:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint9.pt (epoch 9 @ 14131 updates, score 5.262) (writing took 28.45740435551852 seconds)
2022-02-12 12:19:17 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-12 12:19:17 | INFO | train | epoch 009 | loss 5.232 | ppl 37.59 | wps 13800.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 14131 | lr 0.00026602 | gnorm 0.575 | loss_scale 16 | train_wall 7255 | gb_free 10 | wall 67187
2022-02-12 12:19:18 | INFO | fairseq.trainer | begin training epoch 10
2022-02-12 12:19:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 12:24:43 | INFO | train_inner | epoch 010:     69 / 1576 loss=5.158, ppl=35.71, wps=12949.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=14200, lr=0.000265372, gnorm=0.591, loss_scale=16, train_wall=457, gb_free=10, wall=67512
2022-02-12 12:32:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 12:32:39 | INFO | train_inner | epoch 010:    170 / 1576 loss=5.13, ppl=35.03, wps=13755.4, ups=0.21, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.583, loss_scale=16, train_wall=466, gb_free=10, wall=67989
2022-02-12 12:40:30 | INFO | train_inner | epoch 010:    270 / 1576 loss=5.155, ppl=35.64, wps=13902.9, ups=0.21, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.597, loss_scale=16, train_wall=461, gb_free=10, wall=68460
2022-02-12 12:48:22 | INFO | train_inner | epoch 010:    370 / 1576 loss=5.148, ppl=35.45, wps=13903.1, ups=0.21, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.588, loss_scale=16, train_wall=461, gb_free=10, wall=68932
2022-02-12 12:56:13 | INFO | train_inner | epoch 010:    470 / 1576 loss=5.162, ppl=35.8, wps=13905.7, ups=0.21, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.581, loss_scale=32, train_wall=461, gb_free=10, wall=69403
2022-02-12 12:56:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 13:04:09 | INFO | train_inner | epoch 010:    571 / 1576 loss=5.175, ppl=36.13, wps=13771, ups=0.21, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.561, loss_scale=16, train_wall=465, gb_free=10, wall=69879
2022-02-12 13:12:01 | INFO | train_inner | epoch 010:    671 / 1576 loss=5.174, ppl=36.1, wps=13900.1, ups=0.21, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.588, loss_scale=16, train_wall=461, gb_free=10, wall=70350
2022-02-12 13:17:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 13:19:57 | INFO | train_inner | epoch 010:    772 / 1576 loss=5.183, ppl=36.33, wps=13767.4, ups=0.21, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.572, loss_scale=16, train_wall=465, gb_free=10, wall=70826
2022-02-12 13:27:48 | INFO | train_inner | epoch 010:    872 / 1576 loss=5.201, ppl=36.79, wps=13906.5, ups=0.21, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.612, loss_scale=16, train_wall=461, gb_free=10, wall=71298
2022-02-12 13:35:39 | INFO | train_inner | epoch 010:    972 / 1576 loss=5.202, ppl=36.8, wps=13903.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.558, loss_scale=16, train_wall=461, gb_free=10, wall=71769
2022-02-12 13:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 13:43:35 | INFO | train_inner | epoch 010:   1073 / 1576 loss=5.197, ppl=36.68, wps=13765.5, ups=0.21, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.59, loss_scale=16, train_wall=465, gb_free=10, wall=72245
2022-02-12 13:44:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:51:31 | INFO | train_inner | epoch 010:   1174 / 1576 loss=5.205, ppl=36.89, wps=13775.5, ups=0.21, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.635, loss_scale=8, train_wall=465, gb_free=10, wall=72721
2022-02-12 13:59:22 | INFO | train_inner | epoch 010:   1274 / 1576 loss=5.201, ppl=36.77, wps=13907.7, ups=0.21, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.616, loss_scale=8, train_wall=461, gb_free=10, wall=73192
2022-02-12 14:07:13 | INFO | train_inner | epoch 010:   1374 / 1576 loss=5.205, ppl=36.88, wps=13908.5, ups=0.21, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.579, loss_scale=16, train_wall=461, gb_free=10, wall=73663
2022-02-12 14:15:05 | INFO | train_inner | epoch 010:   1474 / 1576 loss=5.201, ppl=36.79, wps=13899.5, ups=0.21, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.604, loss_scale=16, train_wall=461, gb_free=10, wall=74135
2022-02-12 14:22:56 | INFO | train_inner | epoch 010:   1574 / 1576 loss=5.21, ppl=37.01, wps=13900.4, ups=0.21, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.567, loss_scale=16, train_wall=461, gb_free=10, wall=74606
2022-02-12 14:23:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 14:23:08 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.247 | ppl 37.97 | wps 37198.3 | wpb 1021.8 | bsz 2 | num_updates 15702 | best_loss 5.247
2022-02-12 14:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15702 updates
2022-02-12 14:23:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint10.pt
2022-02-12 14:23:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint10.pt
2022-02-12 14:23:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint10.pt (epoch 10 @ 15702 updates, score 5.247) (writing took 28.343138330616057 seconds)
2022-02-12 14:23:36 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-12 14:23:36 | INFO | train | epoch 010 | loss 5.181 | ppl 36.27 | wps 13796.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 15702 | lr 0.000252361 | gnorm 0.589 | loss_scale 16 | train_wall 7257 | gb_free 10 | wall 74646
2022-02-12 14:23:36 | INFO | fairseq.trainer | begin training epoch 11
2022-02-12 14:23:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 14:25:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 14:31:23 | INFO | train_inner | epoch 011:     99 / 1576 loss=5.089, ppl=34.04, wps=12827.1, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=15800, lr=0.000251577, gnorm=0.616, loss_scale=16, train_wall=461, gb_free=10, wall=75113
2022-02-12 14:39:14 | INFO | train_inner | epoch 011:    199 / 1576 loss=5.094, ppl=34.15, wps=13904.2, ups=0.21, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.59, loss_scale=16, train_wall=461, gb_free=10, wall=75584
2022-02-12 14:46:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 14:47:10 | INFO | train_inner | epoch 011:    300 / 1576 loss=5.093, ppl=34.14, wps=13764.9, ups=0.21, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.569, loss_scale=16, train_wall=465, gb_free=10, wall=76060
2022-02-12 14:55:02 | INFO | train_inner | epoch 011:    400 / 1576 loss=5.113, ppl=34.6, wps=13907.2, ups=0.21, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.584, loss_scale=16, train_wall=461, gb_free=10, wall=76531
2022-02-12 15:02:53 | INFO | train_inner | epoch 011:    500 / 1576 loss=5.132, ppl=35.07, wps=13897.7, ups=0.21, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.608, loss_scale=16, train_wall=461, gb_free=10, wall=77003
2022-02-12 15:06:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 15:10:49 | INFO | train_inner | epoch 011:    601 / 1576 loss=5.119, ppl=34.76, wps=13768, ups=0.21, wpb=65532.3, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.568, loss_scale=16, train_wall=465, gb_free=10, wall=77479
2022-02-12 15:18:41 | INFO | train_inner | epoch 011:    701 / 1576 loss=5.146, ppl=35.4, wps=13896.2, ups=0.21, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.615, loss_scale=16, train_wall=461, gb_free=10, wall=77950
2022-02-12 15:26:32 | INFO | train_inner | epoch 011:    801 / 1576 loss=5.14, ppl=35.27, wps=13900.2, ups=0.21, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.589, loss_scale=16, train_wall=461, gb_free=10, wall=78422
2022-02-12 15:28:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 15:33:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:34:33 | INFO | train_inner | epoch 011:    903 / 1576 loss=5.14, ppl=35.26, wps=13630.4, ups=0.21, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.592, loss_scale=8, train_wall=470, gb_free=10, wall=78903
2022-02-12 15:42:24 | INFO | train_inner | epoch 011:   1003 / 1576 loss=5.159, ppl=35.73, wps=13910.9, ups=0.21, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.592, loss_scale=8, train_wall=460, gb_free=10, wall=79374
2022-02-12 15:50:15 | INFO | train_inner | epoch 011:   1103 / 1576 loss=5.161, ppl=35.77, wps=13905.7, ups=0.21, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.579, loss_scale=8, train_wall=461, gb_free=10, wall=79845
2022-02-12 15:58:07 | INFO | train_inner | epoch 011:   1203 / 1576 loss=5.166, ppl=35.89, wps=13899.5, ups=0.21, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.587, loss_scale=16, train_wall=461, gb_free=10, wall=80317
2022-02-12 16:05:58 | INFO | train_inner | epoch 011:   1303 / 1576 loss=5.17, ppl=36.01, wps=13902.6, ups=0.21, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.639, loss_scale=16, train_wall=461, gb_free=10, wall=80788
2022-02-12 16:13:50 | INFO | train_inner | epoch 011:   1403 / 1576 loss=5.166, ppl=35.91, wps=13900.2, ups=0.21, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.599, loss_scale=32, train_wall=461, gb_free=10, wall=81259
2022-02-12 16:13:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 16:21:46 | INFO | train_inner | epoch 011:   1504 / 1576 loss=5.159, ppl=35.72, wps=13766.4, ups=0.21, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.559, loss_scale=16, train_wall=465, gb_free=10, wall=81735
2022-02-12 16:26:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 16:27:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 16:27:27 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.214 | ppl 37.12 | wps 37216.9 | wpb 1021.8 | bsz 2 | num_updates 17271 | best_loss 5.214
2022-02-12 16:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17271 updates
2022-02-12 16:27:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint11.pt
2022-02-12 16:27:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint11.pt
2022-02-12 16:27:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint11.pt (epoch 11 @ 17271 updates, score 5.214) (writing took 28.652240063995123 seconds)
2022-02-12 16:27:55 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-12 16:27:55 | INFO | train | epoch 011 | loss 5.138 | ppl 35.2 | wps 13776.9 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 17271 | lr 0.000240625 | gnorm 0.593 | loss_scale 8 | train_wall 7257 | gb_free 10 | wall 82105
2022-02-12 16:27:56 | INFO | fairseq.trainer | begin training epoch 12
2022-02-12 16:27:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 16:30:12 | INFO | train_inner | epoch 012:     29 / 1576 loss=5.133, ppl=35.1, wps=12828.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=17300, lr=0.000240424, gnorm=0.607, loss_scale=8, train_wall=461, gb_free=10, wall=82242
2022-02-12 16:38:03 | INFO | train_inner | epoch 012:    129 / 1576 loss=5.047, ppl=33.07, wps=13911.1, ups=0.21, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.578, loss_scale=8, train_wall=460, gb_free=10, wall=82713
2022-02-12 16:45:54 | INFO | train_inner | epoch 012:    229 / 1576 loss=5.066, ppl=33.5, wps=13916.6, ups=0.21, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.612, loss_scale=8, train_wall=460, gb_free=10, wall=83184
2022-02-12 16:53:46 | INFO | train_inner | epoch 012:    329 / 1576 loss=5.07, ppl=33.6, wps=13900.7, ups=0.21, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.59, loss_scale=16, train_wall=461, gb_free=10, wall=83655
2022-02-12 17:01:37 | INFO | train_inner | epoch 012:    429 / 1576 loss=5.077, ppl=33.76, wps=13895, ups=0.21, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.599, loss_scale=16, train_wall=461, gb_free=10, wall=84127
2022-02-12 17:01:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 17:09:33 | INFO | train_inner | epoch 012:    530 / 1576 loss=5.077, ppl=33.76, wps=13771.5, ups=0.21, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.603, loss_scale=8, train_wall=465, gb_free=10, wall=84603
2022-02-12 17:17:24 | INFO | train_inner | epoch 012:    630 / 1576 loss=5.114, ppl=34.64, wps=13913.4, ups=0.21, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.619, loss_scale=8, train_wall=460, gb_free=10, wall=85074
2022-02-12 17:25:15 | INFO | train_inner | epoch 012:    730 / 1576 loss=5.096, ppl=34.21, wps=13909.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.595, loss_scale=16, train_wall=461, gb_free=10, wall=85545
2022-02-12 17:33:07 | INFO | train_inner | epoch 012:    830 / 1576 loss=5.097, ppl=34.23, wps=13908.5, ups=0.21, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.601, loss_scale=16, train_wall=461, gb_free=10, wall=86016
2022-02-12 17:40:58 | INFO | train_inner | epoch 012:    930 / 1576 loss=5.107, ppl=34.47, wps=13901.4, ups=0.21, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.601, loss_scale=16, train_wall=461, gb_free=10, wall=86488
2022-02-12 17:42:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 17:48:54 | INFO | train_inner | epoch 012:   1031 / 1576 loss=5.116, ppl=34.68, wps=13771.4, ups=0.21, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.585, loss_scale=16, train_wall=465, gb_free=10, wall=86964
2022-02-12 17:56:45 | INFO | train_inner | epoch 012:   1131 / 1576 loss=5.12, ppl=34.77, wps=13900.8, ups=0.21, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.595, loss_scale=16, train_wall=461, gb_free=10, wall=87435
2022-02-12 18:03:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 18:04:41 | INFO | train_inner | epoch 012:   1232 / 1576 loss=5.129, ppl=35, wps=13768.7, ups=0.21, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.589, loss_scale=16, train_wall=465, gb_free=10, wall=87911
2022-02-12 18:12:33 | INFO | train_inner | epoch 012:   1332 / 1576 loss=5.132, ppl=35.07, wps=13900, ups=0.21, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.592, loss_scale=16, train_wall=461, gb_free=10, wall=88382
2022-02-12 18:20:24 | INFO | train_inner | epoch 012:   1432 / 1576 loss=5.127, ppl=34.94, wps=13902.5, ups=0.21, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.602, loss_scale=16, train_wall=461, gb_free=10, wall=88854
2022-02-12 18:23:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 18:28:20 | INFO | train_inner | epoch 012:   1533 / 1576 loss=5.132, ppl=35.06, wps=13767.8, ups=0.21, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.576, loss_scale=16, train_wall=465, gb_free=10, wall=89330
2022-02-12 18:31:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 18:31:45 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.192 | ppl 36.54 | wps 37332.4 | wpb 1021.8 | bsz 2 | num_updates 18843 | best_loss 5.192
2022-02-12 18:31:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18843 updates
2022-02-12 18:31:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint12.pt
2022-02-12 18:31:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint12.pt
2022-02-12 18:32:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint12.pt (epoch 12 @ 18843 updates, score 5.192) (writing took 28.48722311295569 seconds)
2022-02-12 18:32:13 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-12 18:32:13 | INFO | train | epoch 012 | loss 5.101 | ppl 34.31 | wps 13806.6 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 18843 | lr 0.000230369 | gnorm 0.596 | loss_scale 16 | train_wall 7256 | gb_free 10 | wall 89563
2022-02-12 18:32:13 | INFO | fairseq.trainer | begin training epoch 13
2022-02-12 18:32:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 18:36:42 | INFO | train_inner | epoch 013:     57 / 1576 loss=5.063, ppl=33.42, wps=12951.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=18900, lr=0.000230022, gnorm=0.615, loss_scale=16, train_wall=456, gb_free=10, wall=89831
2022-02-12 18:44:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 18:44:38 | INFO | train_inner | epoch 013:    158 / 1576 loss=5.026, ppl=32.59, wps=13763, ups=0.21, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.583, loss_scale=16, train_wall=465, gb_free=10, wall=90308
2022-02-12 18:52:29 | INFO | train_inner | epoch 013:    258 / 1576 loss=5.032, ppl=32.71, wps=13906.5, ups=0.21, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.577, loss_scale=16, train_wall=461, gb_free=10, wall=90779
2022-02-12 19:00:21 | INFO | train_inner | epoch 013:    358 / 1576 loss=5.039, ppl=32.88, wps=13903.8, ups=0.21, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.606, loss_scale=16, train_wall=461, gb_free=10, wall=91250
2022-02-12 19:05:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 19:08:17 | INFO | train_inner | epoch 013:    459 / 1576 loss=5.047, ppl=33.07, wps=13767, ups=0.21, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.59, loss_scale=16, train_wall=465, gb_free=10, wall=91726
2022-02-12 19:16:08 | INFO | train_inner | epoch 013:    559 / 1576 loss=5.06, ppl=33.35, wps=13901.6, ups=0.21, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.58, loss_scale=16, train_wall=461, gb_free=10, wall=92198
2022-02-12 19:24:00 | INFO | train_inner | epoch 013:    659 / 1576 loss=5.068, ppl=33.54, wps=13898.6, ups=0.21, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.594, loss_scale=16, train_wall=461, gb_free=10, wall=92669
2022-02-12 19:25:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 19:25:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:32:00 | INFO | train_inner | epoch 013:    761 / 1576 loss=5.075, ppl=33.71, wps=13634.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.643, loss_scale=8, train_wall=470, gb_free=10, wall=93150
2022-02-12 19:39:51 | INFO | train_inner | epoch 013:    861 / 1576 loss=5.065, ppl=33.48, wps=13920.4, ups=0.21, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.604, loss_scale=8, train_wall=460, gb_free=10, wall=93621
2022-02-12 19:47:42 | INFO | train_inner | epoch 013:    961 / 1576 loss=5.078, ppl=33.79, wps=13907, ups=0.21, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.596, loss_scale=16, train_wall=461, gb_free=10, wall=94092
2022-02-12 19:55:33 | INFO | train_inner | epoch 013:   1061 / 1576 loss=5.094, ppl=34.16, wps=13910.5, ups=0.21, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.613, loss_scale=16, train_wall=461, gb_free=10, wall=94563
2022-02-12 20:03:25 | INFO | train_inner | epoch 013:   1161 / 1576 loss=5.099, ppl=34.27, wps=13904.7, ups=0.21, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.6, loss_scale=16, train_wall=461, gb_free=10, wall=95034
2022-02-12 20:07:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 20:11:21 | INFO | train_inner | epoch 013:   1262 / 1576 loss=5.079, ppl=33.8, wps=13767.5, ups=0.21, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.61, loss_scale=16, train_wall=465, gb_free=10, wall=95510
2022-02-12 20:19:12 | INFO | train_inner | epoch 013:   1362 / 1576 loss=5.102, ppl=34.34, wps=13908.8, ups=0.21, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.585, loss_scale=16, train_wall=461, gb_free=10, wall=95982
2022-02-12 20:27:03 | INFO | train_inner | epoch 013:   1462 / 1576 loss=5.095, ppl=34.18, wps=13903.1, ups=0.21, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.587, loss_scale=16, train_wall=461, gb_free=10, wall=96453
2022-02-12 20:28:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 20:34:59 | INFO | train_inner | epoch 013:   1563 / 1576 loss=5.104, ppl=34.4, wps=13772.6, ups=0.21, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.595, loss_scale=16, train_wall=465, gb_free=10, wall=96929
2022-02-12 20:35:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 20:36:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.174 | ppl 36.11 | wps 37081.3 | wpb 1021.8 | bsz 2 | num_updates 20413 | best_loss 5.174
2022-02-12 20:36:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20413 updates
2022-02-12 20:36:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint13.pt
2022-02-12 20:36:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint13.pt
2022-02-12 20:36:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint13.pt (epoch 13 @ 20413 updates, score 5.174) (writing took 28.27575039770454 seconds)
2022-02-12 20:36:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-12 20:36:30 | INFO | train | epoch 013 | loss 5.069 | ppl 33.57 | wps 13789.6 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 20413 | lr 0.000221333 | gnorm 0.599 | loss_scale 16 | train_wall 7256 | gb_free 10 | wall 97020
2022-02-12 20:36:31 | INFO | fairseq.trainer | begin training epoch 14
2022-02-12 20:36:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 20:43:21 | INFO | train_inner | epoch 014:     87 / 1576 loss=5.005, ppl=32.11, wps=12952.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=20500, lr=0.000220863, gnorm=0.631, loss_scale=16, train_wall=457, gb_free=10, wall=97430
2022-02-12 20:49:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 20:51:17 | INFO | train_inner | epoch 014:    188 / 1576 loss=4.991, ppl=31.79, wps=13763.1, ups=0.21, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.609, loss_scale=16, train_wall=465, gb_free=10, wall=97906
2022-02-12 20:58:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 20:59:13 | INFO | train_inner | epoch 014:    289 / 1576 loss=5.013, ppl=32.3, wps=13772.5, ups=0.21, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.609, loss_scale=8, train_wall=465, gb_free=10, wall=98382
2022-02-12 21:07:04 | INFO | train_inner | epoch 014:    389 / 1576 loss=5.011, ppl=32.25, wps=13910.4, ups=0.21, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.595, loss_scale=8, train_wall=461, gb_free=10, wall=98853
2022-02-12 21:14:55 | INFO | train_inner | epoch 014:    489 / 1576 loss=5.02, ppl=32.44, wps=13908.3, ups=0.21, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.594, loss_scale=8, train_wall=461, gb_free=10, wall=99325
2022-02-12 21:22:46 | INFO | train_inner | epoch 014:    589 / 1576 loss=5.036, ppl=32.82, wps=13909.6, ups=0.21, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.599, loss_scale=16, train_wall=461, gb_free=10, wall=99796
2022-02-12 21:30:38 | INFO | train_inner | epoch 014:    689 / 1576 loss=5.039, ppl=32.88, wps=13903.4, ups=0.21, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.615, loss_scale=16, train_wall=461, gb_free=10, wall=100267
2022-02-12 21:38:29 | INFO | train_inner | epoch 014:    789 / 1576 loss=5.042, ppl=32.95, wps=13903.9, ups=0.21, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.594, loss_scale=32, train_wall=461, gb_free=10, wall=100739
2022-02-12 21:38:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 21:46:25 | INFO | train_inner | epoch 014:    890 / 1576 loss=5.054, ppl=33.21, wps=13773.1, ups=0.21, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.613, loss_scale=16, train_wall=465, gb_free=10, wall=101214
2022-02-12 21:54:16 | INFO | train_inner | epoch 014:    990 / 1576 loss=5.055, ppl=33.25, wps=13906.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.611, loss_scale=16, train_wall=461, gb_free=10, wall=101686
2022-02-12 21:58:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 22:01:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 22:02:16 | INFO | train_inner | epoch 014:   1092 / 1576 loss=5.052, ppl=33.17, wps=13638.8, ups=0.21, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.61, loss_scale=8, train_wall=470, gb_free=10, wall=102166
2022-02-12 22:10:08 | INFO | train_inner | epoch 014:   1192 / 1576 loss=5.057, ppl=33.29, wps=13912.1, ups=0.21, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.612, loss_scale=8, train_wall=461, gb_free=10, wall=102637
2022-02-12 22:17:59 | INFO | train_inner | epoch 014:   1292 / 1576 loss=5.076, ppl=33.74, wps=13912.1, ups=0.21, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.594, loss_scale=8, train_wall=461, gb_free=10, wall=103108
2022-02-12 22:25:50 | INFO | train_inner | epoch 014:   1392 / 1576 loss=5.076, ppl=33.73, wps=13909.6, ups=0.21, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.589, loss_scale=16, train_wall=461, gb_free=10, wall=103579
2022-02-12 22:26:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 22:33:45 | INFO | train_inner | epoch 014:   1493 / 1576 loss=5.072, ppl=33.63, wps=13779, ups=0.21, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.628, loss_scale=8, train_wall=465, gb_free=10, wall=104055
2022-02-12 22:40:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 22:40:18 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.167 | ppl 35.92 | wps 37212.4 | wpb 1021.8 | bsz 2 | num_updates 21983 | best_loss 5.167
2022-02-12 22:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21983 updates
2022-02-12 22:40:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint14.pt
2022-02-12 22:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint14.pt
2022-02-12 22:40:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint14.pt (epoch 14 @ 21983 updates, score 5.167) (writing took 28.480951787903905 seconds)
2022-02-12 22:40:47 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-12 22:40:47 | INFO | train | epoch 014 | loss 5.041 | ppl 32.92 | wps 13791.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 21983 | lr 0.000213283 | gnorm 0.607 | loss_scale 8 | train_wall 7255 | gb_free 10 | wall 104476
2022-02-12 22:40:47 | INFO | fairseq.trainer | begin training epoch 15
2022-02-12 22:40:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 22:42:07 | INFO | train_inner | epoch 015:     17 / 1576 loss=5.054, ppl=33.22, wps=12954.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=22000, lr=0.000213201, gnorm=0.611, loss_scale=8, train_wall=456, gb_free=10, wall=104557
2022-02-12 22:49:58 | INFO | train_inner | epoch 015:    117 / 1576 loss=4.956, ppl=31.03, wps=13910.5, ups=0.21, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.611, loss_scale=16, train_wall=460, gb_free=10, wall=105028
2022-02-12 22:57:49 | INFO | train_inner | epoch 015:    217 / 1576 loss=4.968, ppl=31.31, wps=13904.3, ups=0.21, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.592, loss_scale=16, train_wall=461, gb_free=10, wall=105499
2022-02-12 23:05:41 | INFO | train_inner | epoch 015:    317 / 1576 loss=4.993, ppl=31.85, wps=13902, ups=0.21, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.617, loss_scale=16, train_wall=461, gb_free=10, wall=105970
2022-02-12 23:07:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 23:13:37 | INFO | train_inner | epoch 015:    418 / 1576 loss=4.989, ppl=31.76, wps=13766.6, ups=0.21, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.627, loss_scale=16, train_wall=465, gb_free=10, wall=106446
2022-02-12 23:21:28 | INFO | train_inner | epoch 015:    518 / 1576 loss=5.005, ppl=32.11, wps=13904.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.593, loss_scale=16, train_wall=461, gb_free=10, wall=106918
2022-02-12 23:26:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 23:29:24 | INFO | train_inner | epoch 015:    619 / 1576 loss=5.009, ppl=32.2, wps=13774.9, ups=0.21, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.643, loss_scale=8, train_wall=465, gb_free=10, wall=107394
2022-02-12 23:37:15 | INFO | train_inner | epoch 015:    719 / 1576 loss=5.022, ppl=32.49, wps=13914.7, ups=0.21, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.606, loss_scale=8, train_wall=460, gb_free=10, wall=107865
2022-02-12 23:45:06 | INFO | train_inner | epoch 015:    819 / 1576 loss=5.024, ppl=32.53, wps=13909, ups=0.21, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.617, loss_scale=8, train_wall=461, gb_free=10, wall=108336
2022-02-12 23:52:57 | INFO | train_inner | epoch 015:    919 / 1576 loss=5.045, ppl=33.02, wps=13912.8, ups=0.21, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.611, loss_scale=16, train_wall=460, gb_free=10, wall=108807
2022-02-13 00:00:48 | INFO | train_inner | epoch 015:   1019 / 1576 loss=5.032, ppl=32.72, wps=13907.3, ups=0.21, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.609, loss_scale=16, train_wall=461, gb_free=10, wall=109278
2022-02-13 00:07:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 00:08:44 | INFO | train_inner | epoch 015:   1120 / 1576 loss=5.039, ppl=32.89, wps=13774.8, ups=0.21, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.618, loss_scale=16, train_wall=465, gb_free=10, wall=109754
2022-02-13 00:14:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:16:40 | INFO | train_inner | epoch 015:   1221 / 1576 loss=5.028, ppl=32.63, wps=13777.3, ups=0.21, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.629, loss_scale=8, train_wall=465, gb_free=10, wall=110229
2022-02-13 00:24:31 | INFO | train_inner | epoch 015:   1321 / 1576 loss=5.045, ppl=33.02, wps=13912.9, ups=0.21, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.6, loss_scale=8, train_wall=460, gb_free=10, wall=110700
2022-02-13 00:32:22 | INFO | train_inner | epoch 015:   1421 / 1576 loss=5.038, ppl=32.86, wps=13914.1, ups=0.21, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.642, loss_scale=8, train_wall=460, gb_free=10, wall=111171
2022-02-13 00:40:13 | INFO | train_inner | epoch 015:   1521 / 1576 loss=5.053, ppl=33.2, wps=13911.5, ups=0.21, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.623, loss_scale=16, train_wall=461, gb_free=10, wall=111643
2022-02-13 00:44:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 00:44:34 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.156 | ppl 35.65 | wps 37316.2 | wpb 1021.8 | bsz 2 | num_updates 23555 | best_loss 5.156
2022-02-13 00:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23555 updates
2022-02-13 00:44:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint15.pt
2022-02-13 00:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint15.pt
2022-02-13 00:45:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint15.pt (epoch 15 @ 23555 updates, score 5.156) (writing took 28.415589183568954 seconds)
2022-02-13 00:45:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-13 00:45:03 | INFO | train | epoch 015 | loss 5.017 | ppl 32.38 | wps 13810 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 23555 | lr 0.000206043 | gnorm 0.616 | loss_scale 16 | train_wall 7254 | gb_free 10 | wall 111932
2022-02-13 00:45:03 | INFO | fairseq.trainer | begin training epoch 16
2022-02-13 00:45:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 00:48:35 | INFO | train_inner | epoch 016:     45 / 1576 loss=4.995, ppl=31.88, wps=12949.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=23600, lr=0.000205847, gnorm=0.618, loss_scale=16, train_wall=457, gb_free=10, wall=112144
2022-02-13 00:54:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 00:56:31 | INFO | train_inner | epoch 016:    146 / 1576 loss=4.939, ppl=30.67, wps=13762, ups=0.21, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.616, loss_scale=16, train_wall=465, gb_free=10, wall=112620
2022-02-13 01:04:22 | INFO | train_inner | epoch 016:    246 / 1576 loss=4.956, ppl=31.03, wps=13904.8, ups=0.21, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.641, loss_scale=16, train_wall=461, gb_free=10, wall=113092
2022-02-13 01:12:13 | INFO | train_inner | epoch 016:    346 / 1576 loss=4.964, ppl=31.2, wps=13904.4, ups=0.21, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.585, loss_scale=16, train_wall=461, gb_free=10, wall=113563
2022-02-13 01:14:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 01:20:09 | INFO | train_inner | epoch 016:    447 / 1576 loss=4.974, ppl=31.42, wps=13775.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.62, loss_scale=8, train_wall=465, gb_free=10, wall=114039
2022-02-13 01:28:00 | INFO | train_inner | epoch 016:    547 / 1576 loss=4.978, ppl=31.51, wps=13906.3, ups=0.21, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.599, loss_scale=8, train_wall=461, gb_free=10, wall=114510
2022-02-13 01:35:52 | INFO | train_inner | epoch 016:    647 / 1576 loss=4.99, ppl=31.78, wps=13909.6, ups=0.21, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.635, loss_scale=16, train_wall=461, gb_free=10, wall=114981
2022-02-13 01:37:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 01:43:47 | INFO | train_inner | epoch 016:    748 / 1576 loss=4.997, ppl=31.93, wps=13777, ups=0.21, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.639, loss_scale=8, train_wall=465, gb_free=10, wall=115457
2022-02-13 01:51:38 | INFO | train_inner | epoch 016:    848 / 1576 loss=5.009, ppl=32.19, wps=13909.5, ups=0.21, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.65, loss_scale=8, train_wall=461, gb_free=10, wall=115928
2022-02-13 01:59:29 | INFO | train_inner | epoch 016:    948 / 1576 loss=5.016, ppl=32.37, wps=13916.9, ups=0.21, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.593, loss_scale=16, train_wall=460, gb_free=10, wall=116399
2022-02-13 02:00:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:07:25 | INFO | train_inner | epoch 016:   1049 / 1576 loss=5.003, ppl=32.06, wps=13777, ups=0.21, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.605, loss_scale=8, train_wall=465, gb_free=10, wall=116875
2022-02-13 02:15:16 | INFO | train_inner | epoch 016:   1149 / 1576 loss=5.02, ppl=32.46, wps=13919.5, ups=0.21, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.611, loss_scale=8, train_wall=460, gb_free=10, wall=117346
2022-02-13 02:22:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:23:12 | INFO | train_inner | epoch 016:   1250 / 1576 loss=5.032, ppl=32.72, wps=13775.2, ups=0.21, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.63, loss_scale=8, train_wall=465, gb_free=10, wall=117821
2022-02-13 02:31:02 | INFO | train_inner | epoch 016:   1350 / 1576 loss=5.015, ppl=32.33, wps=13919, ups=0.21, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.614, loss_scale=8, train_wall=460, gb_free=10, wall=118292
2022-02-13 02:38:53 | INFO | train_inner | epoch 016:   1450 / 1576 loss=5.025, ppl=32.56, wps=13914.6, ups=0.21, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.605, loss_scale=8, train_wall=460, gb_free=10, wall=118763
2022-02-13 02:46:44 | INFO | train_inner | epoch 016:   1550 / 1576 loss=5.031, ppl=32.7, wps=13914.2, ups=0.21, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.641, loss_scale=16, train_wall=460, gb_free=10, wall=119234
2022-02-13 02:48:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 02:48:49 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.146 | ppl 35.4 | wps 37219.4 | wpb 1021.8 | bsz 2 | num_updates 25126 | best_loss 5.146
2022-02-13 02:48:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 25126 updates
2022-02-13 02:48:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint16.pt
2022-02-13 02:48:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint16.pt
2022-02-13 02:49:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint16.pt (epoch 16 @ 25126 updates, score 5.146) (writing took 28.367779050022364 seconds)
2022-02-13 02:49:17 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-13 02:49:17 | INFO | train | epoch 016 | loss 4.996 | ppl 31.9 | wps 13803.6 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 25126 | lr 0.000199498 | gnorm 0.619 | loss_scale 16 | train_wall 7253 | gb_free 10 | wall 119387
2022-02-13 02:49:17 | INFO | fairseq.trainer | begin training epoch 17
2022-02-13 02:49:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 02:55:06 | INFO | train_inner | epoch 017:     74 / 1576 loss=4.949, ppl=30.89, wps=12948.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=25200, lr=0.000199205, gnorm=0.617, loss_scale=16, train_wall=457, gb_free=10, wall=119736
2022-02-13 03:01:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:03:02 | INFO | train_inner | epoch 017:    175 / 1576 loss=4.917, ppl=30.22, wps=13764.9, ups=0.21, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.636, loss_scale=8, train_wall=465, gb_free=10, wall=120212
2022-02-13 03:10:53 | INFO | train_inner | epoch 017:    275 / 1576 loss=4.939, ppl=30.67, wps=13908.5, ups=0.21, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.62, loss_scale=8, train_wall=461, gb_free=10, wall=120683
2022-02-13 03:18:45 | INFO | train_inner | epoch 017:    375 / 1576 loss=4.947, ppl=30.84, wps=13909.7, ups=0.21, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.613, loss_scale=8, train_wall=461, gb_free=10, wall=121154
2022-02-13 03:23:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:26:40 | INFO | train_inner | epoch 017:    476 / 1576 loss=4.955, ppl=31.03, wps=13773.7, ups=0.21, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.625, loss_scale=8, train_wall=465, gb_free=10, wall=121630
2022-02-13 03:34:31 | INFO | train_inner | epoch 017:    576 / 1576 loss=4.967, ppl=31.27, wps=13910.6, ups=0.21, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.633, loss_scale=8, train_wall=460, gb_free=10, wall=122101
2022-02-13 03:42:22 | INFO | train_inner | epoch 017:    676 / 1576 loss=4.978, ppl=31.52, wps=13914.6, ups=0.21, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.621, loss_scale=8, train_wall=460, gb_free=10, wall=122572
2022-02-13 03:47:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:50:18 | INFO | train_inner | epoch 017:    777 / 1576 loss=4.971, ppl=31.36, wps=13769.5, ups=0.21, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.632, loss_scale=8, train_wall=465, gb_free=10, wall=123048
2022-02-13 03:58:10 | INFO | train_inner | epoch 017:    877 / 1576 loss=4.978, ppl=31.51, wps=13909.5, ups=0.21, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.626, loss_scale=8, train_wall=461, gb_free=10, wall=123519
2022-02-13 04:06:01 | INFO | train_inner | epoch 017:    977 / 1576 loss=4.983, ppl=31.62, wps=13913.3, ups=0.21, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.615, loss_scale=8, train_wall=460, gb_free=10, wall=123990
2022-02-13 04:08:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:13:56 | INFO | train_inner | epoch 017:   1078 / 1576 loss=4.994, ppl=31.87, wps=13774.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.636, loss_scale=8, train_wall=465, gb_free=10, wall=124466
2022-02-13 04:21:47 | INFO | train_inner | epoch 017:   1178 / 1576 loss=5.004, ppl=32.1, wps=13922.6, ups=0.21, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.613, loss_scale=8, train_wall=460, gb_free=10, wall=124937
2022-02-13 04:29:38 | INFO | train_inner | epoch 017:   1278 / 1576 loss=5.011, ppl=32.24, wps=13913.2, ups=0.21, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.617, loss_scale=16, train_wall=460, gb_free=10, wall=125408
2022-02-13 04:30:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:37:34 | INFO | train_inner | epoch 017:   1379 / 1576 loss=5.02, ppl=32.45, wps=13782.9, ups=0.21, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.642, loss_scale=8, train_wall=465, gb_free=10, wall=125883
2022-02-13 04:45:25 | INFO | train_inner | epoch 017:   1479 / 1576 loss=5.014, ppl=32.32, wps=13913.3, ups=0.21, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.626, loss_scale=8, train_wall=460, gb_free=10, wall=126354
2022-02-13 04:52:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 04:53:03 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.137 | ppl 35.19 | wps 37315.3 | wpb 1021.8 | bsz 2 | num_updates 26697 | best_loss 5.137
2022-02-13 04:53:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26697 updates
2022-02-13 04:53:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint17.pt
2022-02-13 04:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint17.pt
2022-02-13 04:53:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint17.pt (epoch 17 @ 26697 updates, score 5.137) (writing took 29.33016307093203 seconds)
2022-02-13 04:53:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-13 04:53:33 | INFO | train | epoch 017 | loss 4.976 | ppl 31.48 | wps 13801.4 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 26697 | lr 0.000193539 | gnorm 0.625 | loss_scale 16 | train_wall 7253 | gb_free 10 | wall 126842
2022-02-13 04:53:33 | INFO | fairseq.trainer | begin training epoch 18
2022-02-13 04:53:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 04:53:47 | INFO | train_inner | epoch 018:      3 / 1576 loss=5.009, ppl=32.2, wps=12928.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=26700, lr=0.000193528, gnorm=0.636, loss_scale=16, train_wall=457, gb_free=10, wall=126857
2022-02-13 05:01:38 | INFO | train_inner | epoch 018:    103 / 1576 loss=4.892, ppl=29.7, wps=13911.9, ups=0.21, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.616, loss_scale=16, train_wall=460, gb_free=10, wall=127328
2022-02-13 05:09:29 | INFO | train_inner | epoch 018:    203 / 1576 loss=4.912, ppl=30.11, wps=13905.2, ups=0.21, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.614, loss_scale=16, train_wall=461, gb_free=10, wall=127799
2022-02-13 05:09:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:17:25 | INFO | train_inner | epoch 018:    304 / 1576 loss=4.92, ppl=30.27, wps=13778, ups=0.21, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.652, loss_scale=8, train_wall=465, gb_free=10, wall=128275
2022-02-13 05:25:16 | INFO | train_inner | epoch 018:    404 / 1576 loss=4.944, ppl=30.77, wps=13914.3, ups=0.21, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.654, loss_scale=8, train_wall=460, gb_free=10, wall=128746
2022-02-13 05:30:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:33:12 | INFO | train_inner | epoch 018:    505 / 1576 loss=4.942, ppl=30.73, wps=13779.2, ups=0.21, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.63, loss_scale=8, train_wall=465, gb_free=10, wall=129221
2022-02-13 05:41:03 | INFO | train_inner | epoch 018:    605 / 1576 loss=4.955, ppl=31.01, wps=13918.6, ups=0.21, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.622, loss_scale=8, train_wall=460, gb_free=10, wall=129692
2022-02-13 05:48:53 | INFO | train_inner | epoch 018:    705 / 1576 loss=4.946, ppl=30.83, wps=13923.5, ups=0.21, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.639, loss_scale=8, train_wall=460, gb_free=10, wall=130163
2022-02-13 05:52:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:56:49 | INFO | train_inner | epoch 018:    806 / 1576 loss=4.967, ppl=31.28, wps=13783.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.621, loss_scale=8, train_wall=465, gb_free=10, wall=130638
2022-02-13 06:04:40 | INFO | train_inner | epoch 018:    906 / 1576 loss=4.974, ppl=31.42, wps=13916.9, ups=0.21, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.645, loss_scale=8, train_wall=460, gb_free=10, wall=131109
2022-02-13 06:12:31 | INFO | train_inner | epoch 018:   1006 / 1576 loss=4.972, ppl=31.39, wps=13916.4, ups=0.21, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.621, loss_scale=8, train_wall=460, gb_free=10, wall=131580
2022-02-13 06:14:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 06:20:26 | INFO | train_inner | epoch 018:   1107 / 1576 loss=4.98, ppl=31.56, wps=13781.7, ups=0.21, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.638, loss_scale=8, train_wall=465, gb_free=10, wall=132056
2022-02-13 06:28:17 | INFO | train_inner | epoch 018:   1207 / 1576 loss=4.979, ppl=31.54, wps=13920.1, ups=0.21, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.632, loss_scale=8, train_wall=460, gb_free=10, wall=132527
2022-02-13 06:36:08 | INFO | train_inner | epoch 018:   1307 / 1576 loss=4.996, ppl=31.9, wps=13916.8, ups=0.21, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.633, loss_scale=16, train_wall=460, gb_free=10, wall=132998
2022-02-13 06:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 06:44:03 | INFO | train_inner | epoch 018:   1408 / 1576 loss=4.986, ppl=31.69, wps=13780.1, ups=0.21, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.624, loss_scale=8, train_wall=465, gb_free=10, wall=133473
2022-02-13 06:51:54 | INFO | train_inner | epoch 018:   1508 / 1576 loss=5.004, ppl=32.08, wps=13916.1, ups=0.21, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.612, loss_scale=8, train_wall=460, gb_free=10, wall=133944
2022-02-13 06:57:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 06:57:16 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.134 | ppl 35.12 | wps 37206.2 | wpb 1021.8 | bsz 2 | num_updates 28268 | best_loss 5.134
2022-02-13 06:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 28268 updates
2022-02-13 06:57:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint18.pt
2022-02-13 06:57:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint18.pt
2022-02-13 06:57:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint18.pt (epoch 18 @ 28268 updates, score 5.134) (writing took 29.322975979186594 seconds)
2022-02-13 06:57:46 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-13 06:57:46 | INFO | train | epoch 018 | loss 4.96 | ppl 31.12 | wps 13806.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 28268 | lr 0.000188084 | gnorm 0.63 | loss_scale 8 | train_wall 7250 | gb_free 10 | wall 134295
2022-02-13 06:57:46 | INFO | fairseq.trainer | begin training epoch 19
2022-02-13 06:57:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 07:00:16 | INFO | train_inner | epoch 019:     32 / 1576 loss=4.964, ppl=31.21, wps=12941, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=28300, lr=0.000187978, gnorm=0.636, loss_scale=16, train_wall=456, gb_free=10, wall=134446
2022-02-13 07:03:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:08:12 | INFO | train_inner | epoch 019:    133 / 1576 loss=4.885, ppl=29.55, wps=13782.3, ups=0.21, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.625, loss_scale=8, train_wall=465, gb_free=10, wall=134922
2022-02-13 07:16:03 | INFO | train_inner | epoch 019:    233 / 1576 loss=4.897, ppl=29.8, wps=13906.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.629, loss_scale=8, train_wall=461, gb_free=10, wall=135393
2022-02-13 07:23:54 | INFO | train_inner | epoch 019:    333 / 1576 loss=4.902, ppl=29.91, wps=13922.6, ups=0.21, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.642, loss_scale=16, train_wall=460, gb_free=10, wall=135864
2022-02-13 07:25:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:31:49 | INFO | train_inner | epoch 019:    434 / 1576 loss=4.922, ppl=30.32, wps=13783.6, ups=0.21, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.675, loss_scale=8, train_wall=465, gb_free=10, wall=136339
2022-02-13 07:39:40 | INFO | train_inner | epoch 019:    534 / 1576 loss=4.931, ppl=30.5, wps=13920.1, ups=0.21, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.617, loss_scale=8, train_wall=460, gb_free=10, wall=136810
2022-02-13 07:47:31 | INFO | train_inner | epoch 019:    634 / 1576 loss=4.943, ppl=30.75, wps=13918.6, ups=0.21, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.627, loss_scale=16, train_wall=460, gb_free=10, wall=137281
2022-02-13 07:49:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:55:26 | INFO | train_inner | epoch 019:    735 / 1576 loss=4.95, ppl=30.91, wps=13784.5, ups=0.21, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.646, loss_scale=8, train_wall=465, gb_free=10, wall=137756
2022-02-13 08:03:17 | INFO | train_inner | epoch 019:    835 / 1576 loss=4.951, ppl=30.93, wps=13919.8, ups=0.21, wpb=65536, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.647, loss_scale=8, train_wall=460, gb_free=10, wall=138227
2022-02-13 08:11:08 | INFO | train_inner | epoch 019:    935 / 1576 loss=4.964, ppl=31.21, wps=13916.9, ups=0.21, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.645, loss_scale=16, train_wall=460, gb_free=10, wall=138698
2022-02-13 08:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:19:03 | INFO | train_inner | epoch 019:   1036 / 1576 loss=4.96, ppl=31.12, wps=13785.9, ups=0.21, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.649, loss_scale=8, train_wall=465, gb_free=10, wall=139173
2022-02-13 08:26:54 | INFO | train_inner | epoch 019:   1136 / 1576 loss=4.97, ppl=31.34, wps=13932.1, ups=0.21, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.62, loss_scale=8, train_wall=460, gb_free=10, wall=139644
2022-02-13 08:32:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:34:49 | INFO | train_inner | epoch 019:   1237 / 1576 loss=4.958, ppl=31.08, wps=13796.2, ups=0.21, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.658, loss_scale=8, train_wall=464, gb_free=10, wall=140119
2022-02-13 08:42:39 | INFO | train_inner | epoch 019:   1337 / 1576 loss=4.968, ppl=31.29, wps=13928.2, ups=0.21, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.631, loss_scale=8, train_wall=460, gb_free=10, wall=140589
2022-02-13 08:50:30 | INFO | train_inner | epoch 019:   1437 / 1576 loss=4.982, ppl=31.61, wps=13926.7, ups=0.21, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.64, loss_scale=8, train_wall=460, gb_free=10, wall=141060
2022-02-13 08:58:21 | INFO | train_inner | epoch 019:   1537 / 1576 loss=4.987, ppl=31.71, wps=13922.2, ups=0.21, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.638, loss_scale=16, train_wall=460, gb_free=10, wall=141530
2022-02-13 09:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 09:01:26 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.123 | ppl 34.85 | wps 37339.4 | wpb 1021.8 | bsz 2 | num_updates 29839 | best_loss 5.123
2022-02-13 09:01:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 29839 updates
2022-02-13 09:01:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint19.pt
2022-02-13 09:01:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint19.pt
2022-02-13 09:01:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint19.pt (epoch 19 @ 29839 updates, score 5.123) (writing took 31.64089264627546 seconds)
2022-02-13 09:01:58 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-13 09:01:58 | INFO | train | epoch 019 | loss 4.944 | ppl 30.79 | wps 13807.7 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 29839 | lr 0.000183066 | gnorm 0.641 | loss_scale 16 | train_wall 7248 | gb_free 10 | wall 141748
2022-02-13 09:01:58 | INFO | fairseq.trainer | begin training epoch 20
2022-02-13 09:01:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 09:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 09:06:50 | INFO | train_inner | epoch 020:     62 / 1576 loss=4.921, ppl=30.3, wps=12768.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=29900, lr=0.000182879, gnorm=0.65, loss_scale=8, train_wall=460, gb_free=10, wall=142039
2022-02-13 09:14:40 | INFO | train_inner | epoch 020:    162 / 1576 loss=4.868, ppl=29.21, wps=13925, ups=0.21, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.645, loss_scale=8, train_wall=460, gb_free=10, wall=142510
2022-02-13 09:22:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 09:22:36 | INFO | train_inner | epoch 020:    263 / 1576 loss=4.877, ppl=29.39, wps=13779.2, ups=0.21, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.657, loss_scale=8, train_wall=465, gb_free=10, wall=142985
2022-02-13 09:30:26 | INFO | train_inner | epoch 020:    363 / 1576 loss=4.895, ppl=29.76, wps=13924.6, ups=0.21, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.612, loss_scale=8, train_wall=460, gb_free=10, wall=143456
2022-02-13 09:38:17 | INFO | train_inner | epoch 020:    463 / 1576 loss=4.912, ppl=30.1, wps=13928.6, ups=0.21, wpb=65536, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.652, loss_scale=8, train_wall=460, gb_free=10, wall=143927
2022-02-13 09:46:08 | INFO | train_inner | epoch 020:    563 / 1576 loss=4.918, ppl=30.23, wps=13918.2, ups=0.21, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.647, loss_scale=16, train_wall=460, gb_free=10, wall=144397
2022-02-13 09:50:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 09:54:03 | INFO | train_inner | epoch 020:    664 / 1576 loss=4.914, ppl=30.16, wps=13789.8, ups=0.21, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.667, loss_scale=8, train_wall=465, gb_free=10, wall=144873
2022-02-13 10:01:54 | INFO | train_inner | epoch 020:    764 / 1576 loss=4.931, ppl=30.51, wps=13928.8, ups=0.21, wpb=65536, bsz=128, num_updates=30600, lr=0.000180775, gnorm=0.616, loss_scale=8, train_wall=460, gb_free=10, wall=145343
2022-02-13 10:09:44 | INFO | train_inner | epoch 020:    864 / 1576 loss=4.935, ppl=30.6, wps=13928.3, ups=0.21, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.682, loss_scale=8, train_wall=460, gb_free=10, wall=145814
2022-02-13 10:11:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:17:39 | INFO | train_inner | epoch 020:    965 / 1576 loss=4.941, ppl=30.72, wps=13785.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.664, loss_scale=8, train_wall=465, gb_free=10, wall=146289
2022-02-13 10:25:30 | INFO | train_inner | epoch 020:   1065 / 1576 loss=4.95, ppl=30.92, wps=13930.7, ups=0.21, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.651, loss_scale=8, train_wall=460, gb_free=10, wall=146760
2022-02-13 10:33:20 | INFO | train_inner | epoch 020:   1165 / 1576 loss=4.965, ppl=31.24, wps=13930.7, ups=0.21, wpb=65536, bsz=128, num_updates=31000, lr=0.000179605, gnorm=0.637, loss_scale=16, train_wall=460, gb_free=10, wall=147230
2022-02-13 10:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:41:16 | INFO | train_inner | epoch 020:   1266 / 1576 loss=4.967, ppl=31.28, wps=13784.7, ups=0.21, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.631, loss_scale=8, train_wall=465, gb_free=10, wall=147705
2022-02-13 10:49:06 | INFO | train_inner | epoch 020:   1366 / 1576 loss=4.971, ppl=31.37, wps=13926.6, ups=0.21, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.656, loss_scale=8, train_wall=460, gb_free=10, wall=148176
2022-02-13 10:56:57 | INFO | train_inner | epoch 020:   1466 / 1576 loss=4.971, ppl=31.36, wps=13930.4, ups=0.21, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.612, loss_scale=8, train_wall=460, gb_free=10, wall=148646
2022-02-13 11:04:48 | INFO | train_inner | epoch 020:   1566 / 1576 loss=4.971, ppl=31.37, wps=13922.7, ups=0.21, wpb=65536, bsz=128, num_updates=31400, lr=0.000178458, gnorm=0.656, loss_scale=16, train_wall=460, gb_free=10, wall=149117
2022-02-13 11:05:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 11:05:37 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.117 | ppl 34.71 | wps 36461.1 | wpb 1021.8 | bsz 2 | num_updates 31410 | best_loss 5.117
2022-02-13 11:05:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 31410 updates
2022-02-13 11:05:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint20.pt
2022-02-13 11:05:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint20.pt
2022-02-13 11:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint20.pt (epoch 20 @ 31410 updates, score 5.117) (writing took 30.562655098736286 seconds)
2022-02-13 11:06:07 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-13 11:06:07 | INFO | train | epoch 020 | loss 4.931 | ppl 30.5 | wps 13813.4 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 31410 | lr 0.000178429 | gnorm 0.645 | loss_scale 16 | train_wall 7246 | gb_free 10 | wall 149197
2022-02-13 11:06:07 | INFO | fairseq.trainer | begin training epoch 21
2022-02-13 11:06:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 11:13:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:13:15 | INFO | train_inner | epoch 021:     91 / 1576 loss=4.864, ppl=29.13, wps=12791, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=31500, lr=0.000178174, gnorm=0.657, loss_scale=8, train_wall=460, gb_free=10, wall=149625
2022-02-13 11:21:06 | INFO | train_inner | epoch 021:    191 / 1576 loss=4.867, ppl=29.18, wps=13923.4, ups=0.21, wpb=65536, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.62, loss_scale=8, train_wall=460, gb_free=10, wall=150096
2022-02-13 11:28:57 | INFO | train_inner | epoch 021:    291 / 1576 loss=4.882, ppl=29.49, wps=13924.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.664, loss_scale=8, train_wall=460, gb_free=10, wall=150566
2022-02-13 11:36:47 | INFO | train_inner | epoch 021:    391 / 1576 loss=4.881, ppl=29.47, wps=13928.1, ups=0.21, wpb=65536, bsz=128, num_updates=31800, lr=0.000177332, gnorm=0.659, loss_scale=16, train_wall=460, gb_free=10, wall=151037
2022-02-13 11:43:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:44:42 | INFO | train_inner | epoch 021:    492 / 1576 loss=4.898, ppl=29.82, wps=13793.4, ups=0.21, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.644, loss_scale=8, train_wall=465, gb_free=10, wall=151512
2022-02-13 11:52:33 | INFO | train_inner | epoch 021:    592 / 1576 loss=4.909, ppl=30.05, wps=13931.1, ups=0.21, wpb=65536, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.636, loss_scale=8, train_wall=460, gb_free=10, wall=151982
2022-02-13 12:00:23 | INFO | train_inner | epoch 021:    692 / 1576 loss=4.92, ppl=30.27, wps=13926.7, ups=0.21, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.643, loss_scale=8, train_wall=460, gb_free=10, wall=152453
2022-02-13 12:08:14 | INFO | train_inner | epoch 021:    792 / 1576 loss=4.927, ppl=30.41, wps=13930.1, ups=0.21, wpb=65536, bsz=128, num_updates=32200, lr=0.000176227, gnorm=0.653, loss_scale=16, train_wall=460, gb_free=10, wall=152924
2022-02-13 12:10:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:16:09 | INFO | train_inner | epoch 021:    893 / 1576 loss=4.93, ppl=30.48, wps=13796.3, ups=0.21, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.632, loss_scale=8, train_wall=464, gb_free=10, wall=153399
2022-02-13 12:23:59 | INFO | train_inner | epoch 021:    993 / 1576 loss=4.929, ppl=30.47, wps=13927.8, ups=0.21, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.64, loss_scale=8, train_wall=460, gb_free=10, wall=153869
2022-02-13 12:31:50 | INFO | train_inner | epoch 021:   1093 / 1576 loss=4.95, ppl=30.91, wps=13926.2, ups=0.21, wpb=65536, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.639, loss_scale=16, train_wall=460, gb_free=10, wall=154340
2022-02-13 12:36:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:39:45 | INFO | train_inner | epoch 021:   1194 / 1576 loss=4.941, ppl=30.73, wps=13794.1, ups=0.21, wpb=65536, bsz=128, num_updates=32600, lr=0.000175142, gnorm=0.644, loss_scale=8, train_wall=465, gb_free=10, wall=154815
2022-02-13 12:47:36 | INFO | train_inner | epoch 021:   1294 / 1576 loss=4.952, ppl=30.96, wps=13928.8, ups=0.21, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.656, loss_scale=8, train_wall=460, gb_free=10, wall=155285
2022-02-13 12:55:26 | INFO | train_inner | epoch 021:   1394 / 1576 loss=4.955, ppl=31.02, wps=13926.9, ups=0.21, wpb=65536, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.661, loss_scale=8, train_wall=460, gb_free=10, wall=155756
2022-02-13 13:01:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:03:21 | INFO | train_inner | epoch 021:   1495 / 1576 loss=4.945, ppl=30.81, wps=13791.8, ups=0.21, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.639, loss_scale=8, train_wall=465, gb_free=10, wall=156231
2022-02-13 13:09:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 13:09:45 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.11 | ppl 34.53 | wps 36639.4 | wpb 1021.8 | bsz 2 | num_updates 32981 | best_loss 5.11
2022-02-13 13:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 32981 updates
2022-02-13 13:09:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint21.pt
2022-02-13 13:09:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint21.pt
2022-02-13 13:10:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint21.pt (epoch 21 @ 32981 updates, score 5.11) (writing took 29.19649688526988 seconds)
2022-02-13 13:10:14 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-13 13:10:14 | INFO | train | epoch 021 | loss 4.918 | ppl 30.24 | wps 13818.3 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 32981 | lr 0.000174128 | gnorm 0.645 | loss_scale 8 | train_wall 7245 | gb_free 10 | wall 156643
2022-02-13 13:10:14 | INFO | fairseq.trainer | begin training epoch 22
2022-02-13 13:10:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 13:11:43 | INFO | train_inner | epoch 022:     19 / 1576 loss=4.937, ppl=30.63, wps=12946.1, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=33000, lr=0.000174078, gnorm=0.654, loss_scale=8, train_wall=456, gb_free=10, wall=156733
2022-02-13 13:19:34 | INFO | train_inner | epoch 022:    119 / 1576 loss=4.857, ppl=28.98, wps=13929.1, ups=0.21, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.653, loss_scale=8, train_wall=460, gb_free=10, wall=157203
2022-02-13 13:25:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:27:29 | INFO | train_inner | epoch 022:    220 / 1576 loss=4.854, ppl=28.92, wps=13792.1, ups=0.21, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.653, loss_scale=8, train_wall=464, gb_free=10, wall=157679
2022-02-13 13:35:20 | INFO | train_inner | epoch 022:    320 / 1576 loss=4.876, ppl=29.36, wps=13922.9, ups=0.21, wpb=65536, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.639, loss_scale=8, train_wall=460, gb_free=10, wall=158149
2022-02-13 13:43:10 | INFO | train_inner | epoch 022:    420 / 1576 loss=4.88, ppl=29.45, wps=13925.9, ups=0.21, wpb=65536, bsz=128, num_updates=33400, lr=0.000173032, gnorm=0.649, loss_scale=8, train_wall=460, gb_free=10, wall=158620
2022-02-13 13:50:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:51:05 | INFO | train_inner | epoch 022:    521 / 1576 loss=4.885, ppl=29.55, wps=13791.7, ups=0.21, wpb=65536, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.639, loss_scale=8, train_wall=464, gb_free=10, wall=159095
2022-02-13 13:58:56 | INFO | train_inner | epoch 022:    621 / 1576 loss=4.892, ppl=29.7, wps=13928.8, ups=0.21, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.64, loss_scale=8, train_wall=460, gb_free=10, wall=159566
2022-02-13 14:06:46 | INFO | train_inner | epoch 022:    721 / 1576 loss=4.905, ppl=29.96, wps=13929.9, ups=0.21, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.631, loss_scale=8, train_wall=460, gb_free=10, wall=160036
2022-02-13 14:14:37 | INFO | train_inner | epoch 022:    821 / 1576 loss=4.909, ppl=30.05, wps=13932.4, ups=0.21, wpb=65536, bsz=128, num_updates=33800, lr=0.000172005, gnorm=0.643, loss_scale=16, train_wall=460, gb_free=10, wall=160506
2022-02-13 14:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 14:22:32 | INFO | train_inner | epoch 022:    922 / 1576 loss=4.926, ppl=30.39, wps=13793.2, ups=0.21, wpb=65536, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.655, loss_scale=8, train_wall=465, gb_free=10, wall=160982
2022-02-13 14:30:22 | INFO | train_inner | epoch 022:   1022 / 1576 loss=4.927, ppl=30.42, wps=13930.2, ups=0.21, wpb=65536, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.662, loss_scale=8, train_wall=460, gb_free=10, wall=161452
2022-02-13 14:38:13 | INFO | train_inner | epoch 022:   1122 / 1576 loss=4.924, ppl=30.36, wps=13928.8, ups=0.21, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=0.648, loss_scale=16, train_wall=460, gb_free=10, wall=161922
2022-02-13 14:38:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 14:46:08 | INFO | train_inner | epoch 022:   1223 / 1576 loss=4.934, ppl=30.56, wps=13797.2, ups=0.21, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=0.679, loss_scale=8, train_wall=464, gb_free=10, wall=162397
2022-02-13 14:53:58 | INFO | train_inner | epoch 022:   1323 / 1576 loss=4.931, ppl=30.52, wps=13926.4, ups=0.21, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.657, loss_scale=8, train_wall=460, gb_free=10, wall=162868
2022-02-13 14:59:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:01:53 | INFO | train_inner | epoch 022:   1424 / 1576 loss=4.945, ppl=30.81, wps=13794.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.657, loss_scale=8, train_wall=464, gb_free=10, wall=163343
2022-02-13 15:09:44 | INFO | train_inner | epoch 022:   1524 / 1576 loss=4.947, ppl=30.86, wps=13927.3, ups=0.21, wpb=65536, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.629, loss_scale=8, train_wall=460, gb_free=10, wall=163814
2022-02-13 15:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 15:13:50 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.099 | ppl 34.27 | wps 37244.5 | wpb 1021.8 | bsz 2 | num_updates 34552 | best_loss 5.099
2022-02-13 15:13:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 34552 updates
2022-02-13 15:13:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint22.pt
2022-02-13 15:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint22.pt
2022-02-13 15:14:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint22.pt (epoch 22 @ 34552 updates, score 5.099) (writing took 29.431026847101748 seconds)
2022-02-13 15:14:20 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-13 15:14:20 | INFO | train | epoch 022 | loss 4.907 | ppl 30 | wps 13819 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 34552 | lr 0.000170123 | gnorm 0.65 | loss_scale 8 | train_wall 7245 | gb_free 10 | wall 164090
2022-02-13 15:14:20 | INFO | fairseq.trainer | begin training epoch 23
2022-02-13 15:14:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 15:18:06 | INFO | train_inner | epoch 023:     48 / 1576 loss=4.888, ppl=29.61, wps=12946.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=34600, lr=0.000170005, gnorm=0.657, loss_scale=8, train_wall=456, gb_free=10, wall=164315
2022-02-13 15:25:56 | INFO | train_inner | epoch 023:    148 / 1576 loss=4.841, ppl=28.66, wps=13927.3, ups=0.21, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.634, loss_scale=16, train_wall=460, gb_free=10, wall=164786
2022-02-13 15:27:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:33:52 | INFO | train_inner | epoch 023:    249 / 1576 loss=4.863, ppl=29.1, wps=13790.4, ups=0.21, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.689, loss_scale=8, train_wall=465, gb_free=10, wall=165261
2022-02-13 15:41:42 | INFO | train_inner | epoch 023:    349 / 1576 loss=4.871, ppl=29.27, wps=13932.8, ups=0.21, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.656, loss_scale=8, train_wall=460, gb_free=10, wall=165732
2022-02-13 15:47:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:49:37 | INFO | train_inner | epoch 023:    450 / 1576 loss=4.868, ppl=29.2, wps=13794.3, ups=0.21, wpb=65536, bsz=128, num_updates=35000, lr=0.000169031, gnorm=0.644, loss_scale=8, train_wall=464, gb_free=10, wall=166207
2022-02-13 15:57:28 | INFO | train_inner | epoch 023:    550 / 1576 loss=4.879, ppl=29.42, wps=13927.5, ups=0.21, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.633, loss_scale=8, train_wall=460, gb_free=10, wall=166677
2022-02-13 16:05:18 | INFO | train_inner | epoch 023:    650 / 1576 loss=4.892, ppl=29.7, wps=13931.1, ups=0.21, wpb=65536, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.676, loss_scale=8, train_wall=460, gb_free=10, wall=167148
2022-02-13 16:12:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:13:13 | INFO | train_inner | epoch 023:    751 / 1576 loss=4.887, ppl=29.58, wps=13795.5, ups=0.21, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.635, loss_scale=8, train_wall=464, gb_free=10, wall=167623
2022-02-13 16:21:04 | INFO | train_inner | epoch 023:    851 / 1576 loss=4.901, ppl=29.88, wps=13921.8, ups=0.21, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=0.634, loss_scale=8, train_wall=460, gb_free=10, wall=168094
2022-02-13 16:28:54 | INFO | train_inner | epoch 023:    951 / 1576 loss=4.913, ppl=30.13, wps=13931, ups=0.21, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.679, loss_scale=8, train_wall=460, gb_free=10, wall=168564
2022-02-13 16:35:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:36:50 | INFO | train_inner | epoch 023:   1052 / 1576 loss=4.92, ppl=30.28, wps=13787, ups=0.21, wpb=65536, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.647, loss_scale=8, train_wall=465, gb_free=10, wall=169039
2022-02-13 16:44:40 | INFO | train_inner | epoch 023:   1152 / 1576 loss=4.925, ppl=30.37, wps=13933.1, ups=0.21, wpb=65536, bsz=128, num_updates=35700, lr=0.000167365, gnorm=0.669, loss_scale=8, train_wall=460, gb_free=10, wall=169510
2022-02-13 16:52:30 | INFO | train_inner | epoch 023:   1252 / 1576 loss=4.923, ppl=30.34, wps=13931.5, ups=0.21, wpb=65536, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.661, loss_scale=8, train_wall=460, gb_free=10, wall=169980
2022-02-13 16:55:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:00:26 | INFO | train_inner | epoch 023:   1353 / 1576 loss=4.919, ppl=30.25, wps=13791.8, ups=0.21, wpb=65536, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.679, loss_scale=8, train_wall=465, gb_free=10, wall=170455
2022-02-13 17:08:16 | INFO | train_inner | epoch 023:   1453 / 1576 loss=4.927, ppl=30.42, wps=13928.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.653, loss_scale=8, train_wall=460, gb_free=10, wall=170926
2022-02-13 17:16:06 | INFO | train_inner | epoch 023:   1553 / 1576 loss=4.937, ppl=30.64, wps=13931.9, ups=0.21, wpb=65536, bsz=128, num_updates=36100, lr=0.000166436, gnorm=0.663, loss_scale=16, train_wall=460, gb_free=10, wall=171396
2022-02-13 17:17:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 17:17:57 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.099 | ppl 34.28 | wps 37301.9 | wpb 1021.8 | bsz 2 | num_updates 36123 | best_loss 5.099
2022-02-13 17:17:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 36123 updates
2022-02-13 17:17:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint23.pt
2022-02-13 17:18:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint23.pt
2022-02-13 17:18:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint23.pt (epoch 23 @ 36123 updates, score 5.099) (writing took 29.890944582410157 seconds)
2022-02-13 17:18:27 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-13 17:18:27 | INFO | train | epoch 023 | loss 4.896 | ppl 29.78 | wps 13818.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 36123 | lr 0.000166383 | gnorm 0.657 | loss_scale 16 | train_wall 7245 | gb_free 10 | wall 171536
2022-02-13 17:18:27 | INFO | fairseq.trainer | begin training epoch 24
2022-02-13 17:18:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 17:18:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:24:33 | INFO | train_inner | epoch 024:     78 / 1576 loss=4.86, ppl=29.05, wps=12816.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=36200, lr=0.000166206, gnorm=0.665, loss_scale=8, train_wall=460, gb_free=10, wall=171903
2022-02-13 17:32:24 | INFO | train_inner | epoch 024:    178 / 1576 loss=4.83, ppl=28.45, wps=13931.6, ups=0.21, wpb=65536, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.67, loss_scale=8, train_wall=460, gb_free=10, wall=172373
2022-02-13 17:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:40:19 | INFO | train_inner | epoch 024:    279 / 1576 loss=4.852, ppl=28.88, wps=13794.3, ups=0.21, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.651, loss_scale=8, train_wall=464, gb_free=10, wall=172849
2022-02-13 17:48:09 | INFO | train_inner | epoch 024:    379 / 1576 loss=4.852, ppl=28.88, wps=13935.3, ups=0.21, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=0.652, loss_scale=8, train_wall=460, gb_free=10, wall=173319
2022-02-13 17:56:18 | INFO | train_inner | epoch 024:    479 / 1576 loss=4.863, ppl=29.1, wps=13931.8, ups=0.21, wpb=65536, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.645, loss_scale=8, train_wall=460, gb_free=10, wall=173808
2022-02-13 18:03:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:04:13 | INFO | train_inner | epoch 024:    580 / 1576 loss=4.874, ppl=29.33, wps=13791.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.653, loss_scale=8, train_wall=465, gb_free=10, wall=174283
2022-02-13 18:12:04 | INFO | train_inner | epoch 024:    680 / 1576 loss=4.878, ppl=29.4, wps=13929.6, ups=0.21, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.667, loss_scale=8, train_wall=460, gb_free=10, wall=174753
2022-02-13 18:19:54 | INFO | train_inner | epoch 024:    780 / 1576 loss=4.884, ppl=29.53, wps=13931.8, ups=0.21, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=0.659, loss_scale=8, train_wall=460, gb_free=10, wall=175224
2022-02-13 18:27:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:27:49 | INFO | train_inner | epoch 024:    881 / 1576 loss=4.882, ppl=29.49, wps=13793.6, ups=0.21, wpb=65536, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.654, loss_scale=8, train_wall=464, gb_free=10, wall=175699
2022-02-13 18:35:39 | INFO | train_inner | epoch 024:    981 / 1576 loss=4.891, ppl=29.67, wps=13937.4, ups=0.21, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.651, loss_scale=8, train_wall=460, gb_free=10, wall=176169
2022-02-13 18:43:30 | INFO | train_inner | epoch 024:   1081 / 1576 loss=4.907, ppl=30, wps=13927, ups=0.21, wpb=65536, bsz=128, num_updates=37200, lr=0.000163956, gnorm=0.662, loss_scale=8, train_wall=460, gb_free=10, wall=176640
2022-02-13 18:47:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:51:26 | INFO | train_inner | epoch 024:   1182 / 1576 loss=4.92, ppl=30.27, wps=13784, ups=0.21, wpb=65536, bsz=128, num_updates=37300, lr=0.000163737, gnorm=0.659, loss_scale=8, train_wall=465, gb_free=10, wall=177115
2022-02-13 18:59:16 | INFO | train_inner | epoch 024:   1282 / 1576 loss=4.917, ppl=30.22, wps=13929.2, ups=0.21, wpb=65536, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.641, loss_scale=8, train_wall=460, gb_free=10, wall=177586
2022-02-13 19:07:06 | INFO | train_inner | epoch 024:   1382 / 1576 loss=4.928, ppl=30.44, wps=13933.3, ups=0.21, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.654, loss_scale=8, train_wall=460, gb_free=10, wall=178056
2022-02-13 19:11:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:15:01 | INFO | train_inner | epoch 024:   1483 / 1576 loss=4.927, ppl=30.42, wps=13795.4, ups=0.21, wpb=65536, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.674, loss_scale=8, train_wall=464, gb_free=10, wall=178531
2022-02-13 19:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 19:22:21 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.093 | ppl 34.14 | wps 37044.8 | wpb 1021.8 | bsz 2 | num_updates 37693 | best_loss 5.093
2022-02-13 19:22:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 37693 updates
2022-02-13 19:22:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint24.pt
2022-02-13 19:22:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint24.pt
2022-02-13 19:22:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint24.pt (epoch 24 @ 37693 updates, score 5.093) (writing took 30.188715355470777 seconds)
2022-02-13 19:22:51 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-13 19:22:51 | INFO | train | epoch 024 | loss 4.886 | ppl 29.56 | wps 13776.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 37693 | lr 0.000162881 | gnorm 0.66 | loss_scale 8 | train_wall 7244 | gb_free 10 | wall 179001
2022-02-13 19:22:51 | INFO | fairseq.trainer | begin training epoch 25
2022-02-13 19:22:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 19:23:24 | INFO | train_inner | epoch 025:      7 / 1576 loss=4.914, ppl=30.14, wps=12928.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=37700, lr=0.000162866, gnorm=0.696, loss_scale=8, train_wall=456, gb_free=10, wall=179034
2022-02-13 19:31:14 | INFO | train_inner | epoch 025:    107 / 1576 loss=4.81, ppl=28.05, wps=13931.7, ups=0.21, wpb=65536, bsz=128, num_updates=37800, lr=0.00016265, gnorm=0.654, loss_scale=8, train_wall=460, gb_free=10, wall=179504
2022-02-13 19:31:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:39:10 | INFO | train_inner | epoch 025:    208 / 1576 loss=4.821, ppl=28.26, wps=13789.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.639, loss_scale=8, train_wall=465, gb_free=10, wall=179979
2022-02-13 19:47:00 | INFO | train_inner | epoch 025:    308 / 1576 loss=4.834, ppl=28.52, wps=13931.8, ups=0.21, wpb=65536, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.646, loss_scale=8, train_wall=460, gb_free=10, wall=180450
2022-02-13 19:54:50 | INFO | train_inner | epoch 025:    408 / 1576 loss=4.85, ppl=28.84, wps=13929.2, ups=0.21, wpb=65536, bsz=128, num_updates=38100, lr=0.000162008, gnorm=0.675, loss_scale=16, train_wall=460, gb_free=10, wall=180920
2022-02-13 19:55:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:02:46 | INFO | train_inner | epoch 025:    509 / 1576 loss=4.865, ppl=29.14, wps=13791.2, ups=0.21, wpb=65536, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.663, loss_scale=8, train_wall=465, gb_free=10, wall=181395
2022-02-13 20:10:36 | INFO | train_inner | epoch 025:    609 / 1576 loss=4.874, ppl=29.33, wps=13932, ups=0.21, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.671, loss_scale=8, train_wall=460, gb_free=10, wall=181866
2022-02-13 20:18:27 | INFO | train_inner | epoch 025:    709 / 1576 loss=4.874, ppl=29.31, wps=13926.3, ups=0.21, wpb=65536, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.677, loss_scale=16, train_wall=460, gb_free=10, wall=182336
2022-02-13 20:26:17 | INFO | train_inner | epoch 025:    809 / 1576 loss=4.872, ppl=29.27, wps=13924.5, ups=0.21, wpb=65536, bsz=128, num_updates=38500, lr=0.000161165, gnorm=0.645, loss_scale=16, train_wall=460, gb_free=10, wall=182807
2022-02-13 20:26:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:34:13 | INFO | train_inner | epoch 025:    910 / 1576 loss=4.884, ppl=29.53, wps=13789.3, ups=0.21, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.654, loss_scale=8, train_wall=465, gb_free=10, wall=183282
2022-02-13 20:42:03 | INFO | train_inner | epoch 025:   1010 / 1576 loss=4.905, ppl=29.96, wps=13938.6, ups=0.21, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.682, loss_scale=8, train_wall=460, gb_free=10, wall=183752
2022-02-13 20:49:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:49:58 | INFO | train_inner | epoch 025:   1111 / 1576 loss=4.908, ppl=30.02, wps=13788.8, ups=0.21, wpb=65536, bsz=128, num_updates=38800, lr=0.00016054, gnorm=0.65, loss_scale=8, train_wall=465, gb_free=10, wall=184228
2022-02-13 20:57:48 | INFO | train_inner | epoch 025:   1211 / 1576 loss=4.9, ppl=29.87, wps=13930.6, ups=0.21, wpb=65536, bsz=128, num_updates=38900, lr=0.000160334, gnorm=0.633, loss_scale=8, train_wall=460, gb_free=10, wall=184698
2022-02-13 21:05:39 | INFO | train_inner | epoch 025:   1311 / 1576 loss=4.906, ppl=29.97, wps=13929.6, ups=0.21, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.675, loss_scale=8, train_wall=460, gb_free=10, wall=185169
2022-02-13 21:10:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:13:34 | INFO | train_inner | epoch 025:   1412 / 1576 loss=4.906, ppl=29.99, wps=13786.5, ups=0.21, wpb=65536, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.653, loss_scale=8, train_wall=465, gb_free=10, wall=185644
2022-02-13 21:21:25 | INFO | train_inner | epoch 025:   1512 / 1576 loss=4.91, ppl=30.07, wps=13929.1, ups=0.21, wpb=65536, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.67, loss_scale=8, train_wall=460, gb_free=10, wall=186114
2022-02-13 21:26:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 21:26:28 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.09 | ppl 34.05 | wps 36499.3 | wpb 1021.8 | bsz 2 | num_updates 39264 | best_loss 5.09
2022-02-13 21:26:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 39264 updates
2022-02-13 21:26:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint25.pt
2022-02-13 21:26:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint25.pt
2022-02-13 21:26:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint25.pt (epoch 25 @ 39264 updates, score 5.09) (writing took 30.377654855139554 seconds)
2022-02-13 21:26:58 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-13 21:26:58 | INFO | train | epoch 025 | loss 4.876 | ppl 29.37 | wps 13816.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 39264 | lr 0.000159589 | gnorm 0.66 | loss_scale 8 | train_wall 7245 | gb_free 10 | wall 186448
2022-02-13 21:26:58 | INFO | fairseq.trainer | begin training epoch 26
2022-02-13 21:26:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 21:29:48 | INFO | train_inner | epoch 026:     36 / 1576 loss=4.88, ppl=29.44, wps=12916.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=39300, lr=0.000159516, gnorm=0.669, loss_scale=8, train_wall=456, gb_free=10, wall=186617
2022-02-13 21:37:38 | INFO | train_inner | epoch 026:    136 / 1576 loss=4.815, ppl=28.14, wps=13924.3, ups=0.21, wpb=65536, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.639, loss_scale=16, train_wall=460, gb_free=10, wall=187088
2022-02-13 21:39:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:45:34 | INFO | train_inner | epoch 026:    237 / 1576 loss=4.829, ppl=28.43, wps=13783.8, ups=0.21, wpb=65536, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.653, loss_scale=8, train_wall=465, gb_free=10, wall=187564
2022-02-13 21:53:24 | INFO | train_inner | epoch 026:    337 / 1576 loss=4.827, ppl=28.39, wps=13931.1, ups=0.21, wpb=65536, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.657, loss_scale=8, train_wall=460, gb_free=10, wall=188034
2022-02-13 21:59:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:01:19 | INFO | train_inner | epoch 026:    438 / 1576 loss=4.838, ppl=28.61, wps=13792, ups=0.21, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=0.67, loss_scale=8, train_wall=464, gb_free=10, wall=188509
2022-02-13 22:09:10 | INFO | train_inner | epoch 026:    538 / 1576 loss=4.852, ppl=28.89, wps=13928.8, ups=0.21, wpb=65536, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.657, loss_scale=8, train_wall=460, gb_free=10, wall=188980
2022-02-13 22:17:00 | INFO | train_inner | epoch 026:    638 / 1576 loss=4.859, ppl=29.01, wps=13932.8, ups=0.21, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.669, loss_scale=8, train_wall=460, gb_free=10, wall=189450
2022-02-13 22:23:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:24:56 | INFO | train_inner | epoch 026:    739 / 1576 loss=4.872, ppl=29.29, wps=13786.6, ups=0.21, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.657, loss_scale=8, train_wall=465, gb_free=10, wall=189925
2022-02-13 22:32:46 | INFO | train_inner | epoch 026:    839 / 1576 loss=4.878, ppl=29.4, wps=13936.8, ups=0.21, wpb=65536, bsz=128, num_updates=40100, lr=0.000157917, gnorm=0.67, loss_scale=8, train_wall=460, gb_free=10, wall=190396
2022-02-13 22:40:37 | INFO | train_inner | epoch 026:    939 / 1576 loss=4.884, ppl=29.54, wps=13927.1, ups=0.21, wpb=65536, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.687, loss_scale=8, train_wall=460, gb_free=10, wall=190866
2022-02-13 22:44:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:48:31 | INFO | train_inner | epoch 026:   1040 / 1576 loss=4.879, ppl=29.43, wps=13797.8, ups=0.21, wpb=65536, bsz=128, num_updates=40300, lr=0.000157524, gnorm=0.662, loss_scale=8, train_wall=464, gb_free=10, wall=191341
2022-02-13 22:56:22 | INFO | train_inner | epoch 026:   1140 / 1576 loss=4.904, ppl=29.95, wps=13928.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.664, loss_scale=8, train_wall=460, gb_free=10, wall=191812
2022-02-13 23:04:12 | INFO | train_inner | epoch 026:   1240 / 1576 loss=4.891, ppl=29.66, wps=13931.7, ups=0.21, wpb=65536, bsz=128, num_updates=40500, lr=0.000157135, gnorm=0.648, loss_scale=8, train_wall=460, gb_free=10, wall=192282
2022-02-13 23:07:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 23:12:08 | INFO | train_inner | epoch 026:   1341 / 1576 loss=4.888, ppl=29.62, wps=13793.4, ups=0.21, wpb=65536, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.644, loss_scale=8, train_wall=465, gb_free=10, wall=192757
2022-02-13 23:19:58 | INFO | train_inner | epoch 026:   1441 / 1576 loss=4.904, ppl=29.95, wps=13930.9, ups=0.21, wpb=65536, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.668, loss_scale=8, train_wall=460, gb_free=10, wall=193228
2022-02-13 23:27:48 | INFO | train_inner | epoch 026:   1541 / 1576 loss=4.904, ppl=29.94, wps=13932.8, ups=0.21, wpb=65536, bsz=128, num_updates=40800, lr=0.000156556, gnorm=0.668, loss_scale=16, train_wall=460, gb_free=10, wall=193698
2022-02-13 23:29:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 23:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 23:30:35 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.088 | ppl 34.02 | wps 36486.1 | wpb 1021.8 | bsz 2 | num_updates 40834 | best_loss 5.088
2022-02-13 23:30:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 40834 updates
2022-02-13 23:30:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint26.pt
2022-02-13 23:30:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint26.pt
2022-02-13 23:31:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint26.pt (epoch 26 @ 40834 updates, score 5.088) (writing took 29.35182010103017 seconds)
2022-02-13 23:31:04 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-13 23:31:04 | INFO | train | epoch 026 | loss 4.868 | ppl 29.19 | wps 13810.4 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 40834 | lr 0.000156491 | gnorm 0.662 | loss_scale 8 | train_wall 7245 | gb_free 10 | wall 193894
2022-02-13 23:31:04 | INFO | fairseq.trainer | begin training epoch 27
2022-02-13 23:31:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 23:36:15 | INFO | train_inner | epoch 027:     66 / 1576 loss=4.829, ppl=28.43, wps=12819.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=40900, lr=0.000156365, gnorm=0.666, loss_scale=8, train_wall=461, gb_free=10, wall=194205
2022-02-13 23:44:06 | INFO | train_inner | epoch 027:    166 / 1576 loss=4.808, ppl=28.01, wps=13917.8, ups=0.21, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.665, loss_scale=8, train_wall=460, gb_free=10, wall=194676
2022-02-13 23:51:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 23:52:02 | INFO | train_inner | epoch 027:    267 / 1576 loss=4.813, ppl=28.11, wps=13779.6, ups=0.21, wpb=65536, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.667, loss_scale=8, train_wall=465, gb_free=10, wall=195151
2022-02-13 23:59:52 | INFO | train_inner | epoch 027:    367 / 1576 loss=4.829, ppl=28.43, wps=13925.6, ups=0.21, wpb=65536, bsz=128, num_updates=41200, lr=0.000155794, gnorm=0.659, loss_scale=8, train_wall=460, gb_free=10, wall=195622
2022-02-14 00:07:43 | INFO | train_inner | epoch 027:    467 / 1576 loss=4.839, ppl=28.63, wps=13929.5, ups=0.21, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.662, loss_scale=8, train_wall=460, gb_free=10, wall=196092
2022-02-14 00:15:33 | INFO | train_inner | epoch 027:    567 / 1576 loss=4.832, ppl=28.49, wps=13924.3, ups=0.21, wpb=65536, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.664, loss_scale=16, train_wall=460, gb_free=10, wall=196563
2022-02-14 00:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:23:29 | INFO | train_inner | epoch 027:    668 / 1576 loss=4.857, ppl=28.99, wps=13786.1, ups=0.21, wpb=65536, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.681, loss_scale=8, train_wall=465, gb_free=10, wall=197038
2022-02-14 00:31:19 | INFO | train_inner | epoch 027:    768 / 1576 loss=4.858, ppl=28.99, wps=13922.3, ups=0.21, wpb=65536, bsz=128, num_updates=41600, lr=0.000155043, gnorm=0.653, loss_scale=8, train_wall=460, gb_free=10, wall=197509
2022-02-14 00:37:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:39:15 | INFO | train_inner | epoch 027:    869 / 1576 loss=4.874, ppl=29.33, wps=13787.8, ups=0.21, wpb=65536, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.676, loss_scale=8, train_wall=465, gb_free=10, wall=197984
2022-02-14 00:47:05 | INFO | train_inner | epoch 027:    969 / 1576 loss=4.873, ppl=29.31, wps=13929.2, ups=0.21, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.672, loss_scale=8, train_wall=460, gb_free=10, wall=198455
2022-02-14 00:54:56 | INFO | train_inner | epoch 027:   1069 / 1576 loss=4.887, ppl=29.58, wps=13923.6, ups=0.21, wpb=65536, bsz=128, num_updates=41900, lr=0.000154487, gnorm=0.653, loss_scale=8, train_wall=460, gb_free=10, wall=198926
2022-02-14 00:57:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:02:51 | INFO | train_inner | epoch 027:   1170 / 1576 loss=4.892, ppl=29.7, wps=13792.1, ups=0.21, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=0.66, loss_scale=8, train_wall=465, gb_free=10, wall=199401
2022-02-14 01:10:42 | INFO | train_inner | epoch 027:   1270 / 1576 loss=4.904, ppl=29.94, wps=13921.8, ups=0.21, wpb=65536, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.657, loss_scale=8, train_wall=460, gb_free=10, wall=199872
2022-02-14 01:18:32 | INFO | train_inner | epoch 027:   1370 / 1576 loss=4.885, ppl=29.55, wps=13926.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.674, loss_scale=16, train_wall=460, gb_free=10, wall=200342
2022-02-14 01:26:23 | INFO | train_inner | epoch 027:   1470 / 1576 loss=4.89, ppl=29.65, wps=13918.7, ups=0.21, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.646, loss_scale=16, train_wall=460, gb_free=10, wall=200813
2022-02-14 01:28:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:34:18 | INFO | train_inner | epoch 027:   1571 / 1576 loss=4.896, ppl=29.77, wps=13791.6, ups=0.21, wpb=65536, bsz=128, num_updates=42400, lr=0.000153574, gnorm=0.65, loss_scale=8, train_wall=465, gb_free=10, wall=201288
2022-02-14 01:34:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 01:34:44 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.083 | ppl 33.91 | wps 36659 | wpb 1021.8 | bsz 2 | num_updates 42405 | best_loss 5.083
2022-02-14 01:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 42405 updates
2022-02-14 01:34:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint27.pt
2022-02-14 01:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint27.pt
2022-02-14 01:35:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint27.pt (epoch 27 @ 42405 updates, score 5.083) (writing took 30.588773895986378 seconds)
2022-02-14 01:35:15 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-14 01:35:15 | INFO | train | epoch 027 | loss 4.86 | ppl 29.03 | wps 13811.7 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 42405 | lr 0.000153565 | gnorm 0.662 | loss_scale 8 | train_wall 7247 | gb_free 10 | wall 201344
2022-02-14 01:35:15 | INFO | fairseq.trainer | begin training epoch 28
2022-02-14 01:35:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 01:42:41 | INFO | train_inner | epoch 028:     95 / 1576 loss=4.795, ppl=27.76, wps=12913.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=42500, lr=0.000153393, gnorm=0.677, loss_scale=8, train_wall=456, gb_free=10, wall=201791
2022-02-14 01:50:32 | INFO | train_inner | epoch 028:    195 / 1576 loss=4.802, ppl=27.89, wps=13935.8, ups=0.21, wpb=65536, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.657, loss_scale=16, train_wall=460, gb_free=10, wall=202261
2022-02-14 01:58:23 | INFO | train_inner | epoch 028:    295 / 1576 loss=4.81, ppl=28.06, wps=13914.9, ups=0.21, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.673, loss_scale=16, train_wall=460, gb_free=10, wall=202732
2022-02-14 02:06:13 | INFO | train_inner | epoch 028:    395 / 1576 loss=4.823, ppl=28.31, wps=13920.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=42800, lr=0.000152854, gnorm=0.648, loss_scale=16, train_wall=460, gb_free=10, wall=203203
2022-02-14 02:07:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:14:09 | INFO | train_inner | epoch 028:    496 / 1576 loss=4.82, ppl=28.25, wps=13786.1, ups=0.21, wpb=65536, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.676, loss_scale=8, train_wall=465, gb_free=10, wall=203679
2022-02-14 02:21:59 | INFO | train_inner | epoch 028:    596 / 1576 loss=4.84, ppl=28.64, wps=13933.9, ups=0.21, wpb=65536, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.645, loss_scale=8, train_wall=460, gb_free=10, wall=204149
2022-02-14 02:28:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:29:54 | INFO | train_inner | epoch 028:    697 / 1576 loss=4.85, ppl=28.85, wps=13791.9, ups=0.21, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.669, loss_scale=8, train_wall=465, gb_free=10, wall=204624
2022-02-14 02:37:45 | INFO | train_inner | epoch 028:    797 / 1576 loss=4.843, ppl=28.69, wps=13936.5, ups=0.21, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=0.652, loss_scale=8, train_wall=460, gb_free=10, wall=205094
2022-02-14 02:45:35 | INFO | train_inner | epoch 028:    897 / 1576 loss=4.853, ppl=28.89, wps=13928.7, ups=0.21, wpb=65536, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.671, loss_scale=8, train_wall=460, gb_free=10, wall=205565
2022-02-14 02:49:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:53:30 | INFO | train_inner | epoch 028:    998 / 1576 loss=4.866, ppl=29.17, wps=13791.6, ups=0.21, wpb=65536, bsz=128, num_updates=43400, lr=0.000151794, gnorm=0.651, loss_scale=8, train_wall=465, gb_free=10, wall=206040
2022-02-14 03:01:21 | INFO | train_inner | epoch 028:   1098 / 1576 loss=4.881, ppl=29.47, wps=13933.2, ups=0.21, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.687, loss_scale=8, train_wall=460, gb_free=10, wall=206510
2022-02-14 03:09:11 | INFO | train_inner | epoch 028:   1198 / 1576 loss=4.873, ppl=29.3, wps=13930, ups=0.21, wpb=65536, bsz=128, num_updates=43600, lr=0.000151446, gnorm=0.648, loss_scale=8, train_wall=460, gb_free=10, wall=206981
2022-02-14 03:12:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:17:06 | INFO | train_inner | epoch 028:   1299 / 1576 loss=4.892, ppl=29.69, wps=13789.9, ups=0.21, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.674, loss_scale=8, train_wall=465, gb_free=10, wall=207456
2022-02-14 03:24:57 | INFO | train_inner | epoch 028:   1399 / 1576 loss=4.9, ppl=29.85, wps=13929.5, ups=0.21, wpb=65536, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.673, loss_scale=8, train_wall=460, gb_free=10, wall=207927
2022-02-14 03:32:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:32:52 | INFO | train_inner | epoch 028:   1500 / 1576 loss=4.891, ppl=29.67, wps=13792.6, ups=0.21, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.673, loss_scale=8, train_wall=465, gb_free=10, wall=208402
2022-02-14 03:38:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 03:38:51 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.09 | ppl 34.05 | wps 37305.3 | wpb 1021.8 | bsz 2 | num_updates 43976 | best_loss 5.083
2022-02-14 03:38:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 43976 updates
2022-02-14 03:38:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint28.pt
2022-02-14 03:39:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint28.pt
2022-02-14 03:39:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint28.pt (epoch 28 @ 43976 updates, score 5.09) (writing took 19.446041978895664 seconds)
2022-02-14 03:39:11 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-14 03:39:11 | INFO | train | epoch 028 | loss 4.851 | ppl 28.87 | wps 13837.3 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 43976 | lr 0.000150797 | gnorm 0.664 | loss_scale 8 | train_wall 7245 | gb_free 10 | wall 208781
2022-02-14 03:39:11 | INFO | fairseq.trainer | begin training epoch 29
2022-02-14 03:39:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 03:41:04 | INFO | train_inner | epoch 029:     24 / 1576 loss=4.869, ppl=29.22, wps=13207.7, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=44000, lr=0.000150756, gnorm=0.678, loss_scale=8, train_wall=456, gb_free=10, wall=208894
2022-02-14 03:48:54 | INFO | train_inner | epoch 029:    124 / 1576 loss=4.787, ppl=27.61, wps=13927.5, ups=0.21, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.671, loss_scale=8, train_wall=460, gb_free=10, wall=209364
2022-02-14 03:56:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:56:50 | INFO | train_inner | epoch 029:    225 / 1576 loss=4.797, ppl=27.8, wps=13783.1, ups=0.21, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.697, loss_scale=8, train_wall=465, gb_free=10, wall=209840
2022-02-14 04:04:41 | INFO | train_inner | epoch 029:    325 / 1576 loss=4.81, ppl=28.06, wps=13915.8, ups=0.21, wpb=65536, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.671, loss_scale=8, train_wall=460, gb_free=10, wall=210311
2022-02-14 04:12:31 | INFO | train_inner | epoch 029:    425 / 1576 loss=4.815, ppl=28.16, wps=13927.5, ups=0.21, wpb=65536, bsz=128, num_updates=44400, lr=0.000150075, gnorm=0.642, loss_scale=8, train_wall=460, gb_free=10, wall=210781
2022-02-14 04:20:22 | INFO | train_inner | epoch 029:    525 / 1576 loss=4.825, ppl=28.34, wps=13917.9, ups=0.21, wpb=65536, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.654, loss_scale=16, train_wall=460, gb_free=10, wall=211252
2022-02-14 04:21:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 04:28:17 | INFO | train_inner | epoch 029:    626 / 1576 loss=4.839, ppl=28.62, wps=13793.5, ups=0.21, wpb=65536, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.679, loss_scale=8, train_wall=464, gb_free=10, wall=211727
2022-02-14 04:36:08 | INFO | train_inner | epoch 029:    726 / 1576 loss=4.847, ppl=28.79, wps=13927.6, ups=0.21, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.667, loss_scale=8, train_wall=460, gb_free=10, wall=212198
2022-02-14 04:43:59 | INFO | train_inner | epoch 029:    826 / 1576 loss=4.853, ppl=28.9, wps=13925.7, ups=0.21, wpb=65536, bsz=128, num_updates=44800, lr=0.000149404, gnorm=0.653, loss_scale=16, train_wall=460, gb_free=10, wall=212668
2022-02-14 04:45:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 04:51:54 | INFO | train_inner | epoch 029:    927 / 1576 loss=4.851, ppl=28.87, wps=13791.1, ups=0.21, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.664, loss_scale=8, train_wall=465, gb_free=10, wall=213143
2022-02-14 04:59:44 | INFO | train_inner | epoch 029:   1027 / 1576 loss=4.857, ppl=28.98, wps=13927.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=45000, lr=0.000149071, gnorm=0.675, loss_scale=8, train_wall=460, gb_free=10, wall=213614
2022-02-14 05:07:35 | INFO | train_inner | epoch 029:   1127 / 1576 loss=4.87, ppl=29.24, wps=13924, ups=0.21, wpb=65536, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.672, loss_scale=16, train_wall=460, gb_free=10, wall=214085
2022-02-14 05:09:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:15:30 | INFO | train_inner | epoch 029:   1228 / 1576 loss=4.871, ppl=29.26, wps=13791.9, ups=0.21, wpb=65536, bsz=128, num_updates=45200, lr=0.000148741, gnorm=0.664, loss_scale=8, train_wall=465, gb_free=10, wall=214560
2022-02-14 05:23:21 | INFO | train_inner | epoch 029:   1328 / 1576 loss=4.878, ppl=29.4, wps=13931, ups=0.21, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.681, loss_scale=8, train_wall=460, gb_free=10, wall=215030
2022-02-14 05:31:11 | INFO | train_inner | epoch 029:   1428 / 1576 loss=4.871, ppl=29.27, wps=13930.1, ups=0.21, wpb=65536, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.66, loss_scale=16, train_wall=460, gb_free=10, wall=215501
2022-02-14 05:33:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:39:06 | INFO | train_inner | epoch 029:   1529 / 1576 loss=4.882, ppl=29.49, wps=13797.1, ups=0.21, wpb=65536, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.665, loss_scale=8, train_wall=464, gb_free=10, wall=215976
2022-02-14 05:42:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 05:42:49 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.075 | ppl 33.71 | wps 37333.3 | wpb 1021.8 | bsz 2 | num_updates 45547 | best_loss 5.075
2022-02-14 05:42:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 45547 updates
2022-02-14 05:42:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint29.pt
2022-02-14 05:42:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint29.pt
2022-02-14 05:43:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint29.pt (epoch 29 @ 45547 updates, score 5.075) (writing took 28.793232403695583 seconds)
2022-02-14 05:43:18 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-14 05:43:18 | INFO | train | epoch 029 | loss 4.844 | ppl 28.71 | wps 13817.6 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 45547 | lr 0.000148173 | gnorm 0.668 | loss_scale 8 | train_wall 7246 | gb_free 10 | wall 216228
2022-02-14 05:43:18 | INFO | fairseq.trainer | begin training epoch 30
2022-02-14 05:43:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 05:47:27 | INFO | train_inner | epoch 030:     53 / 1576 loss=4.818, ppl=28.22, wps=12962.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=45600, lr=0.000148087, gnorm=0.662, loss_scale=8, train_wall=456, gb_free=10, wall=216477
2022-02-14 05:55:18 | INFO | train_inner | epoch 030:    153 / 1576 loss=4.791, ppl=27.68, wps=13922.8, ups=0.21, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.673, loss_scale=16, train_wall=460, gb_free=10, wall=216948
2022-02-14 05:56:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:03:13 | INFO | train_inner | epoch 030:    254 / 1576 loss=4.783, ppl=27.53, wps=13783.4, ups=0.21, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.679, loss_scale=8, train_wall=465, gb_free=10, wall=217423
2022-02-14 06:11:04 | INFO | train_inner | epoch 030:    354 / 1576 loss=4.801, ppl=27.88, wps=13925.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=45900, lr=0.000147602, gnorm=0.684, loss_scale=8, train_wall=460, gb_free=10, wall=217894
2022-02-14 06:18:55 | INFO | train_inner | epoch 030:    454 / 1576 loss=4.811, ppl=28.08, wps=13923.9, ups=0.21, wpb=65536, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.668, loss_scale=16, train_wall=460, gb_free=10, wall=218364
2022-02-14 06:20:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:26:50 | INFO | train_inner | epoch 030:    555 / 1576 loss=4.823, ppl=28.31, wps=13791.7, ups=0.21, wpb=65536, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.681, loss_scale=8, train_wall=465, gb_free=10, wall=218840
2022-02-14 06:34:40 | INFO | train_inner | epoch 030:    655 / 1576 loss=4.83, ppl=28.44, wps=13929.5, ups=0.21, wpb=65536, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.681, loss_scale=8, train_wall=460, gb_free=10, wall=219310
2022-02-14 06:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:42:36 | INFO | train_inner | epoch 030:    756 / 1576 loss=4.852, ppl=28.88, wps=13792.5, ups=0.21, wpb=65536, bsz=128, num_updates=46300, lr=0.000146964, gnorm=0.67, loss_scale=8, train_wall=464, gb_free=10, wall=219785
2022-02-14 06:50:26 | INFO | train_inner | epoch 030:    856 / 1576 loss=4.847, ppl=28.78, wps=13923.4, ups=0.21, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.68, loss_scale=8, train_wall=460, gb_free=10, wall=220256
2022-02-14 06:58:17 | INFO | train_inner | epoch 030:    956 / 1576 loss=4.849, ppl=28.83, wps=13923.6, ups=0.21, wpb=65536, bsz=128, num_updates=46500, lr=0.000146647, gnorm=0.676, loss_scale=8, train_wall=460, gb_free=10, wall=220727
2022-02-14 07:03:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:06:12 | INFO | train_inner | epoch 030:   1057 / 1576 loss=4.857, ppl=28.99, wps=13787.2, ups=0.21, wpb=65536, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.67, loss_scale=8, train_wall=465, gb_free=10, wall=221202
2022-02-14 07:14:03 | INFO | train_inner | epoch 030:   1157 / 1576 loss=4.859, ppl=29.02, wps=13924.8, ups=0.21, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=0.685, loss_scale=8, train_wall=460, gb_free=10, wall=221673
2022-02-14 07:21:53 | INFO | train_inner | epoch 030:   1257 / 1576 loss=4.863, ppl=29.09, wps=13932.4, ups=0.21, wpb=65536, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.668, loss_scale=8, train_wall=460, gb_free=10, wall=222143
2022-02-14 07:29:44 | INFO | train_inner | epoch 030:   1357 / 1576 loss=4.871, ppl=29.26, wps=13929.7, ups=0.21, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.665, loss_scale=16, train_wall=460, gb_free=10, wall=222613
2022-02-14 07:35:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:37:39 | INFO | train_inner | epoch 030:   1458 / 1576 loss=4.87, ppl=29.25, wps=13793.5, ups=0.21, wpb=65536, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.659, loss_scale=8, train_wall=464, gb_free=10, wall=223089
2022-02-14 07:45:29 | INFO | train_inner | epoch 030:   1558 / 1576 loss=4.876, ppl=29.36, wps=13929.8, ups=0.21, wpb=65536, bsz=128, num_updates=47100, lr=0.00014571, gnorm=0.663, loss_scale=8, train_wall=460, gb_free=10, wall=223559
2022-02-14 07:46:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 07:46:56 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.074 | ppl 33.69 | wps 37308.7 | wpb 1021.8 | bsz 2 | num_updates 47118 | best_loss 5.074
2022-02-14 07:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 47118 updates
2022-02-14 07:46:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint30.pt
2022-02-14 07:47:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint30.pt
2022-02-14 07:47:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint30.pt (epoch 30 @ 47118 updates, score 5.074) (writing took 28.85047613736242 seconds)
2022-02-14 07:47:25 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-14 07:47:25 | INFO | train | epoch 030 | loss 4.837 | ppl 28.58 | wps 13817.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 47118 | lr 0.000145682 | gnorm 0.674 | loss_scale 8 | train_wall 7246 | gb_free 10 | wall 223674
2022-02-14 07:47:25 | INFO | fairseq.trainer | begin training epoch 31
2022-02-14 07:47:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 07:53:51 | INFO | train_inner | epoch 031:     82 / 1576 loss=4.788, ppl=27.63, wps=12962, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=47200, lr=0.000145556, gnorm=0.686, loss_scale=8, train_wall=456, gb_free=10, wall=224060
2022-02-14 07:56:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:01:46 | INFO | train_inner | epoch 031:    183 / 1576 loss=4.769, ppl=27.26, wps=13788.2, ups=0.21, wpb=65536, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.676, loss_scale=8, train_wall=465, gb_free=10, wall=224536
2022-02-14 08:09:36 | INFO | train_inner | epoch 031:    283 / 1576 loss=4.782, ppl=27.51, wps=13927.1, ups=0.21, wpb=65536, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.672, loss_scale=8, train_wall=460, gb_free=10, wall=225006
2022-02-14 08:17:27 | INFO | train_inner | epoch 031:    383 / 1576 loss=4.803, ppl=27.92, wps=13927, ups=0.21, wpb=65536, bsz=128, num_updates=47500, lr=0.000145095, gnorm=0.659, loss_scale=16, train_wall=460, gb_free=10, wall=225477
2022-02-14 08:18:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:25:22 | INFO | train_inner | epoch 031:    484 / 1576 loss=4.804, ppl=27.94, wps=13792.8, ups=0.21, wpb=65536, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.692, loss_scale=8, train_wall=464, gb_free=10, wall=225952
2022-02-14 08:33:13 | INFO | train_inner | epoch 031:    584 / 1576 loss=4.822, ppl=28.28, wps=13923.8, ups=0.21, wpb=65536, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.681, loss_scale=8, train_wall=460, gb_free=10, wall=226422
2022-02-14 08:38:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:41:08 | INFO | train_inner | epoch 031:    685 / 1576 loss=4.825, ppl=28.35, wps=13791.5, ups=0.21, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.674, loss_scale=8, train_wall=465, gb_free=10, wall=226898
2022-02-14 08:48:58 | INFO | train_inner | epoch 031:    785 / 1576 loss=4.827, ppl=28.38, wps=13928.5, ups=0.21, wpb=65536, bsz=128, num_updates=47900, lr=0.000144488, gnorm=0.669, loss_scale=8, train_wall=460, gb_free=10, wall=227368
2022-02-14 08:56:49 | INFO | train_inner | epoch 031:    885 / 1576 loss=4.849, ppl=28.81, wps=13930.9, ups=0.21, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.668, loss_scale=8, train_wall=460, gb_free=10, wall=227839
2022-02-14 09:04:40 | INFO | train_inner | epoch 031:    985 / 1576 loss=4.843, ppl=28.71, wps=13922.2, ups=0.21, wpb=65536, bsz=128, num_updates=48100, lr=0.000144187, gnorm=0.685, loss_scale=16, train_wall=460, gb_free=10, wall=228309
2022-02-14 09:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:12:35 | INFO | train_inner | epoch 031:   1086 / 1576 loss=4.85, ppl=28.85, wps=13793.4, ups=0.21, wpb=65536, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.67, loss_scale=8, train_wall=464, gb_free=10, wall=228784
2022-02-14 09:20:25 | INFO | train_inner | epoch 031:   1186 / 1576 loss=4.856, ppl=28.97, wps=13935.4, ups=0.21, wpb=65532.3, bsz=128, num_updates=48300, lr=0.000143889, gnorm=0.695, loss_scale=8, train_wall=460, gb_free=10, wall=229255
2022-02-14 09:28:16 | INFO | train_inner | epoch 031:   1286 / 1576 loss=4.858, ppl=29.01, wps=13923.7, ups=0.21, wpb=65536, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.702, loss_scale=16, train_wall=460, gb_free=10, wall=229725
2022-02-14 09:29:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:36:11 | INFO | train_inner | epoch 031:   1387 / 1576 loss=4.862, ppl=29.08, wps=13795.9, ups=0.21, wpb=65536, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.67, loss_scale=8, train_wall=464, gb_free=10, wall=230200
2022-02-14 09:44:01 | INFO | train_inner | epoch 031:   1487 / 1576 loss=4.875, ppl=29.34, wps=13932.9, ups=0.21, wpb=65536, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.663, loss_scale=8, train_wall=460, gb_free=10, wall=230671
2022-02-14 09:50:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 09:51:02 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.078 | ppl 33.77 | wps 37330.8 | wpb 1021.8 | bsz 2 | num_updates 48689 | best_loss 5.074
2022-02-14 09:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 48689 updates
2022-02-14 09:51:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint31.pt
2022-02-14 09:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint31.pt
2022-02-14 09:51:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint31.pt (epoch 31 @ 48689 updates, score 5.078) (writing took 19.393855589441955 seconds)
2022-02-14 09:51:21 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-14 09:51:21 | INFO | train | epoch 031 | loss 4.831 | ppl 28.46 | wps 13837.3 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 48689 | lr 0.000143313 | gnorm 0.678 | loss_scale 16 | train_wall 7244 | gb_free 10 | wall 231111
2022-02-14 09:51:21 | INFO | fairseq.trainer | begin training epoch 32
2022-02-14 09:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 09:52:13 | INFO | train_inner | epoch 032:     11 / 1576 loss=4.881, ppl=29.47, wps=13207.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=48700, lr=0.000143296, gnorm=0.716, loss_scale=16, train_wall=456, gb_free=10, wall=231163
2022-02-14 09:53:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:00:08 | INFO | train_inner | epoch 032:    112 / 1576 loss=4.757, ppl=27.04, wps=13790.5, ups=0.21, wpb=65536, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.66, loss_scale=8, train_wall=464, gb_free=10, wall=231638
2022-02-14 10:07:59 | INFO | train_inner | epoch 032:    212 / 1576 loss=4.78, ppl=27.48, wps=13921.5, ups=0.21, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.679, loss_scale=8, train_wall=460, gb_free=10, wall=232109
2022-02-14 10:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:15:54 | INFO | train_inner | epoch 032:    313 / 1576 loss=4.783, ppl=27.53, wps=13789.8, ups=0.21, wpb=65536, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.688, loss_scale=8, train_wall=465, gb_free=10, wall=232584
2022-02-14 10:23:45 | INFO | train_inner | epoch 032:    413 / 1576 loss=4.804, ppl=27.93, wps=13921.5, ups=0.21, wpb=65536, bsz=128, num_updates=49100, lr=0.000142712, gnorm=0.686, loss_scale=8, train_wall=460, gb_free=10, wall=233055
2022-02-14 10:31:36 | INFO | train_inner | epoch 032:    513 / 1576 loss=4.803, ppl=27.92, wps=13928.1, ups=0.21, wpb=65536, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.69, loss_scale=8, train_wall=460, gb_free=10, wall=233525
2022-02-14 10:39:26 | INFO | train_inner | epoch 032:    613 / 1576 loss=4.813, ppl=28.11, wps=13926.4, ups=0.21, wpb=65536, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.675, loss_scale=16, train_wall=460, gb_free=10, wall=233996
2022-02-14 10:41:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:47:21 | INFO | train_inner | epoch 032:    714 / 1576 loss=4.84, ppl=28.63, wps=13795.5, ups=0.21, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.69, loss_scale=8, train_wall=464, gb_free=10, wall=234471
2022-02-14 10:55:11 | INFO | train_inner | epoch 032:    814 / 1576 loss=4.837, ppl=28.59, wps=13933.6, ups=0.21, wpb=65536, bsz=128, num_updates=49500, lr=0.000142134, gnorm=0.677, loss_scale=8, train_wall=460, gb_free=10, wall=234941
2022-02-14 11:03:02 | INFO | train_inner | epoch 032:    914 / 1576 loss=4.834, ppl=28.52, wps=13932.1, ups=0.21, wpb=65536, bsz=128, num_updates=49600, lr=0.00014199, gnorm=0.683, loss_scale=16, train_wall=460, gb_free=10, wall=235412
2022-02-14 11:04:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:10:57 | INFO | train_inner | epoch 032:   1015 / 1576 loss=4.833, ppl=28.5, wps=13792.8, ups=0.21, wpb=65536, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.68, loss_scale=8, train_wall=464, gb_free=10, wall=235887
2022-02-14 11:18:48 | INFO | train_inner | epoch 032:   1115 / 1576 loss=4.841, ppl=28.67, wps=13925.9, ups=0.21, wpb=65536, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.699, loss_scale=8, train_wall=460, gb_free=10, wall=236357
2022-02-14 11:26:38 | INFO | train_inner | epoch 032:   1215 / 1576 loss=4.84, ppl=28.64, wps=13929.3, ups=0.21, wpb=65536, bsz=128, num_updates=49900, lr=0.000141563, gnorm=0.672, loss_scale=16, train_wall=460, gb_free=10, wall=236828
2022-02-14 11:27:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:34:33 | INFO | train_inner | epoch 032:   1316 / 1576 loss=4.853, ppl=28.9, wps=13793.5, ups=0.21, wpb=65536, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.684, loss_scale=8, train_wall=464, gb_free=10, wall=237303
2022-02-14 11:34:33 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2022-02-14 11:34:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 11:34:39 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.074 | ppl 33.68 | wps 37212.4 | wpb 1021.8 | bsz 2 | num_updates 50000 | best_loss 5.074
2022-02-14 11:34:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 50000 updates
2022-02-14 11:34:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint_best.pt
2022-02-14 11:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint_best.pt
2022-02-14 11:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 32 @ 50000 updates, score 5.074) (writing took 18.873143505305052 seconds)
2022-02-14 11:34:58 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-14 11:34:58 | INFO | train | epoch 032 | loss 4.817 | ppl 28.18 | wps 13820 | ups 0.21 | wpb 65536 | bsz 128 | num_updates 50000 | lr 0.000141421 | gnorm 0.682 | loss_scale 8 | train_wall 6053 | gb_free 10 | wall 237328
2022-02-14 11:34:58 | INFO | fairseq_cli.train | done training in 237327.2 seconds
