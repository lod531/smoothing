Sender: LSF System <lsfadmin@eu-g3-082>
Subject: Job 207345377: <w103_size_0.125_fp16_label_smoothing_0.1_#2> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.1_#2> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 12:42:15 2022
Job was executed on host(s) <eu-g3-082>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Sun Mar  6 12:42:46 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 12:42:46 2022
Terminated at Tue Mar  8 06:20:45 2022
Results reported at Tue Mar  8 06:20:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   149450.17 sec.
    Max Memory :                                 6748 MB
    Average Memory :                             3641.12 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13252.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   149879 sec.
    Turnaround time :                            149910 sec.

The output (if any) follows:

2022-03-06 12:42:53 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 12:42:53 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-06 12:42:56 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-06 12:42:56 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 12:42:56 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 12:42:56 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-06 12:42:56 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-06 12:42:56 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 12:42:56 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-06 12:42:58 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 12:42:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:42:58 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-06 12:42:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:42:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 12:42:58 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 12:42:58 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 12:42:58 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 12:42:58 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 12:42:59 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-06 12:42:59 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 12:42:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:43:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 12:43:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 12:43:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 12:43:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 12:43:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 12:48:42 | INFO | train_inner | epoch 001:    105 / 196 loss=16.606, nll_loss=16.409, ppl=87026.1, wps=20962.8, ups=0.32, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.253, loss_scale=4, train_wall=320, gb_free=19.9, wall=343
2022-03-06 12:53:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 12:53:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.554 | nll_loss 13.014 | ppl 8273.86 | wps 41256 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-06 12:53:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-06 12:53:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 12:53:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 12:53:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.554) (writing took 7.293941190466285 seconds)
2022-03-06 12:53:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 12:53:36 | INFO | train | epoch 001 | loss 15.604 | nll_loss 15.298 | ppl 40285.8 | wps 20582 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.351 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 638
2022-03-06 12:53:37 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 12:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:54:05 | INFO | train_inner | epoch 002:      9 / 196 loss=14.419, nll_loss=13.983, ppl=16189.3, wps=20253.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.337, loss_scale=8, train_wall=288, gb_free=19.9, wall=666
2022-03-06 12:59:16 | INFO | train_inner | epoch 002:    109 / 196 loss=12.684, nll_loss=12.028, ppl=4175.3, wps=21072.1, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.866, loss_scale=16, train_wall=289, gb_free=19.9, wall=977
2022-03-06 13:03:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:03:51 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.249 | nll_loss 10.318 | ppl 1276.17 | wps 41690.4 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 11.249
2022-03-06 13:03:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-06 13:03:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:03:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 2 @ 387 updates, score 11.249) (writing took 7.937834269367158 seconds)
2022-03-06 13:03:59 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 13:03:59 | INFO | train | epoch 002 | loss 12.238 | nll_loss 11.502 | ppl 2899.6 | wps 20615.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.725 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 1260
2022-03-06 13:03:59 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 13:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:04:39 | INFO | train_inner | epoch 003:     13 / 196 loss=11.542, nll_loss=10.682, ppl=1643.14, wps=20195.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.51, loss_scale=32, train_wall=288, gb_free=19.9, wall=1301
2022-03-06 13:09:49 | INFO | train_inner | epoch 003:    113 / 196 loss=11.078, nll_loss=10.099, ppl=1096.81, wps=21133.1, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.473, loss_scale=32, train_wall=288, gb_free=19.9, wall=1611
2022-03-06 13:10:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:14:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.662 | nll_loss 9.602 | ppl 777.26 | wps 41416.4 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.662
2022-03-06 13:14:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-06 13:14:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:14:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 3 @ 582 updates, score 10.662) (writing took 6.9558279607445 seconds)
2022-03-06 13:14:18 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 13:14:18 | INFO | train | epoch 003 | loss 10.985 | nll_loss 9.988 | ppl 1015.86 | wps 20597.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.485 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 1880
2022-03-06 13:14:18 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 13:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:15:14 | INFO | train_inner | epoch 004:     18 / 196 loss=10.797, nll_loss=9.764, ppl=869.26, wps=20111.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.528, loss_scale=32, train_wall=290, gb_free=19.9, wall=1936
2022-03-06 13:15:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:20:28 | INFO | train_inner | epoch 004:    119 / 196 loss=10.534, nll_loss=9.463, ppl=705.6, wps=20893.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.633, loss_scale=16, train_wall=291, gb_free=19.9, wall=2250
2022-03-06 13:24:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:24:31 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.235 | nll_loss 9.115 | ppl 554.59 | wps 41520.6 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 10.235
2022-03-06 13:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-06 13:24:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:24:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:24:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 4 @ 777 updates, score 10.235) (writing took 6.8482631072402 seconds)
2022-03-06 13:24:38 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 13:24:38 | INFO | train | epoch 004 | loss 10.474 | nll_loss 9.394 | ppl 672.7 | wps 20589.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.657 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 2500
2022-03-06 13:24:38 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 13:24:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:25:50 | INFO | train_inner | epoch 005:     23 / 196 loss=10.319, nll_loss=9.215, ppl=594.42, wps=20322.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.676, loss_scale=32, train_wall=287, gb_free=19.9, wall=2571
2022-03-06 13:30:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:31:03 | INFO | train_inner | epoch 005:    124 / 196 loss=10.119, nll_loss=8.986, ppl=506.96, wps=20903.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.746, loss_scale=32, train_wall=291, gb_free=19.9, wall=2885
2022-03-06 13:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:34:51 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.886 | nll_loss 8.719 | ppl 421.51 | wps 41268.5 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 9.886
2022-03-06 13:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-06 13:34:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 5 @ 972 updates, score 9.886) (writing took 6.793444035574794 seconds)
2022-03-06 13:34:58 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 13:34:58 | INFO | train | epoch 005 | loss 10.073 | nll_loss 8.933 | ppl 488.84 | wps 20592.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.741 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 3120
2022-03-06 13:34:58 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 13:34:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:36:25 | INFO | train_inner | epoch 006:     28 / 196 loss=9.935, nll_loss=8.775, ppl=438.18, wps=20316.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.788, loss_scale=32, train_wall=287, gb_free=19.9, wall=3206
2022-03-06 13:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:41:38 | INFO | train_inner | epoch 006:    129 / 196 loss=9.764, nll_loss=8.579, ppl=382.4, wps=20905.3, ups=0.32, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.804, loss_scale=32, train_wall=291, gb_free=19.9, wall=3520
2022-03-06 13:44:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:45:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.59 | nll_loss 8.361 | ppl 328.78 | wps 41540.3 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 9.59
2022-03-06 13:45:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-06 13:45:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:45:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:45:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 6 @ 1166 updates, score 9.59) (writing took 6.730889410711825 seconds)
2022-03-06 13:45:18 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 13:45:18 | INFO | train | epoch 006 | loss 9.74 | nll_loss 8.551 | ppl 374.97 | wps 20491 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.804 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 3739
2022-03-06 13:45:18 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 13:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:47:03 | INFO | train_inner | epoch 007:     34 / 196 loss=9.616, nll_loss=8.409, ppl=340, wps=20122.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.81, loss_scale=32, train_wall=290, gb_free=19.9, wall=3845
2022-03-06 13:51:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:52:17 | INFO | train_inner | epoch 007:    135 / 196 loss=9.476, nll_loss=8.248, ppl=303.92, wps=20904.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.822, loss_scale=32, train_wall=291, gb_free=19.9, wall=4158
2022-03-06 13:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:55:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.36 | nll_loss 8.1 | ppl 274.29 | wps 41656.9 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 9.36
2022-03-06 13:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-06 13:55:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:55:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 13:55:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 7 @ 1361 updates, score 9.36) (writing took 7.234723218716681 seconds)
2022-03-06 13:55:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 13:55:38 | INFO | train | epoch 007 | loss 9.461 | nll_loss 8.231 | ppl 300.38 | wps 20572.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.829 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 4360
2022-03-06 13:55:38 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 13:55:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:57:39 | INFO | train_inner | epoch 008:     39 / 196 loss=9.343, nll_loss=8.095, ppl=273.46, wps=20276.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.832, loss_scale=32, train_wall=288, gb_free=19.9, wall=4481
2022-03-06 13:58:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:02:53 | INFO | train_inner | epoch 008:    140 / 196 loss=9.219, nll_loss=7.952, ppl=247.66, wps=20882.9, ups=0.32, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.845, loss_scale=32, train_wall=291, gb_free=19.9, wall=4794
2022-03-06 14:05:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:05:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:05:51 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.148 | nll_loss 7.851 | ppl 230.81 | wps 41010.9 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 9.148
2022-03-06 14:05:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-06 14:05:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:05:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:05:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 8 @ 1555 updates, score 9.148) (writing took 7.334677535109222 seconds)
2022-03-06 14:05:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 14:05:59 | INFO | train | epoch 008 | loss 9.211 | nll_loss 7.943 | ppl 246.06 | wps 20457.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 0.831 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 4980
2022-03-06 14:05:59 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 14:05:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:08:18 | INFO | train_inner | epoch 009:     45 / 196 loss=9.098, nll_loss=7.813, ppl=224.94, wps=20087.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.853, loss_scale=32, train_wall=290, gb_free=19.9, wall=5120
2022-03-06 14:12:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:13:32 | INFO | train_inner | epoch 009:    146 / 196 loss=8.98, nll_loss=7.677, ppl=204.64, wps=20908, ups=0.32, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.829, loss_scale=32, train_wall=291, gb_free=19.9, wall=5433
2022-03-06 14:16:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:16:11 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.961 | nll_loss 7.633 | ppl 198.53 | wps 41265.1 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 8.961
2022-03-06 14:16:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1750 updates
2022-03-06 14:16:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:16:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:16:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 9 @ 1750 updates, score 8.961) (writing took 7.294533408246934 seconds)
2022-03-06 14:16:19 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 14:16:19 | INFO | train | epoch 009 | loss 8.978 | nll_loss 7.674 | ppl 204.21 | wps 20579.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1750 | lr 0.000218806 | gnorm 0.854 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 5600
2022-03-06 14:16:19 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 14:16:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:18:54 | INFO | train_inner | epoch 010:     50 / 196 loss=8.863, nll_loss=7.542, ppl=186.38, wps=20283.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.844, loss_scale=32, train_wall=287, gb_free=19.9, wall=5756
2022-03-06 14:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 14:24:10 | INFO | train_inner | epoch 010:    152 / 196 loss=8.755, nll_loss=7.418, ppl=170.97, wps=20707.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.833, loss_scale=16, train_wall=294, gb_free=19.9, wall=6072
2022-03-06 14:26:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:26:33 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.807 | nll_loss 7.456 | ppl 175.64 | wps 36357.1 | wpb 510.9 | bsz 1 | num_updates 1944 | best_loss 8.807
2022-03-06 14:26:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1944 updates
2022-03-06 14:26:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:26:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:26:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 10 @ 1944 updates, score 8.807) (writing took 7.288554362021387 seconds)
2022-03-06 14:26:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 14:26:40 | INFO | train | epoch 010 | loss 8.757 | nll_loss 7.42 | ppl 171.25 | wps 20434.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1944 | lr 0.000243051 | gnorm 0.825 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 6222
2022-03-06 14:26:40 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 14:26:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:29:35 | INFO | train_inner | epoch 011:     56 / 196 loss=8.643, nll_loss=7.289, ppl=156.39, wps=20119.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.833, loss_scale=16, train_wall=289, gb_free=19.9, wall=6397
2022-03-06 14:34:46 | INFO | train_inner | epoch 011:    156 / 196 loss=8.551, nll_loss=7.182, ppl=145.23, wps=21078.2, ups=0.32, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.793, loss_scale=32, train_wall=289, gb_free=19.9, wall=6708
2022-03-06 14:36:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:36:55 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.684 | nll_loss 7.308 | ppl 158.41 | wps 41309 | wpb 510.9 | bsz 1 | num_updates 2140 | best_loss 8.684
2022-03-06 14:36:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2140 updates
2022-03-06 14:36:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:36:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:37:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 11 @ 2140 updates, score 8.684) (writing took 7.165339780040085 seconds)
2022-03-06 14:37:02 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 14:37:02 | INFO | train | epoch 011 | loss 8.554 | nll_loss 7.186 | ppl 145.58 | wps 20610 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 2140 | lr 0.000267547 | gnorm 0.811 | loss_scale 32 | train_wall 566 | gb_free 19.9 | wall 6844
2022-03-06 14:37:02 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 14:37:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:37:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:40:12 | INFO | train_inner | epoch 012:     61 / 196 loss=8.439, nll_loss=7.053, ppl=132.76, wps=20058.7, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.807, loss_scale=32, train_wall=291, gb_free=19.9, wall=7034
2022-03-06 14:44:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:45:26 | INFO | train_inner | epoch 012:    162 / 196 loss=8.357, nll_loss=6.958, ppl=124.34, wps=20865.8, ups=0.32, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.794, loss_scale=32, train_wall=292, gb_free=19.9, wall=7348
2022-03-06 14:47:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:47:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.568 | nll_loss 7.188 | ppl 145.78 | wps 41431.5 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 8.568
2022-03-06 14:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-06 14:47:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:47:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:47:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 12 @ 2334 updates, score 8.568) (writing took 7.097924908623099 seconds)
2022-03-06 14:47:24 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 14:47:24 | INFO | train | epoch 012 | loss 8.364 | nll_loss 6.966 | ppl 125.05 | wps 20441.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.798 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 7465
2022-03-06 14:47:24 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 14:47:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:50:49 | INFO | train_inner | epoch 013:     66 / 196 loss=8.248, nll_loss=6.833, ppl=113.99, wps=20258.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.788, loss_scale=32, train_wall=288, gb_free=19.9, wall=7670
2022-03-06 14:52:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:56:03 | INFO | train_inner | epoch 013:    167 / 196 loss=8.188, nll_loss=6.762, ppl=108.56, wps=20866.6, ups=0.32, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.781, loss_scale=32, train_wall=292, gb_free=19.9, wall=7985
2022-03-06 14:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:57:38 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.468 | nll_loss 7.061 | ppl 133.57 | wps 41820 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 8.468
2022-03-06 14:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-06 14:57:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:57:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 14:57:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 13 @ 2529 updates, score 8.468) (writing took 7.1924773985520005 seconds)
2022-03-06 14:57:45 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 14:57:45 | INFO | train | epoch 013 | loss 8.191 | nll_loss 6.766 | ppl 108.81 | wps 20544.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.785 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 8086
2022-03-06 14:57:45 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 14:57:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:59:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:01:29 | INFO | train_inner | epoch 014:     72 / 196 loss=8.077, nll_loss=6.634, ppl=99.34, wps=20065.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.786, loss_scale=32, train_wall=291, gb_free=19.9, wall=8310
2022-03-06 15:06:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:06:43 | INFO | train_inner | epoch 014:    173 / 196 loss=8.022, nll_loss=6.57, ppl=94.99, wps=20847, ups=0.32, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.747, loss_scale=32, train_wall=292, gb_free=19.9, wall=8625
2022-03-06 15:07:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:07:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:07:59 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.387 | nll_loss 6.953 | ppl 123.88 | wps 41544.3 | wpb 510.9 | bsz 1 | num_updates 2722 | best_loss 8.387
2022-03-06 15:07:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2722 updates
2022-03-06 15:07:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:08:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:08:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 14 @ 2722 updates, score 8.387) (writing took 6.781472290866077 seconds)
2022-03-06 15:08:06 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 15:08:06 | INFO | train | epoch 014 | loss 8.031 | nll_loss 6.581 | ppl 95.73 | wps 20334.7 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 2722 | lr 0.000340282 | gnorm 0.763 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 8708
2022-03-06 15:08:06 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 15:08:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:12:09 | INFO | train_inner | epoch 015:     78 / 196 loss=7.921, nll_loss=6.455, ppl=87.71, wps=20063.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.767, loss_scale=16, train_wall=291, gb_free=19.9, wall=8950
2022-03-06 15:17:20 | INFO | train_inner | epoch 015:    178 / 196 loss=7.88, nll_loss=6.406, ppl=84.81, wps=21079.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.745, loss_scale=32, train_wall=289, gb_free=19.9, wall=9261
2022-03-06 15:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:18:20 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.328 | nll_loss 6.897 | ppl 119.15 | wps 41649.1 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 8.328
2022-03-06 15:18:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2918 updates
2022-03-06 15:18:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:18:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:18:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 15 @ 2918 updates, score 8.328) (writing took 6.951046307571232 seconds)
2022-03-06 15:18:27 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 15:18:27 | INFO | train | epoch 015 | loss 7.884 | nll_loss 6.411 | ppl 85.08 | wps 20648.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 2918 | lr 0.000364777 | gnorm 0.755 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 9329
2022-03-06 15:18:27 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 15:18:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:21:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:22:45 | INFO | train_inner | epoch 016:     83 / 196 loss=7.761, nll_loss=6.269, ppl=77.13, wps=20075, ups=0.31, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.737, loss_scale=32, train_wall=291, gb_free=19.9, wall=9587
2022-03-06 15:23:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:27:59 | INFO | train_inner | epoch 016:    184 / 196 loss=7.751, nll_loss=6.257, ppl=76.46, wps=20863.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.74, loss_scale=16, train_wall=292, gb_free=19.9, wall=9901
2022-03-06 15:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:28:41 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.303 | nll_loss 6.858 | ppl 115.97 | wps 41449 | wpb 510.9 | bsz 1 | num_updates 3112 | best_loss 8.303
2022-03-06 15:28:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3112 updates
2022-03-06 15:28:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:28:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:28:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 16 @ 3112 updates, score 8.303) (writing took 6.989290804602206 seconds)
2022-03-06 15:28:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 15:28:48 | INFO | train | epoch 016 | loss 7.744 | nll_loss 6.25 | ppl 76.1 | wps 20443.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3112 | lr 0.000389022 | gnorm 0.752 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 9950
2022-03-06 15:28:48 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 15:28:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:33:22 | INFO | train_inner | epoch 017:     88 / 196 loss=7.622, nll_loss=6.109, ppl=69.04, wps=20262.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.747, loss_scale=32, train_wall=288, gb_free=19.9, wall=10224
2022-03-06 15:37:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:38:39 | INFO | train_inner | epoch 017:    190 / 196 loss=7.624, nll_loss=6.11, ppl=69.05, wps=20667.2, ups=0.32, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.744, loss_scale=16, train_wall=294, gb_free=19.9, wall=10541
2022-03-06 15:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:39:02 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.275 | nll_loss 6.82 | ppl 112.98 | wps 41694.5 | wpb 510.9 | bsz 1 | num_updates 3306 | best_loss 8.275
2022-03-06 15:39:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3306 updates
2022-03-06 15:39:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:39:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 17 @ 3306 updates, score 8.275) (writing took 7.0292009534314275 seconds)
2022-03-06 15:39:09 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 15:39:09 | INFO | train | epoch 017 | loss 7.616 | nll_loss 6.101 | ppl 68.66 | wps 20442 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3306 | lr 0.000413267 | gnorm 0.731 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 10571
2022-03-06 15:39:09 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 15:39:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:44:02 | INFO | train_inner | epoch 018:     94 / 196 loss=7.492, nll_loss=5.959, ppl=62.21, wps=20271.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.741, loss_scale=32, train_wall=288, gb_free=19.9, wall=10863
2022-03-06 15:45:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:49:15 | INFO | train_inner | epoch 018:    195 / 196 loss=7.511, nll_loss=5.979, ppl=63.08, wps=20878.4, ups=0.32, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.735, loss_scale=16, train_wall=291, gb_free=19.9, wall=11177
2022-03-06 15:49:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:49:23 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.258 | nll_loss 6.803 | ppl 111.65 | wps 41623.1 | wpb 510.9 | bsz 1 | num_updates 3501 | best_loss 8.258
2022-03-06 15:49:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3501 updates
2022-03-06 15:49:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:49:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:49:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 18 @ 3501 updates, score 8.258) (writing took 7.049982056953013 seconds)
2022-03-06 15:49:30 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 15:49:30 | INFO | train | epoch 018 | loss 7.497 | nll_loss 5.964 | ppl 62.41 | wps 20558.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3501 | lr 0.000437637 | gnorm 0.739 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 11192
2022-03-06 15:49:30 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 15:49:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:53:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:54:41 | INFO | train_inner | epoch 019:    100 / 196 loss=7.366, nll_loss=5.814, ppl=56.25, wps=20082.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.724, loss_scale=16, train_wall=291, gb_free=19.9, wall=11503
2022-03-06 15:59:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:59:44 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.252 | nll_loss 6.808 | ppl 112.05 | wps 41625.6 | wpb 510.9 | bsz 1 | num_updates 3696 | best_loss 8.252
2022-03-06 15:59:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3696 updates
2022-03-06 15:59:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:59:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 15:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 19 @ 3696 updates, score 8.252) (writing took 7.095206567086279 seconds)
2022-03-06 15:59:51 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 15:59:51 | INFO | train | epoch 019 | loss 7.382 | nll_loss 5.831 | ppl 56.93 | wps 20556.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3696 | lr 0.000462008 | gnorm 0.725 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 11813
2022-03-06 15:59:51 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 15:59:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:00:04 | INFO | train_inner | epoch 020:      4 / 196 loss=7.393, nll_loss=5.842, ppl=57.37, wps=20265.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.725, loss_scale=32, train_wall=288, gb_free=19.9, wall=11825
2022-03-06 16:01:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:05:17 | INFO | train_inner | epoch 020:    105 / 196 loss=7.249, nll_loss=5.678, ppl=51.19, wps=20894.3, ups=0.32, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.734, loss_scale=16, train_wall=291, gb_free=19.9, wall=12139
2022-03-06 16:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:10:04 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.261 | nll_loss 6.812 | ppl 112.37 | wps 41533.8 | wpb 510.9 | bsz 1 | num_updates 3890 | best_loss 8.252
2022-03-06 16:10:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3890 updates
2022-03-06 16:10:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:10:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 20 @ 3890 updates, score 8.261) (writing took 3.4014341086149216 seconds)
2022-03-06 16:10:08 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 16:10:08 | INFO | train | epoch 020 | loss 7.274 | nll_loss 5.706 | ppl 52.2 | wps 20585 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3890 | lr 0.000486253 | gnorm 0.724 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 12429
2022-03-06 16:10:08 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 16:10:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:10:39 | INFO | train_inner | epoch 021:     10 / 196 loss=7.289, nll_loss=5.721, ppl=52.76, wps=20310.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.728, loss_scale=16, train_wall=291, gb_free=19.9, wall=12461
2022-03-06 16:15:50 | INFO | train_inner | epoch 021:    110 / 196 loss=7.146, nll_loss=5.558, ppl=47.12, wps=21083.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.724, loss_scale=16, train_wall=289, gb_free=19.9, wall=12771
2022-03-06 16:16:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:18:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 16:20:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:20:22 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.249 | nll_loss 6.773 | ppl 109.39 | wps 41768.6 | wpb 510.9 | bsz 1 | num_updates 4084 | best_loss 8.249
2022-03-06 16:20:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4084 updates
2022-03-06 16:20:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 16:20:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt
2022-03-06 16:20:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_best.pt (epoch 21 @ 4084 updates, score 8.249) (writing took 7.06152756139636 seconds)
2022-03-06 16:20:29 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 16:20:29 | INFO | train | epoch 021 | loss 7.172 | nll_loss 5.587 | ppl 48.08 | wps 20452.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4084 | lr 0.000494831 | gnorm 0.722 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 13050
2022-03-06 16:20:29 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 16:20:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:21:18 | INFO | train_inner | epoch 022:     16 / 196 loss=7.17, nll_loss=5.584, ppl=47.98, wps=19900.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.698, loss_scale=8, train_wall=293, gb_free=19.9, wall=13100
2022-03-06 16:26:29 | INFO | train_inner | epoch 022:    116 / 196 loss=7.05, nll_loss=5.446, ppl=43.61, wps=21083, ups=0.32, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.7, loss_scale=16, train_wall=289, gb_free=19.9, wall=13411
2022-03-06 16:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:30:42 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.257 | nll_loss 6.758 | ppl 108.22 | wps 41590.9 | wpb 510.9 | bsz 1 | num_updates 4280 | best_loss 8.249
2022-03-06 16:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4280 updates
2022-03-06 16:30:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 22 @ 4280 updates, score 8.257) (writing took 3.3872241666540504 seconds)
2022-03-06 16:30:46 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 16:30:46 | INFO | train | epoch 022 | loss 7.063 | nll_loss 5.462 | ppl 44.07 | wps 20787.1 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 4280 | lr 0.000483368 | gnorm 0.695 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 13667
2022-03-06 16:30:46 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 16:30:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:31:48 | INFO | train_inner | epoch 023:     20 / 196 loss=7.059, nll_loss=5.456, ppl=43.88, wps=20494.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.7, loss_scale=16, train_wall=288, gb_free=19.9, wall=13730
2022-03-06 16:32:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:37:02 | INFO | train_inner | epoch 023:    121 / 196 loss=6.939, nll_loss=5.319, ppl=39.91, wps=20881.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.679, loss_scale=16, train_wall=291, gb_free=19.9, wall=14043
2022-03-06 16:37:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 16:40:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:41:00 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.26 | nll_loss 6.775 | ppl 109.54 | wps 41411.3 | wpb 510.9 | bsz 1 | num_updates 4474 | best_loss 8.249
2022-03-06 16:41:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4474 updates
2022-03-06 16:41:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:41:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 23 @ 4474 updates, score 8.26) (writing took 3.407853844575584 seconds)
2022-03-06 16:41:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 16:41:03 | INFO | train | epoch 023 | loss 6.957 | nll_loss 5.339 | ppl 40.47 | wps 20561 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4474 | lr 0.000472772 | gnorm 0.69 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 14285
2022-03-06 16:41:03 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 16:41:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:42:24 | INFO | train_inner | epoch 024:     26 / 196 loss=6.94, nll_loss=5.318, ppl=39.89, wps=20281.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.691, loss_scale=8, train_wall=291, gb_free=19.9, wall=14366
2022-03-06 16:47:35 | INFO | train_inner | epoch 024:    126 / 196 loss=6.846, nll_loss=5.21, ppl=37.02, wps=21089.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.678, loss_scale=16, train_wall=288, gb_free=19.9, wall=14677
2022-03-06 16:47:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 16:51:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:51:17 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.284 | nll_loss 6.77 | ppl 109.15 | wps 41374.3 | wpb 510.9 | bsz 1 | num_updates 4669 | best_loss 8.249
2022-03-06 16:51:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4669 updates
2022-03-06 16:51:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:51:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 16:51:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 24 @ 4669 updates, score 8.284) (writing took 3.418472667224705 seconds)
2022-03-06 16:51:20 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 16:51:20 | INFO | train | epoch 024 | loss 6.86 | nll_loss 5.226 | ppl 37.42 | wps 20680.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 4669 | lr 0.000462794 | gnorm 0.677 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 14902
2022-03-06 16:51:20 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 16:51:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:52:57 | INFO | train_inner | epoch 025:     31 / 196 loss=6.845, nll_loss=5.209, ppl=36.99, wps=20314.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.671, loss_scale=8, train_wall=291, gb_free=19.9, wall=14998
2022-03-06 16:58:08 | INFO | train_inner | epoch 025:    131 / 196 loss=6.754, nll_loss=5.103, ppl=34.36, wps=21069, ups=0.32, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.679, loss_scale=16, train_wall=289, gb_free=19.9, wall=15309
2022-03-06 17:00:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:01:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:01:34 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.347 | nll_loss 6.878 | ppl 117.66 | wps 41612.2 | wpb 510.9 | bsz 1 | num_updates 4864 | best_loss 8.249
2022-03-06 17:01:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4864 updates
2022-03-06 17:01:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:01:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:01:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 25 @ 4864 updates, score 8.347) (writing took 3.4277896666899323 seconds)
2022-03-06 17:01:38 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:01:38 | INFO | train | epoch 025 | loss 6.769 | nll_loss 5.12 | ppl 34.77 | wps 20671.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.685 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 15519
2022-03-06 17:01:38 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:01:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:03:30 | INFO | train_inner | epoch 026:     36 / 196 loss=6.749, nll_loss=5.098, ppl=34.24, wps=20298.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.686, loss_scale=8, train_wall=291, gb_free=19.9, wall=15631
2022-03-06 17:08:41 | INFO | train_inner | epoch 026:    136 / 196 loss=6.676, nll_loss=5.012, ppl=32.26, wps=21043.4, ups=0.32, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.676, loss_scale=16, train_wall=289, gb_free=19.9, wall=15943
2022-03-06 17:08:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:11:53 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.368 | nll_loss 6.871 | ppl 117.03 | wps 41433.8 | wpb 510.9 | bsz 1 | num_updates 5059 | best_loss 8.249
2022-03-06 17:11:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5059 updates
2022-03-06 17:11:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:11:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 26 @ 5059 updates, score 8.368) (writing took 3.2220481671392918 seconds)
2022-03-06 17:11:56 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:11:56 | INFO | train | epoch 026 | loss 6.68 | nll_loss 5.017 | ppl 32.38 | wps 20648.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5059 | lr 0.000444598 | gnorm 0.67 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 16137
2022-03-06 17:11:56 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:11:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:14:04 | INFO | train_inner | epoch 027:     41 / 196 loss=6.645, nll_loss=4.977, ppl=31.49, wps=20276.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.665, loss_scale=8, train_wall=291, gb_free=19.9, wall=16265
2022-03-06 17:17:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:19:17 | INFO | train_inner | epoch 027:    142 / 196 loss=6.599, nll_loss=4.922, ppl=30.32, wps=20880.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.68, loss_scale=8, train_wall=291, gb_free=19.9, wall=16579
2022-03-06 17:22:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:22:10 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.376 | nll_loss 6.89 | ppl 118.63 | wps 41514.1 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 8.249
2022-03-06 17:22:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5254 updates
2022-03-06 17:22:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:22:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:22:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 27 @ 5254 updates, score 8.376) (writing took 3.358639759942889 seconds)
2022-03-06 17:22:13 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:22:13 | INFO | train | epoch 027 | loss 6.599 | nll_loss 4.923 | ppl 30.34 | wps 20671.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5254 | lr 0.00043627 | gnorm 0.675 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 16755
2022-03-06 17:22:13 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:22:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:24:36 | INFO | train_inner | epoch 028:     46 / 196 loss=6.562, nll_loss=4.88, ppl=29.45, wps=20502.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.674, loss_scale=16, train_wall=288, gb_free=19.9, wall=16898
2022-03-06 17:26:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:29:50 | INFO | train_inner | epoch 028:    147 / 196 loss=6.518, nll_loss=4.829, ppl=28.42, wps=20881.8, ups=0.32, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.671, loss_scale=8, train_wall=291, gb_free=19.9, wall=17212
2022-03-06 17:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:32:27 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.434 | nll_loss 6.95 | ppl 123.64 | wps 41506.5 | wpb 510.9 | bsz 1 | num_updates 5449 | best_loss 8.249
2022-03-06 17:32:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5449 updates
2022-03-06 17:32:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:32:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:32:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 28 @ 5449 updates, score 8.434) (writing took 3.401665458455682 seconds)
2022-03-06 17:32:30 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:32:30 | INFO | train | epoch 028 | loss 6.522 | nll_loss 4.833 | ppl 28.5 | wps 20681.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5449 | lr 0.000428392 | gnorm 0.671 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 17372
2022-03-06 17:32:30 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:32:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:33:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:35:12 | INFO | train_inner | epoch 029:     52 / 196 loss=6.487, nll_loss=4.792, ppl=27.7, wps=20309.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.68, loss_scale=8, train_wall=291, gb_free=19.9, wall=17534
2022-03-06 17:40:23 | INFO | train_inner | epoch 029:    152 / 196 loss=6.45, nll_loss=4.748, ppl=26.87, wps=21091.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.682, loss_scale=8, train_wall=289, gb_free=19.9, wall=17844
2022-03-06 17:41:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:42:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:42:44 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.502 | nll_loss 7.019 | ppl 129.67 | wps 41523.8 | wpb 510.9 | bsz 1 | num_updates 5643 | best_loss 8.249
2022-03-06 17:42:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5643 updates
2022-03-06 17:42:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:42:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:42:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 29 @ 5643 updates, score 8.502) (writing took 3.446968794800341 seconds)
2022-03-06 17:42:47 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:42:47 | INFO | train | epoch 029 | loss 6.448 | nll_loss 4.746 | ppl 26.84 | wps 20575.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5643 | lr 0.000420964 | gnorm 0.681 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 17989
2022-03-06 17:42:47 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:42:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:45:44 | INFO | train_inner | epoch 030:     57 / 196 loss=6.404, nll_loss=4.696, ppl=25.91, wps=20317.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.687, loss_scale=8, train_wall=290, gb_free=19.9, wall=18166
2022-03-06 17:49:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:50:58 | INFO | train_inner | epoch 030:    158 / 196 loss=6.389, nll_loss=4.677, ppl=25.57, wps=20899.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.695, loss_scale=8, train_wall=291, gb_free=19.9, wall=18480
2022-03-06 17:52:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:53:00 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.531 | nll_loss 7.06 | ppl 133.43 | wps 41541.2 | wpb 510.9 | bsz 1 | num_updates 5838 | best_loss 8.249
2022-03-06 17:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5838 updates
2022-03-06 17:53:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 17:53:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 30 @ 5838 updates, score 8.531) (writing took 3.361362303607166 seconds)
2022-03-06 17:53:04 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 17:53:04 | INFO | train | epoch 030 | loss 6.379 | nll_loss 4.666 | ppl 25.38 | wps 20701.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5838 | lr 0.000413874 | gnorm 0.69 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 18605
2022-03-06 17:53:04 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 17:53:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:56:17 | INFO | train_inner | epoch 031:     62 / 196 loss=6.32, nll_loss=4.598, ppl=24.22, wps=20519.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.688, loss_scale=8, train_wall=288, gb_free=19.9, wall=18798
2022-03-06 17:58:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:01:30 | INFO | train_inner | epoch 031:    163 / 196 loss=6.331, nll_loss=4.609, ppl=24.39, wps=20890.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.691, loss_scale=8, train_wall=291, gb_free=19.9, wall=19112
2022-03-06 18:03:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:03:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.556 | nll_loss 7.059 | ppl 133.31 | wps 41619.3 | wpb 510.9 | bsz 1 | num_updates 6033 | best_loss 8.249
2022-03-06 18:03:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6033 updates
2022-03-06 18:03:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:03:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:03:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 31 @ 6033 updates, score 8.556) (writing took 3.3914669789373875 seconds)
2022-03-06 18:03:21 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 18:03:21 | INFO | train | epoch 031 | loss 6.312 | nll_loss 4.588 | ppl 24.05 | wps 20693.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6033 | lr 0.00040713 | gnorm 0.692 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 19222
2022-03-06 18:03:21 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 18:03:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:05:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:06:52 | INFO | train_inner | epoch 032:     68 / 196 loss=6.251, nll_loss=4.517, ppl=22.9, wps=20317.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.703, loss_scale=8, train_wall=291, gb_free=19.9, wall=19434
2022-03-06 18:12:03 | INFO | train_inner | epoch 032:    168 / 196 loss=6.267, nll_loss=4.534, ppl=23.17, wps=21092, ups=0.32, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.698, loss_scale=16, train_wall=288, gb_free=19.9, wall=19744
2022-03-06 18:13:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:13:34 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.62 | nll_loss 7.124 | ppl 139.47 | wps 41644.8 | wpb 510.9 | bsz 1 | num_updates 6227 | best_loss 8.249
2022-03-06 18:13:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6227 updates
2022-03-06 18:13:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:13:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:13:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 32 @ 6227 updates, score 8.62) (writing took 3.29126816149801 seconds)
2022-03-06 18:13:37 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 18:13:37 | INFO | train | epoch 032 | loss 6.247 | nll_loss 4.512 | ppl 22.81 | wps 20584.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6227 | lr 0.000400738 | gnorm 0.704 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 19839
2022-03-06 18:13:37 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 18:13:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:17:24 | INFO | train_inner | epoch 033:     73 / 196 loss=6.185, nll_loss=4.439, ppl=21.7, wps=20321.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.691, loss_scale=8, train_wall=291, gb_free=19.9, wall=20066
2022-03-06 18:22:35 | INFO | train_inner | epoch 033:    173 / 196 loss=6.212, nll_loss=4.469, ppl=22.15, wps=21100, ups=0.32, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.708, loss_scale=16, train_wall=288, gb_free=19.9, wall=20376
2022-03-06 18:23:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:23:51 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.648 | nll_loss 7.168 | ppl 143.78 | wps 41657.3 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 8.249
2022-03-06 18:23:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6423 updates
2022-03-06 18:23:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:23:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:23:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 33 @ 6423 updates, score 8.648) (writing took 3.311447341926396 seconds)
2022-03-06 18:23:54 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 18:23:54 | INFO | train | epoch 033 | loss 6.188 | nll_loss 4.443 | ppl 21.75 | wps 20798.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 6423 | lr 0.000394576 | gnorm 0.699 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 20456
2022-03-06 18:23:54 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 18:23:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:24:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:27:57 | INFO | train_inner | epoch 034:     78 / 196 loss=6.118, nll_loss=4.361, ppl=20.55, wps=20321, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.708, loss_scale=8, train_wall=291, gb_free=19.9, wall=20698
2022-03-06 18:33:07 | INFO | train_inner | epoch 034:    178 / 196 loss=6.156, nll_loss=4.404, ppl=21.17, wps=21085.6, ups=0.32, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.707, loss_scale=16, train_wall=289, gb_free=19.9, wall=21009
2022-03-06 18:33:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:34:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:34:08 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.736 | nll_loss 7.261 | ppl 153.37 | wps 41488.9 | wpb 510.9 | bsz 1 | num_updates 6617 | best_loss 8.249
2022-03-06 18:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6617 updates
2022-03-06 18:34:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:34:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:34:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 34 @ 6617 updates, score 8.736) (writing took 3.3544475687667727 seconds)
2022-03-06 18:34:11 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 18:34:11 | INFO | train | epoch 034 | loss 6.13 | nll_loss 4.374 | ppl 20.74 | wps 20578.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6617 | lr 0.000388749 | gnorm 0.711 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 21073
2022-03-06 18:34:11 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 18:34:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:38:29 | INFO | train_inner | epoch 035:     83 / 196 loss=6.053, nll_loss=4.285, ppl=19.5, wps=20312.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.71, loss_scale=8, train_wall=291, gb_free=19.9, wall=21331
2022-03-06 18:43:40 | INFO | train_inner | epoch 035:    183 / 196 loss=6.112, nll_loss=4.352, ppl=20.42, wps=21080.1, ups=0.32, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.719, loss_scale=16, train_wall=289, gb_free=19.9, wall=21642
2022-03-06 18:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:44:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:44:25 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.747 | nll_loss 7.266 | ppl 153.91 | wps 41780 | wpb 510.9 | bsz 1 | num_updates 6812 | best_loss 8.249
2022-03-06 18:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6812 updates
2022-03-06 18:44:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:44:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:44:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 35 @ 6812 updates, score 8.747) (writing took 3.421200517565012 seconds)
2022-03-06 18:44:28 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 18:44:28 | INFO | train | epoch 035 | loss 6.075 | nll_loss 4.31 | ppl 19.83 | wps 20678.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6812 | lr 0.000383145 | gnorm 0.714 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 21690
2022-03-06 18:44:28 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 18:44:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:49:02 | INFO | train_inner | epoch 036:     88 / 196 loss=5.991, nll_loss=4.212, ppl=18.54, wps=20313.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.722, loss_scale=8, train_wall=291, gb_free=19.9, wall=21963
2022-03-06 18:51:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:54:16 | INFO | train_inner | epoch 036:    189 / 196 loss=6.063, nll_loss=4.295, ppl=19.62, wps=20866.6, ups=0.32, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.716, loss_scale=8, train_wall=292, gb_free=19.9, wall=22277
2022-03-06 18:54:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:54:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.791 | nll_loss 7.314 | ppl 159.13 | wps 41464.9 | wpb 510.9 | bsz 1 | num_updates 7007 | best_loss 8.249
2022-03-06 18:54:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7007 updates
2022-03-06 18:54:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:54:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 18:54:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 36 @ 7007 updates, score 8.791) (writing took 3.41626488417387 seconds)
2022-03-06 18:54:46 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 18:54:46 | INFO | train | epoch 036 | loss 6.021 | nll_loss 4.247 | ppl 18.99 | wps 20672.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7007 | lr 0.000377776 | gnorm 0.722 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 22307
2022-03-06 18:54:46 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 18:54:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:59:38 | INFO | train_inner | epoch 037:     94 / 196 loss=5.937, nll_loss=4.149, ppl=17.74, wps=20297.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.734, loss_scale=8, train_wall=291, gb_free=19.9, wall=22600
2022-03-06 19:04:49 | INFO | train_inner | epoch 037:    194 / 196 loss=6.01, nll_loss=4.232, ppl=18.79, wps=21076.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.718, loss_scale=8, train_wall=289, gb_free=19.9, wall=22910
2022-03-06 19:04:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:05:00 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.848 | nll_loss 7.368 | ppl 165.25 | wps 40595.3 | wpb 510.9 | bsz 1 | num_updates 7202 | best_loss 8.249
2022-03-06 19:05:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7202 updates
2022-03-06 19:05:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:05:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:05:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 37 @ 7202 updates, score 8.848) (writing took 3.3651941446587443 seconds)
2022-03-06 19:05:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 19:05:03 | INFO | train | epoch 037 | loss 5.97 | nll_loss 4.187 | ppl 18.22 | wps 20668.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7202 | lr 0.000372626 | gnorm 0.725 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 22925
2022-03-06 19:05:03 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 19:05:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:06:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:10:12 | INFO | train_inner | epoch 038:     99 / 196 loss=5.883, nll_loss=4.085, ppl=16.98, wps=20246.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.744, loss_scale=8, train_wall=291, gb_free=19.9, wall=23233
2022-03-06 19:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:15:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:15:18 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.892 | nll_loss 7.425 | ppl 171.83 | wps 41765.3 | wpb 510.9 | bsz 1 | num_updates 7396 | best_loss 8.249
2022-03-06 19:15:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7396 updates
2022-03-06 19:15:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:15:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:15:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 38 @ 7396 updates, score 8.892) (writing took 3.4272211557254195 seconds)
2022-03-06 19:15:22 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 19:15:22 | INFO | train | epoch 038 | loss 5.922 | nll_loss 4.131 | ppl 17.52 | wps 20530.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7396 | lr 0.000367707 | gnorm 0.743 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 23543
2022-03-06 19:15:22 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 19:15:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:15:34 | INFO | train_inner | epoch 039:      4 / 196 loss=5.958, nll_loss=4.172, ppl=18.03, wps=20274.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.741, loss_scale=8, train_wall=291, gb_free=19.9, wall=23556
2022-03-06 19:20:45 | INFO | train_inner | epoch 039:    104 / 196 loss=5.83, nll_loss=4.023, ppl=16.26, wps=21083.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.735, loss_scale=8, train_wall=289, gb_free=19.9, wall=23866
2022-03-06 19:22:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:25:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:25:35 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.966 | nll_loss 7.499 | ppl 180.93 | wps 41341.7 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 8.249
2022-03-06 19:25:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-06 19:25:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:25:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:25:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 39 @ 7591 updates, score 8.966) (writing took 3.315815355628729 seconds)
2022-03-06 19:25:39 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 19:25:39 | INFO | train | epoch 039 | loss 5.876 | nll_loss 4.076 | ppl 16.86 | wps 20684.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.74 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 24160
2022-03-06 19:25:39 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 19:25:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:26:07 | INFO | train_inner | epoch 040:      9 / 196 loss=5.911, nll_loss=4.116, ppl=17.34, wps=20314.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.746, loss_scale=8, train_wall=291, gb_free=19.9, wall=24188
2022-03-06 19:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:31:20 | INFO | train_inner | epoch 040:    110 / 196 loss=5.8, nll_loss=3.987, ppl=15.85, wps=20887.6, ups=0.32, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.755, loss_scale=8, train_wall=291, gb_free=19.9, wall=24502
2022-03-06 19:35:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:35:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.027 | nll_loss 7.553 | ppl 187.74 | wps 41511.9 | wpb 510.9 | bsz 1 | num_updates 7786 | best_loss 8.249
2022-03-06 19:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7786 updates
2022-03-06 19:35:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 40 @ 7786 updates, score 9.027) (writing took 3.37162022665143 seconds)
2022-03-06 19:35:55 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 19:35:55 | INFO | train | epoch 040 | loss 5.832 | nll_loss 4.024 | ppl 16.27 | wps 20687.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7786 | lr 0.000358379 | gnorm 0.749 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 24777
2022-03-06 19:35:55 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 19:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:36:39 | INFO | train_inner | epoch 041:     14 / 196 loss=5.857, nll_loss=4.053, ppl=16.6, wps=20512.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.744, loss_scale=8, train_wall=288, gb_free=19.9, wall=24821
2022-03-06 19:38:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:41:53 | INFO | train_inner | epoch 041:    115 / 196 loss=5.757, nll_loss=3.937, ppl=15.31, wps=20880.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.742, loss_scale=8, train_wall=291, gb_free=19.9, wall=25135
2022-03-06 19:46:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:46:09 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.039 | nll_loss 7.575 | ppl 190.71 | wps 41607.9 | wpb 510.9 | bsz 1 | num_updates 7981 | best_loss 8.249
2022-03-06 19:46:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7981 updates
2022-03-06 19:46:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:46:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 41 @ 7981 updates, score 9.039) (writing took 3.415656818076968 seconds)
2022-03-06 19:46:13 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 19:46:13 | INFO | train | epoch 041 | loss 5.789 | nll_loss 3.974 | ppl 15.71 | wps 20679.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7981 | lr 0.000353974 | gnorm 0.751 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 25394
2022-03-06 19:46:13 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 19:46:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:46:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:47:15 | INFO | train_inner | epoch 042:     20 / 196 loss=5.809, nll_loss=3.997, ppl=15.97, wps=20306.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.77, loss_scale=8, train_wall=291, gb_free=19.9, wall=25456
2022-03-06 19:52:26 | INFO | train_inner | epoch 042:    120 / 196 loss=5.722, nll_loss=3.894, ppl=14.87, wps=21074.6, ups=0.32, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.754, loss_scale=8, train_wall=289, gb_free=19.9, wall=25767
2022-03-06 19:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:56:27 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.105 | nll_loss 7.64 | ppl 199.44 | wps 41631 | wpb 510.9 | bsz 1 | num_updates 8176 | best_loss 8.249
2022-03-06 19:56:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8176 updates
2022-03-06 19:56:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:56:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 19:56:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 42 @ 8176 updates, score 9.105) (writing took 3.38442475348711 seconds)
2022-03-06 19:56:30 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 19:56:30 | INFO | train | epoch 042 | loss 5.747 | nll_loss 3.924 | ppl 15.18 | wps 20669.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8176 | lr 0.000349727 | gnorm 0.762 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 26012
2022-03-06 19:56:30 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 19:56:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:57:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:57:48 | INFO | train_inner | epoch 043:     25 / 196 loss=5.76, nll_loss=3.939, ppl=15.33, wps=20302, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.768, loss_scale=8, train_wall=291, gb_free=19.9, wall=26089
2022-03-06 20:02:59 | INFO | train_inner | epoch 043:    125 / 196 loss=5.69, nll_loss=3.857, ppl=14.49, wps=21086.7, ups=0.32, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.771, loss_scale=8, train_wall=289, gb_free=19.9, wall=26400
2022-03-06 20:04:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:06:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:06:44 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.139 | nll_loss 7.681 | ppl 205.16 | wps 41241.3 | wpb 510.9 | bsz 1 | num_updates 8370 | best_loss 8.249
2022-03-06 20:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8370 updates
2022-03-06 20:06:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:06:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 43 @ 8370 updates, score 9.139) (writing took 3.328988785855472 seconds)
2022-03-06 20:06:47 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 20:06:47 | INFO | train | epoch 043 | loss 5.707 | nll_loss 3.877 | ppl 14.69 | wps 20573.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8370 | lr 0.000345651 | gnorm 0.77 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 26629
2022-03-06 20:06:47 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 20:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:08:21 | INFO | train_inner | epoch 044:     30 / 196 loss=5.705, nll_loss=3.875, ppl=14.67, wps=20297.2, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.77, loss_scale=8, train_wall=291, gb_free=19.9, wall=26722
2022-03-06 20:13:31 | INFO | train_inner | epoch 044:    130 / 196 loss=5.657, nll_loss=3.818, ppl=14.1, wps=21087.3, ups=0.32, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.768, loss_scale=16, train_wall=289, gb_free=19.9, wall=27033
2022-03-06 20:16:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:16:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:17:01 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.138 | nll_loss 7.667 | ppl 203.19 | wps 41548.5 | wpb 510.9 | bsz 1 | num_updates 8565 | best_loss 8.249
2022-03-06 20:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8565 updates
2022-03-06 20:17:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:17:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:17:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 44 @ 8565 updates, score 9.138) (writing took 3.429778592661023 seconds)
2022-03-06 20:17:05 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 20:17:05 | INFO | train | epoch 044 | loss 5.67 | nll_loss 3.833 | ppl 14.25 | wps 20672 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8565 | lr 0.000341693 | gnorm 0.774 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 27246
2022-03-06 20:17:05 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 20:17:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:18:54 | INFO | train_inner | epoch 045:     35 / 196 loss=5.669, nll_loss=3.832, ppl=14.24, wps=20290.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.787, loss_scale=8, train_wall=291, gb_free=19.9, wall=27355
2022-03-06 20:24:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:24:07 | INFO | train_inner | epoch 045:    136 / 196 loss=5.623, nll_loss=3.778, ppl=13.72, wps=20873.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.783, loss_scale=8, train_wall=291, gb_free=19.9, wall=27669
2022-03-06 20:27:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:27:19 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.216 | nll_loss 7.763 | ppl 217.28 | wps 41637.5 | wpb 510.9 | bsz 1 | num_updates 8760 | best_loss 8.249
2022-03-06 20:27:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8760 updates
2022-03-06 20:27:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:27:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:27:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 45 @ 8760 updates, score 9.216) (writing took 3.3905987394973636 seconds)
2022-03-06 20:27:22 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 20:27:22 | INFO | train | epoch 045 | loss 5.633 | nll_loss 3.789 | ppl 13.83 | wps 20671.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8760 | lr 0.000337869 | gnorm 0.789 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 27864
2022-03-06 20:27:22 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 20:27:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:29:26 | INFO | train_inner | epoch 046:     40 / 196 loss=5.629, nll_loss=3.785, ppl=13.78, wps=20499.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.786, loss_scale=8, train_wall=288, gb_free=19.9, wall=27988
2022-03-06 20:32:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:34:40 | INFO | train_inner | epoch 046:    141 / 196 loss=5.59, nll_loss=3.739, ppl=13.35, wps=20880.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.789, loss_scale=8, train_wall=291, gb_free=19.9, wall=28302
2022-03-06 20:37:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:37:35 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.244 | nll_loss 7.78 | ppl 219.86 | wps 41602.5 | wpb 510.9 | bsz 1 | num_updates 8955 | best_loss 8.249
2022-03-06 20:37:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8955 updates
2022-03-06 20:37:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:37:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:37:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 46 @ 8955 updates, score 9.244) (writing took 3.3132450161501765 seconds)
2022-03-06 20:37:39 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 20:37:39 | INFO | train | epoch 046 | loss 5.598 | nll_loss 3.749 | ppl 13.44 | wps 20690.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8955 | lr 0.00033417 | gnorm 0.785 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 28480
2022-03-06 20:37:39 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 20:37:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:39:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:40:02 | INFO | train_inner | epoch 047:     46 / 196 loss=5.583, nll_loss=3.731, ppl=13.27, wps=20317.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.787, loss_scale=8, train_wall=291, gb_free=19.9, wall=28623
2022-03-06 20:45:13 | INFO | train_inner | epoch 047:    146 / 196 loss=5.564, nll_loss=3.708, ppl=13.07, wps=21075.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.801, loss_scale=8, train_wall=289, gb_free=19.9, wall=28934
2022-03-06 20:46:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:47:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:47:53 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.279 | nll_loss 7.825 | ppl 226.79 | wps 41591.5 | wpb 510.9 | bsz 1 | num_updates 9149 | best_loss 8.249
2022-03-06 20:47:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9149 updates
2022-03-06 20:47:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:47:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:47:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 47 @ 9149 updates, score 9.279) (writing took 3.3154919603839517 seconds)
2022-03-06 20:47:56 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 20:47:56 | INFO | train | epoch 047 | loss 5.563 | nll_loss 3.707 | ppl 13.06 | wps 20563.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9149 | lr 0.000330608 | gnorm 0.804 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 29098
2022-03-06 20:47:56 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 20:47:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:50:35 | INFO | train_inner | epoch 048:     51 / 196 loss=5.539, nll_loss=3.679, ppl=12.81, wps=20291.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.803, loss_scale=8, train_wall=291, gb_free=19.9, wall=29257
2022-03-06 20:54:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:55:49 | INFO | train_inner | epoch 048:    152 / 196 loss=5.544, nll_loss=3.684, ppl=12.85, wps=20868.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.798, loss_scale=8, train_wall=292, gb_free=19.9, wall=29571
2022-03-06 20:58:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:58:10 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.361 | nll_loss 7.919 | ppl 242.06 | wps 41322.2 | wpb 510.9 | bsz 1 | num_updates 9344 | best_loss 8.249
2022-03-06 20:58:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9344 updates
2022-03-06 20:58:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:58:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 20:58:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 48 @ 9344 updates, score 9.361) (writing took 3.355206466279924 seconds)
2022-03-06 20:58:14 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 20:58:14 | INFO | train | epoch 048 | loss 5.533 | nll_loss 3.671 | ppl 12.73 | wps 20669.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.794 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 29715
2022-03-06 20:58:14 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 20:58:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:01:08 | INFO | train_inner | epoch 049:     56 / 196 loss=5.505, nll_loss=3.638, ppl=12.45, wps=20508.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.8, loss_scale=8, train_wall=288, gb_free=19.9, wall=29889
2022-03-06 21:03:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:06:22 | INFO | train_inner | epoch 049:    157 / 196 loss=5.515, nll_loss=3.649, ppl=12.55, wps=20864.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.81, loss_scale=8, train_wall=292, gb_free=19.9, wall=30203
2022-03-06 21:08:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:08:28 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.429 | nll_loss 8 | ppl 256.01 | wps 41548 | wpb 510.9 | bsz 1 | num_updates 9539 | best_loss 8.249
2022-03-06 21:08:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9539 updates
2022-03-06 21:08:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:08:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 49 @ 9539 updates, score 9.429) (writing took 3.1700522918254137 seconds)
2022-03-06 21:08:31 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 21:08:31 | INFO | train | epoch 049 | loss 5.501 | nll_loss 3.634 | ppl 12.41 | wps 20676.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9539 | lr 0.000323779 | gnorm 0.812 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 30332
2022-03-06 21:08:31 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 21:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:11:41 | INFO | train_inner | epoch 050:     61 / 196 loss=5.47, nll_loss=3.597, ppl=12.1, wps=20480.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.812, loss_scale=16, train_wall=288, gb_free=19.9, wall=30523
2022-03-06 21:15:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:16:55 | INFO | train_inner | epoch 050:    162 / 196 loss=5.486, nll_loss=3.615, ppl=12.25, wps=20856.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.819, loss_scale=8, train_wall=292, gb_free=19.9, wall=30837
2022-03-06 21:18:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:18:45 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.389 | nll_loss 7.934 | ppl 244.57 | wps 41255.9 | wpb 510.9 | bsz 1 | num_updates 9734 | best_loss 8.249
2022-03-06 21:18:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9734 updates
2022-03-06 21:18:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:18:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:18:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 50 @ 9734 updates, score 9.389) (writing took 3.230327444151044 seconds)
2022-03-06 21:18:49 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 21:18:49 | INFO | train | epoch 050 | loss 5.471 | nll_loss 3.598 | ppl 12.11 | wps 20658.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9734 | lr 0.000320519 | gnorm 0.815 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 30950
2022-03-06 21:18:49 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 21:18:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:22:14 | INFO | train_inner | epoch 051:     66 / 196 loss=5.427, nll_loss=3.547, ppl=11.68, wps=20496.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.831, loss_scale=16, train_wall=288, gb_free=19.9, wall=31156
2022-03-06 21:22:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:27:28 | INFO | train_inner | epoch 051:    167 / 196 loss=5.462, nll_loss=3.586, ppl=12.01, wps=20867.5, ups=0.32, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.834, loss_scale=8, train_wall=292, gb_free=19.9, wall=31470
2022-03-06 21:28:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:29:03 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.489 | nll_loss 8.056 | ppl 266.17 | wps 41412.1 | wpb 510.9 | bsz 1 | num_updates 9929 | best_loss 8.249
2022-03-06 21:29:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9929 updates
2022-03-06 21:29:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:29:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:29:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 51 @ 9929 updates, score 9.489) (writing took 3.4111646497622132 seconds)
2022-03-06 21:29:06 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 21:29:06 | INFO | train | epoch 051 | loss 5.44 | nll_loss 3.561 | ppl 11.81 | wps 20663.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9929 | lr 0.000317356 | gnorm 0.835 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 31568
2022-03-06 21:29:06 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 21:29:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:29:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:32:50 | INFO | train_inner | epoch 052:     72 / 196 loss=5.404, nll_loss=3.519, ppl=11.47, wps=20300.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.82, loss_scale=8, train_wall=291, gb_free=19.9, wall=31792
2022-03-06 21:37:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:38:04 | INFO | train_inner | epoch 052:    173 / 196 loss=5.438, nll_loss=3.558, ppl=11.78, wps=20875.3, ups=0.32, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.828, loss_scale=8, train_wall=291, gb_free=19.9, wall=32106
2022-03-06 21:39:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:39:20 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.516 | nll_loss 8.092 | ppl 272.83 | wps 41478.4 | wpb 510.9 | bsz 1 | num_updates 10123 | best_loss 8.249
2022-03-06 21:39:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10123 updates
2022-03-06 21:39:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:39:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:39:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 52 @ 10123 updates, score 9.516) (writing took 3.352778864093125 seconds)
2022-03-06 21:39:23 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 21:39:23 | INFO | train | epoch 052 | loss 5.412 | nll_loss 3.528 | ppl 11.53 | wps 20571.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10123 | lr 0.000314301 | gnorm 0.824 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 32185
2022-03-06 21:39:24 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 21:39:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:43:23 | INFO | train_inner | epoch 053:     77 / 196 loss=5.366, nll_loss=3.475, ppl=11.12, wps=20496.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.834, loss_scale=8, train_wall=288, gb_free=19.9, wall=32425
2022-03-06 21:48:34 | INFO | train_inner | epoch 053:    177 / 196 loss=5.42, nll_loss=3.536, ppl=11.6, wps=21073.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.837, loss_scale=16, train_wall=289, gb_free=19.9, wall=32736
2022-03-06 21:48:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:49:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:49:37 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.579 | nll_loss 8.154 | ppl 284.89 | wps 41592.1 | wpb 510.9 | bsz 1 | num_updates 10318 | best_loss 8.249
2022-03-06 21:49:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10318 updates
2022-03-06 21:49:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:49:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:49:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 53 @ 10318 updates, score 9.579) (writing took 3.366867881268263 seconds)
2022-03-06 21:49:41 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 21:49:41 | INFO | train | epoch 053 | loss 5.386 | nll_loss 3.498 | ppl 11.3 | wps 20672.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10318 | lr 0.000311317 | gnorm 0.837 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 32802
2022-03-06 21:49:41 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 21:49:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:53:56 | INFO | train_inner | epoch 054:     82 / 196 loss=5.325, nll_loss=3.426, ppl=10.74, wps=20324.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.833, loss_scale=8, train_wall=290, gb_free=19.9, wall=33057
2022-03-06 21:59:06 | INFO | train_inner | epoch 054:    182 / 196 loss=5.397, nll_loss=3.51, ppl=11.39, wps=21085, ups=0.32, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.838, loss_scale=16, train_wall=289, gb_free=19.9, wall=33368
2022-03-06 21:59:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:59:54 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.584 | nll_loss 8.168 | ppl 287.63 | wps 41363.8 | wpb 510.9 | bsz 1 | num_updates 10513 | best_loss 8.249
2022-03-06 21:59:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10513 updates
2022-03-06 21:59:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:59:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 21:59:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 54 @ 10513 updates, score 9.584) (writing took 3.3981081880629063 seconds)
2022-03-06 21:59:58 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 21:59:58 | INFO | train | epoch 054 | loss 5.359 | nll_loss 3.465 | ppl 11.04 | wps 20683.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10513 | lr 0.000308416 | gnorm 0.831 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 33419
2022-03-06 21:59:58 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 21:59:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:04:28 | INFO | train_inner | epoch 055:     87 / 196 loss=5.301, nll_loss=3.398, ppl=10.54, wps=20299.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.835, loss_scale=8, train_wall=291, gb_free=19.9, wall=33690
2022-03-06 22:07:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:09:42 | INFO | train_inner | epoch 055:    188 / 196 loss=5.376, nll_loss=3.485, ppl=11.19, wps=20880.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.848, loss_scale=8, train_wall=291, gb_free=19.9, wall=34004
2022-03-06 22:10:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:10:12 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.616 | nll_loss 8.181 | ppl 290.28 | wps 41563.4 | wpb 510.9 | bsz 1 | num_updates 10708 | best_loss 8.249
2022-03-06 22:10:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10708 updates
2022-03-06 22:10:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 55 @ 10708 updates, score 9.616) (writing took 3.3760222094133496 seconds)
2022-03-06 22:10:15 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 22:10:15 | INFO | train | epoch 055 | loss 5.335 | nll_loss 3.436 | ppl 10.82 | wps 20678.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10708 | lr 0.000305595 | gnorm 0.843 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 34037
2022-03-06 22:10:15 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 22:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:15:01 | INFO | train_inner | epoch 056:     92 / 196 loss=5.272, nll_loss=3.363, ppl=10.29, wps=20491.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.844, loss_scale=16, train_wall=288, gb_free=19.9, wall=34323
2022-03-06 22:16:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:20:15 | INFO | train_inner | epoch 056:    193 / 196 loss=5.351, nll_loss=3.455, ppl=10.97, wps=20870.7, ups=0.32, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.849, loss_scale=8, train_wall=292, gb_free=19.9, wall=34637
2022-03-06 22:20:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:20:29 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.623 | nll_loss 8.19 | ppl 292.04 | wps 41443 | wpb 510.9 | bsz 1 | num_updates 10903 | best_loss 8.249
2022-03-06 22:20:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10903 updates
2022-03-06 22:20:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:20:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:20:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 56 @ 10903 updates, score 9.623) (writing took 3.4309792146086693 seconds)
2022-03-06 22:20:33 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 22:20:33 | INFO | train | epoch 056 | loss 5.31 | nll_loss 3.407 | ppl 10.61 | wps 20667.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10903 | lr 0.00030285 | gnorm 0.848 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 34654
2022-03-06 22:20:33 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 22:20:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:23:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:25:37 | INFO | train_inner | epoch 057:     98 / 196 loss=5.246, nll_loss=3.332, ppl=10.07, wps=20290.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.857, loss_scale=8, train_wall=291, gb_free=19.9, wall=34959
2022-03-06 22:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:30:46 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.711 | nll_loss 8.295 | ppl 314.1 | wps 41733.4 | wpb 510.9 | bsz 1 | num_updates 11098 | best_loss 8.249
2022-03-06 22:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11098 updates
2022-03-06 22:30:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:30:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 57 @ 11098 updates, score 9.711) (writing took 3.354518481530249 seconds)
2022-03-06 22:30:50 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 22:30:50 | INFO | train | epoch 057 | loss 5.286 | nll_loss 3.378 | ppl 10.4 | wps 20674.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 11098 | lr 0.000300177 | gnorm 0.857 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 35271
2022-03-06 22:30:50 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 22:30:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:30:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:30:59 | INFO | train_inner | epoch 058:      3 / 196 loss=5.327, nll_loss=3.426, ppl=10.75, wps=20307.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.857, loss_scale=8, train_wall=291, gb_free=19.9, wall=35281
2022-03-06 22:36:10 | INFO | train_inner | epoch 058:    103 / 196 loss=5.221, nll_loss=3.302, ppl=9.86, wps=21083.9, ups=0.32, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.863, loss_scale=8, train_wall=289, gb_free=19.9, wall=35592
2022-03-06 22:38:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:40:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:41:04 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.728 | nll_loss 8.307 | ppl 316.74 | wps 41292.5 | wpb 510.9 | bsz 1 | num_updates 11292 | best_loss 8.249
2022-03-06 22:41:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11292 updates
2022-03-06 22:41:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:41:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 58 @ 11292 updates, score 9.728) (writing took 3.3594683660194278 seconds)
2022-03-06 22:41:07 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 22:41:07 | INFO | train | epoch 058 | loss 5.263 | nll_loss 3.352 | ppl 10.21 | wps 20569.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11292 | lr 0.000297587 | gnorm 0.86 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 35889
2022-03-06 22:41:07 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 22:41:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:41:32 | INFO | train_inner | epoch 059:      8 / 196 loss=5.297, nll_loss=3.391, ppl=10.49, wps=20300, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.857, loss_scale=8, train_wall=291, gb_free=19.9, wall=35914
2022-03-06 22:45:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:46:46 | INFO | train_inner | epoch 059:    109 / 196 loss=5.203, nll_loss=3.281, ppl=9.72, wps=20901.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.861, loss_scale=8, train_wall=291, gb_free=19.9, wall=36227
2022-03-06 22:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:51:20 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.773 | nll_loss 8.358 | ppl 328.04 | wps 41664.3 | wpb 510.9 | bsz 1 | num_updates 11487 | best_loss 8.249
2022-03-06 22:51:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11487 updates
2022-03-06 22:51:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:51:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 22:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 59 @ 11487 updates, score 9.773) (writing took 3.400751887820661 seconds)
2022-03-06 22:51:24 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 22:51:24 | INFO | train | epoch 059 | loss 5.241 | nll_loss 3.326 | ppl 10.03 | wps 20695.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 11487 | lr 0.000295051 | gnorm 0.87 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 36505
2022-03-06 22:51:24 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 22:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:52:04 | INFO | train_inner | epoch 060:     13 / 196 loss=5.275, nll_loss=3.365, ppl=10.3, wps=20515.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.879, loss_scale=8, train_wall=288, gb_free=19.9, wall=36546
2022-03-06 22:57:15 | INFO | train_inner | epoch 060:    113 / 196 loss=5.19, nll_loss=3.265, ppl=9.61, wps=21078.3, ups=0.32, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.857, loss_scale=16, train_wall=289, gb_free=19.9, wall=36857
2022-03-06 22:58:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:01:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:01:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:01:38 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.785 | nll_loss 8.358 | ppl 328.06 | wps 41347.2 | wpb 510.9 | bsz 1 | num_updates 11681 | best_loss 8.249
2022-03-06 23:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11681 updates
2022-03-06 23:01:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:01:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 60 @ 11681 updates, score 9.785) (writing took 3.308030948974192 seconds)
2022-03-06 23:01:41 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 23:01:41 | INFO | train | epoch 060 | loss 5.219 | nll_loss 3.299 | ppl 9.85 | wps 20570.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11681 | lr 0.00029259 | gnorm 0.863 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 37123
2022-03-06 23:01:41 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 23:01:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:02:40 | INFO | train_inner | epoch 061:     19 / 196 loss=5.24, nll_loss=3.324, ppl=10.01, wps=20114.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.874, loss_scale=8, train_wall=294, gb_free=19.9, wall=37182
2022-03-06 23:07:51 | INFO | train_inner | epoch 061:    119 / 196 loss=5.172, nll_loss=3.244, ppl=9.47, wps=21077.1, ups=0.32, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.876, loss_scale=8, train_wall=289, gb_free=19.9, wall=37493
2022-03-06 23:08:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:11:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:11:55 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.837 | nll_loss 8.435 | ppl 346.18 | wps 41118.9 | wpb 510.9 | bsz 1 | num_updates 11876 | best_loss 8.249
2022-03-06 23:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11876 updates
2022-03-06 23:11:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:11:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 61 @ 11876 updates, score 9.837) (writing took 3.2328589372336864 seconds)
2022-03-06 23:11:58 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 23:11:58 | INFO | train | epoch 061 | loss 5.2 | nll_loss 3.276 | ppl 9.69 | wps 20667.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 11876 | lr 0.000290178 | gnorm 0.883 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 37740
2022-03-06 23:11:59 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 23:11:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:13:13 | INFO | train_inner | epoch 062:     24 / 196 loss=5.219, nll_loss=3.299, ppl=9.84, wps=20283.6, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.895, loss_scale=8, train_wall=291, gb_free=19.9, wall=37815
2022-03-06 23:18:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:18:27 | INFO | train_inner | epoch 062:    125 / 196 loss=5.154, nll_loss=3.223, ppl=9.34, wps=20864.9, ups=0.32, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.866, loss_scale=8, train_wall=292, gb_free=19.9, wall=38129
2022-03-06 23:22:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:22:12 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.838 | nll_loss 8.434 | ppl 345.9 | wps 41418.3 | wpb 510.9 | bsz 1 | num_updates 12071 | best_loss 8.249
2022-03-06 23:22:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12071 updates
2022-03-06 23:22:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:22:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:22:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 62 @ 12071 updates, score 9.838) (writing took 3.3076274171471596 seconds)
2022-03-06 23:22:16 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 23:22:16 | INFO | train | epoch 062 | loss 5.178 | nll_loss 3.251 | ppl 9.52 | wps 20673.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 12071 | lr 0.000287825 | gnorm 0.881 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 38357
2022-03-06 23:22:16 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 23:22:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:23:46 | INFO | train_inner | epoch 063:     29 / 196 loss=5.194, nll_loss=3.27, ppl=9.64, wps=20514.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.886, loss_scale=8, train_wall=288, gb_free=19.9, wall=38448
2022-03-06 23:28:57 | INFO | train_inner | epoch 063:    129 / 196 loss=5.14, nll_loss=3.206, ppl=9.23, wps=21090.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.877, loss_scale=16, train_wall=288, gb_free=19.9, wall=38758
2022-03-06 23:30:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:32:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:32:29 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.916 | nll_loss 8.513 | ppl 365.42 | wps 41419 | wpb 510.9 | bsz 1 | num_updates 12266 | best_loss 8.249
2022-03-06 23:32:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12266 updates
2022-03-06 23:32:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:32:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:32:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 63 @ 12266 updates, score 9.916) (writing took 3.3294495083391666 seconds)
2022-03-06 23:32:33 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 23:32:33 | INFO | train | epoch 063 | loss 5.158 | nll_loss 3.228 | ppl 9.37 | wps 20683.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 12266 | lr 0.000285528 | gnorm 0.888 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 38974
2022-03-06 23:32:33 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 23:32:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:34:19 | INFO | train_inner | epoch 064:     34 / 196 loss=5.167, nll_loss=3.238, ppl=9.43, wps=20306.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.896, loss_scale=8, train_wall=291, gb_free=19.9, wall=39080
2022-03-06 23:39:29 | INFO | train_inner | epoch 064:    134 / 196 loss=5.128, nll_loss=3.191, ppl=9.13, wps=21084.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.89, loss_scale=16, train_wall=289, gb_free=19.9, wall=39391
2022-03-06 23:42:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:42:47 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.889 | nll_loss 8.484 | ppl 358.12 | wps 41587.8 | wpb 510.9 | bsz 1 | num_updates 12461 | best_loss 8.249
2022-03-06 23:42:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12461 updates
2022-03-06 23:42:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:42:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:42:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 64 @ 12461 updates, score 9.889) (writing took 3.431114723905921 seconds)
2022-03-06 23:42:50 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 23:42:50 | INFO | train | epoch 064 | loss 5.14 | nll_loss 3.206 | ppl 9.23 | wps 20676.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 12461 | lr 0.000283285 | gnorm 0.889 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 39592
2022-03-06 23:42:50 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 23:42:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:44:51 | INFO | train_inner | epoch 065:     39 / 196 loss=5.141, nll_loss=3.207, ppl=9.23, wps=20298.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.893, loss_scale=8, train_wall=291, gb_free=19.9, wall=39713
2022-03-06 23:50:02 | INFO | train_inner | epoch 065:    139 / 196 loss=5.113, nll_loss=3.174, ppl=9.02, wps=21081.6, ups=0.32, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.898, loss_scale=16, train_wall=289, gb_free=19.9, wall=40024
2022-03-06 23:52:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:53:04 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.939 | nll_loss 8.542 | ppl 372.71 | wps 41546.5 | wpb 510.9 | bsz 1 | num_updates 12657 | best_loss 8.249
2022-03-06 23:53:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12657 updates
2022-03-06 23:53:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:53:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-06 23:53:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 65 @ 12657 updates, score 9.939) (writing took 3.4232505103573203 seconds)
2022-03-06 23:53:07 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-06 23:53:07 | INFO | train | epoch 065 | loss 5.122 | nll_loss 3.184 | ppl 9.09 | wps 20778.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 12657 | lr 0.000281083 | gnorm 0.891 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 40209
2022-03-06 23:53:07 | INFO | fairseq.trainer | begin training epoch 66
2022-03-06 23:53:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:54:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:55:24 | INFO | train_inner | epoch 066:     44 / 196 loss=5.121, nll_loss=3.183, ppl=9.08, wps=20307, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.897, loss_scale=8, train_wall=291, gb_free=19.9, wall=40346
2022-03-07 00:00:35 | INFO | train_inner | epoch 066:    144 / 196 loss=5.102, nll_loss=3.161, ppl=8.95, wps=21078.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.889, loss_scale=8, train_wall=289, gb_free=19.9, wall=40657
2022-03-07 00:02:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:03:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:03:21 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.963 | nll_loss 8.564 | ppl 378.42 | wps 41469.5 | wpb 510.9 | bsz 1 | num_updates 12851 | best_loss 8.249
2022-03-07 00:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12851 updates
2022-03-07 00:03:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:03:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:03:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 66 @ 12851 updates, score 9.963) (writing took 3.320874203927815 seconds)
2022-03-07 00:03:25 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-07 00:03:25 | INFO | train | epoch 066 | loss 5.103 | nll_loss 3.162 | ppl 8.95 | wps 20569.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12851 | lr 0.000278953 | gnorm 0.898 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 40826
2022-03-07 00:03:25 | INFO | fairseq.trainer | begin training epoch 67
2022-03-07 00:03:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:05:57 | INFO | train_inner | epoch 067:     49 / 196 loss=5.091, nll_loss=3.148, ppl=8.87, wps=20304.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.894, loss_scale=8, train_wall=291, gb_free=19.9, wall=40979
2022-03-07 00:11:08 | INFO | train_inner | epoch 067:    149 / 196 loss=5.09, nll_loss=3.146, ppl=8.85, wps=21094.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.904, loss_scale=16, train_wall=288, gb_free=19.9, wall=41289
2022-03-07 00:12:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:13:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:13:38 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.987 | nll_loss 8.58 | ppl 382.56 | wps 41686.8 | wpb 510.9 | bsz 1 | num_updates 13046 | best_loss 8.249
2022-03-07 00:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13046 updates
2022-03-07 00:13:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:13:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:13:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 67 @ 13046 updates, score 9.987) (writing took 3.301856391131878 seconds)
2022-03-07 00:13:42 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-07 00:13:42 | INFO | train | epoch 067 | loss 5.086 | nll_loss 3.142 | ppl 8.83 | wps 20684.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13046 | lr 0.000276861 | gnorm 0.903 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 41443
2022-03-07 00:13:42 | INFO | fairseq.trainer | begin training epoch 68
2022-03-07 00:13:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:16:30 | INFO | train_inner | epoch 068:     54 / 196 loss=5.072, nll_loss=3.126, ppl=8.73, wps=20310.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.927, loss_scale=8, train_wall=291, gb_free=19.9, wall=41611
2022-03-07 00:21:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:21:43 | INFO | train_inner | epoch 068:    155 / 196 loss=5.077, nll_loss=3.132, ppl=8.76, wps=20885.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.916, loss_scale=8, train_wall=291, gb_free=19.9, wall=41925
2022-03-07 00:23:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:23:55 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.016 | nll_loss 8.626 | ppl 395.03 | wps 41509 | wpb 510.9 | bsz 1 | num_updates 13241 | best_loss 8.249
2022-03-07 00:23:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13241 updates
2022-03-07 00:23:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:23:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:23:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 68 @ 13241 updates, score 10.016) (writing took 3.1970129339024425 seconds)
2022-03-07 00:23:58 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-07 00:23:58 | INFO | train | epoch 068 | loss 5.069 | nll_loss 3.122 | ppl 8.7 | wps 20692.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13241 | lr 0.000274814 | gnorm 0.918 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 42060
2022-03-07 00:23:58 | INFO | fairseq.trainer | begin training epoch 69
2022-03-07 00:23:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:27:02 | INFO | train_inner | epoch 069:     59 / 196 loss=5.05, nll_loss=3.099, ppl=8.57, wps=20521.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=0.905, loss_scale=8, train_wall=288, gb_free=19.9, wall=42243
2022-03-07 00:28:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:32:16 | INFO | train_inner | epoch 069:    160 / 196 loss=5.065, nll_loss=3.116, ppl=8.67, wps=20871.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.913, loss_scale=8, train_wall=292, gb_free=19.9, wall=42557
2022-03-07 00:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:34:12 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.02 | nll_loss 8.62 | ppl 393.42 | wps 41418.9 | wpb 510.9 | bsz 1 | num_updates 13436 | best_loss 8.249
2022-03-07 00:34:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13436 updates
2022-03-07 00:34:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:34:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:34:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 69 @ 13436 updates, score 10.02) (writing took 3.255895976908505 seconds)
2022-03-07 00:34:16 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-07 00:34:16 | INFO | train | epoch 069 | loss 5.052 | nll_loss 3.102 | ppl 8.59 | wps 20680.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13436 | lr 0.000272813 | gnorm 0.907 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 42677
2022-03-07 00:34:16 | INFO | fairseq.trainer | begin training epoch 70
2022-03-07 00:34:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:37:35 | INFO | train_inner | epoch 070:     64 / 196 loss=5.028, nll_loss=3.074, ppl=8.42, wps=20508.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.905, loss_scale=16, train_wall=288, gb_free=19.9, wall=42876
2022-03-07 00:40:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:42:48 | INFO | train_inner | epoch 070:    165 / 196 loss=5.055, nll_loss=3.105, ppl=8.6, wps=20875.8, ups=0.32, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.922, loss_scale=8, train_wall=291, gb_free=19.9, wall=43190
2022-03-07 00:44:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:44:29 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.096 | nll_loss 8.7 | ppl 415.91 | wps 41684.8 | wpb 510.9 | bsz 1 | num_updates 13631 | best_loss 8.249
2022-03-07 00:44:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13631 updates
2022-03-07 00:44:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:44:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:44:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 70 @ 13631 updates, score 10.096) (writing took 3.2177843507379293 seconds)
2022-03-07 00:44:33 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-07 00:44:33 | INFO | train | epoch 070 | loss 5.037 | nll_loss 3.084 | ppl 8.48 | wps 20682.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13631 | lr 0.000270855 | gnorm 0.917 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 43294
2022-03-07 00:44:33 | INFO | fairseq.trainer | begin training epoch 71
2022-03-07 00:44:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:48:07 | INFO | train_inner | epoch 071:     69 / 196 loss=5.008, nll_loss=3.05, ppl=8.28, wps=20515.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=0.919, loss_scale=16, train_wall=288, gb_free=19.9, wall=43509
2022-03-07 00:48:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:53:21 | INFO | train_inner | epoch 071:    170 / 196 loss=5.038, nll_loss=3.085, ppl=8.49, wps=20885.5, ups=0.32, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.925, loss_scale=8, train_wall=291, gb_free=19.9, wall=43822
2022-03-07 00:54:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:54:46 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.102 | nll_loss 8.699 | ppl 415.6 | wps 41435.3 | wpb 510.9 | bsz 1 | num_updates 13826 | best_loss 8.249
2022-03-07 00:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13826 updates
2022-03-07 00:54:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:54:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 00:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 71 @ 13826 updates, score 10.102) (writing took 3.269310873001814 seconds)
2022-03-07 00:54:49 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-07 00:54:49 | INFO | train | epoch 071 | loss 5.021 | nll_loss 3.065 | ppl 8.37 | wps 20687.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13826 | lr 0.000268938 | gnorm 0.927 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 43911
2022-03-07 00:54:50 | INFO | fairseq.trainer | begin training epoch 72
2022-03-07 00:54:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:58:40 | INFO | train_inner | epoch 072:     74 / 196 loss=4.988, nll_loss=3.026, ppl=8.14, wps=20506.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.923, loss_scale=16, train_wall=288, gb_free=19.9, wall=44141
2022-03-07 01:00:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:03:53 | INFO | train_inner | epoch 072:    175 / 196 loss=5.028, nll_loss=3.073, ppl=8.42, wps=20891.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.928, loss_scale=8, train_wall=291, gb_free=19.9, wall=44455
2022-03-07 01:04:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:05:03 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.155 | nll_loss 8.77 | ppl 436.46 | wps 41598.5 | wpb 510.9 | bsz 1 | num_updates 14021 | best_loss 8.249
2022-03-07 01:05:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14021 updates
2022-03-07 01:05:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:05:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:05:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 72 @ 14021 updates, score 10.155) (writing took 3.39628981705755 seconds)
2022-03-07 01:05:06 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-07 01:05:06 | INFO | train | epoch 072 | loss 5.005 | nll_loss 3.046 | ppl 8.26 | wps 20685.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 14021 | lr 0.000267061 | gnorm 0.922 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 44528
2022-03-07 01:05:06 | INFO | fairseq.trainer | begin training epoch 73
2022-03-07 01:05:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:09:12 | INFO | train_inner | epoch 073:     79 / 196 loss=4.97, nll_loss=3.005, ppl=8.03, wps=20498.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=0.933, loss_scale=16, train_wall=288, gb_free=19.9, wall=44774
2022-03-07 01:10:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:14:26 | INFO | train_inner | epoch 073:    180 / 196 loss=5.02, nll_loss=3.063, ppl=8.36, wps=20881, ups=0.32, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.93, loss_scale=8, train_wall=291, gb_free=19.9, wall=45088
2022-03-07 01:15:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:15:20 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.087 | nll_loss 8.68 | ppl 410.15 | wps 41608.8 | wpb 510.9 | bsz 1 | num_updates 14216 | best_loss 8.249
2022-03-07 01:15:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14216 updates
2022-03-07 01:15:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:15:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 73 @ 14216 updates, score 10.087) (writing took 3.43907174654305 seconds)
2022-03-07 01:15:24 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-07 01:15:24 | INFO | train | epoch 073 | loss 4.991 | nll_loss 3.029 | ppl 8.16 | wps 20676 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 14216 | lr 0.000265223 | gnorm 0.932 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 45145
2022-03-07 01:15:24 | INFO | fairseq.trainer | begin training epoch 74
2022-03-07 01:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:19:45 | INFO | train_inner | epoch 074:     84 / 196 loss=4.948, nll_loss=2.979, ppl=7.88, wps=20492.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.931, loss_scale=16, train_wall=288, gb_free=19.9, wall=45407
2022-03-07 01:22:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:24:59 | INFO | train_inner | epoch 074:    185 / 196 loss=5.012, nll_loss=3.054, ppl=8.31, wps=20878.4, ups=0.32, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.945, loss_scale=8, train_wall=291, gb_free=19.9, wall=45721
2022-03-07 01:25:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:25:38 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.192 | nll_loss 8.82 | ppl 451.94 | wps 41664.2 | wpb 510.9 | bsz 1 | num_updates 14411 | best_loss 8.249
2022-03-07 01:25:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14411 updates
2022-03-07 01:25:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:25:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:25:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 74 @ 14411 updates, score 10.192) (writing took 3.330808357335627 seconds)
2022-03-07 01:25:41 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-07 01:25:41 | INFO | train | epoch 074 | loss 4.976 | nll_loss 3.012 | ppl 8.07 | wps 20676 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 14411 | lr 0.000263423 | gnorm 0.937 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 45763
2022-03-07 01:25:41 | INFO | fairseq.trainer | begin training epoch 75
2022-03-07 01:25:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:30:18 | INFO | train_inner | epoch 075:     89 / 196 loss=4.928, nll_loss=2.956, ppl=7.76, wps=20515.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=0.933, loss_scale=16, train_wall=288, gb_free=19.9, wall=46039
2022-03-07 01:35:28 | INFO | train_inner | epoch 075:    189 / 196 loss=5.003, nll_loss=3.043, ppl=8.24, wps=21092.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.959, loss_scale=16, train_wall=289, gb_free=19.9, wall=46350
2022-03-07 01:35:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:35:54 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.136 | nll_loss 8.746 | ppl 429.46 | wps 41635.3 | wpb 510.9 | bsz 1 | num_updates 14607 | best_loss 8.249
2022-03-07 01:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14607 updates
2022-03-07 01:35:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:35:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:35:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 75 @ 14607 updates, score 10.136) (writing took 3.415283566340804 seconds)
2022-03-07 01:35:58 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-07 01:35:58 | INFO | train | epoch 075 | loss 4.963 | nll_loss 2.996 | ppl 7.98 | wps 20792.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 14607 | lr 0.000261649 | gnorm 0.947 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 46380
2022-03-07 01:35:58 | INFO | fairseq.trainer | begin training epoch 76
2022-03-07 01:35:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:36:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:39:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:40:53 | INFO | train_inner | epoch 076:     95 / 196 loss=4.913, nll_loss=2.938, ppl=7.66, wps=20115, ups=0.31, wpb=65367, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=0.937, loss_scale=8, train_wall=293, gb_free=19.9, wall=46675
2022-03-07 01:46:04 | INFO | train_inner | epoch 076:    195 / 196 loss=4.985, nll_loss=3.022, ppl=8.12, wps=21083.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.937, loss_scale=16, train_wall=289, gb_free=19.9, wall=46986
2022-03-07 01:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:46:12 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.222 | nll_loss 8.849 | ppl 461.04 | wps 41561.7 | wpb 510.9 | bsz 1 | num_updates 14801 | best_loss 8.249
2022-03-07 01:46:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14801 updates
2022-03-07 01:46:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 76 @ 14801 updates, score 10.222) (writing took 3.373293931595981 seconds)
2022-03-07 01:46:15 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-07 01:46:15 | INFO | train | epoch 076 | loss 4.947 | nll_loss 2.978 | ppl 7.88 | wps 20574.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14801 | lr 0.000259929 | gnorm 0.938 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 46997
2022-03-07 01:46:15 | INFO | fairseq.trainer | begin training epoch 77
2022-03-07 01:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:51:23 | INFO | train_inner | epoch 077:     99 / 196 loss=4.896, nll_loss=2.918, ppl=7.56, wps=20497.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14900, lr=0.000259064, gnorm=0.934, loss_scale=16, train_wall=288, gb_free=19.9, wall=47305
2022-03-07 01:52:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:54:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:56:29 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.234 | nll_loss 8.859 | ppl 464.19 | wps 41514.6 | wpb 510.9 | bsz 1 | num_updates 14995 | best_loss 8.249
2022-03-07 01:56:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14995 updates
2022-03-07 01:56:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 01:56:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 77 @ 14995 updates, score 10.234) (writing took 3.3920549247413874 seconds)
2022-03-07 01:56:32 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-07 01:56:32 | INFO | train | epoch 077 | loss 4.936 | nll_loss 2.965 | ppl 7.81 | wps 20562.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14995 | lr 0.000258242 | gnorm 0.942 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 47614
2022-03-07 01:56:33 | INFO | fairseq.trainer | begin training epoch 78
2022-03-07 01:56:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:56:48 | INFO | train_inner | epoch 078:      5 / 196 loss=4.971, nll_loss=3.006, ppl=8.03, wps=20098.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=0.952, loss_scale=8, train_wall=294, gb_free=19.9, wall=47630
2022-03-07 02:01:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:02:03 | INFO | train_inner | epoch 078:    106 / 196 loss=4.881, nll_loss=2.901, ppl=7.47, wps=20835.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.966, loss_scale=8, train_wall=292, gb_free=19.9, wall=47944
2022-03-07 02:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:06:47 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.214 | nll_loss 8.83 | ppl 455.16 | wps 41539.1 | wpb 510.9 | bsz 1 | num_updates 15190 | best_loss 8.249
2022-03-07 02:06:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15190 updates
2022-03-07 02:06:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 78 @ 15190 updates, score 10.214) (writing took 3.3723828634247184 seconds)
2022-03-07 02:06:51 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-07 02:06:51 | INFO | train | epoch 078 | loss 4.923 | nll_loss 2.949 | ppl 7.72 | wps 20647.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15190 | lr 0.000256579 | gnorm 0.956 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 48232
2022-03-07 02:06:51 | INFO | fairseq.trainer | begin training epoch 79
2022-03-07 02:06:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:07:22 | INFO | train_inner | epoch 079:     10 / 196 loss=4.96, nll_loss=2.993, ppl=7.96, wps=20483.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=0.944, loss_scale=8, train_wall=288, gb_free=19.9, wall=48263
2022-03-07 02:12:33 | INFO | train_inner | epoch 079:    110 / 196 loss=4.874, nll_loss=2.892, ppl=7.42, wps=21072.8, ups=0.32, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.942, loss_scale=16, train_wall=289, gb_free=19.9, wall=48574
2022-03-07 02:13:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:16:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:17:05 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.269 | nll_loss 8.889 | ppl 473.97 | wps 41455.7 | wpb 510.9 | bsz 1 | num_updates 15385 | best_loss 8.249
2022-03-07 02:17:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15385 updates
2022-03-07 02:17:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:17:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:17:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 79 @ 15385 updates, score 10.269) (writing took 3.4598305746912956 seconds)
2022-03-07 02:17:08 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-07 02:17:08 | INFO | train | epoch 079 | loss 4.91 | nll_loss 2.933 | ppl 7.64 | wps 20668.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15385 | lr 0.000254948 | gnorm 0.951 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 48850
2022-03-07 02:17:08 | INFO | fairseq.trainer | begin training epoch 80
2022-03-07 02:17:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:17:55 | INFO | train_inner | epoch 080:     15 / 196 loss=4.938, nll_loss=2.967, ppl=7.82, wps=20296.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=0.961, loss_scale=8, train_wall=291, gb_free=19.9, wall=48896
2022-03-07 02:23:06 | INFO | train_inner | epoch 080:    115 / 196 loss=4.866, nll_loss=2.883, ppl=7.37, wps=21077, ups=0.32, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.95, loss_scale=16, train_wall=289, gb_free=19.9, wall=49207
2022-03-07 02:23:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:27:22 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.245 | nll_loss 8.873 | ppl 469 | wps 41636.5 | wpb 510.9 | bsz 1 | num_updates 15580 | best_loss 8.249
2022-03-07 02:27:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15580 updates
2022-03-07 02:27:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:27:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:27:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 80 @ 15580 updates, score 10.245) (writing took 3.3639127435162663 seconds)
2022-03-07 02:27:25 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-07 02:27:25 | INFO | train | epoch 080 | loss 4.897 | nll_loss 2.919 | ppl 7.56 | wps 20672.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15580 | lr 0.000253347 | gnorm 0.962 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 49467
2022-03-07 02:27:25 | INFO | fairseq.trainer | begin training epoch 81
2022-03-07 02:27:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:28:28 | INFO | train_inner | epoch 081:     20 / 196 loss=4.928, nll_loss=2.955, ppl=7.76, wps=20306.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=0.971, loss_scale=8, train_wall=291, gb_free=19.9, wall=49529
2022-03-07 02:33:39 | INFO | train_inner | epoch 081:    120 / 196 loss=4.861, nll_loss=2.876, ppl=7.34, wps=21079, ups=0.32, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.946, loss_scale=16, train_wall=289, gb_free=19.9, wall=49840
2022-03-07 02:37:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:37:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:37:39 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.285 | nll_loss 8.91 | ppl 481.19 | wps 41278.7 | wpb 510.9 | bsz 1 | num_updates 15775 | best_loss 8.249
2022-03-07 02:37:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15775 updates
2022-03-07 02:37:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:37:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:37:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 81 @ 15775 updates, score 10.285) (writing took 3.4104409655556083 seconds)
2022-03-07 02:37:43 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-07 02:37:43 | INFO | train | epoch 081 | loss 4.885 | nll_loss 2.904 | ppl 7.49 | wps 20672.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15775 | lr 0.000251777 | gnorm 0.948 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 50084
2022-03-07 02:37:43 | INFO | fairseq.trainer | begin training epoch 82
2022-03-07 02:37:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:39:01 | INFO | train_inner | epoch 082:     25 / 196 loss=4.9, nll_loss=2.923, ppl=7.58, wps=20294.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=0.952, loss_scale=16, train_wall=291, gb_free=19.9, wall=50162
2022-03-07 02:42:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:44:15 | INFO | train_inner | epoch 082:    126 / 196 loss=4.853, nll_loss=2.867, ppl=7.29, wps=20877.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.955, loss_scale=8, train_wall=291, gb_free=19.9, wall=50476
2022-03-07 02:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:47:57 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.247 | nll_loss 8.875 | ppl 469.47 | wps 41650.6 | wpb 510.9 | bsz 1 | num_updates 15970 | best_loss 8.249
2022-03-07 02:47:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15970 updates
2022-03-07 02:47:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:48:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 82 @ 15970 updates, score 10.247) (writing took 3.4184604110196233 seconds)
2022-03-07 02:48:00 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-07 02:48:00 | INFO | train | epoch 082 | loss 4.874 | nll_loss 2.891 | ppl 7.42 | wps 20675.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15970 | lr 0.000250235 | gnorm 0.962 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 50702
2022-03-07 02:48:00 | INFO | fairseq.trainer | begin training epoch 83
2022-03-07 02:48:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:49:33 | INFO | train_inner | epoch 083:     30 / 196 loss=4.885, nll_loss=2.904, ppl=7.49, wps=20503.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=0.973, loss_scale=8, train_wall=288, gb_free=19.9, wall=50795
2022-03-07 02:54:44 | INFO | train_inner | epoch 083:    130 / 196 loss=4.848, nll_loss=2.861, ppl=7.26, wps=21082.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.944, loss_scale=16, train_wall=289, gb_free=19.9, wall=51106
2022-03-07 02:56:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:58:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:58:14 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.34 | nll_loss 8.972 | ppl 502.23 | wps 41520.4 | wpb 510.9 | bsz 1 | num_updates 16165 | best_loss 8.249
2022-03-07 02:58:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16165 updates
2022-03-07 02:58:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:58:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 02:58:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 83 @ 16165 updates, score 10.34) (writing took 3.281175861135125 seconds)
2022-03-07 02:58:17 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-07 02:58:17 | INFO | train | epoch 083 | loss 4.862 | nll_loss 2.877 | ppl 7.35 | wps 20682 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16165 | lr 0.000248721 | gnorm 0.958 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 51319
2022-03-07 02:58:17 | INFO | fairseq.trainer | begin training epoch 84
2022-03-07 02:58:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:00:06 | INFO | train_inner | epoch 084:     35 / 196 loss=4.865, nll_loss=2.881, ppl=7.37, wps=20298.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=0.972, loss_scale=16, train_wall=291, gb_free=19.9, wall=51428
2022-03-07 03:03:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:05:21 | INFO | train_inner | epoch 084:    136 / 196 loss=4.846, nll_loss=2.859, ppl=7.25, wps=20848, ups=0.32, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.958, loss_scale=16, train_wall=292, gb_free=19.9, wall=51742
2022-03-07 03:07:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:08:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:08:32 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.368 | nll_loss 8.998 | ppl 511.29 | wps 41351.7 | wpb 510.9 | bsz 1 | num_updates 16359 | best_loss 8.249
2022-03-07 03:08:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16359 updates
2022-03-07 03:08:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:08:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:08:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 84 @ 16359 updates, score 10.368) (writing took 3.2076983777806163 seconds)
2022-03-07 03:08:35 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-07 03:08:35 | INFO | train | epoch 084 | loss 4.85 | nll_loss 2.863 | ppl 7.27 | wps 20547.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16359 | lr 0.000247242 | gnorm 0.967 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 51937
2022-03-07 03:08:35 | INFO | fairseq.trainer | begin training epoch 85
2022-03-07 03:08:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:10:43 | INFO | train_inner | epoch 085:     41 / 196 loss=4.852, nll_loss=2.865, ppl=7.29, wps=20296.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=0.973, loss_scale=8, train_wall=291, gb_free=19.9, wall=52064
2022-03-07 03:15:53 | INFO | train_inner | epoch 085:    141 / 196 loss=4.835, nll_loss=2.846, ppl=7.19, wps=21092, ups=0.32, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.961, loss_scale=16, train_wall=288, gb_free=19.9, wall=52375
2022-03-07 03:16:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:18:49 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.388 | nll_loss 9.02 | ppl 519.22 | wps 41818.4 | wpb 510.9 | bsz 1 | num_updates 16554 | best_loss 8.249
2022-03-07 03:18:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16554 updates
2022-03-07 03:18:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:18:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:18:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 85 @ 16554 updates, score 10.388) (writing took 3.1718645188957453 seconds)
2022-03-07 03:18:52 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-07 03:18:52 | INFO | train | epoch 085 | loss 4.84 | nll_loss 2.851 | ppl 7.21 | wps 20691.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16554 | lr 0.000245781 | gnorm 0.972 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 52553
2022-03-07 03:18:52 | INFO | fairseq.trainer | begin training epoch 86
2022-03-07 03:18:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:21:15 | INFO | train_inner | epoch 086:     46 / 196 loss=4.833, nll_loss=2.843, ppl=7.18, wps=20324.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=0.984, loss_scale=8, train_wall=291, gb_free=19.9, wall=52697
2022-03-07 03:24:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:26:29 | INFO | train_inner | epoch 086:    147 / 196 loss=4.829, nll_loss=2.838, ppl=7.15, wps=20866.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.968, loss_scale=8, train_wall=292, gb_free=19.9, wall=53011
2022-03-07 03:29:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:29:06 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.386 | nll_loss 9.017 | ppl 518.01 | wps 41438.9 | wpb 510.9 | bsz 1 | num_updates 16749 | best_loss 8.249
2022-03-07 03:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16749 updates
2022-03-07 03:29:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:29:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 86 @ 16749 updates, score 10.386) (writing took 3.339888165704906 seconds)
2022-03-07 03:29:09 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-07 03:29:09 | INFO | train | epoch 086 | loss 4.828 | nll_loss 2.838 | ppl 7.15 | wps 20676 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16749 | lr 0.000244346 | gnorm 0.97 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 53171
2022-03-07 03:29:09 | INFO | fairseq.trainer | begin training epoch 87
2022-03-07 03:29:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:31:48 | INFO | train_inner | epoch 087:     51 / 196 loss=4.824, nll_loss=2.833, ppl=7.13, wps=20506.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=0.984, loss_scale=16, train_wall=288, gb_free=19.9, wall=53329
2022-03-07 03:36:59 | INFO | train_inner | epoch 087:    151 / 196 loss=4.818, nll_loss=2.826, ppl=7.09, wps=21070, ups=0.32, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.984, loss_scale=16, train_wall=289, gb_free=19.9, wall=53640
2022-03-07 03:37:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:39:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:39:23 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.397 | nll_loss 9.034 | ppl 524.27 | wps 40675.2 | wpb 510.9 | bsz 1 | num_updates 16944 | best_loss 8.249
2022-03-07 03:39:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16944 updates
2022-03-07 03:39:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:39:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:39:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 87 @ 16944 updates, score 10.397) (writing took 3.422621848061681 seconds)
2022-03-07 03:39:27 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-07 03:39:27 | INFO | train | epoch 087 | loss 4.818 | nll_loss 2.826 | ppl 7.09 | wps 20663.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16944 | lr 0.000242936 | gnorm 0.985 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 53788
2022-03-07 03:39:27 | INFO | fairseq.trainer | begin training epoch 88
2022-03-07 03:39:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:42:21 | INFO | train_inner | epoch 088:     56 / 196 loss=4.809, nll_loss=2.815, ppl=7.04, wps=20298.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=0.982, loss_scale=8, train_wall=291, gb_free=19.9, wall=53962
2022-03-07 03:46:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:47:35 | INFO | train_inner | epoch 088:    157 / 196 loss=4.814, nll_loss=2.821, ppl=7.07, wps=20868.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.98, loss_scale=8, train_wall=292, gb_free=19.9, wall=54276
2022-03-07 03:49:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:49:41 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.408 | nll_loss 9.053 | ppl 531.19 | wps 41382.5 | wpb 510.9 | bsz 1 | num_updates 17139 | best_loss 8.249
2022-03-07 03:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17139 updates
2022-03-07 03:49:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 03:49:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 88 @ 17139 updates, score 10.408) (writing took 3.266399022191763 seconds)
2022-03-07 03:49:44 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-07 03:49:44 | INFO | train | epoch 088 | loss 4.808 | nll_loss 2.814 | ppl 7.03 | wps 20673.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 17139 | lr 0.00024155 | gnorm 0.986 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 54406
2022-03-07 03:49:44 | INFO | fairseq.trainer | begin training epoch 89
2022-03-07 03:49:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:52:54 | INFO | train_inner | epoch 089:     61 / 196 loss=4.792, nll_loss=2.795, ppl=6.94, wps=20487.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=0.982, loss_scale=8, train_wall=288, gb_free=19.9, wall=54595
2022-03-07 03:58:05 | INFO | train_inner | epoch 089:    161 / 196 loss=4.807, nll_loss=2.813, ppl=7.03, wps=21069.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.991, loss_scale=16, train_wall=289, gb_free=19.9, wall=54906
2022-03-07 03:59:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:59:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:59:58 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.434 | nll_loss 9.069 | ppl 537 | wps 41429.3 | wpb 510.9 | bsz 1 | num_updates 17334 | best_loss 8.249
2022-03-07 03:59:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17334 updates
2022-03-07 03:59:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:00:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:00:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 89 @ 17334 updates, score 10.434) (writing took 3.3915885342285037 seconds)
2022-03-07 04:00:02 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-07 04:00:02 | INFO | train | epoch 089 | loss 4.797 | nll_loss 2.801 | ppl 6.97 | wps 20661.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 17334 | lr 0.000240188 | gnorm 0.982 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 55023
2022-03-07 04:00:02 | INFO | fairseq.trainer | begin training epoch 90
2022-03-07 04:00:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:03:27 | INFO | train_inner | epoch 090:     66 / 196 loss=4.78, nll_loss=2.781, ppl=6.87, wps=20280.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=0.982, loss_scale=16, train_wall=291, gb_free=19.9, wall=55229
2022-03-07 04:06:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:08:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:08:44 | INFO | train_inner | epoch 090:    168 / 196 loss=4.807, nll_loss=2.813, ppl=7.03, wps=20671.3, ups=0.32, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.994, loss_scale=8, train_wall=294, gb_free=19.9, wall=55546
2022-03-07 04:10:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:10:16 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.438 | nll_loss 9.077 | ppl 539.97 | wps 41387.7 | wpb 510.9 | bsz 1 | num_updates 17528 | best_loss 8.249
2022-03-07 04:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17528 updates
2022-03-07 04:10:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:10:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:10:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 90 @ 17528 updates, score 10.438) (writing took 3.2599139669910073 seconds)
2022-03-07 04:10:19 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-07 04:10:19 | INFO | train | epoch 090 | loss 4.788 | nll_loss 2.79 | ppl 6.91 | wps 20565.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17528 | lr 0.000238855 | gnorm 0.987 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 55641
2022-03-07 04:10:19 | INFO | fairseq.trainer | begin training epoch 91
2022-03-07 04:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:14:03 | INFO | train_inner | epoch 091:     72 / 196 loss=4.761, nll_loss=2.759, ppl=6.77, wps=20474.2, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=0.983, loss_scale=8, train_wall=288, gb_free=19.9, wall=55865
2022-03-07 04:15:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:19:18 | INFO | train_inner | epoch 091:    173 / 196 loss=4.801, nll_loss=2.805, ppl=6.99, wps=20861.4, ups=0.32, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.979, loss_scale=8, train_wall=292, gb_free=19.9, wall=56179
2022-03-07 04:20:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:20:34 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.442 | nll_loss 9.078 | ppl 540.36 | wps 41361 | wpb 510.9 | bsz 1 | num_updates 17723 | best_loss 8.249
2022-03-07 04:20:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17723 updates
2022-03-07 04:20:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:20:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:20:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 91 @ 17723 updates, score 10.442) (writing took 3.340305281803012 seconds)
2022-03-07 04:20:37 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-07 04:20:37 | INFO | train | epoch 091 | loss 4.779 | nll_loss 2.78 | ppl 6.87 | wps 20653.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 17723 | lr 0.000237537 | gnorm 0.982 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 56259
2022-03-07 04:20:37 | INFO | fairseq.trainer | begin training epoch 92
2022-03-07 04:20:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:24:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:24:40 | INFO | train_inner | epoch 092:     78 / 196 loss=4.748, nll_loss=2.743, ppl=6.7, wps=20285.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=0.982, loss_scale=8, train_wall=291, gb_free=19.9, wall=56501
2022-03-07 04:29:51 | INFO | train_inner | epoch 092:    178 / 196 loss=4.794, nll_loss=2.798, ppl=6.95, wps=21073.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.997, loss_scale=8, train_wall=289, gb_free=19.9, wall=56812
2022-03-07 04:30:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:30:51 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.475 | nll_loss 9.12 | ppl 556.59 | wps 41523.4 | wpb 510.9 | bsz 1 | num_updates 17918 | best_loss 8.249
2022-03-07 04:30:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17918 updates
2022-03-07 04:30:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:30:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:30:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 92 @ 17918 updates, score 10.475) (writing took 3.3354117358103395 seconds)
2022-03-07 04:30:55 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-07 04:30:55 | INFO | train | epoch 092 | loss 4.769 | nll_loss 2.768 | ppl 6.81 | wps 20661 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 17918 | lr 0.000236241 | gnorm 0.988 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 56876
2022-03-07 04:30:55 | INFO | fairseq.trainer | begin training epoch 93
2022-03-07 04:30:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:35:10 | INFO | train_inner | epoch 093:     82 / 196 loss=4.734, nll_loss=2.726, ppl=6.62, wps=20487.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=0.968, loss_scale=16, train_wall=288, gb_free=19.9, wall=57131
2022-03-07 04:37:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:40:24 | INFO | train_inner | epoch 093:    183 / 196 loss=4.791, nll_loss=2.794, ppl=6.94, wps=20845.6, ups=0.32, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.999, loss_scale=16, train_wall=292, gb_free=19.9, wall=57446
2022-03-07 04:41:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:41:09 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.495 | nll_loss 9.136 | ppl 562.66 | wps 41336.6 | wpb 510.9 | bsz 1 | num_updates 18113 | best_loss 8.249
2022-03-07 04:41:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18113 updates
2022-03-07 04:41:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:41:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:41:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 93 @ 18113 updates, score 10.495) (writing took 3.3537349365651608 seconds)
2022-03-07 04:41:13 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-07 04:41:13 | INFO | train | epoch 093 | loss 4.76 | nll_loss 2.757 | ppl 6.76 | wps 20654 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 18113 | lr 0.000234966 | gnorm 0.985 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 57494
2022-03-07 04:41:13 | INFO | fairseq.trainer | begin training epoch 94
2022-03-07 04:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:44:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:45:46 | INFO | train_inner | epoch 094:     88 / 196 loss=4.721, nll_loss=2.712, ppl=6.55, wps=20289.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=0.992, loss_scale=16, train_wall=291, gb_free=19.9, wall=57768
2022-03-07 04:50:57 | INFO | train_inner | epoch 094:    188 / 196 loss=4.786, nll_loss=2.788, ppl=6.91, wps=21073.1, ups=0.32, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.009, loss_scale=16, train_wall=289, gb_free=19.9, wall=58079
2022-03-07 04:51:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:51:27 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.514 | nll_loss 9.161 | ppl 572.35 | wps 41378.1 | wpb 510.9 | bsz 1 | num_updates 18307 | best_loss 8.249
2022-03-07 04:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18307 updates
2022-03-07 04:51:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:51:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 04:51:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 94 @ 18307 updates, score 10.514) (writing took 3.4664312256500125 seconds)
2022-03-07 04:51:30 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-07 04:51:30 | INFO | train | epoch 094 | loss 4.75 | nll_loss 2.746 | ppl 6.71 | wps 20555.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18307 | lr 0.000233718 | gnorm 1 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 58112
2022-03-07 04:51:30 | INFO | fairseq.trainer | begin training epoch 95
2022-03-07 04:51:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:56:20 | INFO | train_inner | epoch 095:     93 / 196 loss=4.713, nll_loss=2.702, ppl=6.51, wps=20282.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=0.997, loss_scale=16, train_wall=291, gb_free=19.9, wall=58401
2022-03-07 04:57:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:01:34 | INFO | train_inner | epoch 095:    194 / 196 loss=4.773, nll_loss=2.773, ppl=6.84, wps=20845.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.999, loss_scale=16, train_wall=292, gb_free=19.9, wall=58716
2022-03-07 05:01:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:01:45 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.535 | nll_loss 9.185 | ppl 581.94 | wps 41130.8 | wpb 510.9 | bsz 1 | num_updates 18502 | best_loss 8.249
2022-03-07 05:01:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18502 updates
2022-03-07 05:01:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:01:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 95 @ 18502 updates, score 10.535) (writing took 3.3335396237671375 seconds)
2022-03-07 05:01:48 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-07 05:01:48 | INFO | train | epoch 095 | loss 4.741 | nll_loss 2.735 | ppl 6.66 | wps 20653.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 18502 | lr 0.000232483 | gnorm 0.997 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 58730
2022-03-07 05:01:48 | INFO | fairseq.trainer | begin training epoch 96
2022-03-07 05:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:04:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:06:57 | INFO | train_inner | epoch 096:     99 / 196 loss=4.699, nll_loss=2.685, ppl=6.43, wps=20266.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=18600, lr=0.000231869, gnorm=1.002, loss_scale=16, train_wall=291, gb_free=19.9, wall=59038
2022-03-07 05:11:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:11:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:12:03 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.503 | nll_loss 9.145 | ppl 566.11 | wps 41389.1 | wpb 510.9 | bsz 1 | num_updates 18696 | best_loss 8.249
2022-03-07 05:12:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18696 updates
2022-03-07 05:12:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:12:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:12:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 96 @ 18696 updates, score 10.503) (writing took 3.32477424480021 seconds)
2022-03-07 05:12:06 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-07 05:12:06 | INFO | train | epoch 096 | loss 4.734 | nll_loss 2.726 | ppl 6.62 | wps 20541 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18696 | lr 0.000231273 | gnorm 1.003 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 59348
2022-03-07 05:12:06 | INFO | fairseq.trainer | begin training epoch 97
2022-03-07 05:12:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:12:19 | INFO | train_inner | epoch 097:      4 / 196 loss=4.765, nll_loss=2.764, ppl=6.79, wps=20282.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.006, loss_scale=16, train_wall=291, gb_free=19.9, wall=59360
2022-03-07 05:17:30 | INFO | train_inner | epoch 097:    104 / 196 loss=4.693, nll_loss=2.678, ppl=6.4, wps=21064.1, ups=0.32, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.01, loss_scale=16, train_wall=289, gb_free=19.9, wall=59672
2022-03-07 05:18:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:22:21 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.56 | nll_loss 9.207 | ppl 591.06 | wps 41343.4 | wpb 510.9 | bsz 1 | num_updates 18891 | best_loss 8.249
2022-03-07 05:22:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18891 updates
2022-03-07 05:22:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:22:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:22:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 97 @ 18891 updates, score 10.56) (writing took 3.329233245924115 seconds)
2022-03-07 05:22:24 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-07 05:22:24 | INFO | train | epoch 097 | loss 4.725 | nll_loss 2.717 | ppl 6.57 | wps 20664.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 18891 | lr 0.000230077 | gnorm 1.014 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 59966
2022-03-07 05:22:24 | INFO | fairseq.trainer | begin training epoch 98
2022-03-07 05:22:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:22:52 | INFO | train_inner | epoch 098:      9 / 196 loss=4.752, nll_loss=2.748, ppl=6.72, wps=20296.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.016, loss_scale=16, train_wall=291, gb_free=19.9, wall=59994
2022-03-07 05:25:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:28:06 | INFO | train_inner | epoch 098:    110 / 196 loss=4.681, nll_loss=2.665, ppl=6.34, wps=20869.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.99, loss_scale=16, train_wall=291, gb_free=19.9, wall=60308
2022-03-07 05:31:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:32:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:32:38 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.569 | nll_loss 9.223 | ppl 597.75 | wps 41249 | wpb 510.9 | bsz 1 | num_updates 19085 | best_loss 8.249
2022-03-07 05:32:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19085 updates
2022-03-07 05:32:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:32:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 98 @ 19085 updates, score 10.569) (writing took 3.3945886185392737 seconds)
2022-03-07 05:32:41 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-07 05:32:41 | INFO | train | epoch 098 | loss 4.715 | nll_loss 2.704 | ppl 6.52 | wps 20566.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19085 | lr 0.000228904 | gnorm 1.01 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 60583
2022-03-07 05:32:41 | INFO | fairseq.trainer | begin training epoch 99
2022-03-07 05:32:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:33:28 | INFO | train_inner | epoch 099:     15 / 196 loss=4.746, nll_loss=2.741, ppl=6.69, wps=20301.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.026, loss_scale=16, train_wall=291, gb_free=19.9, wall=60630
2022-03-07 05:38:39 | INFO | train_inner | epoch 099:    115 / 196 loss=4.678, nll_loss=2.66, ppl=6.32, wps=21073.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.994, loss_scale=16, train_wall=289, gb_free=19.9, wall=60941
2022-03-07 05:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:42:55 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.586 | nll_loss 9.245 | ppl 606.81 | wps 41584 | wpb 510.9 | bsz 1 | num_updates 19280 | best_loss 8.249
2022-03-07 05:42:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19280 updates
2022-03-07 05:42:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:42:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 99 @ 19280 updates, score 10.586) (writing took 3.425032631494105 seconds)
2022-03-07 05:42:59 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-07 05:42:59 | INFO | train | epoch 099 | loss 4.708 | nll_loss 2.696 | ppl 6.48 | wps 20668.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 19280 | lr 0.000227744 | gnorm 1.001 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 61200
2022-03-07 05:42:59 | INFO | fairseq.trainer | begin training epoch 100
2022-03-07 05:42:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:44:01 | INFO | train_inner | epoch 100:     20 / 196 loss=4.733, nll_loss=2.726, ppl=6.61, wps=20296.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.013, loss_scale=16, train_wall=291, gb_free=19.9, wall=61263
2022-03-07 05:45:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:49:15 | INFO | train_inner | epoch 100:    121 / 196 loss=4.678, nll_loss=2.661, ppl=6.32, wps=20873, ups=0.32, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.994, loss_scale=16, train_wall=292, gb_free=19.9, wall=61577
2022-03-07 05:52:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:53:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:53:13 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.542 | nll_loss 9.189 | ppl 583.53 | wps 41614.6 | wpb 510.9 | bsz 1 | num_updates 19474 | best_loss 8.249
2022-03-07 05:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19474 updates
2022-03-07 05:53:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:53:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 05:53:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 100 @ 19474 updates, score 10.542) (writing took 3.471331249922514 seconds)
2022-03-07 05:53:16 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-07 05:53:16 | INFO | train | epoch 100 | loss 4.7 | nll_loss 2.686 | ppl 6.44 | wps 20563.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19474 | lr 0.000226607 | gnorm 1.006 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 61818
2022-03-07 05:53:16 | INFO | fairseq.trainer | begin training epoch 101
2022-03-07 05:53:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:54:37 | INFO | train_inner | epoch 101:     26 / 196 loss=4.716, nll_loss=2.706, ppl=6.52, wps=20294.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.019, loss_scale=16, train_wall=291, gb_free=19.9, wall=61899
2022-03-07 05:59:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:59:51 | INFO | train_inner | epoch 101:    127 / 196 loss=4.676, nll_loss=2.659, ppl=6.32, wps=20869.3, ups=0.32, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.998, loss_scale=16, train_wall=291, gb_free=19.9, wall=62213
2022-03-07 06:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:03:30 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.558 | nll_loss 9.211 | ppl 592.49 | wps 41664.7 | wpb 510.9 | bsz 1 | num_updates 19669 | best_loss 8.249
2022-03-07 06:03:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19669 updates
2022-03-07 06:03:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:03:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:03:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 101 @ 19669 updates, score 10.558) (writing took 3.4519622307270765 seconds)
2022-03-07 06:03:34 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-07 06:03:34 | INFO | train | epoch 101 | loss 4.692 | nll_loss 2.678 | ppl 6.4 | wps 20666.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 19669 | lr 0.00022548 | gnorm 1.011 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 62435
2022-03-07 06:03:34 | INFO | fairseq.trainer | begin training epoch 102
2022-03-07 06:03:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:05:10 | INFO | train_inner | epoch 102:     31 / 196 loss=4.699, nll_loss=2.686, ppl=6.43, wps=20476.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.021, loss_scale=16, train_wall=288, gb_free=19.9, wall=62532
2022-03-07 06:06:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:10:24 | INFO | train_inner | epoch 102:    132 / 196 loss=4.671, nll_loss=2.653, ppl=6.29, wps=20875.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.008, loss_scale=16, train_wall=291, gb_free=19.9, wall=62846
2022-03-07 06:12:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:13:48 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.602 | nll_loss 9.256 | ppl 611.38 | wps 41555.6 | wpb 510.9 | bsz 1 | num_updates 19863 | best_loss 8.249
2022-03-07 06:13:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19863 updates
2022-03-07 06:13:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:13:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:13:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 102 @ 19863 updates, score 10.602) (writing took 3.3643292523920536 seconds)
2022-03-07 06:13:51 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-07 06:13:51 | INFO | train | epoch 102 | loss 4.684 | nll_loss 2.668 | ppl 6.36 | wps 20566.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19863 | lr 0.000224377 | gnorm 1.011 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 63053
2022-03-07 06:13:51 | INFO | fairseq.trainer | begin training epoch 103
2022-03-07 06:13:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:15:46 | INFO | train_inner | epoch 103:     37 / 196 loss=4.696, nll_loss=2.682, ppl=6.42, wps=20301.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.015, loss_scale=16, train_wall=291, gb_free=19.9, wall=63168
2022-03-07 06:19:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:21:00 | INFO | train_inner | epoch 103:    138 / 196 loss=4.669, nll_loss=2.65, ppl=6.28, wps=20870.2, ups=0.32, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.027, loss_scale=16, train_wall=292, gb_free=19.9, wall=63482
2022-03-07 06:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:24:05 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.618 | nll_loss 9.274 | ppl 619.15 | wps 41654.2 | wpb 510.9 | bsz 1 | num_updates 20058 | best_loss 8.249
2022-03-07 06:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20058 updates
2022-03-07 06:24:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:24:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:24:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 103 @ 20058 updates, score 10.618) (writing took 3.4054120322689414 seconds)
2022-03-07 06:24:08 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-07 06:24:08 | INFO | train | epoch 103 | loss 4.676 | nll_loss 2.659 | ppl 6.31 | wps 20672.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20058 | lr 0.000223283 | gnorm 1.022 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 63670
2022-03-07 06:24:08 | INFO | fairseq.trainer | begin training epoch 104
2022-03-07 06:24:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:26:19 | INFO | train_inner | epoch 104:     42 / 196 loss=4.672, nll_loss=2.654, ppl=6.29, wps=20504, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.009, loss_scale=16, train_wall=288, gb_free=19.9, wall=63801
2022-03-07 06:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:31:33 | INFO | train_inner | epoch 104:    143 / 196 loss=4.663, nll_loss=2.643, ppl=6.25, wps=20869.6, ups=0.32, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.012, loss_scale=16, train_wall=292, gb_free=19.9, wall=64115
2022-03-07 06:33:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:34:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:34:22 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.565 | nll_loss 9.207 | ppl 591 | wps 41261.8 | wpb 510.9 | bsz 1 | num_updates 20252 | best_loss 8.249
2022-03-07 06:34:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20252 updates
2022-03-07 06:34:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:34:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:34:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 104 @ 20252 updates, score 10.565) (writing took 3.3596765333786607 seconds)
2022-03-07 06:34:26 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-07 06:34:26 | INFO | train | epoch 104 | loss 4.668 | nll_loss 2.649 | ppl 6.27 | wps 20570.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20252 | lr 0.000222211 | gnorm 1.01 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 64287
2022-03-07 06:34:26 | INFO | fairseq.trainer | begin training epoch 105
2022-03-07 06:34:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:36:55 | INFO | train_inner | epoch 105:     48 / 196 loss=4.668, nll_loss=2.649, ppl=6.27, wps=20306.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.024, loss_scale=16, train_wall=291, gb_free=19.9, wall=64437
2022-03-07 06:40:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:42:09 | INFO | train_inner | epoch 105:    149 / 196 loss=4.67, nll_loss=2.652, ppl=6.28, wps=20882.6, ups=0.32, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.028, loss_scale=16, train_wall=291, gb_free=19.9, wall=64750
2022-03-07 06:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:44:39 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.606 | nll_loss 9.263 | ppl 614.24 | wps 41520.3 | wpb 510.9 | bsz 1 | num_updates 20447 | best_loss 8.249
2022-03-07 06:44:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20447 updates
2022-03-07 06:44:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 105 @ 20447 updates, score 10.606) (writing took 3.403054663911462 seconds)
2022-03-07 06:44:43 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-07 06:44:43 | INFO | train | epoch 105 | loss 4.661 | nll_loss 2.641 | ppl 6.24 | wps 20679.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20447 | lr 0.000221149 | gnorm 1.026 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 64904
2022-03-07 06:44:43 | INFO | fairseq.trainer | begin training epoch 106
2022-03-07 06:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:46:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:47:31 | INFO | train_inner | epoch 106:     54 / 196 loss=4.649, nll_loss=2.627, ppl=6.18, wps=20303.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.026, loss_scale=16, train_wall=291, gb_free=19.9, wall=65072
2022-03-07 06:52:42 | INFO | train_inner | epoch 106:    154 / 196 loss=4.661, nll_loss=2.641, ppl=6.24, wps=21083.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.024, loss_scale=16, train_wall=289, gb_free=19.9, wall=65383
2022-03-07 06:53:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:54:57 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.632 | nll_loss 9.295 | ppl 628.02 | wps 41575.5 | wpb 510.9 | bsz 1 | num_updates 20641 | best_loss 8.249
2022-03-07 06:54:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20641 updates
2022-03-07 06:54:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:55:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 06:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 106 @ 20641 updates, score 10.632) (writing took 3.3670770237222314 seconds)
2022-03-07 06:55:00 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-07 06:55:00 | INFO | train | epoch 106 | loss 4.654 | nll_loss 2.633 | ppl 6.2 | wps 20569.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20641 | lr 0.000220107 | gnorm 1.026 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 65522
2022-03-07 06:55:00 | INFO | fairseq.trainer | begin training epoch 107
2022-03-07 06:55:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:58:04 | INFO | train_inner | epoch 107:     59 / 196 loss=4.64, nll_loss=2.617, ppl=6.13, wps=20287.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.027, loss_scale=16, train_wall=291, gb_free=19.9, wall=65705
2022-03-07 07:00:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:03:18 | INFO | train_inner | epoch 107:    160 / 196 loss=4.657, nll_loss=2.637, ppl=6.22, wps=20858.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.023, loss_scale=16, train_wall=292, gb_free=19.9, wall=66020
2022-03-07 07:05:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:05:14 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.647 | nll_loss 9.311 | ppl 635.12 | wps 41465 | wpb 510.9 | bsz 1 | num_updates 20836 | best_loss 8.249
2022-03-07 07:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20836 updates
2022-03-07 07:05:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:05:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:05:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 107 @ 20836 updates, score 10.647) (writing took 3.29033877979964 seconds)
2022-03-07 07:05:18 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-07 07:05:18 | INFO | train | epoch 107 | loss 4.647 | nll_loss 2.625 | ppl 6.17 | wps 20664.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20836 | lr 0.000219075 | gnorm 1.024 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 66139
2022-03-07 07:05:18 | INFO | fairseq.trainer | begin training epoch 108
2022-03-07 07:05:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:07:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:07:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:08:43 | INFO | train_inner | epoch 108:     66 / 196 loss=4.629, nll_loss=2.604, ppl=6.08, wps=20113.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.024, loss_scale=8, train_wall=294, gb_free=19.9, wall=66345
2022-03-07 07:13:54 | INFO | train_inner | epoch 108:    166 / 196 loss=4.658, nll_loss=2.637, ppl=6.22, wps=21090.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.03, loss_scale=8, train_wall=289, gb_free=19.9, wall=66655
2022-03-07 07:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:15:31 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.615 | nll_loss 9.276 | ppl 619.98 | wps 41757.1 | wpb 510.9 | bsz 1 | num_updates 21030 | best_loss 8.249
2022-03-07 07:15:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21030 updates
2022-03-07 07:15:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:15:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 108 @ 21030 updates, score 10.615) (writing took 3.350906241685152 seconds)
2022-03-07 07:15:35 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-07 07:15:35 | INFO | train | epoch 108 | loss 4.64 | nll_loss 2.617 | ppl 6.13 | wps 20576 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21030 | lr 0.000218062 | gnorm 1.032 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 66756
2022-03-07 07:15:35 | INFO | fairseq.trainer | begin training epoch 109
2022-03-07 07:15:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:19:13 | INFO | train_inner | epoch 109:     70 / 196 loss=4.614, nll_loss=2.586, ppl=6, wps=20496.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.042, loss_scale=16, train_wall=288, gb_free=19.9, wall=66974
2022-03-07 07:19:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:24:26 | INFO | train_inner | epoch 109:    171 / 196 loss=4.656, nll_loss=2.636, ppl=6.21, wps=20877.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.031, loss_scale=8, train_wall=291, gb_free=19.9, wall=67288
2022-03-07 07:25:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:25:49 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.66 | nll_loss 9.328 | ppl 642.51 | wps 41511.6 | wpb 510.9 | bsz 1 | num_updates 21225 | best_loss 8.249
2022-03-07 07:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21225 updates
2022-03-07 07:25:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:25:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:25:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 109 @ 21225 updates, score 10.66) (writing took 3.351897125132382 seconds)
2022-03-07 07:25:52 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-07 07:25:52 | INFO | train | epoch 109 | loss 4.634 | nll_loss 2.609 | ppl 6.1 | wps 20675.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 21225 | lr 0.000217058 | gnorm 1.038 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 67374
2022-03-07 07:25:52 | INFO | fairseq.trainer | begin training epoch 110
2022-03-07 07:25:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:29:45 | INFO | train_inner | epoch 110:     75 / 196 loss=4.608, nll_loss=2.579, ppl=5.97, wps=20498.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.034, loss_scale=16, train_wall=288, gb_free=19.9, wall=67607
2022-03-07 07:32:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:33:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:35:03 | INFO | train_inner | epoch 110:    177 / 196 loss=4.654, nll_loss=2.633, ppl=6.2, wps=20657.4, ups=0.32, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.048, loss_scale=8, train_wall=295, gb_free=19.9, wall=67924
2022-03-07 07:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:36:06 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.614 | nll_loss 9.273 | ppl 618.84 | wps 41743.5 | wpb 510.9 | bsz 1 | num_updates 21419 | best_loss 8.249
2022-03-07 07:36:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21419 updates
2022-03-07 07:36:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:36:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 110 @ 21419 updates, score 10.614) (writing took 3.362952562980354 seconds)
2022-03-07 07:36:09 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-07 07:36:09 | INFO | train | epoch 110 | loss 4.627 | nll_loss 2.601 | ppl 6.07 | wps 20562.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21419 | lr 0.000216073 | gnorm 1.036 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 67991
2022-03-07 07:36:10 | INFO | fairseq.trainer | begin training epoch 111
2022-03-07 07:36:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:40:21 | INFO | train_inner | epoch 111:     81 / 196 loss=4.595, nll_loss=2.564, ppl=5.91, wps=20502.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.032, loss_scale=16, train_wall=288, gb_free=19.9, wall=68243
2022-03-07 07:45:32 | INFO | train_inner | epoch 111:    181 / 196 loss=4.65, nll_loss=2.629, ppl=6.18, wps=21090.9, ups=0.32, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.041, loss_scale=16, train_wall=289, gb_free=19.9, wall=68554
2022-03-07 07:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:46:23 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.672 | nll_loss 9.331 | ppl 643.91 | wps 41461.1 | wpb 510.9 | bsz 1 | num_updates 21615 | best_loss 8.249
2022-03-07 07:46:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21615 updates
2022-03-07 07:46:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:46:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 111 @ 21615 updates, score 10.672) (writing took 3.377324715256691 seconds)
2022-03-07 07:46:27 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-07 07:46:27 | INFO | train | epoch 111 | loss 4.621 | nll_loss 2.594 | ppl 6.04 | wps 20783.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 21615 | lr 0.000215091 | gnorm 1.04 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 68608
2022-03-07 07:46:27 | INFO | fairseq.trainer | begin training epoch 112
2022-03-07 07:46:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:46:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:50:54 | INFO | train_inner | epoch 112:     86 / 196 loss=4.588, nll_loss=2.555, ppl=5.88, wps=20290.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.015, loss_scale=16, train_wall=291, gb_free=19.9, wall=68876
2022-03-07 07:53:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:54:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:56:11 | INFO | train_inner | epoch 112:    188 / 196 loss=4.642, nll_loss=2.619, ppl=6.14, wps=20672.7, ups=0.32, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.03, loss_scale=8, train_wall=294, gb_free=19.9, wall=69193
2022-03-07 07:56:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:56:41 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.711 | nll_loss 9.384 | ppl 668.18 | wps 41491.4 | wpb 510.9 | bsz 1 | num_updates 21808 | best_loss 8.249
2022-03-07 07:56:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21808 updates
2022-03-07 07:56:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:56:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 07:56:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 112 @ 21808 updates, score 10.711) (writing took 3.331688517704606 seconds)
2022-03-07 07:56:44 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-07 07:56:44 | INFO | train | epoch 112 | loss 4.612 | nll_loss 2.584 | ppl 6 | wps 20462.3 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 21808 | lr 0.000214137 | gnorm 1.021 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 69226
2022-03-07 07:56:44 | INFO | fairseq.trainer | begin training epoch 113
2022-03-07 07:56:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:01:30 | INFO | train_inner | epoch 113:     92 / 196 loss=4.575, nll_loss=2.54, ppl=5.82, wps=20503.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.032, loss_scale=16, train_wall=288, gb_free=19.9, wall=69512
2022-03-07 08:03:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:06:44 | INFO | train_inner | epoch 113:    193 / 196 loss=4.646, nll_loss=2.624, ppl=6.16, wps=20880.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.05, loss_scale=8, train_wall=291, gb_free=19.9, wall=69826
2022-03-07 08:06:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:06:58 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.676 | nll_loss 9.344 | ppl 649.99 | wps 41465.5 | wpb 510.9 | bsz 1 | num_updates 22003 | best_loss 8.249
2022-03-07 08:06:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22003 updates
2022-03-07 08:06:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:07:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 113 @ 22003 updates, score 10.676) (writing took 3.3441478684544563 seconds)
2022-03-07 08:07:01 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-07 08:07:01 | INFO | train | epoch 113 | loss 4.608 | nll_loss 2.579 | ppl 5.98 | wps 20680 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22003 | lr 0.000213186 | gnorm 1.04 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 69843
2022-03-07 08:07:01 | INFO | fairseq.trainer | begin training epoch 114
2022-03-07 08:07:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:12:03 | INFO | train_inner | epoch 114:     97 / 196 loss=4.559, nll_loss=2.521, ppl=5.74, wps=20506.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.035, loss_scale=16, train_wall=288, gb_free=19.9, wall=70144
2022-03-07 08:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:17:15 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.716 | nll_loss 9.385 | ppl 668.55 | wps 41601.8 | wpb 510.9 | bsz 1 | num_updates 22198 | best_loss 8.249
2022-03-07 08:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22198 updates
2022-03-07 08:17:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:17:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:17:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 114 @ 22198 updates, score 10.716) (writing took 3.3377195224165916 seconds)
2022-03-07 08:17:18 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-07 08:17:18 | INFO | train | epoch 114 | loss 4.602 | nll_loss 2.571 | ppl 5.94 | wps 20677.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22198 | lr 0.000212248 | gnorm 1.041 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 70460
2022-03-07 08:17:18 | INFO | fairseq.trainer | begin training epoch 115
2022-03-07 08:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:17:25 | INFO | train_inner | epoch 115:      2 / 196 loss=4.645, nll_loss=2.623, ppl=6.16, wps=20301.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.048, loss_scale=16, train_wall=291, gb_free=19.9, wall=70466
2022-03-07 08:22:35 | INFO | train_inner | epoch 115:    102 / 196 loss=4.564, nll_loss=2.527, ppl=5.76, wps=21087.3, ups=0.32, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.027, loss_scale=16, train_wall=289, gb_free=19.9, wall=70777
2022-03-07 08:23:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:27:32 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.733 | nll_loss 9.412 | ppl 681.42 | wps 41411.8 | wpb 510.9 | bsz 1 | num_updates 22393 | best_loss 8.249
2022-03-07 08:27:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22393 updates
2022-03-07 08:27:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 115 @ 22393 updates, score 10.733) (writing took 3.3551672296598554 seconds)
2022-03-07 08:27:36 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-07 08:27:36 | INFO | train | epoch 115 | loss 4.595 | nll_loss 2.563 | ppl 5.91 | wps 20674.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22393 | lr 0.000211322 | gnorm 1.039 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 71077
2022-03-07 08:27:36 | INFO | fairseq.trainer | begin training epoch 116
2022-03-07 08:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:27:57 | INFO | train_inner | epoch 116:      7 / 196 loss=4.619, nll_loss=2.592, ppl=6.03, wps=20296.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.053, loss_scale=16, train_wall=291, gb_free=19.9, wall=71099
2022-03-07 08:30:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:33:11 | INFO | train_inner | epoch 116:    108 / 196 loss=4.557, nll_loss=2.519, ppl=5.73, wps=20877.6, ups=0.32, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.029, loss_scale=16, train_wall=291, gb_free=19.9, wall=71413
2022-03-07 08:33:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:37:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:37:49 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.747 | nll_loss 9.425 | ppl 687.25 | wps 41359.9 | wpb 510.9 | bsz 1 | num_updates 22587 | best_loss 8.249
2022-03-07 08:37:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22587 updates
2022-03-07 08:37:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:37:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:37:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 116 @ 22587 updates, score 10.747) (writing took 3.3158329715952277 seconds)
2022-03-07 08:37:53 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-07 08:37:53 | INFO | train | epoch 116 | loss 4.588 | nll_loss 2.556 | ppl 5.88 | wps 20571.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22587 | lr 0.000210412 | gnorm 1.047 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 71694
2022-03-07 08:37:53 | INFO | fairseq.trainer | begin training epoch 117
2022-03-07 08:37:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:38:33 | INFO | train_inner | epoch 117:     13 / 196 loss=4.614, nll_loss=2.587, ppl=6.01, wps=20300, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.061, loss_scale=8, train_wall=291, gb_free=19.9, wall=71735
2022-03-07 08:43:44 | INFO | train_inner | epoch 117:    113 / 196 loss=4.56, nll_loss=2.523, ppl=5.75, wps=21086.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.032, loss_scale=16, train_wall=289, gb_free=19.9, wall=72046
2022-03-07 08:47:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:48:07 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.715 | nll_loss 9.391 | ppl 671.32 | wps 41646.9 | wpb 510.9 | bsz 1 | num_updates 22782 | best_loss 8.249
2022-03-07 08:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22782 updates
2022-03-07 08:48:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 117 @ 22782 updates, score 10.715) (writing took 3.294208223000169 seconds)
2022-03-07 08:48:10 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-07 08:48:10 | INFO | train | epoch 117 | loss 4.583 | nll_loss 2.549 | ppl 5.85 | wps 20681.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22782 | lr 0.00020951 | gnorm 1.048 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 72311
2022-03-07 08:48:10 | INFO | fairseq.trainer | begin training epoch 118
2022-03-07 08:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:49:06 | INFO | train_inner | epoch 118:     18 / 196 loss=4.601, nll_loss=2.571, ppl=5.94, wps=20308.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.061, loss_scale=16, train_wall=291, gb_free=19.9, wall=72368
2022-03-07 08:54:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:54:20 | INFO | train_inner | epoch 118:    119 / 196 loss=4.554, nll_loss=2.516, ppl=5.72, wps=20874.6, ups=0.32, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.035, loss_scale=16, train_wall=291, gb_free=19.9, wall=72682
2022-03-07 08:58:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:58:24 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.759 | nll_loss 9.437 | ppl 693.23 | wps 41317.7 | wpb 510.9 | bsz 1 | num_updates 22977 | best_loss 8.249
2022-03-07 08:58:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22977 updates
2022-03-07 08:58:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:58:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 08:58:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 118 @ 22977 updates, score 10.759) (writing took 3.2942485939711332 seconds)
2022-03-07 08:58:27 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-07 08:58:27 | INFO | train | epoch 118 | loss 4.577 | nll_loss 2.542 | ppl 5.83 | wps 20677 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22977 | lr 0.000208619 | gnorm 1.044 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 72929
2022-03-07 08:58:27 | INFO | fairseq.trainer | begin training epoch 119
2022-03-07 08:58:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:59:39 | INFO | train_inner | epoch 119:     23 / 196 loss=4.596, nll_loss=2.565, ppl=5.92, wps=20501.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.058, loss_scale=16, train_wall=288, gb_free=19.9, wall=73000
2022-03-07 09:01:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:04:53 | INFO | train_inner | epoch 119:    124 / 196 loss=4.554, nll_loss=2.516, ppl=5.72, wps=20870.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.046, loss_scale=16, train_wall=291, gb_free=19.9, wall=73314
2022-03-07 09:07:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:08:41 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.759 | nll_loss 9.437 | ppl 693.23 | wps 41834.5 | wpb 510.9 | bsz 1 | num_updates 23171 | best_loss 8.249
2022-03-07 09:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23171 updates
2022-03-07 09:08:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:08:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 119 @ 23171 updates, score 10.759) (writing took 3.332045049406588 seconds)
2022-03-07 09:08:44 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-07 09:08:44 | INFO | train | epoch 119 | loss 4.57 | nll_loss 2.534 | ppl 5.79 | wps 20567.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23171 | lr 0.000207744 | gnorm 1.047 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 73546
2022-03-07 09:08:44 | INFO | fairseq.trainer | begin training epoch 120
2022-03-07 09:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:10:15 | INFO | train_inner | epoch 120:     29 / 196 loss=4.581, nll_loss=2.548, ppl=5.85, wps=20309.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.052, loss_scale=16, train_wall=291, gb_free=19.9, wall=73636
2022-03-07 09:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:15:29 | INFO | train_inner | epoch 120:    130 / 196 loss=4.553, nll_loss=2.514, ppl=5.71, wps=20869.8, ups=0.32, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.042, loss_scale=16, train_wall=292, gb_free=19.9, wall=73950
2022-03-07 09:18:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:18:58 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.774 | nll_loss 9.459 | ppl 703.72 | wps 41737.5 | wpb 510.9 | bsz 1 | num_updates 23366 | best_loss 8.249
2022-03-07 09:18:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23366 updates
2022-03-07 09:18:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:19:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:19:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 120 @ 23366 updates, score 10.774) (writing took 3.378083831630647 seconds)
2022-03-07 09:19:02 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-07 09:19:02 | INFO | train | epoch 120 | loss 4.565 | nll_loss 2.529 | ppl 5.77 | wps 20672.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 23366 | lr 0.000206875 | gnorm 1.043 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 74163
2022-03-07 09:19:02 | INFO | fairseq.trainer | begin training epoch 121
2022-03-07 09:19:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:20:48 | INFO | train_inner | epoch 121:     34 / 196 loss=4.57, nll_loss=2.535, ppl=5.8, wps=20492.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.035, loss_scale=16, train_wall=288, gb_free=19.9, wall=74269
2022-03-07 09:21:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:26:02 | INFO | train_inner | epoch 121:    135 / 196 loss=4.55, nll_loss=2.511, ppl=5.7, wps=20856.4, ups=0.32, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.05, loss_scale=16, train_wall=292, gb_free=19.9, wall=74583
2022-03-07 09:28:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:29:16 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.75 | nll_loss 9.429 | ppl 689.27 | wps 41467.8 | wpb 510.9 | bsz 1 | num_updates 23560 | best_loss 8.249
2022-03-07 09:29:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23560 updates
2022-03-07 09:29:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:29:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:29:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 121 @ 23560 updates, score 10.75) (writing took 3.3368102312088013 seconds)
2022-03-07 09:29:19 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-07 09:29:19 | INFO | train | epoch 121 | loss 4.559 | nll_loss 2.522 | ppl 5.74 | wps 20558.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23560 | lr 0.000206021 | gnorm 1.05 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 74781
2022-03-07 09:29:19 | INFO | fairseq.trainer | begin training epoch 122
2022-03-07 09:29:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:31:24 | INFO | train_inner | epoch 122:     40 / 196 loss=4.564, nll_loss=2.527, ppl=5.76, wps=20297.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.057, loss_scale=16, train_wall=291, gb_free=19.9, wall=74905
2022-03-07 09:34:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:36:38 | INFO | train_inner | epoch 122:    141 / 196 loss=4.549, nll_loss=2.509, ppl=5.69, wps=20877.8, ups=0.32, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.058, loss_scale=16, train_wall=291, gb_free=19.9, wall=75219
2022-03-07 09:36:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 09:39:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:39:33 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.787 | nll_loss 9.472 | ppl 710.02 | wps 41699.2 | wpb 510.9 | bsz 1 | num_updates 23754 | best_loss 8.249
2022-03-07 09:39:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23754 updates
2022-03-07 09:39:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:39:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:39:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 122 @ 23754 updates, score 10.787) (writing took 3.3043022407218814 seconds)
2022-03-07 09:39:36 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-07 09:39:36 | INFO | train | epoch 122 | loss 4.553 | nll_loss 2.515 | ppl 5.71 | wps 20574.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23754 | lr 0.000205178 | gnorm 1.06 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 75398
2022-03-07 09:39:37 | INFO | fairseq.trainer | begin training epoch 123
2022-03-07 09:39:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:42:00 | INFO | train_inner | epoch 123:     46 / 196 loss=4.557, nll_loss=2.52, ppl=5.73, wps=20308.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.068, loss_scale=8, train_wall=291, gb_free=19.9, wall=75541
2022-03-07 09:47:11 | INFO | train_inner | epoch 123:    146 / 196 loss=4.546, nll_loss=2.507, ppl=5.68, wps=21076.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.054, loss_scale=16, train_wall=289, gb_free=19.9, wall=75852
2022-03-07 09:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:49:51 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.795 | nll_loss 9.474 | ppl 711.18 | wps 41408.4 | wpb 510.9 | bsz 1 | num_updates 23950 | best_loss 8.249
2022-03-07 09:49:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23950 updates
2022-03-07 09:49:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:49:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 09:49:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 123 @ 23950 updates, score 10.795) (writing took 3.3034078860655427 seconds)
2022-03-07 09:49:54 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-07 09:49:54 | INFO | train | epoch 123 | loss 4.549 | nll_loss 2.51 | ppl 5.7 | wps 20779.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 23950 | lr 0.000204337 | gnorm 1.06 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 76015
2022-03-07 09:49:54 | INFO | fairseq.trainer | begin training epoch 124
2022-03-07 09:49:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:50:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 09:52:32 | INFO | train_inner | epoch 124:     51 / 196 loss=4.545, nll_loss=2.505, ppl=5.68, wps=20306.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.056, loss_scale=8, train_wall=291, gb_free=19.9, wall=76174
2022-03-07 09:57:43 | INFO | train_inner | epoch 124:    151 / 196 loss=4.548, nll_loss=2.509, ppl=5.69, wps=21075.5, ups=0.32, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.065, loss_scale=16, train_wall=289, gb_free=19.9, wall=76485
2022-03-07 10:00:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:00:08 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.758 | nll_loss 9.431 | ppl 690.18 | wps 41458.4 | wpb 510.9 | bsz 1 | num_updates 24145 | best_loss 8.249
2022-03-07 10:00:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24145 updates
2022-03-07 10:00:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:00:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 124 @ 24145 updates, score 10.758) (writing took 3.3382336208596826 seconds)
2022-03-07 10:00:11 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-07 10:00:11 | INFO | train | epoch 124 | loss 4.544 | nll_loss 2.504 | ppl 5.67 | wps 20673.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24145 | lr 0.00020351 | gnorm 1.063 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 76633
2022-03-07 10:00:11 | INFO | fairseq.trainer | begin training epoch 125
2022-03-07 10:00:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:02:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 10:03:05 | INFO | train_inner | epoch 125:     56 / 196 loss=4.539, nll_loss=2.498, ppl=5.65, wps=20305.7, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.079, loss_scale=8, train_wall=291, gb_free=19.9, wall=76807
2022-03-07 10:08:16 | INFO | train_inner | epoch 125:    156 / 196 loss=4.539, nll_loss=2.498, ppl=5.65, wps=21086.2, ups=0.32, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.063, loss_scale=8, train_wall=289, gb_free=19.9, wall=77118
2022-03-07 10:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:10:25 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.827 | nll_loss 9.506 | ppl 727.23 | wps 41485.3 | wpb 510.9 | bsz 1 | num_updates 24340 | best_loss 8.249
2022-03-07 10:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24340 updates
2022-03-07 10:10:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:10:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 125 @ 24340 updates, score 10.827) (writing took 3.332128311507404 seconds)
2022-03-07 10:10:28 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-07 10:10:28 | INFO | train | epoch 125 | loss 4.538 | nll_loss 2.497 | ppl 5.64 | wps 20679.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24340 | lr 0.000202693 | gnorm 1.067 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 77250
2022-03-07 10:10:28 | INFO | fairseq.trainer | begin training epoch 126
2022-03-07 10:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:13:35 | INFO | train_inner | epoch 126:     60 / 196 loss=4.528, nll_loss=2.485, ppl=5.6, wps=20503.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.052, loss_scale=16, train_wall=288, gb_free=19.9, wall=77437
2022-03-07 10:16:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:18:49 | INFO | train_inner | epoch 126:    161 / 196 loss=4.545, nll_loss=2.505, ppl=5.68, wps=20869.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.053, loss_scale=16, train_wall=292, gb_free=19.9, wall=77751
2022-03-07 10:20:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:20:42 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.794 | nll_loss 9.476 | ppl 712.12 | wps 41356.5 | wpb 510.9 | bsz 1 | num_updates 24535 | best_loss 8.249
2022-03-07 10:20:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24535 updates
2022-03-07 10:20:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:20:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:20:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 126 @ 24535 updates, score 10.794) (writing took 3.389986414462328 seconds)
2022-03-07 10:20:46 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-07 10:20:46 | INFO | train | epoch 126 | loss 4.533 | nll_loss 2.491 | ppl 5.62 | wps 20672.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24535 | lr 0.000201886 | gnorm 1.049 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 77867
2022-03-07 10:20:46 | INFO | fairseq.trainer | begin training epoch 127
2022-03-07 10:20:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:22:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:24:11 | INFO | train_inner | epoch 127:     66 / 196 loss=4.516, nll_loss=2.471, ppl=5.54, wps=20295.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.063, loss_scale=16, train_wall=291, gb_free=19.9, wall=78073
2022-03-07 10:29:22 | INFO | train_inner | epoch 127:    166 / 196 loss=4.539, nll_loss=2.499, ppl=5.65, wps=21083.1, ups=0.32, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.057, loss_scale=16, train_wall=289, gb_free=19.9, wall=78383
2022-03-07 10:29:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:30:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:31:00 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.755 | nll_loss 9.429 | ppl 689.48 | wps 41644.1 | wpb 510.9 | bsz 1 | num_updates 24729 | best_loss 8.249
2022-03-07 10:31:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24729 updates
2022-03-07 10:31:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:31:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:31:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 127 @ 24729 updates, score 10.755) (writing took 3.3708590995520353 seconds)
2022-03-07 10:31:03 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-07 10:31:03 | INFO | train | epoch 127 | loss 4.527 | nll_loss 2.484 | ppl 5.6 | wps 20569.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24729 | lr 0.000201093 | gnorm 1.065 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 78485
2022-03-07 10:31:03 | INFO | fairseq.trainer | begin training epoch 128
2022-03-07 10:31:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:34:44 | INFO | train_inner | epoch 128:     71 / 196 loss=4.509, nll_loss=2.463, ppl=5.51, wps=20301.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.052, loss_scale=16, train_wall=291, gb_free=19.9, wall=78705
2022-03-07 10:36:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:39:58 | INFO | train_inner | epoch 128:    172 / 196 loss=4.542, nll_loss=2.501, ppl=5.66, wps=20867.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.076, loss_scale=16, train_wall=292, gb_free=19.9, wall=79019
2022-03-07 10:41:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:41:17 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.765 | nll_loss 9.441 | ppl 694.91 | wps 41393.4 | wpb 510.9 | bsz 1 | num_updates 24924 | best_loss 8.249
2022-03-07 10:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24924 updates
2022-03-07 10:41:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:41:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 128 @ 24924 updates, score 10.765) (writing took 3.348619317635894 seconds)
2022-03-07 10:41:20 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-07 10:41:20 | INFO | train | epoch 128 | loss 4.523 | nll_loss 2.479 | ppl 5.58 | wps 20668.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24924 | lr 0.000200305 | gnorm 1.062 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 79102
2022-03-07 10:41:20 | INFO | fairseq.trainer | begin training epoch 129
2022-03-07 10:41:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:43:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:45:20 | INFO | train_inner | epoch 129:     77 / 196 loss=4.5, nll_loss=2.452, ppl=5.47, wps=20298, ups=0.31, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.064, loss_scale=16, train_wall=291, gb_free=19.9, wall=79341
2022-03-07 10:49:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:50:34 | INFO | train_inner | epoch 129:    178 / 196 loss=4.538, nll_loss=2.497, ppl=5.65, wps=20880.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.061, loss_scale=16, train_wall=291, gb_free=19.9, wall=79655
2022-03-07 10:51:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:51:34 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.829 | nll_loss 9.52 | ppl 734.31 | wps 41524.5 | wpb 510.9 | bsz 1 | num_updates 25118 | best_loss 8.249
2022-03-07 10:51:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25118 updates
2022-03-07 10:51:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:51:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 10:51:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 129 @ 25118 updates, score 10.829) (writing took 3.392133911140263 seconds)
2022-03-07 10:51:38 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-07 10:51:38 | INFO | train | epoch 129 | loss 4.517 | nll_loss 2.472 | ppl 5.55 | wps 20569.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25118 | lr 0.00019953 | gnorm 1.064 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 79719
2022-03-07 10:51:38 | INFO | fairseq.trainer | begin training epoch 130
2022-03-07 10:51:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:55:53 | INFO | train_inner | epoch 130:     82 / 196 loss=4.491, nll_loss=2.442, ppl=5.43, wps=20490.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.069, loss_scale=16, train_wall=288, gb_free=19.9, wall=79974
2022-03-07 10:56:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:01:07 | INFO | train_inner | epoch 130:    183 / 196 loss=4.54, nll_loss=2.5, ppl=5.66, wps=20869.4, ups=0.32, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.072, loss_scale=16, train_wall=292, gb_free=19.9, wall=80288
2022-03-07 11:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:01:52 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.818 | nll_loss 9.5 | ppl 724.3 | wps 41511.2 | wpb 510.9 | bsz 1 | num_updates 25313 | best_loss 8.249
2022-03-07 11:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25313 updates
2022-03-07 11:01:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:01:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 130 @ 25313 updates, score 10.818) (writing took 3.279321420006454 seconds)
2022-03-07 11:01:55 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-07 11:01:55 | INFO | train | epoch 130 | loss 4.513 | nll_loss 2.468 | ppl 5.53 | wps 20673 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25313 | lr 0.00019876 | gnorm 1.071 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 80337
2022-03-07 11:01:55 | INFO | fairseq.trainer | begin training epoch 131
2022-03-07 11:01:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:03:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:06:28 | INFO | train_inner | epoch 131:     88 / 196 loss=4.484, nll_loss=2.433, ppl=5.4, wps=20317, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.087, loss_scale=16, train_wall=291, gb_free=19.9, wall=80610
2022-03-07 11:10:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:11:42 | INFO | train_inner | epoch 131:    189 / 196 loss=4.537, nll_loss=2.496, ppl=5.64, wps=20874, ups=0.32, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.069, loss_scale=16, train_wall=291, gb_free=19.9, wall=80924
2022-03-07 11:12:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:12:09 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.828 | nll_loss 9.512 | ppl 729.98 | wps 41270.1 | wpb 510.9 | bsz 1 | num_updates 25507 | best_loss 8.249
2022-03-07 11:12:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25507 updates
2022-03-07 11:12:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:12:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:12:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 131 @ 25507 updates, score 10.828) (writing took 3.2731854105368257 seconds)
2022-03-07 11:12:12 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-07 11:12:12 | INFO | train | epoch 131 | loss 4.508 | nll_loss 2.462 | ppl 5.51 | wps 20575.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25507 | lr 0.000198002 | gnorm 1.079 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 80954
2022-03-07 11:12:12 | INFO | fairseq.trainer | begin training epoch 132
2022-03-07 11:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:17:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:17:04 | INFO | train_inner | epoch 132:     94 / 196 loss=4.472, nll_loss=2.42, ppl=5.35, wps=20300.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.055, loss_scale=16, train_wall=291, gb_free=19.9, wall=81246
2022-03-07 11:22:15 | INFO | train_inner | epoch 132:    194 / 196 loss=4.539, nll_loss=2.498, ppl=5.65, wps=21074.1, ups=0.32, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=1.065, loss_scale=16, train_wall=289, gb_free=19.9, wall=81557
2022-03-07 11:22:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:22:26 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.832 | nll_loss 9.512 | ppl 730.2 | wps 41199.1 | wpb 510.9 | bsz 1 | num_updates 25702 | best_loss 8.249
2022-03-07 11:22:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25702 updates
2022-03-07 11:22:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:22:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:22:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 132 @ 25702 updates, score 10.832) (writing took 3.2247172575443983 seconds)
2022-03-07 11:22:29 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-07 11:22:29 | INFO | train | epoch 132 | loss 4.503 | nll_loss 2.457 | ppl 5.49 | wps 20670.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25702 | lr 0.00019725 | gnorm 1.059 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 81571
2022-03-07 11:22:30 | INFO | fairseq.trainer | begin training epoch 133
2022-03-07 11:22:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:23:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:27:37 | INFO | train_inner | epoch 133:     99 / 196 loss=4.463, nll_loss=2.409, ppl=5.31, wps=20310, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25800, lr=0.000196875, gnorm=1.055, loss_scale=16, train_wall=291, gb_free=19.9, wall=81879
2022-03-07 11:30:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:32:43 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.832 | nll_loss 9.511 | ppl 729.64 | wps 41567.5 | wpb 510.9 | bsz 1 | num_updates 25896 | best_loss 8.249
2022-03-07 11:32:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25896 updates
2022-03-07 11:32:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:32:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:32:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 133 @ 25896 updates, score 10.832) (writing took 3.3752024173736572 seconds)
2022-03-07 11:32:47 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-07 11:32:47 | INFO | train | epoch 133 | loss 4.498 | nll_loss 2.451 | ppl 5.47 | wps 20577.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25896 | lr 0.00019651 | gnorm 1.067 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 82188
2022-03-07 11:32:47 | INFO | fairseq.trainer | begin training epoch 134
2022-03-07 11:32:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:32:59 | INFO | train_inner | epoch 134:      4 / 196 loss=4.532, nll_loss=2.49, ppl=5.62, wps=20309.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.083, loss_scale=16, train_wall=291, gb_free=19.9, wall=82201
2022-03-07 11:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:38:13 | INFO | train_inner | epoch 134:    105 / 196 loss=4.461, nll_loss=2.407, ppl=5.3, wps=20873.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.055, loss_scale=16, train_wall=292, gb_free=19.9, wall=82515
2022-03-07 11:42:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:43:01 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.887 | nll_loss 9.582 | ppl 766.25 | wps 41781.5 | wpb 510.9 | bsz 1 | num_updates 26091 | best_loss 8.249
2022-03-07 11:43:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26091 updates
2022-03-07 11:43:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:43:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:43:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 134 @ 26091 updates, score 10.887) (writing took 3.2606195816770196 seconds)
2022-03-07 11:43:04 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-07 11:43:04 | INFO | train | epoch 134 | loss 4.494 | nll_loss 2.446 | ppl 5.45 | wps 20675.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26091 | lr 0.000195774 | gnorm 1.07 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 82805
2022-03-07 11:43:04 | INFO | fairseq.trainer | begin training epoch 135
2022-03-07 11:43:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:43:32 | INFO | train_inner | epoch 135:      9 / 196 loss=4.523, nll_loss=2.48, ppl=5.58, wps=20500, ups=0.31, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.084, loss_scale=16, train_wall=288, gb_free=19.9, wall=82834
2022-03-07 11:44:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:48:46 | INFO | train_inner | epoch 135:    110 / 196 loss=4.461, nll_loss=2.407, ppl=5.3, wps=20839, ups=0.32, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.069, loss_scale=16, train_wall=292, gb_free=19.9, wall=83148
2022-03-07 11:50:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:53:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:53:19 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.862 | nll_loss 9.556 | ppl 752.99 | wps 41234.6 | wpb 510.9 | bsz 1 | num_updates 26285 | best_loss 8.249
2022-03-07 11:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26285 updates
2022-03-07 11:53:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 11:53:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 135 @ 26285 updates, score 10.862) (writing took 3.301433254033327 seconds)
2022-03-07 11:53:22 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-07 11:53:22 | INFO | train | epoch 135 | loss 4.49 | nll_loss 2.441 | ppl 5.43 | wps 20531.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26285 | lr 0.00019505 | gnorm 1.082 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 83424
2022-03-07 11:53:22 | INFO | fairseq.trainer | begin training epoch 136
2022-03-07 11:53:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:54:09 | INFO | train_inner | epoch 136:     15 / 196 loss=4.511, nll_loss=2.467, ppl=5.53, wps=20259.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.091, loss_scale=16, train_wall=291, gb_free=19.9, wall=83471
2022-03-07 11:57:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:59:24 | INFO | train_inner | epoch 136:    116 / 196 loss=4.462, nll_loss=2.408, ppl=5.31, wps=20818.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.087, loss_scale=16, train_wall=292, gb_free=19.9, wall=83785
2022-03-07 12:03:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:03:38 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.85 | nll_loss 9.542 | ppl 745.31 | wps 41019.9 | wpb 510.9 | bsz 1 | num_updates 26480 | best_loss 8.249
2022-03-07 12:03:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26480 updates
2022-03-07 12:03:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:03:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:03:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 136 @ 26480 updates, score 10.85) (writing took 3.360659508034587 seconds)
2022-03-07 12:03:41 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-07 12:03:41 | INFO | train | epoch 136 | loss 4.485 | nll_loss 2.436 | ppl 5.41 | wps 20618.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26480 | lr 0.000194331 | gnorm 1.085 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 84043
2022-03-07 12:03:41 | INFO | fairseq.trainer | begin training epoch 137
2022-03-07 12:03:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:04:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:04:47 | INFO | train_inner | epoch 137:     21 / 196 loss=4.507, nll_loss=2.461, ppl=5.51, wps=20242.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.077, loss_scale=16, train_wall=292, gb_free=19.9, wall=84108
2022-03-07 12:09:59 | INFO | train_inner | epoch 137:    121 / 196 loss=4.456, nll_loss=2.402, ppl=5.28, wps=21010.1, ups=0.32, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.065, loss_scale=16, train_wall=289, gb_free=19.9, wall=84420
2022-03-07 12:11:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:13:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:13:57 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.832 | nll_loss 9.516 | ppl 731.96 | wps 40804.4 | wpb 510.9 | bsz 1 | num_updates 26674 | best_loss 8.249
2022-03-07 12:13:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26674 updates
2022-03-07 12:13:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:14:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:14:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 137 @ 26674 updates, score 10.832) (writing took 3.213929820805788 seconds)
2022-03-07 12:14:00 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-07 12:14:00 | INFO | train | epoch 137 | loss 4.479 | nll_loss 2.429 | ppl 5.38 | wps 20508.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26674 | lr 0.000193623 | gnorm 1.076 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 84662
2022-03-07 12:14:00 | INFO | fairseq.trainer | begin training epoch 138
2022-03-07 12:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:15:22 | INFO | train_inner | epoch 138:     26 / 196 loss=4.497, nll_loss=2.45, ppl=5.46, wps=20242.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.089, loss_scale=16, train_wall=292, gb_free=19.9, wall=84743
2022-03-07 12:18:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:20:36 | INFO | train_inner | epoch 138:    127 / 196 loss=4.459, nll_loss=2.405, ppl=5.29, wps=20817.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.072, loss_scale=16, train_wall=292, gb_free=19.9, wall=85058
2022-03-07 12:24:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:24:16 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.88 | nll_loss 9.565 | ppl 757.61 | wps 41262.2 | wpb 510.9 | bsz 1 | num_updates 26869 | best_loss 8.249
2022-03-07 12:24:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26869 updates
2022-03-07 12:24:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:24:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:24:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 138 @ 26869 updates, score 10.88) (writing took 3.363388012163341 seconds)
2022-03-07 12:24:19 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-07 12:24:19 | INFO | train | epoch 138 | loss 4.476 | nll_loss 2.425 | ppl 5.37 | wps 20616.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26869 | lr 0.000192919 | gnorm 1.078 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 85281
2022-03-07 12:24:19 | INFO | fairseq.trainer | begin training epoch 139
2022-03-07 12:24:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:25:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:25:59 | INFO | train_inner | epoch 139:     32 / 196 loss=4.489, nll_loss=2.44, ppl=5.43, wps=20261.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.081, loss_scale=16, train_wall=291, gb_free=19.9, wall=85381
2022-03-07 12:31:10 | INFO | train_inner | epoch 139:    132 / 196 loss=4.459, nll_loss=2.405, ppl=5.3, wps=21049.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.077, loss_scale=16, train_wall=289, gb_free=19.9, wall=85692
2022-03-07 12:31:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:34:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:34:33 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.897 | nll_loss 9.591 | ppl 771.34 | wps 41608 | wpb 510.9 | bsz 1 | num_updates 27063 | best_loss 8.249
2022-03-07 12:34:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27063 updates
2022-03-07 12:34:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:34:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:34:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 139 @ 27063 updates, score 10.897) (writing took 3.37301242724061 seconds)
2022-03-07 12:34:37 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-07 12:34:37 | INFO | train | epoch 139 | loss 4.471 | nll_loss 2.42 | ppl 5.35 | wps 20558.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27063 | lr 0.000192226 | gnorm 1.075 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 85898
2022-03-07 12:34:37 | INFO | fairseq.trainer | begin training epoch 140
2022-03-07 12:34:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:36:32 | INFO | train_inner | epoch 140:     37 / 196 loss=4.479, nll_loss=2.428, ppl=5.38, wps=20335.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.078, loss_scale=16, train_wall=290, gb_free=19.9, wall=86013
2022-03-07 12:38:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:41:45 | INFO | train_inner | epoch 140:    138 / 196 loss=4.465, nll_loss=2.413, ppl=5.32, wps=20907.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.086, loss_scale=16, train_wall=291, gb_free=19.9, wall=86327
2022-03-07 12:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:44:50 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.869 | nll_loss 9.559 | ppl 754.21 | wps 41623.2 | wpb 510.9 | bsz 1 | num_updates 27258 | best_loss 8.249
2022-03-07 12:44:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27258 updates
2022-03-07 12:44:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:44:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 140 @ 27258 updates, score 10.869) (writing took 3.335467983968556 seconds)
2022-03-07 12:44:53 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-07 12:44:53 | INFO | train | epoch 140 | loss 4.469 | nll_loss 2.416 | ppl 5.34 | wps 20713.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 27258 | lr 0.000191537 | gnorm 1.09 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 86515
2022-03-07 12:44:53 | INFO | fairseq.trainer | begin training epoch 141
2022-03-07 12:44:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:45:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:47:06 | INFO | train_inner | epoch 141:     43 / 196 loss=4.468, nll_loss=2.416, ppl=5.34, wps=20347.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.101, loss_scale=16, train_wall=290, gb_free=19.9, wall=86648
2022-03-07 12:52:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:52:20 | INFO | train_inner | epoch 141:    144 / 196 loss=4.462, nll_loss=2.409, ppl=5.31, wps=20909.5, ups=0.32, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.072, loss_scale=16, train_wall=291, gb_free=19.9, wall=86961
2022-03-07 12:55:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:55:06 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.911 | nll_loss 9.601 | ppl 776.74 | wps 41800.1 | wpb 510.9 | bsz 1 | num_updates 27452 | best_loss 8.249
2022-03-07 12:55:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27452 updates
2022-03-07 12:55:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:55:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 12:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 141 @ 27452 updates, score 10.911) (writing took 3.368649546056986 seconds)
2022-03-07 12:55:09 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-07 12:55:09 | INFO | train | epoch 141 | loss 4.463 | nll_loss 2.41 | ppl 5.31 | wps 20610.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27452 | lr 0.000190859 | gnorm 1.083 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 87131
2022-03-07 12:55:09 | INFO | fairseq.trainer | begin training epoch 142
2022-03-07 12:55:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:57:38 | INFO | train_inner | epoch 142:     48 / 196 loss=4.461, nll_loss=2.407, ppl=5.31, wps=20537.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.088, loss_scale=16, train_wall=287, gb_free=19.9, wall=87280
2022-03-07 12:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:02:52 | INFO | train_inner | epoch 142:    149 / 196 loss=4.461, nll_loss=2.408, ppl=5.31, wps=20912.3, ups=0.32, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.072, loss_scale=16, train_wall=291, gb_free=19.9, wall=87593
2022-03-07 13:05:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:05:22 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.921 | nll_loss 9.619 | ppl 786.39 | wps 41674.2 | wpb 510.9 | bsz 1 | num_updates 27647 | best_loss 8.249
2022-03-07 13:05:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27647 updates
2022-03-07 13:05:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:05:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:05:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 142 @ 27647 updates, score 10.921) (writing took 3.3493420351296663 seconds)
2022-03-07 13:05:25 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-07 13:05:25 | INFO | train | epoch 142 | loss 4.459 | nll_loss 2.405 | ppl 5.3 | wps 20711.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 27647 | lr 0.000190185 | gnorm 1.073 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 87747
2022-03-07 13:05:25 | INFO | fairseq.trainer | begin training epoch 143
2022-03-07 13:05:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:05:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:08:13 | INFO | train_inner | epoch 143:     54 / 196 loss=4.453, nll_loss=2.398, ppl=5.27, wps=20339.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.08, loss_scale=16, train_wall=290, gb_free=19.9, wall=87914
2022-03-07 13:12:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:13:26 | INFO | train_inner | epoch 143:    155 / 196 loss=4.457, nll_loss=2.402, ppl=5.29, wps=20911.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.086, loss_scale=16, train_wall=291, gb_free=19.9, wall=88228
2022-03-07 13:15:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:15:38 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.937 | nll_loss 9.633 | ppl 793.96 | wps 41530.5 | wpb 510.9 | bsz 1 | num_updates 27841 | best_loss 8.249
2022-03-07 13:15:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27841 updates
2022-03-07 13:15:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:15:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:15:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 143 @ 27841 updates, score 10.937) (writing took 3.389399296604097 seconds)
2022-03-07 13:15:41 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-07 13:15:41 | INFO | train | epoch 143 | loss 4.455 | nll_loss 2.4 | ppl 5.28 | wps 20606.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27841 | lr 0.000189521 | gnorm 1.094 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 88363
2022-03-07 13:15:41 | INFO | fairseq.trainer | begin training epoch 144
2022-03-07 13:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:18:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 13:18:48 | INFO | train_inner | epoch 144:     60 / 196 loss=4.451, nll_loss=2.396, ppl=5.26, wps=20339.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.098, loss_scale=8, train_wall=290, gb_free=19.9, wall=88549
2022-03-07 13:23:58 | INFO | train_inner | epoch 144:    160 / 196 loss=4.457, nll_loss=2.403, ppl=5.29, wps=21122.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.08, loss_scale=8, train_wall=288, gb_free=19.9, wall=88859
2022-03-07 13:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:25:54 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.924 | nll_loss 9.616 | ppl 784.9 | wps 41522.3 | wpb 510.9 | bsz 1 | num_updates 28036 | best_loss 8.249
2022-03-07 13:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28036 updates
2022-03-07 13:25:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:25:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:25:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 144 @ 28036 updates, score 10.924) (writing took 3.5774676827713847 seconds)
2022-03-07 13:25:58 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-07 13:25:58 | INFO | train | epoch 144 | loss 4.45 | nll_loss 2.394 | ppl 5.26 | wps 20706.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28036 | lr 0.000188861 | gnorm 1.077 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 88979
2022-03-07 13:25:58 | INFO | fairseq.trainer | begin training epoch 145
2022-03-07 13:25:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:29:17 | INFO | train_inner | epoch 145:     64 / 196 loss=4.44, nll_loss=2.383, ppl=5.22, wps=20513.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.066, loss_scale=16, train_wall=287, gb_free=19.9, wall=89178
2022-03-07 13:31:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:34:30 | INFO | train_inner | epoch 145:    165 / 196 loss=4.457, nll_loss=2.403, ppl=5.29, wps=20903.8, ups=0.32, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.082, loss_scale=16, train_wall=291, gb_free=19.9, wall=89492
2022-03-07 13:36:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:36:11 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.914 | nll_loss 9.611 | ppl 781.79 | wps 41316.8 | wpb 510.9 | bsz 1 | num_updates 28231 | best_loss 8.249
2022-03-07 13:36:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28231 updates
2022-03-07 13:36:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:36:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:36:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 145 @ 28231 updates, score 10.914) (writing took 3.205931984819472 seconds)
2022-03-07 13:36:14 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-07 13:36:14 | INFO | train | epoch 145 | loss 4.447 | nll_loss 2.391 | ppl 5.25 | wps 20709.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28231 | lr 0.000188207 | gnorm 1.083 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 89596
2022-03-07 13:36:14 | INFO | fairseq.trainer | begin training epoch 146
2022-03-07 13:36:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:38:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:39:51 | INFO | train_inner | epoch 146:     70 / 196 loss=4.432, nll_loss=2.373, ppl=5.18, wps=20345.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.083, loss_scale=16, train_wall=290, gb_free=19.9, wall=89813
2022-03-07 13:45:02 | INFO | train_inner | epoch 146:    170 / 196 loss=4.458, nll_loss=2.405, ppl=5.29, wps=21111.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.086, loss_scale=16, train_wall=288, gb_free=19.9, wall=90123
2022-03-07 13:45:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:46:27 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.916 | nll_loss 9.621 | ppl 787.32 | wps 41439.9 | wpb 510.9 | bsz 1 | num_updates 28425 | best_loss 8.249
2022-03-07 13:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28425 updates
2022-03-07 13:46:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:46:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:46:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 146 @ 28425 updates, score 10.916) (writing took 3.1730844862759113 seconds)
2022-03-07 13:46:30 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-07 13:46:30 | INFO | train | epoch 146 | loss 4.443 | nll_loss 2.386 | ppl 5.23 | wps 20602.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28425 | lr 0.000187564 | gnorm 1.081 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 90212
2022-03-07 13:46:30 | INFO | fairseq.trainer | begin training epoch 147
2022-03-07 13:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:50:23 | INFO | train_inner | epoch 147:     75 / 196 loss=4.421, nll_loss=2.361, ppl=5.14, wps=20319.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.096, loss_scale=16, train_wall=291, gb_free=19.9, wall=90445
2022-03-07 13:52:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:55:37 | INFO | train_inner | epoch 147:    176 / 196 loss=4.46, nll_loss=2.406, ppl=5.3, wps=20875.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.103, loss_scale=16, train_wall=291, gb_free=19.9, wall=90759
2022-03-07 13:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:56:44 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.918 | nll_loss 9.609 | ppl 780.67 | wps 41585.2 | wpb 510.9 | bsz 1 | num_updates 28620 | best_loss 8.249
2022-03-07 13:56:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28620 updates
2022-03-07 13:56:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 13:56:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 147 @ 28620 updates, score 10.918) (writing took 3.2943817749619484 seconds)
2022-03-07 13:56:47 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-07 13:56:47 | INFO | train | epoch 147 | loss 4.44 | nll_loss 2.383 | ppl 5.21 | wps 20682.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28620 | lr 0.000186924 | gnorm 1.098 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 90829
2022-03-07 13:56:47 | INFO | fairseq.trainer | begin training epoch 148
2022-03-07 13:56:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:58:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:00:59 | INFO | train_inner | epoch 148:     81 / 196 loss=4.419, nll_loss=2.358, ppl=5.13, wps=20325.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28700, lr=0.000186663, gnorm=1.081, loss_scale=16, train_wall=290, gb_free=19.9, wall=91081
2022-03-07 14:05:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:06:12 | INFO | train_inner | epoch 148:    182 / 196 loss=4.459, nll_loss=2.406, ppl=5.3, wps=20901.5, ups=0.32, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=1.079, loss_scale=16, train_wall=291, gb_free=19.9, wall=91394
2022-03-07 14:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:07:00 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.92 | nll_loss 9.623 | ppl 788.6 | wps 41749.2 | wpb 510.9 | bsz 1 | num_updates 28814 | best_loss 8.249
2022-03-07 14:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 28814 updates
2022-03-07 14:07:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:07:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:07:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 148 @ 28814 updates, score 10.92) (writing took 3.1453823866322637 seconds)
2022-03-07 14:07:04 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-07 14:07:04 | INFO | train | epoch 148 | loss 4.435 | nll_loss 2.377 | ppl 5.19 | wps 20601.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28814 | lr 0.000186294 | gnorm 1.079 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 91445
2022-03-07 14:07:04 | INFO | fairseq.trainer | begin training epoch 149
2022-03-07 14:07:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:10:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 14:11:34 | INFO | train_inner | epoch 149:     87 / 196 loss=4.401, nll_loss=2.337, ppl=5.05, wps=20349.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28900, lr=0.000186016, gnorm=1.097, loss_scale=8, train_wall=290, gb_free=19.9, wall=91715
2022-03-07 14:16:44 | INFO | train_inner | epoch 149:    187 / 196 loss=4.466, nll_loss=2.414, ppl=5.33, wps=21124.1, ups=0.32, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=1.098, loss_scale=8, train_wall=288, gb_free=19.9, wall=92026
2022-03-07 14:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:17:16 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.909 | nll_loss 9.603 | ppl 777.5 | wps 41466.4 | wpb 510.9 | bsz 1 | num_updates 29009 | best_loss 8.249
2022-03-07 14:17:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 29009 updates
2022-03-07 14:17:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:17:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:17:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 149 @ 29009 updates, score 10.909) (writing took 3.2341732159256935 seconds)
2022-03-07 14:17:20 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-07 14:17:20 | INFO | train | epoch 149 | loss 4.432 | nll_loss 2.374 | ppl 5.18 | wps 20715.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29009 | lr 0.000185667 | gnorm 1.098 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 92061
2022-03-07 14:17:20 | INFO | fairseq.trainer | begin training epoch 150
2022-03-07 14:17:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:22:02 | INFO | train_inner | epoch 150:     91 / 196 loss=4.403, nll_loss=2.339, ppl=5.06, wps=20538, ups=0.31, wpb=65367, bsz=127.7, num_updates=29100, lr=0.000185376, gnorm=1.075, loss_scale=16, train_wall=287, gb_free=19.9, wall=92344
2022-03-07 14:23:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:26:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 14:27:19 | INFO | train_inner | epoch 150:    193 / 196 loss=4.457, nll_loss=2.404, ppl=5.29, wps=20709.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=29200, lr=0.000185058, gnorm=1.102, loss_scale=8, train_wall=294, gb_free=19.9, wall=92660
2022-03-07 14:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:27:33 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.942 | nll_loss 9.651 | ppl 803.85 | wps 41271.2 | wpb 510.9 | bsz 1 | num_updates 29203 | best_loss 8.249
2022-03-07 14:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 29203 updates
2022-03-07 14:27:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 150 @ 29203 updates, score 10.942) (writing took 3.4039859930053353 seconds)
2022-03-07 14:27:36 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-07 14:27:36 | INFO | train | epoch 150 | loss 4.428 | nll_loss 2.369 | ppl 5.17 | wps 20602 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29203 | lr 0.000185049 | gnorm 1.088 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 92678
2022-03-07 14:27:36 | INFO | fairseq.trainer | begin training epoch 151
2022-03-07 14:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:32:37 | INFO | train_inner | epoch 151:     97 / 196 loss=4.386, nll_loss=2.319, ppl=4.99, wps=20523.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29300, lr=0.000184742, gnorm=1.085, loss_scale=8, train_wall=287, gb_free=19.9, wall=92979
2022-03-07 14:37:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:37:49 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.882 | nll_loss 9.573 | ppl 761.58 | wps 41435.8 | wpb 510.9 | bsz 1 | num_updates 29399 | best_loss 8.249
2022-03-07 14:37:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 29399 updates
2022-03-07 14:37:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:37:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:37:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 151 @ 29399 updates, score 10.882) (writing took 3.34500069078058 seconds)
2022-03-07 14:37:52 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-07 14:37:52 | INFO | train | epoch 151 | loss 4.425 | nll_loss 2.365 | ppl 5.15 | wps 20810.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 29399 | lr 0.000184431 | gnorm 1.093 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 93294
2022-03-07 14:37:52 | INFO | fairseq.trainer | begin training epoch 152
2022-03-07 14:37:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:37:56 | INFO | train_inner | epoch 152:      1 / 196 loss=4.465, nll_loss=2.412, ppl=5.32, wps=20527.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=29400, lr=0.000184428, gnorm=1.103, loss_scale=16, train_wall=288, gb_free=19.9, wall=93297
2022-03-07 14:40:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:43:09 | INFO | train_inner | epoch 152:    102 / 196 loss=4.382, nll_loss=2.315, ppl=4.98, wps=20902.6, ups=0.32, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=1.089, loss_scale=16, train_wall=291, gb_free=19.9, wall=93611
2022-03-07 14:46:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:48:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:48:05 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.965 | nll_loss 9.661 | ppl 809.41 | wps 41585.7 | wpb 510.9 | bsz 1 | num_updates 29593 | best_loss 8.249
2022-03-07 14:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 29593 updates
2022-03-07 14:48:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:48:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 152 @ 29593 updates, score 10.965) (writing took 3.372307274490595 seconds)
2022-03-07 14:48:09 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-07 14:48:09 | INFO | train | epoch 152 | loss 4.421 | nll_loss 2.361 | ppl 5.14 | wps 20600.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29593 | lr 0.000183825 | gnorm 1.103 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 93910
2022-03-07 14:48:09 | INFO | fairseq.trainer | begin training epoch 153
2022-03-07 14:48:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:48:31 | INFO | train_inner | epoch 153:      7 / 196 loss=4.456, nll_loss=2.402, ppl=5.29, wps=20337.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29600, lr=0.000183804, gnorm=1.116, loss_scale=16, train_wall=290, gb_free=19.9, wall=93932
2022-03-07 14:53:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:53:44 | INFO | train_inner | epoch 153:    108 / 196 loss=4.388, nll_loss=2.322, ppl=5, wps=20913.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=29700, lr=0.000183494, gnorm=1.083, loss_scale=16, train_wall=291, gb_free=19.9, wall=94245
2022-03-07 14:57:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 14:58:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:58:22 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.924 | nll_loss 9.631 | ppl 792.66 | wps 41549.8 | wpb 510.9 | bsz 1 | num_updates 29787 | best_loss 8.249
2022-03-07 14:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 29787 updates
2022-03-07 14:58:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:58:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 14:58:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 153 @ 29787 updates, score 10.924) (writing took 3.3496575951576233 seconds)
2022-03-07 14:58:25 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-07 14:58:25 | INFO | train | epoch 153 | loss 4.416 | nll_loss 2.355 | ppl 5.11 | wps 20600.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29787 | lr 0.000183226 | gnorm 1.097 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 94527
2022-03-07 14:58:25 | INFO | fairseq.trainer | begin training epoch 154
2022-03-07 14:58:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:59:06 | INFO | train_inner | epoch 154:     13 / 196 loss=4.439, nll_loss=2.383, ppl=5.21, wps=20322.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=29800, lr=0.000183186, gnorm=1.105, loss_scale=8, train_wall=291, gb_free=19.9, wall=94567
2022-03-07 15:04:16 | INFO | train_inner | epoch 154:    113 / 196 loss=4.389, nll_loss=2.322, ppl=5, wps=21102.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=29900, lr=0.000182879, gnorm=1.095, loss_scale=16, train_wall=288, gb_free=19.9, wall=94878
2022-03-07 15:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:08:38 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.916 | nll_loss 9.618 | ppl 785.98 | wps 41558.1 | wpb 510.9 | bsz 1 | num_updates 29983 | best_loss 8.249
2022-03-07 15:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 29983 updates
2022-03-07 15:08:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:08:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 154 @ 29983 updates, score 10.916) (writing took 3.295810247771442 seconds)
2022-03-07 15:08:41 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-07 15:08:41 | INFO | train | epoch 154 | loss 4.414 | nll_loss 2.353 | ppl 5.11 | wps 20810.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 29983 | lr 0.000182626 | gnorm 1.103 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 95143
2022-03-07 15:08:41 | INFO | fairseq.trainer | begin training epoch 155
2022-03-07 15:08:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:09:34 | INFO | train_inner | epoch 155:     17 / 196 loss=4.439, nll_loss=2.382, ppl=5.21, wps=20537.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=30000, lr=0.000182574, gnorm=1.118, loss_scale=16, train_wall=287, gb_free=19.9, wall=95196
2022-03-07 15:10:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:14:48 | INFO | train_inner | epoch 155:    118 / 196 loss=4.389, nll_loss=2.323, ppl=5, wps=20907.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=30100, lr=0.000182271, gnorm=1.097, loss_scale=16, train_wall=291, gb_free=19.9, wall=95509
2022-03-07 15:17:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:18:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:18:54 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.946 | nll_loss 9.652 | ppl 804.75 | wps 41402 | wpb 510.9 | bsz 1 | num_updates 30177 | best_loss 8.249
2022-03-07 15:18:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 30177 updates
2022-03-07 15:18:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 155 @ 30177 updates, score 10.946) (writing took 3.395030515268445 seconds)
2022-03-07 15:18:58 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-07 15:18:58 | INFO | train | epoch 155 | loss 4.409 | nll_loss 2.347 | ppl 5.09 | wps 20602.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30177 | lr 0.000182038 | gnorm 1.102 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 95759
2022-03-07 15:18:58 | INFO | fairseq.trainer | begin training epoch 156
2022-03-07 15:18:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:20:09 | INFO | train_inner | epoch 156:     23 / 196 loss=4.426, nll_loss=2.368, ppl=5.16, wps=20329.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=30200, lr=0.000181969, gnorm=1.104, loss_scale=16, train_wall=290, gb_free=19.9, wall=95831
2022-03-07 15:24:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:25:26 | INFO | train_inner | epoch 156:    124 / 196 loss=4.393, nll_loss=2.328, ppl=5.02, wps=20703.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=30300, lr=0.000181668, gnorm=1.093, loss_scale=16, train_wall=292, gb_free=19.9, wall=96147
2022-03-07 15:29:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:29:16 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.968 | nll_loss 9.673 | ppl 816.23 | wps 41211.1 | wpb 510.9 | bsz 1 | num_updates 30372 | best_loss 8.249
2022-03-07 15:29:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 30372 updates
2022-03-07 15:29:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:29:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:29:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 156 @ 30372 updates, score 10.968) (writing took 3.3443109104409814 seconds)
2022-03-07 15:29:19 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-07 15:29:19 | INFO | train | epoch 156 | loss 4.408 | nll_loss 2.345 | ppl 5.08 | wps 20536.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 30372 | lr 0.000181453 | gnorm 1.096 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 96381
2022-03-07 15:29:19 | INFO | fairseq.trainer | begin training epoch 157
2022-03-07 15:29:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:29:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 15:30:49 | INFO | train_inner | epoch 157:     29 / 196 loss=4.418, nll_loss=2.357, ppl=5.12, wps=20209, ups=0.31, wpb=65367, bsz=127.7, num_updates=30400, lr=0.000181369, gnorm=1.098, loss_scale=8, train_wall=291, gb_free=19.9, wall=96471
2022-03-07 15:36:00 | INFO | train_inner | epoch 157:    129 / 196 loss=4.387, nll_loss=2.321, ppl=5, wps=21114.5, ups=0.32, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=1.085, loss_scale=8, train_wall=288, gb_free=19.9, wall=96781
2022-03-07 15:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:39:32 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.97 | nll_loss 9.673 | ppl 816.2 | wps 41370 | wpb 510.9 | bsz 1 | num_updates 30567 | best_loss 8.249
2022-03-07 15:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 30567 updates
2022-03-07 15:39:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:39:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:39:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 157 @ 30567 updates, score 10.97) (writing took 3.3135876515880227 seconds)
2022-03-07 15:39:36 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-07 15:39:36 | INFO | train | epoch 157 | loss 4.403 | nll_loss 2.34 | ppl 5.06 | wps 20706.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30567 | lr 0.000180873 | gnorm 1.096 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 96997
2022-03-07 15:39:36 | INFO | fairseq.trainer | begin training epoch 158
2022-03-07 15:39:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:41:18 | INFO | train_inner | epoch 158:     33 / 196 loss=4.415, nll_loss=2.354, ppl=5.11, wps=20530.8, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=30600, lr=0.000180775, gnorm=1.101, loss_scale=16, train_wall=287, gb_free=19.9, wall=97100
2022-03-07 15:43:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:46:32 | INFO | train_inner | epoch 158:    134 / 196 loss=4.392, nll_loss=2.327, ppl=5.02, wps=20901.4, ups=0.32, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=1.1, loss_scale=16, train_wall=291, gb_free=19.9, wall=97413
2022-03-07 15:49:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 15:49:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:49:49 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.991 | nll_loss 9.705 | ppl 834.86 | wps 41168.4 | wpb 510.9 | bsz 1 | num_updates 30761 | best_loss 8.249
2022-03-07 15:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 30761 updates
2022-03-07 15:49:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:49:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 15:49:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 158 @ 30761 updates, score 10.991) (writing took 3.2685796469449997 seconds)
2022-03-07 15:49:52 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-07 15:49:52 | INFO | train | epoch 158 | loss 4.399 | nll_loss 2.336 | ppl 5.05 | wps 20599.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30761 | lr 0.000180302 | gnorm 1.103 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 97613
2022-03-07 15:49:52 | INFO | fairseq.trainer | begin training epoch 159
2022-03-07 15:49:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:51:53 | INFO | train_inner | epoch 159:     39 / 196 loss=4.401, nll_loss=2.337, ppl=5.05, wps=20331.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=30800, lr=0.000180187, gnorm=1.12, loss_scale=8, train_wall=290, gb_free=19.9, wall=97735
2022-03-07 15:57:04 | INFO | train_inner | epoch 159:    139 / 196 loss=4.394, nll_loss=2.329, ppl=5.03, wps=21098.3, ups=0.32, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=1.114, loss_scale=16, train_wall=288, gb_free=19.9, wall=98045
2022-03-07 16:00:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:00:06 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.931 | nll_loss 9.633 | ppl 793.8 | wps 41622.4 | wpb 510.9 | bsz 1 | num_updates 30957 | best_loss 8.249
2022-03-07 16:00:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 30957 updates
2022-03-07 16:00:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:00:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 159 @ 30957 updates, score 10.931) (writing took 3.290066841058433 seconds)
2022-03-07 16:00:09 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-07 16:00:09 | INFO | train | epoch 159 | loss 4.398 | nll_loss 2.334 | ppl 5.04 | wps 20787.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 30957 | lr 0.00017973 | gnorm 1.115 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 98231
2022-03-07 16:00:09 | INFO | fairseq.trainer | begin training epoch 160
2022-03-07 16:00:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:02:23 | INFO | train_inner | epoch 160:     43 / 196 loss=4.398, nll_loss=2.334, ppl=5.04, wps=20495.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31000, lr=0.000179605, gnorm=1.111, loss_scale=16, train_wall=288, gb_free=19.9, wall=98364
2022-03-07 16:03:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:05:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 16:07:39 | INFO | train_inner | epoch 160:    145 / 196 loss=4.39, nll_loss=2.325, ppl=5.01, wps=20696.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=31100, lr=0.000179316, gnorm=1.117, loss_scale=8, train_wall=294, gb_free=19.9, wall=98681
2022-03-07 16:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:10:22 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.917 | nll_loss 9.618 | ppl 785.75 | wps 41729.1 | wpb 510.9 | bsz 1 | num_updates 31151 | best_loss 8.249
2022-03-07 16:10:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 31151 updates
2022-03-07 16:10:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:10:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:10:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 160 @ 31151 updates, score 10.917) (writing took 3.2911739610135555 seconds)
2022-03-07 16:10:25 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-07 16:10:25 | INFO | train | epoch 160 | loss 4.393 | nll_loss 2.328 | ppl 5.02 | wps 20597.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31151 | lr 0.000179169 | gnorm 1.114 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 98847
2022-03-07 16:10:25 | INFO | fairseq.trainer | begin training epoch 161
2022-03-07 16:10:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:12:58 | INFO | train_inner | epoch 161:     49 / 196 loss=4.391, nll_loss=2.326, ppl=5.02, wps=20502.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=31200, lr=0.000179029, gnorm=1.106, loss_scale=16, train_wall=288, gb_free=19.9, wall=99000
2022-03-07 16:18:09 | INFO | train_inner | epoch 161:    149 / 196 loss=4.392, nll_loss=2.328, ppl=5.02, wps=21078.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=31300, lr=0.000178743, gnorm=1.099, loss_scale=16, train_wall=289, gb_free=19.9, wall=99311
2022-03-07 16:18:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:19:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 16:20:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:20:39 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.947 | nll_loss 9.645 | ppl 800.71 | wps 41736.7 | wpb 510.9 | bsz 1 | num_updates 31345 | best_loss 8.249
2022-03-07 16:20:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 31345 updates
2022-03-07 16:20:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:20:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:20:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 161 @ 31345 updates, score 10.947) (writing took 3.3031487399712205 seconds)
2022-03-07 16:20:43 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-07 16:20:43 | INFO | train | epoch 161 | loss 4.39 | nll_loss 2.325 | ppl 5.01 | wps 20572.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31345 | lr 0.000178614 | gnorm 1.107 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 99464
2022-03-07 16:20:43 | INFO | fairseq.trainer | begin training epoch 162
2022-03-07 16:20:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:23:33 | INFO | train_inner | epoch 162:     55 / 196 loss=4.388, nll_loss=2.322, ppl=5, wps=20151.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=31400, lr=0.000178458, gnorm=1.101, loss_scale=8, train_wall=293, gb_free=19.9, wall=99635
2022-03-07 16:26:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 16:28:55 | INFO | train_inner | epoch 162:    156 / 196 loss=4.396, nll_loss=2.332, ppl=5.04, wps=20386.4, ups=0.31, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=1.115, loss_scale=8, train_wall=297, gb_free=19.9, wall=99956
2022-03-07 16:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:31:08 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.962 | nll_loss 9.674 | ppl 816.94 | wps 36986 | wpb 510.9 | bsz 1 | num_updates 31540 | best_loss 8.249
2022-03-07 16:31:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 31540 updates
2022-03-07 16:31:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:31:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:31:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 162 @ 31540 updates, score 10.962) (writing took 3.55069269426167 seconds)
2022-03-07 16:31:12 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-07 16:31:12 | INFO | train | epoch 162 | loss 4.387 | nll_loss 2.322 | ppl 5 | wps 20276.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 31540 | lr 0.000178061 | gnorm 1.1 | loss_scale 8 | train_wall 573 | gb_free 19.9 | wall 100094
2022-03-07 16:31:12 | INFO | fairseq.trainer | begin training epoch 163
2022-03-07 16:31:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:34:20 | INFO | train_inner | epoch 163:     60 / 196 loss=4.369, nll_loss=2.3, ppl=4.92, wps=20132.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31600, lr=0.000177892, gnorm=1.102, loss_scale=16, train_wall=292, gb_free=19.9, wall=100281
2022-03-07 16:39:32 | INFO | train_inner | epoch 163:    160 / 196 loss=4.399, nll_loss=2.336, ppl=5.05, wps=20977.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=31700, lr=0.000177611, gnorm=1.124, loss_scale=16, train_wall=290, gb_free=19.9, wall=100593
2022-03-07 16:40:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:41:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:41:29 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.987 | nll_loss 9.695 | ppl 829.03 | wps 41277.1 | wpb 510.9 | bsz 1 | num_updates 31735 | best_loss 8.249
2022-03-07 16:41:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 31735 updates
2022-03-07 16:41:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:41:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 163 @ 31735 updates, score 10.987) (writing took 3.426469444297254 seconds)
2022-03-07 16:41:32 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-07 16:41:32 | INFO | train | epoch 163 | loss 4.384 | nll_loss 2.318 | ppl 4.98 | wps 20575 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 31735 | lr 0.000177513 | gnorm 1.119 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 100714
2022-03-07 16:41:32 | INFO | fairseq.trainer | begin training epoch 164
2022-03-07 16:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:44:55 | INFO | train_inner | epoch 164:     65 / 196 loss=4.372, nll_loss=2.304, ppl=4.94, wps=20208.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=31800, lr=0.000177332, gnorm=1.117, loss_scale=16, train_wall=292, gb_free=19.9, wall=100917
2022-03-07 16:47:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:50:11 | INFO | train_inner | epoch 164:    166 / 196 loss=4.388, nll_loss=2.323, ppl=5, wps=20778.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=31900, lr=0.000177054, gnorm=1.108, loss_scale=16, train_wall=293, gb_free=19.9, wall=101232
2022-03-07 16:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:51:50 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 11.001 | nll_loss 9.712 | ppl 838.55 | wps 37735.3 | wpb 510.9 | bsz 1 | num_updates 31930 | best_loss 8.249
2022-03-07 16:51:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 31930 updates
2022-03-07 16:51:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:51:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 16:51:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 164 @ 31930 updates, score 11.001) (writing took 3.2966216979548335 seconds)
2022-03-07 16:51:53 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-07 16:51:53 | INFO | train | epoch 164 | loss 4.379 | nll_loss 2.312 | ppl 4.97 | wps 20565.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 31930 | lr 0.00017697 | gnorm 1.109 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 101334
2022-03-07 16:51:53 | INFO | fairseq.trainer | begin training epoch 165
2022-03-07 16:51:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:53:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 16:55:33 | INFO | train_inner | epoch 165:     71 / 196 loss=4.364, nll_loss=2.294, ppl=4.9, wps=20257, ups=0.31, wpb=65367, bsz=127.7, num_updates=32000, lr=0.000176777, gnorm=1.098, loss_scale=8, train_wall=291, gb_free=19.9, wall=101555
2022-03-07 17:00:44 | INFO | train_inner | epoch 165:    171 / 196 loss=4.395, nll_loss=2.331, ppl=5.03, wps=21070.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=32100, lr=0.000176501, gnorm=1.127, loss_scale=16, train_wall=289, gb_free=19.9, wall=101866
2022-03-07 17:02:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:02:07 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.97 | nll_loss 9.677 | ppl 818.84 | wps 41476.8 | wpb 510.9 | bsz 1 | num_updates 32125 | best_loss 8.249
2022-03-07 17:02:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 32125 updates
2022-03-07 17:02:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:02:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:02:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 165 @ 32125 updates, score 10.97) (writing took 3.0801494605839252 seconds)
2022-03-07 17:02:10 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-07 17:02:10 | INFO | train | epoch 165 | loss 4.377 | nll_loss 2.31 | ppl 4.96 | wps 20688.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32125 | lr 0.000176432 | gnorm 1.116 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 101951
2022-03-07 17:02:10 | INFO | fairseq.trainer | begin training epoch 166
2022-03-07 17:02:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:06:03 | INFO | train_inner | epoch 166:     75 / 196 loss=4.355, nll_loss=2.284, ppl=4.87, wps=20530.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32200, lr=0.000176227, gnorm=1.108, loss_scale=16, train_wall=288, gb_free=19.9, wall=102184
2022-03-07 17:07:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:11:16 | INFO | train_inner | epoch 166:    176 / 196 loss=4.394, nll_loss=2.33, ppl=5.03, wps=20893.1, ups=0.32, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=1.111, loss_scale=16, train_wall=291, gb_free=19.9, wall=102498
2022-03-07 17:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:12:23 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 11.006 | nll_loss 9.716 | ppl 841.26 | wps 41466.4 | wpb 510.9 | bsz 1 | num_updates 32320 | best_loss 8.249
2022-03-07 17:12:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 32320 updates
2022-03-07 17:12:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:12:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:12:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 166 @ 32320 updates, score 11.006) (writing took 3.222078434191644 seconds)
2022-03-07 17:12:26 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-07 17:12:26 | INFO | train | epoch 166 | loss 4.374 | nll_loss 2.307 | ppl 4.95 | wps 20696.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32320 | lr 0.000175899 | gnorm 1.115 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 102568
2022-03-07 17:12:26 | INFO | fairseq.trainer | begin training epoch 167
2022-03-07 17:12:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:14:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:16:38 | INFO | train_inner | epoch 167:     81 / 196 loss=4.35, nll_loss=2.278, ppl=4.85, wps=20339, ups=0.31, wpb=65367, bsz=127.7, num_updates=32400, lr=0.000175682, gnorm=1.134, loss_scale=16, train_wall=290, gb_free=19.9, wall=102819
2022-03-07 17:20:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:21:51 | INFO | train_inner | epoch 167:    182 / 196 loss=4.396, nll_loss=2.333, ppl=5.04, wps=20900.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=32500, lr=0.000175412, gnorm=1.121, loss_scale=16, train_wall=291, gb_free=19.9, wall=103133
2022-03-07 17:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:22:39 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.966 | nll_loss 9.667 | ppl 813.05 | wps 41634.4 | wpb 510.9 | bsz 1 | num_updates 32514 | best_loss 8.249
2022-03-07 17:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 32514 updates
2022-03-07 17:22:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:22:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:22:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 167 @ 32514 updates, score 10.966) (writing took 3.24325155839324 seconds)
2022-03-07 17:22:43 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-07 17:22:43 | INFO | train | epoch 167 | loss 4.371 | nll_loss 2.302 | ppl 4.93 | wps 20598.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32514 | lr 0.000175374 | gnorm 1.124 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 103184
2022-03-07 17:22:43 | INFO | fairseq.trainer | begin training epoch 168
2022-03-07 17:22:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:27:10 | INFO | train_inner | epoch 168:     86 / 196 loss=4.345, nll_loss=2.272, ppl=4.83, wps=20531.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=32600, lr=0.000175142, gnorm=1.107, loss_scale=16, train_wall=288, gb_free=19.9, wall=103451
2022-03-07 17:27:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:32:23 | INFO | train_inner | epoch 168:    187 / 196 loss=4.392, nll_loss=2.328, ppl=5.02, wps=20913.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=32700, lr=0.000174874, gnorm=1.112, loss_scale=16, train_wall=291, gb_free=19.9, wall=103765
2022-03-07 17:32:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:32:56 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.998 | nll_loss 9.713 | ppl 839.2 | wps 41662.2 | wpb 510.9 | bsz 1 | num_updates 32709 | best_loss 8.249
2022-03-07 17:32:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 32709 updates
2022-03-07 17:32:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:32:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:32:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 168 @ 32709 updates, score 10.998) (writing took 3.174248735420406 seconds)
2022-03-07 17:32:59 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-07 17:32:59 | INFO | train | epoch 168 | loss 4.368 | nll_loss 2.299 | ppl 4.92 | wps 20716.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32709 | lr 0.00017485 | gnorm 1.106 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 103800
2022-03-07 17:32:59 | INFO | fairseq.trainer | begin training epoch 169
2022-03-07 17:32:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:34:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:37:45 | INFO | train_inner | epoch 169:     92 / 196 loss=4.34, nll_loss=2.267, ppl=4.81, wps=20334.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32800, lr=0.000174608, gnorm=1.103, loss_scale=16, train_wall=290, gb_free=19.9, wall=104086
2022-03-07 17:41:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:42:58 | INFO | train_inner | epoch 169:    193 / 196 loss=4.396, nll_loss=2.333, ppl=5.04, wps=20903.1, ups=0.32, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=1.105, loss_scale=16, train_wall=291, gb_free=19.9, wall=104400
2022-03-07 17:43:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:43:12 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 11.004 | nll_loss 9.711 | ppl 838.13 | wps 41725.2 | wpb 510.9 | bsz 1 | num_updates 32903 | best_loss 8.249
2022-03-07 17:43:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 32903 updates
2022-03-07 17:43:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:43:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 169 @ 32903 updates, score 11.004) (writing took 3.236599884927273 seconds)
2022-03-07 17:43:15 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-07 17:43:15 | INFO | train | epoch 169 | loss 4.365 | nll_loss 2.296 | ppl 4.91 | wps 20597.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32903 | lr 0.000174334 | gnorm 1.104 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 104417
2022-03-07 17:43:15 | INFO | fairseq.trainer | begin training epoch 170
2022-03-07 17:43:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:48:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:48:20 | INFO | train_inner | epoch 170:     98 / 196 loss=4.331, nll_loss=2.256, ppl=4.78, wps=20330.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33000, lr=0.000174078, gnorm=1.11, loss_scale=16, train_wall=290, gb_free=19.9, wall=104721
2022-03-07 17:53:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:53:28 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 11.01 | nll_loss 9.722 | ppl 844.31 | wps 41248.4 | wpb 510.9 | bsz 1 | num_updates 33098 | best_loss 8.249
2022-03-07 17:53:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 33098 updates
2022-03-07 17:53:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:53:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 17:53:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 170 @ 33098 updates, score 11.01) (writing took 3.261509856209159 seconds)
2022-03-07 17:53:32 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-07 17:53:32 | INFO | train | epoch 170 | loss 4.363 | nll_loss 2.293 | ppl 4.9 | wps 20698.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 33098 | lr 0.00017382 | gnorm 1.111 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 105033
2022-03-07 17:53:32 | INFO | fairseq.trainer | begin training epoch 171
2022-03-07 17:53:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:53:38 | INFO | train_inner | epoch 171:      2 / 196 loss=4.395, nll_loss=2.331, ppl=5.03, wps=20526, ups=0.31, wpb=65367, bsz=127.7, num_updates=33100, lr=0.000173814, gnorm=1.114, loss_scale=16, train_wall=288, gb_free=19.9, wall=105040
2022-03-07 17:54:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:58:52 | INFO | train_inner | epoch 171:    103 / 196 loss=4.33, nll_loss=2.255, ppl=4.77, wps=20892.7, ups=0.32, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=1.116, loss_scale=16, train_wall=291, gb_free=19.9, wall=105353
2022-03-07 18:01:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:03:48 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.977 | nll_loss 9.683 | ppl 822.12 | wps 36616.5 | wpb 510.9 | bsz 1 | num_updates 33292 | best_loss 8.249
2022-03-07 18:03:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 33292 updates
2022-03-07 18:03:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:03:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:03:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 171 @ 33292 updates, score 10.977) (writing took 3.4762919759377837 seconds)
2022-03-07 18:03:52 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-07 18:03:52 | INFO | train | epoch 171 | loss 4.358 | nll_loss 2.288 | ppl 4.88 | wps 20474.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33292 | lr 0.000173313 | gnorm 1.127 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 105653
2022-03-07 18:03:52 | INFO | fairseq.trainer | begin training epoch 172
2022-03-07 18:03:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:04:18 | INFO | train_inner | epoch 172:      8 / 196 loss=4.383, nll_loss=2.318, ppl=4.99, wps=20044.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33300, lr=0.000173292, gnorm=1.137, loss_scale=16, train_wall=293, gb_free=19.9, wall=105679
2022-03-07 18:08:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:09:45 | INFO | train_inner | epoch 172:    109 / 196 loss=4.327, nll_loss=2.251, ppl=4.76, wps=20054.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=33400, lr=0.000173032, gnorm=1.104, loss_scale=16, train_wall=301, gb_free=19.9, wall=106006
2022-03-07 18:14:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:14:31 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.974 | nll_loss 9.682 | ppl 821.38 | wps 36989.8 | wpb 510.9 | bsz 1 | num_updates 33487 | best_loss 8.249
2022-03-07 18:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 33487 updates
2022-03-07 18:14:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:14:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 172 @ 33487 updates, score 10.974) (writing took 3.4788463609293103 seconds)
2022-03-07 18:14:35 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-07 18:14:35 | INFO | train | epoch 172 | loss 4.356 | nll_loss 2.286 | ppl 4.88 | wps 19847.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 33487 | lr 0.000172807 | gnorm 1.116 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 106297
2022-03-07 18:14:35 | INFO | fairseq.trainer | begin training epoch 173
2022-03-07 18:14:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:15:17 | INFO | train_inner | epoch 173:     13 / 196 loss=4.382, nll_loss=2.316, ppl=4.98, wps=19660.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=33500, lr=0.000172774, gnorm=1.128, loss_scale=16, train_wall=298, gb_free=19.9, wall=106339
2022-03-07 18:15:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:20:44 | INFO | train_inner | epoch 173:    114 / 196 loss=4.332, nll_loss=2.257, ppl=4.78, wps=20032.5, ups=0.31, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=1.112, loss_scale=16, train_wall=301, gb_free=19.9, wall=106666
2022-03-07 18:21:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 18:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:25:21 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 11.052 | nll_loss 9.765 | ppl 869.8 | wps 34096.5 | wpb 510.9 | bsz 1 | num_updates 33681 | best_loss 8.249
2022-03-07 18:25:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 33681 updates
2022-03-07 18:25:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:25:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 173 @ 33681 updates, score 11.052) (writing took 3.6300048073753715 seconds)
2022-03-07 18:25:24 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-07 18:25:24 | INFO | train | epoch 173 | loss 4.354 | nll_loss 2.283 | ppl 4.87 | wps 19551.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 33681 | lr 0.000172309 | gnorm 1.124 | loss_scale 8 | train_wall 588 | gb_free 19.9 | wall 106946
2022-03-07 18:25:24 | INFO | fairseq.trainer | begin training epoch 174
2022-03-07 18:25:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:26:28 | INFO | train_inner | epoch 174:     19 / 196 loss=4.371, nll_loss=2.303, ppl=4.94, wps=18999.5, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=33700, lr=0.00017226, gnorm=1.135, loss_scale=8, train_wall=307, gb_free=19.9, wall=107010
2022-03-07 18:31:48 | INFO | train_inner | epoch 174:    119 / 196 loss=4.334, nll_loss=2.26, ppl=4.79, wps=20473.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=33800, lr=0.000172005, gnorm=1.108, loss_scale=16, train_wall=296, gb_free=19.9, wall=107330
2022-03-07 18:35:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:36:03 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 11 | nll_loss 9.705 | ppl 834.56 | wps 36924.9 | wpb 510.9 | bsz 1 | num_updates 33876 | best_loss 8.249
2022-03-07 18:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 33876 updates
2022-03-07 18:36:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:36:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:36:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 174 @ 33876 updates, score 11.0) (writing took 3.477736688219011 seconds)
2022-03-07 18:36:06 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-07 18:36:06 | INFO | train | epoch 174 | loss 4.351 | nll_loss 2.279 | ppl 4.85 | wps 19874.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 33876 | lr 0.000171812 | gnorm 1.119 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 107588
2022-03-07 18:36:06 | INFO | fairseq.trainer | begin training epoch 175
2022-03-07 18:36:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:37:24 | INFO | train_inner | epoch 175:     24 / 196 loss=4.364, nll_loss=2.295, ppl=4.91, wps=19467.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=33900, lr=0.000171751, gnorm=1.131, loss_scale=16, train_wall=301, gb_free=19.9, wall=107666
2022-03-07 18:42:48 | INFO | train_inner | epoch 175:    124 / 196 loss=4.334, nll_loss=2.26, ppl=4.79, wps=20238.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=34000, lr=0.000171499, gnorm=1.119, loss_scale=16, train_wall=298, gb_free=19.9, wall=107990
2022-03-07 18:42:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:46:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:46:46 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 11.066 | nll_loss 9.786 | ppl 882.68 | wps 37128.9 | wpb 510.9 | bsz 1 | num_updates 34071 | best_loss 8.249
2022-03-07 18:46:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 34071 updates
2022-03-07 18:46:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:46:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:46:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 175 @ 34071 updates, score 11.066) (writing took 3.463286366313696 seconds)
2022-03-07 18:46:50 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-07 18:46:50 | INFO | train | epoch 175 | loss 4.348 | nll_loss 2.276 | ppl 4.84 | wps 19837.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 34071 | lr 0.00017132 | gnorm 1.122 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 108231
2022-03-07 18:46:50 | INFO | fairseq.trainer | begin training epoch 176
2022-03-07 18:46:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:48:24 | INFO | train_inner | epoch 176:     29 / 196 loss=4.358, nll_loss=2.288, ppl=4.88, wps=19453.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=34100, lr=0.000171247, gnorm=1.122, loss_scale=16, train_wall=301, gb_free=19.9, wall=108326
2022-03-07 18:50:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:53:52 | INFO | train_inner | epoch 176:    130 / 196 loss=4.333, nll_loss=2.259, ppl=4.79, wps=19960.3, ups=0.3, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=1.12, loss_scale=16, train_wall=302, gb_free=19.9, wall=108654
2022-03-07 18:57:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:57:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:57:32 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.994 | nll_loss 9.7 | ppl 831.79 | wps 36855.9 | wpb 510.9 | bsz 1 | num_updates 34265 | best_loss 8.249
2022-03-07 18:57:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 34265 updates
2022-03-07 18:57:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:57:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 18:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 176 @ 34265 updates, score 10.994) (writing took 3.5602112095803022 seconds)
2022-03-07 18:57:35 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-07 18:57:35 | INFO | train | epoch 176 | loss 4.344 | nll_loss 2.271 | ppl 4.83 | wps 19666.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 34265 | lr 0.000170834 | gnorm 1.123 | loss_scale 16 | train_wall 586 | gb_free 19.9 | wall 108877
2022-03-07 18:57:35 | INFO | fairseq.trainer | begin training epoch 177
2022-03-07 18:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:59:29 | INFO | train_inner | epoch 177:     35 / 196 loss=4.355, nll_loss=2.285, ppl=4.87, wps=19402, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=34300, lr=0.000170747, gnorm=1.114, loss_scale=16, train_wall=301, gb_free=19.9, wall=108991
2022-03-07 19:04:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:04:55 | INFO | train_inner | epoch 177:    136 / 196 loss=4.332, nll_loss=2.258, ppl=4.78, wps=20149.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=34400, lr=0.000170499, gnorm=1.116, loss_scale=16, train_wall=300, gb_free=19.9, wall=109316
2022-03-07 19:08:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:08:20 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 11.032 | nll_loss 9.753 | ppl 862.72 | wps 34659.2 | wpb 510.9 | bsz 1 | num_updates 34460 | best_loss 8.249
2022-03-07 19:08:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 34460 updates
2022-03-07 19:08:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:08:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:08:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 177 @ 34460 updates, score 11.032) (writing took 3.6773270051926374 seconds)
2022-03-07 19:08:24 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-07 19:08:24 | INFO | train | epoch 177 | loss 4.342 | nll_loss 2.269 | ppl 4.82 | wps 19688.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 34460 | lr 0.00017035 | gnorm 1.108 | loss_scale 16 | train_wall 587 | gb_free 19.9 | wall 109525
2022-03-07 19:08:24 | INFO | fairseq.trainer | begin training epoch 178
2022-03-07 19:08:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:10:37 | INFO | train_inner | epoch 178:     40 / 196 loss=4.351, nll_loss=2.279, ppl=4.85, wps=19072, ups=0.29, wpb=65367, bsz=127.7, num_updates=34500, lr=0.000170251, gnorm=1.11, loss_scale=16, train_wall=305, gb_free=19.9, wall=109659
2022-03-07 19:11:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:16:02 | INFO | train_inner | epoch 178:    141 / 196 loss=4.331, nll_loss=2.256, ppl=4.78, wps=20192.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=34600, lr=0.000170005, gnorm=1.142, loss_scale=16, train_wall=300, gb_free=19.9, wall=109983
2022-03-07 19:18:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:18:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:19:00 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 11.022 | nll_loss 9.741 | ppl 855.69 | wps 41723.6 | wpb 510.9 | bsz 1 | num_updates 34654 | best_loss 8.249
2022-03-07 19:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 34654 updates
2022-03-07 19:19:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:19:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:19:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 178 @ 34654 updates, score 11.022) (writing took 3.345276347361505 seconds)
2022-03-07 19:19:03 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-07 19:19:03 | INFO | train | epoch 178 | loss 4.339 | nll_loss 2.266 | ppl 4.81 | wps 19841.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 34654 | lr 0.000169873 | gnorm 1.131 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 110165
2022-03-07 19:19:04 | INFO | fairseq.trainer | begin training epoch 179
2022-03-07 19:19:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:21:26 | INFO | train_inner | epoch 179:     46 / 196 loss=4.342, nll_loss=2.269, ppl=4.82, wps=20134.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=34700, lr=0.00016976, gnorm=1.119, loss_scale=16, train_wall=293, gb_free=19.9, wall=110308
2022-03-07 19:25:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:26:50 | INFO | train_inner | epoch 179:    147 / 196 loss=4.34, nll_loss=2.267, ppl=4.81, wps=20265.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=34800, lr=0.000169516, gnorm=1.148, loss_scale=16, train_wall=299, gb_free=19.9, wall=110631
2022-03-07 19:29:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:29:29 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 11.017 | nll_loss 9.727 | ppl 847.38 | wps 38909.4 | wpb 510.9 | bsz 1 | num_updates 34849 | best_loss 8.249
2022-03-07 19:29:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 34849 updates
2022-03-07 19:29:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:29:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 179 @ 34849 updates, score 11.017) (writing took 3.375729518942535 seconds)
2022-03-07 19:29:32 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-07 19:29:32 | INFO | train | epoch 179 | loss 4.337 | nll_loss 2.263 | ppl 4.8 | wps 20292.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 34849 | lr 0.000169397 | gnorm 1.135 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 110794
2022-03-07 19:29:32 | INFO | fairseq.trainer | begin training epoch 180
2022-03-07 19:29:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:32:17 | INFO | train_inner | epoch 180:     52 / 196 loss=4.331, nll_loss=2.257, ppl=4.78, wps=19997, ups=0.31, wpb=65367, bsz=127.7, num_updates=34900, lr=0.000169273, gnorm=1.119, loss_scale=16, train_wall=294, gb_free=19.9, wall=110958
2022-03-07 19:37:27 | INFO | train_inner | epoch 180:    152 / 196 loss=4.335, nll_loss=2.261, ppl=4.79, wps=21108.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=35000, lr=0.000169031, gnorm=1.112, loss_scale=16, train_wall=288, gb_free=19.9, wall=111269
2022-03-07 19:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:39:50 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 11.046 | nll_loss 9.759 | ppl 866.19 | wps 41659.6 | wpb 510.9 | bsz 1 | num_updates 35043 | best_loss 8.249
2022-03-07 19:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 35043 updates
2022-03-07 19:39:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:39:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 180 @ 35043 updates, score 11.046) (writing took 3.185006035491824 seconds)
2022-03-07 19:39:53 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-07 19:39:53 | INFO | train | epoch 180 | loss 4.334 | nll_loss 2.259 | ppl 4.79 | wps 20461.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35043 | lr 0.000168927 | gnorm 1.118 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 111415
2022-03-07 19:39:53 | INFO | fairseq.trainer | begin training epoch 181
2022-03-07 19:39:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:42:51 | INFO | train_inner | epoch 181:     57 / 196 loss=4.329, nll_loss=2.254, ppl=4.77, wps=20177.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=35100, lr=0.00016879, gnorm=1.122, loss_scale=16, train_wall=293, gb_free=19.9, wall=111593
2022-03-07 19:45:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:48:06 | INFO | train_inner | epoch 181:    158 / 196 loss=4.337, nll_loss=2.264, ppl=4.8, wps=20812.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=35200, lr=0.00016855, gnorm=1.12, loss_scale=16, train_wall=292, gb_free=19.9, wall=111908
2022-03-07 19:50:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:50:08 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 11.041 | nll_loss 9.757 | ppl 865.07 | wps 41586.3 | wpb 510.9 | bsz 1 | num_updates 35238 | best_loss 8.249
2022-03-07 19:50:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 35238 updates
2022-03-07 19:50:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:50:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 19:50:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 181 @ 35238 updates, score 11.041) (writing took 3.285859001800418 seconds)
2022-03-07 19:50:12 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-07 19:50:12 | INFO | train | epoch 181 | loss 4.332 | nll_loss 2.257 | ppl 4.78 | wps 20621.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 35238 | lr 0.000168459 | gnorm 1.114 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 112033
2022-03-07 19:50:12 | INFO | fairseq.trainer | begin training epoch 182
2022-03-07 19:50:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:52:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:53:28 | INFO | train_inner | epoch 182:     63 / 196 loss=4.32, nll_loss=2.243, ppl=4.73, wps=20312.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35300, lr=0.000168311, gnorm=1.116, loss_scale=16, train_wall=291, gb_free=19.9, wall=112229
2022-03-07 19:58:52 | INFO | train_inner | epoch 182:    163 / 196 loss=4.341, nll_loss=2.268, ppl=4.82, wps=20196.7, ups=0.31, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=1.14, loss_scale=16, train_wall=299, gb_free=19.9, wall=112554
2022-03-07 19:59:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:00:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:00:45 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 11.009 | nll_loss 9.723 | ppl 845.26 | wps 36806.4 | wpb 510.9 | bsz 1 | num_updates 35432 | best_loss 8.249
2022-03-07 20:00:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 35432 updates
2022-03-07 20:00:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:00:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:00:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 182 @ 35432 updates, score 11.009) (writing took 3.555590565316379 seconds)
2022-03-07 20:00:48 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-07 20:00:48 | INFO | train | epoch 182 | loss 4.329 | nll_loss 2.254 | ppl 4.77 | wps 19948.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 35432 | lr 0.000167997 | gnorm 1.13 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 112670
2022-03-07 20:00:48 | INFO | fairseq.trainer | begin training epoch 183
2022-03-07 20:00:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:04:29 | INFO | train_inner | epoch 183:     68 / 196 loss=4.317, nll_loss=2.24, ppl=4.72, wps=19397.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=35500, lr=0.000167836, gnorm=1.128, loss_scale=16, train_wall=301, gb_free=19.9, wall=112891
2022-03-07 20:06:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:09:53 | INFO | train_inner | epoch 183:    169 / 196 loss=4.343, nll_loss=2.27, ppl=4.82, wps=20269.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=35600, lr=0.0001676, gnorm=1.12, loss_scale=16, train_wall=299, gb_free=19.9, wall=113214
2022-03-07 20:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:11:21 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.987 | nll_loss 9.698 | ppl 830.4 | wps 41693.8 | wpb 510.9 | bsz 1 | num_updates 35627 | best_loss 8.249
2022-03-07 20:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 35627 updates
2022-03-07 20:11:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 183 @ 35627 updates, score 10.987) (writing took 3.062947162427008 seconds)
2022-03-07 20:11:24 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-07 20:11:24 | INFO | train | epoch 183 | loss 4.327 | nll_loss 2.252 | ppl 4.76 | wps 20074.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35627 | lr 0.000167537 | gnorm 1.124 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 113306
2022-03-07 20:11:24 | INFO | fairseq.trainer | begin training epoch 184
2022-03-07 20:11:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:13:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:15:14 | INFO | train_inner | epoch 184:     74 / 196 loss=4.305, nll_loss=2.227, ppl=4.68, wps=20356.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=35700, lr=0.000167365, gnorm=1.122, loss_scale=16, train_wall=290, gb_free=19.9, wall=113535
2022-03-07 20:17:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 20:20:27 | INFO | train_inner | epoch 184:    175 / 196 loss=4.342, nll_loss=2.27, ppl=4.82, wps=20934.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=35800, lr=0.000167132, gnorm=1.134, loss_scale=8, train_wall=291, gb_free=19.9, wall=113848
2022-03-07 20:21:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:21:36 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.985 | nll_loss 9.693 | ppl 827.56 | wps 41636.8 | wpb 510.9 | bsz 1 | num_updates 35821 | best_loss 8.249
2022-03-07 20:21:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 35821 updates
2022-03-07 20:21:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:21:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:21:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 184 @ 35821 updates, score 10.985) (writing took 3.063416074961424 seconds)
2022-03-07 20:21:39 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-07 20:21:39 | INFO | train | epoch 184 | loss 4.323 | nll_loss 2.248 | ppl 4.75 | wps 20632.8 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 35821 | lr 0.000167083 | gnorm 1.131 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 113921
2022-03-07 20:21:39 | INFO | fairseq.trainer | begin training epoch 185
2022-03-07 20:21:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:25:44 | INFO | train_inner | epoch 185:     79 / 196 loss=4.304, nll_loss=2.224, ppl=4.67, wps=20580.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35900, lr=0.000166899, gnorm=1.144, loss_scale=16, train_wall=287, gb_free=19.9, wall=114166
2022-03-07 20:30:54 | INFO | train_inner | epoch 185:    179 / 196 loss=4.342, nll_loss=2.27, ppl=4.82, wps=21139.7, ups=0.32, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=1.131, loss_scale=16, train_wall=288, gb_free=19.9, wall=114476
2022-03-07 20:31:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:31:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:31:52 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 11.048 | nll_loss 9.768 | ppl 872.15 | wps 41462 | wpb 510.9 | bsz 1 | num_updates 36016 | best_loss 8.249
2022-03-07 20:31:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 36016 updates
2022-03-07 20:31:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 185 @ 36016 updates, score 11.048) (writing took 3.087982035242021 seconds)
2022-03-07 20:31:55 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-07 20:31:55 | INFO | train | epoch 185 | loss 4.321 | nll_loss 2.245 | ppl 4.74 | wps 20741.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36016 | lr 0.00016663 | gnorm 1.136 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 114536
2022-03-07 20:31:55 | INFO | fairseq.trainer | begin training epoch 186
2022-03-07 20:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:36:15 | INFO | train_inner | epoch 186:     84 / 196 loss=4.294, nll_loss=2.213, ppl=4.64, wps=20370.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=36100, lr=0.000166436, gnorm=1.133, loss_scale=16, train_wall=290, gb_free=19.9, wall=114797
2022-03-07 20:37:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:41:28 | INFO | train_inner | epoch 186:    185 / 196 loss=4.346, nll_loss=2.274, ppl=4.84, wps=20926.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=36200, lr=0.000166206, gnorm=1.14, loss_scale=16, train_wall=291, gb_free=19.9, wall=115110
2022-03-07 20:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:42:07 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 11.061 | nll_loss 9.781 | ppl 879.63 | wps 41363.9 | wpb 510.9 | bsz 1 | num_updates 36211 | best_loss 8.249
2022-03-07 20:42:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 36211 updates
2022-03-07 20:42:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:42:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:42:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 186 @ 36211 updates, score 11.061) (writing took 3.0506270891055465 seconds)
2022-03-07 20:42:10 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-07 20:42:10 | INFO | train | epoch 186 | loss 4.319 | nll_loss 2.242 | ppl 4.73 | wps 20736.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36211 | lr 0.00016618 | gnorm 1.134 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 115152
2022-03-07 20:42:10 | INFO | fairseq.trainer | begin training epoch 187
2022-03-07 20:42:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:44:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:46:49 | INFO | train_inner | epoch 187:     90 / 196 loss=4.295, nll_loss=2.215, ppl=4.64, wps=20369.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=36300, lr=0.000165977, gnorm=1.118, loss_scale=16, train_wall=290, gb_free=19.9, wall=115431
2022-03-07 20:51:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:52:02 | INFO | train_inner | epoch 187:    191 / 196 loss=4.341, nll_loss=2.269, ppl=4.82, wps=20933.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=36400, lr=0.000165748, gnorm=1.133, loss_scale=16, train_wall=291, gb_free=19.9, wall=115744
2022-03-07 20:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:52:22 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 11.06 | nll_loss 9.779 | ppl 878.67 | wps 41431.4 | wpb 510.9 | bsz 1 | num_updates 36405 | best_loss 8.249
2022-03-07 20:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 36405 updates
2022-03-07 20:52:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:52:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 20:52:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 187 @ 36405 updates, score 11.06) (writing took 3.0022080382332206 seconds)
2022-03-07 20:52:25 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-07 20:52:25 | INFO | train | epoch 187 | loss 4.316 | nll_loss 2.239 | ppl 4.72 | wps 20635.7 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 36405 | lr 0.000165737 | gnorm 1.126 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 115767
2022-03-07 20:52:25 | INFO | fairseq.trainer | begin training epoch 188
2022-03-07 20:52:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:57:20 | INFO | train_inner | epoch 188:     95 / 196 loss=4.285, nll_loss=2.203, ppl=4.6, wps=20575.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=36500, lr=0.000165521, gnorm=1.134, loss_scale=16, train_wall=287, gb_free=19.9, wall=116062
2022-03-07 20:57:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 21:02:32 | INFO | train_inner | epoch 188:    196 / 196 loss=4.345, nll_loss=2.273, ppl=4.83, wps=20930.2, ups=0.32, wpb=65363.4, bsz=127.7, num_updates=36600, lr=0.000165295, gnorm=1.131, loss_scale=8, train_wall=290, gb_free=19.9, wall=116374
2022-03-07 21:02:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:02:38 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 11.018 | nll_loss 9.732 | ppl 850.57 | wps 41476.9 | wpb 510.9 | bsz 1 | num_updates 36600 | best_loss 8.249
2022-03-07 21:02:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 36600 updates
2022-03-07 21:02:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:02:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:02:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 188 @ 36600 updates, score 11.018) (writing took 2.9855141788721085 seconds)
2022-03-07 21:02:41 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-07 21:02:41 | INFO | train | epoch 188 | loss 4.314 | nll_loss 2.236 | ppl 4.71 | wps 20743.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36600 | lr 0.000165295 | gnorm 1.132 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 116382
2022-03-07 21:02:41 | INFO | fairseq.trainer | begin training epoch 189
2022-03-07 21:02:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:07:51 | INFO | train_inner | epoch 189:    100 / 196 loss=4.278, nll_loss=2.194, ppl=4.58, wps=20577.5, ups=0.31, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=1.104, loss_scale=16, train_wall=288, gb_free=19.9, wall=116692
2022-03-07 21:11:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:12:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:12:53 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 11.085 | nll_loss 9.808 | ppl 896.63 | wps 41595.2 | wpb 510.9 | bsz 1 | num_updates 36795 | best_loss 8.249
2022-03-07 21:12:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 36795 updates
2022-03-07 21:12:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:12:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:12:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 189 @ 36795 updates, score 11.085) (writing took 3.0272572096437216 seconds)
2022-03-07 21:12:56 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-07 21:12:56 | INFO | train | epoch 189 | loss 4.311 | nll_loss 2.233 | ppl 4.7 | wps 20740 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36795 | lr 0.000164856 | gnorm 1.128 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 116998
2022-03-07 21:12:56 | INFO | fairseq.trainer | begin training epoch 190
2022-03-07 21:12:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:13:12 | INFO | train_inner | epoch 190:      5 / 196 loss=4.341, nll_loss=2.269, ppl=4.82, wps=20376.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36800, lr=0.000164845, gnorm=1.152, loss_scale=16, train_wall=290, gb_free=19.9, wall=117013
2022-03-07 21:15:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 21:18:25 | INFO | train_inner | epoch 190:    106 / 196 loss=4.277, nll_loss=2.193, ppl=4.57, wps=20923.4, ups=0.32, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=1.126, loss_scale=8, train_wall=291, gb_free=19.9, wall=117326
2022-03-07 21:23:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:23:08 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 11.01 | nll_loss 9.721 | ppl 844.21 | wps 41595.1 | wpb 510.9 | bsz 1 | num_updates 36990 | best_loss 8.249
2022-03-07 21:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 36990 updates
2022-03-07 21:23:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:23:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 190 @ 36990 updates, score 11.01) (writing took 2.991777260787785 seconds)
2022-03-07 21:23:11 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-07 21:23:11 | INFO | train | epoch 190 | loss 4.309 | nll_loss 2.23 | ppl 4.69 | wps 20737.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36990 | lr 0.000164421 | gnorm 1.132 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 117613
2022-03-07 21:23:11 | INFO | fairseq.trainer | begin training epoch 191
2022-03-07 21:23:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:23:43 | INFO | train_inner | epoch 191:     10 / 196 loss=4.338, nll_loss=2.265, ppl=4.81, wps=20574, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37000, lr=0.000164399, gnorm=1.141, loss_scale=16, train_wall=287, gb_free=19.9, wall=117644
2022-03-07 21:28:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:28:56 | INFO | train_inner | epoch 191:    111 / 196 loss=4.279, nll_loss=2.196, ppl=4.58, wps=20927.9, ups=0.32, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=1.132, loss_scale=16, train_wall=291, gb_free=19.9, wall=117957
2022-03-07 21:31:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 21:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:33:24 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 11.036 | nll_loss 9.752 | ppl 862.46 | wps 41714.6 | wpb 510.9 | bsz 1 | num_updates 37184 | best_loss 8.249
2022-03-07 21:33:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 37184 updates
2022-03-07 21:33:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:33:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 191 @ 37184 updates, score 11.036) (writing took 2.981431436724961 seconds)
2022-03-07 21:33:27 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-07 21:33:27 | INFO | train | epoch 191 | loss 4.307 | nll_loss 2.228 | ppl 4.69 | wps 20639 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 37184 | lr 0.000163992 | gnorm 1.141 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 118228
2022-03-07 21:33:27 | INFO | fairseq.trainer | begin training epoch 192
2022-03-07 21:33:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:34:16 | INFO | train_inner | epoch 192:     16 / 196 loss=4.331, nll_loss=2.258, ppl=4.78, wps=20387.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37200, lr=0.000163956, gnorm=1.138, loss_scale=8, train_wall=290, gb_free=19.9, wall=118278
2022-03-07 21:39:26 | INFO | train_inner | epoch 192:    116 / 196 loss=4.285, nll_loss=2.203, ppl=4.6, wps=21132.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=37300, lr=0.000163737, gnorm=1.129, loss_scale=16, train_wall=288, gb_free=19.9, wall=118588
2022-03-07 21:42:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 21:43:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:43:39 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 11.077 | nll_loss 9.791 | ppl 885.77 | wps 41567.4 | wpb 510.9 | bsz 1 | num_updates 37379 | best_loss 8.249
2022-03-07 21:43:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 37379 updates
2022-03-07 21:43:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:43:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:43:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 192 @ 37379 updates, score 11.077) (writing took 3.060973959043622 seconds)
2022-03-07 21:43:42 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-07 21:43:42 | INFO | train | epoch 192 | loss 4.305 | nll_loss 2.226 | ppl 4.68 | wps 20735.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 37379 | lr 0.000163563 | gnorm 1.139 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 118844
2022-03-07 21:43:42 | INFO | fairseq.trainer | begin training epoch 193
2022-03-07 21:43:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:44:47 | INFO | train_inner | epoch 193:     21 / 196 loss=4.321, nll_loss=2.245, ppl=4.74, wps=20370.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37400, lr=0.000163517, gnorm=1.152, loss_scale=8, train_wall=290, gb_free=19.9, wall=118909
2022-03-07 21:49:57 | INFO | train_inner | epoch 193:    121 / 196 loss=4.285, nll_loss=2.202, ppl=4.6, wps=21133.1, ups=0.32, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=1.126, loss_scale=16, train_wall=288, gb_free=19.9, wall=119219
2022-03-07 21:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:53:54 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 11.096 | nll_loss 9.825 | ppl 907.03 | wps 41512.3 | wpb 510.9 | bsz 1 | num_updates 37575 | best_loss 8.249
2022-03-07 21:53:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 37575 updates
2022-03-07 21:53:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:53:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 21:53:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 193 @ 37575 updates, score 11.096) (writing took 3.3308138959109783 seconds)
2022-03-07 21:53:58 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-07 21:53:58 | INFO | train | epoch 193 | loss 4.302 | nll_loss 2.223 | ppl 4.67 | wps 20835.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 37575 | lr 0.000163136 | gnorm 1.134 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 119459
2022-03-07 21:53:58 | INFO | fairseq.trainer | begin training epoch 194
2022-03-07 21:53:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:55:15 | INFO | train_inner | epoch 194:     25 / 196 loss=4.32, nll_loss=2.244, ppl=4.74, wps=20557.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=37600, lr=0.000163082, gnorm=1.151, loss_scale=16, train_wall=287, gb_free=19.9, wall=119537
2022-03-07 21:55:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:00:29 | INFO | train_inner | epoch 194:    126 / 196 loss=4.282, nll_loss=2.2, ppl=4.59, wps=20923.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=37700, lr=0.000162866, gnorm=1.123, loss_scale=16, train_wall=291, gb_free=19.9, wall=119850
2022-03-07 22:02:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:04:10 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 11.057 | nll_loss 9.777 | ppl 877.65 | wps 41782 | wpb 510.9 | bsz 1 | num_updates 37769 | best_loss 8.249
2022-03-07 22:04:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 37769 updates
2022-03-07 22:04:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:04:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:04:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 194 @ 37769 updates, score 11.057) (writing took 3.297404051758349 seconds)
2022-03-07 22:04:13 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-07 22:04:13 | INFO | train | epoch 194 | loss 4.299 | nll_loss 2.219 | ppl 4.66 | wps 20622.3 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 37769 | lr 0.000162717 | gnorm 1.139 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 120075
2022-03-07 22:04:13 | INFO | fairseq.trainer | begin training epoch 195
2022-03-07 22:04:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:05:50 | INFO | train_inner | epoch 195:     31 / 196 loss=4.306, nll_loss=2.228, ppl=4.68, wps=20351.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=37800, lr=0.00016265, gnorm=1.145, loss_scale=16, train_wall=290, gb_free=19.9, wall=120171
2022-03-07 22:09:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:11:03 | INFO | train_inner | epoch 195:    132 / 196 loss=4.287, nll_loss=2.205, ppl=4.61, wps=20915.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=37900, lr=0.000162435, gnorm=1.123, loss_scale=16, train_wall=291, gb_free=19.9, wall=120485
2022-03-07 22:14:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:14:26 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 11.042 | nll_loss 9.762 | ppl 868.14 | wps 41600.7 | wpb 510.9 | bsz 1 | num_updates 37964 | best_loss 8.249
2022-03-07 22:14:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 37964 updates
2022-03-07 22:14:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:14:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:14:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 195 @ 37964 updates, score 11.042) (writing took 3.310522568412125 seconds)
2022-03-07 22:14:29 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-07 22:14:29 | INFO | train | epoch 195 | loss 4.297 | nll_loss 2.217 | ppl 4.65 | wps 20717.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 37964 | lr 0.000162298 | gnorm 1.13 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 120691
2022-03-07 22:14:30 | INFO | fairseq.trainer | begin training epoch 196
2022-03-07 22:14:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:16:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:16:24 | INFO | train_inner | epoch 196:     37 / 196 loss=4.306, nll_loss=2.228, ppl=4.69, wps=20347.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=38000, lr=0.000162221, gnorm=1.139, loss_scale=16, train_wall=290, gb_free=19.9, wall=120806
2022-03-07 22:21:35 | INFO | train_inner | epoch 196:    137 / 196 loss=4.286, nll_loss=2.204, ppl=4.61, wps=21127.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=38100, lr=0.000162008, gnorm=1.142, loss_scale=16, train_wall=288, gb_free=19.9, wall=121116
2022-03-07 22:22:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:24:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:24:42 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 11.017 | nll_loss 9.73 | ppl 849.03 | wps 41647.9 | wpb 510.9 | bsz 1 | num_updates 38158 | best_loss 8.249
2022-03-07 22:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 38158 updates
2022-03-07 22:24:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:24:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:24:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 196 @ 38158 updates, score 11.017) (writing took 3.320691111497581 seconds)
2022-03-07 22:24:45 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-07 22:24:45 | INFO | train | epoch 196 | loss 4.294 | nll_loss 2.214 | ppl 4.64 | wps 20616 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 38158 | lr 0.000161885 | gnorm 1.144 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 121307
2022-03-07 22:24:45 | INFO | fairseq.trainer | begin training epoch 197
2022-03-07 22:24:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:26:56 | INFO | train_inner | epoch 197:     42 / 196 loss=4.298, nll_loss=2.219, ppl=4.65, wps=20347, ups=0.31, wpb=65367, bsz=127.7, num_updates=38200, lr=0.000161796, gnorm=1.151, loss_scale=16, train_wall=290, gb_free=19.9, wall=121437
2022-03-07 22:29:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:32:09 | INFO | train_inner | epoch 197:    143 / 196 loss=4.293, nll_loss=2.212, ppl=4.63, wps=20918.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=38300, lr=0.000161585, gnorm=1.137, loss_scale=16, train_wall=291, gb_free=19.9, wall=121751
2022-03-07 22:34:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:34:58 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 11.04 | nll_loss 9.754 | ppl 863.47 | wps 41586.3 | wpb 510.9 | bsz 1 | num_updates 38353 | best_loss 8.249
2022-03-07 22:34:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 38353 updates
2022-03-07 22:34:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:35:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 197 @ 38353 updates, score 11.04) (writing took 3.316611398011446 seconds)
2022-03-07 22:35:01 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-07 22:35:01 | INFO | train | epoch 197 | loss 4.293 | nll_loss 2.212 | ppl 4.63 | wps 20720.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 38353 | lr 0.000161473 | gnorm 1.145 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 121923
2022-03-07 22:35:01 | INFO | fairseq.trainer | begin training epoch 198
2022-03-07 22:35:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:36:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:37:30 | INFO | train_inner | epoch 198:     48 / 196 loss=4.29, nll_loss=2.209, ppl=4.62, wps=20349.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=38400, lr=0.000161374, gnorm=1.152, loss_scale=16, train_wall=290, gb_free=19.9, wall=122072
2022-03-07 22:42:40 | INFO | train_inner | epoch 198:    148 / 196 loss=4.29, nll_loss=2.209, ppl=4.62, wps=21125.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=38500, lr=0.000161165, gnorm=1.136, loss_scale=16, train_wall=288, gb_free=19.9, wall=122382
2022-03-07 22:42:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:45:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:45:14 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 11.065 | nll_loss 9.784 | ppl 881.54 | wps 41393.8 | wpb 510.9 | bsz 1 | num_updates 38547 | best_loss 8.249
2022-03-07 22:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 38547 updates
2022-03-07 22:45:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:45:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:45:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 198 @ 38547 updates, score 11.065) (writing took 3.3084012679755688 seconds)
2022-03-07 22:45:17 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-07 22:45:17 | INFO | train | epoch 198 | loss 4.289 | nll_loss 2.208 | ppl 4.62 | wps 20614.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38547 | lr 0.000161066 | gnorm 1.143 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 122539
2022-03-07 22:45:17 | INFO | fairseq.trainer | begin training epoch 199
2022-03-07 22:45:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:48:02 | INFO | train_inner | epoch 199:     53 / 196 loss=4.283, nll_loss=2.201, ppl=4.6, wps=20344.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=38600, lr=0.000160956, gnorm=1.142, loss_scale=16, train_wall=290, gb_free=19.9, wall=122703
2022-03-07 22:49:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:50:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 22:53:18 | INFO | train_inner | epoch 199:    155 / 196 loss=4.292, nll_loss=2.211, ppl=4.63, wps=20713.2, ups=0.32, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=1.142, loss_scale=8, train_wall=294, gb_free=19.9, wall=123020
2022-03-07 22:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:55:30 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 11.058 | nll_loss 9.774 | ppl 875.59 | wps 41570.8 | wpb 510.9 | bsz 1 | num_updates 38741 | best_loss 8.249
2022-03-07 22:55:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 38741 updates
2022-03-07 22:55:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:55:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 22:55:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 199 @ 38741 updates, score 11.058) (writing took 3.3970730379223824 seconds)
2022-03-07 22:55:33 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-07 22:55:33 | INFO | train | epoch 199 | loss 4.287 | nll_loss 2.206 | ppl 4.61 | wps 20610.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38741 | lr 0.000160663 | gnorm 1.142 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 123155
2022-03-07 22:55:33 | INFO | fairseq.trainer | begin training epoch 200
2022-03-07 22:55:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:57:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 22:58:39 | INFO | train_inner | epoch 200:     60 / 196 loss=4.285, nll_loss=2.203, ppl=4.61, wps=20346.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=38800, lr=0.00016054, gnorm=1.149, loss_scale=8, train_wall=290, gb_free=19.9, wall=123341
2022-03-07 23:03:50 | INFO | train_inner | epoch 200:    160 / 196 loss=4.291, nll_loss=2.21, ppl=4.63, wps=21131.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=38900, lr=0.000160334, gnorm=1.142, loss_scale=16, train_wall=288, gb_free=19.9, wall=123651
2022-03-07 23:04:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 23:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:05:46 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 11.043 | nll_loss 9.765 | ppl 870.05 | wps 41511.9 | wpb 510.9 | bsz 1 | num_updates 38935 | best_loss 8.249
2022-03-07 23:05:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 38935 updates
2022-03-07 23:05:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:05:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:05:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 200 @ 38935 updates, score 11.043) (writing took 3.3354875864461064 seconds)
2022-03-07 23:05:49 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-07 23:05:49 | INFO | train | epoch 200 | loss 4.286 | nll_loss 2.204 | ppl 4.61 | wps 20617.3 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 38935 | lr 0.000160262 | gnorm 1.147 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 123771
2022-03-07 23:05:49 | INFO | fairseq.trainer | begin training epoch 201
2022-03-07 23:05:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:09:11 | INFO | train_inner | epoch 201:     65 / 196 loss=4.277, nll_loss=2.193, ppl=4.57, wps=20348.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=39000, lr=0.000160128, gnorm=1.15, loss_scale=8, train_wall=290, gb_free=19.9, wall=123972
2022-03-07 23:14:21 | INFO | train_inner | epoch 201:    165 / 196 loss=4.293, nll_loss=2.212, ppl=4.63, wps=21135.3, ups=0.32, wpb=65536, bsz=128, num_updates=39100, lr=0.000159923, gnorm=1.146, loss_scale=16, train_wall=288, gb_free=19.9, wall=124282
2022-03-07 23:15:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:16:01 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 11.023 | nll_loss 9.742 | ppl 856.17 | wps 41773.1 | wpb 510.9 | bsz 1 | num_updates 39131 | best_loss 8.249
2022-03-07 23:16:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 39131 updates
2022-03-07 23:16:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:16:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:16:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 201 @ 39131 updates, score 11.023) (writing took 3.3497627805918455 seconds)
2022-03-07 23:16:05 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-07 23:16:05 | INFO | train | epoch 201 | loss 4.284 | nll_loss 2.202 | ppl 4.6 | wps 20831.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 39131 | lr 0.00015986 | gnorm 1.145 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 124386
2022-03-07 23:16:05 | INFO | fairseq.trainer | begin training epoch 202
2022-03-07 23:16:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:18:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:19:42 | INFO | train_inner | epoch 202:     70 / 196 loss=4.271, nll_loss=2.187, ppl=4.55, wps=20352, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=39200, lr=0.000159719, gnorm=1.146, loss_scale=16, train_wall=290, gb_free=19.9, wall=124604
2022-03-07 23:24:52 | INFO | train_inner | epoch 202:    170 / 196 loss=4.296, nll_loss=2.216, ppl=4.64, wps=21141.8, ups=0.32, wpb=65536, bsz=128, num_updates=39300, lr=0.000159516, gnorm=1.129, loss_scale=16, train_wall=288, gb_free=19.9, wall=124914
2022-03-07 23:24:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:26:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:26:17 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 11.076 | nll_loss 9.797 | ppl 889.81 | wps 41865 | wpb 510.9 | bsz 1 | num_updates 39325 | best_loss 8.249
2022-03-07 23:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 39325 updates
2022-03-07 23:26:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 202 @ 39325 updates, score 11.076) (writing took 3.328876162879169 seconds)
2022-03-07 23:26:20 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-07 23:26:20 | INFO | train | epoch 202 | loss 4.281 | nll_loss 2.198 | ppl 4.59 | wps 20626.2 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 39325 | lr 0.000159465 | gnorm 1.138 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 125002
2022-03-07 23:26:20 | INFO | fairseq.trainer | begin training epoch 203
2022-03-07 23:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:30:13 | INFO | train_inner | epoch 203:     75 / 196 loss=4.267, nll_loss=2.182, ppl=4.54, wps=20349.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=39400, lr=0.000159313, gnorm=1.143, loss_scale=16, train_wall=290, gb_free=19.9, wall=125235
2022-03-07 23:31:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 23:35:29 | INFO | train_inner | epoch 203:    177 / 196 loss=4.295, nll_loss=2.215, ppl=4.64, wps=20720.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=39500, lr=0.000159111, gnorm=1.155, loss_scale=8, train_wall=294, gb_free=19.9, wall=125551
2022-03-07 23:36:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:36:33 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 11.078 | nll_loss 9.804 | ppl 894.14 | wps 41726 | wpb 510.9 | bsz 1 | num_updates 39519 | best_loss 8.249
2022-03-07 23:36:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 39519 updates
2022-03-07 23:36:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:36:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:36:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 203 @ 39519 updates, score 11.078) (writing took 3.366773644462228 seconds)
2022-03-07 23:36:36 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-07 23:36:36 | INFO | train | epoch 203 | loss 4.279 | nll_loss 2.197 | ppl 4.58 | wps 20616.9 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 39519 | lr 0.000159073 | gnorm 1.144 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 125618
2022-03-07 23:36:36 | INFO | fairseq.trainer | begin training epoch 204
2022-03-07 23:36:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:40:48 | INFO | train_inner | epoch 204:     81 / 196 loss=4.259, nll_loss=2.173, ppl=4.51, wps=20549.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39600, lr=0.00015891, gnorm=1.143, loss_scale=16, train_wall=287, gb_free=19.9, wall=125869
2022-03-07 23:45:58 | INFO | train_inner | epoch 204:    181 / 196 loss=4.299, nll_loss=2.22, ppl=4.66, wps=21123.6, ups=0.32, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=1.137, loss_scale=16, train_wall=288, gb_free=19.9, wall=126179
2022-03-07 23:46:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:46:49 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 11.065 | nll_loss 9.78 | ppl 878.9 | wps 41709.2 | wpb 510.9 | bsz 1 | num_updates 39715 | best_loss 8.249
2022-03-07 23:46:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 39715 updates
2022-03-07 23:46:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:46:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:46:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 204 @ 39715 updates, score 11.065) (writing took 3.356834512203932 seconds)
2022-03-07 23:46:52 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-07 23:46:52 | INFO | train | epoch 204 | loss 4.278 | nll_loss 2.195 | ppl 4.58 | wps 20826.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 39715 | lr 0.00015868 | gnorm 1.144 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 126234
2022-03-07 23:46:52 | INFO | fairseq.trainer | begin training epoch 205
2022-03-07 23:46:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:46:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:51:19 | INFO | train_inner | epoch 205:     86 / 196 loss=4.253, nll_loss=2.166, ppl=4.49, wps=20349.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39800, lr=0.000158511, gnorm=1.161, loss_scale=16, train_wall=290, gb_free=19.9, wall=126501
2022-03-07 23:53:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:56:32 | INFO | train_inner | epoch 205:    187 / 196 loss=4.298, nll_loss=2.218, ppl=4.65, wps=20922.2, ups=0.32, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=1.146, loss_scale=16, train_wall=291, gb_free=19.9, wall=126814
2022-03-07 23:56:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:57:05 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 11.073 | nll_loss 9.798 | ppl 890.31 | wps 41726.4 | wpb 510.9 | bsz 1 | num_updates 39909 | best_loss 8.249
2022-03-07 23:57:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 39909 updates
2022-03-07 23:57:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-07 23:57:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 205 @ 39909 updates, score 11.073) (writing took 3.4014081386849284 seconds)
2022-03-07 23:57:08 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-07 23:57:08 | INFO | train | epoch 205 | loss 4.274 | nll_loss 2.19 | ppl 4.56 | wps 20615.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39909 | lr 0.000158294 | gnorm 1.151 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 126850
2022-03-07 23:57:08 | INFO | fairseq.trainer | begin training epoch 206
2022-03-07 23:57:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:00:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:01:53 | INFO | train_inner | epoch 206:     92 / 196 loss=4.25, nll_loss=2.162, ppl=4.47, wps=20358.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=40000, lr=0.000158114, gnorm=1.139, loss_scale=16, train_wall=290, gb_free=19.9, wall=127135
2022-03-08 00:07:03 | INFO | train_inner | epoch 206:    192 / 196 loss=4.302, nll_loss=2.223, ppl=4.67, wps=21143.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=40100, lr=0.000157917, gnorm=1.143, loss_scale=32, train_wall=288, gb_free=19.9, wall=127445
2022-03-08 00:07:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:07:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:07:20 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 11.067 | nll_loss 9.786 | ppl 883.11 | wps 41715.7 | wpb 510.9 | bsz 1 | num_updates 40103 | best_loss 8.249
2022-03-08 00:07:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 40103 updates
2022-03-08 00:07:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:07:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:07:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 206 @ 40103 updates, score 11.067) (writing took 3.3850326500833035 seconds)
2022-03-08 00:07:24 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-08 00:07:24 | INFO | train | epoch 206 | loss 4.273 | nll_loss 2.189 | ppl 4.56 | wps 20629.9 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 40103 | lr 0.000157911 | gnorm 1.14 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 127465
2022-03-08 00:07:24 | INFO | fairseq.trainer | begin training epoch 207
2022-03-08 00:07:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:12:24 | INFO | train_inner | epoch 207:     97 / 196 loss=4.239, nll_loss=2.149, ppl=4.43, wps=20356.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=40200, lr=0.00015772, gnorm=1.131, loss_scale=16, train_wall=290, gb_free=19.9, wall=127766
2022-03-08 00:13:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:17:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:17:36 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 11.115 | nll_loss 9.839 | ppl 915.94 | wps 41664.7 | wpb 510.9 | bsz 1 | num_updates 40298 | best_loss 8.249
2022-03-08 00:17:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 40298 updates
2022-03-08 00:17:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 207 @ 40298 updates, score 11.115) (writing took 3.3650162275880575 seconds)
2022-03-08 00:17:39 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-08 00:17:39 | INFO | train | epoch 207 | loss 4.272 | nll_loss 2.188 | ppl 4.56 | wps 20727.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 40298 | lr 0.000157528 | gnorm 1.137 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 128081
2022-03-08 00:17:39 | INFO | fairseq.trainer | begin training epoch 208
2022-03-08 00:17:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:17:46 | INFO | train_inner | epoch 208:      2 / 196 loss=4.304, nll_loss=2.226, ppl=4.68, wps=20351.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=40300, lr=0.000157524, gnorm=1.143, loss_scale=16, train_wall=290, gb_free=19.9, wall=128087
2022-03-08 00:20:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:22:59 | INFO | train_inner | epoch 208:    103 / 196 loss=4.241, nll_loss=2.152, ppl=4.44, wps=20923.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=40400, lr=0.000157329, gnorm=1.139, loss_scale=16, train_wall=291, gb_free=19.9, wall=128400
2022-03-08 00:27:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:27:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:27:52 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 11.069 | nll_loss 9.794 | ppl 887.99 | wps 41863.8 | wpb 510.9 | bsz 1 | num_updates 40492 | best_loss 8.249
2022-03-08 00:27:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 40492 updates
2022-03-08 00:27:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:27:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:27:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 208 @ 40492 updates, score 11.069) (writing took 3.3653140580281615 seconds)
2022-03-08 00:27:55 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-08 00:27:55 | INFO | train | epoch 208 | loss 4.268 | nll_loss 2.184 | ppl 4.54 | wps 20618.2 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 40492 | lr 0.00015715 | gnorm 1.148 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 128697
2022-03-08 00:27:55 | INFO | fairseq.trainer | begin training epoch 209
2022-03-08 00:27:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:28:20 | INFO | train_inner | epoch 209:      8 / 196 loss=4.292, nll_loss=2.211, ppl=4.63, wps=20351.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=40500, lr=0.000157135, gnorm=1.158, loss_scale=16, train_wall=290, gb_free=19.9, wall=128722
2022-03-08 00:33:30 | INFO | train_inner | epoch 209:    108 / 196 loss=4.236, nll_loss=2.146, ppl=4.43, wps=21127.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=40600, lr=0.000156941, gnorm=1.138, loss_scale=16, train_wall=288, gb_free=19.9, wall=129032
2022-03-08 00:34:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:38:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:38:07 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 11.075 | nll_loss 9.794 | ppl 887.79 | wps 42092.2 | wpb 510.9 | bsz 1 | num_updates 40687 | best_loss 8.249
2022-03-08 00:38:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 40687 updates
2022-03-08 00:38:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:38:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:38:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 209 @ 40687 updates, score 11.075) (writing took 3.36552463658154 seconds)
2022-03-08 00:38:11 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-08 00:38:11 | INFO | train | epoch 209 | loss 4.266 | nll_loss 2.182 | ppl 4.54 | wps 20727.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 40687 | lr 0.000156773 | gnorm 1.147 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 129312
2022-03-08 00:38:11 | INFO | fairseq.trainer | begin training epoch 210
2022-03-08 00:38:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:38:51 | INFO | train_inner | epoch 210:     13 / 196 loss=4.294, nll_loss=2.214, ppl=4.64, wps=20360.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=40700, lr=0.000156748, gnorm=1.155, loss_scale=16, train_wall=290, gb_free=19.9, wall=129353
2022-03-08 00:40:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:44:04 | INFO | train_inner | epoch 210:    114 / 196 loss=4.245, nll_loss=2.156, ppl=4.46, wps=20919.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=40800, lr=0.000156556, gnorm=1.15, loss_scale=16, train_wall=291, gb_free=19.9, wall=129666
2022-03-08 00:47:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:48:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:48:23 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 11.083 | nll_loss 9.804 | ppl 893.87 | wps 41345.5 | wpb 510.9 | bsz 1 | num_updates 40881 | best_loss 8.249
2022-03-08 00:48:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 40881 updates
2022-03-08 00:48:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:48:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:48:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 210 @ 40881 updates, score 11.083) (writing took 3.411901216953993 seconds)
2022-03-08 00:48:27 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-08 00:48:27 | INFO | train | epoch 210 | loss 4.265 | nll_loss 2.18 | ppl 4.53 | wps 20614.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40881 | lr 0.000156401 | gnorm 1.154 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 129928
2022-03-08 00:48:27 | INFO | fairseq.trainer | begin training epoch 211
2022-03-08 00:48:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:49:26 | INFO | train_inner | epoch 211:     19 / 196 loss=4.281, nll_loss=2.199, ppl=4.59, wps=20347.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=40900, lr=0.000156365, gnorm=1.156, loss_scale=16, train_wall=290, gb_free=19.9, wall=129987
2022-03-08 00:54:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:54:39 | INFO | train_inner | epoch 211:    120 / 196 loss=4.245, nll_loss=2.156, ppl=4.46, wps=20919.9, ups=0.32, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=1.137, loss_scale=16, train_wall=291, gb_free=19.9, wall=130301
2022-03-08 00:57:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 00:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:58:39 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 11.062 | nll_loss 9.786 | ppl 882.86 | wps 41628.3 | wpb 510.9 | bsz 1 | num_updates 41075 | best_loss 8.249
2022-03-08 00:58:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 41075 updates
2022-03-08 00:58:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:58:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 00:58:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 211 @ 41075 updates, score 11.062) (writing took 3.340853253379464 seconds)
2022-03-08 00:58:42 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-08 00:58:42 | INFO | train | epoch 211 | loss 4.263 | nll_loss 2.178 | ppl 4.53 | wps 20619.3 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 41075 | lr 0.000156031 | gnorm 1.149 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 130544
2022-03-08 00:58:42 | INFO | fairseq.trainer | begin training epoch 212
2022-03-08 00:58:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:00:00 | INFO | train_inner | epoch 212:     25 / 196 loss=4.276, nll_loss=2.193, ppl=4.57, wps=20351.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41100, lr=0.000155984, gnorm=1.158, loss_scale=8, train_wall=290, gb_free=19.9, wall=130622
2022-03-08 01:05:10 | INFO | train_inner | epoch 212:    125 / 196 loss=4.248, nll_loss=2.16, ppl=4.47, wps=21141, ups=0.32, wpb=65532.4, bsz=128, num_updates=41200, lr=0.000155794, gnorm=1.154, loss_scale=16, train_wall=288, gb_free=19.9, wall=130932
2022-03-08 01:08:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:08:55 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 11.056 | nll_loss 9.776 | ppl 876.7 | wps 41361.9 | wpb 510.9 | bsz 1 | num_updates 41271 | best_loss 8.249
2022-03-08 01:08:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 41271 updates
2022-03-08 01:08:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:08:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:08:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 212 @ 41271 updates, score 11.056) (writing took 3.4423820096999407 seconds)
2022-03-08 01:08:58 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-08 01:08:58 | INFO | train | epoch 212 | loss 4.261 | nll_loss 2.176 | ppl 4.52 | wps 20827.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 41271 | lr 0.00015566 | gnorm 1.156 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 131160
2022-03-08 01:08:58 | INFO | fairseq.trainer | begin training epoch 213
2022-03-08 01:08:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:10:28 | INFO | train_inner | epoch 213:     29 / 196 loss=4.274, nll_loss=2.191, ppl=4.57, wps=20534.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=41300, lr=0.000155606, gnorm=1.152, loss_scale=16, train_wall=287, gb_free=19.9, wall=131250
2022-03-08 01:11:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:15:42 | INFO | train_inner | epoch 213:    130 / 196 loss=4.247, nll_loss=2.159, ppl=4.47, wps=20919.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=41400, lr=0.000155417, gnorm=1.136, loss_scale=16, train_wall=291, gb_free=19.9, wall=131563
2022-03-08 01:18:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:18:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 01:19:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:19:11 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 11.149 | nll_loss 9.875 | ppl 938.7 | wps 41140 | wpb 510.9 | bsz 1 | num_updates 41464 | best_loss 8.249
2022-03-08 01:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 41464 updates
2022-03-08 01:19:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 213 @ 41464 updates, score 11.149) (writing took 3.5907326359301805 seconds)
2022-03-08 01:19:14 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-08 01:19:14 | INFO | train | epoch 213 | loss 4.258 | nll_loss 2.172 | ppl 4.51 | wps 20501.2 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 41464 | lr 0.000155297 | gnorm 1.145 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 131776
2022-03-08 01:19:15 | INFO | fairseq.trainer | begin training epoch 214
2022-03-08 01:19:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:21:06 | INFO | train_inner | epoch 214:     36 / 196 loss=4.265, nll_loss=2.181, ppl=4.53, wps=20146.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=41500, lr=0.00015523, gnorm=1.159, loss_scale=8, train_wall=293, gb_free=19.9, wall=131888
2022-03-08 01:26:16 | INFO | train_inner | epoch 214:    136 / 196 loss=4.248, nll_loss=2.16, ppl=4.47, wps=21134.5, ups=0.32, wpb=65536, bsz=128, num_updates=41600, lr=0.000155043, gnorm=1.149, loss_scale=16, train_wall=288, gb_free=19.9, wall=132198
2022-03-08 01:27:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 01:29:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:29:27 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 11.07 | nll_loss 9.796 | ppl 889.23 | wps 41477.6 | wpb 510.9 | bsz 1 | num_updates 41659 | best_loss 8.249
2022-03-08 01:29:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 41659 updates
2022-03-08 01:29:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:29:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 214 @ 41659 updates, score 11.07) (writing took 3.372541004791856 seconds)
2022-03-08 01:29:30 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-08 01:29:30 | INFO | train | epoch 214 | loss 4.257 | nll_loss 2.171 | ppl 4.5 | wps 20726.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 41659 | lr 0.000154934 | gnorm 1.155 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 132392
2022-03-08 01:29:30 | INFO | fairseq.trainer | begin training epoch 215
2022-03-08 01:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:31:38 | INFO | train_inner | epoch 215:     41 / 196 loss=4.264, nll_loss=2.179, ppl=4.53, wps=20330.1, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=41700, lr=0.000154857, gnorm=1.162, loss_scale=8, train_wall=290, gb_free=19.9, wall=132519
2022-03-08 01:36:49 | INFO | train_inner | epoch 215:    141 / 196 loss=4.248, nll_loss=2.16, ppl=4.47, wps=21048.8, ups=0.32, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=1.142, loss_scale=16, train_wall=289, gb_free=19.9, wall=132831
2022-03-08 01:39:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:39:44 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 11.059 | nll_loss 9.787 | ppl 883.41 | wps 41542 | wpb 510.9 | bsz 1 | num_updates 41855 | best_loss 8.249
2022-03-08 01:39:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 41855 updates
2022-03-08 01:39:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:39:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:39:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 215 @ 41855 updates, score 11.059) (writing took 3.380869586020708 seconds)
2022-03-08 01:39:48 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-08 01:39:48 | INFO | train | epoch 215 | loss 4.255 | nll_loss 2.168 | ppl 4.49 | wps 20776.5 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 41855 | lr 0.00015457 | gnorm 1.142 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 133009
2022-03-08 01:39:48 | INFO | fairseq.trainer | begin training epoch 216
2022-03-08 01:39:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:39:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 01:42:10 | INFO | train_inner | epoch 216:     46 / 196 loss=4.256, nll_loss=2.17, ppl=4.5, wps=20340.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=41900, lr=0.000154487, gnorm=1.147, loss_scale=8, train_wall=290, gb_free=19.9, wall=133152
2022-03-08 01:47:21 | INFO | train_inner | epoch 216:    146 / 196 loss=4.257, nll_loss=2.171, ppl=4.5, wps=21136.9, ups=0.32, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=1.157, loss_scale=16, train_wall=288, gb_free=19.9, wall=133462
2022-03-08 01:49:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:50:00 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 11.116 | nll_loss 9.851 | ppl 923.49 | wps 41647.5 | wpb 510.9 | bsz 1 | num_updates 42050 | best_loss 8.249
2022-03-08 01:50:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 42050 updates
2022-03-08 01:50:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:50:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 01:50:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 216 @ 42050 updates, score 11.116) (writing took 3.4453289872035384 seconds)
2022-03-08 01:50:04 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-08 01:50:04 | INFO | train | epoch 216 | loss 4.253 | nll_loss 2.166 | ppl 4.49 | wps 20720.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42050 | lr 0.000154212 | gnorm 1.159 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 133625
2022-03-08 01:50:04 | INFO | fairseq.trainer | begin training epoch 217
2022-03-08 01:50:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:52:39 | INFO | train_inner | epoch 217:     50 / 196 loss=4.249, nll_loss=2.161, ppl=4.47, wps=20534.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42100, lr=0.00015412, gnorm=1.155, loss_scale=16, train_wall=287, gb_free=19.9, wall=133780
2022-03-08 01:53:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:57:52 | INFO | train_inner | epoch 217:    151 / 196 loss=4.253, nll_loss=2.166, ppl=4.49, wps=20926.6, ups=0.32, wpb=65536, bsz=128, num_updates=42200, lr=0.000153937, gnorm=1.157, loss_scale=16, train_wall=291, gb_free=19.9, wall=134094
2022-03-08 02:00:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:00:16 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 11.062 | nll_loss 9.783 | ppl 880.73 | wps 41644 | wpb 510.9 | bsz 1 | num_updates 42244 | best_loss 8.249
2022-03-08 02:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 42244 updates
2022-03-08 02:00:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 217 @ 42244 updates, score 11.062) (writing took 3.3521691663190722 seconds)
2022-03-08 02:00:19 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-08 02:00:19 | INFO | train | epoch 217 | loss 4.251 | nll_loss 2.164 | ppl 4.48 | wps 20616 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 42244 | lr 0.000153857 | gnorm 1.151 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 134241
2022-03-08 02:00:19 | INFO | fairseq.trainer | begin training epoch 218
2022-03-08 02:00:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:03:13 | INFO | train_inner | epoch 218:     56 / 196 loss=4.246, nll_loss=2.158, ppl=4.46, wps=20349.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42300, lr=0.000153755, gnorm=1.136, loss_scale=16, train_wall=290, gb_free=19.9, wall=134415
2022-03-08 02:05:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 02:08:26 | INFO | train_inner | epoch 218:    157 / 196 loss=4.254, nll_loss=2.167, ppl=4.49, wps=20920, ups=0.32, wpb=65532.4, bsz=128, num_updates=42400, lr=0.000153574, gnorm=1.157, loss_scale=8, train_wall=291, gb_free=19.9, wall=134728
2022-03-08 02:10:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:10:32 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 11.039 | nll_loss 9.758 | ppl 865.88 | wps 41879.4 | wpb 510.9 | bsz 1 | num_updates 42439 | best_loss 8.249
2022-03-08 02:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 42439 updates
2022-03-08 02:10:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:10:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:10:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 218 @ 42439 updates, score 11.039) (writing took 3.410864681005478 seconds)
2022-03-08 02:10:35 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-08 02:10:35 | INFO | train | epoch 218 | loss 4.249 | nll_loss 2.162 | ppl 4.47 | wps 20723.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42439 | lr 0.000153503 | gnorm 1.149 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 134857
2022-03-08 02:10:35 | INFO | fairseq.trainer | begin training epoch 219
2022-03-08 02:10:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:13:45 | INFO | train_inner | epoch 219:     61 / 196 loss=4.241, nll_loss=2.152, ppl=4.45, wps=20548.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=42500, lr=0.000153393, gnorm=1.158, loss_scale=16, train_wall=287, gb_free=19.9, wall=135046
2022-03-08 02:18:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 02:18:58 | INFO | train_inner | epoch 219:    162 / 196 loss=4.256, nll_loss=2.17, ppl=4.5, wps=20925.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=42600, lr=0.000153213, gnorm=1.144, loss_scale=8, train_wall=291, gb_free=19.9, wall=135359
2022-03-08 02:20:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:20:48 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 11.086 | nll_loss 9.811 | ppl 898.23 | wps 41626.4 | wpb 510.9 | bsz 1 | num_updates 42634 | best_loss 8.249
2022-03-08 02:20:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 42634 updates
2022-03-08 02:20:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:20:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:20:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 219 @ 42634 updates, score 11.086) (writing took 3.4402701258659363 seconds)
2022-03-08 02:20:51 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-08 02:20:51 | INFO | train | epoch 219 | loss 4.246 | nll_loss 2.158 | ppl 4.46 | wps 20723.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42634 | lr 0.000153152 | gnorm 1.151 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 135473
2022-03-08 02:20:51 | INFO | fairseq.trainer | begin training epoch 220
2022-03-08 02:20:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:24:16 | INFO | train_inner | epoch 220:     66 / 196 loss=4.234, nll_loss=2.145, ppl=4.42, wps=20541.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42700, lr=0.000153033, gnorm=1.152, loss_scale=8, train_wall=287, gb_free=19.9, wall=135678
2022-03-08 02:29:26 | INFO | train_inner | epoch 220:    166 / 196 loss=4.256, nll_loss=2.17, ppl=4.5, wps=21127.1, ups=0.32, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=1.17, loss_scale=16, train_wall=288, gb_free=19.9, wall=135988
2022-03-08 02:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:31:04 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 11.107 | nll_loss 9.837 | ppl 914.5 | wps 41820.5 | wpb 510.9 | bsz 1 | num_updates 42830 | best_loss 8.249
2022-03-08 02:31:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 42830 updates
2022-03-08 02:31:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:31:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:31:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 220 @ 42830 updates, score 11.107) (writing took 3.4086396265774965 seconds)
2022-03-08 02:31:07 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-08 02:31:07 | INFO | train | epoch 220 | loss 4.245 | nll_loss 2.157 | ppl 4.46 | wps 20826 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 42830 | lr 0.000152801 | gnorm 1.159 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 136089
2022-03-08 02:31:07 | INFO | fairseq.trainer | begin training epoch 221
2022-03-08 02:31:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:31:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 02:34:50 | INFO | train_inner | epoch 221:     72 / 196 loss=4.232, nll_loss=2.142, ppl=4.41, wps=20155.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=42900, lr=0.000152676, gnorm=1.132, loss_scale=8, train_wall=293, gb_free=19.9, wall=136312
2022-03-08 02:38:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 02:40:04 | INFO | train_inner | epoch 221:    173 / 196 loss=4.255, nll_loss=2.169, ppl=4.5, wps=20927.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=43000, lr=0.000152499, gnorm=1.171, loss_scale=8, train_wall=291, gb_free=19.9, wall=136625
2022-03-08 02:41:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:41:19 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 11.095 | nll_loss 9.825 | ppl 906.82 | wps 41881.6 | wpb 510.9 | bsz 1 | num_updates 43023 | best_loss 8.249
2022-03-08 02:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 43023 updates
2022-03-08 02:41:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 221 @ 43023 updates, score 11.095) (writing took 3.3949780240654945 seconds)
2022-03-08 02:41:23 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-08 02:41:23 | INFO | train | epoch 221 | loss 4.242 | nll_loss 2.153 | ppl 4.45 | wps 20512.7 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 43023 | lr 0.000152458 | gnorm 1.153 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 136704
2022-03-08 02:41:23 | INFO | fairseq.trainer | begin training epoch 222
2022-03-08 02:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:45:22 | INFO | train_inner | epoch 222:     77 / 196 loss=4.223, nll_loss=2.131, ppl=4.38, wps=20547.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=43100, lr=0.000152322, gnorm=1.14, loss_scale=8, train_wall=287, gb_free=19.9, wall=136943
2022-03-08 02:50:32 | INFO | train_inner | epoch 222:    177 / 196 loss=4.265, nll_loss=2.18, ppl=4.53, wps=21125.7, ups=0.32, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=1.155, loss_scale=16, train_wall=288, gb_free=19.9, wall=137254
2022-03-08 02:51:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:51:35 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 11.132 | nll_loss 9.858 | ppl 927.88 | wps 41761 | wpb 510.9 | bsz 1 | num_updates 43219 | best_loss 8.249
2022-03-08 02:51:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 43219 updates
2022-03-08 02:51:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:51:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 02:51:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 222 @ 43219 updates, score 11.132) (writing took 3.4326070062816143 seconds)
2022-03-08 02:51:39 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-08 02:51:39 | INFO | train | epoch 222 | loss 4.242 | nll_loss 2.153 | ppl 4.45 | wps 20822.5 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 43219 | lr 0.000152112 | gnorm 1.148 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 137320
2022-03-08 02:51:39 | INFO | fairseq.trainer | begin training epoch 223
2022-03-08 02:51:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:52:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:55:54 | INFO | train_inner | epoch 223:     82 / 196 loss=4.221, nll_loss=2.128, ppl=4.37, wps=20322.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43300, lr=0.000151969, gnorm=1.162, loss_scale=16, train_wall=290, gb_free=19.9, wall=137575
2022-03-08 02:59:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:01:07 | INFO | train_inner | epoch 223:    183 / 196 loss=4.257, nll_loss=2.171, ppl=4.5, wps=20922.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=43400, lr=0.000151794, gnorm=1.18, loss_scale=16, train_wall=291, gb_free=19.9, wall=137888
2022-03-08 03:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:01:52 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 11.041 | nll_loss 9.768 | ppl 871.89 | wps 41728 | wpb 510.9 | bsz 1 | num_updates 43413 | best_loss 8.249
2022-03-08 03:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 43413 updates
2022-03-08 03:01:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:01:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 223 @ 43413 updates, score 11.041) (writing took 3.444624966941774 seconds)
2022-03-08 03:01:55 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-08 03:01:55 | INFO | train | epoch 223 | loss 4.239 | nll_loss 2.15 | ppl 4.44 | wps 20605.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43413 | lr 0.000151771 | gnorm 1.171 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 137937
2022-03-08 03:01:55 | INFO | fairseq.trainer | begin training epoch 224
2022-03-08 03:01:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:05:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:06:28 | INFO | train_inner | epoch 224:     88 / 196 loss=4.217, nll_loss=2.124, ppl=4.36, wps=20346.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=43500, lr=0.00015162, gnorm=1.159, loss_scale=16, train_wall=290, gb_free=19.9, wall=138210
2022-03-08 03:11:38 | INFO | train_inner | epoch 224:    188 / 196 loss=4.261, nll_loss=2.176, ppl=4.52, wps=21138.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=43600, lr=0.000151446, gnorm=1.183, loss_scale=16, train_wall=288, gb_free=19.9, wall=138520
2022-03-08 03:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:12:07 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 11.071 | nll_loss 9.798 | ppl 889.97 | wps 41847.5 | wpb 510.9 | bsz 1 | num_updates 43608 | best_loss 8.249
2022-03-08 03:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 43608 updates
2022-03-08 03:12:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:12:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:12:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 224 @ 43608 updates, score 11.071) (writing took 3.488043670542538 seconds)
2022-03-08 03:12:11 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-08 03:12:11 | INFO | train | epoch 224 | loss 4.237 | nll_loss 2.148 | ppl 4.43 | wps 20724.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 43608 | lr 0.000151432 | gnorm 1.171 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 138552
2022-03-08 03:12:11 | INFO | fairseq.trainer | begin training epoch 225
2022-03-08 03:12:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:12:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:16:59 | INFO | train_inner | epoch 225:     93 / 196 loss=4.21, nll_loss=2.116, ppl=4.34, wps=20347.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=43700, lr=0.000151272, gnorm=1.167, loss_scale=16, train_wall=290, gb_free=19.9, wall=138841
2022-03-08 03:17:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 03:22:12 | INFO | train_inner | epoch 225:    194 / 196 loss=4.264, nll_loss=2.18, ppl=4.53, wps=20931, ups=0.32, wpb=65532.4, bsz=128, num_updates=43800, lr=0.000151099, gnorm=1.169, loss_scale=8, train_wall=291, gb_free=19.9, wall=139154
2022-03-08 03:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:22:23 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 11.108 | nll_loss 9.835 | ppl 913.49 | wps 41436.8 | wpb 510.9 | bsz 1 | num_updates 43802 | best_loss 8.249
2022-03-08 03:22:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 43802 updates
2022-03-08 03:22:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:22:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:22:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 225 @ 43802 updates, score 11.108) (writing took 3.4379635779187083 seconds)
2022-03-08 03:22:27 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-08 03:22:27 | INFO | train | epoch 225 | loss 4.235 | nll_loss 2.146 | ppl 4.42 | wps 20620.9 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 43802 | lr 0.000151096 | gnorm 1.167 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 139168
2022-03-08 03:22:27 | INFO | fairseq.trainer | begin training epoch 226
2022-03-08 03:22:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:27:31 | INFO | train_inner | epoch 226:     98 / 196 loss=4.208, nll_loss=2.113, ppl=4.32, wps=20533.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43900, lr=0.000150927, gnorm=1.167, loss_scale=16, train_wall=287, gb_free=19.9, wall=139472
2022-03-08 03:31:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:31:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 03:32:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:32:39 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 11.09 | nll_loss 9.813 | ppl 899.51 | wps 41468.9 | wpb 510.9 | bsz 1 | num_updates 43996 | best_loss 8.249
2022-03-08 03:32:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 43996 updates
2022-03-08 03:32:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:32:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:32:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 226 @ 43996 updates, score 11.09) (writing took 3.4410308338701725 seconds)
2022-03-08 03:32:43 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-08 03:32:43 | INFO | train | epoch 226 | loss 4.233 | nll_loss 2.144 | ppl 4.42 | wps 20606.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43996 | lr 0.000150763 | gnorm 1.167 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 139784
2022-03-08 03:32:43 | INFO | fairseq.trainer | begin training epoch 227
2022-03-08 03:32:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:32:55 | INFO | train_inner | epoch 227:      4 / 196 loss=4.257, nll_loss=2.172, ppl=4.51, wps=20144.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=44000, lr=0.000150756, gnorm=1.167, loss_scale=8, train_wall=293, gb_free=19.9, wall=139797
2022-03-08 03:38:05 | INFO | train_inner | epoch 227:    104 / 196 loss=4.206, nll_loss=2.111, ppl=4.32, wps=21134.9, ups=0.32, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=1.157, loss_scale=16, train_wall=288, gb_free=19.9, wall=140107
2022-03-08 03:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:42:55 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 11.083 | nll_loss 9.801 | ppl 891.9 | wps 40436.7 | wpb 510.9 | bsz 1 | num_updates 44192 | best_loss 8.249
2022-03-08 03:42:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 44192 updates
2022-03-08 03:42:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:42:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 227 @ 44192 updates, score 11.083) (writing took 3.400195642374456 seconds)
2022-03-08 03:42:59 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-08 03:42:59 | INFO | train | epoch 227 | loss 4.233 | nll_loss 2.143 | ppl 4.42 | wps 20821.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 44192 | lr 0.000150428 | gnorm 1.161 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 140400
2022-03-08 03:42:59 | INFO | fairseq.trainer | begin training epoch 228
2022-03-08 03:42:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:43:24 | INFO | train_inner | epoch 228:      8 / 196 loss=4.259, nll_loss=2.174, ppl=4.51, wps=20522.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44200, lr=0.000150414, gnorm=1.165, loss_scale=16, train_wall=287, gb_free=19.9, wall=140425
2022-03-08 03:44:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:45:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 03:48:40 | INFO | train_inner | epoch 228:    110 / 196 loss=4.202, nll_loss=2.106, ppl=4.31, wps=20701.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=44300, lr=0.000150244, gnorm=1.136, loss_scale=8, train_wall=294, gb_free=19.9, wall=140742
2022-03-08 03:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:53:12 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 11.098 | nll_loss 9.826 | ppl 907.89 | wps 41990.1 | wpb 510.9 | bsz 1 | num_updates 44386 | best_loss 8.249
2022-03-08 03:53:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 44386 updates
2022-03-08 03:53:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 03:53:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 228 @ 44386 updates, score 11.098) (writing took 9.129450335167348 seconds)
2022-03-08 03:53:21 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-08 03:53:21 | INFO | train | epoch 228 | loss 4.229 | nll_loss 2.139 | ppl 4.4 | wps 20419.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44386 | lr 0.000150099 | gnorm 1.15 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 141022
2022-03-08 03:53:21 | INFO | fairseq.trainer | begin training epoch 229
2022-03-08 03:53:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:54:04 | INFO | train_inner | epoch 229:     14 / 196 loss=4.252, nll_loss=2.165, ppl=4.49, wps=20191.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=44400, lr=0.000150075, gnorm=1.164, loss_scale=16, train_wall=287, gb_free=19.9, wall=141066
2022-03-08 03:59:14 | INFO | train_inner | epoch 229:    114 / 196 loss=4.208, nll_loss=2.113, ppl=4.33, wps=21119, ups=0.32, wpb=65532.4, bsz=128, num_updates=44500, lr=0.000149906, gnorm=1.174, loss_scale=16, train_wall=288, gb_free=19.9, wall=141376
2022-03-08 03:59:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:03:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:03:33 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 11.102 | nll_loss 9.827 | ppl 908.22 | wps 41680 | wpb 510.9 | bsz 1 | num_updates 44581 | best_loss 8.249
2022-03-08 04:03:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 44581 updates
2022-03-08 04:03:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:03:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:03:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 229 @ 44581 updates, score 11.102) (writing took 3.4092474076896906 seconds)
2022-03-08 04:03:37 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-08 04:03:37 | INFO | train | epoch 229 | loss 4.229 | nll_loss 2.139 | ppl 4.4 | wps 20719.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 44581 | lr 0.00014977 | gnorm 1.169 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 141638
2022-03-08 04:03:37 | INFO | fairseq.trainer | begin training epoch 230
2022-03-08 04:03:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:04:36 | INFO | train_inner | epoch 230:     19 / 196 loss=4.249, nll_loss=2.162, ppl=4.48, wps=20349.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=44600, lr=0.000149738, gnorm=1.163, loss_scale=16, train_wall=290, gb_free=19.9, wall=141697
2022-03-08 04:06:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:09:49 | INFO | train_inner | epoch 230:    120 / 196 loss=4.207, nll_loss=2.112, ppl=4.32, wps=20918.1, ups=0.32, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=1.164, loss_scale=16, train_wall=291, gb_free=19.9, wall=142011
2022-03-08 04:12:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:13:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 04:13:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:13:49 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 11.095 | nll_loss 9.825 | ppl 907.05 | wps 41597.1 | wpb 510.9 | bsz 1 | num_updates 44774 | best_loss 8.249
2022-03-08 04:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 44774 updates
2022-03-08 04:13:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:13:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 230 @ 44774 updates, score 11.095) (writing took 3.392742629162967 seconds)
2022-03-08 04:13:53 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-08 04:13:53 | INFO | train | epoch 230 | loss 4.226 | nll_loss 2.135 | ppl 4.39 | wps 20508.7 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 44774 | lr 0.000149447 | gnorm 1.167 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 142254
2022-03-08 04:13:53 | INFO | fairseq.trainer | begin training epoch 231
2022-03-08 04:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:15:13 | INFO | train_inner | epoch 231:     26 / 196 loss=4.242, nll_loss=2.154, ppl=4.45, wps=20155.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44800, lr=0.000149404, gnorm=1.164, loss_scale=8, train_wall=293, gb_free=19.9, wall=142335
2022-03-08 04:20:23 | INFO | train_inner | epoch 231:    126 / 196 loss=4.211, nll_loss=2.117, ppl=4.34, wps=21132, ups=0.32, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=1.16, loss_scale=16, train_wall=288, gb_free=19.9, wall=142645
2022-03-08 04:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:24:05 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 11.085 | nll_loss 9.813 | ppl 899.55 | wps 41725.5 | wpb 510.9 | bsz 1 | num_updates 44970 | best_loss 8.249
2022-03-08 04:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 44970 updates
2022-03-08 04:24:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:24:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:24:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 231 @ 44970 updates, score 11.085) (writing took 3.423335712403059 seconds)
2022-03-08 04:24:08 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-08 04:24:08 | INFO | train | epoch 231 | loss 4.226 | nll_loss 2.135 | ppl 4.39 | wps 20826.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 44970 | lr 0.000149121 | gnorm 1.161 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 142870
2022-03-08 04:24:08 | INFO | fairseq.trainer | begin training epoch 232
2022-03-08 04:24:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:25:42 | INFO | train_inner | epoch 232:     30 / 196 loss=4.237, nll_loss=2.148, ppl=4.43, wps=20535.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45000, lr=0.000149071, gnorm=1.165, loss_scale=16, train_wall=287, gb_free=19.9, wall=142963
2022-03-08 04:26:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:30:55 | INFO | train_inner | epoch 232:    131 / 196 loss=4.213, nll_loss=2.119, ppl=4.34, wps=20914.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=45100, lr=0.000148906, gnorm=1.157, loss_scale=16, train_wall=291, gb_free=19.9, wall=143277
2022-03-08 04:33:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:34:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:34:21 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 11.101 | nll_loss 9.823 | ppl 905.96 | wps 41398 | wpb 510.9 | bsz 1 | num_updates 45164 | best_loss 8.249
2022-03-08 04:34:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 45164 updates
2022-03-08 04:34:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:34:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:34:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 232 @ 45164 updates, score 11.101) (writing took 3.3898269599303603 seconds)
2022-03-08 04:34:25 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-08 04:34:25 | INFO | train | epoch 232 | loss 4.223 | nll_loss 2.131 | ppl 4.38 | wps 20609.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45164 | lr 0.0001488 | gnorm 1.149 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 143486
2022-03-08 04:34:25 | INFO | fairseq.trainer | begin training epoch 233
2022-03-08 04:34:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:36:16 | INFO | train_inner | epoch 233:     36 / 196 loss=4.232, nll_loss=2.142, ppl=4.41, wps=20344.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45200, lr=0.000148741, gnorm=1.15, loss_scale=16, train_wall=290, gb_free=19.9, wall=143598
2022-03-08 04:40:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:41:30 | INFO | train_inner | epoch 233:    137 / 196 loss=4.214, nll_loss=2.121, ppl=4.35, wps=20918.1, ups=0.32, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=1.166, loss_scale=16, train_wall=291, gb_free=19.9, wall=143911
2022-03-08 04:44:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:44:37 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 11.118 | nll_loss 9.847 | ppl 921.09 | wps 41643.8 | wpb 510.9 | bsz 1 | num_updates 45359 | best_loss 8.249
2022-03-08 04:44:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 45359 updates
2022-03-08 04:44:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:44:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:44:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 233 @ 45359 updates, score 11.118) (writing took 3.393403531052172 seconds)
2022-03-08 04:44:40 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-08 04:44:40 | INFO | train | epoch 233 | loss 4.222 | nll_loss 2.13 | ppl 4.38 | wps 20719.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 45359 | lr 0.00014848 | gnorm 1.172 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 144102
2022-03-08 04:44:41 | INFO | fairseq.trainer | begin training epoch 234
2022-03-08 04:44:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:46:48 | INFO | train_inner | epoch 234:     41 / 196 loss=4.227, nll_loss=2.136, ppl=4.4, wps=20547.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=45400, lr=0.000148413, gnorm=1.159, loss_scale=16, train_wall=287, gb_free=19.9, wall=144229
2022-03-08 04:47:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:47:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 04:52:04 | INFO | train_inner | epoch 234:    143 / 196 loss=4.217, nll_loss=2.124, ppl=4.36, wps=20719.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=45500, lr=0.00014825, gnorm=1.184, loss_scale=8, train_wall=294, gb_free=19.9, wall=144546
2022-03-08 04:54:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:54:53 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 11.074 | nll_loss 9.797 | ppl 889.45 | wps 41870.2 | wpb 510.9 | bsz 1 | num_updates 45553 | best_loss 8.249
2022-03-08 04:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 45553 updates
2022-03-08 04:54:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:54:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 04:54:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 234 @ 45553 updates, score 11.074) (writing took 3.425793554633856 seconds)
2022-03-08 04:54:56 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-08 04:54:56 | INFO | train | epoch 234 | loss 4.22 | nll_loss 2.128 | ppl 4.37 | wps 20619.9 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 45553 | lr 0.000148164 | gnorm 1.166 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 144718
2022-03-08 04:54:56 | INFO | fairseq.trainer | begin training epoch 235
2022-03-08 04:54:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:57:22 | INFO | train_inner | epoch 235:     47 / 196 loss=4.218, nll_loss=2.126, ppl=4.36, wps=20543.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=45600, lr=0.000148087, gnorm=1.161, loss_scale=16, train_wall=287, gb_free=19.9, wall=144864
2022-03-08 05:01:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:02:35 | INFO | train_inner | epoch 235:    148 / 196 loss=4.22, nll_loss=2.129, ppl=4.37, wps=20920.8, ups=0.32, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=1.171, loss_scale=16, train_wall=291, gb_free=19.9, wall=145177
2022-03-08 05:05:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:05:09 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 11.089 | nll_loss 9.811 | ppl 898.41 | wps 41598.5 | wpb 510.9 | bsz 1 | num_updates 45748 | best_loss 8.249
2022-03-08 05:05:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 45748 updates
2022-03-08 05:05:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 235 @ 45748 updates, score 11.089) (writing took 3.2103372858837247 seconds)
2022-03-08 05:05:12 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-08 05:05:12 | INFO | train | epoch 235 | loss 4.219 | nll_loss 2.127 | ppl 4.37 | wps 20724.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 45748 | lr 0.000147847 | gnorm 1.171 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 145334
2022-03-08 05:05:12 | INFO | fairseq.trainer | begin training epoch 236
2022-03-08 05:05:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:06:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:07:56 | INFO | train_inner | epoch 236:     53 / 196 loss=4.216, nll_loss=2.124, ppl=4.36, wps=20360, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=45800, lr=0.000147764, gnorm=1.162, loss_scale=8, train_wall=290, gb_free=19.9, wall=145498
2022-03-08 05:13:06 | INFO | train_inner | epoch 236:    153 / 196 loss=4.223, nll_loss=2.132, ppl=4.38, wps=21142.2, ups=0.32, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=1.17, loss_scale=16, train_wall=288, gb_free=19.9, wall=145808
2022-03-08 05:15:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:15:24 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 11.099 | nll_loss 9.834 | ppl 912.8 | wps 41659.2 | wpb 510.9 | bsz 1 | num_updates 45943 | best_loss 8.249
2022-03-08 05:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 45943 updates
2022-03-08 05:15:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:15:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 236 @ 45943 updates, score 11.099) (writing took 3.156013005413115 seconds)
2022-03-08 05:15:28 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-08 05:15:28 | INFO | train | epoch 236 | loss 4.217 | nll_loss 2.124 | ppl 4.36 | wps 20734.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 45943 | lr 0.000147533 | gnorm 1.163 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 145949
2022-03-08 05:15:28 | INFO | fairseq.trainer | begin training epoch 237
2022-03-08 05:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:18:24 | INFO | train_inner | epoch 237:     57 / 196 loss=4.204, nll_loss=2.109, ppl=4.32, wps=20563.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=46000, lr=0.000147442, gnorm=1.153, loss_scale=16, train_wall=287, gb_free=19.9, wall=146126
2022-03-08 05:18:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:23:37 | INFO | train_inner | epoch 237:    158 / 196 loss=4.223, nll_loss=2.132, ppl=4.38, wps=20930.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=46100, lr=0.000147282, gnorm=1.181, loss_scale=8, train_wall=291, gb_free=19.9, wall=146439
2022-03-08 05:25:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:25:40 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 11.135 | nll_loss 9.86 | ppl 929.22 | wps 41602.9 | wpb 510.9 | bsz 1 | num_updates 46138 | best_loss 8.249
2022-03-08 05:25:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 46138 updates
2022-03-08 05:25:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:25:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 237 @ 46138 updates, score 11.135) (writing took 3.1379541605710983 seconds)
2022-03-08 05:25:43 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-08 05:25:43 | INFO | train | epoch 237 | loss 4.215 | nll_loss 2.122 | ppl 4.35 | wps 20740 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 46138 | lr 0.000147221 | gnorm 1.167 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 146564
2022-03-08 05:25:43 | INFO | fairseq.trainer | begin training epoch 238
2022-03-08 05:25:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:28:55 | INFO | train_inner | epoch 238:     62 / 196 loss=4.205, nll_loss=2.11, ppl=4.32, wps=20574.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=46200, lr=0.000147122, gnorm=1.161, loss_scale=16, train_wall=287, gb_free=19.9, wall=146757
2022-03-08 05:31:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:33:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:34:11 | INFO | train_inner | epoch 238:    164 / 196 loss=4.224, nll_loss=2.132, ppl=4.38, wps=20727.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=46300, lr=0.000146964, gnorm=1.169, loss_scale=8, train_wall=294, gb_free=19.9, wall=147073
2022-03-08 05:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:35:55 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 11.091 | nll_loss 9.816 | ppl 901.65 | wps 41470.3 | wpb 510.9 | bsz 1 | num_updates 46332 | best_loss 8.249
2022-03-08 05:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 46332 updates
2022-03-08 05:35:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:35:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:35:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 238 @ 46332 updates, score 11.091) (writing took 3.1606290945783257 seconds)
2022-03-08 05:35:58 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-08 05:35:58 | INFO | train | epoch 238 | loss 4.212 | nll_loss 2.119 | ppl 4.34 | wps 20634.7 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 46332 | lr 0.000146913 | gnorm 1.166 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 147180
2022-03-08 05:35:58 | INFO | fairseq.trainer | begin training epoch 239
2022-03-08 05:35:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:39:29 | INFO | train_inner | epoch 239:     68 / 196 loss=4.199, nll_loss=2.103, ppl=4.3, wps=20565.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=46400, lr=0.000146805, gnorm=1.155, loss_scale=8, train_wall=287, gb_free=19.9, wall=147391
2022-03-08 05:44:39 | INFO | train_inner | epoch 239:    168 / 196 loss=4.226, nll_loss=2.135, ppl=4.39, wps=21133.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=46500, lr=0.000146647, gnorm=1.179, loss_scale=16, train_wall=288, gb_free=19.9, wall=147701
2022-03-08 05:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:46:11 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 11.131 | nll_loss 9.865 | ppl 932.79 | wps 41040.9 | wpb 510.9 | bsz 1 | num_updates 46528 | best_loss 8.249
2022-03-08 05:46:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 46528 updates
2022-03-08 05:46:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:46:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:46:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 239 @ 46528 updates, score 11.131) (writing took 3.148892518132925 seconds)
2022-03-08 05:46:14 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-08 05:46:14 | INFO | train | epoch 239 | loss 4.212 | nll_loss 2.119 | ppl 4.34 | wps 20831.7 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 46528 | lr 0.000146603 | gnorm 1.168 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 147796
2022-03-08 05:46:14 | INFO | fairseq.trainer | begin training epoch 240
2022-03-08 05:46:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:47:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:50:00 | INFO | train_inner | epoch 240:     73 / 196 loss=4.2, nll_loss=2.104, ppl=4.3, wps=20370.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=46600, lr=0.00014649, gnorm=1.152, loss_scale=16, train_wall=290, gb_free=19.9, wall=148022
2022-03-08 05:53:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:53:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:55:16 | INFO | train_inner | epoch 240:    175 / 196 loss=4.224, nll_loss=2.133, ppl=4.39, wps=20751, ups=0.32, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=1.167, loss_scale=8, train_wall=293, gb_free=19.9, wall=148338
2022-03-08 05:56:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:56:25 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 11.098 | nll_loss 9.826 | ppl 907.97 | wps 41611.4 | wpb 510.9 | bsz 1 | num_updates 46721 | best_loss 8.249
2022-03-08 05:56:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 46721 updates
2022-03-08 05:56:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:56:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 05:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 240 @ 46721 updates, score 11.098) (writing took 3.160435772500932 seconds)
2022-03-08 05:56:29 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-08 05:56:29 | INFO | train | epoch 240 | loss 4.209 | nll_loss 2.115 | ppl 4.33 | wps 20549.5 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 46721 | lr 0.0001463 | gnorm 1.159 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 148410
2022-03-08 05:56:29 | INFO | fairseq.trainer | begin training epoch 241
2022-03-08 05:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:00:33 | INFO | train_inner | epoch 241:     79 / 196 loss=4.19, nll_loss=2.093, ppl=4.27, wps=20587.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=46800, lr=0.000146176, gnorm=1.168, loss_scale=8, train_wall=287, gb_free=19.9, wall=148655
2022-03-08 06:05:43 | INFO | train_inner | epoch 241:    179 / 196 loss=4.228, nll_loss=2.138, ppl=4.4, wps=21155.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=46900, lr=0.00014602, gnorm=1.173, loss_scale=16, train_wall=288, gb_free=19.9, wall=148965
2022-03-08 06:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:06:40 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 11.113 | nll_loss 9.845 | ppl 919.63 | wps 41231.5 | wpb 510.9 | bsz 1 | num_updates 46917 | best_loss 8.249
2022-03-08 06:06:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 46917 updates
2022-03-08 06:06:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 06:06:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 06:06:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 241 @ 46917 updates, score 11.113) (writing took 3.1500211590901017 seconds)
2022-03-08 06:06:44 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-08 06:06:44 | INFO | train | epoch 241 | loss 4.209 | nll_loss 2.116 | ppl 4.33 | wps 20860.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 46917 | lr 0.000145994 | gnorm 1.173 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 149025
2022-03-08 06:06:44 | INFO | fairseq.trainer | begin training epoch 242
2022-03-08 06:06:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:06:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 06:11:04 | INFO | train_inner | epoch 242:     84 / 196 loss=4.187, nll_loss=2.09, ppl=4.26, wps=20394.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=47000, lr=0.000145865, gnorm=1.155, loss_scale=8, train_wall=290, gb_free=19.9, wall=149285
2022-03-08 06:16:14 | INFO | train_inner | epoch 242:    184 / 196 loss=4.227, nll_loss=2.136, ppl=4.4, wps=21099.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=47100, lr=0.00014571, gnorm=1.171, loss_scale=16, train_wall=289, gb_free=19.9, wall=149596
2022-03-08 06:16:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:16:56 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 11.115 | nll_loss 9.845 | ppl 919.5 | wps 41244.9 | wpb 510.9 | bsz 1 | num_updates 47112 | best_loss 8.249
2022-03-08 06:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 47112 updates
2022-03-08 06:16:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 06:16:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-03-08 06:16:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 242 @ 47112 updates, score 11.115) (writing took 3.163353690877557 seconds)
2022-03-08 06:16:59 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-08 06:16:59 | INFO | train | epoch 242 | loss 4.206 | nll_loss 2.112 | ppl 4.32 | wps 20725.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 47112 | lr 0.000145692 | gnorm 1.161 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 149641
2022-03-08 06:16:59 | INFO | fairseq.trainer | begin training epoch 243
2022-03-08 06:16:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:20:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4319, in multi_head_attention_forward
    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
