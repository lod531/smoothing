Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 206454222: <w103_fp16_size_0.5_jelinek_0.038_0.002_0.96_#6> in cluster <euler> Exited

Job <w103_fp16_size_0.5_jelinek_0.038_0.002_0.96_#6> was submitted from host <eu-login-45> by user <andriusb> in cluster <euler> at Fri Feb 25 08:58:14 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 25 08:58:41 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 25 08:58:41 2022
Terminated at Sun Feb 27 15:16:44 2022
Results reported at Sun Feb 27 15:16:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.038, 0.002, 0.96)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321676 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   195385.06 sec.
    Max Memory :                                 11061 MB
    Average Memory :                             3187.41 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               8939.00 MB
    Max Swap :                                   99 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   195482 sec.
    Turnaround time :                            195510 sec.

The output (if any) follows:

2022-02-25 08:58:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321676, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321676, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.038, 0.002, 0.96)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-25 08:58:49 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-25 08:58:56 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
Calculating frequency stats:
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 668/900675 [00:00<02:15, 6657.63it/s]  0%|          | 1334/900675 [00:00<02:32, 5896.88it/s]  0%|          | 1930/900675 [00:00<02:37, 5688.64it/s]  0%|          | 2502/900675 [00:00<02:37, 5686.27it/s]  0%|          | 3219/900675 [00:00<02:24, 6197.03it/s]  0%|          | 3843/900675 [00:00<02:25, 6168.66it/s]  1%|          | 4564/900675 [00:00<02:17, 6497.04it/s]  1%|          | 5268/900675 [00:00<02:14, 6665.84it/s]  1%|          | 5982/900675 [00:00<02:11, 6811.90it/s]  1%|          | 6665/900675 [00:01<02:24, 6194.81it/s]  1%|          | 7296/900675 [00:01<02:24, 6177.43it/s]  1%|          | 7922/900675 [00:01<02:26, 6113.95it/s]  1%|          | 8539/900675 [00:01<02:31, 5884.96it/s]  1%|          | 9180/900675 [00:01<02:27, 6026.87it/s]  1%|          | 9828/900675 [00:01<02:24, 6156.88it/s]  1%|          | 10448/900675 [00:01<02:25, 6139.28it/s]  1%|          | 11065/900675 [00:01<02:26, 6071.02it/s]  1%|▏         | 11674/900675 [00:01<02:30, 5911.61it/s]  1%|▏         | 12317/900675 [00:02<02:26, 6061.09it/s]  1%|▏         | 12931/900675 [00:02<02:25, 6081.25it/s]  2%|▏         | 13646/900675 [00:02<02:18, 6382.37it/s]  2%|▏         | 14286/900675 [00:02<02:21, 6264.83it/s]  2%|▏         | 14928/900675 [00:02<02:20, 6305.36it/s]  2%|▏         | 15560/900675 [00:02<02:22, 6191.34it/s]  2%|▏         | 16181/900675 [00:02<02:31, 5821.02it/s]  2%|▏         | 16768/900675 [00:02<02:32, 5801.02it/s]  2%|▏         | 17445/900675 [00:02<02:25, 6075.66it/s]  2%|▏         | 18057/900675 [00:02<02:27, 5970.91it/s]  2%|▏         | 18657/900675 [00:03<02:28, 5933.70it/s]  2%|▏         | 19483/900675 [00:03<02:13, 6609.19it/s]  2%|▏         | 20148/900675 [00:03<02:20, 6272.76it/s]  2%|▏         | 20781/900675 [00:03<02:21, 6197.37it/s]  2%|▏         | 21405/900675 [00:03<02:33, 5723.35it/s]  2%|▏         | 22047/900675 [00:03<02:28, 5912.07it/s]  3%|▎         | 22704/900675 [00:03<02:24, 6091.78it/s]  3%|▎         | 23320/900675 [00:03<02:24, 6068.91it/s]  3%|▎         | 24117/900675 [00:03<02:12, 6616.31it/s]  3%|▎         | 24838/900675 [00:04<02:09, 6780.83it/s]  3%|▎         | 25521/900675 [00:04<02:09, 6740.56it/s]  3%|▎         | 26198/900675 [00:04<02:15, 6436.13it/s]  3%|▎         | 26847/900675 [00:04<02:24, 6062.89it/s]  3%|▎         | 27460/900675 [00:04<02:28, 5871.77it/s]  3%|▎         | 28052/900675 [00:04<02:30, 5788.88it/s]  3%|▎         | 28793/900675 [00:04<02:19, 6239.75it/s]  3%|▎         | 29423/900675 [00:04<02:27, 5921.21it/s]  3%|▎         | 30100/900675 [00:04<02:21, 6140.79it/s]  3%|▎         | 30720/900675 [00:05<02:28, 5853.51it/s]  3%|▎         | 31312/900675 [00:05<02:29, 5816.13it/s]  4%|▎         | 31916/900675 [00:05<02:27, 5876.35it/s]  4%|▎         | 32507/900675 [00:05<02:28, 5859.99it/s]  4%|▎         | 33096/900675 [00:05<02:32, 5671.94it/s]  4%|▎         | 33701/900675 [00:05<02:30, 5774.23it/s]  4%|▍         | 34292/900675 [00:05<02:29, 5806.87it/s]  4%|▍         | 35022/900675 [00:05<02:19, 6227.65it/s]  4%|▍         | 35647/900675 [00:05<02:23, 6019.97it/s]  4%|▍         | 36299/900675 [00:05<02:20, 6159.53it/s]  4%|▍         | 36918/900675 [00:06<02:25, 5954.99it/s]  4%|▍         | 37517/900675 [00:06<02:35, 5551.20it/s]  4%|▍         | 38102/900675 [00:06<02:33, 5633.12it/s]  4%|▍         | 38691/900675 [00:06<02:31, 5699.59it/s]  4%|▍         | 39315/900675 [00:06<02:27, 5842.50it/s]  4%|▍         | 39940/900675 [00:06<02:24, 5950.62it/s]  5%|▍         | 40611/900675 [00:06<02:19, 6169.39it/s]  5%|▍         | 41255/900675 [00:06<02:17, 6244.23it/s]  5%|▍         | 41882/900675 [00:06<02:27, 5813.60it/s]  5%|▍         | 42471/900675 [00:07<02:30, 5714.20it/s]  5%|▍         | 43052/900675 [00:07<02:29, 5741.21it/s]  5%|▍         | 43663/900675 [00:07<02:26, 5847.05it/s]  5%|▍         | 44258/900675 [00:07<02:25, 5870.42it/s]  5%|▍         | 44918/900675 [00:07<02:20, 6078.48it/s]  5%|▌         | 45573/900675 [00:07<02:17, 6214.33it/s]  5%|▌         | 46312/900675 [00:07<02:10, 6558.95it/s]  5%|▌         | 46970/900675 [00:07<02:16, 6236.18it/s]  5%|▌         | 47953/900675 [00:07<01:57, 7262.99it/s]  5%|▌         | 48687/900675 [00:07<01:59, 7134.66it/s]  5%|▌         | 49406/900675 [00:08<01:59, 7116.30it/s]  6%|▌         | 50122/900675 [00:08<02:05, 6758.36it/s]  6%|▌         | 50804/900675 [00:08<02:13, 6377.97it/s]  6%|▌         | 51449/900675 [00:08<02:14, 6303.94it/s]  6%|▌         | 52172/900675 [00:08<02:09, 6559.84it/s]  6%|▌         | 52859/900675 [00:08<02:07, 6639.18it/s]  6%|▌         | 53527/900675 [00:08<02:13, 6327.09it/s]  6%|▌         | 54165/900675 [00:08<02:16, 6202.70it/s]  6%|▌         | 54789/900675 [00:08<02:21, 5975.66it/s]  6%|▌         | 55426/900675 [00:09<02:18, 6081.74it/s]  6%|▌         | 56141/900675 [00:09<02:12, 6386.44it/s]  6%|▋         | 56784/900675 [00:09<02:19, 6044.15it/s]  6%|▋         | 57394/900675 [00:09<02:25, 5781.42it/s]  6%|▋         | 58041/900675 [00:09<02:21, 5970.45it/s]  7%|▋         | 58791/900675 [00:09<02:11, 6401.63it/s]  7%|▋         | 59438/900675 [00:09<02:18, 6059.88it/s]  7%|▋         | 60052/900675 [00:09<02:19, 6030.17it/s]  7%|▋         | 60693/900675 [00:09<02:16, 6132.66it/s]  7%|▋         | 61473/900675 [00:09<02:06, 6608.49it/s]  7%|▋         | 62139/900675 [00:10<02:12, 6334.25it/s]  7%|▋         | 62778/900675 [00:10<02:13, 6285.73it/s]  7%|▋         | 63411/900675 [00:10<02:15, 6161.77it/s]  7%|▋         | 64030/900675 [00:10<02:15, 6158.01it/s]  7%|▋         | 64648/900675 [00:10<02:16, 6113.94it/s]  7%|▋         | 65261/900675 [00:10<02:17, 6059.46it/s]  7%|▋         | 65917/900675 [00:10<02:14, 6196.49it/s]  7%|▋         | 66554/900675 [00:10<02:13, 6247.03it/s]  7%|▋         | 67180/900675 [00:10<02:15, 6150.77it/s]  8%|▊         | 67796/900675 [00:11<02:23, 5784.79it/s]  8%|▊         | 68405/900675 [00:11<02:21, 5868.49it/s]  8%|▊         | 69121/900675 [00:11<02:13, 6238.29it/s]  8%|▊         | 69825/900675 [00:11<02:08, 6465.85it/s]  8%|▊         | 70482/900675 [00:11<02:07, 6491.04it/s]  8%|▊         | 71134/900675 [00:11<02:14, 6166.40it/s]  8%|▊         | 71758/900675 [00:11<02:13, 6186.78it/s]  8%|▊         | 72382/900675 [00:11<02:13, 6200.84it/s]  8%|▊         | 73153/900675 [00:11<02:04, 6642.56it/s]  8%|▊         | 73947/900675 [00:11<01:57, 7022.12it/s]  8%|▊         | 74652/900675 [00:12<01:58, 6997.15it/s]  8%|▊         | 75354/900675 [00:12<02:03, 6667.58it/s]  8%|▊         | 76026/900675 [00:12<02:05, 6555.30it/s]  9%|▊         | 76685/900675 [00:12<02:08, 6412.71it/s]  9%|▊         | 77329/900675 [00:12<02:11, 6242.01it/s]  9%|▊         | 78005/900675 [00:12<02:08, 6385.41it/s]  9%|▊         | 78646/900675 [00:12<02:17, 5987.74it/s]  9%|▉         | 79251/900675 [00:12<02:24, 5671.98it/s]  9%|▉         | 79847/900675 [00:12<02:23, 5739.84it/s]  9%|▉         | 80457/900675 [00:13<02:20, 5835.40it/s]  9%|▉         | 81132/900675 [00:13<02:14, 6094.80it/s]  9%|▉         | 81767/900675 [00:13<02:12, 6167.11it/s]  9%|▉         | 82387/900675 [00:13<02:12, 6172.72it/s]  9%|▉         | 83024/900675 [00:13<02:11, 6224.57it/s]  9%|▉         | 83713/900675 [00:13<02:07, 6417.51it/s]  9%|▉         | 84357/900675 [00:13<02:12, 6158.77it/s]  9%|▉         | 84990/900675 [00:13<02:11, 6207.66it/s] 10%|▉         | 85653/900675 [00:13<02:08, 6329.92it/s] 10%|▉         | 86288/900675 [00:13<02:11, 6209.70it/s] 10%|▉         | 87009/900675 [00:14<02:05, 6499.24it/s] 10%|▉         | 87664/900675 [00:14<02:04, 6512.20it/s] 10%|▉         | 88317/900675 [00:14<02:05, 6447.98it/s] 10%|▉         | 88963/900675 [00:14<02:15, 6003.69it/s] 10%|▉         | 89570/900675 [00:14<02:15, 5980.96it/s] 10%|█         | 90173/900675 [00:14<02:18, 5838.86it/s] 10%|█         | 90761/900675 [00:14<02:20, 5782.30it/s] 10%|█         | 91460/900675 [00:14<02:12, 6126.55it/s] 10%|█         | 92078/900675 [00:14<02:11, 6137.78it/s] 10%|█         | 92783/900675 [00:15<02:06, 6395.78it/s] 10%|█         | 93425/900675 [00:15<02:11, 6127.65it/s] 10%|█         | 94042/900675 [00:15<02:12, 6088.94it/s] 11%|█         | 94754/900675 [00:15<02:06, 6370.22it/s] 11%|█         | 95394/900675 [00:15<02:08, 6262.23it/s] 11%|█         | 96023/900675 [00:15<02:10, 6150.66it/s] 11%|█         | 96898/900675 [00:15<01:56, 6897.17it/s] 11%|█         | 97592/900675 [00:15<02:06, 6366.96it/s] 11%|█         | 98274/900675 [00:15<02:03, 6486.44it/s] 11%|█         | 98931/900675 [00:16<02:08, 6223.22it/s] 11%|█         | 99561/900675 [00:16<02:11, 6084.16it/s] 11%|█         | 100175/900675 [00:16<02:14, 5939.42it/s] 11%|█         | 100773/900675 [00:16<02:15, 5924.82it/s] 11%|█▏        | 101397/900675 [00:16<02:13, 5992.52it/s] 11%|█▏        | 102092/900675 [00:16<02:07, 6265.89it/s] 11%|█▏        | 102721/900675 [00:16<02:09, 6164.16it/s] 11%|█▏        | 103340/900675 [00:16<02:11, 6063.84it/s] 12%|█▏        | 103948/900675 [00:16<02:15, 5895.41it/s] 12%|█▏        | 104586/900675 [00:16<02:11, 6033.70it/s] 12%|█▏        | 105192/900675 [00:17<02:13, 5963.78it/s] 12%|█▏        | 105902/900675 [00:17<02:06, 6293.79it/s] 12%|█▏        | 106534/900675 [00:17<02:15, 5853.96it/s] 12%|█▏        | 107127/900675 [00:17<02:16, 5821.08it/s] 12%|█▏        | 107745/900675 [00:17<02:13, 5921.66it/s] 12%|█▏        | 108341/900675 [00:17<02:14, 5900.68it/s] 12%|█▏        | 108934/900675 [00:17<02:19, 5677.76it/s] 12%|█▏        | 109571/900675 [00:17<02:14, 5873.84it/s] 12%|█▏        | 110164/900675 [00:17<02:14, 5889.26it/s] 12%|█▏        | 110834/900675 [00:18<02:08, 6123.54it/s] 12%|█▏        | 111511/900675 [00:18<02:05, 6312.74it/s] 12%|█▏        | 112145/900675 [00:18<02:07, 6174.25it/s] 13%|█▎        | 112806/900675 [00:18<02:05, 6300.44it/s] 13%|█▎        | 113438/900675 [00:18<02:08, 6137.58it/s] 13%|█▎        | 114054/900675 [00:18<02:09, 6082.76it/s] 13%|█▎        | 114664/900675 [00:18<02:10, 6015.70it/s] 13%|█▎        | 115310/900675 [00:18<02:07, 6143.74it/s] 13%|█▎        | 115926/900675 [00:18<02:15, 5806.74it/s] 13%|█▎        | 116511/900675 [00:18<02:16, 5761.58it/s] 13%|█▎        | 117090/900675 [00:19<02:17, 5709.18it/s] 13%|█▎        | 117687/900675 [00:19<02:15, 5783.49it/s] 13%|█▎        | 118317/900675 [00:19<02:11, 5933.47it/s] 13%|█▎        | 118946/900675 [00:19<02:09, 6031.77it/s] 13%|█▎        | 119551/900675 [00:19<02:11, 5937.77it/s] 13%|█▎        | 120320/900675 [00:19<02:00, 6449.28it/s] 13%|█▎        | 120967/900675 [00:19<02:03, 6304.69it/s] 14%|█▎        | 121600/900675 [00:19<02:08, 6055.30it/s] 14%|█▎        | 122224/900675 [00:19<02:07, 6102.43it/s] 14%|█▎        | 122862/900675 [00:19<02:05, 6178.12it/s] 14%|█▎        | 123482/900675 [00:20<02:11, 5907.92it/s] 14%|█▍        | 124087/900675 [00:20<02:10, 5939.13it/s] 14%|█▍        | 124684/900675 [00:20<02:11, 5911.82it/s] 14%|█▍        | 125316/900675 [00:20<02:08, 6026.27it/s] 14%|█▍        | 125992/900675 [00:20<02:04, 6235.61it/s] 14%|█▍        | 126680/900675 [00:20<02:00, 6419.51it/s] 14%|█▍        | 127324/900675 [00:20<02:04, 6190.14it/s] 14%|█▍        | 127993/900675 [00:20<02:01, 6334.66it/s] 14%|█▍        | 128629/900675 [00:20<02:07, 6052.54it/s] 14%|█▍        | 129284/900675 [00:21<02:04, 6192.68it/s] 14%|█▍        | 129907/900675 [00:21<02:10, 5904.62it/s] 14%|█▍        | 130543/900675 [00:21<02:07, 6028.37it/s] 15%|█▍        | 131173/900675 [00:21<02:06, 6100.11it/s] 15%|█▍        | 131787/900675 [00:21<02:20, 5461.43it/s] 15%|█▍        | 132554/900675 [00:21<02:06, 6052.82it/s] 15%|█▍        | 133176/900675 [00:21<02:06, 6061.38it/s] 15%|█▍        | 133794/900675 [00:21<02:09, 5922.39it/s] 15%|█▍        | 134538/900675 [00:21<02:00, 6348.24it/s] 15%|█▌        | 135279/900675 [00:22<01:55, 6653.19it/s] 15%|█▌        | 135952/900675 [00:22<01:59, 6420.40it/s] 15%|█▌        | 136711/900675 [00:22<01:53, 6753.46it/s] 15%|█▌        | 137422/900675 [00:22<01:51, 6848.30it/s] 15%|█▌        | 138112/900675 [00:22<01:57, 6515.17it/s] 15%|█▌        | 138770/900675 [00:22<01:56, 6512.58it/s] 15%|█▌        | 139426/900675 [00:22<02:01, 6290.86it/s] 16%|█▌        | 140060/900675 [00:22<02:01, 6280.41it/s] 16%|█▌        | 140691/900675 [00:22<02:01, 6271.31it/s] 16%|█▌        | 141320/900675 [00:22<02:06, 5997.19it/s] 16%|█▌        | 141974/900675 [00:23<02:03, 6150.72it/s] 16%|█▌        | 142593/900675 [00:23<02:06, 5997.91it/s] 16%|█▌        | 143196/900675 [00:23<02:09, 5848.47it/s] 16%|█▌        | 143993/900675 [00:23<01:57, 6451.13it/s] 16%|█▌        | 144643/900675 [00:23<01:57, 6442.58it/s] 16%|█▌        | 145337/900675 [00:23<01:54, 6584.71it/s] 16%|█▌        | 145999/900675 [00:23<01:54, 6590.68it/s] 16%|█▋        | 146660/900675 [00:23<02:00, 6265.87it/s] 16%|█▋        | 147291/900675 [00:23<02:03, 6119.66it/s] 16%|█▋        | 147907/900675 [00:24<02:07, 5888.54it/s] 16%|█▋        | 148540/900675 [00:24<02:05, 6011.74it/s] 17%|█▋        | 149191/900675 [00:24<02:02, 6147.25it/s] 17%|█▋        | 149809/900675 [00:24<02:05, 5987.80it/s] 17%|█▋        | 150411/900675 [00:24<02:07, 5886.76it/s] 17%|█▋        | 151002/900675 [00:24<02:09, 5802.46it/s] 17%|█▋        | 151637/900675 [00:24<02:05, 5957.39it/s] 17%|█▋        | 152235/900675 [00:24<02:12, 5658.73it/s] 17%|█▋        | 152849/900675 [00:24<02:09, 5791.13it/s] 17%|█▋        | 153554/900675 [00:24<02:01, 6147.59it/s] 17%|█▋        | 154173/900675 [00:25<02:07, 5834.87it/s] 17%|█▋        | 154762/900675 [00:25<02:07, 5847.64it/s] 17%|█▋        | 155436/900675 [00:25<02:02, 6102.27it/s] 17%|█▋        | 156083/900675 [00:25<02:00, 6204.82it/s] 17%|█▋        | 156707/900675 [00:25<02:02, 6092.43it/s] 17%|█▋        | 157414/900675 [00:25<01:56, 6377.11it/s] 18%|█▊        | 158100/900675 [00:25<01:53, 6515.36it/s] 18%|█▊        | 158758/900675 [00:25<01:53, 6529.32it/s] 18%|█▊        | 159413/900675 [00:25<01:59, 6202.37it/s] 18%|█▊        | 160108/900675 [00:26<01:55, 6409.12it/s] 18%|█▊        | 160753/900675 [00:26<01:59, 6190.69it/s] 18%|█▊        | 161411/900675 [00:26<01:57, 6300.59it/s] 18%|█▊        | 162045/900675 [00:26<01:59, 6200.22it/s] 18%|█▊        | 162711/900675 [00:26<01:56, 6332.70it/s] 18%|█▊        | 163363/900675 [00:26<01:55, 6387.17it/s] 18%|█▊        | 164004/900675 [00:26<02:02, 5990.05it/s] 18%|█▊        | 164632/900675 [00:26<02:01, 6071.93it/s] 18%|█▊        | 165346/900675 [00:26<01:55, 6373.36it/s] 18%|█▊        | 166013/900675 [00:26<01:53, 6455.55it/s] 19%|█▊        | 166703/900675 [00:27<01:51, 6581.42it/s] 19%|█▊        | 167364/900675 [00:27<01:52, 6527.39it/s] 19%|█▊        | 168019/900675 [00:27<02:00, 6095.08it/s] 19%|█▊        | 168702/900675 [00:27<01:56, 6299.18it/s] 19%|█▉        | 169338/900675 [00:27<01:57, 6218.37it/s] 19%|█▉        | 169964/900675 [00:27<02:02, 5988.42it/s] 19%|█▉        | 170567/900675 [00:27<02:04, 5876.31it/s] 19%|█▉        | 171158/900675 [00:27<02:06, 5781.58it/s] 19%|█▉        | 171824/900675 [00:27<02:00, 6029.36it/s] 19%|█▉        | 172430/900675 [00:28<02:03, 5904.23it/s] 19%|█▉        | 173040/900675 [00:28<02:02, 5960.22it/s] 19%|█▉        | 173662/900675 [00:28<02:00, 6035.78it/s] 19%|█▉        | 174267/900675 [00:28<02:02, 5930.01it/s] 19%|█▉        | 175000/900675 [00:28<01:54, 6332.51it/s] 20%|█▉        | 175636/900675 [00:28<02:01, 5958.25it/s] 20%|█▉        | 176251/900675 [00:28<02:00, 6007.04it/s] 20%|█▉        | 176856/900675 [00:28<02:03, 5858.13it/s] 20%|█▉        | 177445/900675 [00:28<02:08, 5635.35it/s] 20%|█▉        | 178012/900675 [00:28<02:09, 5582.02it/s] 20%|█▉        | 178694/900675 [00:29<02:01, 5929.97it/s] 20%|█▉        | 179296/900675 [00:29<02:01, 5955.86it/s] 20%|█▉        | 179894/900675 [00:29<02:01, 5912.13it/s] 20%|██        | 180579/900675 [00:29<01:56, 6182.44it/s] 20%|██        | 181200/900675 [00:29<02:02, 5854.49it/s] 20%|██        | 181790/900675 [00:29<02:04, 5776.34it/s] 20%|██        | 182371/900675 [00:29<02:05, 5718.78it/s] 20%|██        | 183009/900675 [00:29<02:01, 5906.46it/s] 20%|██        | 183602/900675 [00:29<02:03, 5789.73it/s] 20%|██        | 184183/900675 [00:30<02:14, 5345.51it/s] 21%|██        | 184870/900675 [00:30<02:04, 5762.10it/s] 21%|██        | 185455/900675 [00:30<02:07, 5588.44it/s] 21%|██        | 186023/900675 [00:30<02:07, 5612.46it/s] 21%|██        | 186601/900675 [00:30<02:06, 5657.33it/s] 21%|██        | 187240/900675 [00:30<02:01, 5854.89it/s] 21%|██        | 187855/900675 [00:30<02:00, 5937.20it/s] 21%|██        | 188513/900675 [00:30<01:56, 6126.45it/s] 21%|██        | 189175/900675 [00:30<01:53, 6263.64it/s] 21%|██        | 189803/900675 [00:30<01:56, 6099.79it/s] 21%|██        | 190441/900675 [00:31<01:54, 6177.53it/s] 21%|██        | 191110/900675 [00:31<01:52, 6325.23it/s] 21%|██▏       | 191744/900675 [00:31<01:56, 6111.36it/s] 21%|██▏       | 192383/900675 [00:31<01:54, 6189.71it/s] 21%|██▏       | 193004/900675 [00:31<01:54, 6176.53it/s] 21%|██▏       | 193641/900675 [00:31<01:53, 6227.97it/s] 22%|██▏       | 194269/900675 [00:31<01:53, 6242.24it/s] 22%|██▏       | 194894/900675 [00:31<01:53, 6200.61it/s] 22%|██▏       | 195524/900675 [00:31<01:53, 6227.90it/s] 22%|██▏       | 196148/900675 [00:32<01:56, 6052.47it/s] 22%|██▏       | 196755/900675 [00:32<01:58, 5961.38it/s] 22%|██▏       | 197761/900675 [00:32<01:38, 7154.26it/s] 22%|██▏       | 198481/900675 [00:32<01:49, 6438.75it/s] 22%|██▏       | 199141/900675 [00:32<01:50, 6345.96it/s] 22%|██▏       | 199787/900675 [00:32<01:55, 6056.63it/s] 22%|██▏       | 200402/900675 [00:32<01:58, 5930.98it/s] 22%|██▏       | 201001/900675 [00:32<01:58, 5908.86it/s] 22%|██▏       | 201596/900675 [00:32<01:59, 5843.76it/s] 22%|██▏       | 202232/900675 [00:32<01:56, 5990.27it/s] 23%|██▎       | 202834/900675 [00:33<01:58, 5911.50it/s] 23%|██▎       | 203535/900675 [00:33<01:51, 6225.20it/s] 23%|██▎       | 204160/900675 [00:33<01:51, 6223.00it/s] 23%|██▎       | 204784/900675 [00:33<01:56, 5972.69it/s] 23%|██▎       | 205403/900675 [00:33<01:55, 6026.66it/s] 23%|██▎       | 206008/900675 [00:33<01:56, 5952.82it/s] 23%|██▎       | 206632/900675 [00:33<01:55, 6033.15it/s] 23%|██▎       | 207237/900675 [00:33<01:57, 5882.22it/s] 23%|██▎       | 207845/900675 [00:33<01:56, 5933.49it/s] 23%|██▎       | 208440/900675 [00:34<01:58, 5850.40it/s] 23%|██▎       | 209026/900675 [00:34<02:04, 5552.20it/s] 23%|██▎       | 209684/900675 [00:34<01:58, 5842.42it/s] 23%|██▎       | 210336/900675 [00:34<01:54, 6033.86it/s] 23%|██▎       | 210987/900675 [00:34<01:51, 6171.65it/s] 23%|██▎       | 211608/900675 [00:34<01:52, 6103.96it/s] 24%|██▎       | 212221/900675 [00:34<01:52, 6111.16it/s] 24%|██▎       | 212941/900675 [00:34<01:46, 6432.32it/s] 24%|██▎       | 213586/900675 [00:34<01:53, 6058.70it/s] 24%|██▍       | 214198/900675 [00:34<01:54, 5983.44it/s] 24%|██▍       | 214800/900675 [00:35<02:04, 5492.32it/s] 24%|██▍       | 215399/900675 [00:35<02:01, 5625.06it/s] 24%|██▍       | 215987/900675 [00:35<02:00, 5695.22it/s] 24%|██▍       | 216563/900675 [00:35<02:02, 5601.26it/s] 24%|██▍       | 217160/900675 [00:35<01:59, 5699.87it/s] 24%|██▍       | 217809/900675 [00:35<01:55, 5928.71it/s] 24%|██▍       | 218479/900675 [00:35<01:50, 6153.33it/s] 24%|██▍       | 219123/900675 [00:35<01:49, 6237.77it/s] 24%|██▍       | 219749/900675 [00:35<01:53, 6009.35it/s] 24%|██▍       | 220373/900675 [00:36<01:52, 6070.48it/s] 25%|██▍       | 220983/900675 [00:36<01:52, 6043.26it/s] 25%|██▍       | 221594/900675 [00:36<01:52, 6061.81it/s] 25%|██▍       | 222202/900675 [00:36<01:55, 5883.13it/s] 25%|██▍       | 222793/900675 [00:36<01:55, 5876.26it/s] 25%|██▍       | 223382/900675 [00:36<02:03, 5484.20it/s] 25%|██▍       | 223995/900675 [00:36<01:59, 5662.20it/s] 25%|██▍       | 224675/900675 [00:36<01:52, 5983.63it/s] 25%|██▌       | 225354/900675 [00:36<01:48, 6216.34it/s] 25%|██▌       | 226124/900675 [00:36<01:41, 6647.02it/s] 25%|██▌       | 226793/900675 [00:37<01:46, 6340.75it/s] 25%|██▌       | 227433/900675 [00:37<01:48, 6217.04it/s] 25%|██▌       | 228059/900675 [00:37<01:52, 5988.36it/s] 25%|██▌       | 228662/900675 [00:37<01:53, 5906.69it/s] 25%|██▌       | 229256/900675 [00:37<01:54, 5872.81it/s] 26%|██▌       | 229845/900675 [00:37<02:01, 5516.13it/s] 26%|██▌       | 230487/900675 [00:37<01:56, 5759.93it/s] 26%|██▌       | 231105/900675 [00:37<01:53, 5874.66it/s] 26%|██▌       | 231697/900675 [00:37<01:56, 5765.71it/s] 26%|██▌       | 232277/900675 [00:38<01:55, 5774.94it/s] 26%|██▌       | 233001/900675 [00:38<01:47, 6196.71it/s] 26%|██▌       | 233722/900675 [00:38<01:42, 6489.88it/s] 26%|██▌       | 234389/900675 [00:38<01:41, 6542.45it/s] 26%|██▌       | 235046/900675 [00:38<01:42, 6500.76it/s] 26%|██▌       | 235710/900675 [00:38<01:41, 6537.24it/s] 26%|██▌       | 236365/900675 [00:38<01:48, 6144.11it/s] 26%|██▋       | 236985/900675 [00:38<01:49, 6051.13it/s] 26%|██▋       | 237609/900675 [00:38<01:48, 6104.77it/s] 26%|██▋       | 238223/900675 [00:38<01:51, 5947.94it/s] 27%|██▋       | 238943/900675 [00:39<01:45, 6298.68it/s] 27%|██▋       | 239577/900675 [00:39<01:46, 6233.84it/s] 27%|██▋       | 240203/900675 [00:39<01:51, 5930.78it/s] 27%|██▋       | 241000/900675 [00:39<01:41, 6506.37it/s] 27%|██▋       | 241657/900675 [00:39<01:41, 6513.21it/s] 27%|██▋       | 242313/900675 [00:39<01:40, 6524.01it/s] 27%|██▋       | 242969/900675 [00:39<01:44, 6314.80it/s] 27%|██▋       | 243632/900675 [00:39<01:42, 6398.71it/s] 27%|██▋       | 244275/900675 [00:39<01:46, 6185.63it/s] 27%|██▋       | 244899/900675 [00:40<01:45, 6193.24it/s] 27%|██▋       | 245535/900675 [00:40<01:44, 6240.25it/s] 27%|██▋       | 246213/900675 [00:40<01:42, 6392.66it/s] 27%|██▋       | 246972/900675 [00:40<01:36, 6745.16it/s] 27%|██▋       | 247649/900675 [00:40<01:40, 6487.79it/s] 28%|██▊       | 248301/900675 [00:40<01:43, 6296.53it/s] 28%|██▊       | 249005/900675 [00:40<01:40, 6502.63it/s] 28%|██▊       | 249707/900675 [00:40<01:38, 6639.63it/s] 28%|██▊       | 250374/900675 [00:40<01:42, 6343.62it/s] 28%|██▊       | 251030/900675 [00:40<01:41, 6401.98it/s] 28%|██▊       | 251678/900675 [00:41<01:41, 6418.35it/s] 28%|██▊       | 252323/900675 [00:41<01:42, 6353.95it/s] 28%|██▊       | 252960/900675 [00:41<01:44, 6211.68it/s] 28%|██▊       | 253583/900675 [00:41<01:44, 6202.91it/s] 28%|██▊       | 254258/900675 [00:41<01:41, 6362.22it/s] 28%|██▊       | 254896/900675 [00:41<01:45, 6148.68it/s] 28%|██▊       | 255513/900675 [00:41<01:46, 6068.28it/s] 28%|██▊       | 256213/900675 [00:41<01:41, 6337.65it/s] 29%|██▊       | 256904/900675 [00:41<01:39, 6500.96it/s] 29%|██▊       | 257556/900675 [00:42<01:39, 6451.44it/s] 29%|██▊       | 258203/900675 [00:42<01:45, 6082.71it/s] 29%|██▊       | 258817/900675 [00:42<01:49, 5865.33it/s] 29%|██▉       | 259408/900675 [00:42<01:51, 5772.31it/s] 29%|██▉       | 260020/900675 [00:42<01:49, 5863.03it/s] 29%|██▉       | 260819/900675 [00:42<01:38, 6465.27it/s] 29%|██▉       | 261470/900675 [00:42<01:42, 6216.46it/s] 29%|██▉       | 262097/900675 [00:42<01:45, 6043.92it/s] 29%|██▉       | 262705/900675 [00:42<01:47, 5934.11it/s] 29%|██▉       | 263301/900675 [00:43<01:48, 5891.12it/s] 29%|██▉       | 263928/900675 [00:43<01:46, 5992.35it/s] 29%|██▉       | 264529/900675 [00:43<01:47, 5899.05it/s] 29%|██▉       | 265163/900675 [00:43<01:45, 6018.73it/s] 30%|██▉       | 265767/900675 [00:43<01:47, 5909.03it/s] 30%|██▉       | 266362/900675 [00:43<01:47, 5920.77it/s] 30%|██▉       | 266955/900675 [00:43<01:49, 5769.35it/s] 30%|██▉       | 267565/900675 [00:43<01:48, 5859.50it/s] 30%|██▉       | 268153/900675 [00:43<01:48, 5852.55it/s] 30%|██▉       | 268782/900675 [00:43<01:45, 5978.53it/s] 30%|██▉       | 269446/900675 [00:44<01:42, 6170.73it/s] 30%|██▉       | 270064/900675 [00:44<01:44, 6053.46it/s] 30%|███       | 270697/900675 [00:44<01:42, 6133.66it/s] 30%|███       | 271312/900675 [00:44<01:43, 6089.58it/s] 30%|███       | 271972/900675 [00:44<01:40, 6235.46it/s] 30%|███       | 272779/900675 [00:44<01:32, 6767.40it/s] 30%|███       | 273458/900675 [00:44<01:32, 6770.27it/s] 30%|███       | 274136/900675 [00:44<01:34, 6647.69it/s] 31%|███       | 274802/900675 [00:44<01:40, 6233.28it/s] 31%|███       | 275431/900675 [00:44<01:43, 6016.05it/s] 31%|███       | 276038/900675 [00:45<01:43, 6021.47it/s] 31%|███       | 276644/900675 [00:45<01:48, 5735.50it/s] 31%|███       | 277257/900675 [00:45<01:46, 5845.42it/s] 31%|███       | 277870/900675 [00:45<01:45, 5925.20it/s] 31%|███       | 278563/900675 [00:45<01:40, 6212.99it/s] 31%|███       | 279188/900675 [00:45<01:43, 5983.38it/s] 31%|███       | 279801/900675 [00:45<01:43, 6016.57it/s] 31%|███       | 280490/900675 [00:45<01:39, 6260.38it/s] 31%|███       | 281119/900675 [00:45<01:44, 5923.89it/s] 31%|███▏      | 281778/900675 [00:46<01:41, 6106.90it/s] 31%|███▏      | 282394/900675 [00:46<01:46, 5798.29it/s] 31%|███▏      | 283071/900675 [00:46<01:41, 6069.06it/s] 32%|███▏      | 283723/900675 [00:46<01:39, 6191.65it/s] 32%|███▏      | 284347/900675 [00:46<01:39, 6174.72it/s] 32%|███▏      | 284968/900675 [00:46<01:41, 6038.06it/s] 32%|███▏      | 285578/900675 [00:46<01:41, 6051.99it/s] 32%|███▏      | 286204/900675 [00:46<01:40, 6110.23it/s] 32%|███▏      | 286817/900675 [00:46<01:41, 6050.08it/s] 32%|███▏      | 287424/900675 [00:46<01:43, 5937.83it/s] 32%|███▏      | 288125/900675 [00:47<01:38, 6249.92it/s] 32%|███▏      | 288752/900675 [00:47<01:38, 6217.48it/s] 32%|███▏      | 289376/900675 [00:47<01:38, 6216.79it/s] 32%|███▏      | 289999/900675 [00:47<01:44, 5870.39it/s] 32%|███▏      | 290612/900675 [00:47<01:42, 5937.39it/s] 32%|███▏      | 291214/900675 [00:47<01:42, 5953.89it/s] 32%|███▏      | 291812/900675 [00:47<01:42, 5951.55it/s] 32%|███▏      | 292493/900675 [00:47<01:38, 6201.44it/s] 33%|███▎      | 293115/900675 [00:47<01:43, 5864.74it/s] 33%|███▎      | 293723/900675 [00:48<01:42, 5921.80it/s] 33%|███▎      | 294319/900675 [00:48<01:42, 5928.87it/s] 33%|███▎      | 295036/900675 [00:48<01:36, 6288.80it/s] 33%|███▎      | 295668/900675 [00:48<01:36, 6243.93it/s] 33%|███▎      | 296295/900675 [00:48<01:38, 6162.99it/s] 33%|███▎      | 296937/900675 [00:48<01:36, 6232.27it/s] 33%|███▎      | 297562/900675 [00:48<01:39, 6071.75it/s] 33%|███▎      | 298244/900675 [00:48<01:35, 6286.72it/s] 33%|███▎      | 298875/900675 [00:48<01:37, 6169.20it/s] 33%|███▎      | 299542/900675 [00:48<01:35, 6311.35it/s] 33%|███▎      | 300175/900675 [00:49<01:35, 6268.70it/s] 33%|███▎      | 300803/900675 [00:49<01:37, 6169.95it/s] 33%|███▎      | 301421/900675 [00:49<01:39, 6010.42it/s] 34%|███▎      | 302024/900675 [00:49<01:43, 5771.68it/s] 34%|███▎      | 302711/900675 [00:49<01:38, 6082.98it/s] 34%|███▎      | 303368/900675 [00:49<01:36, 6221.10it/s] 34%|███▍      | 304023/900675 [00:49<01:34, 6311.92it/s] 34%|███▍      | 304657/900675 [00:49<01:38, 6065.75it/s] 34%|███▍      | 305299/900675 [00:49<01:36, 6164.81it/s] 34%|███▍      | 305919/900675 [00:49<01:38, 6036.56it/s] 34%|███▍      | 306525/900675 [00:50<01:39, 5986.97it/s] 34%|███▍      | 307126/900675 [00:50<01:39, 5978.88it/s] 34%|███▍      | 307815/900675 [00:50<01:35, 6239.73it/s] 34%|███▍      | 308441/900675 [00:50<01:45, 5624.27it/s] 34%|███▍      | 309245/900675 [00:50<01:34, 6277.41it/s] 34%|███▍      | 309992/900675 [00:50<01:29, 6613.00it/s] 34%|███▍      | 310666/900675 [00:50<01:31, 6423.12it/s] 35%|███▍      | 311318/900675 [00:50<01:33, 6292.37it/s] 35%|███▍      | 311962/900675 [00:50<01:32, 6333.97it/s] 35%|███▍      | 312607/900675 [00:51<01:32, 6367.19it/s] 35%|███▍      | 313251/900675 [00:51<01:32, 6384.48it/s] 35%|███▍      | 313977/900675 [00:51<01:28, 6641.05it/s] 35%|███▍      | 314644/900675 [00:51<01:31, 6374.23it/s] 35%|███▌      | 315286/900675 [00:51<01:37, 6008.70it/s] 35%|███▌      | 315918/900675 [00:51<01:35, 6093.17it/s] 35%|███▌      | 316533/900675 [00:51<01:38, 5940.29it/s] 35%|███▌      | 317140/900675 [00:51<01:37, 5970.39it/s] 35%|███▌      | 317740/900675 [00:51<01:43, 5658.93it/s] 35%|███▌      | 318411/900675 [00:52<01:37, 5952.96it/s] 35%|███▌      | 319012/900675 [00:52<01:37, 5945.00it/s] 35%|███▌      | 319641/900675 [00:52<01:36, 6037.85it/s] 36%|███▌      | 320248/900675 [00:52<01:38, 5893.19it/s] 36%|███▌      | 320886/900675 [00:52<01:36, 6033.54it/s] 36%|███▌      | 321576/900675 [00:52<01:32, 6283.61it/s] 36%|███▌      | 322207/900675 [00:52<01:37, 5919.51it/s] 36%|███▌      | 322805/900675 [00:52<01:38, 5857.91it/s] 36%|███▌      | 323427/900675 [00:52<01:36, 5957.95it/s] 36%|███▌      | 324052/900675 [00:52<01:35, 6040.81it/s] 36%|███▌      | 324690/900675 [00:53<01:33, 6138.34it/s] 36%|███▌      | 325306/900675 [00:53<01:39, 5758.72it/s] 36%|███▌      | 325893/900675 [00:53<01:39, 5782.18it/s] 36%|███▋      | 326534/900675 [00:53<01:36, 5957.37it/s] 36%|███▋      | 327264/900675 [00:53<01:30, 6338.69it/s] 36%|███▋      | 328026/900675 [00:53<01:25, 6713.56it/s] 36%|███▋      | 328733/900675 [00:53<01:23, 6813.57it/s] 37%|███▋      | 329417/900675 [00:53<01:29, 6365.27it/s] 37%|███▋      | 330061/900675 [00:53<01:31, 6202.63it/s] 37%|███▋      | 330687/900675 [00:54<01:37, 5825.56it/s] 37%|███▋      | 331294/900675 [00:54<01:36, 5890.23it/s] 37%|███▋      | 331891/900675 [00:54<01:36, 5911.78it/s] 37%|███▋      | 332565/900675 [00:54<01:32, 6148.17it/s] 37%|███▋      | 333184/900675 [00:54<01:35, 5964.01it/s] 37%|███▋      | 333843/900675 [00:54<01:32, 6142.95it/s] 37%|███▋      | 334461/900675 [00:54<01:33, 6082.35it/s] 37%|███▋      | 335086/900675 [00:54<01:32, 6129.53it/s] 37%|███▋      | 335737/900675 [00:54<01:30, 6233.30it/s] 37%|███▋      | 336362/900675 [00:54<01:31, 6153.86it/s] 37%|███▋      | 337015/900675 [00:55<01:30, 6258.75it/s] 37%|███▋      | 337702/900675 [00:55<01:27, 6436.77it/s] 38%|███▊      | 338347/900675 [00:55<01:28, 6372.00it/s] 38%|███▊      | 339022/900675 [00:55<01:26, 6481.59it/s] 38%|███▊      | 339671/900675 [00:55<01:29, 6295.80it/s] 38%|███▊      | 340303/900675 [00:55<01:40, 5586.42it/s] 38%|███▊      | 340937/900675 [00:55<01:36, 5783.92it/s] 38%|███▊      | 341562/900675 [00:55<01:34, 5911.12it/s] 38%|███▊      | 342199/900675 [00:55<01:32, 6041.15it/s] 38%|███▊      | 342813/900675 [00:56<01:31, 6069.45it/s] 38%|███▊      | 343437/900675 [00:56<01:31, 6117.20it/s] 38%|███▊      | 344148/900675 [00:56<01:26, 6409.17it/s] 38%|███▊      | 344793/900675 [00:56<01:30, 6124.68it/s] 38%|███▊      | 345429/900675 [00:56<01:29, 6187.57it/s] 38%|███▊      | 346052/900675 [00:56<01:37, 5710.96it/s] 38%|███▊      | 346727/900675 [00:56<01:32, 5995.11it/s] 39%|███▊      | 347345/900675 [00:56<01:31, 6041.77it/s] 39%|███▊      | 347971/900675 [00:56<01:30, 6100.14it/s] 39%|███▊      | 348586/900675 [00:56<01:31, 6063.87it/s] 39%|███▉      | 349196/900675 [00:57<01:31, 6037.98it/s] 39%|███▉      | 349803/900675 [00:57<01:31, 5993.65it/s] 39%|███▉      | 350667/900675 [00:57<01:21, 6767.75it/s] 39%|███▉      | 351347/900675 [00:57<01:24, 6482.60it/s] 39%|███▉      | 352000/900675 [00:57<01:28, 6178.81it/s] 39%|███▉      | 352623/900675 [00:57<01:36, 5685.60it/s] 39%|███▉      | 353230/900675 [00:57<01:34, 5787.59it/s] 39%|███▉      | 353817/900675 [00:57<01:35, 5738.91it/s] 39%|███▉      | 354577/900675 [00:57<01:27, 6259.45it/s] 39%|███▉      | 355210/900675 [00:58<01:33, 5825.92it/s] 40%|███▉      | 355856/900675 [00:58<01:30, 5995.54it/s] 40%|███▉      | 356483/900675 [00:58<01:29, 6071.57it/s] 40%|███▉      | 357097/900675 [00:58<01:34, 5727.90it/s] 40%|███▉      | 357702/900675 [00:58<01:33, 5816.86it/s] 40%|███▉      | 358290/900675 [00:58<01:33, 5823.82it/s] 40%|███▉      | 358967/900675 [00:58<01:28, 6093.70it/s] 40%|███▉      | 359581/900675 [00:58<01:31, 5945.29it/s] 40%|███▉      | 360243/900675 [00:58<01:28, 6138.93it/s] 40%|████      | 360898/900675 [00:59<01:26, 6258.12it/s] 40%|████      | 361527/900675 [00:59<01:26, 6262.94it/s] 40%|████      | 362282/900675 [00:59<01:21, 6643.60it/s] 40%|████      | 362949/900675 [00:59<01:21, 6572.58it/s] 40%|████      | 363608/900675 [00:59<01:22, 6492.41it/s] 40%|████      | 364307/900675 [00:59<01:20, 6638.30it/s] 41%|████      | 364972/900675 [00:59<01:20, 6629.68it/s] 41%|████      | 365646/900675 [00:59<01:20, 6658.07it/s] 41%|████      | 366313/900675 [00:59<01:21, 6519.14it/s] 41%|████      | 366966/900675 [00:59<01:25, 6219.22it/s] 41%|████      | 367604/900675 [01:00<01:25, 6259.85it/s] 41%|████      | 368233/900675 [01:00<01:31, 5787.83it/s] 41%|████      | 368919/900675 [01:00<01:27, 6080.71it/s] 41%|████      | 369555/900675 [01:00<01:26, 6159.30it/s] 41%|████      | 370200/900675 [01:00<01:24, 6242.80it/s] 41%|████      | 370829/900675 [01:00<01:29, 5931.08it/s] 41%|████      | 371435/900675 [01:00<01:28, 5964.26it/s] 41%|████▏     | 372036/900675 [01:00<01:30, 5873.10it/s] 41%|████▏     | 372679/900675 [01:00<01:27, 6028.67it/s] 41%|████▏     | 373311/900675 [01:01<01:26, 6108.18it/s] 42%|████▏     | 373924/900675 [01:01<01:28, 5948.09it/s] 42%|████▏     | 374664/900675 [01:01<01:22, 6365.92it/s] 42%|████▏     | 375304/900675 [01:01<01:23, 6278.10it/s] 42%|████▏     | 375934/900675 [01:01<01:29, 5859.86it/s] 42%|████▏     | 376542/900675 [01:01<01:28, 5912.92it/s] 42%|████▏     | 377139/900675 [01:01<01:31, 5708.31it/s] 42%|████▏     | 377794/900675 [01:01<01:27, 5943.82it/s] 42%|████▏     | 378430/900675 [01:01<01:26, 6046.21it/s] 42%|████▏     | 379095/900675 [01:01<01:23, 6219.86it/s] 42%|████▏     | 379721/900675 [01:02<01:23, 6228.04it/s] 42%|████▏     | 380371/900675 [01:02<01:22, 6304.80it/s] 42%|████▏     | 381108/900675 [01:02<01:18, 6614.93it/s] 42%|████▏     | 381771/900675 [01:02<01:21, 6345.29it/s] 42%|████▏     | 382409/900675 [01:02<01:27, 5937.61it/s] 43%|████▎     | 383010/900675 [01:02<01:28, 5873.30it/s] 43%|████▎     | 383603/900675 [01:02<01:27, 5887.85it/s] 43%|████▎     | 384288/900675 [01:02<01:23, 6156.66it/s] 43%|████▎     | 384955/900675 [01:02<01:21, 6301.20it/s] 43%|████▎     | 385588/900675 [01:03<01:23, 6204.98it/s] 43%|████▎     | 386211/900675 [01:03<01:26, 5980.32it/s] 43%|████▎     | 386861/900675 [01:03<01:23, 6126.90it/s] 43%|████▎     | 387606/900675 [01:03<01:18, 6510.28it/s] 43%|████▎     | 388261/900675 [01:03<01:20, 6372.95it/s] 43%|████▎     | 388901/900675 [01:03<01:23, 6128.89it/s] 43%|████▎     | 389583/900675 [01:03<01:20, 6319.90it/s] 43%|████▎     | 390263/900675 [01:03<01:19, 6456.75it/s] 43%|████▎     | 390912/900675 [01:03<01:20, 6298.35it/s] 43%|████▎     | 391545/900675 [01:03<01:24, 5993.49it/s] 44%|████▎     | 392149/900675 [01:04<01:27, 5834.05it/s] 44%|████▎     | 392736/900675 [01:04<01:26, 5839.09it/s] 44%|████▎     | 393323/900675 [01:04<01:28, 5722.93it/s] 44%|████▎     | 393980/900675 [01:04<01:25, 5960.14it/s] 44%|████▍     | 394594/900675 [01:04<01:24, 6007.04it/s] 44%|████▍     | 395204/900675 [01:04<01:23, 6028.96it/s] 44%|████▍     | 395823/900675 [01:04<01:23, 6074.15it/s] 44%|████▍     | 396432/900675 [01:04<01:23, 6019.59it/s] 44%|████▍     | 397035/900675 [01:04<01:26, 5846.61it/s] 44%|████▍     | 397622/900675 [01:05<01:27, 5780.60it/s] 44%|████▍     | 398321/900675 [01:05<01:21, 6127.22it/s] 44%|████▍     | 399004/900675 [01:05<01:19, 6331.39it/s] 44%|████▍     | 399698/900675 [01:05<01:16, 6510.86it/s] 44%|████▍     | 400351/900675 [01:05<01:18, 6392.48it/s] 45%|████▍     | 400992/900675 [01:05<01:19, 6262.98it/s] 45%|████▍     | 401620/900675 [01:05<01:21, 6123.37it/s] 45%|████▍     | 402234/900675 [01:05<01:27, 5690.52it/s] 45%|████▍     | 402809/900675 [01:05<01:27, 5658.62it/s] 45%|████▍     | 403427/900675 [01:05<01:25, 5803.78it/s] 45%|████▍     | 404012/900675 [01:06<01:25, 5790.91it/s] 45%|████▍     | 404682/900675 [01:06<01:21, 6051.05it/s] 45%|████▍     | 405290/900675 [01:06<01:25, 5823.28it/s] 45%|████▌     | 405889/900675 [01:06<01:24, 5867.83it/s] 45%|████▌     | 406479/900675 [01:06<01:26, 5724.23it/s] 45%|████▌     | 407126/900675 [01:06<01:23, 5938.57it/s] 45%|████▌     | 407723/900675 [01:06<01:26, 5725.34it/s] 45%|████▌     | 408382/900675 [01:06<01:22, 5971.83it/s] 45%|████▌     | 409058/900675 [01:06<01:19, 6192.26it/s] 45%|████▌     | 409681/900675 [01:07<01:21, 6019.14it/s] 46%|████▌     | 410286/900675 [01:07<01:23, 5858.12it/s] 46%|████▌     | 410970/900675 [01:07<01:19, 6138.79it/s] 46%|████▌     | 411587/900675 [01:07<01:22, 5911.84it/s] 46%|████▌     | 412182/900675 [01:07<01:23, 5868.84it/s] 46%|████▌     | 412772/900675 [01:07<01:29, 5448.70it/s] 46%|████▌     | 413326/900675 [01:07<01:29, 5472.84it/s] 46%|████▌     | 413896/900675 [01:07<01:28, 5524.86it/s] 46%|████▌     | 414453/900675 [01:07<01:29, 5402.90it/s] 46%|████▌     | 414999/900675 [01:07<01:29, 5418.46it/s] 46%|████▌     | 415643/900675 [01:08<01:24, 5712.82it/s] 46%|████▌     | 416217/900675 [01:08<01:26, 5588.58it/s] 46%|████▋     | 416815/900675 [01:08<01:24, 5694.33it/s] 46%|████▋     | 417450/900675 [01:08<01:22, 5878.25it/s] 46%|████▋     | 418142/900675 [01:08<01:18, 6179.37it/s] 46%|████▋     | 418762/900675 [01:08<01:22, 5860.78it/s] 47%|████▋     | 419411/900675 [01:08<01:19, 6037.89it/s] 47%|████▋     | 420019/900675 [01:08<01:19, 6010.93it/s] 47%|████▋     | 420634/900675 [01:08<01:19, 6050.74it/s] 47%|████▋     | 421242/900675 [01:09<01:24, 5660.02it/s] 47%|████▋     | 421815/900675 [01:09<01:25, 5584.29it/s] 47%|████▋     | 422447/900675 [01:09<01:22, 5792.75it/s] 47%|████▋     | 423108/900675 [01:09<01:19, 6025.96it/s] 47%|████▋     | 423715/900675 [01:09<01:20, 5954.14it/s] 47%|████▋     | 424318/900675 [01:09<01:19, 5972.14it/s] 47%|████▋     | 424918/900675 [01:09<01:20, 5886.10it/s] 47%|████▋     | 425509/900675 [01:09<01:22, 5776.39it/s] 47%|████▋     | 426180/900675 [01:09<01:18, 6044.26it/s] 47%|████▋     | 426914/900675 [01:09<01:13, 6420.65it/s] 47%|████▋     | 427559/900675 [01:10<01:14, 6337.67it/s] 48%|████▊     | 428195/900675 [01:10<01:15, 6230.99it/s] 48%|████▊     | 428855/900675 [01:10<01:14, 6338.25it/s] 48%|████▊     | 429519/900675 [01:10<01:13, 6423.08it/s] 48%|████▊     | 430163/900675 [01:10<01:13, 6427.75it/s] 48%|████▊     | 430807/900675 [01:10<01:15, 6195.97it/s] 48%|████▊     | 431429/900675 [01:10<01:19, 5891.96it/s] 48%|████▊     | 432023/900675 [01:10<01:21, 5774.49it/s] 48%|████▊     | 432924/900675 [01:10<01:09, 6689.68it/s] 48%|████▊     | 433601/900675 [01:11<01:13, 6371.93it/s] 48%|████▊     | 434246/900675 [01:11<01:17, 6028.09it/s] 48%|████▊     | 434857/900675 [01:11<01:18, 5911.01it/s] 48%|████▊     | 435492/900675 [01:11<01:17, 6030.92it/s] 48%|████▊     | 436100/900675 [01:11<01:18, 5902.43it/s] 48%|████▊     | 436735/900675 [01:11<01:17, 6023.46it/s] 49%|████▊     | 437386/900675 [01:11<01:15, 6162.95it/s] 49%|████▊     | 438005/900675 [01:11<01:16, 6082.66it/s] 49%|████▊     | 438616/900675 [01:11<01:17, 5989.15it/s] 49%|████▉     | 439217/900675 [01:11<01:19, 5809.60it/s] 49%|████▉     | 439800/900675 [01:12<01:19, 5778.56it/s] 49%|████▉     | 440379/900675 [01:12<01:19, 5760.10it/s] 49%|████▉     | 440988/900675 [01:12<01:18, 5854.49it/s] 49%|████▉     | 441614/900675 [01:12<01:16, 5973.32it/s] 49%|████▉     | 442287/900675 [01:12<01:14, 6189.52it/s] 49%|████▉     | 442907/900675 [01:12<01:15, 6029.44it/s] 49%|████▉     | 443577/900675 [01:12<01:13, 6221.23it/s] 49%|████▉     | 444230/900675 [01:12<01:12, 6309.48it/s] 49%|████▉     | 444863/900675 [01:12<01:20, 5692.33it/s] 49%|████▉     | 445464/900675 [01:13<01:18, 5776.74it/s] 50%|████▉     | 446052/900675 [01:13<01:18, 5805.19it/s] 50%|████▉     | 446640/900675 [01:13<01:19, 5743.33it/s] 50%|████▉     | 447317/900675 [01:13<01:15, 6038.42it/s] 50%|████▉     | 447926/900675 [01:13<01:16, 5938.36it/s] 50%|████▉     | 448582/900675 [01:13<01:13, 6116.39it/s] 50%|████▉     | 449213/900675 [01:13<01:13, 6171.96it/s] 50%|████▉     | 449833/900675 [01:13<01:16, 5856.35it/s] 50%|█████     | 450535/900675 [01:13<01:12, 6188.09it/s] 50%|█████     | 451160/900675 [01:13<01:12, 6200.90it/s] 50%|█████     | 451784/900675 [01:14<01:14, 6053.54it/s] 50%|█████     | 452393/900675 [01:14<01:17, 5819.46it/s] 50%|█████     | 453007/900675 [01:14<01:15, 5908.50it/s] 50%|█████     | 453629/900675 [01:14<01:14, 5994.48it/s] 50%|█████     | 454231/900675 [01:14<01:15, 5901.67it/s] 50%|█████     | 454823/900675 [01:14<01:17, 5780.62it/s] 51%|█████     | 455415/900675 [01:14<01:16, 5814.50it/s] 51%|█████     | 456023/900675 [01:14<01:15, 5891.26it/s] 51%|█████     | 456726/900675 [01:14<01:11, 6226.27it/s] 51%|█████     | 457440/900675 [01:15<01:08, 6493.91it/s] 51%|█████     | 458091/900675 [01:15<01:11, 6180.79it/s] 51%|█████     | 458713/900675 [01:15<01:12, 6101.09it/s] 51%|█████     | 459434/900675 [01:15<01:08, 6413.89it/s] 51%|█████     | 460079/900675 [01:15<01:11, 6134.24it/s] 51%|█████     | 460697/900675 [01:15<01:14, 5893.09it/s] 51%|█████     | 461321/900675 [01:15<01:13, 5988.00it/s] 51%|█████▏    | 461924/900675 [01:15<01:13, 5979.16it/s] 51%|█████▏    | 462525/900675 [01:15<01:14, 5909.55it/s] 51%|█████▏    | 463181/900675 [01:15<01:11, 6094.55it/s] 51%|█████▏    | 463793/900675 [01:16<01:17, 5655.70it/s] 52%|█████▏    | 464446/900675 [01:16<01:13, 5899.78it/s] 52%|█████▏    | 465071/900675 [01:16<01:12, 5989.26it/s] 52%|█████▏    | 465675/900675 [01:16<01:14, 5856.08it/s] 52%|█████▏    | 466357/900675 [01:16<01:10, 6129.11it/s] 52%|█████▏    | 466974/900675 [01:16<01:11, 6079.69it/s] 52%|█████▏    | 467607/900675 [01:16<01:10, 6148.59it/s] 52%|█████▏    | 468224/900675 [01:16<01:11, 6054.77it/s] 52%|█████▏    | 468860/900675 [01:16<01:10, 6140.68it/s] 52%|█████▏    | 469476/900675 [01:17<01:12, 5949.45it/s] 52%|█████▏    | 470073/900675 [01:17<01:13, 5837.08it/s] 52%|█████▏    | 470778/900675 [01:17<01:09, 6175.91it/s] 52%|█████▏    | 471435/900675 [01:17<01:08, 6288.83it/s] 52%|█████▏    | 472066/900675 [01:17<01:12, 5921.54it/s] 52%|█████▏    | 472707/900675 [01:17<01:10, 6053.95it/s] 53%|█████▎    | 473347/900675 [01:17<01:09, 6141.83it/s] 53%|█████▎    | 473965/900675 [01:17<01:10, 6018.76it/s] 53%|█████▎    | 474570/900675 [01:17<01:11, 5937.76it/s] 53%|█████▎    | 475322/900675 [01:17<01:06, 6395.04it/s] 53%|█████▎    | 476061/900675 [01:18<01:03, 6678.80it/s] 53%|█████▎    | 476732/900675 [01:18<01:08, 6196.78it/s] 53%|█████▎    | 477361/900675 [01:18<01:09, 6089.39it/s] 53%|█████▎    | 477976/900675 [01:18<01:10, 6027.60it/s] 53%|█████▎    | 478701/900675 [01:18<01:06, 6374.86it/s] 53%|█████▎    | 479378/900675 [01:18<01:05, 6475.91it/s] 53%|█████▎    | 480030/900675 [01:18<01:08, 6183.02it/s] 53%|█████▎    | 480672/900675 [01:18<01:07, 6242.46it/s] 53%|█████▎    | 481300/900675 [01:18<01:08, 6140.27it/s] 54%|█████▎    | 481917/900675 [01:19<01:09, 6019.17it/s] 54%|█████▎    | 482521/900675 [01:19<01:12, 5789.42it/s] 54%|█████▎    | 483207/900675 [01:19<01:08, 6085.35it/s] 54%|█████▎    | 483819/900675 [01:19<01:09, 5999.37it/s] 54%|█████▍    | 484427/900675 [01:19<01:09, 6017.66it/s] 54%|█████▍    | 485225/900675 [01:19<01:03, 6582.87it/s] 54%|█████▍    | 485887/900675 [01:19<01:04, 6419.06it/s] 54%|█████▍    | 486532/900675 [01:19<01:06, 6195.04it/s] 54%|█████▍    | 487155/900675 [01:19<01:08, 6031.50it/s] 54%|█████▍    | 487798/900675 [01:19<01:07, 6143.96it/s] 54%|█████▍    | 488449/900675 [01:20<01:06, 6244.49it/s] 54%|█████▍    | 489076/900675 [01:20<01:07, 6058.74it/s] 54%|█████▍    | 489685/900675 [01:20<01:08, 6015.94it/s] 54%|█████▍    | 490289/900675 [01:20<01:09, 5863.25it/s] 55%|█████▍    | 490885/900675 [01:20<01:09, 5884.99it/s] 55%|█████▍    | 491514/900675 [01:20<01:08, 6001.34it/s] 55%|█████▍    | 492145/900675 [01:20<01:07, 6087.45it/s] 55%|█████▍    | 492768/900675 [01:20<01:06, 6129.33it/s] 55%|█████▍    | 493382/900675 [01:20<01:08, 5927.28it/s] 55%|█████▍    | 494008/900675 [01:21<01:07, 6022.14it/s] 55%|█████▍    | 494695/900675 [01:21<01:04, 6262.06it/s] 55%|█████▍    | 495346/900675 [01:21<01:04, 6330.02it/s] 55%|█████▌    | 495981/900675 [01:21<01:04, 6241.03it/s] 55%|█████▌    | 496607/900675 [01:21<01:04, 6230.65it/s] 55%|█████▌    | 497231/900675 [01:21<01:04, 6224.36it/s] 55%|█████▌    | 497854/900675 [01:21<01:04, 6217.70it/s] 55%|█████▌    | 498477/900675 [01:21<01:06, 6066.82it/s] 55%|█████▌    | 499179/900675 [01:21<01:03, 6339.19it/s] 55%|█████▌    | 499815/900675 [01:21<01:03, 6342.88it/s] 56%|█████▌    | 500508/900675 [01:22<01:01, 6500.41it/s] 56%|█████▌    | 501159/900675 [01:22<01:02, 6412.90it/s] 56%|█████▌    | 501801/900675 [01:22<01:05, 6103.96it/s] 56%|█████▌    | 502415/900675 [01:22<01:05, 6089.75it/s] 56%|█████▌    | 503027/900675 [01:22<01:07, 5925.52it/s] 56%|█████▌    | 503688/900675 [01:22<01:04, 6118.05it/s] 56%|█████▌    | 504303/900675 [01:22<01:07, 5872.90it/s] 56%|█████▌    | 504899/900675 [01:22<01:07, 5895.57it/s] 56%|█████▌    | 505574/900675 [01:22<01:04, 6139.25it/s] 56%|█████▌    | 506191/900675 [01:23<01:07, 5879.55it/s] 56%|█████▋    | 506870/900675 [01:23<01:04, 6138.34it/s] 56%|█████▋    | 507488/900675 [01:23<01:06, 5946.35it/s] 56%|█████▋    | 508087/900675 [01:23<01:09, 5639.60it/s] 56%|█████▋    | 508656/900675 [01:23<01:09, 5623.09it/s] 57%|█████▋    | 509373/900675 [01:23<01:04, 6058.59it/s] 57%|█████▋    | 510063/900675 [01:23<01:02, 6299.52it/s] 57%|█████▋    | 510698/900675 [01:23<01:04, 6053.33it/s] 57%|█████▋    | 511464/900675 [01:23<00:59, 6510.70it/s] 57%|█████▋    | 512121/900675 [01:23<01:02, 6193.79it/s] 57%|█████▋    | 512819/900675 [01:24<01:00, 6414.96it/s] 57%|█████▋    | 513467/900675 [01:24<01:01, 6330.58it/s] 57%|█████▋    | 514105/900675 [01:24<01:00, 6343.62it/s] 57%|█████▋    | 514743/900675 [01:24<01:03, 6036.44it/s] 57%|█████▋    | 515352/900675 [01:24<01:03, 6041.20it/s] 57%|█████▋    | 516032/900675 [01:24<01:01, 6256.73it/s] 57%|█████▋    | 516661/900675 [01:24<01:04, 5998.97it/s] 57%|█████▋    | 517265/900675 [01:24<01:04, 5911.81it/s] 58%|█████▊    | 517916/900675 [01:24<01:02, 6082.41it/s] 58%|█████▊    | 518584/900675 [01:25<01:01, 6245.61it/s] 58%|█████▊    | 519211/900675 [01:25<01:03, 5976.98it/s] 58%|█████▊    | 519903/900675 [01:25<01:00, 6245.88it/s] 58%|█████▊    | 520532/900675 [01:25<01:02, 6050.39it/s] 58%|█████▊    | 521231/900675 [01:25<01:00, 6313.26it/s] 58%|█████▊    | 521867/900675 [01:25<01:00, 6303.45it/s] 58%|█████▊    | 522500/900675 [01:25<01:01, 6194.79it/s] 58%|█████▊    | 523122/900675 [01:25<01:03, 5976.15it/s] 58%|█████▊    | 523726/900675 [01:25<01:02, 5992.98it/s] 58%|█████▊    | 524347/900675 [01:25<01:02, 6051.84it/s] 58%|█████▊    | 524954/900675 [01:26<01:02, 6013.79it/s] 58%|█████▊    | 525557/900675 [01:26<01:08, 5508.04it/s] 58%|█████▊    | 526321/900675 [01:26<01:01, 6093.31it/s] 59%|█████▊    | 527071/900675 [01:26<00:57, 6491.88it/s] 59%|█████▊    | 527730/900675 [01:26<01:02, 5927.43it/s] 59%|█████▊    | 528426/900675 [01:26<00:59, 6208.50it/s] 59%|█████▊    | 529080/900675 [01:26<00:59, 6295.42it/s] 59%|█████▉    | 529720/900675 [01:26<01:00, 6174.17it/s] 59%|█████▉    | 530345/900675 [01:26<01:01, 6037.67it/s] 59%|█████▉    | 530954/900675 [01:27<01:03, 5825.74it/s] 59%|█████▉    | 531563/900675 [01:27<01:02, 5897.66it/s] 59%|█████▉    | 532188/900675 [01:27<01:01, 5995.97it/s] 59%|█████▉    | 532845/900675 [01:27<00:59, 6155.21it/s] 59%|█████▉    | 533483/900675 [01:27<00:59, 6215.41it/s] 59%|█████▉    | 534107/900675 [01:27<01:01, 5926.71it/s] 59%|█████▉    | 534704/900675 [01:27<01:01, 5903.76it/s] 59%|█████▉    | 535319/900675 [01:27<01:01, 5967.03it/s] 60%|█████▉    | 535918/900675 [01:27<01:03, 5785.93it/s] 60%|█████▉    | 536586/900675 [01:28<01:00, 6036.72it/s] 60%|█████▉    | 537216/900675 [01:28<00:59, 6108.73it/s] 60%|█████▉    | 537829/900675 [01:28<01:01, 5920.10it/s] 60%|█████▉    | 538484/900675 [01:28<00:59, 6101.45it/s] 60%|█████▉    | 539160/900675 [01:28<00:57, 6293.45it/s] 60%|█████▉    | 539792/900675 [01:28<01:00, 5999.66it/s] 60%|██████    | 540410/900675 [01:28<00:59, 6049.87it/s] 60%|██████    | 541019/900675 [01:28<01:00, 5916.97it/s] 60%|██████    | 541647/900675 [01:28<00:59, 6016.70it/s] 60%|██████    | 542251/900675 [01:28<01:00, 5877.96it/s] 60%|██████    | 543006/900675 [01:29<00:56, 6355.29it/s] 60%|██████    | 543645/900675 [01:29<00:58, 6147.48it/s] 60%|██████    | 544290/900675 [01:29<00:57, 6231.77it/s] 61%|██████    | 544916/900675 [01:29<00:58, 6114.37it/s] 61%|██████    | 545530/900675 [01:29<00:58, 6063.84it/s] 61%|██████    | 546138/900675 [01:29<00:59, 5947.78it/s] 61%|██████    | 546816/900675 [01:29<00:57, 6181.85it/s] 61%|██████    | 547436/900675 [01:29<00:59, 5964.47it/s] 61%|██████    | 548069/900675 [01:29<00:58, 6069.02it/s] 61%|██████    | 548741/900675 [01:29<00:56, 6249.62it/s] 61%|██████    | 549368/900675 [01:30<00:58, 6010.58it/s] 61%|██████    | 550022/900675 [01:30<00:56, 6161.29it/s] 61%|██████    | 550641/900675 [01:30<00:57, 6062.81it/s] 61%|██████    | 551266/900675 [01:30<00:57, 6112.11it/s] 61%|██████▏   | 551885/900675 [01:30<00:56, 6129.29it/s] 61%|██████▏   | 552500/900675 [01:30<00:58, 5954.71it/s] 61%|██████▏   | 553098/900675 [01:30<00:58, 5905.57it/s] 61%|██████▏   | 553690/900675 [01:30<00:58, 5906.85it/s] 62%|██████▏   | 554324/900675 [01:30<00:57, 6028.25it/s] 62%|██████▏   | 554928/900675 [01:31<00:59, 5845.74it/s] 62%|██████▏   | 555592/900675 [01:31<00:56, 6066.85it/s] 62%|██████▏   | 556226/900675 [01:31<00:56, 6143.62it/s] 62%|██████▏   | 556876/900675 [01:31<00:55, 6244.83it/s] 62%|██████▏   | 557502/900675 [01:31<00:57, 5933.29it/s] 62%|██████▏   | 558101/900675 [01:31<00:57, 5948.83it/s] 62%|██████▏   | 558699/900675 [01:31<00:59, 5761.52it/s] 62%|██████▏   | 559278/900675 [01:31<01:00, 5623.71it/s] 62%|██████▏   | 559878/900675 [01:31<00:59, 5726.29it/s] 62%|██████▏   | 560484/900675 [01:31<00:58, 5820.51it/s] 62%|██████▏   | 561124/900675 [01:32<00:56, 5989.76it/s] 62%|██████▏   | 561778/900675 [01:32<00:55, 6146.21it/s] 62%|██████▏   | 562474/900675 [01:32<00:52, 6385.90it/s] 63%|██████▎   | 563126/900675 [01:32<00:52, 6423.94it/s] 63%|██████▎   | 563770/900675 [01:32<00:56, 5944.49it/s] 63%|██████▎   | 564402/900675 [01:32<00:55, 6044.63it/s] 63%|██████▎   | 565013/900675 [01:32<00:57, 5814.90it/s] 63%|██████▎   | 565707/900675 [01:32<00:54, 6131.17it/s] 63%|██████▎   | 566326/900675 [01:32<00:58, 5760.85it/s] 63%|██████▎   | 566910/900675 [01:33<00:58, 5672.37it/s] 63%|██████▎   | 567684/900675 [01:33<00:53, 6250.12it/s] 63%|██████▎   | 568317/900675 [01:33<00:54, 6115.35it/s] 63%|██████▎   | 568989/900675 [01:33<00:52, 6283.46it/s] 63%|██████▎   | 569623/900675 [01:33<00:54, 6126.31it/s] 63%|██████▎   | 570359/900675 [01:33<00:51, 6476.01it/s] 63%|██████▎   | 571048/900675 [01:33<00:49, 6596.14it/s] 63%|██████▎   | 571815/900675 [01:33<00:47, 6906.45it/s] 64%|██████▎   | 572509/900675 [01:33<00:50, 6512.69it/s] 64%|██████▎   | 573167/900675 [01:34<00:52, 6249.97it/s] 64%|██████▎   | 573798/900675 [01:34<00:52, 6196.91it/s] 64%|██████▍   | 574422/900675 [01:34<00:54, 5995.89it/s] 64%|██████▍   | 575025/900675 [01:34<00:55, 5895.82it/s] 64%|██████▍   | 575639/900675 [01:34<00:54, 5962.10it/s] 64%|██████▍   | 576281/900675 [01:34<00:53, 6091.95it/s] 64%|██████▍   | 576893/900675 [01:34<00:53, 6027.80it/s] 64%|██████▍   | 577537/900675 [01:34<00:52, 6134.79it/s] 64%|██████▍   | 578152/900675 [01:34<00:56, 5743.65it/s] 64%|██████▍   | 578732/900675 [01:34<00:55, 5757.67it/s] 64%|██████▍   | 579376/900675 [01:35<00:54, 5946.67it/s] 64%|██████▍   | 580053/900675 [01:35<00:51, 6179.67it/s] 64%|██████▍   | 580697/900675 [01:35<00:51, 6254.56it/s] 65%|██████▍   | 581325/900675 [01:35<00:52, 6121.55it/s] 65%|██████▍   | 581955/900675 [01:35<00:51, 6169.28it/s] 65%|██████▍   | 582594/900675 [01:35<00:51, 6227.70it/s] 65%|██████▍   | 583276/900675 [01:35<00:49, 6396.28it/s] 65%|██████▍   | 584036/900675 [01:35<00:46, 6751.50it/s] 65%|██████▍   | 584762/900675 [01:35<00:45, 6900.63it/s] 65%|██████▌   | 585453/900675 [01:35<00:46, 6816.75it/s] 65%|██████▌   | 586136/900675 [01:36<00:48, 6463.36it/s] 65%|██████▌   | 586787/900675 [01:36<00:49, 6321.98it/s] 65%|██████▌   | 587423/900675 [01:36<00:54, 5786.28it/s] 65%|██████▌   | 588066/900675 [01:36<00:52, 5957.86it/s] 65%|██████▌   | 588674/900675 [01:36<00:52, 5989.17it/s] 65%|██████▌   | 589326/900675 [01:36<00:50, 6140.15it/s] 66%|██████▌   | 589946/900675 [01:36<00:52, 5936.84it/s] 66%|██████▌   | 590611/900675 [01:36<00:50, 6140.01it/s] 66%|██████▌   | 591230/900675 [01:36<00:51, 6004.72it/s] 66%|██████▌   | 591892/900675 [01:37<00:49, 6179.45it/s] 66%|██████▌   | 592513/900675 [01:37<00:52, 5816.12it/s] 66%|██████▌   | 593101/900675 [01:37<00:53, 5771.49it/s] 66%|██████▌   | 593696/900675 [01:37<00:52, 5821.72it/s] 66%|██████▌   | 594322/900675 [01:37<00:51, 5948.04it/s] 66%|██████▌   | 595014/900675 [01:37<00:49, 6226.87it/s] 66%|██████▌   | 595640/900675 [01:37<00:52, 5842.41it/s] 66%|██████▌   | 596312/900675 [01:37<00:49, 6090.08it/s] 66%|██████▋   | 596933/900675 [01:37<00:49, 6123.99it/s] 66%|██████▋   | 597550/900675 [01:38<00:50, 5994.70it/s] 66%|██████▋   | 598153/900675 [01:38<00:50, 5997.95it/s] 66%|██████▋   | 598756/900675 [01:38<00:50, 5960.51it/s] 67%|██████▋   | 599405/900675 [01:38<00:49, 6115.22it/s] 67%|██████▋   | 600068/900675 [01:38<00:47, 6266.84it/s] 67%|██████▋   | 600696/900675 [01:38<00:48, 6189.56it/s] 67%|██████▋   | 601317/900675 [01:38<00:51, 5809.73it/s] 67%|██████▋   | 601904/900675 [01:38<00:52, 5677.98it/s] 67%|██████▋   | 602513/900675 [01:38<00:51, 5788.40it/s] 67%|██████▋   | 603096/900675 [01:38<00:51, 5761.28it/s] 67%|██████▋   | 603813/900675 [01:39<00:48, 6169.01it/s] 67%|██████▋   | 604433/900675 [01:39<00:48, 6092.40it/s] 67%|██████▋   | 605052/900675 [01:39<00:48, 6112.64it/s] 67%|██████▋   | 605665/900675 [01:39<00:49, 5913.60it/s] 67%|██████▋   | 606322/900675 [01:39<00:48, 6097.51it/s] 67%|██████▋   | 606948/900675 [01:39<00:47, 6144.83it/s] 67%|██████▋   | 607565/900675 [01:39<00:48, 6004.46it/s] 68%|██████▊   | 608180/900675 [01:39<00:48, 6046.59it/s] 68%|██████▊   | 608786/900675 [01:39<00:50, 5815.55it/s] 68%|██████▊   | 609370/900675 [01:40<00:50, 5725.00it/s] 68%|██████▊   | 610093/900675 [01:40<00:47, 6157.27it/s] 68%|██████▊   | 610818/900675 [01:40<00:44, 6467.18it/s] 68%|██████▊   | 611468/900675 [01:40<00:46, 6186.77it/s] 68%|██████▊   | 612104/900675 [01:40<00:46, 6234.25it/s] 68%|██████▊   | 612731/900675 [01:40<00:46, 6208.46it/s] 68%|██████▊   | 613355/900675 [01:40<00:47, 6103.25it/s] 68%|██████▊   | 613968/900675 [01:40<00:47, 5988.01it/s] 68%|██████▊   | 614569/900675 [01:40<00:50, 5642.46it/s] 68%|██████▊   | 615245/900675 [01:40<00:47, 5954.32it/s] 68%|██████▊   | 615846/900675 [01:41<00:49, 5742.86it/s] 68%|██████▊   | 616494/900675 [01:41<00:47, 5949.61it/s] 69%|██████▊   | 617312/900675 [01:41<00:43, 6585.47it/s] 69%|██████▊   | 617994/900675 [01:41<00:42, 6650.65it/s] 69%|██████▊   | 618694/900675 [01:41<00:41, 6748.86it/s] 69%|██████▉   | 619411/900675 [01:41<00:40, 6867.31it/s] 69%|██████▉   | 620216/900675 [01:41<00:38, 7206.64it/s] 69%|██████▉   | 620939/900675 [01:41<00:39, 7031.03it/s] 69%|██████▉   | 621645/900675 [01:41<00:44, 6305.32it/s] 69%|██████▉   | 622291/900675 [01:42<00:44, 6248.26it/s] 69%|██████▉   | 623088/900675 [01:42<00:41, 6722.26it/s] 69%|██████▉   | 623772/900675 [01:42<00:42, 6445.77it/s] 69%|██████▉   | 624426/900675 [01:42<00:43, 6357.34it/s] 69%|██████▉   | 625068/900675 [01:42<00:43, 6354.12it/s] 69%|██████▉   | 625783/900675 [01:42<00:41, 6580.54it/s] 70%|██████▉   | 626446/900675 [01:42<00:42, 6517.92it/s] 70%|██████▉   | 627101/900675 [01:42<00:44, 6138.25it/s] 70%|██████▉   | 627747/900675 [01:42<00:43, 6220.48it/s] 70%|██████▉   | 628374/900675 [01:42<00:44, 6088.03it/s] 70%|██████▉   | 629049/900675 [01:43<00:43, 6274.88it/s] 70%|██████▉   | 629680/900675 [01:43<00:43, 6235.46it/s] 70%|██████▉   | 630306/900675 [01:43<00:45, 5958.31it/s] 70%|███████   | 631064/900675 [01:43<00:42, 6414.85it/s] 70%|███████   | 631758/900675 [01:43<00:41, 6551.72it/s] 70%|███████   | 632418/900675 [01:43<00:41, 6450.24it/s] 70%|███████   | 633066/900675 [01:43<00:41, 6434.06it/s] 70%|███████   | 633770/900675 [01:43<00:40, 6609.84it/s] 70%|███████   | 634433/900675 [01:43<00:41, 6465.27it/s] 71%|███████   | 635082/900675 [01:44<00:41, 6347.88it/s] 71%|███████   | 635719/900675 [01:44<00:42, 6268.89it/s] 71%|███████   | 636372/900675 [01:44<00:41, 6339.49it/s] 71%|███████   | 637007/900675 [01:44<00:43, 6017.73it/s] 71%|███████   | 637641/900675 [01:44<00:43, 6101.42it/s] 71%|███████   | 638319/900675 [01:44<00:41, 6289.65it/s] 71%|███████   | 638964/900675 [01:44<00:41, 6330.09it/s] 71%|███████   | 639600/900675 [01:44<00:42, 6104.40it/s] 71%|███████   | 640214/900675 [01:44<00:43, 6030.61it/s] 71%|███████   | 641011/900675 [01:44<00:39, 6586.64it/s] 71%|███████   | 641674/900675 [01:45<00:39, 6528.52it/s] 71%|███████▏  | 642367/900675 [01:45<00:38, 6640.20it/s] 71%|███████▏  | 643033/900675 [01:45<00:41, 6180.66it/s] 71%|███████▏  | 643659/900675 [01:45<00:42, 6068.54it/s] 72%|███████▏  | 644271/900675 [01:45<00:42, 5966.27it/s] 72%|███████▏  | 644871/900675 [01:45<00:43, 5859.49it/s] 72%|███████▏  | 645547/900675 [01:45<00:41, 6112.54it/s] 72%|███████▏  | 646199/900675 [01:45<00:40, 6220.27it/s] 72%|███████▏  | 646824/900675 [01:45<00:41, 6051.68it/s] 72%|███████▏  | 647432/900675 [01:46<00:43, 5842.46it/s] 72%|███████▏  | 648019/900675 [01:46<00:46, 5445.34it/s] 72%|███████▏  | 648576/900675 [01:46<00:46, 5476.56it/s] 72%|███████▏  | 649188/900675 [01:46<00:44, 5654.90it/s] 72%|███████▏  | 649758/900675 [01:46<00:45, 5553.85it/s] 72%|███████▏  | 650334/900675 [01:46<00:44, 5608.81it/s] 72%|███████▏  | 650938/900675 [01:46<00:43, 5729.55it/s] 72%|███████▏  | 651640/900675 [01:46<00:40, 6106.56it/s] 72%|███████▏  | 652253/900675 [01:46<00:41, 5984.38it/s] 72%|███████▏  | 652854/900675 [01:46<00:41, 5924.71it/s] 73%|███████▎  | 653570/900675 [01:47<00:39, 6281.74it/s] 73%|███████▎  | 654201/900675 [01:47<00:39, 6229.65it/s] 73%|███████▎  | 654826/900675 [01:47<00:39, 6211.96it/s] 73%|███████▎  | 655449/900675 [01:47<00:39, 6148.27it/s] 73%|███████▎  | 656065/900675 [01:47<00:40, 6075.71it/s] 73%|███████▎  | 656696/900675 [01:47<00:39, 6144.35it/s] 73%|███████▎  | 657370/900675 [01:47<00:38, 6319.80it/s] 73%|███████▎  | 658003/900675 [01:47<00:38, 6254.13it/s] 73%|███████▎  | 658629/900675 [01:47<00:39, 6165.31it/s] 73%|███████▎  | 659290/900675 [01:48<00:38, 6295.33it/s] 73%|███████▎  | 659952/900675 [01:48<00:37, 6385.14it/s] 73%|███████▎  | 660592/900675 [01:48<00:37, 6367.83it/s] 73%|███████▎  | 661230/900675 [01:48<00:39, 6059.93it/s] 73%|███████▎  | 661906/900675 [01:48<00:38, 6259.13it/s] 74%|███████▎  | 662535/900675 [01:48<00:38, 6152.21it/s] 74%|███████▎  | 663153/900675 [01:48<00:39, 6033.68it/s] 74%|███████▎  | 663759/900675 [01:48<00:39, 5986.42it/s] 74%|███████▍  | 664459/900675 [01:48<00:37, 6279.36it/s] 74%|███████▍  | 665089/900675 [01:48<00:38, 6150.31it/s] 74%|███████▍  | 665710/900675 [01:49<00:38, 6167.42it/s] 74%|███████▍  | 666386/900675 [01:49<00:36, 6335.68it/s] 74%|███████▍  | 667021/900675 [01:49<00:37, 6222.81it/s] 74%|███████▍  | 667645/900675 [01:49<00:38, 6116.21it/s] 74%|███████▍  | 668283/900675 [01:49<00:37, 6191.33it/s] 74%|███████▍  | 668963/900675 [01:49<00:36, 6365.43it/s] 74%|███████▍  | 669675/900675 [01:49<00:35, 6588.26it/s] 74%|███████▍  | 670335/900675 [01:49<00:36, 6303.86it/s] 74%|███████▍  | 670984/900675 [01:49<00:36, 6357.41it/s] 75%|███████▍  | 671653/900675 [01:49<00:35, 6437.44it/s] 75%|███████▍  | 672299/900675 [01:50<00:36, 6338.06it/s] 75%|███████▍  | 672963/900675 [01:50<00:35, 6416.37it/s] 75%|███████▍  | 673606/900675 [01:50<00:35, 6404.89it/s] 75%|███████▍  | 674248/900675 [01:50<00:36, 6211.29it/s] 75%|███████▍  | 674871/900675 [01:50<00:36, 6215.38it/s] 75%|███████▍  | 675494/900675 [01:50<00:36, 6206.13it/s] 75%|███████▌  | 676116/900675 [01:50<00:36, 6127.76it/s] 75%|███████▌  | 676730/900675 [01:50<00:37, 5920.78it/s] 75%|███████▌  | 677335/900675 [01:50<00:37, 5955.84it/s] 75%|███████▌  | 677948/900675 [01:51<00:37, 6003.02it/s] 75%|███████▌  | 678550/900675 [01:51<00:37, 5933.24it/s] 75%|███████▌  | 679145/900675 [01:51<00:37, 5924.87it/s] 75%|███████▌  | 679739/900675 [01:51<00:38, 5756.86it/s] 76%|███████▌  | 680474/900675 [01:51<00:35, 6215.18it/s] 76%|███████▌  | 681154/900675 [01:51<00:34, 6384.95it/s] 76%|███████▌  | 681795/900675 [01:51<00:35, 6212.08it/s] 76%|███████▌  | 682419/900675 [01:51<00:36, 6034.43it/s] 76%|███████▌  | 683025/900675 [01:51<00:36, 5903.39it/s] 76%|███████▌  | 683740/900675 [01:51<00:34, 6254.53it/s] 76%|███████▌  | 684448/900675 [01:52<00:33, 6492.74it/s] 76%|███████▌  | 685101/900675 [01:52<00:33, 6400.05it/s] 76%|███████▌  | 685744/900675 [01:52<00:34, 6242.11it/s] 76%|███████▌  | 686374/900675 [01:52<00:34, 6258.68it/s] 76%|███████▋  | 687002/900675 [01:52<00:36, 5879.03it/s] 76%|███████▋  | 687657/900675 [01:52<00:35, 6064.27it/s] 76%|███████▋  | 688270/900675 [01:52<00:34, 6079.98it/s] 76%|███████▋  | 688882/900675 [01:52<00:35, 5956.70it/s] 77%|███████▋  | 689483/900675 [01:52<00:35, 5971.05it/s] 77%|███████▋  | 690087/900675 [01:53<00:35, 5981.21it/s] 77%|███████▋  | 690687/900675 [01:53<00:36, 5727.24it/s] 77%|███████▋  | 691263/900675 [01:53<00:36, 5715.92it/s] 77%|███████▋  | 691837/900675 [01:53<00:36, 5717.35it/s] 77%|███████▋  | 692411/900675 [01:53<00:36, 5654.75it/s] 77%|███████▋  | 692986/900675 [01:53<00:36, 5675.75it/s] 77%|███████▋  | 693591/900675 [01:53<00:35, 5784.33it/s] 77%|███████▋  | 694218/900675 [01:53<00:34, 5921.84it/s] 77%|███████▋  | 694843/900675 [01:53<00:34, 6010.70it/s] 77%|███████▋  | 695445/900675 [01:53<00:35, 5716.20it/s] 77%|███████▋  | 696083/900675 [01:54<00:34, 5906.50it/s] 77%|███████▋  | 696693/900675 [01:54<00:34, 5958.49it/s] 77%|███████▋  | 697292/900675 [01:54<00:34, 5940.57it/s] 77%|███████▋  | 697983/900675 [01:54<00:32, 6224.73it/s] 78%|███████▊  | 698608/900675 [01:54<00:33, 6055.79it/s] 78%|███████▊  | 699216/900675 [01:54<00:34, 5803.36it/s] 78%|███████▊  | 699860/900675 [01:54<00:33, 5983.88it/s] 78%|███████▊  | 700465/900675 [01:54<00:33, 5999.96it/s] 78%|███████▊  | 701134/900675 [01:54<00:32, 6193.30it/s] 78%|███████▊  | 701837/900675 [01:54<00:30, 6436.36it/s] 78%|███████▊  | 702483/900675 [01:55<00:30, 6400.70it/s] 78%|███████▊  | 703154/900675 [01:55<00:30, 6489.74it/s] 78%|███████▊  | 703805/900675 [01:55<00:30, 6422.02it/s] 78%|███████▊  | 704449/900675 [01:55<00:31, 6216.32it/s] 78%|███████▊  | 705073/900675 [01:55<00:32, 5944.92it/s] 78%|███████▊  | 705671/900675 [01:55<00:34, 5703.44it/s] 78%|███████▊  | 706245/900675 [01:55<00:34, 5603.95it/s] 78%|███████▊  | 706899/900675 [01:55<00:33, 5864.90it/s] 79%|███████▊  | 707489/900675 [01:55<00:33, 5796.64it/s] 79%|███████▊  | 708108/900675 [01:56<00:32, 5898.38it/s] 79%|███████▊  | 708789/900675 [01:56<00:31, 6157.72it/s] 79%|███████▉  | 709521/900675 [01:56<00:29, 6497.20it/s] 79%|███████▉  | 710173/900675 [01:56<00:29, 6377.03it/s] 79%|███████▉  | 710813/900675 [01:56<00:30, 6177.29it/s] 79%|███████▉  | 711511/900675 [01:56<00:29, 6404.15it/s] 79%|███████▉  | 712154/900675 [01:56<00:29, 6292.23it/s] 79%|███████▉  | 712820/900675 [01:56<00:29, 6398.66it/s] 79%|███████▉  | 713465/900675 [01:56<00:29, 6413.00it/s] 79%|███████▉  | 714108/900675 [01:56<00:29, 6362.24it/s] 79%|███████▉  | 714746/900675 [01:57<00:30, 6080.91it/s] 79%|███████▉  | 715399/900675 [01:57<00:29, 6200.55it/s] 80%|███████▉  | 716041/900675 [01:57<00:29, 6260.36it/s] 80%|███████▉  | 716703/900675 [01:57<00:28, 6358.62it/s] 80%|███████▉  | 717341/900675 [01:57<00:29, 6197.71it/s] 80%|███████▉  | 717963/900675 [01:57<00:31, 5746.45it/s] 80%|███████▉  | 718618/900675 [01:57<00:30, 5959.82it/s] 80%|███████▉  | 719221/900675 [01:57<00:32, 5644.31it/s] 80%|███████▉  | 719796/900675 [01:57<00:31, 5671.35it/s] 80%|███████▉  | 720369/900675 [01:58<00:31, 5685.45it/s] 80%|████████  | 720982/900675 [01:58<00:30, 5813.30it/s] 80%|████████  | 721631/900675 [01:58<00:29, 6006.29it/s] 80%|████████  | 722327/900675 [01:58<00:28, 6281.73it/s] 80%|████████  | 722958/900675 [01:58<00:30, 5892.56it/s] 80%|████████  | 723608/900675 [01:58<00:29, 6063.77it/s] 80%|████████  | 724220/900675 [01:58<00:29, 6073.70it/s] 80%|████████  | 724832/900675 [01:58<00:29, 5956.20it/s] 81%|████████  | 725431/900675 [01:58<00:30, 5795.24it/s] 81%|████████  | 726054/900675 [01:58<00:29, 5919.55it/s] 81%|████████  | 726649/900675 [01:59<00:29, 5820.39it/s] 81%|████████  | 727319/900675 [01:59<00:28, 6074.52it/s] 81%|████████  | 728017/900675 [01:59<00:27, 6335.36it/s] 81%|████████  | 728653/900675 [01:59<00:29, 5912.44it/s] 81%|████████  | 729251/900675 [01:59<00:29, 5782.87it/s] 81%|████████  | 729870/900675 [01:59<00:28, 5897.38it/s] 81%|████████  | 730464/900675 [01:59<00:29, 5744.78it/s] 81%|████████  | 731111/900675 [01:59<00:28, 5950.66it/s] 81%|████████  | 731710/900675 [01:59<00:29, 5802.92it/s] 81%|████████▏ | 732321/900675 [02:00<00:28, 5890.17it/s] 81%|████████▏ | 732988/900675 [02:00<00:27, 6115.53it/s] 81%|████████▏ | 733735/900675 [02:00<00:25, 6511.41it/s] 82%|████████▏ | 734421/900675 [02:00<00:25, 6611.94it/s] 82%|████████▏ | 735085/900675 [02:00<00:26, 6262.19it/s] 82%|████████▏ | 735717/900675 [02:00<00:27, 5902.37it/s] 82%|████████▏ | 736401/900675 [02:00<00:26, 6160.55it/s] 82%|████████▏ | 737257/900675 [02:00<00:23, 6839.44it/s] 82%|████████▏ | 737950/900675 [02:00<00:25, 6501.18it/s] 82%|████████▏ | 738609/900675 [02:01<00:25, 6410.35it/s] 82%|████████▏ | 739256/900675 [02:01<00:25, 6362.47it/s] 82%|████████▏ | 739897/900675 [02:01<00:26, 6144.58it/s] 82%|████████▏ | 740516/900675 [02:01<00:26, 6098.86it/s] 82%|████████▏ | 741142/900675 [02:01<00:25, 6138.42it/s] 82%|████████▏ | 741758/900675 [02:01<00:27, 5856.62it/s] 82%|████████▏ | 742347/900675 [02:01<00:27, 5818.46it/s] 82%|████████▏ | 742954/900675 [02:01<00:26, 5889.31it/s] 83%|████████▎ | 743645/900675 [02:01<00:25, 6177.04it/s] 83%|████████▎ | 744265/900675 [02:01<00:26, 5815.95it/s] 83%|████████▎ | 744883/900675 [02:02<00:26, 5918.38it/s] 83%|████████▎ | 745487/900675 [02:02<00:26, 5951.82it/s] 83%|████████▎ | 746086/900675 [02:02<00:26, 5880.00it/s] 83%|████████▎ | 746736/900675 [02:02<00:25, 6058.16it/s] 83%|████████▎ | 747344/900675 [02:02<00:26, 5870.93it/s] 83%|████████▎ | 747934/900675 [02:02<00:26, 5847.16it/s] 83%|████████▎ | 748521/900675 [02:02<00:26, 5812.21it/s] 83%|████████▎ | 749197/900675 [02:02<00:24, 6088.55it/s] 83%|████████▎ | 749808/900675 [02:02<00:25, 6014.66it/s] 83%|████████▎ | 750422/900675 [02:03<00:24, 6051.23it/s] 83%|████████▎ | 751029/900675 [02:03<00:25, 5941.15it/s] 83%|████████▎ | 751625/900675 [02:03<00:25, 5870.13it/s] 84%|████████▎ | 752213/900675 [02:03<00:25, 5828.10it/s] 84%|████████▎ | 752884/900675 [02:03<00:24, 6085.75it/s] 84%|████████▎ | 753494/900675 [02:03<00:24, 6080.58it/s] 84%|████████▎ | 754134/900675 [02:03<00:23, 6173.58it/s] 84%|████████▍ | 754752/900675 [02:03<00:24, 5860.58it/s] 84%|████████▍ | 755342/900675 [02:03<00:25, 5786.29it/s] 84%|████████▍ | 755924/900675 [02:03<00:25, 5654.43it/s] 84%|████████▍ | 756500/900675 [02:04<00:25, 5683.28it/s] 84%|████████▍ | 757093/900675 [02:04<00:24, 5751.09it/s] 84%|████████▍ | 757742/900675 [02:04<00:23, 5967.51it/s] 84%|████████▍ | 758381/900675 [02:04<00:23, 6087.26it/s] 84%|████████▍ | 758991/900675 [02:04<00:24, 5862.43it/s] 84%|████████▍ | 759648/900675 [02:04<00:23, 6067.23it/s] 84%|████████▍ | 760298/900675 [02:04<00:22, 6187.52it/s] 84%|████████▍ | 760945/900675 [02:04<00:22, 6267.51it/s] 85%|████████▍ | 761574/900675 [02:04<00:22, 6253.00it/s] 85%|████████▍ | 762201/900675 [02:04<00:22, 6136.96it/s] 85%|████████▍ | 762816/900675 [02:05<00:23, 5893.36it/s] 85%|████████▍ | 763408/900675 [02:05<00:23, 5767.35it/s] 85%|████████▍ | 764005/900675 [02:05<00:23, 5819.35it/s] 85%|████████▍ | 764622/900675 [02:05<00:22, 5916.62it/s] 85%|████████▍ | 765216/900675 [02:05<00:22, 5902.08it/s] 85%|████████▌ | 765808/900675 [02:05<00:23, 5758.26it/s] 85%|████████▌ | 766509/900675 [02:05<00:21, 6121.10it/s] 85%|████████▌ | 767155/900675 [02:05<00:21, 6218.96it/s] 85%|████████▌ | 767779/900675 [02:05<00:21, 6165.32it/s] 85%|████████▌ | 768397/900675 [02:06<00:21, 6060.39it/s] 85%|████████▌ | 769005/900675 [02:06<00:22, 5737.06it/s] 85%|████████▌ | 769583/900675 [02:06<00:23, 5694.49it/s] 86%|████████▌ | 770155/900675 [02:06<00:22, 5679.22it/s] 86%|████████▌ | 770734/900675 [02:06<00:22, 5708.57it/s] 86%|████████▌ | 771331/900675 [02:06<00:22, 5784.75it/s] 86%|████████▌ | 772004/900675 [02:06<00:21, 6052.53it/s] 86%|████████▌ | 772611/900675 [02:06<00:21, 5967.82it/s] 86%|████████▌ | 773271/900675 [02:06<00:20, 6141.01it/s] 86%|████████▌ | 773928/900675 [02:06<00:20, 6265.16it/s] 86%|████████▌ | 774556/900675 [02:07<00:20, 6216.02it/s] 86%|████████▌ | 775179/900675 [02:07<00:21, 5905.94it/s] 86%|████████▌ | 775822/900675 [02:07<00:20, 6055.80it/s] 86%|████████▌ | 776431/900675 [02:07<00:21, 5844.73it/s] 86%|████████▋ | 777019/900675 [02:07<00:21, 5748.72it/s] 86%|████████▋ | 777682/900675 [02:07<00:20, 5995.49it/s] 86%|████████▋ | 778299/900675 [02:07<00:20, 6041.10it/s] 86%|████████▋ | 778924/900675 [02:07<00:19, 6093.40it/s] 87%|████████▋ | 779535/900675 [02:07<00:20, 5860.68it/s] 87%|████████▋ | 780124/900675 [02:08<00:20, 5845.41it/s] 87%|████████▋ | 780715/900675 [02:08<00:20, 5859.10it/s] 87%|████████▋ | 781339/900675 [02:08<00:19, 5968.92it/s] 87%|████████▋ | 782082/900675 [02:08<00:18, 6395.21it/s] 87%|████████▋ | 782723/900675 [02:08<00:20, 5726.89it/s] 87%|████████▋ | 783310/900675 [02:08<00:20, 5762.69it/s] 87%|████████▋ | 783897/900675 [02:08<00:20, 5619.27it/s] 87%|████████▋ | 784530/900675 [02:08<00:19, 5817.00it/s] 87%|████████▋ | 785119/900675 [02:08<00:19, 5794.50it/s] 87%|████████▋ | 785748/900675 [02:08<00:19, 5937.11it/s] 87%|████████▋ | 786450/900675 [02:09<00:18, 6252.93it/s] 87%|████████▋ | 787079/900675 [02:09<00:18, 6090.14it/s] 87%|████████▋ | 787692/900675 [02:09<00:18, 6083.46it/s] 88%|████████▊ | 788349/900675 [02:09<00:18, 6217.19it/s] 88%|████████▊ | 788973/900675 [02:09<00:18, 6057.95it/s] 88%|████████▊ | 789583/900675 [02:09<00:18, 6065.10it/s] 88%|████████▊ | 790241/900675 [02:09<00:17, 6214.97it/s] 88%|████████▊ | 790876/900675 [02:09<00:17, 6254.19it/s] 88%|████████▊ | 791503/900675 [02:09<00:18, 6035.02it/s] 88%|████████▊ | 792109/900675 [02:10<00:19, 5666.26it/s] 88%|████████▊ | 792821/900675 [02:10<00:17, 6069.48it/s] 88%|████████▊ | 793480/900675 [02:10<00:17, 6214.29it/s] 88%|████████▊ | 794107/900675 [02:10<00:18, 5631.31it/s] 88%|████████▊ | 794777/900675 [02:10<00:17, 5919.68it/s] 88%|████████▊ | 795401/900675 [02:10<00:17, 6008.53it/s] 88%|████████▊ | 796040/900675 [02:10<00:17, 6106.97it/s] 88%|████████▊ | 796658/900675 [02:10<00:16, 6126.69it/s] 89%|████████▊ | 797276/900675 [02:10<00:16, 6124.06it/s] 89%|████████▊ | 797892/900675 [02:10<00:17, 6014.70it/s] 89%|████████▊ | 798515/900675 [02:11<00:16, 6068.11it/s] 89%|████████▊ | 799148/900675 [02:11<00:16, 6141.34it/s] 89%|████████▉ | 799764/900675 [02:11<00:16, 6111.52it/s] 89%|████████▉ | 800377/900675 [02:11<00:17, 5886.74it/s] 89%|████████▉ | 800968/900675 [02:11<00:17, 5754.90it/s] 89%|████████▉ | 801601/900675 [02:11<00:16, 5919.18it/s] 89%|████████▉ | 802219/900675 [02:11<00:16, 5992.31it/s] 89%|████████▉ | 802820/900675 [02:11<00:17, 5512.89it/s] 89%|████████▉ | 803526/900675 [02:11<00:16, 5933.99it/s] 89%|████████▉ | 804129/900675 [02:12<00:16, 5841.87it/s] 89%|████████▉ | 804769/900675 [02:12<00:15, 5998.72it/s] 89%|████████▉ | 805426/900675 [02:12<00:15, 6161.55it/s] 89%|████████▉ | 806047/900675 [02:12<00:15, 5932.36it/s] 90%|████████▉ | 806645/900675 [02:12<00:15, 5926.99it/s] 90%|████████▉ | 807257/900675 [02:12<00:15, 5980.87it/s] 90%|████████▉ | 807858/900675 [02:12<00:15, 5876.15it/s] 90%|████████▉ | 808448/900675 [02:12<00:16, 5446.76it/s] 90%|████████▉ | 809176/900675 [02:12<00:15, 5948.50it/s] 90%|████████▉ | 809780/900675 [02:12<00:15, 5782.99it/s] 90%|████████▉ | 810365/900675 [02:13<00:15, 5651.76it/s] 90%|█████████ | 810935/900675 [02:13<00:16, 5484.80it/s] 90%|█████████ | 811507/900675 [02:13<00:16, 5550.56it/s] 90%|█████████ | 812146/900675 [02:13<00:15, 5790.31it/s] 90%|█████████ | 812729/900675 [02:13<00:15, 5724.88it/s] 90%|█████████ | 813443/900675 [02:13<00:14, 6133.22it/s] 90%|█████████ | 814079/900675 [02:13<00:13, 6195.74it/s] 90%|█████████ | 814761/900675 [02:13<00:13, 6377.89it/s] 91%|█████████ | 815490/900675 [02:13<00:12, 6645.17it/s] 91%|█████████ | 816186/900675 [02:14<00:12, 6731.14it/s] 91%|█████████ | 816922/900675 [02:14<00:12, 6905.34it/s] 91%|█████████ | 817614/900675 [02:14<00:12, 6892.56it/s] 91%|█████████ | 818338/900675 [02:14<00:11, 6989.75it/s] 91%|█████████ | 819038/900675 [02:14<00:12, 6289.59it/s] 91%|█████████ | 819681/900675 [02:14<00:13, 6201.45it/s] 91%|█████████ | 820414/900675 [02:14<00:12, 6513.04it/s] 91%|█████████ | 821104/900675 [02:14<00:12, 6623.09it/s] 91%|█████████ | 821773/900675 [02:14<00:12, 6239.37it/s] 91%|█████████▏| 822406/900675 [02:14<00:13, 5980.92it/s] 91%|█████████▏| 823050/900675 [02:15<00:12, 6105.64it/s] 91%|█████████▏| 823679/900675 [02:15<00:12, 6154.08it/s] 92%|█████████▏| 824313/900675 [02:15<00:12, 6206.97it/s] 92%|█████████▏| 824938/900675 [02:15<00:13, 5697.57it/s] 92%|█████████▏| 825518/900675 [02:15<00:13, 5535.35it/s] 92%|█████████▏| 826126/900675 [02:15<00:13, 5683.21it/s] 92%|█████████▏| 826761/900675 [02:15<00:12, 5869.01it/s] 92%|█████████▏| 827409/900675 [02:15<00:12, 6030.89it/s] 92%|█████████▏| 828064/900675 [02:15<00:11, 6175.48it/s] 92%|█████████▏| 828686/900675 [02:16<00:12, 5958.22it/s] 92%|█████████▏| 829286/900675 [02:16<00:13, 5331.34it/s] 92%|█████████▏| 829979/900675 [02:16<00:12, 5748.78it/s] 92%|█████████▏| 830569/900675 [02:16<00:12, 5646.81it/s] 92%|█████████▏| 831176/900675 [02:16<00:12, 5761.08it/s] 92%|█████████▏| 831774/900675 [02:16<00:11, 5821.90it/s] 92%|█████████▏| 832362/900675 [02:16<00:11, 5719.01it/s] 92%|█████████▏| 833025/900675 [02:16<00:11, 5979.47it/s] 93%|█████████▎| 833696/900675 [02:16<00:10, 6181.45it/s] 93%|█████████▎| 834318/900675 [02:17<00:11, 6009.80it/s] 93%|█████████▎| 834967/900675 [02:17<00:10, 6144.34it/s] 93%|█████████▎| 835611/900675 [02:17<00:10, 6221.02it/s] 93%|█████████▎| 836236/900675 [02:17<00:10, 6209.26it/s] 93%|█████████▎| 836862/900675 [02:17<00:10, 6214.91it/s] 93%|█████████▎| 837485/900675 [02:17<00:10, 6214.06it/s] 93%|█████████▎| 838139/900675 [02:17<00:09, 6302.05it/s] 93%|█████████▎| 838770/900675 [02:17<00:10, 6120.63it/s] 93%|█████████▎| 839384/900675 [02:17<00:10, 5914.66it/s] 93%|█████████▎| 839978/900675 [02:17<00:10, 5844.64it/s] 93%|█████████▎| 840774/900675 [02:18<00:09, 6448.56it/s] 93%|█████████▎| 841423/900675 [02:18<00:09, 6264.79it/s] 93%|█████████▎| 842053/900675 [02:18<00:09, 6243.72it/s] 94%|█████████▎| 842680/900675 [02:18<00:10, 5753.85it/s] 94%|█████████▎| 843264/900675 [02:18<00:10, 5457.38it/s] 94%|█████████▎| 843817/900675 [02:18<00:10, 5366.10it/s] 94%|█████████▎| 844359/900675 [02:18<00:10, 5173.58it/s] 94%|█████████▍| 844936/900675 [02:18<00:10, 5337.06it/s] 94%|█████████▍| 845520/900675 [02:18<00:10, 5474.44it/s] 94%|█████████▍| 846099/900675 [02:19<00:09, 5563.11it/s] 94%|█████████▍| 846674/900675 [02:19<00:09, 5616.33it/s] 94%|█████████▍| 847306/900675 [02:19<00:09, 5820.82it/s] 94%|█████████▍| 847938/900675 [02:19<00:08, 5967.02it/s] 94%|█████████▍| 848537/900675 [02:19<00:08, 5925.34it/s] 94%|█████████▍| 849131/900675 [02:19<00:08, 5893.24it/s] 94%|█████████▍| 849722/900675 [02:19<00:08, 5884.51it/s] 94%|█████████▍| 850366/900675 [02:19<00:08, 6047.65it/s] 94%|█████████▍| 850972/900675 [02:19<00:08, 5656.48it/s] 95%|█████████▍| 851636/900675 [02:19<00:08, 5927.47it/s] 95%|█████████▍| 852266/900675 [02:20<00:08, 6033.68it/s] 95%|█████████▍| 852874/900675 [02:20<00:08, 5588.96it/s] 95%|█████████▍| 853448/900675 [02:20<00:08, 5630.48it/s] 95%|█████████▍| 854079/900675 [02:20<00:08, 5818.60it/s] 95%|█████████▍| 854685/900675 [02:20<00:07, 5887.87it/s] 95%|█████████▍| 855278/900675 [02:20<00:07, 5882.73it/s] 95%|█████████▌| 855922/900675 [02:20<00:07, 6045.86it/s] 95%|█████████▌| 856529/900675 [02:20<00:07, 6006.85it/s] 95%|█████████▌| 857188/900675 [02:20<00:07, 6178.69it/s] 95%|█████████▌| 857808/900675 [02:21<00:07, 5896.17it/s] 95%|█████████▌| 858421/900675 [02:21<00:07, 5956.33it/s] 95%|█████████▌| 859020/900675 [02:21<00:07, 5843.83it/s] 95%|█████████▌| 859666/900675 [02:21<00:06, 6020.02it/s] 96%|█████████▌| 860300/900675 [02:21<00:06, 6111.66it/s] 96%|█████████▌| 861039/900675 [02:21<00:06, 6486.63it/s] 96%|█████████▌| 861690/900675 [02:21<00:06, 6373.25it/s] 96%|█████████▌| 862332/900675 [02:21<00:06, 6381.49it/s] 96%|█████████▌| 862986/900675 [02:21<00:05, 6413.27it/s] 96%|█████████▌| 863629/900675 [02:21<00:06, 6107.76it/s] 96%|█████████▌| 864277/900675 [02:22<00:05, 6198.91it/s] 96%|█████████▌| 864900/900675 [02:22<00:05, 6063.40it/s] 96%|█████████▌| 865525/900675 [02:22<00:05, 6115.80it/s] 96%|█████████▌| 866139/900675 [02:22<00:05, 5776.22it/s] 96%|█████████▌| 866814/900675 [02:22<00:05, 6048.48it/s] 96%|█████████▋| 867424/900675 [02:22<00:05, 6010.34it/s] 96%|█████████▋| 868037/900675 [02:22<00:05, 6032.81it/s] 96%|█████████▋| 868643/900675 [02:22<00:05, 5945.02it/s] 97%|█████████▋| 869248/900675 [02:22<00:05, 5975.06it/s] 97%|█████████▋| 869928/900675 [02:22<00:04, 6207.43it/s] 97%|█████████▋| 870551/900675 [02:23<00:04, 6212.07it/s] 97%|█████████▋| 871306/900675 [02:23<00:04, 6603.16it/s] 97%|█████████▋| 871968/900675 [02:23<00:04, 6121.40it/s] 97%|█████████▋| 872588/900675 [02:23<00:04, 6024.67it/s] 97%|█████████▋| 873263/900675 [02:23<00:04, 6229.43it/s] 97%|█████████▋| 873891/900675 [02:23<00:04, 6169.93it/s] 97%|█████████▋| 874517/900675 [02:23<00:04, 6195.33it/s] 97%|█████████▋| 875139/900675 [02:23<00:04, 6170.04it/s] 97%|█████████▋| 875766/900675 [02:23<00:04, 6191.87it/s] 97%|█████████▋| 876404/900675 [02:24<00:03, 6244.94it/s] 97%|█████████▋| 877030/900675 [02:24<00:03, 6100.33it/s] 97%|█████████▋| 877642/900675 [02:24<00:03, 5966.76it/s] 98%|█████████▊| 878292/900675 [02:24<00:03, 6119.89it/s] 98%|█████████▊| 878906/900675 [02:24<00:03, 5985.90it/s] 98%|█████████▊| 879529/900675 [02:24<00:03, 6048.60it/s] 98%|█████████▊| 880136/900675 [02:24<00:03, 5821.14it/s] 98%|█████████▊| 880721/900675 [02:24<00:03, 5757.32it/s] 98%|█████████▊| 881337/900675 [02:24<00:03, 5869.46it/s] 98%|█████████▊| 881926/900675 [02:24<00:03, 5539.35it/s] 98%|█████████▊| 882485/900675 [02:25<00:03, 5533.14it/s] 98%|█████████▊| 883184/900675 [02:25<00:02, 5949.12it/s] 98%|█████████▊| 883783/900675 [02:25<00:03, 5628.43it/s] 98%|█████████▊| 884409/900675 [02:25<00:02, 5803.50it/s] 98%|█████████▊| 884995/900675 [02:25<00:02, 5679.95it/s] 98%|█████████▊| 885676/900675 [02:25<00:02, 5999.73it/s] 98%|█████████▊| 886404/900675 [02:25<00:02, 6370.41it/s] 98%|█████████▊| 887046/900675 [02:25<00:02, 6314.32it/s] 99%|█████████▊| 887681/900675 [02:25<00:02, 6006.80it/s] 99%|█████████▊| 888287/900675 [02:26<00:02, 5961.40it/s] 99%|█████████▊| 888887/900675 [02:26<00:02, 5785.95it/s] 99%|█████████▉| 889581/900675 [02:26<00:01, 6110.20it/s] 99%|█████████▉| 890196/900675 [02:26<00:01, 6058.04it/s] 99%|█████████▉| 890847/900675 [02:26<00:01, 6185.87it/s] 99%|█████████▉| 891468/900675 [02:26<00:01, 6055.02it/s] 99%|█████████▉| 892076/900675 [02:26<00:01, 5926.05it/s] 99%|█████████▉| 892699/900675 [02:26<00:01, 6007.22it/s] 99%|█████████▉| 893302/900675 [02:26<00:01, 5757.87it/s] 99%|█████████▉| 893881/900675 [02:27<00:01, 5550.76it/s] 99%|█████████▉| 894446/900675 [02:27<00:01, 5572.30it/s] 99%|█████████▉| 895006/900675 [02:27<00:01, 5564.69it/s] 99%|█████████▉| 895619/900675 [02:27<00:00, 5720.78it/s]100%|█████████▉| 896308/900675 [02:27<00:00, 6059.85it/s]100%|█████████▉| 896916/900675 [02:27<00:00, 5998.55it/s]100%|█████████▉| 897542/900675 [02:27<00:00, 6075.17it/s]100%|█████████▉| 898200/900675 [02:27<00:00, 6220.34it/s]100%|█████████▉| 898872/900675 [02:27<00:00, 6368.44it/s]100%|█████████▉| 899510/900675 [02:27<00:00, 6332.49it/s]100%|█████████▉| 900144/900675 [02:28<00:00, 6231.54it/s]100%|██████████| 900675/900675 [02:28<00:00, 6081.10it/s]

gathering stats for n=1
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 1907/900675 [00:00<00:47, 19066.88it/s]  0%|          | 3974/900675 [00:00<00:44, 20003.05it/s]  1%|          | 6215/900675 [00:00<00:42, 21083.78it/s]  1%|          | 8324/900675 [00:00<00:44, 20149.87it/s]  1%|          | 10345/900675 [00:00<00:44, 20023.45it/s]  1%|▏         | 12351/900675 [00:00<00:44, 19894.73it/s]  2%|▏         | 14406/900675 [00:00<00:44, 20101.65it/s]  2%|▏         | 16419/900675 [00:00<00:44, 19928.86it/s]  2%|▏         | 18414/900675 [00:00<00:44, 19791.30it/s]  2%|▏         | 20518/900675 [00:01<00:43, 20169.81it/s]  3%|▎         | 22537/900675 [00:01<00:43, 20112.19it/s]  3%|▎         | 24823/900675 [00:01<00:41, 20938.97it/s]  3%|▎         | 26919/900675 [00:01<00:42, 20503.23it/s]  3%|▎         | 28973/900675 [00:01<00:42, 20352.78it/s]  3%|▎         | 31011/900675 [00:01<00:43, 19913.93it/s]  4%|▎         | 33006/900675 [00:01<00:44, 19658.84it/s]  4%|▍         | 35048/900675 [00:01<00:43, 19876.57it/s]  4%|▍         | 37038/900675 [00:01<00:43, 19635.62it/s]  4%|▍         | 39004/900675 [00:01<00:44, 19453.82it/s]  5%|▍         | 41037/900675 [00:02<00:43, 19706.82it/s]  5%|▍         | 43010/900675 [00:02<00:44, 19233.26it/s]  5%|▌         | 45040/900675 [00:02<00:43, 19537.86it/s]  5%|▌         | 47144/900675 [00:02<00:42, 19978.02it/s]  6%|▌         | 49594/900675 [00:02<00:39, 21308.41it/s]  6%|▌         | 51729/900675 [00:02<00:40, 20715.39it/s]  6%|▌         | 53931/900675 [00:02<00:40, 21095.94it/s]  6%|▌         | 56046/900675 [00:02<00:40, 20707.25it/s]  6%|▋         | 58122/900675 [00:02<00:41, 20120.95it/s]  7%|▋         | 60140/900675 [00:02<00:41, 20132.33it/s]  7%|▋         | 62272/900675 [00:03<00:40, 20478.26it/s]  7%|▋         | 64324/900675 [00:03<00:41, 20282.86it/s]  7%|▋         | 66355/900675 [00:03<00:41, 20233.74it/s]  8%|▊         | 68381/900675 [00:03<00:41, 19983.82it/s]  8%|▊         | 70560/900675 [00:03<00:40, 20514.16it/s]  8%|▊         | 72614/900675 [00:03<00:40, 20459.55it/s]  8%|▊         | 74986/900675 [00:03<00:38, 21423.52it/s]  9%|▊         | 77131/900675 [00:03<00:39, 21045.84it/s]  9%|▉         | 79239/900675 [00:03<00:40, 20326.88it/s]  9%|▉         | 81303/900675 [00:04<00:40, 20417.10it/s]  9%|▉         | 83380/900675 [00:04<00:39, 20518.31it/s]  9%|▉         | 85436/900675 [00:04<00:39, 20437.91it/s] 10%|▉         | 87536/900675 [00:04<00:39, 20603.48it/s] 10%|▉         | 89599/900675 [00:04<00:40, 20110.14it/s] 10%|█         | 91614/900675 [00:04<00:40, 20063.70it/s] 10%|█         | 93682/900675 [00:04<00:39, 20244.36it/s] 11%|█         | 95709/900675 [00:04<00:40, 20118.61it/s] 11%|█         | 97874/900675 [00:04<00:39, 20565.01it/s] 11%|█         | 99933/900675 [00:04<00:39, 20213.33it/s] 11%|█▏        | 101957/900675 [00:05<00:39, 20180.08it/s] 12%|█▏        | 103977/900675 [00:05<00:40, 19807.91it/s] 12%|█▏        | 106001/900675 [00:05<00:39, 19932.13it/s] 12%|█▏        | 107996/900675 [00:05<00:39, 19894.23it/s] 12%|█▏        | 109987/900675 [00:05<00:40, 19641.83it/s] 12%|█▏        | 112075/900675 [00:05<00:39, 19997.87it/s] 13%|█▎        | 114077/900675 [00:05<00:39, 19999.37it/s] 13%|█▎        | 116078/900675 [00:05<00:39, 19720.21it/s] 13%|█▎        | 118052/900675 [00:05<00:40, 19508.06it/s] 13%|█▎        | 120171/900675 [00:05<00:39, 20000.16it/s] 14%|█▎        | 122173/900675 [00:06<00:38, 19994.67it/s] 14%|█▍        | 124174/900675 [00:06<00:39, 19808.07it/s] 14%|█▍        | 126265/900675 [00:06<00:38, 20128.88it/s] 14%|█▍        | 128284/900675 [00:06<00:38, 20143.17it/s] 14%|█▍        | 130300/900675 [00:06<00:38, 19867.49it/s] 15%|█▍        | 132289/900675 [00:06<00:39, 19574.27it/s] 15%|█▍        | 134355/900675 [00:06<00:38, 19893.02it/s] 15%|█▌        | 136604/900675 [00:06<00:36, 20657.19it/s] 15%|█▌        | 138733/900675 [00:06<00:36, 20844.51it/s] 16%|█▌        | 140820/900675 [00:06<00:36, 20621.51it/s] 16%|█▌        | 142884/900675 [00:07<00:37, 20070.03it/s] 16%|█▌        | 145149/900675 [00:07<00:36, 20820.54it/s] 16%|█▋        | 147236/900675 [00:07<00:36, 20513.88it/s] 17%|█▋        | 149292/900675 [00:07<00:37, 20115.64it/s] 17%|█▋        | 151308/900675 [00:07<00:37, 19846.87it/s] 17%|█▋        | 153327/900675 [00:07<00:37, 19945.68it/s] 17%|█▋        | 155324/900675 [00:07<00:37, 19674.69it/s] 17%|█▋        | 157485/900675 [00:07<00:36, 20240.10it/s] 18%|█▊        | 159512/900675 [00:07<00:36, 20034.70it/s] 18%|█▊        | 161603/900675 [00:08<00:36, 20291.48it/s] 18%|█▊        | 163635/900675 [00:08<00:36, 20291.86it/s] 18%|█▊        | 165786/900675 [00:08<00:35, 20648.66it/s] 19%|█▊        | 167853/900675 [00:08<00:35, 20598.60it/s] 19%|█▉        | 169914/900675 [00:08<00:35, 20341.33it/s] 19%|█▉        | 171950/900675 [00:08<00:36, 20059.92it/s] 19%|█▉        | 173958/900675 [00:08<00:36, 20016.00it/s] 20%|█▉        | 175961/900675 [00:08<00:36, 19633.19it/s] 20%|█▉        | 177927/900675 [00:08<00:37, 19025.92it/s] 20%|█▉        | 179919/900675 [00:08<00:37, 19281.89it/s] 20%|██        | 181852/900675 [00:09<00:37, 19269.77it/s] 20%|██        | 183782/900675 [00:09<00:37, 18980.49it/s] 21%|██        | 185683/900675 [00:09<00:37, 18981.61it/s] 21%|██        | 187705/900675 [00:09<00:36, 19343.23it/s] 21%|██        | 189771/900675 [00:09<00:36, 19727.88it/s] 21%|██▏       | 191808/900675 [00:09<00:35, 19916.62it/s] 22%|██▏       | 193846/900675 [00:09<00:35, 20051.85it/s] 22%|██▏       | 195953/900675 [00:09<00:34, 20349.09it/s] 22%|██▏       | 198120/900675 [00:09<00:33, 20742.90it/s] 22%|██▏       | 200196/900675 [00:09<00:34, 20410.01it/s] 22%|██▏       | 202239/900675 [00:10<00:34, 20096.17it/s] 23%|██▎       | 204270/900675 [00:10<00:34, 20155.94it/s] 23%|██▎       | 206288/900675 [00:10<00:34, 20101.72it/s] 23%|██▎       | 208300/900675 [00:10<00:35, 19734.75it/s] 23%|██▎       | 210276/900675 [00:10<00:35, 19633.59it/s] 24%|██▎       | 212356/900675 [00:10<00:34, 19973.03it/s] 24%|██▍       | 214355/900675 [00:10<00:34, 19865.58it/s] 24%|██▍       | 216343/900675 [00:10<00:34, 19584.07it/s] 24%|██▍       | 218346/900675 [00:10<00:34, 19713.74it/s] 24%|██▍       | 220355/900675 [00:10<00:34, 19822.44it/s] 25%|██▍       | 222339/900675 [00:11<00:34, 19381.87it/s] 25%|██▍       | 224320/900675 [00:11<00:34, 19505.41it/s] 25%|██▌       | 226516/900675 [00:11<00:33, 20221.40it/s] 25%|██▌       | 228541/900675 [00:11<00:33, 19821.88it/s] 26%|██▌       | 230527/900675 [00:11<00:34, 19482.65it/s] 26%|██▌       | 232479/900675 [00:11<00:34, 19461.68it/s] 26%|██▌       | 234773/900675 [00:11<00:32, 20480.33it/s] 26%|██▋       | 236825/900675 [00:11<00:32, 20140.23it/s] 27%|██▋       | 238911/900675 [00:11<00:32, 20350.57it/s] 27%|██▋       | 240987/900675 [00:11<00:32, 20471.01it/s] 27%|██▋       | 243067/900675 [00:12<00:31, 20566.00it/s] 27%|██▋       | 245126/900675 [00:12<00:32, 20410.87it/s] 27%|██▋       | 247373/900675 [00:12<00:31, 21021.40it/s] 28%|██▊       | 249507/900675 [00:12<00:30, 21111.90it/s] 28%|██▊       | 251620/900675 [00:12<00:30, 21018.06it/s] 28%|██▊       | 253723/900675 [00:12<00:31, 20824.98it/s] 28%|██▊       | 255820/900675 [00:12<00:30, 20865.98it/s] 29%|██▊       | 257908/900675 [00:12<00:30, 20852.52it/s] 29%|██▉       | 259994/900675 [00:12<00:31, 20029.68it/s] 29%|██▉       | 262053/900675 [00:13<00:31, 20183.05it/s] 29%|██▉       | 264077/900675 [00:13<00:32, 19822.92it/s] 30%|██▉       | 266064/900675 [00:13<00:32, 19575.86it/s] 30%|██▉       | 268025/900675 [00:13<00:32, 19537.58it/s] 30%|██▉       | 270072/900675 [00:13<00:31, 19807.72it/s] 30%|███       | 272135/900675 [00:13<00:31, 20048.58it/s] 30%|███       | 274389/900675 [00:13<00:30, 20785.41it/s] 31%|███       | 276470/900675 [00:13<00:31, 19836.20it/s] 31%|███       | 278580/900675 [00:13<00:30, 20198.86it/s] 31%|███       | 280609/900675 [00:13<00:30, 20075.98it/s] 31%|███▏      | 282623/900675 [00:14<00:31, 19846.10it/s] 32%|███▏      | 284681/900675 [00:14<00:30, 20050.73it/s] 32%|███▏      | 286690/900675 [00:14<00:30, 19915.33it/s] 32%|███▏      | 288710/900675 [00:14<00:30, 19991.05it/s] 32%|███▏      | 290711/900675 [00:14<00:30, 19680.39it/s] 33%|███▎      | 292759/900675 [00:14<00:30, 19912.20it/s] 33%|███▎      | 294753/900675 [00:14<00:30, 19822.94it/s] 33%|███▎      | 296807/900675 [00:14<00:30, 20034.44it/s] 33%|███▎      | 298857/900675 [00:14<00:29, 20170.32it/s] 33%|███▎      | 300875/900675 [00:14<00:29, 20142.09it/s] 34%|███▎      | 302890/900675 [00:15<00:29, 20043.35it/s] 34%|███▍      | 304943/900675 [00:15<00:29, 20184.72it/s] 34%|███▍      | 306962/900675 [00:15<00:30, 19765.75it/s] 34%|███▍      | 308941/900675 [00:15<00:30, 19718.55it/s] 35%|███▍      | 311120/900675 [00:15<00:28, 20329.52it/s] 35%|███▍      | 313195/900675 [00:15<00:28, 20443.71it/s] 35%|███▌      | 315241/900675 [00:15<00:28, 20352.24it/s] 35%|███▌      | 317278/900675 [00:15<00:29, 20095.67it/s] 35%|███▌      | 319289/900675 [00:15<00:29, 20027.85it/s] 36%|███▌      | 321344/900675 [00:15<00:28, 20179.05it/s] 36%|███▌      | 323363/900675 [00:16<00:29, 19775.42it/s] 36%|███▌      | 325343/900675 [00:16<00:29, 19541.30it/s] 36%|███▋      | 327424/900675 [00:16<00:28, 19908.62it/s] 37%|███▋      | 329566/900675 [00:16<00:28, 20350.08it/s] 37%|███▋      | 331604/900675 [00:16<00:28, 19796.66it/s] 37%|███▋      | 333598/900675 [00:16<00:28, 19837.92it/s] 37%|███▋      | 335647/900675 [00:16<00:28, 20026.50it/s] 38%|███▊      | 337802/900675 [00:16<00:27, 20475.38it/s] 38%|███▊      | 339852/900675 [00:16<00:27, 20198.43it/s] 38%|███▊      | 341875/900675 [00:17<00:28, 19936.23it/s] 38%|███▊      | 344014/900675 [00:17<00:27, 20359.48it/s] 38%|███▊      | 346053/900675 [00:17<00:28, 19737.79it/s] 39%|███▊      | 348130/900675 [00:17<00:27, 20035.03it/s] 39%|███▉      | 350336/900675 [00:17<00:26, 20626.01it/s] 39%|███▉      | 352404/900675 [00:17<00:27, 19761.74it/s] 39%|███▉      | 354447/900675 [00:17<00:27, 19949.54it/s] 40%|███▉      | 356450/900675 [00:17<00:27, 19787.08it/s] 40%|███▉      | 358434/900675 [00:17<00:28, 19330.15it/s] 40%|████      | 360471/900675 [00:17<00:27, 19626.49it/s] 40%|████      | 362683/900675 [00:18<00:26, 20354.97it/s] 41%|████      | 364843/900675 [00:18<00:25, 20719.26it/s] 41%|████      | 366920/900675 [00:18<00:25, 20589.73it/s] 41%|████      | 368982/900675 [00:18<00:26, 20365.00it/s] 41%|████      | 371021/900675 [00:18<00:26, 20216.15it/s] 41%|████▏     | 373045/900675 [00:18<00:26, 20046.07it/s] 42%|████▏     | 375113/900675 [00:18<00:25, 20226.37it/s] 42%|████▏     | 377137/900675 [00:18<00:26, 19522.77it/s] 42%|████▏     | 379213/900675 [00:18<00:26, 19878.27it/s] 42%|████▏     | 381414/900675 [00:18<00:25, 20501.52it/s] 43%|████▎     | 383470/900675 [00:19<00:26, 19827.11it/s] 43%|████▎     | 385573/900675 [00:19<00:25, 20168.66it/s] 43%|████▎     | 387719/900675 [00:19<00:24, 20545.93it/s] 43%|████▎     | 389780/900675 [00:19<00:25, 20344.42it/s] 44%|████▎     | 391819/900675 [00:19<00:25, 20049.19it/s] 44%|████▎     | 393828/900675 [00:19<00:25, 19709.97it/s] 44%|████▍     | 395886/900675 [00:19<00:25, 19960.58it/s] 44%|████▍     | 397885/900675 [00:19<00:25, 19595.72it/s] 44%|████▍     | 400095/900675 [00:19<00:24, 20319.91it/s] 45%|████▍     | 402131/900675 [00:20<00:25, 19689.25it/s] 45%|████▍     | 404106/900675 [00:20<00:25, 19536.21it/s] 45%|████▌     | 406073/900675 [00:20<00:25, 19572.51it/s] 45%|████▌     | 408034/900675 [00:20<00:25, 19328.29it/s] 46%|████▌     | 410075/900675 [00:20<00:24, 19638.56it/s] 46%|████▌     | 412042/900675 [00:20<00:24, 19624.74it/s] 46%|████▌     | 414007/900675 [00:20<00:25, 18816.62it/s] 46%|████▌     | 415897/900675 [00:20<00:25, 18823.05it/s] 46%|████▋     | 417884/900675 [00:20<00:25, 19125.26it/s] 47%|████▋     | 419851/900675 [00:20<00:24, 19282.74it/s] 47%|████▋     | 421783/900675 [00:21<00:25, 19079.61it/s] 47%|████▋     | 423765/900675 [00:21<00:24, 19294.69it/s] 47%|████▋     | 425697/900675 [00:21<00:24, 19300.30it/s] 48%|████▊     | 427822/900675 [00:21<00:23, 19871.86it/s] 48%|████▊     | 429951/900675 [00:21<00:23, 20292.37it/s] 48%|████▊     | 431982/900675 [00:21<00:23, 19599.72it/s] 48%|████▊     | 434178/900675 [00:21<00:22, 20283.05it/s] 48%|████▊     | 436213/900675 [00:21<00:23, 19840.09it/s] 49%|████▊     | 438216/900675 [00:21<00:23, 19889.36it/s] 49%|████▉     | 440210/900675 [00:21<00:23, 19439.52it/s] 49%|████▉     | 442280/900675 [00:22<00:23, 19805.45it/s] 49%|████▉     | 444285/900675 [00:22<00:22, 19874.31it/s] 50%|████▉     | 446276/900675 [00:22<00:23, 19269.11it/s] 50%|████▉     | 448366/900675 [00:22<00:22, 19733.27it/s] 50%|█████     | 450398/900675 [00:22<00:22, 19895.39it/s] 50%|█████     | 452392/900675 [00:22<00:22, 19554.78it/s] 50%|█████     | 454352/900675 [00:22<00:22, 19454.73it/s] 51%|█████     | 456379/900675 [00:22<00:22, 19688.47it/s] 51%|█████     | 458395/900675 [00:22<00:22, 19825.20it/s] 51%|█████     | 460416/900675 [00:22<00:22, 19939.26it/s] 51%|█████▏    | 462412/900675 [00:23<00:22, 19696.06it/s] 52%|█████▏    | 464402/900675 [00:23<00:22, 19753.27it/s] 52%|█████▏    | 466433/900675 [00:23<00:21, 19915.83it/s] 52%|█████▏    | 468426/900675 [00:23<00:21, 19910.23it/s] 52%|█████▏    | 470418/900675 [00:23<00:21, 19676.20it/s] 52%|█████▏    | 472447/900675 [00:23<00:21, 19856.21it/s] 53%|█████▎    | 474434/900675 [00:23<00:21, 19858.33it/s] 53%|█████▎    | 476559/900675 [00:23<00:20, 20269.92it/s] 53%|█████▎    | 478609/900675 [00:23<00:20, 20334.33it/s] 53%|█████▎    | 480659/900675 [00:24<00:20, 20380.61it/s] 54%|█████▎    | 482698/900675 [00:24<00:21, 19828.49it/s] 54%|█████▍    | 484782/900675 [00:24<00:20, 20121.51it/s] 54%|█████▍    | 486838/900675 [00:24<00:20, 20248.63it/s] 54%|█████▍    | 488882/900675 [00:24<00:20, 20292.14it/s] 55%|█████▍    | 490913/900675 [00:24<00:20, 19788.92it/s] 55%|█████▍    | 492953/900675 [00:24<00:20, 19958.92it/s] 55%|█████▍    | 495051/900675 [00:24<00:20, 20259.92it/s] 55%|█████▌    | 497080/900675 [00:24<00:20, 20074.97it/s] 55%|█████▌    | 499125/900675 [00:24<00:19, 20176.78it/s] 56%|█████▌    | 501252/900675 [00:25<00:19, 20496.92it/s] 56%|█████▌    | 503304/900675 [00:25<00:19, 19996.98it/s] 56%|█████▌    | 505308/900675 [00:25<00:19, 19911.84it/s] 56%|█████▋    | 507368/900675 [00:25<00:19, 20112.46it/s] 57%|█████▋    | 509382/900675 [00:25<00:19, 19826.28it/s] 57%|█████▋    | 511603/900675 [00:25<00:18, 20517.34it/s] 57%|█████▋    | 513658/900675 [00:25<00:19, 20292.31it/s] 57%|█████▋    | 515690/900675 [00:25<00:19, 20082.25it/s] 57%|█████▋    | 517700/900675 [00:25<00:19, 19981.51it/s] 58%|█████▊    | 519749/900675 [00:25<00:18, 20130.16it/s] 58%|█████▊    | 521764/900675 [00:26<00:18, 20033.68it/s] 58%|█████▊    | 523769/900675 [00:26<00:18, 19840.31it/s] 58%|█████▊    | 525754/900675 [00:26<00:19, 19553.66it/s] 59%|█████▊    | 527764/900675 [00:26<00:18, 19713.22it/s] 59%|█████▉    | 529843/900675 [00:26<00:18, 20026.31it/s] 59%|█████▉    | 531847/900675 [00:26<00:18, 19910.69it/s] 59%|█████▉    | 533839/900675 [00:26<00:18, 19713.12it/s] 59%|█████▉    | 535812/900675 [00:26<00:18, 19654.28it/s] 60%|█████▉    | 537795/900675 [00:26<00:18, 19700.10it/s] 60%|█████▉    | 539823/900675 [00:26<00:18, 19870.44it/s] 60%|██████    | 541818/900675 [00:27<00:18, 19892.67it/s] 60%|██████    | 543896/900675 [00:27<00:17, 20157.34it/s] 61%|██████    | 545913/900675 [00:27<00:17, 19796.68it/s] 61%|██████    | 547929/900675 [00:27<00:17, 19897.24it/s] 61%|██████    | 550026/900675 [00:27<00:17, 20214.99it/s] 61%|██████▏   | 552049/900675 [00:27<00:17, 20116.00it/s] 62%|██████▏   | 554062/900675 [00:27<00:17, 19836.94it/s] 62%|██████▏   | 556047/900675 [00:27<00:17, 19728.32it/s] 62%|██████▏   | 558021/900675 [00:27<00:17, 19661.48it/s] 62%|██████▏   | 559988/900675 [00:27<00:17, 19140.47it/s] 62%|██████▏   | 562054/900675 [00:28<00:17, 19583.16it/s] 63%|██████▎   | 564093/900675 [00:28<00:16, 19818.69it/s] 63%|██████▎   | 566078/900675 [00:28<00:17, 19603.86it/s] 63%|██████▎   | 568042/900675 [00:28<00:16, 19612.67it/s] 63%|██████▎   | 570134/900675 [00:28<00:16, 19997.80it/s] 64%|██████▎   | 572275/900675 [00:28<00:16, 20410.35it/s] 64%|██████▍   | 574318/900675 [00:28<00:16, 20262.35it/s] 64%|██████▍   | 576346/900675 [00:28<00:16, 19997.71it/s] 64%|██████▍   | 578348/900675 [00:28<00:16, 19626.50it/s] 64%|██████▍   | 580451/900675 [00:29<00:15, 20035.88it/s] 65%|██████▍   | 582488/900675 [00:29<00:15, 20133.64it/s] 65%|██████▍   | 584781/900675 [00:29<00:15, 20949.66it/s] 65%|██████▌   | 586879/900675 [00:29<00:15, 20739.35it/s] 65%|██████▌   | 588955/900675 [00:29<00:15, 20102.65it/s] 66%|██████▌   | 591028/900675 [00:29<00:15, 20280.50it/s] 66%|██████▌   | 593061/900675 [00:29<00:15, 19909.55it/s] 66%|██████▌   | 595121/900675 [00:29<00:15, 20108.41it/s] 66%|██████▋   | 597135/900675 [00:29<00:15, 19987.31it/s] 67%|██████▋   | 599136/900675 [00:29<00:15, 19856.28it/s] 67%|██████▋   | 601134/900675 [00:30<00:15, 19887.46it/s] 67%|██████▋   | 603124/900675 [00:30<00:15, 19430.29it/s] 67%|██████▋   | 605182/900675 [00:30<00:14, 19758.13it/s] 67%|██████▋   | 607182/900675 [00:30<00:14, 19819.91it/s] 68%|██████▊   | 609166/900675 [00:30<00:15, 19338.46it/s] 68%|██████▊   | 611273/900675 [00:30<00:14, 19840.95it/s] 68%|██████▊   | 613278/900675 [00:30<00:14, 19901.78it/s] 68%|██████▊   | 615271/900675 [00:30<00:14, 19565.69it/s] 69%|██████▊   | 617427/900675 [00:30<00:14, 20146.26it/s] 69%|██████▉   | 619706/900675 [00:30<00:13, 20923.25it/s] 69%|██████▉   | 621802/900675 [00:31<00:13, 20927.42it/s] 69%|██████▉   | 623898/900675 [00:31<00:13, 20387.89it/s] 70%|██████▉   | 626016/900675 [00:31<00:13, 20617.66it/s] 70%|██████▉   | 628082/900675 [00:31<00:13, 20315.77it/s] 70%|██████▉   | 630117/900675 [00:31<00:13, 20146.60it/s] 70%|███████   | 632323/900675 [00:31<00:12, 20701.79it/s] 70%|███████   | 634506/900675 [00:31<00:12, 21032.66it/s] 71%|███████   | 636612/900675 [00:31<00:12, 20624.30it/s] 71%|███████   | 638723/900675 [00:31<00:12, 20762.09it/s] 71%|███████   | 640802/900675 [00:31<00:12, 20612.41it/s] 71%|███████▏  | 642914/900675 [00:32<00:12, 20761.78it/s] 72%|███████▏  | 644992/900675 [00:32<00:12, 19982.60it/s] 72%|███████▏  | 647064/900675 [00:32<00:12, 20192.39it/s] 72%|███████▏  | 649089/900675 [00:32<00:12, 19492.00it/s] 72%|███████▏  | 651046/900675 [00:32<00:12, 19226.18it/s] 73%|███████▎  | 653105/900675 [00:32<00:12, 19617.16it/s] 73%|███████▎  | 655141/900675 [00:32<00:12, 19833.28it/s] 73%|███████▎  | 657171/900675 [00:32<00:12, 19965.46it/s] 73%|███████▎  | 659258/900675 [00:32<00:11, 20230.07it/s] 73%|███████▎  | 661284/900675 [00:33<00:11, 20204.89it/s] 74%|███████▎  | 663307/900675 [00:33<00:11, 20144.08it/s] 74%|███████▍  | 665344/900675 [00:33<00:11, 20206.00it/s] 74%|███████▍  | 667382/900675 [00:33<00:11, 20250.90it/s] 74%|███████▍  | 669561/900675 [00:33<00:11, 20707.43it/s] 75%|███████▍  | 671633/900675 [00:33<00:11, 20674.70it/s] 75%|███████▍  | 673701/900675 [00:33<00:11, 20619.98it/s] 75%|███████▌  | 675792/900675 [00:33<00:10, 20699.07it/s] 75%|███████▌  | 677863/900675 [00:33<00:11, 20231.42it/s] 75%|███████▌  | 679889/900675 [00:33<00:11, 19887.82it/s] 76%|███████▌  | 681992/900675 [00:34<00:10, 20216.30it/s] 76%|███████▌  | 684091/900675 [00:34<00:10, 20441.34it/s] 76%|███████▌  | 686138/900675 [00:34<00:10, 20441.91it/s] 76%|███████▋  | 688184/900675 [00:34<00:10, 20058.92it/s] 77%|███████▋  | 690193/900675 [00:34<00:10, 19874.16it/s] 77%|███████▋  | 692183/900675 [00:34<00:10, 19370.76it/s] 77%|███████▋  | 694124/900675 [00:34<00:10, 19287.51it/s] 77%|███████▋  | 696055/900675 [00:34<00:10, 19208.27it/s] 78%|███████▊  | 698056/900675 [00:34<00:10, 19441.31it/s] 78%|███████▊  | 700002/900675 [00:34<00:10, 19412.71it/s] 78%|███████▊  | 702153/900675 [00:35<00:09, 20033.42it/s] 78%|███████▊  | 704197/900675 [00:35<00:09, 20153.58it/s] 78%|███████▊  | 706214/900675 [00:35<00:10, 19345.52it/s] 79%|███████▊  | 708218/900675 [00:35<00:09, 19546.33it/s] 79%|███████▉  | 710359/900675 [00:35<00:09, 20091.99it/s] 79%|███████▉  | 712413/900675 [00:35<00:09, 20219.87it/s] 79%|███████▉  | 714481/900675 [00:35<00:09, 20353.21it/s] 80%|███████▉  | 716552/900675 [00:35<00:08, 20458.93it/s] 80%|███████▉  | 718601/900675 [00:35<00:09, 20148.76it/s] 80%|████████  | 720619/900675 [00:35<00:09, 19544.75it/s] 80%|████████  | 722690/900675 [00:36<00:08, 19883.00it/s] 80%|████████  | 724683/900675 [00:36<00:08, 19777.96it/s] 81%|████████  | 726664/900675 [00:36<00:08, 19491.74it/s] 81%|████████  | 728667/900675 [00:36<00:08, 19641.55it/s] 81%|████████  | 730634/900675 [00:36<00:08, 19493.43it/s] 81%|████████▏ | 732650/900675 [00:36<00:08, 19689.35it/s] 82%|████████▏ | 734873/900675 [00:36<00:08, 20439.05it/s] 82%|████████▏ | 736919/900675 [00:36<00:08, 20275.74it/s] 82%|████████▏ | 738949/900675 [00:36<00:08, 20185.67it/s] 82%|████████▏ | 740969/900675 [00:37<00:07, 20059.15it/s] 82%|████████▏ | 742976/900675 [00:37<00:08, 19550.37it/s] 83%|████████▎ | 744946/900675 [00:37<00:07, 19591.41it/s] 83%|████████▎ | 746908/900675 [00:37<00:07, 19524.41it/s] 83%|████████▎ | 748862/900675 [00:37<00:07, 19340.82it/s] 83%|████████▎ | 750937/900675 [00:37<00:07, 19751.17it/s] 84%|████████▎ | 752914/900675 [00:37<00:07, 19731.25it/s] 84%|████████▍ | 754889/900675 [00:37<00:07, 19542.50it/s] 84%|████████▍ | 756845/900675 [00:37<00:07, 19276.41it/s] 84%|████████▍ | 758901/900675 [00:37<00:07, 19651.02it/s] 84%|████████▍ | 761015/900675 [00:38<00:06, 20080.01it/s] 85%|████████▍ | 763025/900675 [00:38<00:07, 19635.62it/s] 85%|████████▍ | 765000/900675 [00:38<00:06, 19668.28it/s] 85%|████████▌ | 767088/900675 [00:38<00:06, 20024.80it/s] 85%|████████▌ | 769093/900675 [00:38<00:06, 19582.46it/s] 86%|████████▌ | 771055/900675 [00:38<00:06, 19426.24it/s] 86%|████████▌ | 773122/900675 [00:38<00:06, 19786.39it/s] 86%|████████▌ | 775103/900675 [00:38<00:06, 19744.75it/s] 86%|████████▋ | 777080/900675 [00:38<00:06, 19540.52it/s] 87%|████████▋ | 779094/900675 [00:38<00:06, 19714.86it/s] 87%|████████▋ | 781067/900675 [00:39<00:06, 19323.87it/s] 87%|████████▋ | 783002/900675 [00:39<00:06, 19296.87it/s] 87%|████████▋ | 784934/900675 [00:39<00:06, 19228.30it/s] 87%|████████▋ | 787005/900675 [00:39<00:05, 19665.17it/s] 88%|████████▊ | 788980/900675 [00:39<00:05, 19678.20it/s] 88%|████████▊ | 791027/900675 [00:39<00:05, 19910.50it/s] 88%|████████▊ | 793019/900675 [00:39<00:05, 19815.18it/s] 88%|████████▊ | 795002/900675 [00:39<00:05, 19653.39it/s] 88%|████████▊ | 797054/900675 [00:39<00:05, 19910.18it/s] 89%|████████▊ | 799046/900675 [00:39<00:05, 19756.67it/s] 89%|████████▉ | 801023/900675 [00:40<00:05, 19601.18it/s] 89%|████████▉ | 802984/900675 [00:40<00:05, 19416.03it/s] 89%|████████▉ | 805079/900675 [00:40<00:04, 19866.14it/s] 90%|████████▉ | 807067/900675 [00:40<00:04, 19812.65it/s] 90%|████████▉ | 809050/900675 [00:40<00:04, 19522.29it/s] 90%|█████████ | 811004/900675 [00:40<00:04, 18954.51it/s] 90%|█████████ | 812989/900675 [00:40<00:04, 19207.52it/s] 91%|█████████ | 815214/900675 [00:40<00:04, 20098.03it/s] 91%|█████████ | 817468/900675 [00:40<00:03, 20817.94it/s] 91%|█████████ | 819555/900675 [00:40<00:03, 20538.22it/s] 91%|█████████ | 821676/900675 [00:41<00:03, 20732.51it/s] 91%|█████████▏| 823753/900675 [00:41<00:03, 20322.53it/s] 92%|█████████▏| 825789/900675 [00:41<00:03, 19462.39it/s] 92%|█████████▏| 827842/900675 [00:41<00:03, 19766.83it/s] 92%|█████████▏| 829827/900675 [00:41<00:03, 19151.72it/s] 92%|█████████▏| 831750/900675 [00:41<00:03, 19138.34it/s] 93%|█████████▎| 833791/900675 [00:41<00:03, 19502.69it/s] 93%|█████████▎| 835813/900675 [00:41<00:03, 19704.70it/s] 93%|█████████▎| 837873/900675 [00:41<00:03, 19968.39it/s] 93%|█████████▎| 839874/900675 [00:42<00:03, 19623.83it/s] 93%|█████████▎| 841973/900675 [00:42<00:02, 20022.89it/s] 94%|█████████▎| 843979/900675 [00:42<00:02, 19097.36it/s] 94%|█████████▍| 845899/900675 [00:42<00:02, 18950.73it/s] 94%|█████████▍| 847884/900675 [00:42<00:02, 19210.04it/s] 94%|█████████▍| 849832/900675 [00:42<00:02, 19287.77it/s] 95%|█████████▍| 851766/900675 [00:42<00:02, 19181.80it/s] 95%|█████████▍| 853688/900675 [00:42<00:02, 19048.89it/s] 95%|█████████▌| 855655/900675 [00:42<00:02, 19224.81it/s] 95%|█████████▌| 857704/900675 [00:42<00:02, 19592.47it/s] 95%|█████████▌| 859665/900675 [00:43<00:02, 19480.41it/s] 96%|█████████▌| 861796/900675 [00:43<00:01, 20016.27it/s] 96%|█████████▌| 863852/900675 [00:43<00:01, 20170.48it/s] 96%|█████████▌| 865871/900675 [00:43<00:01, 19735.27it/s] 96%|█████████▋| 867882/900675 [00:43<00:01, 19844.66it/s] 97%|█████████▋| 869874/900675 [00:43<00:01, 19865.53it/s] 97%|█████████▋| 871940/900675 [00:43<00:01, 20100.64it/s] 97%|█████████▋| 873952/900675 [00:43<00:01, 20096.83it/s] 97%|█████████▋| 875999/900675 [00:43<00:01, 20207.15it/s] 97%|█████████▋| 878021/900675 [00:43<00:01, 20004.73it/s] 98%|█████████▊| 880023/900675 [00:44<00:01, 19786.82it/s] 98%|█████████▊| 882003/900675 [00:44<00:00, 19218.77it/s] 98%|█████████▊| 884010/900675 [00:44<00:00, 19466.13it/s] 98%|█████████▊| 886046/900675 [00:44<00:00, 19720.13it/s] 99%|█████████▊| 888039/900675 [00:44<00:00, 19774.41it/s] 99%|█████████▉| 890023/900675 [00:44<00:00, 19789.02it/s] 99%|█████████▉| 892004/900675 [00:44<00:00, 19766.95it/s] 99%|█████████▉| 893982/900675 [00:44<00:00, 19291.11it/s] 99%|█████████▉| 895915/900675 [00:44<00:00, 19161.35it/s]100%|█████████▉| 898030/900675 [00:45<00:00, 19741.38it/s]100%|█████████▉| 900095/900675 [00:45<00:00, 20005.43it/s]100%|██████████| 900675/900675 [00:45<00:00, 19957.70it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 14.81it/s]2022-02-25 09:02:20 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-25 09:02:20 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-25 09:02:20 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-25 09:02:20 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-02-25 09:02:20 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-25 09:02:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-25 09:02:20 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-25 09:02:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-25 09:02:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 09:02:20 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-25 09:02:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 09:02:20 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-25 09:02:20 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-02-25 09:02:20 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_last.pt
2022-02-25 09:02:20 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_last.pt
2022-02-25 09:02:20 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-25 09:02:20 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-25 09:02:20 | INFO | fairseq.trainer | begin training epoch 1
2022-02-25 09:02:20 | INFO | fairseq_cli.train | Start iterating over samples

2022-02-25 09:02:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-25 09:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 09:03:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 09:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 09:04:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-25 09:14:10 | INFO | train_inner | epoch 001:    105 / 788 loss=17.652, ppl=205962, wps=10126.7, ups=0.15, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.579, loss_scale=4, train_wall=704, gb_free=4, wall=709
2022-02-25 09:24:50 | INFO | train_inner | epoch 001:    205 / 788 loss=15.178, ppl=37069.9, wps=10231.8, ups=0.16, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.673, loss_scale=4, train_wall=636, gb_free=4, wall=1350
2022-02-25 09:35:30 | INFO | train_inner | epoch 001:    305 / 788 loss=12.935, ppl=7829.17, wps=10232.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.192, loss_scale=4, train_wall=635, gb_free=4, wall=1990
2022-02-25 09:46:11 | INFO | train_inner | epoch 001:    405 / 788 loss=11.271, ppl=2471.82, wps=10235.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.651, loss_scale=4, train_wall=635, gb_free=4, wall=2631
2022-02-25 09:56:51 | INFO | train_inner | epoch 001:    505 / 788 loss=10.633, ppl=1588.24, wps=10236.9, ups=0.16, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.493, loss_scale=4, train_wall=635, gb_free=4, wall=3271
2022-02-25 10:07:31 | INFO | train_inner | epoch 001:    605 / 788 loss=10.3, ppl=1261.03, wps=10233.6, ups=0.16, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.556, loss_scale=8, train_wall=636, gb_free=4, wall=3911
2022-02-25 10:18:12 | INFO | train_inner | epoch 001:    705 / 788 loss=10.027, ppl=1043.6, wps=10233.4, ups=0.16, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.639, loss_scale=8, train_wall=636, gb_free=4, wall=4552
2022-02-25 10:27:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 10:27:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.626 | ppl 790.1 | wps 23717.2 | wpb 2034.1 | bsz 4 | num_updates 783
2022-02-25 10:27:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 783 updates
2022-02-25 10:27:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 10:27:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 10:27:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 1 @ 783 updates, score 9.626) (writing took 7.8491694470867515 seconds)
2022-02-25 10:27:18 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-25 10:27:18 | INFO | train | epoch 001 | loss 12.28 | ppl 4972.93 | wps 10185.7 | ups 0.16 | wpb 65497.3 | bsz 127.9 | num_updates 783 | lr 9.79554e-05 | gnorm 1.195 | loss_scale 8 | train_wall 5041 | gb_free 4 | wall 5097
2022-02-25 10:27:18 | INFO | fairseq.trainer | begin training epoch 2
2022-02-25 10:27:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 10:29:06 | INFO | train_inner | epoch 002:     17 / 788 loss=9.788, ppl=883.96, wps=9966.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.7, loss_scale=8, train_wall=633, gb_free=4, wall=5206
2022-02-25 10:39:47 | INFO | train_inner | epoch 002:    117 / 788 loss=9.565, ppl=757.28, wps=10234.1, ups=0.16, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.777, loss_scale=8, train_wall=635, gb_free=4, wall=5847
2022-02-25 10:50:27 | INFO | train_inner | epoch 002:    217 / 788 loss=9.379, ppl=665.7, wps=10234.8, ups=0.16, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.808, loss_scale=8, train_wall=635, gb_free=4, wall=6487
2022-02-25 11:01:08 | INFO | train_inner | epoch 002:    317 / 788 loss=9.201, ppl=588.38, wps=10232.5, ups=0.16, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.91, loss_scale=16, train_wall=636, gb_free=4, wall=7127
2022-02-25 11:11:48 | INFO | train_inner | epoch 002:    417 / 788 loss=9.045, ppl=528.28, wps=10232.7, ups=0.16, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.894, loss_scale=16, train_wall=636, gb_free=4, wall=7768
2022-02-25 11:22:29 | INFO | train_inner | epoch 002:    517 / 788 loss=8.907, ppl=479.92, wps=10231.1, ups=0.16, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.902, loss_scale=16, train_wall=636, gb_free=4, wall=8408
2022-02-25 11:33:09 | INFO | train_inner | epoch 002:    617 / 788 loss=8.781, ppl=439.98, wps=10229.6, ups=0.16, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.886, loss_scale=16, train_wall=636, gb_free=4, wall=9049
2022-02-25 11:43:50 | INFO | train_inner | epoch 002:    717 / 788 loss=8.654, ppl=402.85, wps=10229, ups=0.16, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.911, loss_scale=16, train_wall=636, gb_free=4, wall=9690
2022-02-25 11:51:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 11:51:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.362 | ppl 328.91 | wps 23670.6 | wpb 2034.1 | bsz 4 | num_updates 1571 | best_loss 8.362
2022-02-25 11:51:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1571 updates
2022-02-25 11:51:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 11:51:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 11:51:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 2 @ 1571 updates, score 8.362) (writing took 7.23534551076591 seconds)
2022-02-25 11:51:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-25 11:51:38 | INFO | train | epoch 002 | loss 9.041 | ppl 526.86 | wps 10198.3 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 1571 | lr 0.000196436 | gnorm 0.87 | loss_scale 32 | train_wall 5006 | gb_free 4 | wall 10158
2022-02-25 11:51:38 | INFO | fairseq.trainer | begin training epoch 3
2022-02-25 11:51:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 11:54:44 | INFO | train_inner | epoch 003:     29 / 788 loss=8.518, ppl=366.66, wps=9970.6, ups=0.15, wpb=65232.6, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.906, loss_scale=32, train_wall=633, gb_free=4, wall=10344
2022-02-25 12:05:25 | INFO | train_inner | epoch 003:    129 / 788 loss=8.381, ppl=333.29, wps=10229.8, ups=0.16, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.899, loss_scale=32, train_wall=636, gb_free=4, wall=10985
2022-02-25 12:16:05 | INFO | train_inner | epoch 003:    229 / 788 loss=8.283, ppl=311.46, wps=10230, ups=0.16, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.885, loss_scale=32, train_wall=636, gb_free=4, wall=11625
2022-02-25 12:26:46 | INFO | train_inner | epoch 003:    329 / 788 loss=8.19, ppl=292.13, wps=10225.8, ups=0.16, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.904, loss_scale=32, train_wall=636, gb_free=4, wall=12266
2022-02-25 12:37:27 | INFO | train_inner | epoch 003:    429 / 788 loss=8.098, ppl=273.94, wps=10225.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.863, loss_scale=32, train_wall=636, gb_free=4, wall=12907
2022-02-25 12:44:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 12:48:14 | INFO | train_inner | epoch 003:    530 / 788 loss=8.014, ppl=258.42, wps=10129.1, ups=0.15, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.846, loss_scale=32, train_wall=642, gb_free=4, wall=13554
2022-02-25 12:58:55 | INFO | train_inner | epoch 003:    630 / 788 loss=7.932, ppl=244.27, wps=10227.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.823, loss_scale=32, train_wall=636, gb_free=4, wall=14195
2022-02-25 13:03:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 13:09:42 | INFO | train_inner | epoch 003:    731 / 788 loss=7.853, ppl=231.19, wps=10130.5, ups=0.15, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.835, loss_scale=16, train_wall=642, gb_free=4, wall=14842
2022-02-25 13:15:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 13:15:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.631 | ppl 198.16 | wps 23668 | wpb 2034.1 | bsz 4 | num_updates 2357 | best_loss 7.631
2022-02-25 13:15:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2357 updates
2022-02-25 13:15:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 13:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 13:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 3 @ 2357 updates, score 7.631) (writing took 7.29933157376945 seconds)
2022-02-25 13:16:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-25 13:16:01 | INFO | train | epoch 003 | loss 8.096 | ppl 273.67 | wps 10169.5 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 2357 | lr 0.000294666 | gnorm 0.86 | loss_scale 16 | train_wall 5007 | gb_free 4 | wall 15220
2022-02-25 13:16:01 | INFO | fairseq.trainer | begin training epoch 4
2022-02-25 13:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 13:20:36 | INFO | train_inner | epoch 004:     43 / 788 loss=7.735, ppl=213.07, wps=9972.3, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.81, loss_scale=16, train_wall=633, gb_free=4, wall=15496
2022-02-25 13:31:16 | INFO | train_inner | epoch 004:    143 / 788 loss=7.636, ppl=198.85, wps=10232.5, ups=0.16, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.779, loss_scale=16, train_wall=636, gb_free=4, wall=16136
2022-02-25 13:41:57 | INFO | train_inner | epoch 004:    243 / 788 loss=7.586, ppl=192.16, wps=10226.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.796, loss_scale=16, train_wall=636, gb_free=4, wall=16777
2022-02-25 13:52:39 | INFO | train_inner | epoch 004:    343 / 788 loss=7.533, ppl=185.22, wps=10215.8, ups=0.16, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.773, loss_scale=16, train_wall=636, gb_free=4, wall=17419
2022-02-25 14:03:20 | INFO | train_inner | epoch 004:    443 / 788 loss=7.471, ppl=177.45, wps=10216.1, ups=0.16, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.756, loss_scale=32, train_wall=637, gb_free=4, wall=18060
2022-02-25 14:11:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 14:14:08 | INFO | train_inner | epoch 004:    544 / 788 loss=7.423, ppl=171.61, wps=10114.7, ups=0.15, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.767, loss_scale=16, train_wall=643, gb_free=4, wall=18708
2022-02-25 14:24:50 | INFO | train_inner | epoch 004:    644 / 788 loss=7.365, ppl=164.89, wps=10218.9, ups=0.16, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.732, loss_scale=16, train_wall=636, gb_free=4, wall=19349
2022-02-25 14:35:31 | INFO | train_inner | epoch 004:    744 / 788 loss=7.309, ppl=158.52, wps=10219.7, ups=0.16, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.721, loss_scale=16, train_wall=636, gb_free=4, wall=19991
2022-02-25 14:40:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 14:40:20 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.163 | ppl 143.3 | wps 23636.4 | wpb 2034.1 | bsz 4 | num_updates 3144 | best_loss 7.163
2022-02-25 14:40:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3144 updates
2022-02-25 14:40:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 14:40:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 14:40:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 4 @ 3144 updates, score 7.163) (writing took 6.470452292822301 seconds)
2022-02-25 14:40:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-25 14:40:26 | INFO | train | epoch 004 | loss 7.474 | ppl 177.84 | wps 10176 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 3144 | lr 0.000393021 | gnorm 0.766 | loss_scale 16 | train_wall 5011 | gb_free 4 | wall 20286
2022-02-25 14:40:26 | INFO | fairseq.trainer | begin training epoch 5
2022-02-25 14:40:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 14:46:25 | INFO | train_inner | epoch 005:     56 / 788 loss=7.203, ppl=147.35, wps=9973.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.732, loss_scale=16, train_wall=633, gb_free=4, wall=20645
2022-02-25 14:57:06 | INFO | train_inner | epoch 005:    156 / 788 loss=7.135, ppl=140.53, wps=10225.5, ups=0.16, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.707, loss_scale=16, train_wall=636, gb_free=4, wall=21286
2022-02-25 15:07:47 | INFO | train_inner | epoch 005:    256 / 788 loss=7.107, ppl=137.84, wps=10225.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.716, loss_scale=32, train_wall=636, gb_free=4, wall=21927
2022-02-25 15:13:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 15:18:34 | INFO | train_inner | epoch 005:    357 / 788 loss=7.077, ppl=135.02, wps=10124.3, ups=0.15, wpb=65520.6, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.703, loss_scale=16, train_wall=642, gb_free=4, wall=22574
2022-02-25 15:29:15 | INFO | train_inner | epoch 005:    457 / 788 loss=7.047, ppl=132.25, wps=10225.1, ups=0.16, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.681, loss_scale=16, train_wall=636, gb_free=4, wall=23215
2022-02-25 15:39:56 | INFO | train_inner | epoch 005:    557 / 788 loss=7.017, ppl=129.49, wps=10224.7, ups=0.16, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.703, loss_scale=16, train_wall=636, gb_free=4, wall=23856
2022-02-25 15:50:37 | INFO | train_inner | epoch 005:    657 / 788 loss=6.987, ppl=126.89, wps=10225.4, ups=0.16, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.662, loss_scale=16, train_wall=636, gb_free=4, wall=24497
2022-02-25 16:01:18 | INFO | train_inner | epoch 005:    757 / 788 loss=6.951, ppl=123.72, wps=10223.8, ups=0.16, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.666, loss_scale=16, train_wall=636, gb_free=4, wall=25138
2022-02-25 16:04:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 16:04:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.864 | ppl 116.5 | wps 23659.5 | wpb 2034.1 | bsz 4 | num_updates 3931 | best_loss 6.864
2022-02-25 16:04:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3931 updates
2022-02-25 16:04:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 16:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 16:04:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 5 @ 3931 updates, score 6.864) (writing took 6.402807094156742 seconds)
2022-02-25 16:04:49 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-25 16:04:49 | INFO | train | epoch 005 | loss 7.049 | ppl 132.39 | wps 10180.5 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 3931 | lr 0.000491377 | gnorm 0.69 | loss_scale 16 | train_wall 5009 | gb_free 4 | wall 25349
2022-02-25 16:04:49 | INFO | fairseq.trainer | begin training epoch 6
2022-02-25 16:04:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 16:12:12 | INFO | train_inner | epoch 006:     69 / 788 loss=6.829, ppl=113.73, wps=9979, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.648, loss_scale=32, train_wall=633, gb_free=4, wall=25792
2022-02-25 16:14:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 16:22:59 | INFO | train_inner | epoch 006:    170 / 788 loss=6.78, ppl=109.86, wps=10125.4, ups=0.15, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.641, loss_scale=16, train_wall=642, gb_free=4, wall=26439
2022-02-25 16:33:40 | INFO | train_inner | epoch 006:    270 / 788 loss=6.774, ppl=109.4, wps=10227.2, ups=0.16, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.621, loss_scale=16, train_wall=636, gb_free=4, wall=27080
2022-02-25 16:44:21 | INFO | train_inner | epoch 006:    370 / 788 loss=6.754, ppl=107.95, wps=10226.3, ups=0.16, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.604, loss_scale=16, train_wall=636, gb_free=4, wall=27720
2022-02-25 16:55:01 | INFO | train_inner | epoch 006:    470 / 788 loss=6.729, ppl=106.07, wps=10227, ups=0.16, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.603, loss_scale=16, train_wall=636, gb_free=4, wall=28361
2022-02-25 17:05:42 | INFO | train_inner | epoch 006:    570 / 788 loss=6.725, ppl=105.79, wps=10226.6, ups=0.16, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.595, loss_scale=16, train_wall=636, gb_free=4, wall=29002
2022-02-25 17:15:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 17:16:30 | INFO | train_inner | epoch 006:    671 / 788 loss=6.704, ppl=104.24, wps=10123.5, ups=0.15, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.59, loss_scale=16, train_wall=642, gb_free=4, wall=29649
2022-02-25 17:27:10 | INFO | train_inner | epoch 006:    771 / 788 loss=6.683, ppl=102.73, wps=10225.8, ups=0.16, wpb=65519.3, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.558, loss_scale=16, train_wall=636, gb_free=4, wall=30290
2022-02-25 17:28:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 17:29:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.654 | ppl 100.7 | wps 23673.2 | wpb 2034.1 | bsz 4 | num_updates 4717 | best_loss 6.654
2022-02-25 17:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4717 updates
2022-02-25 17:29:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 17:29:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 17:29:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 6 @ 4717 updates, score 6.654) (writing took 7.392026375979185 seconds)
2022-02-25 17:29:13 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-25 17:29:13 | INFO | train | epoch 006 | loss 6.738 | ppl 106.78 | wps 10166.6 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 4717 | lr 0.000460434 | gnorm 0.606 | loss_scale 16 | train_wall 5008 | gb_free 4 | wall 30413
2022-02-25 17:29:13 | INFO | fairseq.trainer | begin training epoch 7
2022-02-25 17:29:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 17:38:05 | INFO | train_inner | epoch 007:     83 / 788 loss=6.548, ppl=93.57, wps=9963.8, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.558, loss_scale=16, train_wall=633, gb_free=4, wall=30945
2022-02-25 17:48:46 | INFO | train_inner | epoch 007:    183 / 788 loss=6.52, ppl=91.79, wps=10223.8, ups=0.16, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.573, loss_scale=16, train_wall=636, gb_free=4, wall=31586
2022-02-25 17:59:27 | INFO | train_inner | epoch 007:    283 / 788 loss=6.511, ppl=91.19, wps=10224.5, ups=0.16, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.567, loss_scale=16, train_wall=636, gb_free=4, wall=32227
2022-02-25 18:10:08 | INFO | train_inner | epoch 007:    383 / 788 loss=6.507, ppl=90.93, wps=10225.2, ups=0.16, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.553, loss_scale=16, train_wall=636, gb_free=4, wall=32868
2022-02-25 18:20:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 18:20:55 | INFO | train_inner | epoch 007:    484 / 788 loss=6.512, ppl=91.29, wps=10122.2, ups=0.15, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.542, loss_scale=16, train_wall=642, gb_free=4, wall=33515
2022-02-25 18:31:36 | INFO | train_inner | epoch 007:    584 / 788 loss=6.5, ppl=90.52, wps=10226.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.537, loss_scale=16, train_wall=636, gb_free=4, wall=34156
2022-02-25 18:42:17 | INFO | train_inner | epoch 007:    684 / 788 loss=6.483, ppl=89.47, wps=10225.9, ups=0.16, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.555, loss_scale=16, train_wall=636, gb_free=4, wall=34797
2022-02-25 18:52:58 | INFO | train_inner | epoch 007:    784 / 788 loss=6.489, ppl=89.8, wps=10226.7, ups=0.16, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.55, loss_scale=16, train_wall=636, gb_free=4, wall=35438
2022-02-25 18:53:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 18:53:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.503 | ppl 90.68 | wps 23602.7 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 6.503
2022-02-25 18:53:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5504 updates
2022-02-25 18:53:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 18:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 18:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 7 @ 5504 updates, score 6.503) (writing took 7.501393214799464 seconds)
2022-02-25 18:53:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-25 18:53:38 | INFO | train | epoch 007 | loss 6.505 | ppl 90.83 | wps 10178.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 5504 | lr 0.000426246 | gnorm 0.554 | loss_scale 16 | train_wall 5009 | gb_free 4 | wall 35477
2022-02-25 18:53:38 | INFO | fairseq.trainer | begin training epoch 8
2022-02-25 18:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 19:03:53 | INFO | train_inner | epoch 008:     96 / 788 loss=6.33, ppl=80.47, wps=9955.6, ups=0.15, wpb=65248, bsz=127.4, num_updates=5600, lr=0.000422577, gnorm=0.535, loss_scale=16, train_wall=633, gb_free=4, wall=36093
2022-02-25 19:14:34 | INFO | train_inner | epoch 008:    196 / 788 loss=6.331, ppl=80.52, wps=10226.7, ups=0.16, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.545, loss_scale=16, train_wall=636, gb_free=4, wall=36734
2022-02-25 19:15:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 19:25:21 | INFO | train_inner | epoch 008:    297 / 788 loss=6.347, ppl=81.39, wps=10126.7, ups=0.15, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.549, loss_scale=16, train_wall=642, gb_free=4, wall=37381
2022-02-25 19:36:02 | INFO | train_inner | epoch 008:    397 / 788 loss=6.341, ppl=81.05, wps=10225.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.526, loss_scale=16, train_wall=636, gb_free=4, wall=38022
2022-02-25 19:46:43 | INFO | train_inner | epoch 008:    497 / 788 loss=6.348, ppl=81.43, wps=10225.8, ups=0.16, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.528, loss_scale=16, train_wall=636, gb_free=4, wall=38663
2022-02-25 19:57:24 | INFO | train_inner | epoch 008:    597 / 788 loss=6.35, ppl=81.59, wps=10227.9, ups=0.16, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.523, loss_scale=16, train_wall=636, gb_free=4, wall=39304
2022-02-25 20:08:05 | INFO | train_inner | epoch 008:    697 / 788 loss=6.346, ppl=81.33, wps=10226.7, ups=0.16, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.536, loss_scale=16, train_wall=636, gb_free=4, wall=39944
2022-02-25 20:10:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 20:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 20:17:54 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.412 | ppl 85.14 | wps 23673.8 | wpb 2034.1 | bsz 4 | num_updates 6290 | best_loss 6.412
2022-02-25 20:17:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6290 updates
2022-02-25 20:17:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 20:18:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 20:18:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 8 @ 6290 updates, score 6.412) (writing took 7.300530152395368 seconds)
2022-02-25 20:18:02 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-25 20:18:02 | INFO | train | epoch 008 | loss 6.34 | ppl 81.02 | wps 10166.1 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 6290 | lr 0.000398726 | gnorm 0.533 | loss_scale 16 | train_wall 5008 | gb_free 4 | wall 40541
2022-02-25 20:18:02 | INFO | fairseq.trainer | begin training epoch 9
2022-02-25 20:18:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 20:19:06 | INFO | train_inner | epoch 009:     10 / 788 loss=6.315, ppl=79.64, wps=9869.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.523, loss_scale=16, train_wall=640, gb_free=4, wall=40606
2022-02-25 20:29:47 | INFO | train_inner | epoch 009:    110 / 788 loss=6.187, ppl=72.84, wps=10225.9, ups=0.16, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.543, loss_scale=16, train_wall=636, gb_free=4, wall=41246
2022-02-25 20:40:28 | INFO | train_inner | epoch 009:    210 / 788 loss=6.193, ppl=73.16, wps=10226, ups=0.16, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.52, loss_scale=16, train_wall=636, gb_free=4, wall=41887
2022-02-25 20:51:08 | INFO | train_inner | epoch 009:    310 / 788 loss=6.22, ppl=74.54, wps=10224.5, ups=0.16, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.523, loss_scale=16, train_wall=636, gb_free=4, wall=42528
2022-02-25 21:01:49 | INFO | train_inner | epoch 009:    410 / 788 loss=6.219, ppl=74.51, wps=10224.9, ups=0.16, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.536, loss_scale=16, train_wall=636, gb_free=4, wall=43169
2022-02-25 21:07:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 21:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 21:12:43 | INFO | train_inner | epoch 009:    512 / 788 loss=6.233, ppl=75.23, wps=10028.1, ups=0.15, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.518, loss_scale=8, train_wall=648, gb_free=4, wall=43823
2022-02-25 21:23:23 | INFO | train_inner | epoch 009:    612 / 788 loss=6.223, ppl=74.7, wps=10229.8, ups=0.16, wpb=65519.3, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.545, loss_scale=8, train_wall=636, gb_free=4, wall=44463
2022-02-25 21:34:04 | INFO | train_inner | epoch 009:    712 / 788 loss=6.232, ppl=75.15, wps=10229.2, ups=0.16, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.522, loss_scale=8, train_wall=636, gb_free=4, wall=45104
2022-02-25 21:42:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 21:42:17 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.356 | ppl 81.92 | wps 23628.6 | wpb 2034.1 | bsz 4 | num_updates 7076 | best_loss 6.356
2022-02-25 21:42:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7076 updates
2022-02-25 21:42:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 21:42:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 21:42:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 9 @ 7076 updates, score 6.356) (writing took 7.072260525077581 seconds)
2022-02-25 21:42:25 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-25 21:42:25 | INFO | train | epoch 009 | loss 6.216 | ppl 74.35 | wps 10168.2 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 7076 | lr 0.000375929 | gnorm 0.529 | loss_scale 8 | train_wall 5008 | gb_free 4 | wall 45604
2022-02-25 21:42:25 | INFO | fairseq.trainer | begin training epoch 10
2022-02-25 21:42:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 21:44:58 | INFO | train_inner | epoch 010:     24 / 788 loss=6.196, ppl=73.32, wps=9973, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.531, loss_scale=8, train_wall=633, gb_free=4, wall=45758
2022-02-25 21:55:39 | INFO | train_inner | epoch 010:    124 / 788 loss=6.076, ppl=67.48, wps=10229.9, ups=0.16, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.538, loss_scale=8, train_wall=636, gb_free=4, wall=46399
2022-02-25 22:06:20 | INFO | train_inner | epoch 010:    224 / 788 loss=6.082, ppl=67.73, wps=10227.4, ups=0.16, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.513, loss_scale=16, train_wall=636, gb_free=4, wall=47040
2022-02-25 22:11:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 22:17:07 | INFO | train_inner | epoch 010:    325 / 788 loss=6.118, ppl=69.44, wps=10124.9, ups=0.15, wpb=65534.7, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.538, loss_scale=8, train_wall=642, gb_free=4, wall=47687
2022-02-25 22:27:48 | INFO | train_inner | epoch 010:    425 / 788 loss=6.12, ppl=69.53, wps=10230.1, ups=0.16, wpb=65520.6, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.513, loss_scale=8, train_wall=636, gb_free=4, wall=48327
2022-02-25 22:38:28 | INFO | train_inner | epoch 010:    525 / 788 loss=6.132, ppl=70.15, wps=10229.1, ups=0.16, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.506, loss_scale=8, train_wall=636, gb_free=4, wall=48968
2022-02-25 22:49:09 | INFO | train_inner | epoch 010:    625 / 788 loss=6.135, ppl=70.28, wps=10230.7, ups=0.16, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.531, loss_scale=8, train_wall=636, gb_free=4, wall=49609
2022-02-25 22:59:50 | INFO | train_inner | epoch 010:    725 / 788 loss=6.154, ppl=71.22, wps=10228.5, ups=0.16, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.553, loss_scale=8, train_wall=636, gb_free=4, wall=50249
2022-02-25 23:06:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 23:06:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 23:06:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.307 | ppl 79.19 | wps 23697.4 | wpb 2034.1 | bsz 4 | num_updates 7862 | best_loss 6.307
2022-02-25 23:06:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7862 updates
2022-02-25 23:06:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 23:06:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-25 23:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 10 @ 7862 updates, score 6.307) (writing took 7.0142027623951435 seconds)
2022-02-25 23:06:47 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-25 23:06:47 | INFO | train | epoch 010 | loss 6.119 | ppl 69.48 | wps 10170 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 7862 | lr 0.000356643 | gnorm 0.528 | loss_scale 8 | train_wall 5007 | gb_free 4 | wall 50666
2022-02-25 23:06:47 | INFO | fairseq.trainer | begin training epoch 11
2022-02-25 23:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 23:10:50 | INFO | train_inner | epoch 011:     38 / 788 loss=6.085, ppl=67.87, wps=9878.5, ups=0.15, wpb=65248, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.513, loss_scale=8, train_wall=639, gb_free=4, wall=50910
2022-02-25 23:21:31 | INFO | train_inner | epoch 011:    138 / 788 loss=5.996, ppl=63.83, wps=10227, ups=0.16, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.534, loss_scale=8, train_wall=636, gb_free=4, wall=51551
2022-02-25 23:32:11 | INFO | train_inner | epoch 011:    238 / 788 loss=6.023, ppl=65.03, wps=10228.2, ups=0.16, wpb=65520.6, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.542, loss_scale=8, train_wall=636, gb_free=4, wall=52191
2022-02-25 23:42:52 | INFO | train_inner | epoch 011:    338 / 788 loss=6.033, ppl=65.5, wps=10231.4, ups=0.16, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.51, loss_scale=8, train_wall=636, gb_free=4, wall=52832
2022-02-25 23:53:33 | INFO | train_inner | epoch 011:    438 / 788 loss=6.038, ppl=65.7, wps=10231.2, ups=0.16, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.532, loss_scale=8, train_wall=636, gb_free=4, wall=53472
2022-02-26 00:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 00:04:19 | INFO | train_inner | epoch 011:    539 / 788 loss=6.054, ppl=66.44, wps=10130.1, ups=0.15, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.539, loss_scale=8, train_wall=642, gb_free=4, wall=54119
2022-02-26 00:15:00 | INFO | train_inner | epoch 011:    639 / 788 loss=6.065, ppl=66.93, wps=10229.9, ups=0.16, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.517, loss_scale=8, train_wall=636, gb_free=4, wall=54760
2022-02-26 00:25:41 | INFO | train_inner | epoch 011:    739 / 788 loss=6.066, ppl=67.01, wps=10228.5, ups=0.16, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.524, loss_scale=8, train_wall=636, gb_free=4, wall=55401
2022-02-26 00:30:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 00:31:01 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.278 | ppl 77.63 | wps 23628.9 | wpb 2034.1 | bsz 4 | num_updates 8649 | best_loss 6.278
2022-02-26 00:31:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8649 updates
2022-02-26 00:31:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 00:31:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 00:31:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 11 @ 8649 updates, score 6.278) (writing took 7.069678334519267 seconds)
2022-02-26 00:31:08 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-26 00:31:08 | INFO | train | epoch 011 | loss 6.039 | ppl 65.75 | wps 10184 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 8649 | lr 0.00034003 | gnorm 0.526 | loss_scale 8 | train_wall 5006 | gb_free 4 | wall 55728
2022-02-26 00:31:08 | INFO | fairseq.trainer | begin training epoch 12
2022-02-26 00:31:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 00:36:35 | INFO | train_inner | epoch 012:     51 / 788 loss=5.995, ppl=63.77, wps=9975.4, ups=0.15, wpb=65248, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.54, loss_scale=8, train_wall=633, gb_free=4, wall=56055
2022-02-26 00:47:16 | INFO | train_inner | epoch 012:    151 / 788 loss=5.933, ppl=61.08, wps=10230.2, ups=0.16, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.526, loss_scale=8, train_wall=636, gb_free=4, wall=56695
2022-02-26 00:57:56 | INFO | train_inner | epoch 012:    251 / 788 loss=5.947, ppl=61.71, wps=10230.3, ups=0.16, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.544, loss_scale=8, train_wall=636, gb_free=4, wall=57336
2022-02-26 01:00:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 01:08:43 | INFO | train_inner | epoch 012:    352 / 788 loss=5.959, ppl=62.22, wps=10128, ups=0.15, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.527, loss_scale=8, train_wall=642, gb_free=4, wall=57983
2022-02-26 01:19:24 | INFO | train_inner | epoch 012:    452 / 788 loss=5.974, ppl=62.86, wps=10228.1, ups=0.16, wpb=65520.6, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.528, loss_scale=8, train_wall=636, gb_free=4, wall=58624
2022-02-26 01:30:05 | INFO | train_inner | epoch 012:    552 / 788 loss=5.993, ppl=63.67, wps=10226.8, ups=0.16, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.519, loss_scale=8, train_wall=636, gb_free=4, wall=59264
2022-02-26 01:40:45 | INFO | train_inner | epoch 012:    652 / 788 loss=6.003, ppl=64.14, wps=10227.6, ups=0.16, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.526, loss_scale=8, train_wall=636, gb_free=4, wall=59905
2022-02-26 01:51:26 | INFO | train_inner | epoch 012:    752 / 788 loss=6.011, ppl=64.47, wps=10230.2, ups=0.16, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.519, loss_scale=8, train_wall=636, gb_free=4, wall=60546
2022-02-26 01:55:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 01:55:23 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.249 | ppl 76.03 | wps 23619.8 | wpb 2034.1 | bsz 4 | num_updates 9436 | best_loss 6.249
2022-02-26 01:55:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9436 updates
2022-02-26 01:55:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 01:55:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 01:55:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 12 @ 9436 updates, score 6.249) (writing took 6.688653134740889 seconds)
2022-02-26 01:55:30 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-26 01:55:30 | INFO | train | epoch 012 | loss 5.972 | ppl 62.77 | wps 10183.7 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 9436 | lr 0.000325541 | gnorm 0.53 | loss_scale 16 | train_wall 5007 | gb_free 4 | wall 60790
2022-02-26 01:55:30 | INFO | fairseq.trainer | begin training epoch 13
2022-02-26 01:55:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 01:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:02:26 | INFO | train_inner | epoch 013:     65 / 788 loss=5.908, ppl=60.04, wps=9884, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.522, loss_scale=8, train_wall=639, gb_free=4, wall=61206
2022-02-26 02:13:07 | INFO | train_inner | epoch 013:    165 / 788 loss=5.871, ppl=58.54, wps=10230.7, ups=0.16, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.529, loss_scale=8, train_wall=636, gb_free=4, wall=61847
2022-02-26 02:23:47 | INFO | train_inner | epoch 013:    265 / 788 loss=5.887, ppl=59.19, wps=10229.1, ups=0.16, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.548, loss_scale=8, train_wall=636, gb_free=4, wall=62487
2022-02-26 02:34:28 | INFO | train_inner | epoch 013:    365 / 788 loss=5.906, ppl=59.96, wps=10230, ups=0.16, wpb=65534.7, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.523, loss_scale=8, train_wall=636, gb_free=4, wall=63128
2022-02-26 02:45:08 | INFO | train_inner | epoch 013:    465 / 788 loss=5.937, ppl=61.26, wps=10233.1, ups=0.16, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.53, loss_scale=8, train_wall=636, gb_free=4, wall=63768
2022-02-26 02:51:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:55:55 | INFO | train_inner | epoch 013:    566 / 788 loss=5.944, ppl=61.57, wps=10132, ups=0.15, wpb=65520.6, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.542, loss_scale=8, train_wall=642, gb_free=4, wall=64415
2022-02-26 03:06:36 | INFO | train_inner | epoch 013:    666 / 788 loss=5.939, ppl=61.34, wps=10230.5, ups=0.16, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.536, loss_scale=8, train_wall=636, gb_free=4, wall=65056
2022-02-26 03:17:16 | INFO | train_inner | epoch 013:    766 / 788 loss=5.953, ppl=61.96, wps=10231.2, ups=0.16, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.547, loss_scale=8, train_wall=636, gb_free=4, wall=65696
2022-02-26 03:19:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 03:19:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.228 | ppl 74.96 | wps 23656 | wpb 2034.1 | bsz 4 | num_updates 10222 | best_loss 6.228
2022-02-26 03:19:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10222 updates
2022-02-26 03:19:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 03:19:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 03:19:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 13 @ 10222 updates, score 6.228) (writing took 6.975303716957569 seconds)
2022-02-26 03:19:51 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 03:19:51 | INFO | train | epoch 013 | loss 5.915 | ppl 60.34 | wps 10172.4 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 10222 | lr 0.000312775 | gnorm 0.535 | loss_scale 8 | train_wall 5006 | gb_free 4 | wall 65850
2022-02-26 03:19:51 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 03:19:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 03:28:10 | INFO | train_inner | epoch 014:     78 / 788 loss=5.834, ppl=57.04, wps=9976, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=10300, lr=0.000311588, gnorm=0.547, loss_scale=8, train_wall=633, gb_free=4, wall=66350
2022-02-26 03:38:51 | INFO | train_inner | epoch 014:    178 / 788 loss=5.826, ppl=56.72, wps=10230.4, ups=0.16, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.53, loss_scale=8, train_wall=636, gb_free=4, wall=66991
2022-02-26 03:47:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 03:49:38 | INFO | train_inner | epoch 014:    279 / 788 loss=5.847, ppl=57.57, wps=10127.4, ups=0.15, wpb=65534.7, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.55, loss_scale=8, train_wall=642, gb_free=4, wall=67638
2022-02-26 04:00:19 | INFO | train_inner | epoch 014:    379 / 788 loss=5.861, ppl=58.14, wps=10231.1, ups=0.16, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.531, loss_scale=8, train_wall=636, gb_free=4, wall=68278
2022-02-26 04:10:59 | INFO | train_inner | epoch 014:    479 / 788 loss=5.872, ppl=58.56, wps=10229.3, ups=0.16, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.537, loss_scale=8, train_wall=636, gb_free=4, wall=68919
2022-02-26 04:21:40 | INFO | train_inner | epoch 014:    579 / 788 loss=5.894, ppl=59.47, wps=10229.5, ups=0.16, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.525, loss_scale=8, train_wall=636, gb_free=4, wall=69560
2022-02-26 04:32:20 | INFO | train_inner | epoch 014:    679 / 788 loss=5.9, ppl=59.72, wps=10231.1, ups=0.16, wpb=65520.6, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.555, loss_scale=8, train_wall=635, gb_free=4, wall=70200
2022-02-26 04:43:01 | INFO | train_inner | epoch 014:    779 / 788 loss=5.905, ppl=59.93, wps=10231.6, ups=0.16, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.513, loss_scale=16, train_wall=636, gb_free=4, wall=70841
2022-02-26 04:43:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:43:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 04:44:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.207 | ppl 73.9 | wps 23603.6 | wpb 2034.1 | bsz 4 | num_updates 11008 | best_loss 6.207
2022-02-26 04:44:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 11008 updates
2022-02-26 04:44:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 04:44:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 04:44:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 14 @ 11008 updates, score 6.207) (writing took 6.582234461791813 seconds)
2022-02-26 04:44:12 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 04:44:12 | INFO | train | epoch 014 | loss 5.865 | ppl 58.3 | wps 10172.2 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 11008 | lr 0.000301402 | gnorm 0.536 | loss_scale 8 | train_wall 5006 | gb_free 4 | wall 70911
2022-02-26 04:44:12 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 04:44:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 04:54:01 | INFO | train_inner | epoch 015:     92 / 788 loss=5.769, ppl=54.52, wps=9883, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11100, lr=0.00030015, gnorm=0.551, loss_scale=8, train_wall=639, gb_free=4, wall=71501
2022-02-26 05:04:42 | INFO | train_inner | epoch 015:    192 / 788 loss=5.781, ppl=55, wps=10228.6, ups=0.16, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.516, loss_scale=8, train_wall=636, gb_free=4, wall=72142
2022-02-26 05:15:22 | INFO | train_inner | epoch 015:    292 / 788 loss=5.804, ppl=55.87, wps=10229.9, ups=0.16, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.546, loss_scale=8, train_wall=636, gb_free=4, wall=72782
2022-02-26 05:26:03 | INFO | train_inner | epoch 015:    392 / 788 loss=5.833, ppl=57.02, wps=10231.2, ups=0.16, wpb=65519.3, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.535, loss_scale=8, train_wall=635, gb_free=4, wall=73423
2022-02-26 05:36:43 | INFO | train_inner | epoch 015:    492 / 788 loss=5.843, ppl=57.4, wps=10230.5, ups=0.16, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.565, loss_scale=8, train_wall=636, gb_free=4, wall=74063
2022-02-26 05:41:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:47:31 | INFO | train_inner | epoch 015:    593 / 788 loss=5.842, ppl=57.35, wps=10128.3, ups=0.15, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.53, loss_scale=8, train_wall=642, gb_free=4, wall=74710
2022-02-26 05:58:11 | INFO | train_inner | epoch 015:    693 / 788 loss=5.85, ppl=57.67, wps=10229.5, ups=0.16, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.533, loss_scale=8, train_wall=636, gb_free=4, wall=75351
2022-02-26 06:03:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-26 06:08:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 06:08:26 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.187 | ppl 72.86 | wps 23669.2 | wpb 2034.1 | bsz 4 | num_updates 11794 | best_loss 6.187
2022-02-26 06:08:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11794 updates
2022-02-26 06:08:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 06:08:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 06:08:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 15 @ 11794 updates, score 6.187) (writing took 7.041582029312849 seconds)
2022-02-26 06:08:33 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 06:08:33 | INFO | train | epoch 015 | loss 5.822 | ppl 56.59 | wps 10171 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 11794 | lr 0.000291185 | gnorm 0.544 | loss_scale 4 | train_wall 5006 | gb_free 4 | wall 75973
2022-02-26 06:08:33 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 06:08:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 06:09:12 | INFO | train_inner | epoch 016:      6 / 788 loss=5.858, ppl=57.99, wps=9879.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.573, loss_scale=4, train_wall=639, gb_free=4, wall=76011
2022-02-26 06:19:52 | INFO | train_inner | epoch 016:    106 / 788 loss=5.718, ppl=52.64, wps=10231.8, ups=0.16, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.53, loss_scale=4, train_wall=636, gb_free=4, wall=76652
2022-02-26 06:30:33 | INFO | train_inner | epoch 016:    206 / 788 loss=5.741, ppl=53.47, wps=10229.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.545, loss_scale=4, train_wall=636, gb_free=4, wall=77292
2022-02-26 06:41:13 | INFO | train_inner | epoch 016:    306 / 788 loss=5.772, ppl=54.63, wps=10232.6, ups=0.16, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.547, loss_scale=4, train_wall=636, gb_free=4, wall=77933
2022-02-26 06:51:54 | INFO | train_inner | epoch 016:    406 / 788 loss=5.788, ppl=55.24, wps=10233.2, ups=0.16, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.533, loss_scale=4, train_wall=635, gb_free=4, wall=78573
2022-02-26 07:02:34 | INFO | train_inner | epoch 016:    506 / 788 loss=5.802, ppl=55.79, wps=10233.7, ups=0.16, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.559, loss_scale=8, train_wall=635, gb_free=4, wall=79214
2022-02-26 07:13:15 | INFO | train_inner | epoch 016:    606 / 788 loss=5.806, ppl=55.94, wps=10229.8, ups=0.16, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.55, loss_scale=8, train_wall=636, gb_free=4, wall=79854
2022-02-26 07:23:55 | INFO | train_inner | epoch 016:    706 / 788 loss=5.824, ppl=56.64, wps=10230.2, ups=0.16, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.554, loss_scale=8, train_wall=636, gb_free=4, wall=80495
2022-02-26 07:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 07:32:47 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.182 | ppl 72.63 | wps 23726.3 | wpb 2034.1 | bsz 4 | num_updates 12582 | best_loss 6.182
2022-02-26 07:32:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12582 updates
2022-02-26 07:32:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 07:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 07:32:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 16 @ 12582 updates, score 6.182) (writing took 6.659406109713018 seconds)
2022-02-26 07:32:54 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 07:32:54 | INFO | train | epoch 016 | loss 5.784 | ppl 55.1 | wps 10199.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 12582 | lr 0.00028192 | gnorm 0.543 | loss_scale 8 | train_wall 5005 | gb_free 4 | wall 81033
2022-02-26 07:32:54 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 07:32:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 07:34:49 | INFO | train_inner | epoch 017:     18 / 788 loss=5.806, ppl=55.93, wps=9979.3, ups=0.15, wpb=65248, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.531, loss_scale=8, train_wall=633, gb_free=4, wall=81149
2022-02-26 07:45:30 | INFO | train_inner | epoch 017:    118 / 788 loss=5.685, ppl=51.44, wps=10227.7, ups=0.16, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.544, loss_scale=8, train_wall=636, gb_free=4, wall=81790
2022-02-26 07:54:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:56:17 | INFO | train_inner | epoch 017:    219 / 788 loss=5.707, ppl=52.24, wps=10127.8, ups=0.15, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.545, loss_scale=8, train_wall=642, gb_free=4, wall=82437
2022-02-26 08:06:58 | INFO | train_inner | epoch 017:    319 / 788 loss=5.739, ppl=53.42, wps=10228.6, ups=0.16, wpb=65534.7, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.552, loss_scale=8, train_wall=636, gb_free=4, wall=83077
2022-02-26 08:17:38 | INFO | train_inner | epoch 017:    419 / 788 loss=5.752, ppl=53.9, wps=10229.6, ups=0.16, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.549, loss_scale=8, train_wall=636, gb_free=4, wall=83718
2022-02-26 08:28:19 | INFO | train_inner | epoch 017:    519 / 788 loss=5.774, ppl=54.72, wps=10230.4, ups=0.16, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.575, loss_scale=8, train_wall=636, gb_free=4, wall=84359
2022-02-26 08:38:59 | INFO | train_inner | epoch 017:    619 / 788 loss=5.786, ppl=55.16, wps=10230.5, ups=0.16, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.548, loss_scale=8, train_wall=636, gb_free=4, wall=84999
2022-02-26 08:49:40 | INFO | train_inner | epoch 017:    719 / 788 loss=5.789, ppl=55.29, wps=10231.5, ups=0.16, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.571, loss_scale=16, train_wall=636, gb_free=4, wall=85640
2022-02-26 08:50:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:56:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 08:57:08 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.175 | ppl 72.23 | wps 23570.9 | wpb 2034.1 | bsz 4 | num_updates 13368 | best_loss 6.175
2022-02-26 08:57:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13368 updates
2022-02-26 08:57:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 08:57:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 08:57:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 17 @ 13368 updates, score 6.175) (writing took 6.767089574597776 seconds)
2022-02-26 08:57:15 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 08:57:15 | INFO | train | epoch 017 | loss 5.749 | ppl 53.8 | wps 10171.2 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 13368 | lr 0.000273506 | gnorm 0.557 | loss_scale 8 | train_wall 5006 | gb_free 4 | wall 86095
2022-02-26 08:57:15 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 08:57:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 09:00:40 | INFO | train_inner | epoch 018:     32 / 788 loss=5.747, ppl=53.71, wps=9880.6, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.57, loss_scale=8, train_wall=639, gb_free=4, wall=86300
2022-02-26 09:11:21 | INFO | train_inner | epoch 018:    132 / 788 loss=5.659, ppl=50.54, wps=10232.1, ups=0.16, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.549, loss_scale=8, train_wall=636, gb_free=4, wall=86940
2022-02-26 09:22:01 | INFO | train_inner | epoch 018:    232 / 788 loss=5.683, ppl=51.36, wps=10233.2, ups=0.16, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.558, loss_scale=8, train_wall=635, gb_free=4, wall=87581
2022-02-26 09:32:42 | INFO | train_inner | epoch 018:    332 / 788 loss=5.696, ppl=51.84, wps=10230.9, ups=0.16, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.534, loss_scale=8, train_wall=636, gb_free=4, wall=88221
2022-02-26 09:43:22 | INFO | train_inner | epoch 018:    432 / 788 loss=5.725, ppl=52.88, wps=10229.7, ups=0.16, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.554, loss_scale=8, train_wall=636, gb_free=4, wall=88862
2022-02-26 09:45:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:54:09 | INFO | train_inner | epoch 018:    533 / 788 loss=5.742, ppl=53.52, wps=10133.9, ups=0.15, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.556, loss_scale=8, train_wall=642, gb_free=4, wall=89509
2022-02-26 10:04:49 | INFO | train_inner | epoch 018:    633 / 788 loss=5.753, ppl=53.93, wps=10233, ups=0.16, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.535, loss_scale=8, train_wall=636, gb_free=4, wall=90149
2022-02-26 10:14:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-26 10:15:36 | INFO | train_inner | epoch 018:    734 / 788 loss=5.764, ppl=54.34, wps=10132, ups=0.15, wpb=65520.6, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.574, loss_scale=4, train_wall=642, gb_free=4, wall=90796
2022-02-26 10:21:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 10:21:28 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.171 | ppl 72.05 | wps 23668.6 | wpb 2034.1 | bsz 4 | num_updates 14154 | best_loss 6.171
2022-02-26 10:21:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14154 updates
2022-02-26 10:21:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 10:21:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 10:21:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 18 @ 14154 updates, score 6.171) (writing took 6.792529349215329 seconds)
2022-02-26 10:21:35 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 10:21:35 | INFO | train | epoch 018 | loss 5.718 | ppl 52.65 | wps 10173.9 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 14154 | lr 0.000265803 | gnorm 0.551 | loss_scale 4 | train_wall 5005 | gb_free 4 | wall 91155
2022-02-26 10:21:35 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 10:21:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 10:26:30 | INFO | train_inner | epoch 019:     46 / 788 loss=5.705, ppl=52.18, wps=9981.9, ups=0.15, wpb=65248, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.546, loss_scale=4, train_wall=633, gb_free=4, wall=91450
2022-02-26 10:37:10 | INFO | train_inner | epoch 019:    146 / 788 loss=5.647, ppl=50.09, wps=10234.5, ups=0.16, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.553, loss_scale=4, train_wall=635, gb_free=4, wall=92090
2022-02-26 10:47:51 | INFO | train_inner | epoch 019:    246 / 788 loss=5.655, ppl=50.39, wps=10233.1, ups=0.16, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.563, loss_scale=4, train_wall=636, gb_free=4, wall=92730
2022-02-26 10:58:31 | INFO | train_inner | epoch 019:    346 / 788 loss=5.671, ppl=50.96, wps=10232.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.558, loss_scale=4, train_wall=635, gb_free=4, wall=93371
2022-02-26 11:09:11 | INFO | train_inner | epoch 019:    446 / 788 loss=5.696, ppl=51.82, wps=10237, ups=0.16, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.564, loss_scale=8, train_wall=635, gb_free=4, wall=94011
2022-02-26 11:19:52 | INFO | train_inner | epoch 019:    546 / 788 loss=5.706, ppl=52.2, wps=10229.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.556, loss_scale=8, train_wall=636, gb_free=4, wall=94651
2022-02-26 11:30:32 | INFO | train_inner | epoch 019:    646 / 788 loss=5.724, ppl=52.87, wps=10232, ups=0.16, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.56, loss_scale=8, train_wall=636, gb_free=4, wall=95292
2022-02-26 11:41:13 | INFO | train_inner | epoch 019:    746 / 788 loss=5.739, ppl=53.42, wps=10231.4, ups=0.16, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.566, loss_scale=8, train_wall=636, gb_free=4, wall=95933
2022-02-26 11:45:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 11:45:48 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.165 | ppl 71.77 | wps 23706 | wpb 2034.1 | bsz 4 | num_updates 14942 | best_loss 6.165
2022-02-26 11:45:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14942 updates
2022-02-26 11:45:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 11:45:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 11:45:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 19 @ 14942 updates, score 6.165) (writing took 6.502515112049878 seconds)
2022-02-26 11:45:55 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 11:45:55 | INFO | train | epoch 019 | loss 5.69 | ppl 51.64 | wps 10201.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 14942 | lr 0.0002587 | gnorm 0.561 | loss_scale 8 | train_wall 5005 | gb_free 4 | wall 96214
2022-02-26 11:45:55 | INFO | fairseq.trainer | begin training epoch 20
2022-02-26 11:45:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 11:52:06 | INFO | train_inner | epoch 020:     58 / 788 loss=5.657, ppl=50.44, wps=9986, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.562, loss_scale=8, train_wall=633, gb_free=4, wall=96586
2022-02-26 12:02:47 | INFO | train_inner | epoch 020:    158 / 788 loss=5.601, ppl=48.52, wps=10232.5, ups=0.16, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.553, loss_scale=8, train_wall=635, gb_free=4, wall=97226
2022-02-26 12:04:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 12:13:34 | INFO | train_inner | epoch 020:    259 / 788 loss=5.64, ppl=49.88, wps=10129.8, ups=0.15, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.561, loss_scale=8, train_wall=642, gb_free=4, wall=97873
2022-02-26 12:24:14 | INFO | train_inner | epoch 020:    359 / 788 loss=5.656, ppl=50.43, wps=10232, ups=0.16, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.556, loss_scale=8, train_wall=636, gb_free=4, wall=98514
2022-02-26 12:34:54 | INFO | train_inner | epoch 020:    459 / 788 loss=5.676, ppl=51.14, wps=10231.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.575, loss_scale=8, train_wall=635, gb_free=4, wall=99154
2022-02-26 12:45:35 | INFO | train_inner | epoch 020:    559 / 788 loss=5.694, ppl=51.77, wps=10232.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.579, loss_scale=8, train_wall=636, gb_free=4, wall=99795
2022-02-26 12:56:15 | INFO | train_inner | epoch 020:    659 / 788 loss=5.704, ppl=52.12, wps=10231.5, ups=0.16, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.54, loss_scale=8, train_wall=636, gb_free=4, wall=100435
2022-02-26 13:00:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 13:07:02 | INFO | train_inner | epoch 020:    760 / 788 loss=5.702, ppl=52.06, wps=10130.1, ups=0.15, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.546, loss_scale=8, train_wall=642, gb_free=4, wall=101082
2022-02-26 13:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 13:10:08 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.157 | ppl 71.38 | wps 23640.2 | wpb 2034.1 | bsz 4 | num_updates 15728 | best_loss 6.157
2022-02-26 13:10:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15728 updates
2022-02-26 13:10:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 13:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 13:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 20 @ 15728 updates, score 6.157) (writing took 6.515346636995673 seconds)
2022-02-26 13:10:15 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-26 13:10:15 | INFO | train | epoch 020 | loss 5.664 | ppl 50.71 | wps 10174 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 15728 | lr 0.000252152 | gnorm 0.557 | loss_scale 8 | train_wall 5005 | gb_free 4 | wall 101274
2022-02-26 13:10:15 | INFO | fairseq.trainer | begin training epoch 21
2022-02-26 13:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 13:17:56 | INFO | train_inner | epoch 021:     72 / 788 loss=5.612, ppl=48.92, wps=9982.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=15800, lr=0.000251577, gnorm=0.559, loss_scale=8, train_wall=633, gb_free=4, wall=101736
2022-02-26 13:28:36 | INFO | train_inner | epoch 021:    172 / 788 loss=5.595, ppl=48.33, wps=10233.1, ups=0.16, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.563, loss_scale=8, train_wall=635, gb_free=4, wall=102376
2022-02-26 13:39:17 | INFO | train_inner | epoch 021:    272 / 788 loss=5.618, ppl=49.11, wps=10233.9, ups=0.16, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.563, loss_scale=8, train_wall=635, gb_free=4, wall=103017
2022-02-26 13:49:57 | INFO | train_inner | epoch 021:    372 / 788 loss=5.637, ppl=49.76, wps=10239.4, ups=0.16, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.554, loss_scale=8, train_wall=635, gb_free=4, wall=103657
2022-02-26 14:00:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 14:00:44 | INFO | train_inner | epoch 021:    473 / 788 loss=5.641, ppl=49.92, wps=10135.1, ups=0.15, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.574, loss_scale=8, train_wall=642, gb_free=4, wall=104303
2022-02-26 14:11:24 | INFO | train_inner | epoch 021:    573 / 788 loss=5.664, ppl=50.69, wps=10237.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.578, loss_scale=8, train_wall=635, gb_free=4, wall=104943
2022-02-26 14:22:04 | INFO | train_inner | epoch 021:    673 / 788 loss=5.681, ppl=51.31, wps=10240, ups=0.16, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.567, loss_scale=8, train_wall=635, gb_free=4, wall=105583
2022-02-26 14:32:44 | INFO | train_inner | epoch 021:    773 / 788 loss=5.688, ppl=51.56, wps=10239.1, ups=0.16, wpb=65520.6, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.562, loss_scale=8, train_wall=635, gb_free=4, wall=106223
2022-02-26 14:34:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 14:34:26 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.163 | ppl 71.66 | wps 23728.5 | wpb 2034.1 | bsz 4 | num_updates 16515 | best_loss 6.157
2022-02-26 14:34:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16515 updates
2022-02-26 14:34:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-26 14:34:26 | INFO | train | epoch 021 | loss 5.64 | ppl 49.87 | wps 10204.6 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 16515 | lr 0.000246071 | gnorm 0.565 | loss_scale 8 | train_wall 5003 | gb_free 4 | wall 106326
2022-02-26 14:34:26 | INFO | fairseq.trainer | begin training epoch 22
2022-02-26 14:34:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 14:43:30 | INFO | train_inner | epoch 022:     85 / 788 loss=5.573, ppl=47.6, wps=10094.8, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=16600, lr=0.00024544, gnorm=0.55, loss_scale=8, train_wall=632, gb_free=4, wall=106870
2022-02-26 14:54:10 | INFO | train_inner | epoch 022:    185 / 788 loss=5.573, ppl=47.6, wps=10241.9, ups=0.16, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.552, loss_scale=8, train_wall=635, gb_free=4, wall=107509
2022-02-26 14:55:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 15:04:56 | INFO | train_inner | epoch 022:    286 / 788 loss=5.594, ppl=48.31, wps=10139.3, ups=0.15, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.56, loss_scale=8, train_wall=641, gb_free=4, wall=108156
2022-02-26 15:15:36 | INFO | train_inner | epoch 022:    386 / 788 loss=5.608, ppl=48.78, wps=10242.6, ups=0.16, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.582, loss_scale=8, train_wall=635, gb_free=4, wall=108796
2022-02-26 15:26:16 | INFO | train_inner | epoch 022:    486 / 788 loss=5.634, ppl=49.65, wps=10237.2, ups=0.16, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.568, loss_scale=8, train_wall=635, gb_free=4, wall=109436
2022-02-26 15:36:56 | INFO | train_inner | epoch 022:    586 / 788 loss=5.639, ppl=49.82, wps=10238.6, ups=0.16, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.55, loss_scale=8, train_wall=635, gb_free=4, wall=110076
2022-02-26 15:47:36 | INFO | train_inner | epoch 022:    686 / 788 loss=5.661, ppl=50.58, wps=10236.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.578, loss_scale=8, train_wall=635, gb_free=4, wall=110716
2022-02-26 15:50:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 15:58:23 | INFO | train_inner | epoch 022:    787 / 788 loss=5.68, ppl=51.26, wps=10137.1, ups=0.15, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.567, loss_scale=8, train_wall=642, gb_free=4, wall=111363
2022-02-26 15:58:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 15:58:36 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.158 | ppl 71.41 | wps 23716 | wpb 2034.1 | bsz 4 | num_updates 17301 | best_loss 6.157
2022-02-26 15:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17301 updates
2022-02-26 15:58:36 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-26 15:58:36 | INFO | train | epoch 022 | loss 5.618 | ppl 49.13 | wps 10195 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 17301 | lr 0.000240417 | gnorm 0.564 | loss_scale 8 | train_wall 5002 | gb_free 4 | wall 111375
2022-02-26 15:58:36 | INFO | fairseq.trainer | begin training epoch 23
2022-02-26 15:58:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 16:09:09 | INFO | train_inner | epoch 023:     99 / 788 loss=5.532, ppl=46.27, wps=10093, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=17400, lr=0.000239732, gnorm=0.565, loss_scale=8, train_wall=632, gb_free=4, wall=112009
2022-02-26 16:19:49 | INFO | train_inner | epoch 023:    199 / 788 loss=5.55, ppl=46.84, wps=10241.2, ups=0.16, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.551, loss_scale=8, train_wall=635, gb_free=4, wall=112649
2022-02-26 16:30:29 | INFO | train_inner | epoch 023:    299 / 788 loss=5.57, ppl=47.49, wps=10239.9, ups=0.16, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.583, loss_scale=8, train_wall=635, gb_free=4, wall=113289
2022-02-26 16:41:09 | INFO | train_inner | epoch 023:    399 / 788 loss=5.598, ppl=48.43, wps=10240.3, ups=0.16, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.562, loss_scale=8, train_wall=635, gb_free=4, wall=113929
2022-02-26 16:46:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:51:55 | INFO | train_inner | epoch 023:    500 / 788 loss=5.608, ppl=48.76, wps=10139.8, ups=0.15, wpb=65520.6, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.576, loss_scale=8, train_wall=641, gb_free=4, wall=114575
2022-02-26 17:02:35 | INFO | train_inner | epoch 023:    600 / 788 loss=5.621, ppl=49.2, wps=10239.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.573, loss_scale=8, train_wall=635, gb_free=4, wall=115215
2022-02-26 17:13:16 | INFO | train_inner | epoch 023:    700 / 788 loss=5.649, ppl=50.19, wps=10236.2, ups=0.16, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.543, loss_scale=8, train_wall=635, gb_free=4, wall=115855
2022-02-26 17:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 17:22:48 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.151 | ppl 71.04 | wps 23488.9 | wpb 2034.1 | bsz 4 | num_updates 18088 | best_loss 6.151
2022-02-26 17:22:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18088 updates
2022-02-26 17:22:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 17:22:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 17:22:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 23 @ 18088 updates, score 6.151) (writing took 6.932708486914635 seconds)
2022-02-26 17:22:55 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-26 17:22:55 | INFO | train | epoch 023 | loss 5.598 | ppl 48.43 | wps 10189.2 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 18088 | lr 0.000235128 | gnorm 0.566 | loss_scale 8 | train_wall 5004 | gb_free 4 | wall 116434
2022-02-26 17:22:55 | INFO | fairseq.trainer | begin training epoch 24
2022-02-26 17:22:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 17:24:12 | INFO | train_inner | epoch 024:     12 / 788 loss=5.644, ppl=50.01, wps=9945.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.57, loss_scale=8, train_wall=635, gb_free=4, wall=116512
2022-02-26 17:34:53 | INFO | train_inner | epoch 024:    112 / 788 loss=5.522, ppl=45.95, wps=10215.1, ups=0.16, wpb=65519.3, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.576, loss_scale=8, train_wall=636, gb_free=4, wall=117153
2022-02-26 17:42:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 17:45:41 | INFO | train_inner | epoch 024:    213 / 788 loss=5.542, ppl=46.6, wps=10113, ups=0.15, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.576, loss_scale=8, train_wall=643, gb_free=4, wall=117801
2022-02-26 17:56:23 | INFO | train_inner | epoch 024:    313 / 788 loss=5.563, ppl=47.28, wps=10212.8, ups=0.16, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.578, loss_scale=8, train_wall=637, gb_free=4, wall=118443
2022-02-26 18:07:04 | INFO | train_inner | epoch 024:    413 / 788 loss=5.574, ppl=47.63, wps=10217.6, ups=0.16, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.561, loss_scale=8, train_wall=636, gb_free=4, wall=119084
2022-02-26 18:17:46 | INFO | train_inner | epoch 024:    513 / 788 loss=5.589, ppl=48.13, wps=10213.7, ups=0.16, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.619, loss_scale=8, train_wall=637, gb_free=4, wall=119726
2022-02-26 18:28:28 | INFO | train_inner | epoch 024:    613 / 788 loss=5.605, ppl=48.69, wps=10214.4, ups=0.16, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.581, loss_scale=8, train_wall=637, gb_free=4, wall=120367
2022-02-26 18:39:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 18:39:16 | INFO | train_inner | epoch 024:    714 / 788 loss=5.626, ppl=49.38, wps=10112, ups=0.15, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.57, loss_scale=8, train_wall=643, gb_free=4, wall=121015
2022-02-26 18:47:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 18:47:17 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.154 | ppl 71.19 | wps 23458.5 | wpb 2034.1 | bsz 4 | num_updates 18874 | best_loss 6.151
2022-02-26 18:47:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18874 updates
2022-02-26 18:47:17 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-26 18:47:17 | INFO | train | epoch 024 | loss 5.579 | ppl 47.79 | wps 10169.9 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 18874 | lr 0.00023018 | gnorm 0.579 | loss_scale 8 | train_wall 5013 | gb_free 4 | wall 121496
2022-02-26 18:47:17 | INFO | fairseq.trainer | begin training epoch 25
2022-02-26 18:47:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 18:50:03 | INFO | train_inner | epoch 025:     26 / 788 loss=5.591, ppl=48.19, wps=10071.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.569, loss_scale=8, train_wall=633, gb_free=4, wall=121663
2022-02-26 19:00:45 | INFO | train_inner | epoch 025:    126 / 788 loss=5.503, ppl=45.34, wps=10223.5, ups=0.16, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.579, loss_scale=8, train_wall=636, gb_free=4, wall=122304
2022-02-26 19:11:25 | INFO | train_inner | epoch 025:    226 / 788 loss=5.523, ppl=45.98, wps=10226, ups=0.16, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.566, loss_scale=8, train_wall=636, gb_free=4, wall=122945
2022-02-26 19:22:06 | INFO | train_inner | epoch 025:    326 / 788 loss=5.538, ppl=46.45, wps=10227.8, ups=0.16, wpb=65519.3, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.572, loss_scale=8, train_wall=636, gb_free=4, wall=123586
2022-02-26 19:32:47 | INFO | train_inner | epoch 025:    426 / 788 loss=5.56, ppl=47.17, wps=10221.1, ups=0.16, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.575, loss_scale=8, train_wall=636, gb_free=4, wall=124227
2022-02-26 19:34:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 19:43:34 | INFO | train_inner | epoch 025:    527 / 788 loss=5.583, ppl=47.93, wps=10132.1, ups=0.15, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.575, loss_scale=8, train_wall=642, gb_free=4, wall=124874
2022-02-26 19:54:15 | INFO | train_inner | epoch 025:    627 / 788 loss=5.598, ppl=48.45, wps=10224.8, ups=0.16, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.577, loss_scale=8, train_wall=636, gb_free=4, wall=125515
2022-02-26 20:04:57 | INFO | train_inner | epoch 025:    727 / 788 loss=5.615, ppl=49.01, wps=10213.1, ups=0.16, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.574, loss_scale=8, train_wall=637, gb_free=4, wall=126156
2022-02-26 20:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 20:11:35 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.16 | ppl 71.49 | wps 23417.4 | wpb 2034.1 | bsz 4 | num_updates 19661 | best_loss 6.151
2022-02-26 20:11:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19661 updates
2022-02-26 20:11:35 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-26 20:11:35 | INFO | train | epoch 025 | loss 5.561 | ppl 47.22 | wps 10191.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 19661 | lr 0.000225526 | gnorm 0.575 | loss_scale 8 | train_wall 5009 | gb_free 4 | wall 126554
2022-02-26 20:11:35 | INFO | fairseq.trainer | begin training epoch 26
2022-02-26 20:11:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 20:15:45 | INFO | train_inner | epoch 026:     39 / 788 loss=5.556, ppl=47.05, wps=10063.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.598, loss_scale=8, train_wall=634, gb_free=4, wall=126805
2022-02-26 20:26:27 | INFO | train_inner | epoch 026:    139 / 788 loss=5.479, ppl=44.62, wps=10210.8, ups=0.16, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.568, loss_scale=8, train_wall=637, gb_free=4, wall=127447
2022-02-26 20:30:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 20:37:15 | INFO | train_inner | epoch 026:    240 / 788 loss=5.514, ppl=45.68, wps=10111.5, ups=0.15, wpb=65519.3, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.569, loss_scale=8, train_wall=643, gb_free=4, wall=128095
2022-02-26 20:47:56 | INFO | train_inner | epoch 026:    340 / 788 loss=5.529, ppl=46.16, wps=10217.2, ups=0.16, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.601, loss_scale=8, train_wall=636, gb_free=4, wall=128736
2022-02-26 20:58:37 | INFO | train_inner | epoch 026:    440 / 788 loss=5.547, ppl=46.74, wps=10223.5, ups=0.16, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.584, loss_scale=8, train_wall=636, gb_free=4, wall=129377
2022-02-26 21:09:18 | INFO | train_inner | epoch 026:    540 / 788 loss=5.565, ppl=47.34, wps=10221.8, ups=0.16, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.575, loss_scale=8, train_wall=636, gb_free=4, wall=130018
2022-02-26 21:19:59 | INFO | train_inner | epoch 026:    640 / 788 loss=5.581, ppl=47.87, wps=10224.5, ups=0.16, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.595, loss_scale=8, train_wall=636, gb_free=4, wall=130659
2022-02-26 21:30:40 | INFO | train_inner | epoch 026:    740 / 788 loss=5.597, ppl=48.4, wps=10222.7, ups=0.16, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.578, loss_scale=16, train_wall=636, gb_free=4, wall=131300
2022-02-26 21:35:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 21:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 21:35:55 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.151 | ppl 71.07 | wps 23506 | wpb 2034.1 | bsz 4 | num_updates 20447 | best_loss 6.151
2022-02-26 21:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20447 updates
2022-02-26 21:35:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 21:36:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-26 21:36:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 26 @ 20447 updates, score 6.151) (writing took 7.566319953650236 seconds)
2022-02-26 21:36:02 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-26 21:36:02 | INFO | train | epoch 026 | loss 5.544 | ppl 46.66 | wps 10158.3 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 20447 | lr 0.000221149 | gnorm 0.583 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 131622
2022-02-26 21:36:03 | INFO | fairseq.trainer | begin training epoch 27
2022-02-26 21:36:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 21:41:43 | INFO | train_inner | epoch 027:     53 / 788 loss=5.523, ppl=45.99, wps=9854.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=20500, lr=0.000220863, gnorm=0.602, loss_scale=8, train_wall=640, gb_free=4, wall=131962
2022-02-26 21:52:24 | INFO | train_inner | epoch 027:    153 / 788 loss=5.471, ppl=44.36, wps=10213, ups=0.16, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.571, loss_scale=8, train_wall=637, gb_free=4, wall=132604
2022-02-26 22:03:05 | INFO | train_inner | epoch 027:    253 / 788 loss=5.5, ppl=45.24, wps=10223.1, ups=0.16, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.588, loss_scale=8, train_wall=636, gb_free=4, wall=133245
2022-02-26 22:13:46 | INFO | train_inner | epoch 027:    353 / 788 loss=5.523, ppl=45.97, wps=10231.4, ups=0.16, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.574, loss_scale=8, train_wall=636, gb_free=4, wall=133886
2022-02-26 22:24:27 | INFO | train_inner | epoch 027:    453 / 788 loss=5.535, ppl=46.37, wps=10220.4, ups=0.16, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.6, loss_scale=8, train_wall=636, gb_free=4, wall=134527
2022-02-26 22:32:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 22:35:15 | INFO | train_inner | epoch 027:    554 / 788 loss=5.549, ppl=46.81, wps=10121.6, ups=0.15, wpb=65534.7, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.585, loss_scale=8, train_wall=642, gb_free=4, wall=135174
2022-02-26 22:45:56 | INFO | train_inner | epoch 027:    654 / 788 loss=5.558, ppl=47.1, wps=10220.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.585, loss_scale=8, train_wall=636, gb_free=4, wall=135815
2022-02-26 22:56:37 | INFO | train_inner | epoch 027:    754 / 788 loss=5.586, ppl=48.05, wps=10218.9, ups=0.16, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.571, loss_scale=8, train_wall=636, gb_free=4, wall=136457
2022-02-26 23:00:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 23:00:21 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.154 | ppl 71.19 | wps 23484.5 | wpb 2034.1 | bsz 4 | num_updates 21234 | best_loss 6.151
2022-02-26 23:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21234 updates
2022-02-26 23:00:21 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-26 23:00:21 | INFO | train | epoch 027 | loss 5.528 | ppl 46.15 | wps 10189.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 21234 | lr 0.000217012 | gnorm 0.585 | loss_scale 8 | train_wall 5010 | gb_free 4 | wall 136681
2022-02-26 23:00:22 | INFO | fairseq.trainer | begin training epoch 28
2022-02-26 23:00:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 23:07:25 | INFO | train_inner | epoch 028:     66 / 788 loss=5.483, ppl=44.72, wps=10071.1, ups=0.15, wpb=65248, bsz=127.4, num_updates=21300, lr=0.000216676, gnorm=0.587, loss_scale=8, train_wall=633, gb_free=4, wall=137105
2022-02-26 23:18:06 | INFO | train_inner | epoch 028:    166 / 788 loss=5.456, ppl=43.91, wps=10222.1, ups=0.16, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.608, loss_scale=8, train_wall=636, gb_free=4, wall=137746
2022-02-26 23:28:47 | INFO | train_inner | epoch 028:    266 / 788 loss=5.493, ppl=45.04, wps=10217, ups=0.16, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.585, loss_scale=16, train_wall=636, gb_free=4, wall=138387
2022-02-26 23:31:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 23:39:35 | INFO | train_inner | epoch 028:    367 / 788 loss=5.505, ppl=45.42, wps=10114.2, ups=0.15, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.587, loss_scale=8, train_wall=643, gb_free=4, wall=139035
2022-02-26 23:50:17 | INFO | train_inner | epoch 028:    467 / 788 loss=5.515, ppl=45.72, wps=10213.7, ups=0.16, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.599, loss_scale=8, train_wall=636, gb_free=4, wall=139677
2022-02-27 00:00:59 | INFO | train_inner | epoch 028:    567 / 788 loss=5.532, ppl=46.28, wps=10214, ups=0.16, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.588, loss_scale=8, train_wall=637, gb_free=4, wall=140318
2022-02-27 00:11:40 | INFO | train_inner | epoch 028:    667 / 788 loss=5.563, ppl=47.27, wps=10219.5, ups=0.16, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.576, loss_scale=8, train_wall=636, gb_free=4, wall=140960
2022-02-27 00:22:21 | INFO | train_inner | epoch 028:    767 / 788 loss=5.566, ppl=47.39, wps=10222.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.596, loss_scale=8, train_wall=636, gb_free=4, wall=141601
2022-02-27 00:24:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 00:24:42 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.149 | ppl 70.97 | wps 23703.3 | wpb 2034.1 | bsz 4 | num_updates 22021 | best_loss 6.149
2022-02-27 00:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 22021 updates
2022-02-27 00:24:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-27 00:24:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-27 00:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 28 @ 22021 updates, score 6.149) (writing took 7.89237676281482 seconds)
2022-02-27 00:24:50 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-27 00:24:50 | INFO | train | epoch 028 | loss 5.514 | ppl 45.69 | wps 10170.8 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 22021 | lr 0.000213099 | gnorm 0.59 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 141749
2022-02-27 00:24:50 | INFO | fairseq.trainer | begin training epoch 29
2022-02-27 00:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 00:31:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 00:33:22 | INFO | train_inner | epoch 029:     80 / 788 loss=5.458, ppl=43.97, wps=9869.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22100, lr=0.000212718, gnorm=0.574, loss_scale=8, train_wall=639, gb_free=4, wall=142262
2022-02-27 00:44:03 | INFO | train_inner | epoch 029:    180 / 788 loss=5.454, ppl=43.85, wps=10227.2, ups=0.16, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.588, loss_scale=8, train_wall=636, gb_free=4, wall=142903
2022-02-27 00:54:44 | INFO | train_inner | epoch 029:    280 / 788 loss=5.474, ppl=44.46, wps=10223.3, ups=0.16, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.578, loss_scale=8, train_wall=636, gb_free=4, wall=143544
2022-02-27 01:05:25 | INFO | train_inner | epoch 029:    380 / 788 loss=5.493, ppl=45.03, wps=10227, ups=0.16, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.592, loss_scale=8, train_wall=636, gb_free=4, wall=144184
2022-02-27 01:16:06 | INFO | train_inner | epoch 029:    480 / 788 loss=5.514, ppl=45.71, wps=10215.3, ups=0.16, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.591, loss_scale=8, train_wall=636, gb_free=4, wall=144826
2022-02-27 01:26:48 | INFO | train_inner | epoch 029:    580 / 788 loss=5.529, ppl=46.16, wps=10212.6, ups=0.16, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.585, loss_scale=16, train_wall=637, gb_free=4, wall=145468
2022-02-27 01:27:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 01:37:36 | INFO | train_inner | epoch 029:    681 / 788 loss=5.533, ppl=46.32, wps=10113.3, ups=0.15, wpb=65519.3, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.603, loss_scale=8, train_wall=643, gb_free=4, wall=146116
2022-02-27 01:48:17 | INFO | train_inner | epoch 029:    781 / 788 loss=5.556, ppl=47.04, wps=10213.5, ups=0.16, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.59, loss_scale=8, train_wall=637, gb_free=4, wall=146757
2022-02-27 01:49:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 01:49:09 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.147 | ppl 70.86 | wps 23410.1 | wpb 2034.1 | bsz 4 | num_updates 22807 | best_loss 6.147
2022-02-27 01:49:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22807 updates
2022-02-27 01:49:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-27 01:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt
2022-02-27 01:49:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.038_0.002_0.96_#6/checkpoint_best.pt (epoch 29 @ 22807 updates, score 6.147) (writing took 7.488875626586378 seconds)
2022-02-27 01:49:16 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-27 01:49:16 | INFO | train | epoch 029 | loss 5.5 | ppl 45.24 | wps 10160.5 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 22807 | lr 0.000209395 | gnorm 0.588 | loss_scale 8 | train_wall 5010 | gb_free 4 | wall 146816
2022-02-27 01:49:16 | INFO | fairseq.trainer | begin training epoch 30
2022-02-27 01:49:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 01:59:13 | INFO | train_inner | epoch 030:     93 / 788 loss=5.425, ppl=42.96, wps=9954.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22900, lr=0.000208969, gnorm=0.592, loss_scale=8, train_wall=634, gb_free=4, wall=147413
2022-02-27 02:09:54 | INFO | train_inner | epoch 030:    193 / 788 loss=5.442, ppl=43.47, wps=10218, ups=0.16, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.586, loss_scale=8, train_wall=636, gb_free=4, wall=148054
2022-02-27 02:20:36 | INFO | train_inner | epoch 030:    293 / 788 loss=5.474, ppl=44.45, wps=10219.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.599, loss_scale=8, train_wall=636, gb_free=4, wall=148695
2022-02-27 02:31:17 | INFO | train_inner | epoch 030:    393 / 788 loss=5.479, ppl=44.6, wps=10219.2, ups=0.16, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.584, loss_scale=16, train_wall=636, gb_free=4, wall=149337
2022-02-27 02:32:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 02:42:04 | INFO | train_inner | epoch 030:    494 / 788 loss=5.507, ppl=45.48, wps=10120.4, ups=0.15, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.618, loss_scale=8, train_wall=643, gb_free=4, wall=149984
2022-02-27 02:52:45 | INFO | train_inner | epoch 030:    594 / 788 loss=5.509, ppl=45.53, wps=10228.1, ups=0.16, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.587, loss_scale=8, train_wall=636, gb_free=4, wall=150625
2022-02-27 03:03:26 | INFO | train_inner | epoch 030:    694 / 788 loss=5.529, ppl=46.16, wps=10233.9, ups=0.16, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.596, loss_scale=8, train_wall=635, gb_free=4, wall=151265
2022-02-27 03:13:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 03:13:35 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.16 | ppl 71.52 | wps 23490.6 | wpb 2034.1 | bsz 4 | num_updates 23594 | best_loss 6.147
2022-02-27 03:13:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23594 updates
2022-02-27 03:13:35 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-27 03:13:35 | INFO | train | epoch 030 | loss 5.486 | ppl 44.82 | wps 10190.2 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 23594 | lr 0.000205873 | gnorm 0.594 | loss_scale 8 | train_wall 5010 | gb_free 4 | wall 151875
2022-02-27 03:13:35 | INFO | fairseq.trainer | begin training epoch 31
2022-02-27 03:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 03:14:13 | INFO | train_inner | epoch 031:      6 / 788 loss=5.524, ppl=46.02, wps=10070.2, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.587, loss_scale=8, train_wall=633, gb_free=4, wall=151913
2022-02-27 03:24:55 | INFO | train_inner | epoch 031:    106 / 788 loss=5.407, ppl=42.42, wps=10217.6, ups=0.16, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.594, loss_scale=8, train_wall=636, gb_free=4, wall=152554
2022-02-27 03:27:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 03:35:43 | INFO | train_inner | epoch 031:    207 / 788 loss=5.428, ppl=43.05, wps=10114, ups=0.15, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.611, loss_scale=8, train_wall=643, gb_free=4, wall=153202
2022-02-27 03:46:24 | INFO | train_inner | epoch 031:    307 / 788 loss=5.454, ppl=43.82, wps=10213.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.596, loss_scale=8, train_wall=637, gb_free=4, wall=153844
2022-02-27 03:57:06 | INFO | train_inner | epoch 031:    407 / 788 loss=5.469, ppl=44.29, wps=10215.6, ups=0.16, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.596, loss_scale=8, train_wall=636, gb_free=4, wall=154486
2022-02-27 04:07:47 | INFO | train_inner | epoch 031:    507 / 788 loss=5.491, ppl=44.98, wps=10215.9, ups=0.16, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.602, loss_scale=8, train_wall=636, gb_free=4, wall=155127
2022-02-27 04:18:29 | INFO | train_inner | epoch 031:    607 / 788 loss=5.507, ppl=45.46, wps=10217.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.594, loss_scale=8, train_wall=636, gb_free=4, wall=155768
2022-02-27 04:23:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 04:29:16 | INFO | train_inner | epoch 031:    708 / 788 loss=5.509, ppl=45.55, wps=10118.2, ups=0.15, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.594, loss_scale=8, train_wall=643, gb_free=4, wall=156416
2022-02-27 04:37:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 04:37:56 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.156 | ppl 71.3 | wps 23446.3 | wpb 2034.1 | bsz 4 | num_updates 24380 | best_loss 6.147
2022-02-27 04:37:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24380 updates
2022-02-27 04:37:56 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-27 04:37:56 | INFO | train | epoch 031 | loss 5.473 | ppl 44.42 | wps 10171.7 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 24380 | lr 0.000202527 | gnorm 0.596 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 156936
2022-02-27 04:37:56 | INFO | fairseq.trainer | begin training epoch 32
2022-02-27 04:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 04:40:04 | INFO | train_inner | epoch 032:     20 / 788 loss=5.51, ppl=45.57, wps=10070.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.58, loss_scale=8, train_wall=634, gb_free=4, wall=157064
2022-02-27 04:50:46 | INFO | train_inner | epoch 032:    120 / 788 loss=5.4, ppl=42.23, wps=10220.2, ups=0.16, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.603, loss_scale=8, train_wall=636, gb_free=4, wall=157705
2022-02-27 05:01:27 | INFO | train_inner | epoch 032:    220 / 788 loss=5.419, ppl=42.8, wps=10216.2, ups=0.16, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.604, loss_scale=8, train_wall=636, gb_free=4, wall=158347
2022-02-27 05:12:08 | INFO | train_inner | epoch 032:    320 / 788 loss=5.443, ppl=43.5, wps=10216.1, ups=0.16, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.632, loss_scale=8, train_wall=636, gb_free=4, wall=158988
2022-02-27 05:22:49 | INFO | train_inner | epoch 032:    420 / 788 loss=5.466, ppl=44.19, wps=10233.7, ups=0.16, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.613, loss_scale=16, train_wall=635, gb_free=4, wall=159629
2022-02-27 05:22:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 05:33:36 | INFO | train_inner | epoch 032:    521 / 788 loss=5.476, ppl=44.52, wps=10120, ups=0.15, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.607, loss_scale=8, train_wall=642, gb_free=4, wall=160276
2022-02-27 05:44:17 | INFO | train_inner | epoch 032:    621 / 788 loss=5.489, ppl=44.92, wps=10221.2, ups=0.16, wpb=65519.3, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.6, loss_scale=8, train_wall=636, gb_free=4, wall=160917
2022-02-27 05:54:59 | INFO | train_inner | epoch 032:    721 / 788 loss=5.508, ppl=45.52, wps=10216, ups=0.16, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.589, loss_scale=8, train_wall=636, gb_free=4, wall=161559
2022-02-27 06:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 06:02:15 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.157 | ppl 71.34 | wps 23382.4 | wpb 2034.1 | bsz 4 | num_updates 25167 | best_loss 6.147
2022-02-27 06:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25167 updates
2022-02-27 06:02:15 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-27 06:02:15 | INFO | train | epoch 032 | loss 5.461 | ppl 44.05 | wps 10188.4 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 25167 | lr 0.000199335 | gnorm 0.607 | loss_scale 8 | train_wall 5010 | gb_free 4 | wall 161995
2022-02-27 06:02:15 | INFO | fairseq.trainer | begin training epoch 33
2022-02-27 06:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 06:05:47 | INFO | train_inner | epoch 033:     33 / 788 loss=5.473, ppl=44.42, wps=10066.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.603, loss_scale=8, train_wall=634, gb_free=4, wall=162207
2022-02-27 06:16:29 | INFO | train_inner | epoch 033:    133 / 788 loss=5.38, ppl=41.65, wps=10215.3, ups=0.16, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.596, loss_scale=8, train_wall=636, gb_free=4, wall=162849
2022-02-27 06:18:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 06:27:17 | INFO | train_inner | epoch 033:    234 / 788 loss=5.421, ppl=42.85, wps=10115.4, ups=0.15, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.594, loss_scale=8, train_wall=643, gb_free=4, wall=163496
2022-02-27 06:37:58 | INFO | train_inner | epoch 033:    334 / 788 loss=5.433, ppl=43.21, wps=10221.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.607, loss_scale=8, train_wall=636, gb_free=4, wall=164137
2022-02-27 06:48:38 | INFO | train_inner | epoch 033:    434 / 788 loss=5.446, ppl=43.59, wps=10226.8, ups=0.16, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.604, loss_scale=8, train_wall=636, gb_free=4, wall=164778
2022-02-27 06:59:19 | INFO | train_inner | epoch 033:    534 / 788 loss=5.477, ppl=44.52, wps=10226, ups=0.16, wpb=65534.7, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.608, loss_scale=8, train_wall=636, gb_free=4, wall=165419
2022-02-27 07:10:01 | INFO | train_inner | epoch 033:    634 / 788 loss=5.485, ppl=44.79, wps=10220.3, ups=0.16, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.604, loss_scale=8, train_wall=636, gb_free=4, wall=166060
2022-02-27 07:19:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 07:20:49 | INFO | train_inner | epoch 033:    735 / 788 loss=5.502, ppl=45.32, wps=10113.5, ups=0.15, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.601, loss_scale=8, train_wall=643, gb_free=4, wall=166708
2022-02-27 07:26:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 07:26:35 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.151 | ppl 71.05 | wps 23473.8 | wpb 2034.1 | bsz 4 | num_updates 25953 | best_loss 6.147
2022-02-27 07:26:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25953 updates
2022-02-27 07:26:35 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-27 07:26:35 | INFO | train | epoch 033 | loss 5.45 | ppl 43.71 | wps 10174.5 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 25953 | lr 0.000196294 | gnorm 0.602 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 167055
2022-02-27 07:26:35 | INFO | fairseq.trainer | begin training epoch 34
2022-02-27 07:26:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 07:31:37 | INFO | train_inner | epoch 034:     47 / 788 loss=5.439, ppl=43.38, wps=10066.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=26000, lr=0.000196116, gnorm=0.608, loss_scale=8, train_wall=634, gb_free=4, wall=167357
2022-02-27 07:42:18 | INFO | train_inner | epoch 034:    147 / 788 loss=5.375, ppl=41.51, wps=10223.4, ups=0.16, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.605, loss_scale=8, train_wall=636, gb_free=4, wall=167998
2022-02-27 07:52:58 | INFO | train_inner | epoch 034:    247 / 788 loss=5.401, ppl=42.26, wps=10231.1, ups=0.16, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.619, loss_scale=8, train_wall=636, gb_free=4, wall=168638
2022-02-27 08:03:40 | INFO | train_inner | epoch 034:    347 / 788 loss=5.431, ppl=43.15, wps=10213.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.611, loss_scale=8, train_wall=636, gb_free=4, wall=169280
2022-02-27 08:14:21 | INFO | train_inner | epoch 034:    447 / 788 loss=5.45, ppl=43.72, wps=10217.9, ups=0.16, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.605, loss_scale=16, train_wall=636, gb_free=4, wall=169921
2022-02-27 08:22:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 08:25:09 | INFO | train_inner | epoch 034:    548 / 788 loss=5.456, ppl=43.89, wps=10110.7, ups=0.15, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.609, loss_scale=8, train_wall=643, gb_free=4, wall=170569
2022-02-27 08:35:51 | INFO | train_inner | epoch 034:    648 / 788 loss=5.479, ppl=44.6, wps=10213.9, ups=0.16, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.619, loss_scale=8, train_wall=637, gb_free=4, wall=171211
2022-02-27 08:46:33 | INFO | train_inner | epoch 034:    748 / 788 loss=5.491, ppl=44.96, wps=10214.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.604, loss_scale=8, train_wall=637, gb_free=4, wall=171852
2022-02-27 08:50:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 08:50:56 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.154 | ppl 71.2 | wps 23455.4 | wpb 2034.1 | bsz 4 | num_updates 26740 | best_loss 6.147
2022-02-27 08:50:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 26740 updates
2022-02-27 08:50:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-27 08:50:56 | INFO | train | epoch 034 | loss 5.439 | ppl 43.39 | wps 10185.7 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 26740 | lr 0.000193383 | gnorm 0.611 | loss_scale 8 | train_wall 5012 | gb_free 4 | wall 172116
2022-02-27 08:50:56 | INFO | fairseq.trainer | begin training epoch 35
2022-02-27 08:50:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 08:57:21 | INFO | train_inner | epoch 035:     60 / 788 loss=5.413, ppl=42.61, wps=10067.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=26800, lr=0.000193167, gnorm=0.604, loss_scale=8, train_wall=634, gb_free=4, wall=172501
2022-02-27 09:08:02 | INFO | train_inner | epoch 035:    160 / 788 loss=5.375, ppl=41.48, wps=10217.2, ups=0.16, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.611, loss_scale=8, train_wall=636, gb_free=4, wall=173142
2022-02-27 09:18:44 | INFO | train_inner | epoch 035:    260 / 788 loss=5.41, ppl=42.51, wps=10216.8, ups=0.16, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.607, loss_scale=16, train_wall=636, gb_free=4, wall=173783
2022-02-27 09:24:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 09:29:31 | INFO | train_inner | epoch 035:    361 / 788 loss=5.415, ppl=42.67, wps=10118.4, ups=0.15, wpb=65534.7, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.62, loss_scale=8, train_wall=643, gb_free=4, wall=174431
2022-02-27 09:40:13 | INFO | train_inner | epoch 035:    461 / 788 loss=5.435, ppl=43.26, wps=10216.1, ups=0.16, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.608, loss_scale=8, train_wall=636, gb_free=4, wall=175073
2022-02-27 09:50:54 | INFO | train_inner | epoch 035:    561 / 788 loss=5.455, ppl=43.85, wps=10219, ups=0.16, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.607, loss_scale=8, train_wall=636, gb_free=4, wall=175714
2022-02-27 10:01:35 | INFO | train_inner | epoch 035:    661 / 788 loss=5.462, ppl=44.07, wps=10221.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.609, loss_scale=8, train_wall=636, gb_free=4, wall=176355
2022-02-27 10:12:15 | INFO | train_inner | epoch 035:    761 / 788 loss=5.479, ppl=44.61, wps=10234.5, ups=0.16, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.621, loss_scale=8, train_wall=635, gb_free=4, wall=176995
2022-02-27 10:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 10:15:15 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.159 | ppl 71.44 | wps 23680 | wpb 2034.1 | bsz 4 | num_updates 27527 | best_loss 6.147
2022-02-27 10:15:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 27527 updates
2022-02-27 10:15:15 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-27 10:15:15 | INFO | train | epoch 035 | loss 5.429 | ppl 43.08 | wps 10189.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 27527 | lr 0.000190599 | gnorm 0.611 | loss_scale 8 | train_wall 5010 | gb_free 4 | wall 177175
2022-02-27 10:15:15 | INFO | fairseq.trainer | begin training epoch 36
2022-02-27 10:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 10:19:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 10:23:09 | INFO | train_inner | epoch 036:     74 / 788 loss=5.384, ppl=41.75, wps=9980.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=27600, lr=0.000190347, gnorm=0.615, loss_scale=8, train_wall=639, gb_free=4, wall=177649
2022-02-27 10:33:51 | INFO | train_inner | epoch 036:    174 / 788 loss=5.37, ppl=41.35, wps=10220.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.605, loss_scale=8, train_wall=636, gb_free=4, wall=178290
2022-02-27 10:44:32 | INFO | train_inner | epoch 036:    274 / 788 loss=5.385, ppl=41.79, wps=10214, ups=0.16, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.626, loss_scale=8, train_wall=637, gb_free=4, wall=178932
2022-02-27 10:55:14 | INFO | train_inner | epoch 036:    374 / 788 loss=5.411, ppl=42.54, wps=10215.6, ups=0.16, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.627, loss_scale=8, train_wall=636, gb_free=4, wall=179573
2022-02-27 11:05:55 | INFO | train_inner | epoch 036:    474 / 788 loss=5.427, ppl=43.02, wps=10217.3, ups=0.16, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.611, loss_scale=8, train_wall=636, gb_free=4, wall=180215
2022-02-27 11:16:36 | INFO | train_inner | epoch 036:    574 / 788 loss=5.449, ppl=43.68, wps=10217.8, ups=0.16, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.606, loss_scale=16, train_wall=636, gb_free=4, wall=180856
2022-02-27 11:27:18 | INFO | train_inner | epoch 036:    674 / 788 loss=5.462, ppl=44.08, wps=10217.8, ups=0.16, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.608, loss_scale=16, train_wall=636, gb_free=4, wall=181498
2022-02-27 11:37:59 | INFO | train_inner | epoch 036:    774 / 788 loss=5.48, ppl=44.64, wps=10212.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=28300, lr=0.000187978, gnorm=0.608, loss_scale=16, train_wall=637, gb_free=4, wall=182139
2022-02-27 11:39:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 11:39:36 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.154 | ppl 71.2 | wps 23450.4 | wpb 2034.1 | bsz 4 | num_updates 28314 | best_loss 6.147
2022-02-27 11:39:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 28314 updates
2022-02-27 11:39:36 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-27 11:39:36 | INFO | train | epoch 036 | loss 5.419 | ppl 42.79 | wps 10185.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 28314 | lr 0.000187931 | gnorm 0.614 | loss_scale 16 | train_wall 5012 | gb_free 4 | wall 182236
2022-02-27 11:39:36 | INFO | fairseq.trainer | begin training epoch 37
2022-02-27 11:39:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 11:46:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 11:48:54 | INFO | train_inner | epoch 037:     87 / 788 loss=5.352, ppl=40.83, wps=9963.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=28400, lr=0.000187647, gnorm=0.607, loss_scale=8, train_wall=640, gb_free=4, wall=182794
2022-02-27 11:59:36 | INFO | train_inner | epoch 037:    187 / 788 loss=5.353, ppl=40.88, wps=10213.5, ups=0.16, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.616, loss_scale=8, train_wall=637, gb_free=4, wall=183436
2022-02-27 12:10:17 | INFO | train_inner | epoch 037:    287 / 788 loss=5.384, ppl=41.76, wps=10218.4, ups=0.16, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.619, loss_scale=8, train_wall=636, gb_free=4, wall=184077
2022-02-27 12:20:59 | INFO | train_inner | epoch 037:    387 / 788 loss=5.398, ppl=42.16, wps=10217, ups=0.16, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.615, loss_scale=8, train_wall=636, gb_free=4, wall=184719
2022-02-27 12:31:39 | INFO | train_inner | epoch 037:    487 / 788 loss=5.426, ppl=43, wps=10228, ups=0.16, wpb=65520.6, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.617, loss_scale=8, train_wall=636, gb_free=4, wall=185359
2022-02-27 12:42:20 | INFO | train_inner | epoch 037:    587 / 788 loss=5.449, ppl=43.69, wps=10229.2, ups=0.16, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.615, loss_scale=16, train_wall=636, gb_free=4, wall=186000
2022-02-27 12:53:01 | INFO | train_inner | epoch 037:    687 / 788 loss=5.453, ppl=43.82, wps=10217.5, ups=0.16, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.618, loss_scale=16, train_wall=636, gb_free=4, wall=186641
2022-02-27 12:53:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 13:03:46 | INFO | train_inner | epoch 037:    788 / 788 loss=5.474, ppl=44.44, wps=10116.9, ups=0.16, wpb=65248, bsz=127.4, num_updates=29100, lr=0.000185376, gnorm=0.616, loss_scale=8, train_wall=640, gb_free=4, wall=187286
2022-02-27 13:03:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 13:03:56 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.158 | ppl 71.43 | wps 23603 | wpb 2034.1 | bsz 4 | num_updates 29100 | best_loss 6.147
2022-02-27 13:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 29100 updates
2022-02-27 13:03:56 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-27 13:03:56 | INFO | train | epoch 037 | loss 5.41 | ppl 42.52 | wps 10174.4 | ups 0.16 | wpb 65497.4 | bsz 127.9 | num_updates 29100 | lr 0.000185376 | gnorm 0.615 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 187295
2022-02-27 13:03:56 | INFO | fairseq.trainer | begin training epoch 38
2022-02-27 13:03:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 13:14:37 | INFO | train_inner | epoch 038:    100 / 788 loss=5.321, ppl=39.98, wps=10072.6, ups=0.15, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.61, loss_scale=8, train_wall=636, gb_free=4, wall=187937
2022-02-27 13:25:19 | INFO | train_inner | epoch 038:    200 / 788 loss=5.352, ppl=40.83, wps=10216.4, ups=0.16, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.627, loss_scale=8, train_wall=636, gb_free=4, wall=188578
2022-02-27 13:36:00 | INFO | train_inner | epoch 038:    300 / 788 loss=5.384, ppl=41.76, wps=10216.4, ups=0.16, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.622, loss_scale=8, train_wall=636, gb_free=4, wall=189220
2022-02-27 13:46:41 | INFO | train_inner | epoch 038:    400 / 788 loss=5.406, ppl=42.41, wps=10217.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.635, loss_scale=8, train_wall=636, gb_free=4, wall=189861
2022-02-27 13:57:23 | INFO | train_inner | epoch 038:    500 / 788 loss=5.417, ppl=42.72, wps=10220.2, ups=0.16, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.619, loss_scale=16, train_wall=636, gb_free=4, wall=190502
2022-02-27 14:01:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 14:08:10 | INFO | train_inner | epoch 038:    601 / 788 loss=5.436, ppl=43.3, wps=10116.3, ups=0.15, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.62, loss_scale=8, train_wall=643, gb_free=4, wall=191150
2022-02-27 14:18:52 | INFO | train_inner | epoch 038:    701 / 788 loss=5.447, ppl=43.63, wps=10220.7, ups=0.16, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.615, loss_scale=8, train_wall=636, gb_free=4, wall=191791
2022-02-27 14:28:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 14:28:16 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.161 | ppl 71.56 | wps 23489.1 | wpb 2034.1 | bsz 4 | num_updates 29887 | best_loss 6.147
2022-02-27 14:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 29887 updates
2022-02-27 14:28:16 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-27 14:28:16 | INFO | train | epoch 038 | loss 5.402 | ppl 42.27 | wps 10186.5 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 29887 | lr 0.000182919 | gnorm 0.62 | loss_scale 8 | train_wall 5011 | gb_free 4 | wall 192356
2022-02-27 14:28:16 | INFO | fairseq.trainer | begin training epoch 39
2022-02-27 14:28:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 14:29:39 | INFO | train_inner | epoch 039:     13 / 788 loss=5.44, ppl=43.41, wps=10070.4, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=29900, lr=0.000182879, gnorm=0.614, loss_scale=8, train_wall=633, gb_free=4, wall=192439
2022-02-27 14:40:21 | INFO | train_inner | epoch 039:    113 / 788 loss=5.321, ppl=39.98, wps=10220.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.627, loss_scale=8, train_wall=636, gb_free=4, wall=193080
2022-02-27 14:51:02 | INFO | train_inner | epoch 039:    213 / 788 loss=5.355, ppl=40.91, wps=10224.7, ups=0.16, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.622, loss_scale=8, train_wall=636, gb_free=4, wall=193721
2022-02-27 15:01:42 | INFO | train_inner | epoch 039:    313 / 788 loss=5.375, ppl=41.49, wps=10234, ups=0.16, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.621, loss_scale=16, train_wall=635, gb_free=4, wall=194362
2022-02-27 15:12:23 | INFO | train_inner | epoch 039:    413 / 788 loss=5.39, ppl=41.94, wps=10223.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.633, loss_scale=16, train_wall=636, gb_free=4, wall=195003
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
