Sender: LSF System <lsfadmin@eu-g3-054>
Subject: Job 210657995: <wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.02_0.08_0.9_#1> in cluster <euler> Exited

Job <wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.02_0.08_0.9_#1> was submitted from host <eu-login-27> by user <andriusb> in cluster <euler> at Wed Mar 23 19:48:50 2022
Job was executed on host(s) <eu-g3-054>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Wed Mar 23 19:48:57 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 19:48:57 2022
Terminated at Wed Mar 23 19:50:06 2022
Results reported at Wed Mar 23 19:50:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-cleaned-bpe-size0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.02_0.08_0.9_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.35 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.02,0.08,0.9\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --seed 1321671 --no-epoch-checkpoints --no-last-checkpoints --patience 3 --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   66.27 sec.
    Max Memory :                                 2769 MB
    Average Memory :                             2224.00 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               17231.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   69 sec.
    Turnaround time :                            76 sec.

The output (if any) follows:

2022-03-23 19:49:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.02_0.08_0.9_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.35, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-cleaned-bpe-size0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02,0.08,0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 19:49:03 | INFO | fairseq.tasks.language_modeling | dictionary: 39136 types
2022-03-23 19:49:04 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
Calculating frequency stats:
  0%|          | 0/112584 [00:00<?, ?it/s]  1%|          | 640/112584 [00:00<00:17, 6398.79it/s]  1%|          | 1280/112584 [00:00<00:20, 5543.18it/s]  2%|▏         | 1842/112584 [00:00<00:20, 5439.89it/s]  2%|▏         | 2390/112584 [00:00<00:20, 5281.76it/s]  3%|▎         | 3132/112584 [00:00<00:18, 6004.63it/s]  3%|▎         | 3739/112584 [00:00<00:18, 5792.98it/s]  4%|▍         | 4386/112584 [00:00<00:18, 6001.73it/s]  5%|▍         | 5086/112584 [00:00<00:17, 6306.47it/s]  5%|▌         | 5721/112584 [00:00<00:16, 6294.81it/s]  6%|▌         | 6354/112584 [00:01<00:17, 6060.10it/s]  6%|▌         | 6964/112584 [00:01<00:18, 5669.69it/s]  7%|▋         | 7568/112584 [00:01<00:18, 5769.80it/s]  7%|▋         | 8151/112584 [00:01<00:18, 5656.46it/s]  8%|▊         | 8721/112584 [00:01<00:18, 5586.97it/s]  8%|▊         | 9358/112584 [00:01<00:17, 5804.76it/s]  9%|▉         | 9958/112584 [00:01<00:17, 5848.27it/s]  9%|▉         | 10545/112584 [00:01<00:17, 5697.80it/s] 10%|▉         | 11139/112584 [00:01<00:17, 5766.13it/s] 10%|█         | 11718/112584 [00:02<00:17, 5616.58it/s] 11%|█         | 12323/112584 [00:02<00:17, 5726.08it/s] 11%|█▏        | 12910/112584 [00:02<00:17, 5767.87it/s] 12%|█▏        | 13577/112584 [00:02<00:16, 6032.21it/s] 13%|█▎        | 14182/112584 [00:02<00:16, 5898.72it/s] 13%|█▎        | 14779/112584 [00:02<00:16, 5914.52it/s] 14%|█▎        | 15372/112584 [00:02<00:16, 5903.74it/s] 14%|█▍        | 15964/112584 [00:02<00:16, 5692.62it/s] 15%|█▍        | 16536/112584 [00:02<00:17, 5643.23it/s] 15%|█▌        | 17109/112584 [00:02<00:16, 5662.70it/s] 16%|█▌        | 17677/112584 [00:03<00:16, 5613.88it/s] 16%|█▌        | 18269/112584 [00:03<00:16, 5698.93it/s] 17%|█▋        | 18926/112584 [00:03<00:15, 5954.18it/s] 17%|█▋        | 19606/112584 [00:03<00:14, 6204.56it/s] 18%|█▊        | 20228/112584 [00:03<00:15, 5855.46it/s] 18%|█▊        | 20819/112584 [00:03<00:15, 5842.02it/s] 19%|█▉        | 21407/112584 [00:03<00:16, 5556.53it/s] 20%|█▉        | 22027/112584 [00:03<00:15, 5734.17it/s] 20%|██        | 22628/112584 [00:03<00:15, 5813.00it/s] 21%|██        | 23238/112584 [00:03<00:15, 5893.14it/s] 21%|██▏       | 23967/112584 [00:04<00:14, 6300.25it/s] 22%|██▏       | 24700/112584 [00:04<00:13, 6601.91it/s] 23%|██▎       | 25363/112584 [00:04<00:13, 6506.93it/s] 23%|██▎       | 26016/112584 [00:04<00:14, 6082.45it/s] 24%|██▎       | 26631/112584 [00:04<00:14, 5849.78it/s] 24%|██▍       | 27222/112584 [00:04<00:15, 5541.70it/s] 25%|██▍       | 27782/112584 [00:04<00:15, 5555.45it/s] 25%|██▌       | 28434/112584 [00:04<00:14, 5824.92it/s] 26%|██▌       | 29083/112584 [00:04<00:13, 6007.46it/s] 26%|██▋       | 29688/112584 [00:05<00:14, 5816.42it/s] 27%|██▋       | 30274/112584 [00:05<00:14, 5807.42it/s] 27%|██▋       | 30858/112584 [00:05<00:14, 5458.92it/s] 28%|██▊       | 31439/112584 [00:05<00:14, 5555.86it/s] 28%|██▊       | 31999/112584 [00:05<00:14, 5515.34it/s] 29%|██▉       | 32557/112584 [00:05<00:14, 5532.62it/s] 29%|██▉       | 33113/112584 [00:05<00:14, 5341.09it/s] 30%|██▉       | 33677/112584 [00:05<00:14, 5426.16it/s] 30%|███       | 34227/112584 [00:05<00:14, 5446.82it/s] 31%|███       | 34922/112584 [00:06<00:13, 5881.07it/s] 32%|███▏      | 35513/112584 [00:06<00:13, 5614.32it/s] 32%|███▏      | 36095/112584 [00:06<00:13, 5670.94it/s] 33%|███▎      | 36665/112584 [00:06<00:13, 5654.64it/s] 33%|███▎      | 37233/112584 [00:06<00:13, 5476.96it/s] 34%|███▎      | 37783/112584 [00:06<00:13, 5418.49it/s] 34%|███▍      | 38327/112584 [00:06<00:13, 5391.88it/s] 35%|███▍      | 38921/112584 [00:06<00:13, 5549.15it/s] 35%|███▌      | 39478/112584 [00:06<00:13, 5411.10it/s] 36%|███▌      | 40114/112584 [00:06<00:12, 5677.15it/s] 36%|███▌      | 40775/112584 [00:07<00:12, 5949.45it/s] 37%|███▋      | 41372/112584 [00:07<00:12, 5833.44it/s] 37%|███▋      | 41958/112584 [00:07<00:12, 5499.92it/s] 38%|███▊      | 42513/112584 [00:07<00:12, 5390.68it/s] 38%|███▊      | 43081/112584 [00:07<00:12, 5465.14it/s] 39%|███▉      | 43681/112584 [00:07<00:12, 5616.05it/s] 39%|███▉      | 44246/112584 [00:07<00:12, 5583.92it/s] 40%|███▉      | 44887/112584 [00:07<00:11, 5822.39it/s] 40%|████      | 45472/112584 [00:07<00:11, 5828.25it/s] 41%|████      | 46106/112584 [00:08<00:11, 5975.49it/s] 41%|████▏     | 46718/112584 [00:08<00:10, 6012.51it/s] 42%|████▏     | 47640/112584 [00:08<00:09, 6964.81it/s] 43%|████▎     | 48338/112584 [00:08<00:09, 6871.75it/s] 44%|████▎     | 49027/112584 [00:08<00:09, 6720.70it/s] 44%|████▍     | 49701/112584 [00:08<00:09, 6611.25it/s] 45%|████▍     | 50364/112584 [00:08<00:10, 6111.97it/s] 45%|████▌     | 50983/112584 [00:08<00:10, 5961.32it/s] 46%|████▌     | 51585/112584 [00:08<00:10, 5892.44it/s] 47%|████▋     | 52390/112584 [00:08<00:09, 6492.61it/s] 47%|████▋     | 53046/112584 [00:09<00:09, 6317.02it/s] 48%|████▊     | 53683/112584 [00:09<00:09, 6143.40it/s] 48%|████▊     | 54301/112584 [00:09<00:10, 5793.33it/s] 49%|████▉     | 54886/112584 [00:09<00:10, 5750.67it/s] 49%|████▉     | 55478/112584 [00:09<00:09, 5791.96it/s] 50%|████▉     | 56155/112584 [00:09<00:09, 6064.70it/s] 50%|█████     | 56765/112584 [00:09<00:09, 5771.70it/s] 51%|█████     | 57347/112584 [00:09<00:10, 5455.71it/s] 51%|█████▏    | 57949/112584 [00:09<00:09, 5606.40it/s] 52%|█████▏    | 58701/112584 [00:10<00:08, 6132.60it/s] 53%|█████▎    | 59321/112584 [00:10<00:09, 5828.25it/s] 53%|█████▎    | 59911/112584 [00:10<00:09, 5703.27it/s] 54%|█████▎    | 60487/112584 [00:10<00:09, 5704.39it/s] 54%|█████▍    | 61250/112584 [00:10<00:08, 6246.97it/s] 55%|█████▍    | 61880/112584 [00:10<00:08, 5937.05it/s] 56%|█████▌    | 62495/112584 [00:10<00:08, 5992.19it/s] 56%|█████▌    | 63099/112584 [00:10<00:08, 5759.29it/s] 57%|█████▋    | 63709/112584 [00:10<00:08, 5854.38it/s] 57%|█████▋    | 64299/112584 [00:11<00:08, 5656.61it/s] 58%|█████▊    | 64916/112584 [00:11<00:08, 5801.81it/s] 58%|█████▊    | 65500/112584 [00:11<00:08, 5569.18it/s] 59%|█████▊    | 66116/112584 [00:11<00:08, 5730.75it/s] 59%|█████▉    | 66833/112584 [00:11<00:07, 6143.66it/s] 60%|█████▉    | 67452/112584 [00:11<00:07, 5740.80it/s] 60%|██████    | 68034/112584 [00:11<00:08, 5509.58it/s] 61%|██████    | 68737/112584 [00:11<00:07, 5928.98it/s] 62%|██████▏   | 69338/112584 [00:11<00:07, 5829.16it/s] 62%|██████▏   | 70026/112584 [00:11<00:06, 6125.24it/s] 63%|██████▎   | 70645/112584 [00:12<00:07, 5864.44it/s] 63%|██████▎   | 71237/112584 [00:12<00:07, 5845.93it/s] 64%|██████▍   | 71826/112584 [00:12<00:07, 5790.65it/s] 64%|██████▍   | 72438/112584 [00:12<00:06, 5880.20it/s] 65%|██████▍   | 73175/112584 [00:12<00:06, 6313.72it/s] 66%|██████▌   | 73844/112584 [00:12<00:06, 6423.92it/s] 66%|██████▋   | 74589/112584 [00:12<00:05, 6716.89it/s] 67%|██████▋   | 75263/112584 [00:12<00:05, 6417.87it/s] 67%|██████▋   | 75909/112584 [00:12<00:05, 6265.41it/s] 68%|██████▊   | 76539/112584 [00:13<00:05, 6265.35it/s] 69%|██████▊   | 77168/112584 [00:13<00:05, 5986.65it/s] 69%|██████▉   | 77826/112584 [00:13<00:05, 6147.43it/s] 70%|██████▉   | 78445/112584 [00:13<00:05, 5890.99it/s] 70%|███████   | 79038/112584 [00:13<00:06, 5561.30it/s] 71%|███████   | 79600/112584 [00:13<00:06, 5461.63it/s] 71%|███████   | 80150/112584 [00:13<00:06, 5393.42it/s] 72%|███████▏  | 80782/112584 [00:13<00:05, 5651.56it/s] 72%|███████▏  | 81430/112584 [00:13<00:05, 5886.22it/s] 73%|███████▎  | 82022/112584 [00:14<00:05, 5699.65it/s] 73%|███████▎  | 82635/112584 [00:14<00:05, 5810.91it/s] 74%|███████▍  | 83306/112584 [00:14<00:04, 6071.29it/s] 75%|███████▍  | 83916/112584 [00:14<00:04, 6059.25it/s] 75%|███████▌  | 84524/112584 [00:14<00:04, 5845.01it/s] 76%|███████▌  | 85112/112584 [00:14<00:04, 5816.41it/s] 76%|███████▌  | 85716/112584 [00:14<00:04, 5879.69it/s] 77%|███████▋  | 86306/112584 [00:14<00:04, 5854.51it/s] 77%|███████▋  | 86981/112584 [00:14<00:04, 6112.18it/s] 78%|███████▊  | 87610/112584 [00:14<00:04, 6160.73it/s] 78%|███████▊  | 88239/112584 [00:15<00:03, 6194.59it/s] 79%|███████▉  | 88860/112584 [00:15<00:04, 5648.43it/s] 79%|███████▉  | 89435/112584 [00:15<00:04, 5567.02it/s] 80%|███████▉  | 89999/112584 [00:15<00:04, 5495.83it/s] 80%|████████  | 90561/112584 [00:15<00:03, 5524.84it/s] 81%|████████  | 91129/112584 [00:15<00:03, 5563.40it/s] 82%|████████▏ | 91771/112584 [00:15<00:03, 5810.84it/s] 82%|████████▏ | 92432/112584 [00:15<00:03, 6042.50it/s] 83%|████████▎ | 93039/112584 [00:15<00:03, 5847.92it/s] 83%|████████▎ | 93632/112584 [00:16<00:03, 5857.95it/s] 84%|████████▎ | 94220/112584 [00:16<00:03, 5700.53it/s] 84%|████████▍ | 94907/112584 [00:16<00:02, 6035.94it/s] 85%|████████▍ | 95514/112584 [00:16<00:02, 5834.85it/s] 85%|████████▌ | 96107/112584 [00:16<00:02, 5845.33it/s] 86%|████████▌ | 96925/112584 [00:16<00:02, 6518.25it/s] 87%|████████▋ | 97581/112584 [00:16<00:02, 6057.09it/s] 87%|████████▋ | 98207/112584 [00:16<00:02, 6113.24it/s] 88%|████████▊ | 98825/112584 [00:16<00:02, 5874.12it/s] 88%|████████▊ | 99419/112584 [00:16<00:02, 5660.09it/s] 89%|████████▉ | 100007/112584 [00:17<00:02, 5716.08it/s] 89%|████████▉ | 100583/112584 [00:17<00:02, 5492.69it/s] 90%|████████▉ | 101229/112584 [00:17<00:01, 5755.05it/s] 90%|█████████ | 101809/112584 [00:17<00:01, 5739.45it/s] 91%|█████████ | 102403/112584 [00:17<00:01, 5788.31it/s] 91%|█████████▏| 103014/112584 [00:17<00:01, 5876.96it/s] 92%|█████████▏| 103604/112584 [00:17<00:01, 5745.31it/s] 93%|█████████▎| 104181/112584 [00:17<00:01, 5598.12it/s] 93%|█████████▎| 104806/112584 [00:17<00:01, 5782.50it/s] 94%|█████████▎| 105387/112584 [00:18<00:01, 5717.61it/s] 94%|█████████▍| 105998/112584 [00:18<00:01, 5824.03it/s] 95%|█████████▍| 106582/112584 [00:18<00:01, 5754.74it/s] 95%|█████████▌| 107159/112584 [00:18<00:00, 5572.04it/s] 96%|█████████▌| 107759/112584 [00:18<00:00, 5690.18it/s] 96%|█████████▌| 108330/112584 [00:18<00:00, 5673.51it/s] 97%|█████████▋| 108899/112584 [00:18<00:00, 5443.17it/s] 97%|█████████▋| 109461/112584 [00:18<00:00, 5488.55it/s] 98%|█████████▊| 110027/112584 [00:18<00:00, 5535.40it/s] 98%|█████████▊| 110656/112584 [00:18<00:00, 5753.57it/s] 99%|█████████▉| 111299/112584 [00:19<00:00, 5952.51it/s] 99%|█████████▉| 111896/112584 [00:19<00:00, 5841.13it/s]100%|█████████▉| 112534/112584 [00:19<00:00, 5998.03it/s]100%|██████████| 112584/112584 [00:19<00:00, 5840.74it/s]

gathering stats for n=1
  0%|          | 0/112584 [00:00<?, ?it/s]  2%|▏         | 1859/112584 [00:00<00:05, 18585.54it/s]  3%|▎         | 3844/112584 [00:00<00:05, 19328.33it/s]  5%|▌         | 6063/112584 [00:00<00:05, 20627.50it/s]  7%|▋         | 8126/112584 [00:00<00:05, 19545.71it/s]  9%|▉         | 10089/112584 [00:00<00:05, 19566.15it/s] 11%|█         | 12052/112584 [00:00<00:05, 19582.24it/s] 12%|█▏        | 14014/112584 [00:00<00:05, 19561.63it/s] 14%|█▍        | 15973/112584 [00:00<00:04, 19455.77it/s] 16%|█▌        | 17959/112584 [00:00<00:04, 19580.28it/s] 18%|█▊        | 20038/112584 [00:01<00:04, 19949.63it/s] 20%|█▉        | 22035/112584 [00:01<00:04, 19555.71it/s] 21%|██▏       | 24204/112584 [00:01<00:04, 20190.18it/s] 23%|██▎       | 26255/112584 [00:01<00:04, 20283.80it/s] 25%|██▌       | 28286/112584 [00:01<00:04, 19792.90it/s] 27%|██▋       | 30277/112584 [00:01<00:04, 19823.60it/s] 29%|██▊       | 32262/112584 [00:01<00:04, 19198.45it/s] 30%|███       | 34188/112584 [00:01<00:04, 19071.97it/s] 32%|███▏      | 36192/112584 [00:01<00:03, 19353.13it/s] 34%|███▍      | 38131/112584 [00:01<00:03, 18970.31it/s] 36%|███▌      | 40060/112584 [00:02<00:03, 19061.28it/s] 37%|███▋      | 41971/112584 [00:02<00:03, 19072.28it/s] 39%|███▉      | 43881/112584 [00:02<00:03, 19080.23it/s] 41%|████      | 45909/112584 [00:02<00:03, 19434.76it/s] 43%|████▎     | 48325/112584 [00:02<00:03, 20839.44it/s] 45%|████▍     | 50411/112584 [00:02<00:03, 20510.84it/s] 47%|████▋     | 52586/112584 [00:02<00:02, 20869.29it/s] 49%|████▊     | 54676/112584 [00:02<00:02, 20125.24it/s] 50%|█████     | 56722/112584 [00:02<00:02, 20212.83it/s] 52%|█████▏    | 58749/112584 [00:02<00:02, 20176.36it/s] 54%|█████▍    | 60771/112584 [00:03<00:02, 19847.01it/s] 56%|█████▌    | 62808/112584 [00:03<00:02, 19997.49it/s] 58%|█████▊    | 64811/112584 [00:03<00:02, 19844.93it/s] 59%|█████▉    | 66875/112584 [00:03<00:02, 20074.05it/s] 61%|██████    | 68885/112584 [00:03<00:02, 19840.28it/s] 63%|██████▎   | 70871/112584 [00:03<00:02, 19625.27it/s] 65%|██████▍   | 73015/112584 [00:03<00:01, 20153.73it/s] 67%|██████▋   | 75237/112584 [00:03<00:01, 20757.67it/s] 69%|██████▊   | 77315/112584 [00:03<00:01, 20463.68it/s] 70%|███████   | 79364/112584 [00:04<00:01, 19649.51it/s] 72%|███████▏  | 81397/112584 [00:04<00:01, 19844.44it/s] 74%|███████▍  | 83423/112584 [00:04<00:01, 19961.46it/s] 76%|███████▌  | 85424/112584 [00:04<00:01, 19878.43it/s] 78%|███████▊  | 87517/112584 [00:04<00:01, 20187.65it/s] 80%|███████▉  | 89539/112584 [00:04<00:01, 19733.23it/s] 81%|████████▏ | 91517/112584 [00:04<00:01, 19624.57it/s] 83%|████████▎ | 93512/112584 [00:04<00:00, 19718.79it/s] 85%|████████▍ | 95498/112584 [00:04<00:00, 19751.17it/s] 87%|████████▋ | 97616/112584 [00:04<00:00, 20173.17it/s] 88%|████████▊ | 99635/112584 [00:05<00:00, 19889.36it/s] 90%|█████████ | 101626/112584 [00:05<00:00, 19743.34it/s] 92%|█████████▏| 103602/112584 [00:05<00:00, 19663.51it/s] 94%|█████████▍| 105581/112584 [00:05<00:00, 19693.84it/s] 96%|█████████▌| 107552/112584 [00:05<00:00, 19384.65it/s] 97%|█████████▋| 109492/112584 [00:05<00:00, 19113.51it/s] 99%|█████████▉| 111545/112584 [00:05<00:00, 19523.93it/s]100%|██████████| 112584/112584 [00:05<00:00, 19779.58it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 609.99it/s]2022-03-23 19:49:32 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39136, bias=False)
  )
)
2022-03-23 19:49:32 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-23 19:49:32 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-23 19:49:32 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-23 19:49:32 | INFO | fairseq_cli.train | num. shared model params: 38,951,936 (num. trained: 38,951,936)
2022-03-23 19:49:32 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 19:49:32 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/valid
2022-03-23 19:49:32 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 19:49:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 19:49:32 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 19:49:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 19:49:32 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 19:49:32 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-23 19:49:32 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.02_0.08_0.9_#1/checkpoint_last.pt
2022-03-23 19:49:32 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.02_0.08_0.9_#1/checkpoint_last.pt
2022-03-23 19:49:32 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 19:49:32 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
2022-03-23 19:49:32 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 19:49:32 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-23 19:49:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-23 19:49:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 19:49:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 19:49:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 103, in forward
    loss, _ = self.compute_loss(model, net_output, sample, reduce=reduce)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 126, in compute_loss
    self.KL_div_uniform += KL_div_uniform.item()
KeyboardInterrupt
