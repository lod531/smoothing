Sender: LSF System <lsfadmin@eu-g3-061>
Subject: Job 208120481: <iwslt14_de_en_dropout_0.7> in cluster <euler> Done

Job <iwslt14_de_en_dropout_0.7> was submitted from host <eu-login-20> by user <andriusb> in cluster <euler> at Mon Mar 14 09:13:13 2022
Job was executed on host(s) <eu-g3-061>, in queue <gpuhe.4h>, as user <andriusb> in cluster <euler> at Mon Mar 14 09:13:33 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Mon Mar 14 09:13:33 2022
Terminated at Mon Mar 14 09:42:19 2022
Results reported at Mon Mar 14 09:42:19 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en --save-dir /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7 --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas "(0.9, 0.98)" --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.7 --weight-decay 0.0001 --criterion cross_entropy --max-tokens 32768 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --fp16 --no-epoch-checkpoints --patience 3 --seed 66575611 --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1717.40 sec.
    Max Memory :                                 4150 MB
    Average Memory :                             3149.57 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15850.00 MB
    Max Swap :                                   65 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   1726 sec.
    Turnaround time :                            1746 sec.

The output (if any) follows:

2022-03-14 09:13:41 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 32768, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 32768, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.7, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=32768, max_tokens_valid=32768, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7', save_interval=1, save_interval_updates=0, scoring='bleu', seed=66575611, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-14 09:13:42 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2022-03-14 09:13:42 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2022-03-14 09:13:42 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2022-03-14 09:13:42 | INFO | fairseq_cli.train | task: TranslationTask
2022-03-14 09:13:42 | INFO | fairseq_cli.train | model: TransformerModel
2022-03-14 09:13:42 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-14 09:13:42 | INFO | fairseq_cli.train | num. shared model params: 39,469,056 (num. trained: 39,469,056)
2022-03-14 09:13:42 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-14 09:13:42 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-03-14 09:13:42 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-03-14 09:13:42 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2022-03-14 09:13:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-14 09:13:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-14 09:13:46 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-14 09:13:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-14 09:13:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-14 09:13:46 | INFO | fairseq_cli.train | max tokens per device = 32768 and max sentences per device = None
2022-03-14 09:13:46 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:13:46 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:13:46 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-14 09:13:46 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-03-14 09:13:46 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-03-14 09:13:46 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2022-03-14 09:13:47 | INFO | fairseq.trainer | begin training epoch 1
2022-03-14 09:13:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:13:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-14 09:13:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-14 09:13:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-14 09:14:22 | INFO | train_inner | epoch 001:    103 / 157 loss=12.32, ppl=5112.11, wps=80663.6, ups=3.21, wpb=25148.6, bsz=969, num_updates=100, lr=1.25e-05, gnorm=2.451, loss_scale=16, train_wall=34, gb_free=15, wall=35
2022-03-14 09:14:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-14 09:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:14:41 | INFO | fairseq.tasks.translation | example hypothesis: ....
2022-03-14 09:14:41 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:14:44 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-14 09:14:44 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:14:46 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-14 09:14:46 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:14:48 | INFO | fairseq.tasks.translation | example hypothesis: ....
2022-03-14 09:14:48 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:14:50 | INFO | fairseq.tasks.translation | example hypothesis: ....
2022-03-14 09:14:50 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:14:52 | INFO | fairseq.tasks.translation | example hypothesis: ....
2022-03-14 09:14:52 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:14:54 | INFO | fairseq.tasks.translation | example hypothesis: ....
2022-03-14 09:14:54 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:14:56 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-14 09:14:56 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:14:57 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-14 09:14:57 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:14:57 | INFO | fairseq.tasks.translation | example hypothesis: ...
2022-03-14 09:14:57 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:14:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.417 | ppl 1367.3 | bleu 0 | wps 10091.3 | wpb 17862.2 | bsz 728.3 | num_updates 153
2022-03-14 09:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 153 updates
2022-03-14 09:14:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 1 @ 153 updates, score 0.0) (writing took 2.18412577197887 seconds)
2022-03-14 09:15:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-14 09:15:00 | INFO | train | epoch 001 | loss 11.83 | ppl 3641.84 | wps 55357.8 | ups 2.2 | wpb 25152.3 | bsz 980.9 | num_updates 153 | lr 1.9125e-05 | gnorm 2.117 | loss_scale 8 | train_wall 50 | gb_free 22.4 | wall 73
2022-03-14 09:15:00 | INFO | fairseq.trainer | begin training epoch 2
2022-03-14 09:15:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:15:15 | INFO | train_inner | epoch 002:     47 / 157 loss=10.621, ppl=1574.77, wps=47673.7, ups=1.87, wpb=25442.4, bsz=1078.6, num_updates=200, lr=2.5e-05, gnorm=1.403, loss_scale=8, train_wall=30, gb_free=15.1, wall=89
2022-03-14 09:15:46 | INFO | train_inner | epoch 002:    147 / 157 loss=9.835, ppl=913.3, wps=80638.5, ups=3.2, wpb=25185, bsz=961.8, num_updates=300, lr=3.75e-05, gnorm=0.948, loss_scale=8, train_wall=31, gb_free=14.5, wall=120
2022-03-14 09:15:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:15:52 | INFO | fairseq.tasks.translation | example hypothesis: ..
2022-03-14 09:15:52 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:15:54 | INFO | fairseq.tasks.translation | example hypothesis: .
2022-03-14 09:15:54 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:15:57 | INFO | fairseq.tasks.translation | example hypothesis: .
2022-03-14 09:15:57 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:16:01 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:16:01 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:16:06 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:16:06 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:16:11 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:16:11 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:16:16 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:16:16 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:16:22 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:16:22 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:16:29 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:16:29 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:16:31 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:16:31 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:16:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.492 | ppl 720.08 | bleu 0 | wps 4146 | wpb 17862.2 | bsz 728.3 | num_updates 310 | best_bleu 0
2022-03-14 09:16:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 310 updates
2022-03-14 09:16:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:16:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 2 @ 310 updates, score 0.0) (writing took 2.2422035220079124 seconds)
2022-03-14 09:16:33 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-14 09:16:33 | INFO | train | epoch 002 | loss 9.964 | ppl 998.85 | wps 42097.5 | ups 1.67 | wpb 25153.6 | bsz 1020.6 | num_updates 310 | lr 3.875e-05 | gnorm 1.046 | loss_scale 8 | train_wall 48 | gb_free 14.3 | wall 167
2022-03-14 09:16:34 | INFO | fairseq.trainer | begin training epoch 3
2022-03-14 09:16:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:17:02 | INFO | train_inner | epoch 003:     90 / 157 loss=9.504, ppl=725.91, wps=32448.2, ups=1.32, wpb=24585.2, bsz=969, num_updates=400, lr=5e-05, gnorm=0.903, loss_scale=8, train_wall=30, gb_free=14.1, wall=196
2022-03-14 09:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:17:26 | INFO | fairseq.tasks.translation | example hypothesis: we we we we we.
2022-03-14 09:17:26 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:17:29 | INFO | fairseq.tasks.translation | example hypothesis: the the the the the the.
2022-03-14 09:17:29 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:17:33 | INFO | fairseq.tasks.translation | example hypothesis: the the the the the the the the the the the the the the the the the the the the the the.
2022-03-14 09:17:33 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:17:38 | INFO | fairseq.tasks.translation | example hypothesis: and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:17:38 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:17:43 | INFO | fairseq.tasks.translation | example hypothesis: and we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we
2022-03-14 09:17:43 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:17:49 | INFO | fairseq.tasks.translation | example hypothesis: and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2022-03-14 09:17:49 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:17:54 | INFO | fairseq.tasks.translation | example hypothesis: and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:17:54 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:18:00 | INFO | fairseq.tasks.translation | example hypothesis: and we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we
2022-03-14 09:18:00 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:18:08 | INFO | fairseq.tasks.translation | example hypothesis: and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:08 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:18:10 | INFO | fairseq.tasks.translation | example hypothesis: and we we,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:18:10 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:18:10 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.409 | ppl 679.97 | bleu 0.01 | wps 3697.2 | wpb 17862.2 | bsz 728.3 | num_updates 467 | best_bleu 0.01
2022-03-14 09:18:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 467 updates
2022-03-14 09:18:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:18:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 3 @ 467 updates, score 0.01) (writing took 2.1653858160134405 seconds)
2022-03-14 09:18:12 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-14 09:18:12 | INFO | train | epoch 003 | loss 9.399 | ppl 675.24 | wps 40005.7 | ups 1.59 | wpb 25153.6 | bsz 1020.6 | num_updates 467 | lr 5.8375e-05 | gnorm 1.059 | loss_scale 8 | train_wall 48 | gb_free 14.1 | wall 266
2022-03-14 09:18:13 | INFO | fairseq.trainer | begin training epoch 4
2022-03-14 09:18:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:18:23 | INFO | train_inner | epoch 004:     33 / 157 loss=9.23, ppl=600.63, wps=31282.3, ups=1.23, wpb=25454.8, bsz=1088.2, num_updates=500, lr=6.25e-05, gnorm=1.184, loss_scale=8, train_wall=31, gb_free=14.3, wall=277
2022-03-14 09:18:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-14 09:18:55 | INFO | train_inner | epoch 004:    134 / 157 loss=9.111, ppl=552.79, wps=80392.2, ups=3.18, wpb=25304.6, bsz=1029.8, num_updates=600, lr=7.5e-05, gnorm=1.065, loss_scale=4, train_wall=31, gb_free=14.7, wall=308
2022-03-14 09:19:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:19:05 | INFO | fairseq.tasks.translation | example hypothesis: we we we we we we.
2022-03-14 09:19:05 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:19:08 | INFO | fairseq.tasks.translation | example hypothesis: the the the the the the.
2022-03-14 09:19:08 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:19:13 | INFO | fairseq.tasks.translation | example hypothesis: so the the the the the the the the the the the the the the the.
2022-03-14 09:19:13 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:19:18 | INFO | fairseq.tasks.translation | example hypothesis: and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:18 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:19:24 | INFO | fairseq.tasks.translation | example hypothesis: and we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we
2022-03-14 09:19:24 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:19:29 | INFO | fairseq.tasks.translation | example hypothesis: and and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2022-03-14 09:19:29 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:19:35 | INFO | fairseq.tasks.translation | example hypothesis: and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:35 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:19:41 | INFO | fairseq.tasks.translation | example hypothesis: and we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we
2022-03-14 09:19:41 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:19:48 | INFO | fairseq.tasks.translation | example hypothesis: and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:48 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:19:51 | INFO | fairseq.tasks.translation | example hypothesis: we,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-14 09:19:51 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:19:51 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.13 | ppl 560.34 | bleu 0.02 | wps 3573.1 | wpb 17862.2 | bsz 728.3 | num_updates 623 | best_bleu 0.02
2022-03-14 09:19:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 623 updates
2022-03-14 09:19:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:19:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 4 @ 623 updates, score 0.02) (writing took 2.1050994601100683 seconds)
2022-03-14 09:19:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-14 09:19:53 | INFO | train | epoch 004 | loss 9.115 | ppl 554.5 | wps 39070.7 | ups 1.55 | wpb 25170.8 | bsz 1025.9 | num_updates 623 | lr 7.7875e-05 | gnorm 1.048 | loss_scale 4 | train_wall 48 | gb_free 14.3 | wall 366
2022-03-14 09:19:53 | INFO | fairseq.trainer | begin training epoch 5
2022-03-14 09:19:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:20:17 | INFO | train_inner | epoch 005:     77 / 157 loss=9.063, ppl=534.94, wps=29690.3, ups=1.21, wpb=24464.6, bsz=968, num_updates=700, lr=8.75e-05, gnorm=1.173, loss_scale=4, train_wall=30, gb_free=15.5, wall=391
2022-03-14 09:20:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:20:46 | INFO | fairseq.tasks.translation | example hypothesis: we we we we we we we we we we.
2022-03-14 09:20:46 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:20:50 | INFO | fairseq.tasks.translation | example hypothesis: and the is is is is is is is is is is is is is is the the.
2022-03-14 09:20:50 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:20:55 | INFO | fairseq.tasks.translation | example hypothesis: and the the the the the the the the the the of the of the of of of of of of of of of of of of of of of of of of the of the
2022-03-14 09:20:55 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:21:01 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and
2022-03-14 09:21:01 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:21:06 | INFO | fairseq.tasks.translation | example hypothesis: and we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we
2022-03-14 09:21:06 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:21:12 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and the the the the the the the the the the the the the the the the the the the the the the the
2022-03-14 09:21:12 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:21:18 | INFO | fairseq.tasks.translation | example hypothesis: and you you, you, you, you, you you, you, they, they, they, they, they, they, they, they, they, you, they, they, they, they, they, they, you you, they, you you, they, you you, they, you you, they, you you, they,
2022-03-14 09:21:18 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:21:24 | INFO | fairseq.tasks.translation | example hypothesis: and we we we we we we we we we we we we we we, we we we we we we we we we we we we we we we we we we we we, we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we, we we we we we we we we we we we we we we we we we we we we we we
2022-03-14 09:21:24 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:21:32 | INFO | fairseq.tasks.translation | example hypothesis: "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""
2022-03-14 09:21:32 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:21:34 | INFO | fairseq.tasks.translation | example hypothesis: we we, we we we we we we we we, we we we, we we we we we we, we we we we we we we, we we we we we we we we we we, we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we, we we we we we we we we we we, we we we we we we we we we we we we we we we we we we we we we we we we we we, we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we, we we we we we we we we, we we we we we we we we we we we we we we we
2022-03-14 09:21:34 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:21:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.026 | ppl 521.47 | bleu 0.04 | wps 3398.1 | wpb 17862.2 | bsz 728.3 | num_updates 780 | best_bleu 0.04
2022-03-14 09:21:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 780 updates
2022-03-14 09:21:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:21:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:21:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 5 @ 780 updates, score 0.04) (writing took 2.2226824129465967 seconds)
2022-03-14 09:21:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-14 09:21:36 | INFO | train | epoch 005 | loss 8.975 | ppl 503.17 | wps 38198.2 | ups 1.52 | wpb 25153.6 | bsz 1020.6 | num_updates 780 | lr 9.75e-05 | gnorm 1.063 | loss_scale 4 | train_wall 48 | gb_free 14.4 | wall 470
2022-03-14 09:21:36 | INFO | fairseq.trainer | begin training epoch 6
2022-03-14 09:21:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:21:43 | INFO | train_inner | epoch 006:     20 / 157 loss=8.916, ppl=483.19, wps=29719.5, ups=1.17, wpb=25435.1, bsz=1018.2, num_updates=800, lr=0.0001, gnorm=0.965, loss_scale=4, train_wall=30, gb_free=13, wall=476
2022-03-14 09:22:14 | INFO | train_inner | epoch 006:    120 / 157 loss=8.796, ppl=444.55, wps=81078.3, ups=3.2, wpb=25302.4, bsz=1024.5, num_updates=900, lr=0.0001125, gnorm=1.042, loss_scale=4, train_wall=31, gb_free=14.5, wall=508
2022-03-14 09:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:22:31 | INFO | fairseq.tasks.translation | example hypothesis: we we we we we we we we we we we we we we we we we we we we we we we we the
2022-03-14 09:22:31 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:22:36 | INFO | fairseq.tasks.translation | example hypothesis: the is is is is is is is is is is is is is is is is is is is is is is is is is is the is the
2022-03-14 09:22:36 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:22:42 | INFO | fairseq.tasks.translation | example hypothesis: the the of the of the of the of the of the of the.
2022-03-14 09:22:42 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:22:47 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and it's's's's's's's's's's it's's's's's's's it it it it it it it it it it it it it it it it's a
2022-03-14 09:22:47 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:22:53 | INFO | fairseq.tasks.translation | example hypothesis: and and we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we's
2022-03-14 09:22:53 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:22:59 | INFO | fairseq.tasks.translation | example hypothesis: and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the, and the
2022-03-14 09:22:59 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:23:05 | INFO | fairseq.tasks.translation | example hypothesis: and if if if if if if you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you
2022-03-14 09:23:05 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:23:11 | INFO | fairseq.tasks.translation | example hypothesis: and and we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we the
2022-03-14 09:23:11 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:23:18 | INFO | fairseq.tasks.translation | example hypothesis: so "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "
2022-03-14 09:23:18 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:23:21 | INFO | fairseq.tasks.translation | example hypothesis: so we, if we, we, we, we we, if we, we we, we we we we, we we, if we, if we we we, we we we, we we, we we we we, we we we, we we we we, we we we we, we we we, if we, we we we we we we we, we we we we, we we we we we we we, we we we we we, if we we we we we, if we we we we, if we we we we we, if we we we we, if we we we we we we we, if we we we we we we we, if we we we we we, if we we we, if we we, if we we we we we, if we we we we we we we, if we we we we we we, if we we we we we we, if we we we we, if we, if we we we we we, if we we we we we we we we we,
2022-03-14 09:23:21 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:23:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.968 | ppl 500.78 | bleu 0.1 | wps 3256.9 | wpb 17862.2 | bsz 728.3 | num_updates 937 | best_bleu 0.1
2022-03-14 09:23:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 937 updates
2022-03-14 09:23:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:23:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 6 @ 937 updates, score 0.1) (writing took 2.19279761496 seconds)
2022-03-14 09:23:23 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-14 09:23:23 | INFO | train | epoch 006 | loss 8.777 | ppl 438.82 | wps 36905.9 | ups 1.47 | wpb 25153.6 | bsz 1020.6 | num_updates 937 | lr 0.000117125 | gnorm 1.003 | loss_scale 4 | train_wall 48 | gb_free 15.1 | wall 577
2022-03-14 09:23:23 | INFO | fairseq.trainer | begin training epoch 7
2022-03-14 09:23:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:23:43 | INFO | train_inner | epoch 007:     63 / 157 loss=8.624, ppl=394.44, wps=28209.8, ups=1.12, wpb=25148.3, bsz=1033.1, num_updates=1000, lr=0.000125, gnorm=0.903, loss_scale=4, train_wall=30, gb_free=15.3, wall=597
2022-03-14 09:24:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:24:18 | INFO | fairseq.tasks.translation | example hypothesis: and we're we're we're we're we're we've've've've've've've've've've've're the
2022-03-14 09:24:18 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:24:24 | INFO | fairseq.tasks.translation | example hypothesis: and the is the is the is the is the is the is the is the is the is the is the is the is the is the is the
2022-03-14 09:24:24 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:24:30 | INFO | fairseq.tasks.translation | example hypothesis: and we can're're the world, and we're're're are the world, and we're're are the world to're our the world, and we're our the world
2022-03-14 09:24:30 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:24:35 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and
2022-03-14 09:24:35 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:24:41 | INFO | fairseq.tasks.translation | example hypothesis: and and and it's it's that we's not't's not't're not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not it's not
2022-03-14 09:24:41 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:24:47 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and
2022-03-14 09:24:47 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:24:52 | INFO | fairseq.tasks.translation | example hypothesis: and you're't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't're
2022-03-14 09:24:52 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:24:58 | INFO | fairseq.tasks.translation | example hypothesis: and and we're we're to're to're to're to're to're to're to're to're to're to're to're our to're our our our our our our our our our our our our our our and and we're our and and we're our our our our our our our and we're our and and we're we're our our our our our our our our our our our our our our our our our our and and we're our
2022-03-14 09:24:58 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:25:06 | INFO | fairseq.tasks.translation | example hypothesis: and the, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "
2022-03-14 09:25:06 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:25:08 | INFO | fairseq.tasks.translation | example hypothesis: and if we can're to're to're to're to're to're to're to're to're to're to're to're to're to're to're a to're a to're to're to're to're to're to're to're to're to're to're to're to're to're to're to're to're a to're to're to're to're to're to're to're a a to're to're to're to're to're to're to're to're to're a a a a a to're a to're a to're a a a to're to're a a to're a to're to're a to're be a a a a a to're a to're to're a to're a to be to be to're a to be to be to be to be to be to're a to be to be to're to be to're be a a a a a a a a a a a a a a a a a a a a a a a a a a a a to be
2022-03-14 09:25:08 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:25:08 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.666 | ppl 406.13 | bleu 0.22 | wps 3259 | wpb 17862.2 | bsz 728.3 | num_updates 1094 | best_bleu 0.22
2022-03-14 09:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1094 updates
2022-03-14 09:25:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:25:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:25:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 7 @ 1094 updates, score 0.22) (writing took 2.2501977919600904 seconds)
2022-03-14 09:25:10 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-14 09:25:10 | INFO | train | epoch 007 | loss 8.542 | ppl 372.67 | wps 36776.9 | ups 1.46 | wpb 25153.6 | bsz 1020.6 | num_updates 1094 | lr 0.00013675 | gnorm 1.053 | loss_scale 4 | train_wall 48 | gb_free 14.9 | wall 684
2022-03-14 09:25:11 | INFO | fairseq.trainer | begin training epoch 8
2022-03-14 09:25:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:25:13 | INFO | train_inner | epoch 008:      6 / 157 loss=8.483, ppl=357.83, wps=27905.1, ups=1.12, wpb=25024, bsz=1033.8, num_updates=1100, lr=0.0001375, gnorm=1.111, loss_scale=4, train_wall=30, gb_free=14.8, wall=686
2022-03-14 09:25:44 | INFO | train_inner | epoch 008:    106 / 157 loss=8.228, ppl=299.79, wps=81524.9, ups=3.23, wpb=25229.1, bsz=1097.2, num_updates=1200, lr=0.00015, gnorm=0.991, loss_scale=4, train_wall=30, gb_free=15.1, wall=717
2022-03-14 09:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:26:05 | INFO | fairseq.tasks.translation | example hypothesis: we've're the world we've have the world in the world in the world in the world we're the world we're
2022-03-14 09:26:05 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:26:11 | INFO | fairseq.tasks.translation | example hypothesis: this is the world is the world is the world is the world is the world of the world is the world of the world.
2022-03-14 09:26:11 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:26:16 | INFO | fairseq.tasks.translation | example hypothesis: and we can're the same world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the
2022-03-14 09:26:16 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:26:22 | INFO | fairseq.tasks.translation | example hypothesis: and and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot
2022-03-14 09:26:22 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:26:28 | INFO | fairseq.tasks.translation | example hypothesis: and and it's not not not not not not not not not not not not not not not not not not not that we't't't't't't't't't't't't't't't't't't't't't't't't
2022-03-14 09:26:28 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:26:33 | INFO | fairseq.tasks.translation | example hypothesis: and and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world and the world
2022-03-14 09:26:33 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:26:39 | INFO | fairseq.tasks.translation | example hypothesis: and they't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't
2022-03-14 09:26:39 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:26:45 | INFO | fairseq.tasks.translation | example hypothesis: and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we're the world, and we can have
2022-03-14 09:26:45 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:26:53 | INFO | fairseq.tasks.translation | example hypothesis: and he said "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "
2022-03-14 09:26:53 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:26:55 | INFO | fairseq.tasks.translation | example hypothesis: and if if we think, if we're a lot, if we're a lot, if we're a lot, we can be a lot, we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we can be a lot, and we're be a lot, and we can be a lot, and we can be a lot, and we're be a lot, and we're be a lot, and we can be a lot, and we're be a lot, and we're be a lot, and we can be a lot, and we're be a lot, and we can be a lot, and we have
2022-03-14 09:26:55 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:26:55 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.068 | ppl 268.36 | bleu 0.58 | wps 3271.9 | wpb 17862.2 | bsz 728.3 | num_updates 1251 | best_bleu 0.58
2022-03-14 09:26:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1251 updates
2022-03-14 09:26:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:26:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:26:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 8 @ 1251 updates, score 0.58) (writing took 2.1595030028838664 seconds)
2022-03-14 09:26:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-14 09:26:57 | INFO | train | epoch 008 | loss 8.248 | ppl 304.05 | wps 37039.2 | ups 1.47 | wpb 25153.6 | bsz 1020.6 | num_updates 1251 | lr 0.000156375 | gnorm 0.979 | loss_scale 4 | train_wall 48 | gb_free 14 | wall 791
2022-03-14 09:26:57 | INFO | fairseq.trainer | begin training epoch 9
2022-03-14 09:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:27:13 | INFO | train_inner | epoch 009:     49 / 157 loss=8.158, ppl=285.7, wps=28717.7, ups=1.12, wpb=25665, bsz=991.6, num_updates=1300, lr=0.0001625, gnorm=1.044, loss_scale=4, train_wall=31, gb_free=15.3, wall=807
2022-03-14 09:27:44 | INFO | train_inner | epoch 009:    149 / 157 loss=7.965, ppl=249.94, wps=80764.8, ups=3.25, wpb=24819.9, bsz=982.3, num_updates=1400, lr=0.000175, gnorm=0.925, loss_scale=4, train_wall=30, gb_free=14.8, wall=837
2022-03-14 09:27:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:27:51 | INFO | fairseq.tasks.translation | example hypothesis: we've've have to have the world in the world.
2022-03-14 09:27:51 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:27:55 | INFO | fairseq.tasks.translation | example hypothesis: this is the world is the world is the world is the world.
2022-03-14 09:27:55 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:28:01 | INFO | fairseq.tasks.translation | example hypothesis: and we can have a lot of the world of the world of the world of the world of the world of the world.
2022-03-14 09:28:01 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:28:06 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot,
2022-03-14 09:28:06 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:28:12 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and and and we think, and we think, and we think, and we think, and we think, and we think, and we think, and we think, and we think, and we think, and we think
2022-03-14 09:28:12 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:28:18 | INFO | fairseq.tasks.translation | example hypothesis: and and this is the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world is the world of the world of the world, and the world
2022-03-14 09:28:18 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:28:23 | INFO | fairseq.tasks.translation | example hypothesis: and it's not a lot, and they can't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't't be and and they're the lot, and and they're the lot and they're
2022-03-14 09:28:23 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:28:29 | INFO | fairseq.tasks.translation | example hypothesis: and we can have to see to see to see a lot, and we can have a lot, and we can have a lot, and we can see to see a lot, and we can see to be a lot, and we can have a lot, and we can have a lot, and we can see to see to see to see to see to see to be a lot, and we can see to see to be a lot, and we can see to
2022-03-14 09:28:29 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:28:37 | INFO | fairseq.tasks.translation | example hypothesis: and i said "" "" "" "" "" "" "" "" i said, "" "" "" "" "" "we said," "" "" "" "" we said, "" "" "" "" "" "" he said, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "he said," "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""
2022-03-14 09:28:37 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:28:39 | INFO | fairseq.tasks.translation | example hypothesis: and if if if if we think, if we think, if we think, if we think, if we think, if we think, if we think, we can have to have to have to have to have to see to see to have to be to see to be to be to be to be to be to be to be to be to be to be a lot, and we can have to be a lot, and we can have to be to be a lot, and we can have to be a lot, and we can be a lot, and we can see to see to be a lot, and we can see to be a lot, and we can see to see to see to be a lot, and we can see to see to be a lot, and we can see to be a lot, and we can see to see to be a lot, and we can see to be a lot, and we can have to see to see to see to see to be a lot, and we can see to see to see to see to be
2022-03-14 09:28:39 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:28:39 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.814 | ppl 225.08 | bleu 0.72 | wps 3346.5 | wpb 17862.2 | bsz 728.3 | num_updates 1408 | best_bleu 0.72
2022-03-14 09:28:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1408 updates
2022-03-14 09:28:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:28:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 9 @ 1408 updates, score 0.72) (writing took 2.1910992399789393 seconds)
2022-03-14 09:28:42 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-14 09:28:42 | INFO | train | epoch 009 | loss 7.989 | ppl 254.1 | wps 37790.6 | ups 1.5 | wpb 25153.6 | bsz 1020.6 | num_updates 1408 | lr 0.000176 | gnorm 0.988 | loss_scale 4 | train_wall 48 | gb_free 15.1 | wall 895
2022-03-14 09:28:42 | INFO | fairseq.trainer | begin training epoch 10
2022-03-14 09:28:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:29:11 | INFO | train_inner | epoch 010:     92 / 157 loss=7.786, ppl=220.7, wps=28788.3, ups=1.15, wpb=25102.3, bsz=1000.6, num_updates=1500, lr=0.0001875, gnorm=0.858, loss_scale=4, train_wall=31, gb_free=14.7, wall=925
2022-03-14 09:29:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:29:35 | INFO | fairseq.tasks.translation | example hypothesis: we've have been been in the world of the world.
2022-03-14 09:29:35 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:29:40 | INFO | fairseq.tasks.translation | example hypothesis: this is the world of the world is the world of the world of the world.
2022-03-14 09:29:40 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:29:45 | INFO | fairseq.tasks.translation | example hypothesis: so we're have a lot of the world of the world.
2022-03-14 09:29:45 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:29:50 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot,
2022-03-14 09:29:50 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:29:56 | INFO | fairseq.tasks.translation | example hypothesis: and we don't't't know that we know, and we don't do it's not not not going to do it and we don't't do it and we're not not not not not not going to do it and we're not
2022-03-14 09:29:56 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:30:01 | INFO | fairseq.tasks.translation | example hypothesis: and and and and this is the world of the world and the world and and the world and the world and the world and the world and the world and the world and the world and the world and the world and the world and the world and the world and the world and the world and and
2022-03-14 09:30:01 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:30:07 | INFO | fairseq.tasks.translation | example hypothesis: and and it's not not not not not not not not not not not not and and and they're not not not not not not not not and and they're not not not not not not not not and they're going to be and they're going and they're going to be and they know and they're going and they're going and they can be
2022-03-14 09:30:07 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:30:13 | INFO | fairseq.tasks.translation | example hypothesis: and we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that, and we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see
2022-03-14 09:30:13 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:30:21 | INFO | fairseq.tasks.translation | example hypothesis: and we said, "we said," we said, "we said," we said, "we said," we said, "we said," we said, "we said," we said, "we said," we said, "we said," we said, "we said," we said, "we said," we said, "we said," we said, "" "we said," we said, "we said," "we said," "" we said, "" "" we said, "we said," we said, "we said," "" we said, "we said," we said, "we said," "" we said, "we said," "we said," we said, "we said," we said, "" "we said," "" we said
2022-03-14 09:30:21 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:30:24 | INFO | fairseq.tasks.translation | example hypothesis: and we think, we think that we think that we think that we can see that we think that we can see that we think that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see
2022-03-14 09:30:24 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:30:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.697 | ppl 207.48 | bleu 0.75 | wps 3367.3 | wpb 17862.2 | bsz 728.3 | num_updates 1565 | best_bleu 0.75
2022-03-14 09:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1565 updates
2022-03-14 09:30:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:30:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 10 @ 1565 updates, score 0.75) (writing took 2.213695944985375 seconds)
2022-03-14 09:30:26 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-14 09:30:26 | INFO | train | epoch 010 | loss 7.719 | ppl 210.69 | wps 37873.3 | ups 1.51 | wpb 25153.6 | bsz 1020.6 | num_updates 1565 | lr 0.000195625 | gnorm 0.882 | loss_scale 4 | train_wall 48 | gb_free 14.2 | wall 1000
2022-03-14 09:30:26 | INFO | fairseq.trainer | begin training epoch 11
2022-03-14 09:30:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:30:37 | INFO | train_inner | epoch 011:     35 / 157 loss=7.652, ppl=201.13, wps=28820.6, ups=1.16, wpb=24855.9, bsz=1006.2, num_updates=1600, lr=0.0002, gnorm=0.938, loss_scale=4, train_wall=30, gb_free=13.8, wall=1011
2022-03-14 09:31:08 | INFO | train_inner | epoch 011:    135 / 157 loss=7.503, ppl=181.39, wps=82191.5, ups=3.22, wpb=25548.4, bsz=1066.4, num_updates=1700, lr=0.0002125, gnorm=0.917, loss_scale=4, train_wall=31, gb_free=13.7, wall=1042
2022-03-14 09:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:31:20 | INFO | fairseq.tasks.translation | example hypothesis: we've see in the world in the world.
2022-03-14 09:31:20 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:31:25 | INFO | fairseq.tasks.translation | example hypothesis: and this is the first thing of the world of the world, the world, the world is the world of the world.
2022-03-14 09:31:25 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:31:30 | INFO | fairseq.tasks.translation | example hypothesis: so we can have able to have a lot percent of the brain.
2022-03-14 09:31:30 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:31:35 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot, and it's a lot,
2022-03-14 09:31:35 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:31:41 | INFO | fairseq.tasks.translation | example hypothesis: and we don't know that we don't have a lot of a lot, and we don't know that we don't have a lot of a lot of a lot of a lot, and we don't have a lot of a lot,
2022-03-14 09:31:41 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:31:47 | INFO | fairseq.tasks.translation | example hypothesis: and this is a lot of the world, and it's a lot of the world, and it's a lot of the world, and it's a lot of the world, and it's a lot of the world, and it's a lot of the world, and it's a lot
2022-03-14 09:31:47 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:31:53 | INFO | fairseq.tasks.translation | example hypothesis: and it's not not not not not a lot, but you can't see, but you can see, but you can see it, but you can see, but you can see it, but you can see, but you can see, but you can see it, but you can see, but you can see it, but you can see it,
2022-03-14 09:31:53 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:31:59 | INFO | fairseq.tasks.translation | example hypothesis: and if we can see a lot of the brain, and we can see that we can see a lot of the brain, and we can see, and we can see, and we can see, and we can see that we can see a lot of a lot of the brain, and we can see, and we can see, and we can see that we can see, and we can see, and we can see that we can see that we can see,
2022-03-14 09:31:59 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:32:07 | INFO | fairseq.tasks.translation | example hypothesis: and if if we said, "we said," "" "" "" "" if we said, "if we said," "" you can say, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" we said, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "if we said," "" "" "" "" "" "" "" ""
2022-03-14 09:32:07 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:32:09 | INFO | fairseq.tasks.translation | example hypothesis: and if if if if if if if if if if if if if if if if if if if if if we can see, if we can see, if we can be a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot, we can be a lot of a lot of a lot of a lot of a lot, we can
2022-03-14 09:32:09 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:32:09 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.447 | ppl 174.49 | bleu 0.89 | wps 3334.2 | wpb 17862.2 | bsz 728.3 | num_updates 1722 | best_bleu 0.89
2022-03-14 09:32:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1722 updates
2022-03-14 09:32:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:32:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:32:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 11 @ 1722 updates, score 0.89) (writing took 2.2913543588947505 seconds)
2022-03-14 09:32:11 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-14 09:32:11 | INFO | train | epoch 011 | loss 7.55 | ppl 187.36 | wps 37442.8 | ups 1.49 | wpb 25153.6 | bsz 1020.6 | num_updates 1722 | lr 0.00021525 | gnorm 0.934 | loss_scale 4 | train_wall 48 | gb_free 14.5 | wall 1105
2022-03-14 09:32:12 | INFO | fairseq.trainer | begin training epoch 12
2022-03-14 09:32:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:32:36 | INFO | train_inner | epoch 012:     78 / 157 loss=7.472, ppl=177.53, wps=28522.6, ups=1.14, wpb=24994.5, bsz=978.4, num_updates=1800, lr=0.000225, gnorm=0.887, loss_scale=4, train_wall=30, gb_free=14.4, wall=1130
2022-03-14 09:33:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:33:05 | INFO | fairseq.tasks.translation | example hypothesis: we went to see the world.
2022-03-14 09:33:05 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:33:09 | INFO | fairseq.tasks.translation | example hypothesis: and this is the first thing that is the first thing that is the first thing is the first thing is the world.
2022-03-14 09:33:09 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:33:14 | INFO | fairseq.tasks.translation | example hypothesis: so we're going to get a lot of the world of the world of the world of the world of the world of the world of the world.
2022-03-14 09:33:14 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:33:20 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot, and it's a lot, and it's, and it's a lot, and it's a lot, and it's, and it's a lot, and it's,
2022-03-14 09:33:20 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:33:26 | INFO | fairseq.tasks.translation | example hypothesis: and we don't know, and we don't know, we don't know it's not not not not not not not a lot of a lot of a lot of us.
2022-03-14 09:33:26 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:33:31 | INFO | fairseq.tasks.translation | example hypothesis: and this is a lot of the world, and it's a lot of the world and the world, and it's a lot of the world, and it's a lot of the world, and it's a lot of the world, and it's a lot of the world and the world
2022-03-14 09:33:31 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:33:37 | INFO | fairseq.tasks.translation | example hypothesis: and it's not not not not not not not a lot of the way, but it's not not not not, but it's not not not not, but it's not not not not not not not not not a lot of the same, and they can be not not not not not not not not not not not, but it, but it's
2022-03-14 09:33:37 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:33:43 | INFO | fairseq.tasks.translation | example hypothesis: and if we can see this, we can see that we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it, and we can see it,
2022-03-14 09:33:43 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:33:51 | INFO | fairseq.tasks.translation | example hypothesis: and he said, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""
2022-03-14 09:33:51 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:33:53 | INFO | fairseq.tasks.translation | example hypothesis: and if if we can see it, if we can be a lot of a lot of that we can see that we can see it, and we can see that we can see it, and we can be a lot of a lot of a lot of a lot of a lot of the way that we can see that we can see it, and we can see that we can see that we can see that we can see that we can be a lot of a lot of a lot of a lot of a lot of a lot of the way that we can see that we can see it, and we can see that we can see that we can see that we can see that we can see that we can see that we can see it, and we can see that we can see that we can see that we can see that we can see that we can see that we can see it, and we can see that we can see that we can see it, and we can see that we can see that we can see that we can see it, and we can
2022-03-14 09:33:53 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:33:53 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.386 | ppl 167.25 | bleu 0.94 | wps 3359.1 | wpb 17862.2 | bsz 728.3 | num_updates 1879 | best_bleu 0.94
2022-03-14 09:33:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1879 updates
2022-03-14 09:33:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:33:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 12 @ 1879 updates, score 0.94) (writing took 2.1031330339610577 seconds)
2022-03-14 09:33:55 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-14 09:33:55 | INFO | train | epoch 012 | loss 7.412 | ppl 170.36 | wps 37934.8 | ups 1.51 | wpb 25153.6 | bsz 1020.6 | num_updates 1879 | lr 0.000234875 | gnorm 0.876 | loss_scale 4 | train_wall 48 | gb_free 14.5 | wall 1209
2022-03-14 09:33:56 | INFO | fairseq.trainer | begin training epoch 13
2022-03-14 09:33:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:34:02 | INFO | train_inner | epoch 013:     21 / 157 loss=7.36, ppl=164.24, wps=29032.1, ups=1.16, wpb=25100.1, bsz=1056.5, num_updates=1900, lr=0.0002375, gnorm=0.901, loss_scale=4, train_wall=30, gb_free=14.3, wall=1216
2022-03-14 09:34:34 | INFO | train_inner | epoch 013:    121 / 157 loss=7.288, ppl=156.24, wps=81200.8, ups=3.21, wpb=25287.4, bsz=1028.2, num_updates=2000, lr=0.00025, gnorm=0.913, loss_scale=4, train_wall=31, gb_free=14, wall=1247
2022-03-14 09:34:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:34:48 | INFO | fairseq.tasks.translation | example hypothesis: we went to go in the world.
2022-03-14 09:34:48 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:34:53 | INFO | fairseq.tasks.translation | example hypothesis: this is the first thing of the world.
2022-03-14 09:34:53 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:34:57 | INFO | fairseq.tasks.translation | example hypothesis: these are a lot of the united percent.
2022-03-14 09:34:57 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:35:02 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot, and it's a lot of, and it's, and it's, and it's a lot of, and it's, and it's, and it's a lot of
2022-03-14 09:35:02 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:35:07 | INFO | fairseq.tasks.translation | example hypothesis: and we don't don't know that we're not not not not not not not not not not not going to do it.
2022-03-14 09:35:07 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:35:12 | INFO | fairseq.tasks.translation | example hypothesis: and this is a lot of the world, and it's a lot of the world, and it's in the world, and it's in the world, and it's in the world, and it's in the world, and it's in the world, and it's in the world
2022-03-14 09:35:12 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:35:18 | INFO | fairseq.tasks.translation | example hypothesis: and it's not not not not not not not not not not not not not not not no, but if you can't see, but you can't see, but you can't see, but you can't see, but you can't see, but you can't see, but you can't see, and you can't see, you can't
2022-03-14 09:35:18 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:35:24 | INFO | fairseq.tasks.translation | example hypothesis: and if we can see this, we can see it, and we can see it, and then we can see the brain, and we can see that we can see the brain, and then we can see the brain, and we can see that we can see that we can see that we can see that we can see, and we can see that we can see that we can see the brain, we can see that we can see, and we can see it
2022-03-14 09:35:24 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:35:32 | INFO | fairseq.tasks.translation | example hypothesis: and we said, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""
2022-03-14 09:35:32 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:35:34 | INFO | fairseq.tasks.translation | example hypothesis: it's a lot of the way that we can have to be a lot of the way that we can have to do that we can have to be a lot of the way that we can have to do that we can have to do that we can have to be to be a lot of the way that we can have to be that we can have to do that we can have to be to be a lot of the way that we can have to be a lot of the way that we can see that it, and we can see that we can have to be a lot of the way that we can have to be a way that we can have to be to be to be to be a lot of the way that we can have to be a lot of the way that we can have to do that we can have to be to be to be to be to be to be to be to be to be to be a way that we can have to be to be to be a way that we can be a little little little little bit that we can have to be
2022-03-14 09:35:34 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:35:34 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.176 | ppl 144.61 | bleu 1.21 | wps 3581.8 | wpb 17862.2 | bsz 728.3 | num_updates 2036 | best_bleu 1.21
2022-03-14 09:35:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2036 updates
2022-03-14 09:35:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:35:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:35:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 13 @ 2036 updates, score 1.21) (writing took 2.130044142017141 seconds)
2022-03-14 09:35:37 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-14 09:35:37 | INFO | train | epoch 013 | loss 7.286 | ppl 156.06 | wps 39043.2 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 2036 | lr 0.0002545 | gnorm 0.887 | loss_scale 4 | train_wall 48 | gb_free 13.9 | wall 1310
2022-03-14 09:35:37 | INFO | fairseq.trainer | begin training epoch 14
2022-03-14 09:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:35:57 | INFO | train_inner | epoch 014:     64 / 157 loss=7.219, ppl=149.01, wps=29929.8, ups=1.2, wpb=24965.5, bsz=985.9, num_updates=2100, lr=0.0002625, gnorm=0.769, loss_scale=4, train_wall=30, gb_free=14.4, wall=1331
2022-03-14 09:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:36:30 | INFO | fairseq.tasks.translation | example hypothesis: we're going to go back back back back back back back back back back back back back back back.
2022-03-14 09:36:30 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:36:35 | INFO | fairseq.tasks.translation | example hypothesis: this is the most thing that the most thing is the most thing that the most thing is the most thing.
2022-03-14 09:36:35 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:36:39 | INFO | fairseq.tasks.translation | example hypothesis: they're going to use these cells.
2022-03-14 09:36:39 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:36:44 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of water, and it's a lot of water.
2022-03-14 09:36:44 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:36:49 | INFO | fairseq.tasks.translation | example hypothesis: and we're not not not not not not not not not not not not going to do that we don't know, and we don't know that we don't have a lot of a lot of the way.
2022-03-14 09:36:49 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:36:54 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of people in the world, and it's a lot of the world, and it's a lot of the world.
2022-03-14 09:36:54 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:36:59 | INFO | fairseq.tasks.translation | example hypothesis: and they don't know, but you can't see it, and they're not not a lot of the water.
2022-03-14 09:36:59 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:37:04 | INFO | fairseq.tasks.translation | example hypothesis: and we can see a lot of the brain, and we can see a lot of the brain, and we can be a lot of the brain, and we can be a lot of the brain.
2022-03-14 09:37:04 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:37:09 | INFO | fairseq.tasks.translation | example hypothesis: and we said, "" we said, "" "" "" "" "" "" "" "" we said, "we said," "we said," "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "we said," we said, "we said," we said, "" "" "" "" "" "" "" "" "" "" "" "" "we said," we said, "" "" "" "" "" "" "" "" "" "" "" "" "" "we said," we said, "we said," we said, "" "" "" "" "" "" "" "" ""
2022-03-14 09:37:09 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:37:12 | INFO | fairseq.tasks.translation | example hypothesis: and we've got a lot of the way that we could have a lot of a lot of, and we could have a lot of a lot of, and we have a lot of the way that we have a lot of the way, and we have a lot of the end of the way that we have a lot of the world, and we have to be a lot of the way that we have a lot of the world, and we could have a lot of the end of the way that we could have to be a lot of the world.
2022-03-14 09:37:12 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:37:12 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.061 | ppl 133.55 | bleu 1.7 | wps 3943 | wpb 17862.2 | bsz 728.3 | num_updates 2193 | best_bleu 1.7
2022-03-14 09:37:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2193 updates
2022-03-14 09:37:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:37:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt
2022-03-14 09:37:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_best.pt (epoch 14 @ 2193 updates, score 1.7) (writing took 2.2101866940502077 seconds)
2022-03-14 09:37:14 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-14 09:37:14 | INFO | train | epoch 014 | loss 7.128 | ppl 139.84 | wps 40631 | ups 1.62 | wpb 25153.6 | bsz 1020.6 | num_updates 2193 | lr 0.000274125 | gnorm 0.807 | loss_scale 4 | train_wall 48 | gb_free 14.2 | wall 1407
2022-03-14 09:37:14 | INFO | fairseq.trainer | begin training epoch 15
2022-03-14 09:37:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:37:17 | INFO | train_inner | epoch 015:      7 / 157 loss=7.059, ppl=133.36, wps=31965.8, ups=1.25, wpb=25541.8, bsz=1065.6, num_updates=2200, lr=0.000275, gnorm=0.815, loss_scale=4, train_wall=30, gb_free=14.3, wall=1411
2022-03-14 09:37:48 | INFO | train_inner | epoch 015:    107 / 157 loss=6.974, ppl=125.75, wps=80938.2, ups=3.22, wpb=25146.5, bsz=1064.7, num_updates=2300, lr=0.0002875, gnorm=0.881, loss_scale=4, train_wall=31, gb_free=14.3, wall=1442
2022-03-14 09:38:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:38:08 | INFO | fairseq.tasks.translation | example hypothesis: we're going to go back to the world.
2022-03-14 09:38:08 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:38:12 | INFO | fairseq.tasks.translation | example hypothesis: this is the first thing.
2022-03-14 09:38:12 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:38:17 | INFO | fairseq.tasks.translation | example hypothesis: so we're going to use these cells.
2022-03-14 09:38:17 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:38:22 | INFO | fairseq.tasks.translation | example hypothesis: and there's a lot of, and there's a lot of, and there's a lot of, and there's a lot of, and there's a lot of, and there's a lot of,
2022-03-14 09:38:22 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:38:27 | INFO | fairseq.tasks.translation | example hypothesis: and we don't know, we don't know, we don't know, we don't know, and we don't don't know it.
2022-03-14 09:38:27 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:38:33 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, in the world, in the world, in the world, and the world, in the world, in the world, and the world, and the world, and the world, and the world, and the world, and the world, and the world, in the world
2022-03-14 09:38:33 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:38:38 | INFO | fairseq.tasks.translation | example hypothesis: and if you can't't see, you can't't see, you can't't't see, you can't't't see, but you can't't't't't't't see, but you can't't't't see, but you can't't't see, but you can't't see, you can't't't't see,
2022-03-14 09:38:38 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:38:44 | INFO | fairseq.tasks.translation | example hypothesis: and if we can see, we can see, we can see, we can see, and we can see, and we can see that, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see, and we can see,
2022-03-14 09:38:44 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:38:52 | INFO | fairseq.tasks.translation | example hypothesis: and i said, "" "" "you know," you know, "" "" "" "" "" "" "" "" "oh," oh, "oh," oh, "you know," oh, "oh," oh, "oh," oh, "you know," oh, "you know," "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "
2022-03-14 09:38:52 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:38:54 | INFO | fairseq.tasks.translation | example hypothesis: and if we could have a lot of, if you have a lot of, if you have a lot of, we could have a lot of, if you could have a lot of, or we could have a lot of the way, or we could have a lot of the way, or we could have a lot of the way, or we could have to be a lot of the way, or we could have to be a lot of the way, or or or or or or we could have a lot of the way, or we could have to be a lot of the way, or if we could have to be a lot of the way, or we could have to be a lot of the way, or we could have to be a lot of the way, or we could have to be a lot of the way, or we could have to be, or we could have to be a lot of the way, or or or or or or we could have to be a lot of the way, or if we could have to be a lot of
2022-03-14 09:38:54 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:38:54 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.004 | ppl 128.35 | bleu 1.48 | wps 3516.9 | wpb 17862.2 | bsz 728.3 | num_updates 2350 | best_bleu 1.7
2022-03-14 09:38:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2350 updates
2022-03-14 09:38:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:38:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt (epoch 15 @ 2350 updates, score 1.48) (writing took 1.0776887401007116 seconds)
2022-03-14 09:38:55 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-14 09:38:55 | INFO | train | epoch 015 | loss 7.001 | ppl 128.07 | wps 38913.7 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 2350 | lr 0.00029375 | gnorm 0.836 | loss_scale 4 | train_wall 48 | gb_free 14.2 | wall 1509
2022-03-14 09:38:56 | INFO | fairseq.trainer | begin training epoch 16
2022-03-14 09:38:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:39:12 | INFO | train_inner | epoch 016:     50 / 157 loss=7.034, ppl=131.07, wps=30392.1, ups=1.2, wpb=25427.2, bsz=928.4, num_updates=2400, lr=0.0003, gnorm=0.763, loss_scale=4, train_wall=31, gb_free=14.7, wall=1525
2022-03-14 09:39:42 | INFO | train_inner | epoch 016:    150 / 157 loss=6.841, ppl=114.67, wps=80656.1, ups=3.27, wpb=24656.8, bsz=1032.6, num_updates=2500, lr=0.0003125, gnorm=0.806, loss_scale=4, train_wall=30, gb_free=14.9, wall=1556
2022-03-14 09:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:39:48 | INFO | fairseq.tasks.translation | example hypothesis: we were going to go back back in the world.
2022-03-14 09:39:48 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:39:53 | INFO | fairseq.tasks.translation | example hypothesis: this is the first news.
2022-03-14 09:39:53 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:39:57 | INFO | fairseq.tasks.translation | example hypothesis: so we're going to use these two cells.
2022-03-14 09:39:57 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:40:01 | INFO | fairseq.tasks.translation | example hypothesis: and there's a lot of water.
2022-03-14 09:40:01 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:40:06 | INFO | fairseq.tasks.translation | example hypothesis: and we don't know that we don't don't know that we don't don't have to do it.
2022-03-14 09:40:06 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:40:11 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of people in the world in the world, and the world in the world and the world and the world in the world and the world in the world and the world and the world and the world and the world and the world in the world in the world and the world
2022-03-14 09:40:11 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:40:17 | INFO | fairseq.tasks.translation | example hypothesis: but if you're not not not not not not not not not not not not not not not not not not not not not not not not not not not not not able to see that they're not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not
2022-03-14 09:40:17 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:40:23 | INFO | fairseq.tasks.translation | example hypothesis: and if we can see the brain that we can see the brain, we can see the brain, we can see the brain, we can see the brain, and we can use the brain, and we can see the brain, and we can see that we can use the brain, and we can see the brain, and we can see the brain, and we can see the brain, and we can see that we can see the brain, and we can see the
2022-03-14 09:40:23 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:40:31 | INFO | fairseq.tasks.translation | example hypothesis: and we said, "" "" "" "" "" "" "" "we said," "" "" "" "" we have a "" "" "" "" "" "" "" "" "" "" "" "" we said, "we said," "" "we have a" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "we have a" we have a "" "" "" "" we have a "" "" "" "" "" "we have a" "" "" "" "" "" we said, "" "" we said, "" "" "" "" "" "" "" "" "
2022-03-14 09:40:31 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:40:33 | INFO | fairseq.tasks.translation | example hypothesis: and we have a lot of the idea that we could have to have to be able to be a little bit that we could be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to
2022-03-14 09:40:33 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:40:33 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.82 | ppl 112.99 | bleu 1.48 | wps 3692.2 | wpb 17862.2 | bsz 728.3 | num_updates 2507 | best_bleu 1.7
2022-03-14 09:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2507 updates
2022-03-14 09:40:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:40:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:40:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt (epoch 16 @ 2507 updates, score 1.48) (writing took 1.0679194598924369 seconds)
2022-03-14 09:40:34 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-14 09:40:34 | INFO | train | epoch 016 | loss 6.882 | ppl 117.91 | wps 39883.2 | ups 1.59 | wpb 25153.6 | bsz 1020.6 | num_updates 2507 | lr 0.000313375 | gnorm 0.806 | loss_scale 4 | train_wall 48 | gb_free 14.4 | wall 1608
2022-03-14 09:40:35 | INFO | fairseq.trainer | begin training epoch 17
2022-03-14 09:40:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-14 09:41:04 | INFO | train_inner | epoch 017:     93 / 157 loss=6.782, ppl=110.04, wps=31032.9, ups=1.23, wpb=25300.9, bsz=1053.6, num_updates=2600, lr=0.000325, gnorm=0.822, loss_scale=4, train_wall=30, gb_free=15.3, wall=1637
2022-03-14 09:41:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-14 09:41:28 | INFO | fairseq.tasks.translation | example hypothesis: we've got to go back in our lab.
2022-03-14 09:41:28 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-14 09:41:32 | INFO | fairseq.tasks.translation | example hypothesis: this is the most one of the most news of the most news, the most of the most of the most of the most of the most news is
2022-03-14 09:41:32 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-14 09:41:38 | INFO | fairseq.tasks.translation | example hypothesis: so we're going to talk about two million cells.
2022-03-14 09:41:38 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-14 09:41:43 | INFO | fairseq.tasks.translation | example hypothesis: and there's a lot of water, and it's a lot of water, and there's a lot of water, and it's a lot of water, and it's a lot of water, and there
2022-03-14 09:41:43 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-14 09:41:48 | INFO | fairseq.tasks.translation | example hypothesis: and we don't know that we don't know it, and we don't know that it doesn't do it.
2022-03-14 09:41:48 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-14 09:41:53 | INFO | fairseq.tasks.translation | example hypothesis: and this is a lot of people in the world, and in the world, and in the world, and in the world, and in the world, and the world, and it's a lot of the world, and a lot of the world, and the world, and the world,
2022-03-14 09:41:53 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-14 09:41:59 | INFO | fairseq.tasks.translation | example hypothesis: but you can't see it, but you can't see it, but you can't see it, but you can't see it, but you can't see it, but you can't see it, but you can't see it, but you can't see it, but you can't see it, but you can't be a lot of the
2022-03-14 09:41:59 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-14 09:42:05 | INFO | fairseq.tasks.translation | example hypothesis: and then we can see that, and we can see that we can be able to make a little bit of the brain, and then we can see that we can see it, and we can see it, and we can be a lot of the brain, and we can be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to
2022-03-14 09:42:05 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-14 09:42:13 | INFO | fairseq.tasks.translation | example hypothesis: and i said, "well," well, "i'm going to say," well, "well," well, "well," well, "i'm going to say," well, "well," well, "well," i'm going to say, "well," well, "well," well, "i'm going to say," well, "well," well, "well," well, "well," i'm going to say, "well," well, "well," well, "well," well, "well," well, "well," well, "well," well, "well," i'm going to say, "well," well, "well," well, "well," well, "well," well, "well," well, "well," ""
2022-03-14 09:42:13 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-14 09:42:15 | INFO | fairseq.tasks.translation | example hypothesis: and we've been a lot of the idea that we've got to do is that we could be a little bit of that we could be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be
2022-03-14 09:42:15 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-14 09:42:15 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.789 | ppl 110.57 | bleu 1.45 | wps 3424.6 | wpb 17862.2 | bsz 728.3 | num_updates 2664 | best_bleu 1.7
2022-03-14 09:42:15 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-14 09:42:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2664 updates
2022-03-14 09:42:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:42:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt
2022-03-14 09:42:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.7/checkpoint_last.pt (epoch 17 @ 2664 updates, score 1.45) (writing took 1.086776247015223 seconds)
2022-03-14 09:42:17 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-14 09:42:17 | INFO | train | epoch 017 | loss 6.786 | ppl 110.32 | wps 38619.5 | ups 1.54 | wpb 25153.6 | bsz 1020.6 | num_updates 2664 | lr 0.000333 | gnorm 0.842 | loss_scale 4 | train_wall 48 | gb_free 14.1 | wall 1710
2022-03-14 09:42:17 | INFO | fairseq_cli.train | done training in 1709.8 seconds
