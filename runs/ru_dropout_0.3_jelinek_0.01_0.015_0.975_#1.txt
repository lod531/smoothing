Sender: LSF System <lsfadmin@eu-g2-04>
Subject: Job 208727543: <ru_dropout_0.3_jelinek_0.01_0.015_0.975_#1> in cluster <euler> Done

Job <ru_dropout_0.3_jelinek_0.01_0.015_0.975_#1> was submitted from host <eu-login-24> by user <andriusb> in cluster <euler> at Tue Mar 15 16:41:10 2022
Job was executed on host(s) <eu-g2-04>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 16:41:22 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 16:41:22 2022
Terminated at Wed Mar 16 16:25:17 2022
Results reported at Wed Mar 16 16:25:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/ru --save-dir /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.01,0.015,0.975\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   89486.94 sec.
    Max Memory :                                 5055 MB
    Average Memory :                             4099.52 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14945.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   85434 sec.
    Turnaround time :                            85447 sec.

The output (if any) follows:

2022-03-15 16:41:33 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/ru', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.01,0.015,0.975)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 16:41:33 | INFO | fairseq.tasks.language_modeling | dictionary: 35920 types
2022-03-15 16:41:34 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
Calculating frequency stats:
  0%|          | 0/53136 [00:00<?, ?it/s]  0%|          | 81/53136 [00:00<01:08, 773.64it/s]  0%|          | 159/53136 [00:00<01:32, 572.00it/s]  0%|          | 228/53136 [00:00<01:25, 615.68it/s]  1%|          | 293/53136 [00:00<01:29, 591.69it/s]  1%|          | 372/53136 [00:00<01:20, 654.96it/s]  1%|          | 477/53136 [00:00<01:07, 778.39it/s]  1%|          | 572/53136 [00:00<01:03, 829.14it/s]  1%|          | 657/53136 [00:00<01:03, 832.48it/s]  1%|▏         | 753/53136 [00:00<01:00, 869.85it/s]  2%|▏         | 841/53136 [00:01<01:09, 748.36it/s]  2%|▏         | 940/53136 [00:01<01:04, 813.21it/s]  2%|▏         | 1025/53136 [00:01<01:07, 777.60it/s]  2%|▏         | 1106/53136 [00:01<01:07, 767.64it/s]  2%|▏         | 1185/53136 [00:01<01:07, 764.89it/s]  2%|▏         | 1277/53136 [00:01<01:04, 807.29it/s]  3%|▎         | 1359/53136 [00:01<01:06, 783.21it/s]  3%|▎         | 1464/53136 [00:01<01:00, 858.08it/s]  3%|▎         | 1566/53136 [00:01<00:57, 896.24it/s]  3%|▎         | 1685/53136 [00:02<00:53, 962.84it/s]  3%|▎         | 1791/53136 [00:02<00:52, 986.99it/s]  4%|▎         | 1891/53136 [00:02<00:53, 956.13it/s]  4%|▎         | 1988/53136 [00:02<00:56, 905.87it/s]  4%|▍         | 2090/53136 [00:02<00:54, 936.67it/s]  4%|▍         | 2185/53136 [00:02<00:56, 903.36it/s]  4%|▍         | 2304/53136 [00:02<00:52, 971.99it/s]  5%|▍         | 2402/53136 [00:02<00:53, 952.36it/s]  5%|▍         | 2498/53136 [00:03<01:07, 752.70it/s]  5%|▍         | 2597/53136 [00:03<01:02, 807.52it/s]  5%|▌         | 2701/53136 [00:03<00:58, 864.59it/s]  5%|▌         | 2805/53136 [00:03<00:55, 909.66it/s]  5%|▌         | 2900/53136 [00:03<01:01, 816.19it/s]  6%|▌         | 2988/53136 [00:03<01:00, 827.92it/s]  6%|▌         | 3074/53136 [00:03<01:03, 787.80it/s]  6%|▌         | 3156/53136 [00:03<01:09, 718.88it/s]  6%|▌         | 3231/53136 [00:04<01:14, 674.13it/s]  6%|▋         | 3328/53136 [00:04<01:07, 743.33it/s]  6%|▋         | 3405/53136 [00:04<01:08, 724.20it/s]  7%|▋         | 3489/53136 [00:04<01:05, 754.47it/s]  7%|▋         | 3573/53136 [00:04<01:16, 648.32it/s]  7%|▋         | 3659/53136 [00:04<01:10, 700.05it/s]  7%|▋         | 3733/53136 [00:04<01:11, 687.56it/s]  7%|▋         | 3830/53136 [00:04<01:05, 755.17it/s]  7%|▋         | 3911/53136 [00:04<01:05, 746.77it/s]  8%|▊         | 4018/53136 [00:05<00:59, 822.62it/s]  8%|▊         | 4103/53136 [00:05<00:59, 829.48it/s]  8%|▊         | 4197/53136 [00:05<00:56, 860.88it/s]  8%|▊         | 4285/53136 [00:05<00:56, 858.04it/s]  8%|▊         | 4372/53136 [00:05<01:05, 743.59it/s]  8%|▊         | 4450/53136 [00:05<01:06, 735.74it/s]  9%|▊         | 4532/53136 [00:05<01:04, 757.17it/s]  9%|▊         | 4614/53136 [00:05<01:02, 772.23it/s]  9%|▉         | 4704/53136 [00:05<00:59, 807.90it/s]  9%|▉         | 4791/53136 [00:06<01:12, 671.39it/s]  9%|▉         | 4873/53136 [00:06<01:08, 706.68it/s]  9%|▉         | 4950/53136 [00:06<01:07, 710.25it/s] 10%|▉         | 5054/53136 [00:06<01:00, 798.64it/s] 10%|▉         | 5137/53136 [00:06<01:09, 686.85it/s] 10%|▉         | 5220/53136 [00:06<01:06, 722.96it/s] 10%|▉         | 5297/53136 [00:06<01:09, 686.44it/s] 10%|█         | 5369/53136 [00:06<01:13, 652.61it/s] 10%|█         | 5477/53136 [00:06<01:02, 762.81it/s] 10%|█         | 5559/53136 [00:07<01:01, 776.90it/s] 11%|█         | 5655/53136 [00:07<00:57, 823.64it/s] 11%|█         | 5748/53136 [00:07<00:56, 844.59it/s] 11%|█         | 5838/53136 [00:07<00:55, 856.62it/s] 11%|█         | 5925/53136 [00:07<00:55, 849.24it/s] 11%|█▏        | 6017/53136 [00:07<00:54, 869.23it/s] 12%|█▏        | 6117/53136 [00:07<00:51, 906.35it/s] 12%|█▏        | 6209/53136 [00:07<01:02, 746.36it/s] 12%|█▏        | 6289/53136 [00:07<01:01, 756.76it/s] 12%|█▏        | 6402/53136 [00:08<00:55, 846.44it/s] 12%|█▏        | 6495/53136 [00:08<00:53, 866.94it/s] 12%|█▏        | 6609/53136 [00:08<00:49, 940.87it/s] 13%|█▎        | 6712/53136 [00:08<00:48, 963.80it/s] 13%|█▎        | 6811/53136 [00:08<00:48, 956.11it/s] 13%|█▎        | 6908/53136 [00:08<00:52, 878.10it/s] 13%|█▎        | 6998/53136 [00:08<01:07, 679.64it/s] 13%|█▎        | 7079/53136 [00:08<01:05, 704.09it/s] 14%|█▎        | 7199/53136 [00:09<00:55, 822.41it/s] 14%|█▎        | 7288/53136 [00:09<00:54, 837.64it/s] 14%|█▍        | 7377/53136 [00:09<01:12, 634.52it/s] 14%|█▍        | 7451/53136 [00:09<01:14, 614.67it/s] 14%|█▍        | 7527/53136 [00:09<01:10, 647.74it/s] 14%|█▍        | 7626/53136 [00:09<01:02, 729.14it/s] 15%|█▍        | 7705/53136 [00:09<01:04, 706.90it/s] 15%|█▍        | 7808/53136 [00:09<00:57, 789.73it/s] 15%|█▍        | 7900/53136 [00:10<00:54, 823.54it/s] 15%|█▌        | 7993/53136 [00:10<00:53, 851.11it/s] 15%|█▌        | 8081/53136 [00:10<00:56, 804.23it/s] 15%|█▌        | 8164/53136 [00:10<00:56, 797.15it/s] 16%|█▌        | 8246/53136 [00:10<01:01, 731.59it/s] 16%|█▌        | 8330/53136 [00:10<01:00, 741.04it/s] 16%|█▌        | 8406/53136 [00:10<01:06, 677.14it/s] 16%|█▌        | 8480/53136 [00:10<01:04, 692.83it/s] 16%|█▌        | 8551/53136 [00:10<01:05, 676.72it/s] 16%|█▋        | 8637/53136 [00:11<01:05, 675.96it/s] 16%|█▋        | 8729/53136 [00:11<01:00, 738.92it/s] 17%|█▋        | 8822/53136 [00:11<00:56, 790.59it/s] 17%|█▋        | 8903/53136 [00:11<00:56, 783.04it/s] 17%|█▋        | 8983/53136 [00:11<01:04, 689.33it/s] 17%|█▋        | 9055/53136 [00:11<01:03, 691.82it/s] 17%|█▋        | 9159/53136 [00:11<00:57, 766.23it/s] 17%|█▋        | 9261/53136 [00:11<00:53, 814.52it/s] 18%|█▊        | 9344/53136 [00:11<00:55, 789.24it/s] 18%|█▊        | 9424/53136 [00:12<00:58, 748.12it/s] 18%|█▊        | 9528/53136 [00:12<00:52, 827.12it/s] 18%|█▊        | 9630/53136 [00:12<00:49, 880.24it/s] 18%|█▊        | 9720/53136 [00:12<00:50, 862.62it/s] 18%|█▊        | 9808/53136 [00:12<01:13, 589.80it/s] 19%|█▊        | 9890/53136 [00:12<01:07, 638.30it/s] 19%|█▉        | 9965/53136 [00:12<01:07, 643.74it/s] 19%|█▉        | 10037/53136 [00:12<01:06, 651.11it/s] 19%|█▉        | 10142/53136 [00:13<00:57, 751.80it/s] 19%|█▉        | 10226/53136 [00:13<00:55, 773.84it/s] 19%|█▉        | 10308/53136 [00:13<00:59, 718.74it/s] 20%|█▉        | 10385/53136 [00:13<00:58, 729.43it/s] 20%|█▉        | 10461/53136 [00:13<01:05, 651.21it/s] 20%|█▉        | 10534/53136 [00:13<01:03, 671.03it/s] 20%|█▉        | 10604/53136 [00:13<01:09, 613.87it/s] 20%|██        | 10702/53136 [00:13<01:00, 707.03it/s] 20%|██        | 10776/53136 [00:14<00:59, 714.01it/s] 20%|██        | 10850/53136 [00:14<00:59, 711.81it/s] 21%|██        | 10957/53136 [00:14<00:52, 810.66it/s] 21%|██        | 11042/53136 [00:14<00:57, 733.34it/s] 21%|██        | 11118/53136 [00:14<01:04, 648.50it/s] 21%|██        | 11208/53136 [00:14<00:58, 711.20it/s] 21%|██        | 11283/53136 [00:14<01:00, 695.49it/s] 21%|██▏       | 11367/53136 [00:14<00:58, 708.07it/s] 22%|██▏       | 11442/53136 [00:14<00:58, 714.66it/s] 22%|██▏       | 11528/53136 [00:15<00:55, 754.71it/s] 22%|██▏       | 11605/53136 [00:15<01:04, 639.32it/s] 22%|██▏       | 11683/53136 [00:15<01:01, 672.65it/s] 22%|██▏       | 11772/53136 [00:15<00:56, 726.25it/s] 22%|██▏       | 11860/53136 [00:15<00:53, 766.18it/s] 22%|██▏       | 11948/53136 [00:15<00:51, 792.79it/s] 23%|██▎       | 12038/53136 [00:15<00:50, 821.66it/s] 23%|██▎       | 12122/53136 [00:15<01:01, 670.48it/s] 23%|██▎       | 12197/53136 [00:16<01:02, 654.63it/s] 23%|██▎       | 12302/53136 [00:16<00:54, 754.36it/s] 23%|██▎       | 12417/53136 [00:16<00:47, 859.21it/s] 24%|██▎       | 12508/53136 [00:16<00:47, 854.52it/s] 24%|██▎       | 12597/53136 [00:16<00:53, 757.10it/s] 24%|██▍       | 12677/53136 [00:16<01:04, 624.79it/s] 24%|██▍       | 12755/53136 [00:16<01:01, 656.25it/s] 24%|██▍       | 12826/53136 [00:16<01:05, 620.11it/s] 24%|██▍       | 12908/53136 [00:17<01:01, 650.98it/s] 24%|██▍       | 13011/53136 [00:17<00:53, 746.48it/s] 25%|██▍       | 13099/53136 [00:17<00:51, 770.22it/s] 25%|██▍       | 13187/53136 [00:17<00:50, 791.36it/s] 25%|██▍       | 13269/53136 [00:17<00:50, 792.06it/s] 25%|██▌       | 13360/53136 [00:17<00:49, 804.06it/s] 25%|██▌       | 13445/53136 [00:17<00:48, 815.21it/s] 25%|██▌       | 13528/53136 [00:17<00:51, 769.90it/s] 26%|██▌       | 13606/53136 [00:17<00:54, 719.28it/s] 26%|██▌       | 13696/53136 [00:18<00:51, 760.36it/s] 26%|██▌       | 13797/53136 [00:18<00:47, 827.15it/s] 26%|██▌       | 13882/53136 [00:18<00:56, 692.67it/s] 26%|██▋       | 13956/53136 [00:18<00:59, 663.89it/s] 26%|██▋       | 14033/53136 [00:18<00:56, 687.52it/s] 27%|██▋       | 14119/53136 [00:18<00:53, 731.81it/s] 27%|██▋       | 14195/53136 [00:18<01:00, 645.71it/s] 27%|██▋       | 14274/53136 [00:18<00:57, 681.66it/s] 27%|██▋       | 14346/53136 [00:18<00:57, 674.35it/s] 27%|██▋       | 14427/53136 [00:19<00:54, 709.71it/s] 27%|██▋       | 14506/53136 [00:19<00:52, 729.55it/s] 27%|██▋       | 14609/53136 [00:19<00:47, 810.40it/s] 28%|██▊       | 14727/53136 [00:19<00:41, 916.53it/s] 28%|██▊       | 14821/53136 [00:19<00:42, 906.08it/s] 28%|██▊       | 14913/53136 [00:19<00:49, 772.99it/s] 28%|██▊       | 14995/53136 [00:19<00:50, 752.56it/s] 28%|██▊       | 15073/53136 [00:19<00:50, 754.06it/s] 29%|██▊       | 15151/53136 [00:19<00:52, 727.19it/s] 29%|██▊       | 15238/53136 [00:20<00:49, 762.21it/s] 29%|██▉       | 15316/53136 [00:20<01:03, 595.59it/s] 29%|██▉       | 15399/53136 [00:20<00:58, 649.36it/s] 29%|██▉       | 15470/53136 [00:20<00:58, 640.73it/s] 29%|██▉       | 15543/53136 [00:20<00:56, 663.09it/s] 29%|██▉       | 15613/53136 [00:20<01:01, 607.78it/s] 30%|██▉       | 15697/53136 [00:20<00:56, 663.73it/s] 30%|██▉       | 15787/53136 [00:20<00:51, 726.73it/s] 30%|██▉       | 15863/53136 [00:21<00:55, 676.84it/s] 30%|███       | 15945/53136 [00:21<00:52, 713.94it/s] 30%|███       | 16035/53136 [00:21<00:48, 760.71it/s] 30%|███       | 16116/53136 [00:21<00:47, 773.74it/s] 30%|███       | 16195/53136 [00:21<00:54, 681.55it/s] 31%|███       | 16266/53136 [00:21<00:55, 661.04it/s] 31%|███       | 16349/53136 [00:21<00:52, 705.67it/s] 31%|███       | 16446/53136 [00:21<00:47, 773.14it/s] 31%|███       | 16540/53136 [00:21<00:44, 819.17it/s] 31%|███▏      | 16638/53136 [00:22<00:42, 858.54it/s] 31%|███▏      | 16735/53136 [00:22<00:41, 885.45it/s] 32%|███▏      | 16828/53136 [00:22<00:40, 898.28it/s] 32%|███▏      | 16919/53136 [00:22<00:41, 877.47it/s] 32%|███▏      | 17008/53136 [00:22<00:46, 769.73it/s] 32%|███▏      | 17106/53136 [00:22<00:44, 817.40it/s] 32%|███▏      | 17200/53136 [00:22<00:42, 850.59it/s] 33%|███▎      | 17288/53136 [00:22<00:45, 786.86it/s] 33%|███▎      | 17380/53136 [00:22<00:45, 779.85it/s] 33%|███▎      | 17470/53136 [00:23<00:44, 803.76it/s] 33%|███▎      | 17557/53136 [00:23<00:43, 820.18it/s] 33%|███▎      | 17641/53136 [00:23<00:48, 738.80it/s] 33%|███▎      | 17724/53136 [00:23<00:46, 760.43it/s] 34%|███▎      | 17810/53136 [00:23<00:44, 786.73it/s] 34%|███▎      | 17891/53136 [00:23<00:46, 756.87it/s] 34%|███▍      | 17968/53136 [00:23<00:46, 751.93it/s] 34%|███▍      | 18060/53136 [00:23<00:43, 798.08it/s] 34%|███▍      | 18141/53136 [00:23<00:49, 708.78it/s] 34%|███▍      | 18221/53136 [00:24<00:47, 732.57it/s] 34%|███▍      | 18297/53136 [00:24<00:50, 688.82it/s] 35%|███▍      | 18384/53136 [00:24<00:47, 734.41it/s] 35%|███▍      | 18460/53136 [00:24<00:56, 615.89it/s] 35%|███▍      | 18526/53136 [00:24<00:55, 618.85it/s] 35%|███▌      | 18607/53136 [00:24<00:51, 667.76it/s] 35%|███▌      | 18677/53136 [00:24<00:52, 659.61it/s] 35%|███▌      | 18785/53136 [00:24<00:44, 771.78it/s] 36%|███▌      | 18893/53136 [00:24<00:39, 857.28it/s] 36%|███▌      | 18981/53136 [00:25<00:45, 754.83it/s] 36%|███▌      | 19089/53136 [00:25<00:41, 827.85it/s] 36%|███▌      | 19176/53136 [00:25<00:42, 805.70it/s] 36%|███▋      | 19265/53136 [00:25<00:43, 781.00it/s] 36%|███▋      | 19357/53136 [00:25<00:42, 800.06it/s] 37%|███▋      | 19439/53136 [00:25<00:43, 775.72it/s] 37%|███▋      | 19518/53136 [00:25<00:49, 685.86it/s] 37%|███▋      | 19591/53136 [00:25<00:48, 695.00it/s] 37%|███▋      | 19689/53136 [00:26<00:43, 769.80it/s] 37%|███▋      | 19769/53136 [00:26<00:45, 731.03it/s] 37%|███▋      | 19845/53136 [00:26<00:45, 733.10it/s] 38%|███▊      | 19943/53136 [00:26<00:45, 722.63it/s] 38%|███▊      | 20028/53136 [00:26<00:44, 749.71it/s] 38%|███▊      | 20127/53136 [00:26<00:40, 814.66it/s] 38%|███▊      | 20210/53136 [00:26<00:49, 671.63it/s] 38%|███▊      | 20302/53136 [00:26<00:44, 732.59it/s] 38%|███▊      | 20380/53136 [00:27<00:51, 634.47it/s] 39%|███▊      | 20463/53136 [00:27<00:48, 680.24it/s] 39%|███▊      | 20541/53136 [00:27<00:50, 651.52it/s] 39%|███▉      | 20616/53136 [00:27<00:48, 672.87it/s] 39%|███▉      | 20687/53136 [00:27<00:50, 643.10it/s] 39%|███▉      | 20793/53136 [00:27<00:44, 732.35it/s] 39%|███▉      | 20869/53136 [00:27<00:43, 738.78it/s] 39%|███▉      | 20950/53136 [00:27<00:42, 753.79it/s] 40%|███▉      | 21041/53136 [00:27<00:40, 797.63it/s] 40%|███▉      | 21122/53136 [00:28<00:42, 753.64it/s] 40%|███▉      | 21199/53136 [00:28<00:43, 729.51it/s] 40%|████      | 21297/53136 [00:28<00:40, 794.81it/s] 40%|████      | 21378/53136 [00:28<00:40, 790.65it/s] 40%|████      | 21467/53136 [00:28<00:38, 818.34it/s] 41%|████      | 21552/53136 [00:28<00:40, 789.13it/s] 41%|████      | 21632/53136 [00:28<00:45, 698.62it/s] 41%|████      | 21704/53136 [00:28<00:49, 633.40it/s] 41%|████      | 21770/53136 [00:29<00:56, 559.47it/s] 41%|████      | 21877/53136 [00:29<00:45, 681.06it/s] 41%|████▏     | 21950/53136 [00:29<00:45, 690.55it/s] 41%|████▏     | 22027/53136 [00:29<00:43, 711.16it/s] 42%|████▏     | 22110/53136 [00:29<00:41, 742.96it/s] 42%|████▏     | 22191/53136 [00:29<00:40, 761.28it/s] 42%|████▏     | 22274/53136 [00:29<00:39, 780.34it/s] 42%|████▏     | 22360/53136 [00:29<00:38, 801.61it/s] 42%|████▏     | 22457/53136 [00:29<00:36, 850.68it/s] 42%|████▏     | 22543/53136 [00:29<00:39, 778.77it/s] 43%|████▎     | 22637/53136 [00:30<00:37, 822.81it/s] 43%|████▎     | 22729/53136 [00:30<00:36, 840.34it/s] 43%|████▎     | 22815/53136 [00:30<00:35, 842.59it/s] 43%|████▎     | 22901/53136 [00:30<00:37, 808.92it/s] 43%|████▎     | 22983/53136 [00:30<00:43, 685.84it/s] 43%|████▎     | 23071/53136 [00:30<00:40, 734.05it/s] 44%|████▎     | 23160/53136 [00:30<00:38, 772.35it/s] 44%|████▍     | 23252/53136 [00:30<00:36, 812.31it/s] 44%|████▍     | 23336/53136 [00:31<00:39, 748.26it/s] 44%|████▍     | 23414/53136 [00:31<00:44, 674.89it/s] 44%|████▍     | 23485/53136 [00:31<00:43, 678.07it/s] 44%|████▍     | 23566/53136 [00:31<00:43, 676.78it/s] 44%|████▍     | 23636/53136 [00:31<00:47, 626.00it/s] 45%|████▍     | 23700/53136 [00:31<00:48, 608.25it/s] 45%|████▍     | 23812/53136 [00:31<00:42, 688.14it/s] 45%|████▍     | 23895/53136 [00:31<00:40, 721.83it/s] 45%|████▌     | 23977/53136 [00:31<00:39, 746.76it/s] 45%|████▌     | 24069/53136 [00:32<00:36, 792.38it/s] 45%|████▌     | 24161/53136 [00:32<00:34, 827.90it/s] 46%|████▌     | 24245/53136 [00:32<00:35, 816.60it/s] 46%|████▌     | 24334/53136 [00:32<00:34, 830.27it/s] 46%|████▌     | 24418/53136 [00:32<00:38, 744.18it/s] 46%|████▌     | 24521/53136 [00:32<00:34, 820.77it/s] 46%|████▋     | 24625/53136 [00:32<00:32, 880.82it/s] 47%|████▋     | 24716/53136 [00:32<00:31, 888.47it/s] 47%|████▋     | 24807/53136 [00:32<00:34, 826.27it/s] 47%|████▋     | 24900/53136 [00:33<00:33, 854.34it/s] 47%|████▋     | 24987/53136 [00:33<00:37, 754.59it/s] 47%|████▋     | 25095/53136 [00:33<00:33, 835.07it/s] 47%|████▋     | 25182/53136 [00:33<00:35, 795.37it/s] 48%|████▊     | 25271/53136 [00:33<00:33, 819.72it/s] 48%|████▊     | 25371/53136 [00:33<00:31, 867.82it/s] 48%|████▊     | 25460/53136 [00:33<00:33, 818.61it/s] 48%|████▊     | 25544/53136 [00:33<00:35, 774.24it/s] 48%|████▊     | 25623/53136 [00:33<00:35, 771.93it/s] 48%|████▊     | 25702/53136 [00:34<00:35, 771.41it/s] 49%|████▊     | 25781/53136 [00:34<00:35, 776.10it/s] 49%|████▊     | 25876/53136 [00:34<00:33, 825.20it/s] 49%|████▉     | 25961/53136 [00:34<00:32, 830.18it/s] 49%|████▉     | 26052/53136 [00:34<00:31, 847.66it/s] 49%|████▉     | 26138/53136 [00:34<00:41, 648.63it/s] 49%|████▉     | 26213/53136 [00:34<00:40, 671.90it/s] 49%|████▉     | 26286/53136 [00:34<00:40, 667.24it/s] 50%|████▉     | 26362/53136 [00:35<00:39, 685.88it/s] 50%|████▉     | 26465/53136 [00:35<00:34, 775.15it/s] 50%|████▉     | 26546/53136 [00:35<00:35, 750.69it/s] 50%|█████     | 26636/53136 [00:35<00:33, 791.74it/s] 50%|█████     | 26717/53136 [00:35<00:33, 787.42it/s] 50%|█████     | 26812/53136 [00:35<00:32, 804.03it/s] 51%|█████     | 26894/53136 [00:35<00:35, 744.67it/s] 51%|█████     | 26970/53136 [00:35<00:35, 730.44it/s] 51%|█████     | 27069/53136 [00:35<00:32, 797.80it/s] 51%|█████     | 27150/53136 [00:36<00:34, 747.19it/s] 51%|█████     | 27231/53136 [00:36<00:34, 760.03it/s] 51%|█████▏    | 27309/53136 [00:36<00:33, 764.16it/s] 52%|█████▏    | 27387/53136 [00:36<00:38, 675.35it/s] 52%|█████▏    | 27469/53136 [00:36<00:36, 705.72it/s] 52%|█████▏    | 27545/53136 [00:36<00:35, 714.16it/s] 52%|█████▏    | 27618/53136 [00:36<00:36, 704.39it/s] 52%|█████▏    | 27690/53136 [00:36<00:41, 613.49it/s] 52%|█████▏    | 27800/53136 [00:36<00:34, 733.80it/s] 52%|█████▏    | 27877/53136 [00:37<00:34, 740.10it/s] 53%|█████▎    | 27958/53136 [00:37<00:33, 757.76it/s] 53%|█████▎    | 28036/53136 [00:37<00:32, 763.98it/s] 53%|█████▎    | 28114/53136 [00:37<00:40, 620.45it/s] 53%|█████▎    | 28206/53136 [00:37<00:35, 693.97it/s] 53%|█████▎    | 28287/53136 [00:37<00:34, 724.10it/s] 53%|█████▎    | 28364/53136 [00:37<00:33, 728.70it/s] 54%|█████▎    | 28461/53136 [00:37<00:31, 794.20it/s] 54%|█████▍    | 28578/53136 [00:37<00:27, 892.56it/s] 54%|█████▍    | 28670/53136 [00:38<00:27, 890.17it/s] 54%|█████▍    | 28761/53136 [00:38<00:28, 862.29it/s] 54%|█████▍    | 28849/53136 [00:38<00:33, 730.92it/s] 55%|█████▍    | 28962/53136 [00:38<00:29, 831.64it/s] 55%|█████▍    | 29053/53136 [00:38<00:28, 842.63it/s] 55%|█████▍    | 29141/53136 [00:38<00:29, 818.07it/s] 55%|█████▌    | 29226/53136 [00:38<00:30, 774.79it/s] 55%|█████▌    | 29307/53136 [00:38<00:30, 773.14it/s] 55%|█████▌    | 29386/53136 [00:39<00:33, 700.70it/s] 55%|█████▌    | 29475/53136 [00:39<00:32, 736.48it/s] 56%|█████▌    | 29574/53136 [00:39<00:29, 803.89it/s] 56%|█████▌    | 29657/53136 [00:39<00:29, 804.42it/s] 56%|█████▌    | 29739/53136 [00:39<00:35, 662.72it/s] 56%|█████▌    | 29811/53136 [00:39<00:34, 672.83it/s] 56%|█████▌    | 29884/53136 [00:39<00:33, 686.78it/s] 56%|█████▋    | 29976/53136 [00:39<00:31, 734.54it/s] 57%|█████▋    | 30052/53136 [00:39<00:36, 627.53it/s] 57%|█████▋    | 30133/53136 [00:40<00:34, 668.25it/s] 57%|█████▋    | 30221/53136 [00:40<00:31, 722.42it/s] 57%|█████▋    | 30319/53136 [00:40<00:28, 788.91it/s] 57%|█████▋    | 30401/53136 [00:40<00:35, 648.70it/s] 57%|█████▋    | 30488/53136 [00:40<00:32, 700.32it/s] 58%|█████▊    | 30564/53136 [00:40<00:32, 701.56it/s] 58%|█████▊    | 30639/53136 [00:40<00:31, 707.42it/s] 58%|█████▊    | 30738/53136 [00:40<00:28, 782.33it/s] 58%|█████▊    | 30819/53136 [00:41<00:32, 679.55it/s] 58%|█████▊    | 30907/53136 [00:41<00:30, 730.43it/s] 58%|█████▊    | 30984/53136 [00:41<00:31, 693.99it/s] 58%|█████▊    | 31076/53136 [00:41<00:29, 753.35it/s] 59%|█████▊    | 31155/53136 [00:41<00:30, 722.92it/s] 59%|█████▉    | 31261/53136 [00:41<00:27, 807.48it/s] 59%|█████▉    | 31358/53136 [00:41<00:25, 851.99it/s] 59%|█████▉    | 31451/53136 [00:41<00:24, 867.92it/s] 59%|█████▉    | 31540/53136 [00:41<00:30, 708.48it/s] 60%|█████▉    | 31617/53136 [00:42<00:33, 638.96it/s] 60%|█████▉    | 31709/53136 [00:42<00:30, 703.60it/s] 60%|█████▉    | 31813/53136 [00:42<00:27, 786.42it/s] 60%|██████    | 31897/53136 [00:42<00:27, 782.45it/s] 60%|██████    | 31979/53136 [00:42<00:28, 732.11it/s] 60%|██████    | 32059/53136 [00:42<00:28, 749.18it/s] 60%|██████    | 32137/53136 [00:42<00:28, 742.90it/s] 61%|██████    | 32232/53136 [00:42<00:26, 799.05it/s] 61%|██████    | 32314/53136 [00:42<00:26, 781.00it/s] 61%|██████    | 32401/53136 [00:43<00:26, 787.07it/s] 61%|██████    | 32494/53136 [00:43<00:25, 822.87it/s] 61%|██████▏   | 32579/53136 [00:43<00:24, 829.96it/s] 61%|██████▏   | 32667/53136 [00:43<00:24, 835.35it/s] 62%|██████▏   | 32751/53136 [00:43<00:24, 830.07it/s] 62%|██████▏   | 32855/53136 [00:43<00:22, 891.30it/s] 62%|██████▏   | 32955/53136 [00:43<00:21, 922.83it/s] 62%|██████▏   | 33048/53136 [00:43<00:25, 800.50it/s] 62%|██████▏   | 33132/53136 [00:43<00:25, 784.03it/s] 63%|██████▎   | 33215/53136 [00:44<00:25, 795.87it/s] 63%|██████▎   | 33297/53136 [00:44<00:27, 728.62it/s] 63%|██████▎   | 33372/53136 [00:44<00:26, 733.55it/s] 63%|██████▎   | 33485/53136 [00:44<00:23, 839.53it/s] 63%|██████▎   | 33571/53136 [00:44<00:27, 705.72it/s] 63%|██████▎   | 33657/53136 [00:44<00:26, 743.52it/s] 63%|██████▎   | 33736/53136 [00:44<00:28, 691.35it/s] 64%|██████▎   | 33809/53136 [00:44<00:30, 641.27it/s] 64%|██████▍   | 33906/53136 [00:45<00:26, 717.75it/s] 64%|██████▍   | 34018/53136 [00:45<00:23, 818.88it/s] 64%|██████▍   | 34105/53136 [00:45<00:23, 820.07it/s] 64%|██████▍   | 34217/53136 [00:45<00:21, 879.23it/s] 65%|██████▍   | 34310/53136 [00:45<00:21, 893.09it/s] 65%|██████▍   | 34401/53136 [00:45<00:26, 699.58it/s] 65%|██████▍   | 34479/53136 [00:45<00:26, 716.99it/s] 65%|██████▌   | 34557/53136 [00:45<00:27, 685.63it/s] 65%|██████▌   | 34657/53136 [00:45<00:24, 760.83it/s] 65%|██████▌   | 34746/53136 [00:46<00:23, 793.40it/s] 66%|██████▌   | 34832/53136 [00:46<00:22, 811.32it/s] 66%|██████▌   | 34916/53136 [00:46<00:28, 632.84it/s] 66%|██████▌   | 34989/53136 [00:46<00:27, 654.12it/s] 66%|██████▌   | 35061/53136 [00:46<00:29, 606.70it/s] 66%|██████▌   | 35127/53136 [00:46<00:30, 594.32it/s] 66%|██████▌   | 35190/53136 [00:46<00:29, 598.28it/s] 66%|██████▋   | 35280/53136 [00:46<00:26, 663.70it/s] 67%|██████▋   | 35370/53136 [00:47<00:24, 720.12it/s] 67%|██████▋   | 35448/53136 [00:47<00:28, 629.19it/s] 67%|██████▋   | 35526/53136 [00:47<00:26, 663.27it/s] 67%|██████▋   | 35627/53136 [00:47<00:23, 753.51it/s] 67%|██████▋   | 35718/53136 [00:47<00:21, 793.81it/s] 67%|██████▋   | 35817/53136 [00:47<00:20, 847.51it/s] 68%|██████▊   | 35904/53136 [00:47<00:23, 747.27it/s] 68%|██████▊   | 35983/53136 [00:47<00:26, 653.22it/s] 68%|██████▊   | 36078/53136 [00:48<00:23, 725.27it/s] 68%|██████▊   | 36156/53136 [00:48<00:23, 717.51it/s] 68%|██████▊   | 36231/53136 [00:48<00:30, 556.06it/s] 68%|██████▊   | 36325/53136 [00:48<00:26, 641.89it/s] 69%|██████▊   | 36416/53136 [00:48<00:23, 706.68it/s] 69%|██████▊   | 36494/53136 [00:48<00:23, 700.05it/s] 69%|██████▉   | 36573/53136 [00:48<00:22, 723.12it/s] 69%|██████▉   | 36650/53136 [00:48<00:24, 685.49it/s] 69%|██████▉   | 36722/53136 [00:49<00:23, 685.41it/s] 69%|██████▉   | 36793/53136 [00:49<00:27, 588.47it/s] 69%|██████▉   | 36868/53136 [00:49<00:25, 628.61it/s] 70%|██████▉   | 36948/53136 [00:49<00:24, 673.27it/s] 70%|██████▉   | 37019/53136 [00:49<00:23, 677.67it/s] 70%|██████▉   | 37089/53136 [00:49<00:23, 680.53it/s] 70%|██████▉   | 37159/53136 [00:49<00:23, 670.42it/s] 70%|███████   | 37247/53136 [00:49<00:22, 719.21it/s] 70%|███████   | 37320/53136 [00:49<00:24, 644.82it/s] 70%|███████   | 37411/53136 [00:50<00:21, 715.54it/s] 71%|███████   | 37487/53136 [00:50<00:21, 726.25it/s] 71%|███████   | 37588/53136 [00:50<00:19, 805.03it/s] 71%|███████   | 37673/53136 [00:50<00:18, 816.34it/s] 71%|███████   | 37756/53136 [00:50<00:18, 813.04it/s] 71%|███████   | 37857/53136 [00:50<00:17, 848.83it/s] 71%|███████▏  | 37943/53136 [00:50<00:17, 847.80it/s] 72%|███████▏  | 38029/53136 [00:50<00:18, 814.52it/s] 72%|███████▏  | 38117/53136 [00:50<00:18, 830.27it/s] 72%|███████▏  | 38201/53136 [00:51<00:23, 635.60it/s] 72%|███████▏  | 38282/53136 [00:51<00:22, 675.14it/s] 72%|███████▏  | 38356/53136 [00:51<00:22, 671.07it/s] 72%|███████▏  | 38428/53136 [00:51<00:22, 640.36it/s] 72%|███████▏  | 38523/53136 [00:51<00:20, 720.10it/s] 73%|███████▎  | 38599/53136 [00:51<00:20, 716.54it/s] 73%|███████▎  | 38673/53136 [00:51<00:20, 700.94it/s] 73%|███████▎  | 38745/53136 [00:51<00:21, 671.85it/s] 73%|███████▎  | 38831/53136 [00:51<00:20, 702.95it/s] 73%|███████▎  | 38921/53136 [00:52<00:19, 743.97it/s] 73%|███████▎  | 39016/53136 [00:52<00:18, 771.62it/s] 74%|███████▎  | 39109/53136 [00:52<00:17, 813.87it/s] 74%|███████▍  | 39203/53136 [00:52<00:16, 848.48it/s] 74%|███████▍  | 39289/53136 [00:52<00:17, 772.30it/s] 74%|███████▍  | 39374/53136 [00:52<00:17, 783.88it/s] 74%|███████▍  | 39457/53136 [00:52<00:17, 795.39it/s] 74%|███████▍  | 39538/53136 [00:52<00:19, 696.08it/s] 75%|███████▍  | 39611/53136 [00:52<00:19, 700.07it/s] 75%|███████▍  | 39688/53136 [00:53<00:18, 718.74it/s] 75%|███████▍  | 39762/53136 [00:53<00:19, 682.03it/s] 75%|███████▌  | 39856/53136 [00:53<00:17, 751.32it/s] 75%|███████▌  | 39953/53136 [00:53<00:16, 811.26it/s] 75%|███████▌  | 40042/53136 [00:53<00:15, 830.65it/s] 76%|███████▌  | 40127/53136 [00:53<00:15, 817.57it/s] 76%|███████▌  | 40239/53136 [00:53<00:14, 904.22it/s] 76%|███████▌  | 40331/53136 [00:53<00:15, 841.85it/s] 76%|███████▌  | 40417/53136 [00:53<00:16, 792.68it/s] 76%|███████▌  | 40504/53136 [00:54<00:15, 812.69it/s] 76%|███████▋  | 40595/53136 [00:54<00:15, 834.34it/s] 77%|███████▋  | 40683/53136 [00:54<00:14, 846.08it/s] 77%|███████▋  | 40769/53136 [00:54<00:16, 731.96it/s] 77%|███████▋  | 40846/53136 [00:54<00:16, 723.42it/s] 77%|███████▋  | 40942/53136 [00:54<00:15, 785.61it/s] 77%|███████▋  | 41034/53136 [00:54<00:14, 819.95it/s] 77%|███████▋  | 41129/53136 [00:54<00:15, 775.15it/s] 78%|███████▊  | 41209/53136 [00:55<00:16, 744.00it/s] 78%|███████▊  | 41285/53136 [00:55<00:18, 639.26it/s] 78%|███████▊  | 41377/53136 [00:55<00:16, 704.73it/s] 78%|███████▊  | 41472/53136 [00:55<00:15, 766.87it/s] 78%|███████▊  | 41575/53136 [00:55<00:13, 836.48it/s] 78%|███████▊  | 41662/53136 [00:55<00:13, 821.06it/s] 79%|███████▊  | 41747/53136 [00:55<00:14, 791.47it/s] 79%|███████▊  | 41832/53136 [00:55<00:14, 806.81it/s] 79%|███████▉  | 41914/53136 [00:55<00:14, 782.86it/s] 79%|███████▉  | 41994/53136 [00:56<00:14, 759.13it/s] 79%|███████▉  | 42071/53136 [00:56<00:15, 696.02it/s] 79%|███████▉  | 42150/53136 [00:56<00:15, 720.56it/s] 80%|███████▉  | 42245/53136 [00:56<00:14, 775.24it/s] 80%|███████▉  | 42324/53136 [00:56<00:13, 772.88it/s] 80%|███████▉  | 42429/53136 [00:56<00:12, 850.88it/s] 80%|████████  | 42526/53136 [00:56<00:12, 877.96it/s] 80%|████████  | 42615/53136 [00:56<00:12, 857.07it/s] 80%|████████  | 42706/53136 [00:56<00:11, 872.06it/s] 81%|████████  | 42794/53136 [00:57<00:12, 823.14it/s] 81%|████████  | 42878/53136 [00:57<00:12, 824.39it/s] 81%|████████  | 42968/53136 [00:57<00:12, 842.68it/s] 81%|████████  | 43053/53136 [00:57<00:12, 808.39it/s] 81%|████████  | 43135/53136 [00:57<00:15, 633.35it/s] 81%|████████▏ | 43216/53136 [00:57<00:14, 675.56it/s] 82%|████████▏ | 43308/53136 [00:57<00:13, 733.52it/s] 82%|████████▏ | 43392/53136 [00:57<00:13, 735.56it/s] 82%|████████▏ | 43472/53136 [00:57<00:13, 706.05it/s] 82%|████████▏ | 43545/53136 [00:58<00:15, 628.36it/s] 82%|████████▏ | 43634/53136 [00:58<00:13, 685.27it/s] 82%|████████▏ | 43706/53136 [00:58<00:14, 648.69it/s] 82%|████████▏ | 43773/53136 [00:58<00:14, 653.26it/s] 83%|████████▎ | 43857/53136 [00:58<00:13, 699.34it/s] 83%|████████▎ | 43929/53136 [00:58<00:14, 650.43it/s] 83%|████████▎ | 44006/53136 [00:58<00:13, 675.40it/s] 83%|████████▎ | 44089/53136 [00:58<00:12, 716.79it/s] 83%|████████▎ | 44162/53136 [00:59<00:14, 625.95it/s] 83%|████████▎ | 44228/53136 [00:59<00:14, 600.61it/s] 83%|████████▎ | 44325/53136 [00:59<00:12, 684.40it/s] 84%|████████▎ | 44402/53136 [00:59<00:13, 661.36it/s] 84%|████████▎ | 44492/53136 [00:59<00:12, 707.33it/s] 84%|████████▍ | 44589/53136 [00:59<00:11, 776.73it/s] 84%|████████▍ | 44669/53136 [00:59<00:11, 763.35it/s] 84%|████████▍ | 44747/53136 [00:59<00:11, 753.58it/s] 84%|████████▍ | 44841/53136 [00:59<00:10, 801.90it/s] 85%|████████▍ | 44946/53136 [01:00<00:09, 870.64it/s] 85%|████████▍ | 45034/53136 [01:00<00:09, 867.02it/s] 85%|████████▍ | 45137/53136 [01:00<00:08, 913.76it/s] 85%|████████▌ | 45230/53136 [01:00<00:08, 902.57it/s] 85%|████████▌ | 45321/53136 [01:00<00:09, 856.96it/s] 85%|████████▌ | 45408/53136 [01:00<00:09, 782.42it/s] 86%|████████▌ | 45500/53136 [01:00<00:10, 696.24it/s] 86%|████████▌ | 45573/53136 [01:00<00:11, 677.36it/s] 86%|████████▌ | 45643/53136 [01:00<00:11, 676.79it/s] 86%|████████▌ | 45712/53136 [01:01<00:11, 639.90it/s] 86%|████████▌ | 45777/53136 [01:01<00:12, 578.91it/s] 86%|████████▋ | 45863/53136 [01:01<00:11, 647.65it/s] 86%|████████▋ | 45956/53136 [01:01<00:09, 721.89it/s] 87%|████████▋ | 46031/53136 [01:01<00:09, 715.54it/s] 87%|████████▋ | 46132/53136 [01:01<00:08, 793.59it/s] 87%|████████▋ | 46214/53136 [01:01<00:10, 659.59it/s] 87%|████████▋ | 46305/53136 [01:01<00:09, 716.75it/s] 87%|████████▋ | 46412/53136 [01:02<00:08, 808.95it/s] 88%|████████▊ | 46504/53136 [01:02<00:07, 838.52it/s] 88%|████████▊ | 46592/53136 [01:02<00:08, 736.57it/s] 88%|████████▊ | 46692/53136 [01:02<00:08, 801.77it/s] 88%|████████▊ | 46777/53136 [01:02<00:08, 714.97it/s] 88%|████████▊ | 46870/53136 [01:02<00:08, 767.95it/s] 88%|████████▊ | 46952/53136 [01:02<00:07, 777.52it/s] 89%|████████▊ | 47033/53136 [01:02<00:08, 724.14it/s] 89%|████████▊ | 47124/53136 [01:02<00:07, 770.29it/s] 89%|████████▉ | 47204/53136 [01:03<00:07, 768.69it/s] 89%|████████▉ | 47283/53136 [01:03<00:07, 738.86it/s] 89%|████████▉ | 47376/53136 [01:03<00:07, 786.06it/s] 89%|████████▉ | 47456/53136 [01:03<00:07, 788.16it/s] 89%|████████▉ | 47536/53136 [01:03<00:08, 669.55it/s] 90%|████████▉ | 47633/53136 [01:03<00:07, 745.62it/s] 90%|████████▉ | 47720/53136 [01:03<00:06, 777.77it/s] 90%|████████▉ | 47808/53136 [01:03<00:06, 787.75it/s] 90%|█████████ | 47889/53136 [01:04<00:07, 673.83it/s] 90%|█████████ | 47974/53136 [01:04<00:07, 716.86it/s] 90%|█████████ | 48061/53136 [01:04<00:06, 753.90it/s] 91%|█████████ | 48140/53136 [01:04<00:06, 743.07it/s] 91%|█████████ | 48232/53136 [01:04<00:06, 791.19it/s] 91%|█████████ | 48314/53136 [01:04<00:06, 799.08it/s] 91%|█████████ | 48407/53136 [01:04<00:05, 835.90it/s] 91%|█████████▏| 48492/53136 [01:04<00:05, 789.16it/s] 91%|█████████▏| 48573/53136 [01:04<00:05, 786.47it/s] 92%|█████████▏| 48675/53136 [01:04<00:05, 851.19it/s] 92%|█████████▏| 48762/53136 [01:05<00:06, 692.66it/s] 92%|█████████▏| 48840/53136 [01:05<00:06, 650.83it/s] 92%|█████████▏| 48935/53136 [01:05<00:05, 723.92it/s] 92%|█████████▏| 49044/53136 [01:05<00:05, 814.95it/s] 92%|█████████▏| 49130/53136 [01:05<00:04, 820.56it/s] 93%|█████████▎| 49216/53136 [01:05<00:05, 760.54it/s] 93%|█████████▎| 49295/53136 [01:05<00:05, 761.46it/s] 93%|█████████▎| 49374/53136 [01:05<00:05, 725.62it/s] 93%|█████████▎| 49474/53136 [01:06<00:04, 797.47it/s] 93%|█████████▎| 49556/53136 [01:06<00:04, 795.33it/s] 93%|█████████▎| 49637/53136 [01:06<00:04, 796.57it/s] 94%|█████████▎| 49733/53136 [01:06<00:04, 837.52it/s] 94%|█████████▍| 49818/53136 [01:06<00:03, 834.62it/s] 94%|█████████▍| 49930/53136 [01:06<00:03, 913.63it/s] 94%|█████████▍| 50022/53136 [01:06<00:03, 846.24it/s] 94%|█████████▍| 50135/53136 [01:06<00:03, 918.02it/s] 95%|█████████▍| 50229/53136 [01:06<00:03, 894.18it/s] 95%|█████████▍| 50320/53136 [01:07<00:03, 892.33it/s] 95%|█████████▍| 50420/53136 [01:07<00:02, 919.98it/s] 95%|█████████▌| 50513/53136 [01:07<00:03, 826.47it/s] 95%|█████████▌| 50598/53136 [01:07<00:03, 687.56it/s] 95%|█████████▌| 50681/53136 [01:07<00:03, 721.35it/s] 96%|█████████▌| 50790/53136 [01:07<00:02, 815.22it/s] 96%|█████████▌| 50877/53136 [01:07<00:03, 710.48it/s] 96%|█████████▌| 50970/53136 [01:07<00:02, 758.55it/s] 96%|█████████▌| 51051/53136 [01:08<00:02, 709.23it/s] 96%|█████████▌| 51139/53136 [01:08<00:02, 752.13it/s] 96%|█████████▋| 51228/53136 [01:08<00:02, 788.72it/s] 97%|█████████▋| 51310/53136 [01:08<00:02, 778.79it/s] 97%|█████████▋| 51390/53136 [01:08<00:02, 656.03it/s] 97%|█████████▋| 51469/53136 [01:08<00:02, 688.73it/s] 97%|█████████▋| 51542/53136 [01:08<00:02, 633.78it/s] 97%|█████████▋| 51627/53136 [01:08<00:02, 665.80it/s] 97%|█████████▋| 51696/53136 [01:09<00:02, 552.39it/s] 97%|█████████▋| 51783/53136 [01:09<00:02, 626.16it/s] 98%|█████████▊| 51879/53136 [01:09<00:01, 709.46it/s] 98%|█████████▊| 51972/53136 [01:09<00:01, 764.96it/s] 98%|█████████▊| 52056/53136 [01:09<00:01, 763.91it/s] 98%|█████████▊| 52136/53136 [01:09<00:01, 720.51it/s] 98%|█████████▊| 52211/53136 [01:09<00:01, 636.16it/s] 98%|█████████▊| 52278/53136 [01:09<00:01, 519.34it/s] 99%|█████████▊| 52377/53136 [01:10<00:01, 624.08it/s] 99%|█████████▊| 52457/53136 [01:10<00:01, 665.08it/s] 99%|█████████▉| 52530/53136 [01:10<00:00, 647.83it/s] 99%|█████████▉| 52599/53136 [01:10<00:00, 632.51it/s] 99%|█████████▉| 52666/53136 [01:10<00:00, 602.26it/s] 99%|█████████▉| 52760/53136 [01:10<00:00, 684.49it/s]100%|█████████▉| 52871/53136 [01:10<00:00, 797.90it/s]100%|█████████▉| 52954/53136 [01:10<00:00, 735.29it/s]100%|█████████▉| 53031/53136 [01:11<00:00, 618.83it/s]100%|█████████▉| 53098/53136 [01:11<00:00, 625.91it/s]100%|██████████| 53136/53136 [01:11<00:00, 746.58it/s]

gathering stats for n=1
  0%|          | 0/53136 [00:00<?, ?it/s]  1%|          | 359/53136 [00:00<00:14, 3580.90it/s]  1%|▏         | 723/53136 [00:00<00:14, 3602.74it/s]  2%|▏         | 1084/53136 [00:00<00:15, 3369.79it/s]  3%|▎         | 1429/53136 [00:00<00:15, 3396.28it/s]  3%|▎         | 1828/53136 [00:00<00:14, 3602.47it/s]  4%|▍         | 2190/53136 [00:00<00:14, 3502.18it/s]  5%|▍         | 2542/53136 [00:00<00:14, 3431.79it/s]  5%|▌         | 2894/53136 [00:00<00:14, 3458.58it/s]  6%|▌         | 3241/53136 [00:00<00:15, 3242.48it/s]  7%|▋         | 3568/53136 [00:01<00:15, 3221.96it/s]  7%|▋         | 3892/53136 [00:01<00:15, 3143.91it/s]  8%|▊         | 4227/53136 [00:01<00:15, 3201.31it/s]  9%|▊         | 4549/53136 [00:01<00:15, 3205.34it/s]  9%|▉         | 4871/53136 [00:01<00:15, 3147.52it/s] 10%|▉         | 5187/53136 [00:01<00:15, 3074.40it/s] 10%|█         | 5519/53136 [00:01<00:15, 3143.74it/s] 11%|█         | 5855/53136 [00:01<00:14, 3203.47it/s] 12%|█▏        | 6186/53136 [00:01<00:14, 3232.40it/s] 12%|█▏        | 6551/53136 [00:01<00:13, 3352.62it/s] 13%|█▎        | 6907/53136 [00:02<00:13, 3408.93it/s] 14%|█▎        | 7249/53136 [00:02<00:13, 3329.60it/s] 14%|█▍        | 7583/53136 [00:02<00:14, 3172.24it/s] 15%|█▌        | 7991/53136 [00:02<00:13, 3428.66it/s] 16%|█▌        | 8337/53136 [00:02<00:13, 3352.28it/s] 16%|█▋        | 8698/53136 [00:02<00:12, 3418.61it/s] 17%|█▋        | 9054/53136 [00:02<00:12, 3455.60it/s] 18%|█▊        | 9458/53136 [00:02<00:12, 3623.87it/s] 18%|█▊        | 9822/53136 [00:02<00:12, 3496.35it/s] 19%|█▉        | 10207/53136 [00:03<00:11, 3584.12it/s] 20%|█▉        | 10567/53136 [00:03<00:12, 3444.94it/s] 21%|██        | 10967/53136 [00:03<00:11, 3602.89it/s] 21%|██▏       | 11330/53136 [00:03<00:11, 3516.50it/s] 22%|██▏       | 11684/53136 [00:03<00:11, 3477.78it/s] 23%|██▎       | 12063/53136 [00:03<00:11, 3567.56it/s] 23%|██▎       | 12457/53136 [00:03<00:11, 3672.90it/s] 24%|██▍       | 12826/53136 [00:03<00:12, 3323.07it/s] 25%|██▍       | 13201/53136 [00:03<00:11, 3439.65it/s] 26%|██▌       | 13551/53136 [00:04<00:11, 3377.11it/s] 26%|██▌       | 13893/53136 [00:04<00:11, 3314.99it/s] 27%|██▋       | 14228/53136 [00:04<00:11, 3282.25it/s] 27%|██▋       | 14559/53136 [00:04<00:11, 3214.91it/s] 28%|██▊       | 14931/53136 [00:04<00:11, 3357.61it/s] 29%|██▊       | 15269/53136 [00:04<00:12, 3133.98it/s] 29%|██▉       | 15587/53136 [00:04<00:12, 3093.95it/s] 30%|██▉       | 15899/53136 [00:04<00:12, 3075.11it/s] 31%|███       | 16211/53136 [00:04<00:12, 3072.02it/s] 31%|███       | 16565/53136 [00:04<00:11, 3197.25it/s] 32%|███▏      | 16926/53136 [00:05<00:10, 3313.73it/s] 33%|███▎      | 17273/53136 [00:05<00:10, 3342.53it/s] 33%|███▎      | 17609/53136 [00:05<00:10, 3313.68it/s] 34%|███▍      | 17941/53136 [00:05<00:10, 3218.65it/s] 34%|███▍      | 18264/53136 [00:05<00:11, 3153.18it/s] 35%|███▍      | 18581/53136 [00:05<00:11, 2993.63it/s] 36%|███▌      | 18921/53136 [00:05<00:11, 3096.46it/s] 36%|███▋      | 19265/53136 [00:05<00:10, 3152.12it/s] 37%|███▋      | 19582/53136 [00:05<00:10, 3119.82it/s] 37%|███▋      | 19897/53136 [00:06<00:10, 3126.36it/s] 38%|███▊      | 20211/53136 [00:06<00:10, 3070.97it/s] 39%|███▊      | 20541/53136 [00:06<00:10, 3047.92it/s] 39%|███▉      | 20867/53136 [00:06<00:10, 3106.37it/s] 40%|███▉      | 21179/53136 [00:06<00:10, 3023.85it/s] 40%|████      | 21506/53136 [00:06<00:10, 3093.87it/s] 41%|████      | 21817/53136 [00:06<00:10, 2854.07it/s] 42%|████▏     | 22125/53136 [00:06<00:10, 2916.30it/s] 42%|████▏     | 22440/53136 [00:06<00:10, 2982.22it/s] 43%|████▎     | 22768/53136 [00:06<00:09, 3054.31it/s] 43%|████▎     | 23076/53136 [00:07<00:09, 3015.96it/s] 44%|████▍     | 23396/53136 [00:07<00:09, 3069.11it/s] 45%|████▍     | 23705/53136 [00:07<00:09, 2957.43it/s] 45%|████▌     | 24035/53136 [00:07<00:09, 3054.07it/s] 46%|████▌     | 24351/53136 [00:07<00:09, 3083.92it/s] 46%|████▋     | 24701/53136 [00:07<00:08, 3205.81it/s] 47%|████▋     | 25023/53136 [00:07<00:08, 3160.96it/s] 48%|████▊     | 25371/53136 [00:07<00:08, 3247.17it/s] 48%|████▊     | 25697/53136 [00:07<00:08, 3161.48it/s] 49%|████▉     | 26031/53136 [00:08<00:08, 3205.22it/s] 50%|████▉     | 26353/53136 [00:08<00:08, 3053.11it/s] 50%|█████     | 26675/53136 [00:08<00:08, 3090.31it/s] 51%|█████     | 26998/53136 [00:08<00:08, 3130.02it/s] 51%|█████▏    | 27313/53136 [00:08<00:08, 3117.15it/s] 52%|█████▏    | 27626/53136 [00:08<00:08, 3052.97it/s] 53%|█████▎    | 27952/53136 [00:08<00:08, 3110.28it/s] 53%|█████▎    | 28264/53136 [00:08<00:08, 3055.94it/s] 54%|█████▍    | 28612/53136 [00:08<00:07, 3171.11it/s] 55%|█████▍    | 28966/53136 [00:08<00:07, 3278.89it/s] 55%|█████▌    | 29295/53136 [00:09<00:07, 3201.48it/s] 56%|█████▌    | 29623/53136 [00:09<00:07, 3217.83it/s] 56%|█████▋    | 29946/53136 [00:09<00:07, 3080.11it/s] 57%|█████▋    | 30256/53136 [00:09<00:07, 2942.96it/s] 57%|█████▋    | 30553/53136 [00:09<00:07, 2930.84it/s] 58%|█████▊    | 30864/53136 [00:09<00:07, 2981.77it/s] 59%|█████▊    | 31164/53136 [00:09<00:07, 2960.97it/s] 59%|█████▉    | 31569/53136 [00:09<00:06, 3273.88it/s] 60%|██████    | 31931/53136 [00:09<00:06, 3335.16it/s] 61%|██████    | 32298/53136 [00:10<00:06, 3424.47it/s] 61%|██████▏   | 32667/53136 [00:10<00:05, 3501.69it/s] 62%|██████▏   | 33054/53136 [00:10<00:05, 3602.57it/s] 63%|██████▎   | 33415/53136 [00:10<00:05, 3458.46it/s] 64%|██████▎   | 33763/53136 [00:10<00:05, 3237.42it/s] 64%|██████▍   | 34098/53136 [00:10<00:05, 3267.70it/s] 65%|██████▍   | 34428/53136 [00:10<00:05, 3214.15it/s] 65%|██████▌   | 34752/53136 [00:10<00:05, 3212.43it/s] 66%|██████▌   | 35075/53136 [00:10<00:06, 2944.29it/s] 67%|██████▋   | 35375/53136 [00:10<00:06, 2920.38it/s] 67%|██████▋   | 35684/53136 [00:11<00:05, 2964.26it/s] 68%|██████▊   | 35983/53136 [00:11<00:05, 2949.44it/s] 68%|██████▊   | 36280/53136 [00:11<00:05, 2890.58it/s] 69%|██████▉   | 36612/53136 [00:11<00:05, 3011.47it/s] 69%|██████▉   | 36915/53136 [00:11<00:05, 2936.02it/s] 70%|███████   | 37210/53136 [00:11<00:05, 2891.88it/s] 71%|███████   | 37521/53136 [00:11<00:05, 2952.08it/s] 71%|███████   | 37857/53136 [00:11<00:04, 3056.98it/s] 72%|███████▏  | 38164/53136 [00:11<00:05, 2944.99it/s] 72%|███████▏  | 38460/53136 [00:12<00:04, 2936.26it/s] 73%|███████▎  | 38755/53136 [00:12<00:04, 2937.62it/s] 74%|███████▎  | 39075/53136 [00:12<00:04, 3004.90it/s] 74%|███████▍  | 39413/53136 [00:12<00:04, 3114.94it/s] 75%|███████▍  | 39726/53136 [00:12<00:04, 2989.39it/s] 75%|███████▌  | 40061/53136 [00:12<00:04, 3091.75it/s] 76%|███████▌  | 40380/53136 [00:12<00:04, 3098.91it/s] 77%|███████▋  | 40724/53136 [00:12<00:03, 3197.28it/s] 77%|███████▋  | 41048/53136 [00:12<00:03, 3206.71it/s] 78%|███████▊  | 41370/53136 [00:12<00:03, 3135.43it/s] 78%|███████▊  | 41710/53136 [00:13<00:03, 3210.10it/s] 79%|███████▉  | 42032/53136 [00:13<00:03, 3103.85it/s] 80%|███████▉  | 42369/53136 [00:13<00:03, 3178.51it/s] 80%|████████  | 42726/53136 [00:13<00:03, 3291.06it/s] 81%|████████  | 43057/53136 [00:13<00:03, 3278.15it/s] 82%|████████▏ | 43386/53136 [00:13<00:03, 3241.34it/s] 82%|████████▏ | 43711/53136 [00:13<00:03, 3092.11it/s] 83%|████████▎ | 44022/53136 [00:13<00:02, 3093.99it/s] 83%|████████▎ | 44333/53136 [00:13<00:02, 3010.55it/s] 84%|████████▍ | 44666/53136 [00:14<00:02, 3101.81it/s] 85%|████████▍ | 45002/53136 [00:14<00:02, 3176.69it/s] 85%|████████▌ | 45339/53136 [00:14<00:02, 3226.84it/s] 86%|████████▌ | 45663/53136 [00:14<00:02, 3084.00it/s] 87%|████████▋ | 45992/53136 [00:14<00:02, 3142.97it/s] 87%|████████▋ | 46308/53136 [00:14<00:02, 3110.12it/s] 88%|████████▊ | 46670/53136 [00:14<00:01, 3257.47it/s] 88%|████████▊ | 46997/53136 [00:14<00:01, 3235.40it/s] 89%|████████▉ | 47322/53136 [00:14<00:01, 3190.93it/s] 90%|████████▉ | 47642/53136 [00:14<00:01, 3182.31it/s] 90%|█████████ | 47961/53136 [00:15<00:01, 3044.02it/s] 91%|█████████ | 48267/53136 [00:15<00:01, 3021.59it/s] 91%|█████████▏| 48574/53136 [00:15<00:01, 3032.99it/s] 92%|█████████▏| 48878/53136 [00:15<00:01, 2910.44it/s] 93%|█████████▎| 49194/53136 [00:15<00:01, 2972.56it/s] 93%|█████████▎| 49493/53136 [00:15<00:01, 2920.09it/s] 94%|█████████▎| 49786/53136 [00:15<00:01, 2912.10it/s] 94%|█████████▍| 50160/53136 [00:15<00:00, 3152.16it/s] 95%|█████████▍| 50477/53136 [00:15<00:00, 2804.55it/s] 96%|█████████▌| 50766/53136 [00:16<00:00, 2805.79it/s] 96%|█████████▌| 51053/53136 [00:16<00:00, 2819.89it/s] 97%|█████████▋| 51368/53136 [00:16<00:00, 2859.02it/s] 97%|█████████▋| 51657/53136 [00:16<00:00, 2706.33it/s] 98%|█████████▊| 52008/53136 [00:16<00:00, 2925.64it/s] 98%|█████████▊| 52305/53136 [00:16<00:00, 2648.90it/s] 99%|█████████▉| 52618/53136 [00:16<00:00, 2763.98it/s]100%|█████████▉| 52934/53136 [00:16<00:00, 2867.23it/s]100%|██████████| 53136/53136 [00:16<00:00, 3149.36it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 56.70it/s]2022-03-15 16:43:11 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=35920, bias=False)
  )
)
2022-03-15 16:43:11 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 16:43:11 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 16:43:11 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 16:43:11 | INFO | fairseq_cli.train | num. shared model params: 37,305,344 (num. trained: 37,305,344)
2022-03-15 16:43:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 16:43:11 | INFO | fairseq.data.data_utils | loaded 2,558 examples from: data-bin/ru/valid
2022-03-15 16:43:11 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 16:43:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:43:11 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 16:43:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:43:11 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 16:43:11 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 16:43:11 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-15 16:43:11 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-15 16:43:11 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 16:43:11 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
2022-03-15 16:43:11 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 16:43:11 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 16:43:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 16:43:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:43:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 16:43:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 16:49:15 | INFO | train_inner | epoch 001:    104 / 407 loss=14.824, ppl=28998.1, wps=19273.4, ups=0.29, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.221, loss_scale=8, train_wall=337, gb_free=9.6, wall=364
2022-03-15 16:54:54 | INFO | train_inner | epoch 001:    204 / 407 loss=13.342, ppl=10384.5, wps=19372.9, ups=0.3, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.656, loss_scale=16, train_wall=313, gb_free=9.6, wall=703
2022-03-15 17:00:31 | INFO | train_inner | epoch 001:    304 / 407 loss=12.458, ppl=5627.79, wps=19414.9, ups=0.3, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.408, loss_scale=32, train_wall=312, gb_free=9.6, wall=1040
2022-03-15 17:06:10 | INFO | train_inner | epoch 001:    404 / 407 loss=12.009, ppl=4122.15, wps=19357.3, ups=0.3, wpb=65534.2, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.394, loss_scale=64, train_wall=313, gb_free=9.6, wall=1379
2022-03-15 17:06:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:07:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.826 | ppl 3630.61 | wps 30557.7 | wpb 511.9 | bsz 1 | num_updates 403
2022-03-15 17:07:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 403 updates
2022-03-15 17:07:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 17:07:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 17:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 1 @ 403 updates, score 11.826) (writing took 2.5047764140181243 seconds)
2022-03-15 17:07:05 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 17:07:05 | INFO | train | epoch 001 | loss 13.15 | ppl 9089.34 | wps 18716 | ups 0.29 | wpb 65492.3 | bsz 127.9 | num_updates 403 | lr 5.04649e-05 | gnorm 0.916 | loss_scale 64 | train_wall 1283 | gb_free 9.6 | wall 1434
KL Stats: Epoch 1 Divergences: Uniform: 0.7065317630100449 Unigram: 0.8193413667913124
2022-03-15 17:07:05 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 17:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:10:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:12:36 | INFO | train_inner | epoch 002:     98 / 407 loss=11.82, ppl=3614.87, wps=16922.7, ups=0.26, wpb=65360.1, bsz=127.7, num_updates=500, lr=6.25875e-05, gnorm=0.412, loss_scale=32, train_wall=314, gb_free=9.6, wall=1765
2022-03-15 17:18:18 | INFO | train_inner | epoch 002:    198 / 407 loss=11.591, ppl=3084.94, wps=19181.2, ups=0.29, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.451, loss_scale=64, train_wall=315, gb_free=9.6, wall=2107
2022-03-15 17:22:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:23:59 | INFO | train_inner | epoch 002:    299 / 407 loss=11.297, ppl=2515.32, wps=19193.9, ups=0.29, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.45, loss_scale=32, train_wall=315, gb_free=9.6, wall=2448
2022-03-15 17:29:37 | INFO | train_inner | epoch 002:    399 / 407 loss=10.955, ppl=1985.08, wps=19367.2, ups=0.3, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.519, loss_scale=64, train_wall=313, gb_free=9.6, wall=2786
2022-03-15 17:29:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:30:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.553 | ppl 1502.62 | wps 30614.8 | wpb 511.9 | bsz 1 | num_updates 807 | best_loss 10.553
2022-03-15 17:30:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 807 updates
2022-03-15 17:30:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 17:30:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 17:30:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 2 @ 807 updates, score 10.553) (writing took 2.470995098934509 seconds)
2022-03-15 17:30:50 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 17:30:50 | INFO | train | epoch 002 | loss 11.401 | ppl 2704.07 | wps 18577.4 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 807 | lr 0.000100955 | gnorm 0.459 | loss_scale 32 | train_wall 1272 | gb_free 9.6 | wall 2859
KL Stats: Epoch 2 Divergences: Uniform: 1.4655501469405163 Unigram: 0.5486650374513701
2022-03-15 17:30:50 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 17:30:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:36:02 | INFO | train_inner | epoch 003:     93 / 407 loss=10.602, ppl=1553.95, wps=17003, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=900, lr=0.000112578, gnorm=0.532, loss_scale=32, train_wall=312, gb_free=9.6, wall=3171
2022-03-15 17:37:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:41:47 | INFO | train_inner | epoch 003:    194 / 407 loss=10.339, ppl=1295.34, wps=18991.8, ups=0.29, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.534, loss_scale=32, train_wall=319, gb_free=9.6, wall=3516
2022-03-15 17:46:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:47:31 | INFO | train_inner | epoch 003:    295 / 407 loss=10.113, ppl=1107.53, wps=19037.3, ups=0.29, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.603, loss_scale=32, train_wall=318, gb_free=9.6, wall=3860
2022-03-15 17:53:10 | INFO | train_inner | epoch 003:    395 / 407 loss=9.904, ppl=958.24, wps=19370.7, ups=0.3, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.616, loss_scale=32, train_wall=312, gb_free=9.6, wall=4198
2022-03-15 17:53:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:54:33 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.577 | ppl 763.9 | wps 30701.4 | wpb 511.9 | bsz 1 | num_updates 1211 | best_loss 9.577
2022-03-15 17:54:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1211 updates
2022-03-15 17:54:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 17:54:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 17:54:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 3 @ 1211 updates, score 9.577) (writing took 2.434882879955694 seconds)
2022-03-15 17:54:35 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 17:54:35 | INFO | train | epoch 003 | loss 10.218 | ppl 1191.12 | wps 18559.8 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 1211 | lr 0.000151445 | gnorm 0.575 | loss_scale 32 | train_wall 1274 | gb_free 9.6 | wall 4284
KL Stats: Epoch 3 Divergences: Uniform: 2.081157933874404 Unigram: 1.47926901096805
2022-03-15 17:54:35 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 17:54:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:59:37 | INFO | train_inner | epoch 004:     89 / 407 loss=9.691, ppl=826.67, wps=16876, ups=0.26, wpb=65360.1, bsz=127.7, num_updates=1300, lr=0.000162568, gnorm=0.641, loss_scale=32, train_wall=315, gb_free=9.6, wall=4586
2022-03-15 18:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:05:20 | INFO | train_inner | epoch 004:    190 / 407 loss=9.502, ppl=725.06, wps=19094.7, ups=0.29, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.697, loss_scale=32, train_wall=317, gb_free=9.6, wall=4929
2022-03-15 18:09:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:11:00 | INFO | train_inner | epoch 004:    291 / 407 loss=9.313, ppl=635.85, wps=19251.9, ups=0.29, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.684, loss_scale=32, train_wall=314, gb_free=9.6, wall=5269
2022-03-15 18:16:39 | INFO | train_inner | epoch 004:    391 / 407 loss=9.134, ppl=561.71, wps=19346.8, ups=0.3, wpb=65536, bsz=128, num_updates=1600, lr=0.00020006, gnorm=0.707, loss_scale=32, train_wall=313, gb_free=9.6, wall=5608
2022-03-15 18:17:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:17:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:18:16 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.81 | ppl 448.92 | wps 30743.6 | wpb 511.9 | bsz 1 | num_updates 1615 | best_loss 8.81
2022-03-15 18:18:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1615 updates
2022-03-15 18:18:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 18:18:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 18:18:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 4 @ 1615 updates, score 8.81) (writing took 2.5591956289717928 seconds)
2022-03-15 18:18:19 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 18:18:19 | INFO | train | epoch 004 | loss 9.386 | ppl 669.28 | wps 18591.2 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 1615 | lr 0.000201935 | gnorm 0.683 | loss_scale 32 | train_wall 1271 | gb_free 9.6 | wall 5707
KL Stats: Epoch 4 Divergences: Uniform: 2.633722519760976 Unigram: 2.0214488123960885
2022-03-15 18:18:19 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 18:18:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:22:59 | INFO | train_inner | epoch 005:     85 / 407 loss=8.956, ppl=496.46, wps=17218.9, ups=0.26, wpb=65360.1, bsz=127.7, num_updates=1700, lr=0.000212558, gnorm=0.698, loss_scale=32, train_wall=308, gb_free=9.6, wall=5988
2022-03-15 18:25:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:28:37 | INFO | train_inner | epoch 005:    186 / 407 loss=8.801, ppl=445.88, wps=19394.1, ups=0.3, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.748, loss_scale=32, train_wall=312, gb_free=9.6, wall=6326
2022-03-15 18:34:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:34:22 | INFO | train_inner | epoch 005:    287 / 407 loss=8.649, ppl=401.32, wps=18970.9, ups=0.29, wpb=65534.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.727, loss_scale=32, train_wall=319, gb_free=9.6, wall=6671
2022-03-15 18:40:03 | INFO | train_inner | epoch 005:    387 / 407 loss=8.524, ppl=368.02, wps=19203.5, ups=0.29, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.708, loss_scale=32, train_wall=315, gb_free=9.6, wall=7012
2022-03-15 18:41:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:41:55 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.2 | ppl 294.11 | wps 30175.7 | wpb 511.9 | bsz 1 | num_updates 2020 | best_loss 8.2
2022-03-15 18:41:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2020 updates
2022-03-15 18:41:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 18:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 18:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 5 @ 2020 updates, score 8.2) (writing took 2.5608235530089587 seconds)
2022-03-15 18:41:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 18:41:57 | INFO | train | epoch 005 | loss 8.706 | ppl 417.73 | wps 18695.4 | ups 0.29 | wpb 65492.6 | bsz 127.9 | num_updates 2020 | lr 0.00025255 | gnorm 0.723 | loss_scale 32 | train_wall 1267 | gb_free 9.6 | wall 7126
KL Stats: Epoch 5 Divergences: Uniform: 3.107844680263635 Unigram: 2.393276668386768
2022-03-15 18:41:57 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 18:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:43:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:46:33 | INFO | train_inner | epoch 006:     81 / 407 loss=8.374, ppl=331.71, wps=16800.2, ups=0.26, wpb=65360.1, bsz=127.7, num_updates=2100, lr=0.000262548, gnorm=0.72, loss_scale=32, train_wall=316, gb_free=9.6, wall=7401
2022-03-15 18:51:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:52:14 | INFO | train_inner | epoch 006:    182 / 407 loss=8.246, ppl=303.61, wps=19203.7, ups=0.29, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.728, loss_scale=32, train_wall=315, gb_free=9.6, wall=7743
2022-03-15 18:57:52 | INFO | train_inner | epoch 006:    282 / 407 loss=8.144, ppl=282.88, wps=19363.1, ups=0.3, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.706, loss_scale=32, train_wall=313, gb_free=9.6, wall=8081
2022-03-15 18:59:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:03:37 | INFO | train_inner | epoch 006:    383 / 407 loss=8.044, ppl=263.99, wps=19025.2, ups=0.29, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.704, loss_scale=32, train_wall=318, gb_free=9.6, wall=8426
2022-03-15 19:04:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:05:42 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.776 | ppl 219.12 | wps 30549.5 | wpb 511.9 | bsz 1 | num_updates 2424 | best_loss 7.776
2022-03-15 19:05:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2424 updates
2022-03-15 19:05:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 19:05:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 19:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 6 @ 2424 updates, score 7.776) (writing took 2.6233747099759057 seconds)
2022-03-15 19:05:44 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 19:05:44 | INFO | train | epoch 006 | loss 8.179 | ppl 289.74 | wps 18540.4 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 2424 | lr 0.000303039 | gnorm 0.707 | loss_scale 32 | train_wall 1275 | gb_free 9.6 | wall 8553
KL Stats: Epoch 6 Divergences: Uniform: 3.50358492451517 Unigram: 2.6891736624151146
2022-03-15 19:05:44 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 19:05:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:08:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:10:07 | INFO | train_inner | epoch 007:     77 / 407 loss=7.917, ppl=241.76, wps=16755, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=2500, lr=0.000312538, gnorm=0.682, loss_scale=32, train_wall=317, gb_free=9.6, wall=8816
2022-03-15 19:11:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:15:49 | INFO | train_inner | epoch 007:    178 / 407 loss=7.843, ppl=229.63, wps=19161.3, ups=0.29, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.681, loss_scale=16, train_wall=316, gb_free=9.6, wall=9158
2022-03-15 19:21:26 | INFO | train_inner | epoch 007:    278 / 407 loss=7.77, ppl=218.22, wps=19442, ups=0.3, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.664, loss_scale=32, train_wall=311, gb_free=9.6, wall=9495
2022-03-15 19:24:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:27:11 | INFO | train_inner | epoch 007:    379 / 407 loss=7.69, ppl=206.48, wps=19017.7, ups=0.29, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.669, loss_scale=16, train_wall=318, gb_free=9.6, wall=9839
2022-03-15 19:28:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:29:27 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.449 | ppl 174.75 | wps 30978.6 | wpb 511.9 | bsz 1 | num_updates 2828 | best_loss 7.449
2022-03-15 19:29:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2828 updates
2022-03-15 19:29:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 19:29:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 19:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 7 @ 2828 updates, score 7.449) (writing took 2.6022053830092773 seconds)
2022-03-15 19:29:30 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 19:29:30 | INFO | train | epoch 007 | loss 7.783 | ppl 220.23 | wps 18563.1 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 2828 | lr 0.000353529 | gnorm 0.672 | loss_scale 16 | train_wall 1274 | gb_free 9.6 | wall 9979
KL Stats: Epoch 7 Divergences: Uniform: 3.7969849711961676 Unigram: 2.9063609302223092
2022-03-15 19:29:30 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 19:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:33:29 | INFO | train_inner | epoch 008:     72 / 407 loss=7.586, ppl=192.11, wps=17256.7, ups=0.26, wpb=65360.1, bsz=127.7, num_updates=2900, lr=0.000362528, gnorm=0.637, loss_scale=32, train_wall=307, gb_free=9.6, wall=10218
2022-03-15 19:34:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:39:06 | INFO | train_inner | epoch 008:    173 / 407 loss=7.523, ppl=183.89, wps=19448.2, ups=0.3, wpb=65534.2, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.631, loss_scale=16, train_wall=311, gb_free=9.6, wall=10555
2022-03-15 19:44:46 | INFO | train_inner | epoch 008:    273 / 407 loss=7.471, ppl=177.43, wps=19307.6, ups=0.29, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.631, loss_scale=32, train_wall=313, gb_free=9.6, wall=10895
2022-03-15 19:48:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:50:26 | INFO | train_inner | epoch 008:    374 / 407 loss=7.415, ppl=170.61, wps=19250.3, ups=0.29, wpb=65536, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.604, loss_scale=32, train_wall=314, gb_free=9.6, wall=11235
2022-03-15 19:51:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:53:00 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.198 | ppl 146.82 | wps 30760.7 | wpb 511.9 | bsz 1 | num_updates 3232 | best_loss 7.198
2022-03-15 19:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3232 updates
2022-03-15 19:53:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 19:53:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 19:53:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 8 @ 3232 updates, score 7.198) (writing took 2.6294286740012467 seconds)
2022-03-15 19:53:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 19:53:03 | INFO | train | epoch 008 | loss 7.477 | ppl 178.19 | wps 18724.2 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 3232 | lr 0.000404019 | gnorm 0.625 | loss_scale 16 | train_wall 1261 | gb_free 9.6 | wall 11392
KL Stats: Epoch 8 Divergences: Uniform: 4.000265661944323 Unigram: 3.062367860521391
2022-03-15 19:53:03 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 19:53:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:56:50 | INFO | train_inner | epoch 009:     68 / 407 loss=7.318, ppl=159.59, wps=17036.3, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=3300, lr=0.000412518, gnorm=0.619, loss_scale=16, train_wall=311, gb_free=9.6, wall=11619
2022-03-15 20:02:32 | INFO | train_inner | epoch 009:    168 / 407 loss=7.257, ppl=153, wps=19150, ups=0.29, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.597, loss_scale=32, train_wall=316, gb_free=9.6, wall=11961
2022-03-15 20:06:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 20:08:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:08:19 | INFO | train_inner | epoch 009:    270 / 407 loss=7.213, ppl=148.34, wps=18888, ups=0.29, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.592, loss_scale=16, train_wall=320, gb_free=9.6, wall=12308
2022-03-15 20:13:57 | INFO | train_inner | epoch 009:    370 / 407 loss=7.162, ppl=143.24, wps=19388.2, ups=0.3, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.581, loss_scale=16, train_wall=312, gb_free=9.6, wall=12646
2022-03-15 20:16:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:16:45 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.937 | ppl 122.55 | wps 30452 | wpb 511.9 | bsz 1 | num_updates 3637 | best_loss 6.937
2022-03-15 20:16:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3637 updates
2022-03-15 20:16:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 20:16:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 20:16:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 9 @ 3637 updates, score 6.937) (writing took 2.440384653978981 seconds)
2022-03-15 20:16:48 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 20:16:48 | INFO | train | epoch 009 | loss 7.217 | ppl 148.82 | wps 18612.9 | ups 0.28 | wpb 65492.6 | bsz 127.9 | num_updates 3637 | lr 0.000454634 | gnorm 0.595 | loss_scale 32 | train_wall 1272 | gb_free 9.6 | wall 12817
KL Stats: Epoch 9 Divergences: Uniform: 4.157190798482631 Unigram: 3.1829111590017294
2022-03-15 20:16:48 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 20:16:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:20:22 | INFO | train_inner | epoch 010:     63 / 407 loss=7.069, ppl=134.3, wps=16961.7, ups=0.26, wpb=65360.1, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.578, loss_scale=32, train_wall=313, gb_free=9.6, wall=13031
2022-03-15 20:21:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:26:08 | INFO | train_inner | epoch 010:    164 / 407 loss=7.017, ppl=129.49, wps=18944.1, ups=0.29, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.574, loss_scale=16, train_wall=319, gb_free=9.6, wall=13377
2022-03-15 20:29:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:31:55 | INFO | train_inner | epoch 010:    265 / 407 loss=6.98, ppl=126.26, wps=18881.8, ups=0.29, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.555, loss_scale=16, train_wall=320, gb_free=9.6, wall=13724
2022-03-15 20:37:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:37:41 | INFO | train_inner | epoch 010:    366 / 407 loss=6.938, ppl=122.63, wps=18954, ups=0.29, wpb=65534.2, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.552, loss_scale=16, train_wall=319, gb_free=9.6, wall=14070
2022-03-15 20:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:40:43 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.725 | ppl 105.8 | wps 30376.1 | wpb 511.9 | bsz 1 | num_updates 4041 | best_loss 6.725
2022-03-15 20:40:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4041 updates
2022-03-15 20:40:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 20:40:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 20:40:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 10 @ 4041 updates, score 6.725) (writing took 2.4210497790481895 seconds)
2022-03-15 20:40:46 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 20:40:46 | INFO | train | epoch 010 | loss 6.981 | ppl 126.32 | wps 18400.2 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 4041 | lr 0.000497457 | gnorm 0.562 | loss_scale 16 | train_wall 1285 | gb_free 9.6 | wall 14255
KL Stats: Epoch 10 Divergences: Uniform: 4.281212013593071 Unigram: 3.2839611021852813
2022-03-15 20:40:46 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 20:40:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:44:07 | INFO | train_inner | epoch 011:     59 / 407 loss=6.851, ppl=115.47, wps=16949.6, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.555, loss_scale=16, train_wall=313, gb_free=9.6, wall=14456
2022-03-15 20:46:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:49:49 | INFO | train_inner | epoch 011:    160 / 407 loss=6.802, ppl=111.56, wps=19132, ups=0.29, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.536, loss_scale=16, train_wall=316, gb_free=9.6, wall=14798
2022-03-15 20:55:32 | INFO | train_inner | epoch 011:    260 / 407 loss=6.758, ppl=108.26, wps=19146.8, ups=0.29, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.53, loss_scale=32, train_wall=316, gb_free=9.6, wall=15140
2022-03-15 20:59:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:01:17 | INFO | train_inner | epoch 011:    361 / 407 loss=6.738, ppl=106.73, wps=18962.7, ups=0.29, wpb=65534.2, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.538, loss_scale=16, train_wall=319, gb_free=9.6, wall=15486
2022-03-15 21:03:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:04:38 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.55 | ppl 93.7 | wps 30239.9 | wpb 511.9 | bsz 1 | num_updates 4446 | best_loss 6.55
2022-03-15 21:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4446 updates
2022-03-15 21:04:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 21:04:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 21:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 11 @ 4446 updates, score 6.55) (writing took 2.5250705839134753 seconds)
2022-03-15 21:04:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 21:04:40 | INFO | train | epoch 011 | loss 6.767 | ppl 108.91 | wps 18490 | ups 0.28 | wpb 65492.6 | bsz 127.9 | num_updates 4446 | lr 0.000474259 | gnorm 0.536 | loss_scale 16 | train_wall 1281 | gb_free 9.6 | wall 15689
KL Stats: Epoch 11 Divergences: Uniform: 4.3908531096358105 Unigram: 3.374768235785226
2022-03-15 21:04:40 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 21:04:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:07:44 | INFO | train_inner | epoch 012:     54 / 407 loss=6.661, ppl=101.16, wps=16889.4, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.511, loss_scale=32, train_wall=314, gb_free=9.6, wall=15873
2022-03-15 21:10:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:13:29 | INFO | train_inner | epoch 012:    155 / 407 loss=6.609, ppl=97.62, wps=18980.2, ups=0.29, wpb=65534.2, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.512, loss_scale=16, train_wall=319, gb_free=9.6, wall=16218
2022-03-15 21:19:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:19:16 | INFO | train_inner | epoch 012:    256 / 407 loss=6.605, ppl=97.34, wps=18886.6, ups=0.29, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.507, loss_scale=16, train_wall=320, gb_free=9.6, wall=16565
2022-03-15 21:24:59 | INFO | train_inner | epoch 012:    356 / 407 loss=6.579, ppl=95.64, wps=19147, ups=0.29, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.506, loss_scale=16, train_wall=316, gb_free=9.6, wall=16908
2022-03-15 21:27:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:28:35 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.444 | ppl 87.06 | wps 30447.2 | wpb 511.9 | bsz 1 | num_updates 4851 | best_loss 6.444
2022-03-15 21:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4851 updates
2022-03-15 21:28:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 21:28:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 21:28:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 12 @ 4851 updates, score 6.444) (writing took 2.394077775068581 seconds)
2022-03-15 21:28:37 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 21:28:37 | INFO | train | epoch 012 | loss 6.597 | ppl 96.83 | wps 18462.7 | ups 0.28 | wpb 65492.6 | bsz 127.9 | num_updates 4851 | lr 0.00045403 | gnorm 0.508 | loss_scale 32 | train_wall 1283 | gb_free 9.6 | wall 17126
KL Stats: Epoch 12 Divergences: Uniform: 4.479397389340489 Unigram: 3.449746013386363
2022-03-15 21:28:37 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 21:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:31:23 | INFO | train_inner | epoch 013:     49 / 407 loss=6.525, ppl=92.1, wps=17000.1, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.499, loss_scale=32, train_wall=312, gb_free=9.6, wall=17292
2022-03-15 21:34:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:37:05 | INFO | train_inner | epoch 013:    150 / 407 loss=6.476, ppl=89.04, wps=19159.8, ups=0.29, wpb=65534.2, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.493, loss_scale=32, train_wall=316, gb_free=9.6, wall=17634
2022-03-15 21:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:42:50 | INFO | train_inner | epoch 013:    251 / 407 loss=6.475, ppl=88.94, wps=19034.5, ups=0.29, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.485, loss_scale=32, train_wall=318, gb_free=9.6, wall=17978
2022-03-15 21:48:33 | INFO | train_inner | epoch 013:    351 / 407 loss=6.459, ppl=87.95, wps=19105.8, ups=0.29, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.494, loss_scale=32, train_wall=317, gb_free=9.6, wall=18321
2022-03-15 21:49:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:52:28 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.346 | ppl 81.37 | wps 30304.1 | wpb 511.9 | bsz 1 | num_updates 5254 | best_loss 6.346
2022-03-15 21:52:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5254 updates
2022-03-15 21:52:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 21:52:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 21:52:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 13 @ 5254 updates, score 6.346) (writing took 2.4996681259945035 seconds)
2022-03-15 21:52:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 21:52:30 | INFO | train | epoch 013 | loss 6.468 | ppl 88.5 | wps 18416.2 | ups 0.28 | wpb 65492.3 | bsz 127.9 | num_updates 5254 | lr 0.00043627 | gnorm 0.491 | loss_scale 16 | train_wall 1280 | gb_free 9.6 | wall 18559
KL Stats: Epoch 13 Divergences: Uniform: 4.54503712971049 Unigram: 3.5063288938725616
2022-03-15 21:52:30 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 21:52:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:55:08 | INFO | train_inner | epoch 014:     46 / 407 loss=6.405, ppl=84.75, wps=16545.2, ups=0.25, wpb=65360.1, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.494, loss_scale=16, train_wall=322, gb_free=9.6, wall=18716
2022-03-15 22:00:47 | INFO | train_inner | epoch 014:    146 / 407 loss=6.368, ppl=82.6, wps=19326.9, ups=0.29, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.487, loss_scale=32, train_wall=313, gb_free=9.6, wall=19056
2022-03-15 22:06:25 | INFO | train_inner | epoch 014:    246 / 407 loss=6.37, ppl=82.71, wps=19393.6, ups=0.3, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.484, loss_scale=32, train_wall=312, gb_free=9.6, wall=19393
2022-03-15 22:06:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:09:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:12:13 | INFO | train_inner | epoch 014:    348 / 407 loss=6.363, ppl=82.28, wps=18826.9, ups=0.29, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.482, loss_scale=16, train_wall=321, gb_free=9.6, wall=19742
2022-03-15 22:15:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:16:18 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.274 | ppl 77.39 | wps 30158.6 | wpb 511.9 | bsz 1 | num_updates 5659 | best_loss 6.274
2022-03-15 22:16:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5659 updates
2022-03-15 22:16:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 22:16:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 22:16:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 14 @ 5659 updates, score 6.274) (writing took 2.6092036810005084 seconds)
2022-03-15 22:16:20 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 22:16:20 | INFO | train | epoch 014 | loss 6.364 | ppl 82.36 | wps 18547.6 | ups 0.28 | wpb 65492.6 | bsz 127.9 | num_updates 5659 | lr 0.000420368 | gnorm 0.485 | loss_scale 16 | train_wall 1277 | gb_free 9.6 | wall 19989
KL Stats: Epoch 14 Divergences: Uniform: 4.600311374379378 Unigram: 3.552770006739856
2022-03-15 22:16:20 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 22:16:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:18:41 | INFO | train_inner | epoch 015:     41 / 407 loss=6.322, ppl=79.99, wps=16843, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.483, loss_scale=32, train_wall=315, gb_free=9.6, wall=20130
2022-03-15 22:23:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:24:25 | INFO | train_inner | epoch 015:    142 / 407 loss=6.291, ppl=78.31, wps=19042, ups=0.29, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.484, loss_scale=16, train_wall=318, gb_free=9.6, wall=20474
2022-03-15 22:30:05 | INFO | train_inner | epoch 015:    242 / 407 loss=6.281, ppl=77.74, wps=19253.2, ups=0.29, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.481, loss_scale=16, train_wall=314, gb_free=9.6, wall=20814
2022-03-15 22:35:47 | INFO | train_inner | epoch 015:    342 / 407 loss=6.275, ppl=77.43, wps=19169.1, ups=0.29, wpb=65534.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.481, loss_scale=32, train_wall=316, gb_free=9.6, wall=21156
2022-03-15 22:37:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:39:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:40:14 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.216 | ppl 74.33 | wps 30116.5 | wpb 511.9 | bsz 1 | num_updates 6064 | best_loss 6.216
2022-03-15 22:40:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 6064 updates
2022-03-15 22:40:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 22:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 22:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 15 @ 6064 updates, score 6.216) (writing took 2.453747163992375 seconds)
2022-03-15 22:40:17 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 22:40:17 | INFO | train | epoch 015 | loss 6.278 | ppl 77.62 | wps 18466.4 | ups 0.28 | wpb 65492.6 | bsz 127.9 | num_updates 6064 | lr 0.000406088 | gnorm 0.483 | loss_scale 32 | train_wall 1283 | gb_free 9.6 | wall 21425
KL Stats: Epoch 15 Divergences: Uniform: 4.649276048486379 Unigram: 3.593945542452927
2022-03-15 22:40:17 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 22:40:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:42:21 | INFO | train_inner | epoch 016:     36 / 407 loss=6.236, ppl=75.36, wps=16617.6, ups=0.25, wpb=65361.9, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.482, loss_scale=32, train_wall=320, gb_free=9.6, wall=21549
2022-03-15 22:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:48:07 | INFO | train_inner | epoch 016:    137 / 407 loss=6.208, ppl=73.92, wps=18899.6, ups=0.29, wpb=65534.2, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.478, loss_scale=32, train_wall=320, gb_free=9.6, wall=21896
2022-03-15 22:53:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:53:51 | INFO | train_inner | epoch 016:    238 / 407 loss=6.214, ppl=74.25, wps=19062.6, ups=0.29, wpb=65536, bsz=128, num_updates=6300, lr=0.00039841, gnorm=0.471, loss_scale=32, train_wall=317, gb_free=9.6, wall=22240
2022-03-15 22:59:32 | INFO | train_inner | epoch 016:    338 / 407 loss=6.202, ppl=73.61, wps=19208.8, ups=0.29, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.475, loss_scale=32, train_wall=315, gb_free=9.6, wall=22581
2022-03-15 23:00:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:03:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:04:13 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.157 | ppl 71.38 | wps 30078.2 | wpb 511.9 | bsz 1 | num_updates 6468 | best_loss 6.157
2022-03-15 23:04:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6468 updates
2022-03-15 23:04:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 23:04:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 23:04:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 16 @ 6468 updates, score 6.157) (writing took 2.448328602942638 seconds)
2022-03-15 23:04:15 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 23:04:15 | INFO | train | epoch 016 | loss 6.205 | ppl 73.79 | wps 18392.8 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 6468 | lr 0.000393201 | gnorm 0.474 | loss_scale 32 | train_wall 1285 | gb_free 9.6 | wall 22864
KL Stats: Epoch 16 Divergences: Uniform: 4.689875743637687 Unigram: 3.6273612615330872
2022-03-15 23:04:15 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 23:04:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:06:05 | INFO | train_inner | epoch 017:     32 / 407 loss=6.183, ppl=72.68, wps=16624.6, ups=0.25, wpb=65361.9, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.477, loss_scale=32, train_wall=320, gb_free=9.6, wall=22974
2022-03-15 23:08:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:09:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:11:54 | INFO | train_inner | epoch 017:    134 / 407 loss=6.137, ppl=70.36, wps=18803.2, ups=0.29, wpb=65534.2, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.473, loss_scale=16, train_wall=322, gb_free=9.6, wall=23323
2022-03-15 23:17:34 | INFO | train_inner | epoch 017:    234 / 407 loss=6.142, ppl=70.64, wps=19278.5, ups=0.29, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.475, loss_scale=32, train_wall=314, gb_free=9.6, wall=23663
2022-03-15 23:23:13 | INFO | train_inner | epoch 017:    334 / 407 loss=6.143, ppl=70.68, wps=19315.1, ups=0.29, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.484, loss_scale=32, train_wall=313, gb_free=9.6, wall=24002
2022-03-15 23:23:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:25:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:27:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:28:05 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.118 | ppl 69.43 | wps 30287.5 | wpb 511.9 | bsz 1 | num_updates 6871 | best_loss 6.118
2022-03-15 23:28:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6871 updates
2022-03-15 23:28:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 23:28:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 23:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 17 @ 6871 updates, score 6.118) (writing took 2.4248768979450688 seconds)
2022-03-15 23:28:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 23:28:08 | INFO | train | epoch 017 | loss 6.143 | ppl 70.65 | wps 18423 | ups 0.28 | wpb 65492.3 | bsz 127.9 | num_updates 6871 | lr 0.000381496 | gnorm 0.478 | loss_scale 16 | train_wall 1280 | gb_free 9.6 | wall 24297
KL Stats: Epoch 17 Divergences: Uniform: 4.7262077986561914 Unigram: 3.6588309284784684
2022-03-15 23:28:08 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 23:28:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:29:47 | INFO | train_inner | epoch 018:     29 / 407 loss=6.125, ppl=69.8, wps=16595.3, ups=0.25, wpb=65361.9, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.478, loss_scale=16, train_wall=320, gb_free=9.6, wall=24396
2022-03-15 23:35:30 | INFO | train_inner | epoch 018:    129 / 407 loss=6.08, ppl=67.63, wps=19095.9, ups=0.29, wpb=65534.2, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.47, loss_scale=32, train_wall=317, gb_free=9.6, wall=24739
2022-03-15 23:36:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:41:16 | INFO | train_inner | epoch 018:    230 / 407 loss=6.092, ppl=68.2, wps=18981.4, ups=0.29, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.471, loss_scale=16, train_wall=319, gb_free=9.6, wall=25084
2022-03-15 23:46:55 | INFO | train_inner | epoch 018:    330 / 407 loss=6.092, ppl=68.23, wps=19286, ups=0.29, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.475, loss_scale=32, train_wall=314, gb_free=9.6, wall=25424
2022-03-15 23:51:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:51:59 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.074 | ppl 67.36 | wps 30461.9 | wpb 511.9 | bsz 1 | num_updates 7276 | best_loss 6.074
2022-03-15 23:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7276 updates
2022-03-15 23:51:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 23:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-15 23:52:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 18 @ 7276 updates, score 6.074) (writing took 2.418432398000732 seconds)
2022-03-15 23:52:02 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 23:52:02 | INFO | train | epoch 018 | loss 6.089 | ppl 68.07 | wps 18500.1 | ups 0.28 | wpb 65492.6 | bsz 127.9 | num_updates 7276 | lr 0.000370727 | gnorm 0.478 | loss_scale 32 | train_wall 1281 | gb_free 9.6 | wall 25730
KL Stats: Epoch 18 Divergences: Uniform: 4.753903716229933 Unigram: 3.681389301092443
2022-03-15 23:52:02 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 23:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:53:23 | INFO | train_inner | epoch 019:     24 / 407 loss=6.086, ppl=67.91, wps=16850.2, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.491, loss_scale=32, train_wall=315, gb_free=9.6, wall=25812
2022-03-15 23:59:04 | INFO | train_inner | epoch 019:    124 / 407 loss=6.03, ppl=65.35, wps=19223.3, ups=0.29, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.476, loss_scale=32, train_wall=315, gb_free=9.6, wall=26153
2022-03-15 23:59:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:04:50 | INFO | train_inner | epoch 019:    225 / 407 loss=6.034, ppl=65.52, wps=18936.2, ups=0.29, wpb=65534.2, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.474, loss_scale=32, train_wall=320, gb_free=9.6, wall=26499
2022-03-16 00:06:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:10:36 | INFO | train_inner | epoch 019:    326 / 407 loss=6.052, ppl=66.36, wps=18968.7, ups=0.29, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.473, loss_scale=32, train_wall=319, gb_free=9.6, wall=26845
2022-03-16 00:14:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:15:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:15:52 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.047 | ppl 66.14 | wps 30744.4 | wpb 511.9 | bsz 1 | num_updates 7680 | best_loss 6.047
2022-03-16 00:15:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7680 updates
2022-03-16 00:15:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 00:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 00:15:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 19 @ 7680 updates, score 6.047) (writing took 2.437736190040596 seconds)
2022-03-16 00:15:54 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-16 00:15:54 | INFO | train | epoch 019 | loss 6.041 | ppl 65.82 | wps 18471.2 | ups 0.28 | wpb 65492.5 | bsz 127.9 | num_updates 7680 | lr 0.000360844 | gnorm 0.473 | loss_scale 32 | train_wall 1280 | gb_free 9.6 | wall 27163
KL Stats: Epoch 19 Divergences: Uniform: 4.785178393805585 Unigram: 3.708108719287037
2022-03-16 00:15:54 | INFO | fairseq.trainer | begin training epoch 20
2022-03-16 00:15:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:17:02 | INFO | train_inner | epoch 020:     20 / 407 loss=6.036, ppl=65.64, wps=16912.4, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=7700, lr=0.000360375, gnorm=0.471, loss_scale=32, train_wall=314, gb_free=9.6, wall=27231
2022-03-16 00:22:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:22:49 | INFO | train_inner | epoch 020:    121 / 407 loss=5.979, ppl=63.06, wps=18885.3, ups=0.29, wpb=65534.2, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.482, loss_scale=32, train_wall=320, gb_free=9.6, wall=27578
2022-03-16 00:28:33 | INFO | train_inner | epoch 020:    221 / 407 loss=6.005, ppl=64.22, wps=19086.1, ups=0.29, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.474, loss_scale=32, train_wall=317, gb_free=9.6, wall=27921
2022-03-16 00:29:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:33:24 | INFO | train_inner | epoch 020:    322 / 407 loss=6.004, ppl=64.17, wps=22453.5, ups=0.34, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.473, loss_scale=32, train_wall=268, gb_free=9.6, wall=28213
2022-03-16 00:35:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:36:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:37:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:38:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.017 | ppl 64.75 | wps 34343.3 | wpb 511.9 | bsz 1 | num_updates 8083 | best_loss 6.017
2022-03-16 00:38:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 8083 updates
2022-03-16 00:38:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 00:38:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 00:38:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 20 @ 8083 updates, score 6.017) (writing took 2.384393143001944 seconds)
2022-03-16 00:38:10 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-16 00:38:10 | INFO | train | epoch 020 | loss 5.998 | ppl 63.9 | wps 19762 | ups 0.3 | wpb 65492.3 | bsz 127.9 | num_updates 8083 | lr 0.000351733 | gnorm 0.478 | loss_scale 16 | train_wall 1192 | gb_free 9.6 | wall 28498
KL Stats: Epoch 20 Divergences: Uniform: 4.808751293228629 Unigram: 3.7289846110599085
2022-03-16 00:38:10 | INFO | fairseq.trainer | begin training epoch 21
2022-03-16 00:38:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:38:58 | INFO | train_inner | epoch 021:     17 / 407 loss=5.994, ppl=63.75, wps=19568.7, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=8100, lr=0.000351364, gnorm=0.48, loss_scale=16, train_wall=268, gb_free=9.6, wall=28547
2022-03-16 00:43:46 | INFO | train_inner | epoch 021:    117 / 407 loss=5.94, ppl=61.41, wps=22774.5, ups=0.35, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.474, loss_scale=32, train_wall=264, gb_free=9.6, wall=28835
2022-03-16 00:48:33 | INFO | train_inner | epoch 021:    217 / 407 loss=5.964, ppl=62.43, wps=22851.6, ups=0.35, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.476, loss_scale=32, train_wall=263, gb_free=9.6, wall=29122
2022-03-16 00:49:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:53:23 | INFO | train_inner | epoch 021:    318 / 407 loss=5.965, ppl=62.45, wps=22621.7, ups=0.35, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.476, loss_scale=32, train_wall=266, gb_free=9.6, wall=29412
2022-03-16 00:56:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:57:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:58:16 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.985 | ppl 63.34 | wps 34315.2 | wpb 511.9 | bsz 1 | num_updates 8488 | best_loss 5.985
2022-03-16 00:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8488 updates
2022-03-16 00:58:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 00:58:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 00:58:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 21 @ 8488 updates, score 5.985) (writing took 2.2410667380318046 seconds)
2022-03-16 00:58:19 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-16 00:58:19 | INFO | train | epoch 021 | loss 5.959 | ppl 62.22 | wps 21937 | ups 0.33 | wpb 65492.6 | bsz 127.9 | num_updates 8488 | lr 0.00034324 | gnorm 0.476 | loss_scale 32 | train_wall 1071 | gb_free 9.6 | wall 29708
KL Stats: Epoch 21 Divergences: Uniform: 4.832788436516302 Unigram: 3.748074824949315
2022-03-16 00:58:19 | INFO | fairseq.trainer | begin training epoch 22
2022-03-16 00:58:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:58:53 | INFO | train_inner | epoch 022:     12 / 407 loss=5.969, ppl=62.64, wps=19768, ups=0.3, wpb=65360.1, bsz=127.7, num_updates=8500, lr=0.000342997, gnorm=0.478, loss_scale=32, train_wall=265, gb_free=9.6, wall=29742
2022-03-16 01:03:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:03:43 | INFO | train_inner | epoch 022:    113 / 407 loss=5.908, ppl=60.03, wps=22605.9, ups=0.34, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.48, loss_scale=32, train_wall=266, gb_free=9.6, wall=30032
2022-03-16 01:08:31 | INFO | train_inner | epoch 022:    213 / 407 loss=5.921, ppl=60.59, wps=22786, ups=0.35, wpb=65534.2, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.476, loss_scale=32, train_wall=264, gb_free=9.6, wall=30320
2022-03-16 01:09:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:13:20 | INFO | train_inner | epoch 022:    314 / 407 loss=5.935, ppl=61.18, wps=22642.6, ups=0.35, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.479, loss_scale=32, train_wall=265, gb_free=9.6, wall=30609
2022-03-16 01:15:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:17:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:18:26 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.972 | ppl 62.78 | wps 34262.7 | wpb 511.9 | bsz 1 | num_updates 8892 | best_loss 5.972
2022-03-16 01:18:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8892 updates
2022-03-16 01:18:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 01:18:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 01:18:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 22 @ 8892 updates, score 5.972) (writing took 2.3153801650041714 seconds)
2022-03-16 01:18:28 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-16 01:18:28 | INFO | train | epoch 022 | loss 5.924 | ppl 60.72 | wps 21880.9 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 8892 | lr 0.000335352 | gnorm 0.476 | loss_scale 32 | train_wall 1071 | gb_free 9.6 | wall 30917
KL Stats: Epoch 22 Divergences: Uniform: 4.857446124303708 Unigram: 3.769475920478638
2022-03-16 01:18:28 | INFO | fairseq.trainer | begin training epoch 23
2022-03-16 01:18:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:18:51 | INFO | train_inner | epoch 023:      8 / 407 loss=5.931, ppl=61, wps=19762.3, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=8900, lr=0.000335201, gnorm=0.47, loss_scale=32, train_wall=265, gb_free=9.6, wall=30940
2022-03-16 01:22:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:23:41 | INFO | train_inner | epoch 023:    109 / 407 loss=5.873, ppl=58.62, wps=22596.7, ups=0.34, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.477, loss_scale=32, train_wall=266, gb_free=9.6, wall=31230
2022-03-16 01:28:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:28:32 | INFO | train_inner | epoch 023:    210 / 407 loss=5.886, ppl=59.13, wps=22565.7, ups=0.34, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.478, loss_scale=32, train_wall=266, gb_free=9.6, wall=31520
2022-03-16 01:33:18 | INFO | train_inner | epoch 023:    310 / 407 loss=5.914, ppl=60.3, wps=22856.7, ups=0.35, wpb=65534.2, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.474, loss_scale=32, train_wall=263, gb_free=9.6, wall=31807
2022-03-16 01:34:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:38:35 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.952 | ppl 61.89 | wps 34252.1 | wpb 511.9 | bsz 1 | num_updates 9296 | best_loss 5.952
2022-03-16 01:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9296 updates
2022-03-16 01:38:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 01:38:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 01:38:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 23 @ 9296 updates, score 5.952) (writing took 2.240667543024756 seconds)
2022-03-16 01:38:37 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-16 01:38:37 | INFO | train | epoch 023 | loss 5.893 | ppl 59.43 | wps 21877.5 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 9296 | lr 0.000327983 | gnorm 0.476 | loss_scale 32 | train_wall 1071 | gb_free 9.6 | wall 32126
KL Stats: Epoch 23 Divergences: Uniform: 4.877661192350246 Unigram: 3.785326230938935
2022-03-16 01:38:37 | INFO | fairseq.trainer | begin training epoch 24
2022-03-16 01:38:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:38:49 | INFO | train_inner | epoch 024:      4 / 407 loss=5.899, ppl=59.66, wps=19769.3, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=9300, lr=0.000327913, gnorm=0.478, loss_scale=32, train_wall=265, gb_free=9.6, wall=32138
2022-03-16 01:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:43:39 | INFO | train_inner | epoch 024:    105 / 407 loss=5.84, ppl=57.3, wps=22608.7, ups=0.34, wpb=65534.2, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.484, loss_scale=32, train_wall=266, gb_free=9.6, wall=32428
2022-03-16 01:47:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:48:29 | INFO | train_inner | epoch 024:    206 / 407 loss=5.859, ppl=58.06, wps=22593.6, ups=0.34, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.471, loss_scale=32, train_wall=266, gb_free=9.6, wall=32718
2022-03-16 01:53:16 | INFO | train_inner | epoch 024:    306 / 407 loss=5.872, ppl=58.56, wps=22841.3, ups=0.35, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.474, loss_scale=32, train_wall=263, gb_free=9.6, wall=33005
2022-03-16 01:53:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:58:04 | INFO | train_inner | epoch 024:    407 / 407 loss=5.881, ppl=58.93, wps=22641.7, ups=0.35, wpb=65361.9, bsz=127.7, num_updates=9700, lr=0.000321081, gnorm=0.478, loss_scale=32, train_wall=265, gb_free=9.6, wall=33293
2022-03-16 01:58:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:58:43 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.93 | ppl 60.96 | wps 34378.7 | wpb 511.9 | bsz 1 | num_updates 9700 | best_loss 5.93
2022-03-16 01:58:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9700 updates
2022-03-16 01:58:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 01:58:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 01:58:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 24 @ 9700 updates, score 5.93) (writing took 2.2436211510794237 seconds)
2022-03-16 01:58:46 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-16 01:58:46 | INFO | train | epoch 024 | loss 5.863 | ppl 58.19 | wps 21899.1 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 9700 | lr 0.000321081 | gnorm 0.477 | loss_scale 32 | train_wall 1070 | gb_free 9.6 | wall 33334
KL Stats: Epoch 24 Divergences: Uniform: 4.895388360583164 Unigram: 3.799297552157943
2022-03-16 01:58:46 | INFO | fairseq.trainer | begin training epoch 25
2022-03-16 01:58:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:00:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:03:36 | INFO | train_inner | epoch 025:    101 / 407 loss=5.814, ppl=56.26, wps=19779.6, ups=0.3, wpb=65534.2, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.484, loss_scale=32, train_wall=266, gb_free=9.6, wall=33625
2022-03-16 02:06:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:08:26 | INFO | train_inner | epoch 025:    202 / 407 loss=5.826, ppl=56.72, wps=22588.4, ups=0.34, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.478, loss_scale=32, train_wall=266, gb_free=9.6, wall=33915
2022-03-16 02:11:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:13:16 | INFO | train_inner | epoch 025:    303 / 407 loss=5.853, ppl=57.79, wps=22562.9, ups=0.34, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.482, loss_scale=16, train_wall=267, gb_free=9.6, wall=34205
2022-03-16 02:18:03 | INFO | train_inner | epoch 025:    403 / 407 loss=5.851, ppl=57.71, wps=22853.3, ups=0.35, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.477, loss_scale=32, train_wall=263, gb_free=9.6, wall=34492
2022-03-16 02:18:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:18:53 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.912 | ppl 60.21 | wps 34325.4 | wpb 511.9 | bsz 1 | num_updates 10104 | best_loss 5.912
2022-03-16 02:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 10104 updates
2022-03-16 02:18:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 02:18:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 02:18:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 25 @ 10104 updates, score 5.912) (writing took 2.2718462410848588 seconds)
2022-03-16 02:18:55 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-16 02:18:55 | INFO | train | epoch 025 | loss 5.836 | ppl 57.11 | wps 21876.2 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 10104 | lr 0.000314596 | gnorm 0.48 | loss_scale 32 | train_wall 1071 | gb_free 9.6 | wall 34544
KL Stats: Epoch 25 Divergences: Uniform: 4.916419123307207 Unigram: 3.815740957447261
2022-03-16 02:18:55 | INFO | fairseq.trainer | begin training epoch 26
2022-03-16 02:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:23:31 | INFO | train_inner | epoch 026:     96 / 407 loss=5.791, ppl=55.35, wps=19901, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.481, loss_scale=32, train_wall=263, gb_free=9.6, wall=34820
2022-03-16 02:24:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:28:22 | INFO | train_inner | epoch 026:    197 / 407 loss=5.813, ppl=56.2, wps=22575.6, ups=0.34, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.483, loss_scale=32, train_wall=266, gb_free=9.6, wall=35111
2022-03-16 02:30:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:33:12 | INFO | train_inner | epoch 026:    298 / 407 loss=5.809, ppl=56.06, wps=22589.5, ups=0.34, wpb=65534.2, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.476, loss_scale=32, train_wall=266, gb_free=9.6, wall=35401
2022-03-16 02:36:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:38:02 | INFO | train_inner | epoch 026:    399 / 407 loss=5.83, ppl=56.89, wps=22585.8, ups=0.34, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.473, loss_scale=32, train_wall=266, gb_free=9.6, wall=35691
2022-03-16 02:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:39:03 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.903 | ppl 59.82 | wps 34213.8 | wpb 511.9 | bsz 1 | num_updates 10508 | best_loss 5.903
2022-03-16 02:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10508 updates
2022-03-16 02:39:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 02:39:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 02:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 26 @ 10508 updates, score 5.903) (writing took 2.260318633983843 seconds)
2022-03-16 02:39:06 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 02:39:06 | INFO | train | epoch 026 | loss 5.811 | ppl 56.15 | wps 21854.5 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 10508 | lr 0.000308489 | gnorm 0.478 | loss_scale 32 | train_wall 1073 | gb_free 9.6 | wall 35755
KL Stats: Epoch 26 Divergences: Uniform: 4.931912232899423 Unigram: 3.8286610709073394
2022-03-16 02:39:06 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 02:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:43:31 | INFO | train_inner | epoch 027:     92 / 407 loss=5.775, ppl=54.75, wps=19890.1, ups=0.3, wpb=65360.1, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.483, loss_scale=32, train_wall=263, gb_free=9.6, wall=36020
2022-03-16 02:43:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:48:22 | INFO | train_inner | epoch 027:    193 / 407 loss=5.79, ppl=55.34, wps=22501.5, ups=0.34, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.483, loss_scale=32, train_wall=267, gb_free=9.6, wall=36311
2022-03-16 02:49:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:53:13 | INFO | train_inner | epoch 027:    294 / 407 loss=5.783, ppl=55.07, wps=22505.1, ups=0.34, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.484, loss_scale=16, train_wall=267, gb_free=9.6, wall=36602
2022-03-16 02:58:01 | INFO | train_inner | epoch 027:    394 / 407 loss=5.813, ppl=56.23, wps=22760.5, ups=0.35, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.477, loss_scale=32, train_wall=264, gb_free=9.6, wall=36890
2022-03-16 02:58:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:58:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:59:16 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.883 | ppl 59.02 | wps 34380.7 | wpb 511.9 | bsz 1 | num_updates 10912 | best_loss 5.883
2022-03-16 02:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10912 updates
2022-03-16 02:59:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 02:59:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 02:59:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 27 @ 10912 updates, score 5.883) (writing took 2.2512298499932513 seconds)
2022-03-16 02:59:19 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 02:59:19 | INFO | train | epoch 027 | loss 5.789 | ppl 55.28 | wps 21814.2 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 10912 | lr 0.000302725 | gnorm 0.482 | loss_scale 16 | train_wall 1075 | gb_free 9.6 | wall 36968
KL Stats: Epoch 27 Divergences: Uniform: 4.950126237693932 Unigram: 3.8422949528802106
2022-03-16 02:59:19 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 02:59:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:03:32 | INFO | train_inner | epoch 028:     88 / 407 loss=5.74, ppl=53.44, wps=19732.8, ups=0.3, wpb=65360.1, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.488, loss_scale=16, train_wall=266, gb_free=9.6, wall=37221
2022-03-16 03:08:20 | INFO | train_inner | epoch 028:    188 / 407 loss=5.756, ppl=54.04, wps=22787.3, ups=0.35, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.477, loss_scale=32, train_wall=264, gb_free=9.6, wall=37509
2022-03-16 03:11:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:13:10 | INFO | train_inner | epoch 028:    289 / 407 loss=5.783, ppl=55.07, wps=22582.8, ups=0.34, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.484, loss_scale=32, train_wall=266, gb_free=9.6, wall=37799
2022-03-16 03:17:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:18:01 | INFO | train_inner | epoch 028:    390 / 407 loss=5.782, ppl=55.03, wps=22542.7, ups=0.34, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.485, loss_scale=32, train_wall=267, gb_free=9.6, wall=38090
2022-03-16 03:18:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:19:28 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.871 | ppl 58.53 | wps 34257.5 | wpb 511.9 | bsz 1 | num_updates 11317 | best_loss 5.871
2022-03-16 03:19:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11317 updates
2022-03-16 03:19:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 03:19:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 03:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 28 @ 11317 updates, score 5.871) (writing took 2.2521286599803716 seconds)
2022-03-16 03:19:30 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 03:19:30 | INFO | train | epoch 028 | loss 5.766 | ppl 54.43 | wps 21890.8 | ups 0.33 | wpb 65492.6 | bsz 127.9 | num_updates 11317 | lr 0.000297259 | gnorm 0.483 | loss_scale 32 | train_wall 1074 | gb_free 9.6 | wall 38179
KL Stats: Epoch 28 Divergences: Uniform: 4.962755900305224 Unigram: 3.8526534698489683
2022-03-16 03:19:30 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 03:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:23:29 | INFO | train_inner | epoch 029:     83 / 407 loss=5.734, ppl=53.23, wps=19911.6, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=11400, lr=0.000296174, gnorm=0.48, loss_scale=32, train_wall=263, gb_free=9.6, wall=38418
2022-03-16 03:24:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:28:20 | INFO | train_inner | epoch 029:    184 / 407 loss=5.752, ppl=53.9, wps=22541.4, ups=0.34, wpb=65534.2, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.486, loss_scale=32, train_wall=267, gb_free=9.6, wall=38709
2022-03-16 03:30:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:33:10 | INFO | train_inner | epoch 029:    285 / 407 loss=5.738, ppl=53.38, wps=22577.6, ups=0.34, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.481, loss_scale=32, train_wall=266, gb_free=9.6, wall=38999
2022-03-16 03:36:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:38:01 | INFO | train_inner | epoch 029:    386 / 407 loss=5.763, ppl=54.3, wps=22556.2, ups=0.34, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.486, loss_scale=32, train_wall=267, gb_free=9.6, wall=39289
2022-03-16 03:39:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:39:39 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.858 | ppl 57.99 | wps 34275.9 | wpb 511.9 | bsz 1 | num_updates 11721 | best_loss 5.858
2022-03-16 03:39:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11721 updates
2022-03-16 03:39:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 03:39:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 03:39:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 29 @ 11721 updates, score 5.858) (writing took 2.259138750960119 seconds)
2022-03-16 03:39:42 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 03:39:42 | INFO | train | epoch 029 | loss 5.747 | ppl 53.69 | wps 21842.4 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 11721 | lr 0.000292091 | gnorm 0.484 | loss_scale 32 | train_wall 1073 | gb_free 9.6 | wall 39391
KL Stats: Epoch 29 Divergences: Uniform: 4.978153258605858 Unigram: 3.8650740069109673
2022-03-16 03:39:42 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 03:39:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:43:29 | INFO | train_inner | epoch 030:     79 / 407 loss=5.722, ppl=52.79, wps=19902.2, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=11800, lr=0.000291111, gnorm=0.484, loss_scale=32, train_wall=263, gb_free=9.6, wall=39618
2022-03-16 03:43:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:48:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:48:23 | INFO | train_inner | epoch 030:    181 / 407 loss=5.712, ppl=52.43, wps=22315.3, ups=0.34, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.488, loss_scale=16, train_wall=269, gb_free=9.6, wall=39912
2022-03-16 03:53:10 | INFO | train_inner | epoch 030:    281 / 407 loss=5.737, ppl=53.35, wps=22796.1, ups=0.35, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.484, loss_scale=16, train_wall=264, gb_free=9.6, wall=40199
2022-03-16 03:57:59 | INFO | train_inner | epoch 030:    381 / 407 loss=5.743, ppl=53.55, wps=22713.7, ups=0.35, wpb=65534.2, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.489, loss_scale=32, train_wall=265, gb_free=9.6, wall=40488
2022-03-16 03:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:59:52 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.849 | ppl 57.62 | wps 34217.9 | wpb 511.9 | bsz 1 | num_updates 12126 | best_loss 5.849
2022-03-16 03:59:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 12126 updates
2022-03-16 03:59:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 03:59:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 03:59:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 30 @ 12126 updates, score 5.849) (writing took 2.2480198489502072 seconds)
2022-03-16 03:59:54 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 03:59:54 | INFO | train | epoch 030 | loss 5.728 | ppl 53 | wps 21875.8 | ups 0.33 | wpb 65492.6 | bsz 127.9 | num_updates 12126 | lr 0.000287171 | gnorm 0.486 | loss_scale 32 | train_wall 1075 | gb_free 9.6 | wall 40603
KL Stats: Epoch 30 Divergences: Uniform: 4.991808111882912 Unigram: 3.8753580446338494
2022-03-16 03:59:54 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 03:59:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:01:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:03:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:03:34 | INFO | train_inner | epoch 031:     76 / 407 loss=5.692, ppl=51.68, wps=19513.7, ups=0.3, wpb=65360.1, bsz=127.7, num_updates=12200, lr=0.000286299, gnorm=0.486, loss_scale=16, train_wall=269, gb_free=9.6, wall=40823
2022-03-16 04:08:21 | INFO | train_inner | epoch 031:    176 / 407 loss=5.702, ppl=52.06, wps=22800.9, ups=0.35, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.489, loss_scale=16, train_wall=264, gb_free=9.6, wall=41110
2022-03-16 04:13:09 | INFO | train_inner | epoch 031:    276 / 407 loss=5.718, ppl=52.62, wps=22778.5, ups=0.35, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.485, loss_scale=32, train_wall=264, gb_free=9.6, wall=41398
2022-03-16 04:15:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:17:59 | INFO | train_inner | epoch 031:    377 / 407 loss=5.733, ppl=53.19, wps=22574.7, ups=0.34, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.488, loss_scale=32, train_wall=266, gb_free=9.6, wall=41688
2022-03-16 04:19:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:20:04 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.841 | ppl 57.34 | wps 34359.7 | wpb 511.9 | bsz 1 | num_updates 12530 | best_loss 5.841
2022-03-16 04:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12530 updates
2022-03-16 04:20:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 04:20:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 04:20:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 31 @ 12530 updates, score 5.841) (writing took 2.2478735799668357 seconds)
2022-03-16 04:20:06 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 04:20:06 | INFO | train | epoch 031 | loss 5.71 | ppl 52.34 | wps 21832.1 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 12530 | lr 0.000282504 | gnorm 0.487 | loss_scale 32 | train_wall 1074 | gb_free 9.6 | wall 41815
KL Stats: Epoch 31 Divergences: Uniform: 5.005756178794975 Unigram: 3.8873103504215196
2022-03-16 04:20:06 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 04:20:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:22:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:23:31 | INFO | train_inner | epoch 032:     71 / 407 loss=5.681, ppl=51.3, wps=19687.5, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=12600, lr=0.000281718, gnorm=0.483, loss_scale=32, train_wall=267, gb_free=9.6, wall=42020
2022-03-16 04:27:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:28:22 | INFO | train_inner | epoch 032:    172 / 407 loss=5.684, ppl=51.4, wps=22542.4, ups=0.34, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.488, loss_scale=16, train_wall=267, gb_free=9.6, wall=42311
2022-03-16 04:33:10 | INFO | train_inner | epoch 032:    272 / 407 loss=5.699, ppl=51.94, wps=22761.3, ups=0.35, wpb=65534.2, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.486, loss_scale=16, train_wall=264, gb_free=9.6, wall=42599
2022-03-16 04:37:57 | INFO | train_inner | epoch 032:    372 / 407 loss=5.714, ppl=52.49, wps=22780.7, ups=0.35, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.484, loss_scale=32, train_wall=264, gb_free=9.6, wall=42886
2022-03-16 04:39:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:40:16 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.832 | ppl 56.97 | wps 34253.3 | wpb 511.9 | bsz 1 | num_updates 12935 | best_loss 5.832
2022-03-16 04:40:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12935 updates
2022-03-16 04:40:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 04:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 04:40:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 32 @ 12935 updates, score 5.832) (writing took 2.24662431597244 seconds)
2022-03-16 04:40:19 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 04:40:19 | INFO | train | epoch 032 | loss 5.693 | ppl 51.73 | wps 21876.8 | ups 0.33 | wpb 65492.6 | bsz 127.9 | num_updates 12935 | lr 0.000278046 | gnorm 0.486 | loss_scale 32 | train_wall 1074 | gb_free 9.6 | wall 43027
KL Stats: Epoch 32 Divergences: Uniform: 5.020567501902314 Unigram: 3.897617429285727
2022-03-16 04:40:19 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 04:40:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:40:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:43:29 | INFO | train_inner | epoch 033:     66 / 407 loss=5.665, ppl=50.72, wps=19737.1, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=13000, lr=0.00027735, gnorm=0.486, loss_scale=32, train_wall=266, gb_free=9.6, wall=43217
2022-03-16 04:44:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:48:19 | INFO | train_inner | epoch 033:    167 / 407 loss=5.667, ppl=50.81, wps=22531.1, ups=0.34, wpb=65534.2, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.491, loss_scale=16, train_wall=267, gb_free=9.6, wall=43508
2022-03-16 04:52:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:53:10 | INFO | train_inner | epoch 033:    268 / 407 loss=5.685, ppl=51.45, wps=22551.7, ups=0.34, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.488, loss_scale=16, train_wall=267, gb_free=9.6, wall=43799
2022-03-16 04:57:58 | INFO | train_inner | epoch 033:    368 / 407 loss=5.691, ppl=51.65, wps=22770.3, ups=0.35, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.488, loss_scale=16, train_wall=264, gb_free=9.6, wall=44087
2022-03-16 04:59:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:00:29 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.818 | ppl 56.43 | wps 34353.7 | wpb 511.9 | bsz 1 | num_updates 13339 | best_loss 5.818
2022-03-16 05:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13339 updates
2022-03-16 05:00:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 05:00:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 05:00:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 33 @ 13339 updates, score 5.818) (writing took 2.2461324440082535 seconds)
2022-03-16 05:00:31 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 05:00:31 | INFO | train | epoch 033 | loss 5.677 | ppl 51.15 | wps 21826.3 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 13339 | lr 0.000273803 | gnorm 0.489 | loss_scale 32 | train_wall 1074 | gb_free 9.6 | wall 44240
KL Stats: Epoch 33 Divergences: Uniform: 5.03252059873184 Unigram: 3.9069519516336984
2022-03-16 05:00:31 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 05:00:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:00:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:03:30 | INFO | train_inner | epoch 034:     62 / 407 loss=5.656, ppl=50.42, wps=19684, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=13400, lr=0.000273179, gnorm=0.494, loss_scale=16, train_wall=267, gb_free=9.6, wall=44419
2022-03-16 05:08:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:08:21 | INFO | train_inner | epoch 034:    163 / 407 loss=5.647, ppl=50.1, wps=22481.3, ups=0.34, wpb=65534.2, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.491, loss_scale=16, train_wall=267, gb_free=9.6, wall=44710
2022-03-16 05:13:09 | INFO | train_inner | epoch 034:    263 / 407 loss=5.672, ppl=51, wps=22765.9, ups=0.35, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.483, loss_scale=16, train_wall=264, gb_free=9.6, wall=44998
2022-03-16 05:17:57 | INFO | train_inner | epoch 034:    363 / 407 loss=5.681, ppl=51.29, wps=22804.3, ups=0.35, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.48, loss_scale=32, train_wall=264, gb_free=9.6, wall=45286
2022-03-16 05:20:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:20:42 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.813 | ppl 56.24 | wps 34234.4 | wpb 511.9 | bsz 1 | num_updates 13744 | best_loss 5.813
2022-03-16 05:20:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13744 updates
2022-03-16 05:20:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 05:20:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 05:20:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 34 @ 13744 updates, score 5.813) (writing took 2.260550341103226 seconds)
2022-03-16 05:20:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 05:20:44 | INFO | train | epoch 034 | loss 5.662 | ppl 50.63 | wps 21859.7 | ups 0.33 | wpb 65492.6 | bsz 127.9 | num_updates 13744 | lr 0.000269739 | gnorm 0.487 | loss_scale 32 | train_wall 1075 | gb_free 9.6 | wall 45453
KL Stats: Epoch 34 Divergences: Uniform: 5.043975626728325 Unigram: 3.9164138736679828
2022-03-16 05:20:44 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 05:20:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:21:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:23:29 | INFO | train_inner | epoch 035:     57 / 407 loss=5.638, ppl=49.8, wps=19692.1, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=13800, lr=0.000269191, gnorm=0.489, loss_scale=32, train_wall=266, gb_free=9.6, wall=45617
2022-03-16 05:27:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:28:19 | INFO | train_inner | epoch 035:    158 / 407 loss=5.636, ppl=49.73, wps=22558.4, ups=0.34, wpb=65534.2, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.49, loss_scale=32, train_wall=267, gb_free=9.6, wall=45908
2022-03-16 05:33:07 | INFO | train_inner | epoch 035:    258 / 407 loss=5.654, ppl=50.35, wps=22744.2, ups=0.35, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.492, loss_scale=32, train_wall=264, gb_free=9.6, wall=46196
2022-03-16 05:33:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:37:58 | INFO | train_inner | epoch 035:    359 / 407 loss=5.666, ppl=50.77, wps=22578.9, ups=0.34, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.488, loss_scale=32, train_wall=266, gb_free=9.6, wall=46486
2022-03-16 05:39:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:40:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:40:54 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.8 | ppl 55.72 | wps 34299.2 | wpb 511.9 | bsz 1 | num_updates 14147 | best_loss 5.8
2022-03-16 05:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 14147 updates
2022-03-16 05:40:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 05:40:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 05:40:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 35 @ 14147 updates, score 5.8) (writing took 2.242251859046519 seconds)
2022-03-16 05:40:56 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 05:40:56 | INFO | train | epoch 035 | loss 5.647 | ppl 50.1 | wps 21780.1 | ups 0.33 | wpb 65492.3 | bsz 127.9 | num_updates 14147 | lr 0.000265869 | gnorm 0.49 | loss_scale 32 | train_wall 1074 | gb_free 9.6 | wall 46665
KL Stats: Epoch 35 Divergences: Uniform: 5.0544885606216425 Unigram: 3.924077445084879
2022-03-16 05:40:56 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 05:40:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:43:29 | INFO | train_inner | epoch 036:     53 / 407 loss=5.637, ppl=49.76, wps=19728.1, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=14200, lr=0.000265372, gnorm=0.49, loss_scale=32, train_wall=266, gb_free=9.6, wall=46818
2022-03-16 05:46:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:48:19 | INFO | train_inner | epoch 036:    154 / 407 loss=5.626, ppl=49.37, wps=22557.3, ups=0.34, wpb=65534.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.494, loss_scale=32, train_wall=266, gb_free=9.6, wall=47108
2022-03-16 05:51:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:53:10 | INFO | train_inner | epoch 036:    255 / 407 loss=5.631, ppl=49.56, wps=22572, ups=0.34, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.492, loss_scale=16, train_wall=266, gb_free=9.6, wall=47399
2022-03-16 05:57:57 | INFO | train_inner | epoch 036:    355 / 407 loss=5.648, ppl=50.15, wps=22788.6, ups=0.35, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.489, loss_scale=32, train_wall=264, gb_free=9.6, wall=47686
2022-03-16 06:00:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:01:05 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.793 | ppl 55.43 | wps 34309 | wpb 511.9 | bsz 1 | num_updates 14552 | best_loss 5.793
2022-03-16 06:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14552 updates
2022-03-16 06:01:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 06:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 06:01:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 36 @ 14552 updates, score 5.793) (writing took 2.2438991679809988 seconds)
2022-03-16 06:01:08 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 06:01:08 | INFO | train | epoch 036 | loss 5.634 | ppl 49.67 | wps 21890.7 | ups 0.33 | wpb 65492.6 | bsz 127.9 | num_updates 14552 | lr 0.000262143 | gnorm 0.491 | loss_scale 32 | train_wall 1074 | gb_free 9.6 | wall 47877
KL Stats: Epoch 36 Divergences: Uniform: 5.064477563212708 Unigram: 3.9306866695472693
2022-03-16 06:01:08 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 06:01:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:03:26 | INFO | train_inner | epoch 037:     48 / 407 loss=5.614, ppl=48.99, wps=19888.2, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=0.491, loss_scale=32, train_wall=264, gb_free=9.6, wall=48015
2022-03-16 06:04:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:06:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:08:19 | INFO | train_inner | epoch 037:    150 / 407 loss=5.605, ppl=48.68, wps=22330.5, ups=0.34, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.486, loss_scale=16, train_wall=269, gb_free=9.6, wall=48308
2022-03-16 06:13:07 | INFO | train_inner | epoch 037:    250 / 407 loss=5.624, ppl=49.31, wps=22793.3, ups=0.35, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.491, loss_scale=32, train_wall=264, gb_free=9.6, wall=48596
2022-03-16 06:14:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:17:58 | INFO | train_inner | epoch 037:    351 / 407 loss=5.644, ppl=50.02, wps=22523.5, ups=0.34, wpb=65534.2, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.494, loss_scale=16, train_wall=267, gb_free=9.6, wall=48887
2022-03-16 06:20:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:21:24 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.788 | ppl 55.25 | wps 33677.6 | wpb 511.9 | bsz 1 | num_updates 14956 | best_loss 5.788
2022-03-16 06:21:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14956 updates
2022-03-16 06:21:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 06:21:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 06:21:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 37 @ 14956 updates, score 5.788) (writing took 2.25982096104417 seconds)
2022-03-16 06:21:26 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 06:21:26 | INFO | train | epoch 037 | loss 5.621 | ppl 49.2 | wps 21719.6 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 14956 | lr 0.000258578 | gnorm 0.489 | loss_scale 32 | train_wall 1080 | gb_free 9.6 | wall 49095
KL Stats: Epoch 37 Divergences: Uniform: 5.076899253944738 Unigram: 3.9407581221019665
2022-03-16 06:21:26 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 06:21:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:23:37 | INFO | train_inner | epoch 038:     44 / 407 loss=5.609, ppl=48.82, wps=19264.8, ups=0.29, wpb=65361.9, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=0.485, loss_scale=32, train_wall=273, gb_free=9.6, wall=49226
2022-03-16 06:27:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:28:36 | INFO | train_inner | epoch 038:    145 / 407 loss=5.582, ppl=47.89, wps=21898.6, ups=0.33, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.491, loss_scale=32, train_wall=275, gb_free=9.6, wall=49525
2022-03-16 06:33:33 | INFO | train_inner | epoch 038:    245 / 407 loss=5.622, ppl=49.26, wps=22078.7, ups=0.34, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.489, loss_scale=32, train_wall=273, gb_free=9.6, wall=49822
2022-03-16 06:33:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:38:32 | INFO | train_inner | epoch 038:    346 / 407 loss=5.622, ppl=49.26, wps=21929.6, ups=0.33, wpb=65534.2, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.491, loss_scale=32, train_wall=275, gb_free=9.6, wall=50121
2022-03-16 06:40:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:41:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:41:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:42:12 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.783 | ppl 55.05 | wps 33817.5 | wpb 511.9 | bsz 1 | num_updates 15359 | best_loss 5.783
2022-03-16 06:42:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 15359 updates
2022-03-16 06:42:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 06:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 06:42:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 38 @ 15359 updates, score 5.783) (writing took 2.2717184409266338 seconds)
2022-03-16 06:42:14 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 06:42:14 | INFO | train | epoch 038 | loss 5.608 | ppl 48.78 | wps 21143.6 | ups 0.32 | wpb 65492.3 | bsz 127.9 | num_updates 15359 | lr 0.000255163 | gnorm 0.49 | loss_scale 16 | train_wall 1109 | gb_free 9.6 | wall 50343
KL Stats: Epoch 38 Divergences: Uniform: 5.086176830858331 Unigram: 3.949290958567674
2022-03-16 06:42:14 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 06:42:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:44:16 | INFO | train_inner | epoch 039:     41 / 407 loss=5.607, ppl=48.73, wps=18985.6, ups=0.29, wpb=65360.1, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=0.494, loss_scale=16, train_wall=278, gb_free=9.6, wall=50465
2022-03-16 06:49:13 | INFO | train_inner | epoch 039:    141 / 407 loss=5.583, ppl=47.92, wps=22099.9, ups=0.34, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.5, loss_scale=32, train_wall=273, gb_free=9.6, wall=50762
2022-03-16 06:54:01 | INFO | train_inner | epoch 039:    241 / 407 loss=5.598, ppl=48.44, wps=22766.4, ups=0.35, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.49, loss_scale=32, train_wall=264, gb_free=9.6, wall=51050
2022-03-16 06:54:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:58:51 | INFO | train_inner | epoch 039:    342 / 407 loss=5.609, ppl=48.82, wps=22575.3, ups=0.34, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.495, loss_scale=32, train_wall=266, gb_free=9.6, wall=51340
2022-03-16 07:00:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:02:37 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.783 | ppl 55.06 | wps 34315.7 | wpb 511.9 | bsz 1 | num_updates 15764 | best_loss 5.783
2022-03-16 07:02:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15764 updates
2022-03-16 07:02:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 07:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 07:02:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 39 @ 15764 updates, score 5.783) (writing took 2.245826819911599 seconds)
2022-03-16 07:02:39 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 07:02:39 | INFO | train | epoch 039 | loss 5.597 | ppl 48.4 | wps 21654.8 | ups 0.33 | wpb 65492.6 | bsz 127.9 | num_updates 15764 | lr 0.000251864 | gnorm 0.496 | loss_scale 32 | train_wall 1087 | gb_free 9.6 | wall 51568
KL Stats: Epoch 39 Divergences: Uniform: 5.0982096247214255 Unigram: 3.957657815943857
2022-03-16 07:02:39 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 07:02:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:04:22 | INFO | train_inner | epoch 040:     36 / 407 loss=5.596, ppl=48.37, wps=19737.4, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=0.494, loss_scale=32, train_wall=266, gb_free=9.6, wall=51671
2022-03-16 07:07:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:09:14 | INFO | train_inner | epoch 040:    137 / 407 loss=5.563, ppl=47.29, wps=22430.8, ups=0.34, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.495, loss_scale=32, train_wall=268, gb_free=9.6, wall=51963
2022-03-16 07:12:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:14:15 | INFO | train_inner | epoch 040:    238 / 407 loss=5.58, ppl=47.83, wps=21802.2, ups=0.33, wpb=65534.2, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.494, loss_scale=16, train_wall=276, gb_free=9.6, wall=52264
2022-03-16 07:19:12 | INFO | train_inner | epoch 040:    338 / 407 loss=5.606, ppl=48.69, wps=22075.9, ups=0.34, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.495, loss_scale=32, train_wall=273, gb_free=9.6, wall=52561
2022-03-16 07:22:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:22:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:23:15 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.773 | ppl 54.68 | wps 33708.3 | wpb 511.9 | bsz 1 | num_updates 16168 | best_loss 5.773
2022-03-16 07:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 16168 updates
2022-03-16 07:23:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 07:23:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 07:23:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 40 @ 16168 updates, score 5.773) (writing took 2.2123231909936294 seconds)
2022-03-16 07:23:17 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 07:23:17 | INFO | train | epoch 040 | loss 5.585 | ppl 48.01 | wps 21377 | ups 0.33 | wpb 65492.5 | bsz 127.9 | num_updates 16168 | lr 0.000248698 | gnorm 0.493 | loss_scale 16 | train_wall 1098 | gb_free 9.6 | wall 52806
KL Stats: Epoch 40 Divergences: Uniform: 5.10857937379373 Unigram: 3.9631977909121523
2022-03-16 07:23:17 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 07:23:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:24:52 | INFO | train_inner | epoch 041:     32 / 407 loss=5.581, ppl=47.87, wps=19239.4, ups=0.29, wpb=65360.1, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=0.492, loss_scale=16, train_wall=274, gb_free=9.6, wall=52900
2022-03-16 07:29:48 | INFO | train_inner | epoch 041:    132 / 407 loss=5.564, ppl=47.3, wps=22075.6, ups=0.34, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.5, loss_scale=32, train_wall=273, gb_free=9.6, wall=53197
2022-03-16 07:34:45 | INFO | train_inner | epoch 041:    232 / 407 loss=5.575, ppl=47.67, wps=22138.2, ups=0.34, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.495, loss_scale=32, train_wall=272, gb_free=9.6, wall=53493
2022-03-16 07:35:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:39:44 | INFO | train_inner | epoch 041:    333 / 407 loss=5.586, ppl=48.05, wps=21879.1, ups=0.33, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.495, loss_scale=32, train_wall=275, gb_free=9.6, wall=53793
2022-03-16 07:41:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:43:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:44:02 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.771 | ppl 54.61 | wps 33648.4 | wpb 511.9 | bsz 1 | num_updates 16573 | best_loss 5.771
2022-03-16 07:44:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16573 updates
2022-03-16 07:44:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 07:44:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 07:44:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 41 @ 16573 updates, score 5.771) (writing took 2.246946414001286 seconds)
2022-03-16 07:44:05 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 07:44:05 | INFO | train | epoch 041 | loss 5.574 | ppl 47.64 | wps 21255.3 | ups 0.32 | wpb 65492.6 | bsz 127.9 | num_updates 16573 | lr 0.00024564 | gnorm 0.498 | loss_scale 16 | train_wall 1108 | gb_free 9.6 | wall 54054
KL Stats: Epoch 41 Divergences: Uniform: 5.115875400355304 Unigram: 3.9702740628848443
2022-03-16 07:44:05 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 07:44:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:45:25 | INFO | train_inner | epoch 042:     27 / 407 loss=5.575, ppl=47.67, wps=19193.3, ups=0.29, wpb=65361.9, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=0.503, loss_scale=16, train_wall=274, gb_free=9.6, wall=54133
2022-03-16 07:50:21 | INFO | train_inner | epoch 042:    127 / 407 loss=5.546, ppl=46.73, wps=22111.8, ups=0.34, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.491, loss_scale=32, train_wall=272, gb_free=9.6, wall=54430
2022-03-16 07:54:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:55:21 | INFO | train_inner | epoch 042:    228 / 407 loss=5.557, ppl=47.07, wps=21870.7, ups=0.33, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.496, loss_scale=32, train_wall=275, gb_free=9.6, wall=54729
2022-03-16 08:00:18 | INFO | train_inner | epoch 042:    328 / 407 loss=5.584, ppl=47.98, wps=22019.9, ups=0.34, wpb=65534.2, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.501, loss_scale=32, train_wall=274, gb_free=9.6, wall=55027
2022-03-16 08:01:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:03:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:04:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:04:52 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.759 | ppl 54.17 | wps 33759.3 | wpb 511.9 | bsz 1 | num_updates 16977 | best_loss 5.759
2022-03-16 08:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16977 updates
2022-03-16 08:04:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 08:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 08:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 42 @ 16977 updates, score 5.759) (writing took 2.2831041070166975 seconds)
2022-03-16 08:04:54 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 08:04:54 | INFO | train | epoch 042 | loss 5.564 | ppl 47.32 | wps 21179.2 | ups 0.32 | wpb 65492.5 | bsz 127.9 | num_updates 16977 | lr 0.0002427 | gnorm 0.496 | loss_scale 16 | train_wall 1110 | gb_free 9.6 | wall 55303
KL Stats: Epoch 42 Divergences: Uniform: 5.122039589579037 Unigram: 3.97575333353895
2022-03-16 08:04:54 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 08:04:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:06:03 | INFO | train_inner | epoch 043:     23 / 407 loss=5.569, ppl=47.48, wps=18969.5, ups=0.29, wpb=65361.9, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=0.497, loss_scale=16, train_wall=278, gb_free=9.6, wall=55372
2022-03-16 08:10:59 | INFO | train_inner | epoch 043:    123 / 407 loss=5.535, ppl=46.35, wps=22109, ups=0.34, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.497, loss_scale=32, train_wall=272, gb_free=9.6, wall=55668
2022-03-16 08:13:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:15:59 | INFO | train_inner | epoch 043:    224 / 407 loss=5.561, ppl=47.21, wps=21828, ups=0.33, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.498, loss_scale=16, train_wall=276, gb_free=9.6, wall=55968
2022-03-16 08:20:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:21:00 | INFO | train_inner | epoch 043:    325 / 407 loss=5.562, ppl=47.24, wps=21819.5, ups=0.33, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.499, loss_scale=16, train_wall=276, gb_free=9.6, wall=56269
2022-03-16 08:25:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:25:43 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.759 | ppl 54.17 | wps 33559.1 | wpb 511.9 | bsz 1 | num_updates 17382 | best_loss 5.759
2022-03-16 08:25:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 17382 updates
2022-03-16 08:25:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 08:25:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 08:25:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 43 @ 17382 updates, score 5.759) (writing took 2.2754725869745016 seconds)
2022-03-16 08:25:45 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 08:25:45 | INFO | train | epoch 043 | loss 5.555 | ppl 47 | wps 21198.8 | ups 0.32 | wpb 65492.6 | bsz 127.9 | num_updates 17382 | lr 0.000239856 | gnorm 0.498 | loss_scale 16 | train_wall 1111 | gb_free 9.6 | wall 56554
KL Stats: Epoch 43 Divergences: Uniform: 5.134555827378368 Unigram: 3.985912348462564
2022-03-16 08:25:45 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 08:25:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:26:39 | INFO | train_inner | epoch 044:     18 / 407 loss=5.561, ppl=47.2, wps=19282, ups=0.3, wpb=65360.1, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=0.499, loss_scale=16, train_wall=273, gb_free=9.6, wall=56608
2022-03-16 08:31:36 | INFO | train_inner | epoch 044:    118 / 407 loss=5.525, ppl=46.03, wps=22040.2, ups=0.34, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.496, loss_scale=32, train_wall=273, gb_free=9.6, wall=56905
2022-03-16 08:34:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:36:37 | INFO | train_inner | epoch 044:    219 / 407 loss=5.544, ppl=46.64, wps=21810, ups=0.33, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.5, loss_scale=32, train_wall=276, gb_free=9.6, wall=57205
2022-03-16 08:40:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:41:36 | INFO | train_inner | epoch 044:    320 / 407 loss=5.56, ppl=47.17, wps=21863.6, ups=0.33, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.501, loss_scale=16, train_wall=276, gb_free=9.6, wall=57505
2022-03-16 08:45:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:46:32 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.751 | ppl 53.87 | wps 33692.2 | wpb 511.9 | bsz 1 | num_updates 17787 | best_loss 5.751
2022-03-16 08:46:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17787 updates
2022-03-16 08:46:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 08:46:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 08:46:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 44 @ 17787 updates, score 5.751) (writing took 2.214453562046401 seconds)
2022-03-16 08:46:35 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 08:46:35 | INFO | train | epoch 044 | loss 5.545 | ppl 46.68 | wps 21230.7 | ups 0.32 | wpb 65492.6 | bsz 127.9 | num_updates 17787 | lr 0.000237109 | gnorm 0.498 | loss_scale 16 | train_wall 1110 | gb_free 9.6 | wall 57803
KL Stats: Epoch 44 Divergences: Uniform: 5.143277876900222 Unigram: 3.9898583843142337
2022-03-16 08:46:35 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 08:46:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:47:13 | INFO | train_inner | epoch 045:     13 / 407 loss=5.548, ppl=46.79, wps=19396.5, ups=0.3, wpb=65360.1, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=0.498, loss_scale=32, train_wall=271, gb_free=9.6, wall=57842
2022-03-16 08:49:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:52:12 | INFO | train_inner | epoch 045:    114 / 407 loss=5.507, ppl=45.47, wps=21921.3, ups=0.33, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.5, loss_scale=16, train_wall=275, gb_free=9.6, wall=58141
2022-03-16 08:57:09 | INFO | train_inner | epoch 045:    214 / 407 loss=5.527, ppl=46.12, wps=22088, ups=0.34, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.494, loss_scale=32, train_wall=273, gb_free=9.6, wall=58438
2022-03-16 09:02:06 | INFO | train_inner | epoch 045:    314 / 407 loss=5.558, ppl=47.1, wps=22043.3, ups=0.34, wpb=65534.2, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.51, loss_scale=32, train_wall=273, gb_free=9.6, wall=58735
2022-03-16 09:02:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:06:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:07:21 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.748 | ppl 53.73 | wps 33616.1 | wpb 511.9 | bsz 1 | num_updates 18192 | best_loss 5.748
2022-03-16 09:07:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 18192 updates
2022-03-16 09:07:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 09:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 09:07:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 45 @ 18192 updates, score 5.748) (writing took 2.2469012410147116 seconds)
2022-03-16 09:07:23 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 09:07:23 | INFO | train | epoch 045 | loss 5.535 | ppl 46.36 | wps 21246.9 | ups 0.32 | wpb 65492.6 | bsz 127.9 | num_updates 18192 | lr 0.000234455 | gnorm 0.502 | loss_scale 32 | train_wall 1109 | gb_free 9.6 | wall 59052
KL Stats: Epoch 45 Divergences: Uniform: 5.150858385688497 Unigram: 3.995949090477263
2022-03-16 09:07:23 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 09:07:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:07:47 | INFO | train_inner | epoch 046:      8 / 407 loss=5.552, ppl=46.93, wps=19207.4, ups=0.29, wpb=65361.9, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=0.499, loss_scale=32, train_wall=274, gb_free=9.6, wall=59075
2022-03-16 09:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:12:47 | INFO | train_inner | epoch 046:    109 / 407 loss=5.5, ppl=45.25, wps=21843.7, ups=0.33, wpb=65534.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.5, loss_scale=32, train_wall=276, gb_free=9.6, wall=59375
2022-03-16 09:15:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:17:47 | INFO | train_inner | epoch 046:    210 / 407 loss=5.528, ppl=46.15, wps=21853.9, ups=0.33, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.498, loss_scale=32, train_wall=276, gb_free=9.6, wall=59675
2022-03-16 09:21:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:22:47 | INFO | train_inner | epoch 046:    311 / 407 loss=5.537, ppl=46.43, wps=21791.2, ups=0.33, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.499, loss_scale=16, train_wall=277, gb_free=9.6, wall=59976
2022-03-16 09:27:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:28:06 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.735 | ppl 53.27 | wps 35848.5 | wpb 511.9 | bsz 1 | num_updates 18596 | best_loss 5.735
2022-03-16 09:28:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18596 updates
2022-03-16 09:28:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 09:28:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 09:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 46 @ 18596 updates, score 5.735) (writing took 2.308653589920141 seconds)
2022-03-16 09:28:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 09:28:08 | INFO | train | epoch 046 | loss 5.526 | ppl 46.08 | wps 21243.6 | ups 0.32 | wpb 65492.5 | bsz 127.9 | num_updates 18596 | lr 0.000231894 | gnorm 0.498 | loss_scale 16 | train_wall 1108 | gb_free 9.6 | wall 60297
KL Stats: Epoch 46 Divergences: Uniform: 5.15722739275716 Unigram: 4.00058667968147
2022-03-16 09:28:09 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 09:28:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:28:19 | INFO | train_inner | epoch 047:      4 / 407 loss=5.542, ppl=46.58, wps=19673.1, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=18600, lr=0.000231869, gnorm=0.495, loss_scale=16, train_wall=269, gb_free=9.6, wall=60308
2022-03-16 09:32:59 | INFO | train_inner | epoch 047:    104 / 407 loss=5.49, ppl=44.94, wps=23474.6, ups=0.36, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.498, loss_scale=32, train_wall=256, gb_free=9.6, wall=60588
2022-03-16 09:34:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:37:30 | INFO | train_inner | epoch 047:    205 / 407 loss=5.502, ppl=45.31, wps=24132, ups=0.37, wpb=65534.2, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.503, loss_scale=32, train_wall=248, gb_free=9.6, wall=60859
2022-03-16 09:40:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:42:02 | INFO | train_inner | epoch 047:    306 / 407 loss=5.539, ppl=46.5, wps=24076.2, ups=0.37, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.504, loss_scale=32, train_wall=249, gb_free=9.6, wall=61131
2022-03-16 09:42:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:46:31 | INFO | train_inner | epoch 047:    407 / 407 loss=5.541, ppl=46.57, wps=24362.7, ups=0.37, wpb=65361.9, bsz=127.7, num_updates=19000, lr=0.000229416, gnorm=0.507, loss_scale=16, train_wall=245, gb_free=9.6, wall=61400
2022-03-16 09:46:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:47:08 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.738 | ppl 53.37 | wps 36302.7 | wpb 511.9 | bsz 1 | num_updates 19000 | best_loss 5.735
2022-03-16 09:47:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 19000 updates
2022-03-16 09:47:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 09:47:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 09:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt (epoch 47 @ 19000 updates, score 5.738) (writing took 1.3363999529974535 seconds)
2022-03-16 09:47:09 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 09:47:09 | INFO | train | epoch 047 | loss 5.518 | ppl 45.82 | wps 23200.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 19000 | lr 0.000229416 | gnorm 0.503 | loss_scale 16 | train_wall 1007 | gb_free 9.6 | wall 61438
KL Stats: Epoch 47 Divergences: Uniform: 5.167320898370995 Unigram: 4.0079402039298255
2022-03-16 09:47:09 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 09:47:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:51:36 | INFO | train_inner | epoch 048:    100 / 407 loss=5.49, ppl=44.94, wps=21480, ups=0.33, wpb=65534.2, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.503, loss_scale=32, train_wall=243, gb_free=9.6, wall=61705
2022-03-16 09:54:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:56:05 | INFO | train_inner | epoch 048:    201 / 407 loss=5.503, ppl=45.34, wps=24332.2, ups=0.37, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.499, loss_scale=32, train_wall=246, gb_free=9.6, wall=61974
2022-03-16 09:58:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:00:35 | INFO | train_inner | epoch 048:    302 / 407 loss=5.518, ppl=45.82, wps=24326.5, ups=0.37, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.499, loss_scale=16, train_wall=246, gb_free=9.6, wall=62243
2022-03-16 10:05:01 | INFO | train_inner | epoch 048:    402 / 407 loss=5.527, ppl=46.1, wps=24593.4, ups=0.38, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.5, loss_scale=32, train_wall=243, gb_free=9.6, wall=62510
2022-03-16 10:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:05:50 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.731 | ppl 53.12 | wps 36322.2 | wpb 511.9 | bsz 1 | num_updates 19405 | best_loss 5.731
2022-03-16 10:05:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 19405 updates
2022-03-16 10:05:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 10:05:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 10:05:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 48 @ 19405 updates, score 5.731) (writing took 2.313831476960331 seconds)
2022-03-16 10:05:53 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 10:05:53 | INFO | train | epoch 048 | loss 5.509 | ppl 45.55 | wps 23600.6 | ups 0.36 | wpb 65492.6 | bsz 127.9 | num_updates 19405 | lr 0.000227009 | gnorm 0.5 | loss_scale 32 | train_wall 990 | gb_free 9.6 | wall 62562
KL Stats: Epoch 48 Divergences: Uniform: 5.173617932765646 Unigram: 4.012172571520918
2022-03-16 10:05:53 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 10:05:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:10:06 | INFO | train_inner | epoch 049:     95 / 407 loss=5.474, ppl=44.45, wps=21422.5, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=0.506, loss_scale=32, train_wall=242, gb_free=9.6, wall=62815
2022-03-16 10:10:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:14:35 | INFO | train_inner | epoch 049:    196 / 407 loss=5.506, ppl=45.45, wps=24366.9, ups=0.37, wpb=65534.2, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.503, loss_scale=32, train_wall=245, gb_free=9.6, wall=63084
2022-03-16 10:16:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:16:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:19:07 | INFO | train_inner | epoch 049:    298 / 407 loss=5.509, ppl=45.54, wps=24101.6, ups=0.37, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.503, loss_scale=16, train_wall=248, gb_free=9.6, wall=63356
2022-03-16 10:22:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:23:39 | INFO | train_inner | epoch 049:    399 / 407 loss=5.514, ppl=45.71, wps=24073, ups=0.37, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.5, loss_scale=16, train_wall=249, gb_free=9.6, wall=63628
2022-03-16 10:24:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:24:39 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.728 | ppl 53.02 | wps 35217.9 | wpb 511.9 | bsz 1 | num_updates 19808 | best_loss 5.728
2022-03-16 10:24:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19808 updates
2022-03-16 10:24:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 10:24:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 10:24:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 49 @ 19808 updates, score 5.728) (writing took 2.218300740001723 seconds)
2022-03-16 10:24:41 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 10:24:41 | INFO | train | epoch 049 | loss 5.501 | ppl 45.3 | wps 23399.6 | ups 0.36 | wpb 65492.3 | bsz 127.9 | num_updates 19808 | lr 0.000224688 | gnorm 0.503 | loss_scale 16 | train_wall 993 | gb_free 9.6 | wall 63690
KL Stats: Epoch 49 Divergences: Uniform: 5.18247580452235 Unigram: 4.018515276683928
2022-03-16 10:24:41 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 10:24:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:28:53 | INFO | train_inner | epoch 050:     92 / 407 loss=5.469, ppl=44.31, wps=20844.7, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=0.499, loss_scale=16, train_wall=250, gb_free=9.6, wall=63942
2022-03-16 10:33:27 | INFO | train_inner | epoch 050:    192 / 407 loss=5.49, ppl=44.96, wps=23928, ups=0.37, wpb=65534.2, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.5, loss_scale=32, train_wall=250, gb_free=9.6, wall=64216
2022-03-16 10:34:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:37:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:38:07 | INFO | train_inner | epoch 050:    294 / 407 loss=5.496, ppl=45.13, wps=23408.4, ups=0.36, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.499, loss_scale=16, train_wall=256, gb_free=9.6, wall=64496
2022-03-16 10:42:40 | INFO | train_inner | epoch 050:    394 / 407 loss=5.515, ppl=45.72, wps=23955.4, ups=0.37, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.502, loss_scale=16, train_wall=250, gb_free=9.6, wall=64769
2022-03-16 10:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:43:53 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.722 | ppl 52.8 | wps 35360.5 | wpb 511.9 | bsz 1 | num_updates 20213 | best_loss 5.722
2022-03-16 10:43:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 20213 updates
2022-03-16 10:43:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 10:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 10:43:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 50 @ 20213 updates, score 5.722) (writing took 2.2118309939978644 seconds)
2022-03-16 10:43:55 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 10:43:55 | INFO | train | epoch 050 | loss 5.493 | ppl 45.05 | wps 22977.7 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 20213 | lr 0.000222426 | gnorm 0.5 | loss_scale 32 | train_wall 1019 | gb_free 9.6 | wall 64844
KL Stats: Epoch 50 Divergences: Uniform: 5.187689656705444 Unigram: 4.02158459713283
2022-03-16 10:43:55 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 10:43:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:45:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:47:56 | INFO | train_inner | epoch 051:     88 / 407 loss=5.466, ppl=44.2, wps=20723.8, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=0.504, loss_scale=16, train_wall=252, gb_free=9.6, wall=65085
2022-03-16 10:52:29 | INFO | train_inner | epoch 051:    188 / 407 loss=5.481, ppl=44.65, wps=23963.3, ups=0.37, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.505, loss_scale=32, train_wall=250, gb_free=9.6, wall=65358
2022-03-16 10:57:08 | INFO | train_inner | epoch 051:    288 / 407 loss=5.496, ppl=45.13, wps=23539.1, ups=0.36, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.507, loss_scale=32, train_wall=255, gb_free=9.6, wall=65636
2022-03-16 10:57:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:02:01 | INFO | train_inner | epoch 051:    389 / 407 loss=5.507, ppl=45.49, wps=22348, ups=0.34, wpb=65534.2, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.507, loss_scale=32, train_wall=269, gb_free=9.6, wall=65930
2022-03-16 11:02:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:03:27 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.719 | ppl 52.69 | wps 35347.4 | wpb 511.9 | bsz 1 | num_updates 20618 | best_loss 5.719
2022-03-16 11:03:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20618 updates
2022-03-16 11:03:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 11:03:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 11:03:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 51 @ 20618 updates, score 5.719) (writing took 2.23042210994754 seconds)
2022-03-16 11:03:30 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 11:03:30 | INFO | train | epoch 051 | loss 5.487 | ppl 44.83 | wps 22584.4 | ups 0.34 | wpb 65492.6 | bsz 127.9 | num_updates 20618 | lr 0.00022023 | gnorm 0.505 | loss_scale 32 | train_wall 1039 | gb_free 9.6 | wall 66018
KL Stats: Epoch 51 Divergences: Uniform: 5.194232912336415 Unigram: 4.02838140321926
2022-03-16 11:03:30 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 11:03:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:04:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:07:17 | INFO | train_inner | epoch 052:     83 / 407 loss=5.464, ppl=44.13, wps=20644.7, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=0.504, loss_scale=32, train_wall=253, gb_free=9.6, wall=66246
2022-03-16 11:09:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:11:54 | INFO | train_inner | epoch 052:    184 / 407 loss=5.478, ppl=44.58, wps=23708.8, ups=0.36, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.51, loss_scale=16, train_wall=253, gb_free=9.6, wall=66523
2022-03-16 11:16:28 | INFO | train_inner | epoch 052:    284 / 407 loss=5.487, ppl=44.83, wps=23880.7, ups=0.36, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.506, loss_scale=32, train_wall=251, gb_free=9.6, wall=66797
2022-03-16 11:18:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:21:05 | INFO | train_inner | epoch 052:    385 / 407 loss=5.492, ppl=45.01, wps=23657.9, ups=0.36, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.51, loss_scale=16, train_wall=253, gb_free=9.6, wall=67074
2022-03-16 11:22:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:22:43 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.719 | ppl 52.67 | wps 35099.4 | wpb 511.9 | bsz 1 | num_updates 21022 | best_loss 5.719
2022-03-16 11:22:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 21022 updates
2022-03-16 11:22:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 11:22:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 11:22:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 52 @ 21022 updates, score 5.719) (writing took 2.2054199570557103 seconds)
2022-03-16 11:22:46 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 11:22:46 | INFO | train | epoch 052 | loss 5.48 | ppl 44.63 | wps 22889.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 21022 | lr 0.000218104 | gnorm 0.508 | loss_scale 16 | train_wall 1020 | gb_free 9.6 | wall 67174
KL Stats: Epoch 52 Divergences: Uniform: 5.2024823654456425 Unigram: 4.034104355519688
2022-03-16 11:22:46 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 11:22:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:26:20 | INFO | train_inner | epoch 053:     78 / 407 loss=5.451, ppl=43.75, wps=20779.2, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=0.509, loss_scale=32, train_wall=251, gb_free=9.6, wall=67389
2022-03-16 11:30:54 | INFO | train_inner | epoch 053:    178 / 407 loss=5.458, ppl=43.94, wps=23919.9, ups=0.36, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.506, loss_scale=32, train_wall=251, gb_free=9.6, wall=67663
2022-03-16 11:31:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:35:31 | INFO | train_inner | epoch 053:    279 / 407 loss=5.496, ppl=45.13, wps=23677.6, ups=0.36, wpb=65534.2, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.505, loss_scale=32, train_wall=253, gb_free=9.6, wall=67939
2022-03-16 11:36:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:40:08 | INFO | train_inner | epoch 053:    380 / 407 loss=5.489, ppl=44.92, wps=23657.2, ups=0.36, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.502, loss_scale=16, train_wall=253, gb_free=9.6, wall=68216
2022-03-16 11:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:41:59 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.719 | ppl 52.69 | wps 35179.6 | wpb 511.9 | bsz 1 | num_updates 21427 | best_loss 5.719
2022-03-16 11:41:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 21427 updates
2022-03-16 11:41:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 11:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 11:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 53 @ 21427 updates, score 5.719) (writing took 2.246583681087941 seconds)
2022-03-16 11:42:02 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 11:42:02 | INFO | train | epoch 053 | loss 5.472 | ppl 44.4 | wps 22944.6 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 21427 | lr 0.000216033 | gnorm 0.506 | loss_scale 16 | train_wall 1020 | gb_free 9.6 | wall 68330
KL Stats: Epoch 53 Divergences: Uniform: 5.211780610710801 Unigram: 4.03990130355158
2022-03-16 11:42:02 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 11:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:45:21 | INFO | train_inner | epoch 054:     73 / 407 loss=5.434, ppl=43.24, wps=20825.2, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=0.509, loss_scale=32, train_wall=250, gb_free=9.6, wall=68530
2022-03-16 11:49:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:49:58 | INFO | train_inner | epoch 054:    174 / 407 loss=5.461, ppl=44.05, wps=23711.3, ups=0.36, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.504, loss_scale=32, train_wall=253, gb_free=9.6, wall=68807
2022-03-16 11:54:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:54:34 | INFO | train_inner | epoch 054:    275 / 407 loss=5.474, ppl=44.46, wps=23712.1, ups=0.36, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.505, loss_scale=16, train_wall=253, gb_free=9.6, wall=69083
2022-03-16 11:59:08 | INFO | train_inner | epoch 054:    375 / 407 loss=5.486, ppl=44.81, wps=23972.1, ups=0.37, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.507, loss_scale=16, train_wall=250, gb_free=9.6, wall=69356
2022-03-16 12:00:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:01:13 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.714 | ppl 52.5 | wps 35321.2 | wpb 511.9 | bsz 1 | num_updates 21832 | best_loss 5.714
2022-03-16 12:01:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21832 updates
2022-03-16 12:01:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:01:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:01:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 54 @ 21832 updates, score 5.714) (writing took 2.2042168750194833 seconds)
2022-03-16 12:01:15 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 12:01:15 | INFO | train | epoch 054 | loss 5.466 | ppl 44.19 | wps 22994.4 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 21832 | lr 0.000214019 | gnorm 0.507 | loss_scale 32 | train_wall 1018 | gb_free 9.6 | wall 69484
KL Stats: Epoch 54 Divergences: Uniform: 5.216585850064071 Unigram: 4.043330123654265
2022-03-16 12:01:15 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 12:01:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:02:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:04:24 | INFO | train_inner | epoch 055:     69 / 407 loss=5.457, ppl=43.93, wps=20646.3, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=0.513, loss_scale=16, train_wall=253, gb_free=9.6, wall=69673
2022-03-16 12:08:58 | INFO | train_inner | epoch 055:    169 / 407 loss=5.453, ppl=43.8, wps=23959.6, ups=0.37, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.505, loss_scale=32, train_wall=250, gb_free=9.6, wall=69947
2022-03-16 12:13:32 | INFO | train_inner | epoch 055:    269 / 407 loss=5.452, ppl=43.77, wps=23905.7, ups=0.36, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.507, loss_scale=32, train_wall=251, gb_free=9.6, wall=70221
2022-03-16 12:14:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:18:09 | INFO | train_inner | epoch 055:    370 / 407 loss=5.474, ppl=44.44, wps=23675.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.502, loss_scale=32, train_wall=253, gb_free=9.6, wall=70498
2022-03-16 12:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:20:28 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.709 | ppl 52.3 | wps 35113.5 | wpb 511.9 | bsz 1 | num_updates 22237 | best_loss 5.709
2022-03-16 12:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 22237 updates
2022-03-16 12:20:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:20:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 55 @ 22237 updates, score 5.709) (writing took 2.2488259479869157 seconds)
2022-03-16 12:20:30 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 12:20:30 | INFO | train | epoch 055 | loss 5.459 | ppl 44 | wps 22962.3 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 22237 | lr 0.000212062 | gnorm 0.506 | loss_scale 32 | train_wall 1019 | gb_free 9.6 | wall 70639
KL Stats: Epoch 55 Divergences: Uniform: 5.223611965553514 Unigram: 4.047605909318645
2022-03-16 12:20:30 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 12:20:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:21:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:23:26 | INFO | train_inner | epoch 056:     64 / 407 loss=5.447, ppl=43.63, wps=20599.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=22300, lr=0.000211762, gnorm=0.506, loss_scale=32, train_wall=253, gb_free=9.6, wall=70815
2022-03-16 12:24:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:28:04 | INFO | train_inner | epoch 056:    165 / 407 loss=5.439, ppl=43.4, wps=23601.4, ups=0.36, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.515, loss_scale=16, train_wall=254, gb_free=9.6, wall=71092
2022-03-16 12:32:38 | INFO | train_inner | epoch 056:    265 / 407 loss=5.459, ppl=43.99, wps=23886.7, ups=0.36, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.51, loss_scale=32, train_wall=251, gb_free=9.6, wall=71367
2022-03-16 12:36:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:37:15 | INFO | train_inner | epoch 056:    366 / 407 loss=5.471, ppl=44.35, wps=23663.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.507, loss_scale=32, train_wall=253, gb_free=9.6, wall=71644
2022-03-16 12:39:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:39:45 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.703 | ppl 52.1 | wps 35185.4 | wpb 511.9 | bsz 1 | num_updates 22641 | best_loss 5.703
2022-03-16 12:39:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 22641 updates
2022-03-16 12:39:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:39:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:39:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 56 @ 22641 updates, score 5.703) (writing took 2.2263157540000975 seconds)
2022-03-16 12:39:47 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 12:39:47 | INFO | train | epoch 056 | loss 5.453 | ppl 43.79 | wps 22865.6 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 22641 | lr 0.000210161 | gnorm 0.51 | loss_scale 32 | train_wall 1021 | gb_free 9.6 | wall 71796
KL Stats: Epoch 56 Divergences: Uniform: 5.227404032349316 Unigram: 4.050325443516122
2022-03-16 12:39:47 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 12:39:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:42:29 | INFO | train_inner | epoch 057:     59 / 407 loss=5.439, ppl=43.39, wps=20819.7, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=22700, lr=0.000209888, gnorm=0.512, loss_scale=32, train_wall=250, gb_free=9.6, wall=71958
2022-03-16 12:43:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:46:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:47:08 | INFO | train_inner | epoch 057:    161 / 407 loss=5.435, ppl=43.25, wps=23462.8, ups=0.36, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.512, loss_scale=16, train_wall=255, gb_free=9.6, wall=72237
2022-03-16 12:51:41 | INFO | train_inner | epoch 057:    261 / 407 loss=5.446, ppl=43.6, wps=24046.1, ups=0.37, wpb=65534.2, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.509, loss_scale=16, train_wall=249, gb_free=9.6, wall=72510
2022-03-16 12:52:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:56:12 | INFO | train_inner | epoch 057:    362 / 407 loss=5.467, ppl=44.24, wps=24180, ups=0.37, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.514, loss_scale=16, train_wall=247, gb_free=9.6, wall=72781
2022-03-16 12:58:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:58:51 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.7 | ppl 51.98 | wps 36029.2 | wpb 511.9 | bsz 1 | num_updates 23045 | best_loss 5.7
2022-03-16 12:58:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 23045 updates
2022-03-16 12:58:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:58:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 12:58:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 57 @ 23045 updates, score 5.7) (writing took 2.3202787829795852 seconds)
2022-03-16 12:58:53 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 12:58:53 | INFO | train | epoch 057 | loss 5.447 | ppl 43.62 | wps 23085.6 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 23045 | lr 0.000208311 | gnorm 0.511 | loss_scale 16 | train_wall 1011 | gb_free 9.6 | wall 72942
KL Stats: Epoch 57 Divergences: Uniform: 5.236438271073224 Unigram: 4.056703424192955
2022-03-16 12:58:54 | INFO | fairseq.trainer | begin training epoch 58
2022-03-16 12:58:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:01:23 | INFO | train_inner | epoch 058:     55 / 407 loss=5.437, ppl=43.32, wps=21016.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=23100, lr=0.000208063, gnorm=0.507, loss_scale=32, train_wall=248, gb_free=9.6, wall=73092
2022-03-16 13:04:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:05:54 | INFO | train_inner | epoch 058:    156 / 407 loss=5.428, ppl=43.06, wps=24152.7, ups=0.37, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.506, loss_scale=32, train_wall=248, gb_free=9.6, wall=73363
2022-03-16 13:09:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:10:25 | INFO | train_inner | epoch 058:    257 / 407 loss=5.44, ppl=43.41, wps=24177.5, ups=0.37, wpb=65534.2, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.508, loss_scale=16, train_wall=247, gb_free=9.6, wall=73634
2022-03-16 13:14:53 | INFO | train_inner | epoch 058:    357 / 407 loss=5.457, ppl=43.91, wps=24445.5, ups=0.37, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.509, loss_scale=16, train_wall=245, gb_free=9.6, wall=73902
2022-03-16 13:17:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:17:44 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 5.707 | ppl 52.25 | wps 35820.6 | wpb 511.9 | bsz 1 | num_updates 23450 | best_loss 5.7
2022-03-16 13:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 23450 updates
2022-03-16 13:17:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 13:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 13:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt (epoch 58 @ 23450 updates, score 5.707) (writing took 1.2891987600596622 seconds)
2022-03-16 13:17:45 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-16 13:17:45 | INFO | train | epoch 058 | loss 5.441 | ppl 43.45 | wps 23433.2 | ups 0.36 | wpb 65492.6 | bsz 127.9 | num_updates 23450 | lr 0.000206504 | gnorm 0.508 | loss_scale 32 | train_wall 998 | gb_free 9.6 | wall 74074
KL Stats: Epoch 58 Divergences: Uniform: 5.243092172240491 Unigram: 4.06127070939933
2022-03-16 13:17:45 | INFO | fairseq.trainer | begin training epoch 59
2022-03-16 13:17:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:20:00 | INFO | train_inner | epoch 059:     50 / 407 loss=5.439, ppl=43.38, wps=21323.7, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=23500, lr=0.000206284, gnorm=0.507, loss_scale=32, train_wall=244, gb_free=9.6, wall=74209
2022-03-16 13:21:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:24:30 | INFO | train_inner | epoch 059:    151 / 407 loss=5.415, ppl=42.66, wps=24231.9, ups=0.37, wpb=65534.2, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.514, loss_scale=32, train_wall=247, gb_free=9.6, wall=74479
2022-03-16 13:25:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:29:00 | INFO | train_inner | epoch 059:    252 / 407 loss=5.44, ppl=43.4, wps=24293, ups=0.37, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.511, loss_scale=16, train_wall=246, gb_free=9.6, wall=74749
2022-03-16 13:33:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:33:30 | INFO | train_inner | epoch 059:    353 / 407 loss=5.447, ppl=43.62, wps=24272.8, ups=0.37, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.505, loss_scale=16, train_wall=246, gb_free=9.6, wall=75019
2022-03-16 13:35:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:36:31 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.699 | ppl 51.95 | wps 36205 | wpb 511.9 | bsz 1 | num_updates 23854 | best_loss 5.699
2022-03-16 13:36:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23854 updates
2022-03-16 13:36:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 13:36:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 13:36:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 59 @ 23854 updates, score 5.699) (writing took 2.297168932040222 seconds)
2022-03-16 13:36:33 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-16 13:36:33 | INFO | train | epoch 059 | loss 5.435 | ppl 43.25 | wps 23465.7 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 23854 | lr 0.000204748 | gnorm 0.51 | loss_scale 16 | train_wall 993 | gb_free 9.6 | wall 75202
KL Stats: Epoch 59 Divergences: Uniform: 5.248531828806134 Unigram: 4.0656550102128985
2022-03-16 13:36:33 | INFO | fairseq.trainer | begin training epoch 60
2022-03-16 13:36:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:38:37 | INFO | train_inner | epoch 060:     46 / 407 loss=5.432, ppl=43.17, wps=21319.2, ups=0.33, wpb=65360.1, bsz=127.7, num_updates=23900, lr=0.000204551, gnorm=0.512, loss_scale=16, train_wall=244, gb_free=9.6, wall=75325
2022-03-16 13:43:04 | INFO | train_inner | epoch 060:    146 / 407 loss=5.414, ppl=42.62, wps=24493.1, ups=0.37, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.511, loss_scale=32, train_wall=244, gb_free=9.6, wall=75593
2022-03-16 13:45:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:47:34 | INFO | train_inner | epoch 060:    247 / 407 loss=5.427, ppl=43.02, wps=24245.6, ups=0.37, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.506, loss_scale=32, train_wall=247, gb_free=9.6, wall=75863
2022-03-16 13:51:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:52:05 | INFO | train_inner | epoch 060:    348 / 407 loss=5.45, ppl=43.71, wps=24246.1, ups=0.37, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.514, loss_scale=32, train_wall=247, gb_free=9.6, wall=76134
2022-03-16 13:54:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:54:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:55:19 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.698 | ppl 51.91 | wps 36193 | wpb 511.9 | bsz 1 | num_updates 24258 | best_loss 5.698
2022-03-16 13:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 24258 updates
2022-03-16 13:55:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 13:55:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 13:55:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 60 @ 24258 updates, score 5.698) (writing took 2.2975295989308506 seconds)
2022-03-16 13:55:21 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-16 13:55:21 | INFO | train | epoch 060 | loss 5.429 | ppl 43.09 | wps 23452.2 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 24258 | lr 0.000203036 | gnorm 0.51 | loss_scale 16 | train_wall 994 | gb_free 9.6 | wall 76330
KL Stats: Epoch 60 Divergences: Uniform: 5.253673076759979 Unigram: 4.0697068444374835
2022-03-16 13:55:21 | INFO | fairseq.trainer | begin training epoch 61
2022-03-16 13:55:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:57:14 | INFO | train_inner | epoch 061:     42 / 407 loss=5.43, ppl=43.1, wps=21122, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=24300, lr=0.00020286, gnorm=0.512, loss_scale=16, train_wall=247, gb_free=9.6, wall=76443
2022-03-16 14:01:42 | INFO | train_inner | epoch 061:    142 / 407 loss=5.408, ppl=42.45, wps=24480.7, ups=0.37, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.511, loss_scale=32, train_wall=244, gb_free=9.6, wall=76711
2022-03-16 14:05:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:06:12 | INFO | train_inner | epoch 061:    243 / 407 loss=5.418, ppl=42.76, wps=24245.9, ups=0.37, wpb=65534.2, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.511, loss_scale=16, train_wall=247, gb_free=9.6, wall=76981
2022-03-16 14:10:40 | INFO | train_inner | epoch 061:    343 / 407 loss=5.443, ppl=43.51, wps=24512.9, ups=0.37, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.512, loss_scale=16, train_wall=244, gb_free=9.6, wall=77248
2022-03-16 14:13:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:14:07 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 5.691 | ppl 51.68 | wps 36199 | wpb 511.9 | bsz 1 | num_updates 24664 | best_loss 5.691
2022-03-16 14:14:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 24664 updates
2022-03-16 14:14:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 14:14:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 14:14:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 61 @ 24664 updates, score 5.691) (writing took 2.203519514994696 seconds)
2022-03-16 14:14:09 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-16 14:14:09 | INFO | train | epoch 061 | loss 5.424 | ppl 42.94 | wps 23572.7 | ups 0.36 | wpb 65492.7 | bsz 127.9 | num_updates 24664 | lr 0.000201358 | gnorm 0.512 | loss_scale 32 | train_wall 994 | gb_free 9.6 | wall 77458
KL Stats: Epoch 61 Divergences: Uniform: 5.2593151419653354 Unigram: 4.072987575930829
2022-03-16 14:14:09 | INFO | fairseq.trainer | begin training epoch 62
2022-03-16 14:14:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:15:46 | INFO | train_inner | epoch 062:     36 / 407 loss=5.427, ppl=43.03, wps=21349.6, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=24700, lr=0.000201211, gnorm=0.516, loss_scale=32, train_wall=244, gb_free=9.6, wall=77555
2022-03-16 14:17:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:20:16 | INFO | train_inner | epoch 062:    137 / 407 loss=5.405, ppl=42.37, wps=24203.4, ups=0.37, wpb=65534.2, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.515, loss_scale=32, train_wall=247, gb_free=9.6, wall=77825
2022-03-16 14:23:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:24:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:24:49 | INFO | train_inner | epoch 062:    239 / 407 loss=5.415, ppl=42.67, wps=24018.1, ups=0.37, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.514, loss_scale=16, train_wall=249, gb_free=9.6, wall=78098
2022-03-16 14:29:17 | INFO | train_inner | epoch 062:    339 / 407 loss=5.433, ppl=43.22, wps=24505.2, ups=0.37, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.511, loss_scale=16, train_wall=244, gb_free=9.6, wall=78366
2022-03-16 14:30:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:32:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:32:55 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 5.694 | ppl 51.78 | wps 36239.9 | wpb 511.9 | bsz 1 | num_updates 25067 | best_loss 5.691
2022-03-16 14:32:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 25067 updates
2022-03-16 14:32:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 14:32:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 14:32:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt (epoch 62 @ 25067 updates, score 5.694) (writing took 1.3001455379417166 seconds)
2022-03-16 14:32:56 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-16 14:32:56 | INFO | train | epoch 062 | loss 5.418 | ppl 42.76 | wps 23418 | ups 0.36 | wpb 65492.3 | bsz 127.9 | num_updates 25067 | lr 0.000199733 | gnorm 0.514 | loss_scale 16 | train_wall 994 | gb_free 9.6 | wall 78585
KL Stats: Epoch 62 Divergences: Uniform: 5.266344394216843 Unigram: 4.078658057818272
2022-03-16 14:32:56 | INFO | fairseq.trainer | begin training epoch 63
2022-03-16 14:32:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:34:25 | INFO | train_inner | epoch 063:     33 / 407 loss=5.417, ppl=42.74, wps=21229.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=25100, lr=0.000199601, gnorm=0.511, loss_scale=16, train_wall=246, gb_free=9.6, wall=78674
2022-03-16 14:38:53 | INFO | train_inner | epoch 063:    133 / 407 loss=5.402, ppl=42.27, wps=24439.4, ups=0.37, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.512, loss_scale=32, train_wall=245, gb_free=9.6, wall=78942
2022-03-16 14:40:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:43:23 | INFO | train_inner | epoch 063:    234 / 407 loss=5.405, ppl=42.38, wps=24272.2, ups=0.37, wpb=65534.2, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.522, loss_scale=16, train_wall=246, gb_free=9.6, wall=79212
2022-03-16 14:47:50 | INFO | train_inner | epoch 063:    334 / 407 loss=5.428, ppl=43.05, wps=24528.5, ups=0.37, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.52, loss_scale=32, train_wall=244, gb_free=9.6, wall=79479
2022-03-16 14:51:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:51:41 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 5.69 | ppl 51.62 | wps 36136.2 | wpb 511.9 | bsz 1 | num_updates 25473 | best_loss 5.69
2022-03-16 14:51:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 25473 updates
2022-03-16 14:51:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 14:51:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 14:51:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 63 @ 25473 updates, score 5.69) (writing took 2.3247648529941216 seconds)
2022-03-16 14:51:44 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-16 14:51:44 | INFO | train | epoch 063 | loss 5.413 | ppl 42.62 | wps 23580.9 | ups 0.36 | wpb 65492.7 | bsz 127.9 | num_updates 25473 | lr 0.000198134 | gnorm 0.516 | loss_scale 32 | train_wall 993 | gb_free 9.6 | wall 79713
KL Stats: Epoch 63 Divergences: Uniform: 5.2697859019118285 Unigram: 4.081756490992566
2022-03-16 14:51:44 | INFO | fairseq.trainer | begin training epoch 64
2022-03-16 14:51:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:52:56 | INFO | train_inner | epoch 064:     27 / 407 loss=5.42, ppl=42.83, wps=21338.6, ups=0.33, wpb=65360.1, bsz=127.7, num_updates=25500, lr=0.00019803, gnorm=0.515, loss_scale=32, train_wall=244, gb_free=9.6, wall=79785
2022-03-16 14:53:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:57:27 | INFO | train_inner | epoch 064:    128 / 407 loss=5.39, ppl=41.94, wps=24204.6, ups=0.37, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.518, loss_scale=32, train_wall=247, gb_free=9.6, wall=80056
2022-03-16 14:58:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:01:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:02:00 | INFO | train_inner | epoch 064:    230 / 407 loss=5.407, ppl=42.43, wps=24020.5, ups=0.37, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.52, loss_scale=16, train_wall=249, gb_free=9.6, wall=80329
2022-03-16 15:06:28 | INFO | train_inner | epoch 064:    330 / 407 loss=5.415, ppl=42.67, wps=24447.1, ups=0.37, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.515, loss_scale=16, train_wall=245, gb_free=9.6, wall=80597
2022-03-16 15:09:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:10:31 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 5.689 | ppl 51.59 | wps 36139.3 | wpb 511.9 | bsz 1 | num_updates 25877 | best_loss 5.689
2022-03-16 15:10:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25877 updates
2022-03-16 15:10:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 15:10:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 15:10:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 64 @ 25877 updates, score 5.689) (writing took 2.278310436056927 seconds)
2022-03-16 15:10:33 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-16 15:10:33 | INFO | train | epoch 064 | loss 5.408 | ppl 42.45 | wps 23435.1 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 25877 | lr 0.000196582 | gnorm 0.517 | loss_scale 32 | train_wall 995 | gb_free 9.6 | wall 80842
KL Stats: Epoch 64 Divergences: Uniform: 5.276561064048248 Unigram: 4.0859632757284645
2022-03-16 15:10:33 | INFO | fairseq.trainer | begin training epoch 65
2022-03-16 15:10:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 15:11:35 | INFO | train_inner | epoch 065:     23 / 407 loss=5.419, ppl=42.79, wps=21310.5, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=0.513, loss_scale=32, train_wall=244, gb_free=9.6, wall=80904
2022-03-16 15:13:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:16:06 | INFO | train_inner | epoch 065:    124 / 407 loss=5.392, ppl=41.98, wps=24131.3, ups=0.37, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.513, loss_scale=32, train_wall=248, gb_free=9.6, wall=81175
2022-03-16 15:19:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:20:36 | INFO | train_inner | epoch 065:    225 / 407 loss=5.401, ppl=42.26, wps=24318.4, ups=0.37, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.518, loss_scale=32, train_wall=246, gb_free=9.6, wall=81445
2022-03-16 15:24:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:25:04 | INFO | train_inner | epoch 065:    326 / 407 loss=5.415, ppl=42.66, wps=24421.7, ups=0.37, wpb=65534.2, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.517, loss_scale=32, train_wall=245, gb_free=9.6, wall=81713
2022-03-16 15:26:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:28:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:29:15 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 5.679 | ppl 51.23 | wps 36385.6 | wpb 511.9 | bsz 1 | num_updates 26280 | best_loss 5.679
2022-03-16 15:29:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 26280 updates
2022-03-16 15:29:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 15:29:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 15:29:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 65 @ 26280 updates, score 5.679) (writing took 2.3427099629770964 seconds)
2022-03-16 15:29:18 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-16 15:29:18 | INFO | train | epoch 065 | loss 5.403 | ppl 42.32 | wps 23462.5 | ups 0.36 | wpb 65492.3 | bsz 127.9 | num_updates 26280 | lr 0.000195069 | gnorm 0.515 | loss_scale 16 | train_wall 991 | gb_free 9.6 | wall 81967
KL Stats: Epoch 65 Divergences: Uniform: 5.279792604048333 Unigram: 4.086811056863386
2022-03-16 15:29:18 | INFO | fairseq.trainer | begin training epoch 66
2022-03-16 15:29:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 15:30:11 | INFO | train_inner | epoch 066:     20 / 407 loss=5.407, ppl=42.43, wps=21284.2, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=0.516, loss_scale=16, train_wall=244, gb_free=9.6, wall=82020
2022-03-16 15:34:37 | INFO | train_inner | epoch 066:    120 / 407 loss=5.375, ppl=41.49, wps=24649.3, ups=0.38, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.519, loss_scale=32, train_wall=243, gb_free=9.6, wall=82286
2022-03-16 15:35:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:39:06 | INFO | train_inner | epoch 066:    221 / 407 loss=5.398, ppl=42.15, wps=24402.2, ups=0.37, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.518, loss_scale=16, train_wall=245, gb_free=9.6, wall=82554
2022-03-16 15:43:31 | INFO | train_inner | epoch 066:    321 / 407 loss=5.408, ppl=42.46, wps=24648.7, ups=0.38, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.512, loss_scale=32, train_wall=243, gb_free=9.6, wall=82820
2022-03-16 15:46:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:47:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:47:56 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 5.683 | ppl 51.37 | wps 36394.7 | wpb 511.9 | bsz 1 | num_updates 26685 | best_loss 5.679
2022-03-16 15:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 26685 updates
2022-03-16 15:47:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 15:47:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 15:47:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt (epoch 66 @ 26685 updates, score 5.683) (writing took 1.3194736830191687 seconds)
2022-03-16 15:47:57 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-16 15:47:57 | INFO | train | epoch 066 | loss 5.399 | ppl 42.18 | wps 23688.8 | ups 0.36 | wpb 65492.6 | bsz 127.9 | num_updates 26685 | lr 0.000193583 | gnorm 0.516 | loss_scale 32 | train_wall 987 | gb_free 9.6 | wall 83086
KL Stats: Epoch 66 Divergences: Uniform: 5.28661335181944 Unigram: 4.091925993465364
2022-03-16 15:47:58 | INFO | fairseq.trainer | begin training epoch 67
2022-03-16 15:47:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 15:48:37 | INFO | train_inner | epoch 067:     15 / 407 loss=5.414, ppl=42.63, wps=21361, ups=0.33, wpb=65360.1, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=0.515, loss_scale=32, train_wall=244, gb_free=9.6, wall=83126
2022-03-16 15:53:03 | INFO | train_inner | epoch 067:    115 / 407 loss=5.366, ppl=41.23, wps=24672.9, ups=0.38, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.519, loss_scale=32, train_wall=242, gb_free=9.6, wall=83392
2022-03-16 15:53:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:57:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:57:34 | INFO | train_inner | epoch 067:    217 / 407 loss=5.394, ppl=42.04, wps=24180.7, ups=0.37, wpb=65534.2, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.521, loss_scale=16, train_wall=247, gb_free=9.6, wall=83663
2022-03-16 16:02:00 | INFO | train_inner | epoch 067:    317 / 407 loss=5.406, ppl=42.41, wps=24693.7, ups=0.38, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.519, loss_scale=16, train_wall=242, gb_free=9.6, wall=83928
2022-03-16 16:05:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 16:05:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 16:06:35 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 5.682 | ppl 51.34 | wps 36446.2 | wpb 511.9 | bsz 1 | num_updates 27089 | best_loss 5.679
2022-03-16 16:06:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 27089 updates
2022-03-16 16:06:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 16:06:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt
2022-03-16 16:06:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_last.pt (epoch 67 @ 27089 updates, score 5.682) (writing took 1.301456560031511 seconds)
2022-03-16 16:06:36 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-16 16:06:36 | INFO | train | epoch 067 | loss 5.394 | ppl 42.04 | wps 23658.6 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 27089 | lr 0.000192134 | gnorm 0.519 | loss_scale 16 | train_wall 986 | gb_free 9.6 | wall 84205
KL Stats: Epoch 67 Divergences: Uniform: 5.293316850196133 Unigram: 4.096963407601334
2022-03-16 16:06:36 | INFO | fairseq.trainer | begin training epoch 68
2022-03-16 16:06:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 16:07:05 | INFO | train_inner | epoch 068:     11 / 407 loss=5.407, ppl=42.43, wps=21384.7, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=0.516, loss_scale=16, train_wall=244, gb_free=9.6, wall=84234
2022-03-16 16:11:31 | INFO | train_inner | epoch 068:    111 / 407 loss=5.365, ppl=41.21, wps=24685.6, ups=0.38, wpb=65534.2, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.521, loss_scale=16, train_wall=242, gb_free=9.6, wall=84499
2022-03-16 16:13:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 16:15:59 | INFO | train_inner | epoch 068:    212 / 407 loss=5.384, ppl=41.77, wps=24418.9, ups=0.37, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.518, loss_scale=16, train_wall=245, gb_free=9.6, wall=84768
2022-03-16 16:20:25 | INFO | train_inner | epoch 068:    312 / 407 loss=5.403, ppl=42.31, wps=24678.5, ups=0.38, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.513, loss_scale=32, train_wall=242, gb_free=9.6, wall=85033
2022-03-16 16:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 16:25:13 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 5.679 | ppl 51.23 | wps 36479 | wpb 511.9 | bsz 1 | num_updates 27495 | best_loss 5.679
2022-03-16 16:25:13 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 16:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 27495 updates
2022-03-16 16:25:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 16:25:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt
2022-03-16 16:25:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.01_0.015_0.975/checkpoint_best.pt (epoch 68 @ 27495 updates, score 5.679) (writing took 2.21285439201165 seconds)
2022-03-16 16:25:15 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-16 16:25:15 | INFO | train | epoch 068 | loss 5.39 | ppl 41.92 | wps 23759.1 | ups 0.36 | wpb 65492.7 | bsz 127.9 | num_updates 27495 | lr 0.00019071 | gnorm 0.518 | loss_scale 32 | train_wall 986 | gb_free 9.6 | wall 85324
2022-03-16 16:25:15 | INFO | fairseq_cli.train | done training in 85323.7 seconds
