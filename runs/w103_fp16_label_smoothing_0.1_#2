Sender: LSF System <lsfadmin@eu-g3-076>
Subject: Job 204577722: <w103_fp16_label_smoothing_0.1_#2> in cluster <euler> Exited

Job <w103_fp16_label_smoothing_0.1_#2> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 08:44:39 2022
Job was executed on host(s) <eu-g3-076>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 09:28:19 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 09:28:19 2022
Terminated at Fri Feb 11 09:28:53 2022
Results reported at Fri Feb 11 09:28:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521652 --fp16 --max-update 5000000
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.83 sec.
    Max Memory :                                 4140 MB
    Average Memory :                             307.00 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15860.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   34 sec.
    Turnaround time :                            2654 sec.

The output (if any) follows:

2022-02-11 09:28:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521652, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521652, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 09:28:30 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 09:28:36 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 09:28:36 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 09:28:36 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 09:28:36 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 09:28:36 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 09:28:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 09:28:36 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 09:28:45 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 09:28:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 09:28:45 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 09:28:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 09:28:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 09:28:45 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 09:28:45 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-11 09:28:45 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-11 09:28:45 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 09:28:48 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 09:28:49 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 09:28:49 | INFO | fairseq_cli.train | Start iterating over samples
> /cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py(50)label_smoothed_nll_loss()
-> loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
(Pdb) 
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 81, in forward
    loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 112, in compute_loss
    loss, nll_loss = label_smoothed_nll_loss(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 50, in label_smoothed_nll_loss
    loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 50, in label_smoothed_nll_loss
    loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 204594665: <w103_fp16_label_smoothing_0.1_#2> in cluster <euler> Exited

Job <w103_fp16_label_smoothing_0.1_#2> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 12:05:25 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 14:05:34 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 14:05:34 2022
Terminated at Fri Feb 11 15:15:28 2022
Results reported at Fri Feb 11 15:15:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521652 --fp16 --max-update 5000000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   4178.16 sec.
    Max Memory :                                 3678 MB
    Average Memory :                             3095.61 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16322.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   4195 sec.
    Turnaround time :                            11403 sec.

The output (if any) follows:

2022-02-11 14:05:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521652, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521652, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 14:05:46 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 14:05:51 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 14:05:51 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 14:05:51 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 14:05:51 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 14:05:51 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 14:05:51 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 14:05:51 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 14:05:59 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 14:05:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 14:05:59 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 14:05:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 14:05:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 14:05:59 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 14:05:59 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-11 14:05:59 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-11 14:05:59 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 14:05:59 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 14:06:00 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 14:06:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 14:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 14:06:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 14:06:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 14:06:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 14:16:14 | INFO | train_inner | epoch 001:    104 / 1576 loss=18.556, nll_loss=18.399, ppl=345706, wps=11684.1, ups=0.18, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.699, loss_scale=8, train_wall=600, gb_free=8.8, wall=615
2022-02-11 14:25:35 | INFO | train_inner | epoch 001:    204 / 1576 loss=16.258, nll_loss=15.849, ppl=59022.8, wps=11670.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.62, loss_scale=8, train_wall=551, gb_free=8.8, wall=1176
2022-02-11 14:34:57 | INFO | train_inner | epoch 001:    304 / 1576 loss=14.036, nll_loss=13.357, ppl=10490.6, wps=11673.9, ups=0.18, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.234, loss_scale=16, train_wall=551, gb_free=8.8, wall=1738
2022-02-11 14:44:18 | INFO | train_inner | epoch 001:    404 / 1576 loss=12.338, nll_loss=11.382, ppl=2669.67, wps=11677.7, ups=0.18, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.737, loss_scale=16, train_wall=551, gb_free=8.8, wall=2299
2022-02-11 14:53:39 | INFO | train_inner | epoch 001:    504 / 1576 loss=11.725, nll_loss=10.61, ppl=1563.17, wps=11673.6, ups=0.18, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.459, loss_scale=16, train_wall=551, gb_free=8.8, wall=2860
2022-02-11 15:03:01 | INFO | train_inner | epoch 001:    604 / 1576 loss=11.514, nll_loss=10.343, ppl=1299.18, wps=11673.4, ups=0.18, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.431, loss_scale=32, train_wall=551, gb_free=8.8, wall=3422
2022-02-11 15:12:22 | INFO | train_inner | epoch 001:    704 / 1576 loss=11.316, nll_loss=10.116, ppl=1109.93, wps=11672.4, ups=0.18, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.444, loss_scale=32, train_wall=551, gb_free=8.8, wall=3983
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 328, in extract_features_scriptable
    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-076>
Subject: Job 204616324: <w103_fp16_label_smoothing_0.1_#2> in cluster <euler> Done

Job <w103_fp16_label_smoothing_0.1_#2> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 16:38:42 2022
Job was executed on host(s) <eu-g3-076>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 17:39:13 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 17:39:13 2022
Terminated at Mon Feb 14 23:58:00 2022
Results reported at Mon Feb 14 23:58:00 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521652 --fp16 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   281807.62 sec.
    Max Memory :                                 19338 MB
    Average Memory :                             3093.88 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               662.00 MB
    Max Swap :                                   1080 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   281926 sec.
    Turnaround time :                            285558 sec.

The output (if any) follows:

2022-02-11 17:39:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521652, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521652, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 17:39:23 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 17:39:29 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 17:39:29 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 17:39:29 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 17:39:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 17:39:29 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 17:39:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 17:39:29 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 17:39:38 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 17:39:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:38 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 17:39:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:38 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 17:39:38 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 17:39:38 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-11 17:39:38 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-11 17:39:38 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 17:39:41 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 17:39:42 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 17:39:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 17:39:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 17:40:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 17:40:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 17:40:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 17:40:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 17:49:59 | INFO | train_inner | epoch 001:    105 / 1576 loss=18.272, nll_loss=18.081, ppl=277339, wps=11584.4, ups=0.18, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.352, loss_scale=4, train_wall=603, gb_free=8.8, wall=621
2022-02-11 17:59:20 | INFO | train_inner | epoch 001:    205 / 1576 loss=15.898, nll_loss=15.448, ppl=44706.5, wps=11685.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.627, loss_scale=4, train_wall=550, gb_free=8.8, wall=1182
2022-02-11 18:08:40 | INFO | train_inner | epoch 001:    305 / 1576 loss=13.729, nll_loss=13.012, ppl=8260.65, wps=11700.8, ups=0.18, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.142, loss_scale=8, train_wall=550, gb_free=8.8, wall=1742
2022-02-11 18:18:00 | INFO | train_inner | epoch 001:    405 / 1576 loss=12.083, nll_loss=11.098, ppl=2191.82, wps=11690.7, ups=0.18, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.63, loss_scale=8, train_wall=550, gb_free=8.8, wall=2303
2022-02-11 18:27:21 | INFO | train_inner | epoch 001:    505 / 1576 loss=11.435, nll_loss=10.29, ppl=1251.83, wps=11679.5, ups=0.18, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.479, loss_scale=8, train_wall=550, gb_free=8.8, wall=2864
2022-02-11 18:36:43 | INFO | train_inner | epoch 001:    605 / 1576 loss=11.118, nll_loss=9.907, ppl=960.1, wps=11673.7, ups=0.18, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.507, loss_scale=16, train_wall=551, gb_free=8.8, wall=3425
2022-02-11 18:46:04 | INFO | train_inner | epoch 001:    705 / 1576 loss=10.858, nll_loss=9.609, ppl=780.87, wps=11674.1, ups=0.18, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.564, loss_scale=16, train_wall=551, gb_free=8.8, wall=3987
2022-02-11 18:55:26 | INFO | train_inner | epoch 001:    805 / 1576 loss=10.639, nll_loss=9.359, ppl=656.72, wps=11671.9, ups=0.18, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.621, loss_scale=32, train_wall=551, gb_free=8.8, wall=4548
2022-02-11 19:04:48 | INFO | train_inner | epoch 001:    905 / 1576 loss=10.454, nll_loss=9.151, ppl=568.31, wps=11664.3, ups=0.18, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.723, loss_scale=32, train_wall=551, gb_free=8.8, wall=5110
2022-02-11 19:14:09 | INFO | train_inner | epoch 001:   1005 / 1576 loss=10.272, nll_loss=8.945, ppl=492.73, wps=11667.6, ups=0.18, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.738, loss_scale=32, train_wall=551, gb_free=8.8, wall=5672
2022-02-11 19:16:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:23:36 | INFO | train_inner | epoch 001:   1106 / 1576 loss=10.113, nll_loss=8.765, ppl=435.14, wps=11558, ups=0.18, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.783, loss_scale=32, train_wall=556, gb_free=8.8, wall=6239
2022-02-11 19:32:58 | INFO | train_inner | epoch 001:   1206 / 1576 loss=9.967, nll_loss=8.601, ppl=388.32, wps=11664.2, ups=0.18, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.792, loss_scale=32, train_wall=551, gb_free=8.8, wall=6800
2022-02-11 19:40:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:42:25 | INFO | train_inner | epoch 001:   1307 / 1576 loss=9.833, nll_loss=8.45, ppl=349.63, wps=11553, ups=0.18, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.81, loss_scale=32, train_wall=557, gb_free=8.8, wall=7368
2022-02-11 19:51:47 | INFO | train_inner | epoch 001:   1407 / 1576 loss=9.711, nll_loss=8.312, ppl=317.82, wps=11666.2, ups=0.18, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.828, loss_scale=32, train_wall=551, gb_free=8.8, wall=7929
2022-02-11 20:01:09 | INFO | train_inner | epoch 001:   1507 / 1576 loss=9.6, nll_loss=8.186, ppl=291.18, wps=11657.7, ups=0.18, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.812, loss_scale=32, train_wall=551, gb_free=8.8, wall=8492
2022-02-11 20:05:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:07:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 20:07:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.317 | nll_loss 7.858 | ppl 232.06 | wps 32151.3 | wpb 1021.8 | bsz 2 | num_updates 1568
2022-02-11 20:07:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1568 updates
2022-02-11 20:07:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint1.pt
2022-02-11 20:07:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint1.pt
2022-02-11 20:08:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint1.pt (epoch 1 @ 1568 updates, score 9.317) (writing took 32.30669019604102 seconds)
2022-02-11 20:08:11 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-11 20:08:11 | INFO | train | epoch 001 | loss 11.509 | nll_loss 10.377 | ppl 1330.23 | wps 11594.1 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 1568 | lr 0.000196061 | gnorm 0.956 | loss_scale 32 | train_wall 8700 | gb_free 8.8 | wall 8914
2022-02-11 20:08:11 | INFO | fairseq.trainer | begin training epoch 2
2022-02-11 20:08:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 20:11:11 | INFO | train_inner | epoch 002:     32 / 1576 loss=9.473, nll_loss=8.043, ppl=263.66, wps=10799, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=0.852, loss_scale=32, train_wall=552, gb_free=8.8, wall=9093
2022-02-11 20:20:33 | INFO | train_inner | epoch 002:    132 / 1576 loss=9.373, nll_loss=7.93, ppl=243.84, wps=11655.6, ups=0.18, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.807, loss_scale=32, train_wall=551, gb_free=8.8, wall=9655
2022-02-11 20:29:55 | INFO | train_inner | epoch 002:    232 / 1576 loss=9.27, nll_loss=7.812, ppl=224.79, wps=11655.9, ups=0.18, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.806, loss_scale=64, train_wall=551, gb_free=8.8, wall=10218
2022-02-11 20:30:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:39:23 | INFO | train_inner | epoch 002:    333 / 1576 loss=9.186, nll_loss=7.718, ppl=210.54, wps=11547.5, ups=0.18, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.792, loss_scale=32, train_wall=557, gb_free=8.8, wall=10785
2022-02-11 20:48:45 | INFO | train_inner | epoch 002:    433 / 1576 loss=9.096, nll_loss=7.617, ppl=196.29, wps=11656.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.772, loss_scale=32, train_wall=551, gb_free=8.8, wall=11347
2022-02-11 20:57:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:58:13 | INFO | train_inner | epoch 002:    534 / 1576 loss=9.007, nll_loss=7.516, ppl=183.04, wps=11530.7, ups=0.18, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.768, loss_scale=32, train_wall=557, gb_free=8.8, wall=11916
2022-02-11 21:07:36 | INFO | train_inner | epoch 002:    634 / 1576 loss=8.932, nll_loss=7.431, ppl=172.55, wps=11649.3, ups=0.18, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.761, loss_scale=32, train_wall=552, gb_free=8.8, wall=12478
2022-02-11 21:16:58 | INFO | train_inner | epoch 002:    734 / 1576 loss=8.871, nll_loss=7.363, ppl=164.61, wps=11653.7, ups=0.18, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.736, loss_scale=32, train_wall=552, gb_free=8.8, wall=13041
2022-02-11 21:23:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 21:26:26 | INFO | train_inner | epoch 002:    835 / 1576 loss=8.801, nll_loss=7.283, ppl=155.78, wps=11540.9, ups=0.18, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.735, loss_scale=32, train_wall=557, gb_free=8.8, wall=13609
2022-02-11 21:31:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 21:35:53 | INFO | train_inner | epoch 002:    936 / 1576 loss=8.729, nll_loss=7.203, ppl=147.33, wps=11558.8, ups=0.18, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.716, loss_scale=16, train_wall=556, gb_free=8.8, wall=14176
2022-02-11 21:45:15 | INFO | train_inner | epoch 002:   1036 / 1576 loss=8.66, nll_loss=7.124, ppl=139.52, wps=11662.4, ups=0.18, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.7, loss_scale=16, train_wall=551, gb_free=8.8, wall=14738
2022-02-11 21:54:37 | INFO | train_inner | epoch 002:   1136 / 1576 loss=8.606, nll_loss=7.064, ppl=133.8, wps=11668.3, ups=0.18, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.707, loss_scale=16, train_wall=551, gb_free=8.8, wall=15299
2022-02-11 22:03:59 | INFO | train_inner | epoch 002:   1236 / 1576 loss=8.551, nll_loss=7.002, ppl=128.17, wps=11666.1, ups=0.18, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.683, loss_scale=32, train_wall=551, gb_free=8.8, wall=15861
2022-02-11 22:13:21 | INFO | train_inner | epoch 002:   1336 / 1576 loss=8.501, nll_loss=6.946, ppl=123.26, wps=11656.5, ups=0.18, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.672, loss_scale=32, train_wall=552, gb_free=8.8, wall=16423
2022-02-11 22:19:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 22:20:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:22:54 | INFO | train_inner | epoch 002:   1438 / 1576 loss=8.44, nll_loss=6.876, ppl=117.46, wps=11425.5, ups=0.17, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.674, loss_scale=16, train_wall=563, gb_free=8.8, wall=16997
2022-02-11 22:32:16 | INFO | train_inner | epoch 002:   1538 / 1576 loss=8.399, nll_loss=6.83, ppl=113.8, wps=11666.6, ups=0.18, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.655, loss_scale=16, train_wall=551, gb_free=8.8, wall=17559
2022-02-11 22:35:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 22:35:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.205 | nll_loss 6.568 | ppl 94.89 | wps 32015.2 | wpb 1021.8 | bsz 2 | num_updates 3138 | best_loss 8.205
2022-02-11 22:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3138 updates
2022-02-11 22:35:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint2.pt
2022-02-11 22:36:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint2.pt
2022-02-11 22:36:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint2.pt (epoch 2 @ 3138 updates, score 8.205) (writing took 32.976702872198075 seconds)
2022-02-11 22:36:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-11 22:36:25 | INFO | train | epoch 002 | loss 8.83 | nll_loss 7.316 | ppl 159.36 | wps 11563.1 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 3138 | lr 0.000392272 | gnorm 0.732 | loss_scale 16 | train_wall 8685 | gb_free 8.8 | wall 17807
2022-02-11 22:36:25 | INFO | fairseq.trainer | begin training epoch 3
2022-02-11 22:36:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 22:42:12 | INFO | train_inner | epoch 003:     62 / 1576 loss=8.327, nll_loss=6.749, ppl=107.53, wps=10898.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.656, loss_scale=16, train_wall=545, gb_free=8.8, wall=18155
2022-02-11 22:51:34 | INFO | train_inner | epoch 003:    162 / 1576 loss=8.252, nll_loss=6.664, ppl=101.44, wps=11666.5, ups=0.18, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.642, loss_scale=32, train_wall=551, gb_free=8.8, wall=18716
2022-02-11 23:00:56 | INFO | train_inner | epoch 003:    262 / 1576 loss=8.239, nll_loss=6.649, ppl=100.36, wps=11657.9, ups=0.18, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.658, loss_scale=32, train_wall=551, gb_free=8.8, wall=19278
2022-02-11 23:04:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:10:24 | INFO | train_inner | epoch 003:    363 / 1576 loss=8.184, nll_loss=6.587, ppl=96.15, wps=11550.7, ups=0.18, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.618, loss_scale=16, train_wall=557, gb_free=8.8, wall=19846
2022-02-11 23:19:45 | INFO | train_inner | epoch 003:    463 / 1576 loss=8.152, nll_loss=6.551, ppl=93.8, wps=11675.7, ups=0.18, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.635, loss_scale=16, train_wall=551, gb_free=8.8, wall=20407
2022-02-11 23:29:06 | INFO | train_inner | epoch 003:    563 / 1576 loss=8.117, nll_loss=6.512, ppl=91.26, wps=11673.7, ups=0.18, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.611, loss_scale=32, train_wall=551, gb_free=8.8, wall=20969
2022-02-11 23:31:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:38:33 | INFO | train_inner | epoch 003:    664 / 1576 loss=8.086, nll_loss=6.477, ppl=89.1, wps=11557.1, ups=0.18, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.617, loss_scale=16, train_wall=556, gb_free=8.8, wall=21536
2022-02-11 23:47:55 | INFO | train_inner | epoch 003:    764 / 1576 loss=8.069, nll_loss=6.458, ppl=87.91, wps=11675.9, ups=0.18, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.597, loss_scale=16, train_wall=551, gb_free=8.8, wall=22097
2022-02-11 23:57:16 | INFO | train_inner | epoch 003:    864 / 1576 loss=8.026, nll_loss=6.41, ppl=85.05, wps=11672.4, ups=0.18, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.589, loss_scale=32, train_wall=551, gb_free=8.8, wall=22658
2022-02-11 23:57:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:06:43 | INFO | train_inner | epoch 003:    965 / 1576 loss=8.009, nll_loss=6.391, ppl=83.92, wps=11556.2, ups=0.18, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.582, loss_scale=16, train_wall=556, gb_free=8.8, wall=23226
2022-02-12 00:16:05 | INFO | train_inner | epoch 003:   1065 / 1576 loss=7.97, nll_loss=6.347, ppl=81.39, wps=11671.2, ups=0.18, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.553, loss_scale=16, train_wall=551, gb_free=8.8, wall=23787
2022-02-12 00:24:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:25:32 | INFO | train_inner | epoch 003:   1166 / 1576 loss=7.936, nll_loss=6.309, ppl=79.29, wps=11557.3, ups=0.18, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.556, loss_scale=16, train_wall=556, gb_free=8.8, wall=24354
2022-02-12 00:34:53 | INFO | train_inner | epoch 003:   1266 / 1576 loss=7.909, nll_loss=6.279, ppl=77.66, wps=11668.9, ups=0.18, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.531, loss_scale=16, train_wall=551, gb_free=8.8, wall=24916
2022-02-12 00:44:15 | INFO | train_inner | epoch 003:   1366 / 1576 loss=7.885, nll_loss=6.253, ppl=76.24, wps=11675.5, ups=0.18, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.525, loss_scale=16, train_wall=551, gb_free=8.8, wall=25477
2022-02-12 00:53:36 | INFO | train_inner | epoch 003:   1466 / 1576 loss=7.85, nll_loss=6.213, ppl=74.16, wps=11670.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.524, loss_scale=32, train_wall=551, gb_free=8.8, wall=26039
2022-02-12 00:55:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:03:03 | INFO | train_inner | epoch 003:   1567 / 1576 loss=7.833, nll_loss=6.194, ppl=73.22, wps=11558.6, ups=0.18, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.522, loss_scale=16, train_wall=556, gb_free=8.8, wall=26606
2022-02-12 01:03:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 01:03:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.671 | nll_loss 5.99 | ppl 63.55 | wps 32046.6 | wpb 1021.8 | bsz 2 | num_updates 4709 | best_loss 7.671
2022-02-12 01:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4709 updates
2022-02-12 01:03:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint3.pt
2022-02-12 01:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint3.pt
2022-02-12 01:04:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint3.pt (epoch 3 @ 4709 updates, score 7.671) (writing took 33.34667083900422 seconds)
2022-02-12 01:04:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-12 01:04:29 | INFO | train | epoch 003 | loss 8.044 | nll_loss 6.43 | ppl 86.23 | wps 11581.8 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 4709 | lr 0.000460825 | gnorm 0.586 | loss_scale 16 | train_wall 8676 | gb_free 8.8 | wall 26691
2022-02-12 01:04:29 | INFO | fairseq.trainer | begin training epoch 4
2022-02-12 01:04:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 01:13:00 | INFO | train_inner | epoch 004:     91 / 1576 loss=7.742, nll_loss=6.091, ppl=68.18, wps=10879, ups=0.17, wpb=64958.8, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.523, loss_scale=16, train_wall=546, gb_free=8.8, wall=27203
2022-02-12 01:20:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:22:28 | INFO | train_inner | epoch 004:    192 / 1576 loss=7.717, nll_loss=6.063, ppl=66.86, wps=11554.4, ups=0.18, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.512, loss_scale=16, train_wall=556, gb_free=8.8, wall=27770
2022-02-12 01:31:49 | INFO | train_inner | epoch 004:    292 / 1576 loss=7.712, nll_loss=6.058, ppl=66.6, wps=11669.2, ups=0.18, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.503, loss_scale=16, train_wall=551, gb_free=8.8, wall=28331
2022-02-12 01:41:10 | INFO | train_inner | epoch 004:    392 / 1576 loss=7.701, nll_loss=6.046, ppl=66.07, wps=11678.5, ups=0.18, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.503, loss_scale=16, train_wall=550, gb_free=8.8, wall=28893
2022-02-12 01:50:32 | INFO | train_inner | epoch 004:    492 / 1576 loss=7.684, nll_loss=6.027, ppl=65.2, wps=11665.4, ups=0.18, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.492, loss_scale=32, train_wall=551, gb_free=8.8, wall=29454
2022-02-12 01:52:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:59:59 | INFO | train_inner | epoch 004:    593 / 1576 loss=7.667, nll_loss=6.008, ppl=64.36, wps=11558.4, ups=0.18, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.492, loss_scale=16, train_wall=556, gb_free=8.8, wall=30021
2022-02-12 02:09:20 | INFO | train_inner | epoch 004:    693 / 1576 loss=7.663, nll_loss=6.003, ppl=64.15, wps=11674.1, ups=0.18, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.504, loss_scale=16, train_wall=551, gb_free=8.8, wall=30583
2022-02-12 02:18:42 | INFO | train_inner | epoch 004:    793 / 1576 loss=7.635, nll_loss=5.972, ppl=62.78, wps=11677.5, ups=0.18, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.477, loss_scale=32, train_wall=550, gb_free=8.8, wall=31144
2022-02-12 02:20:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:28:09 | INFO | train_inner | epoch 004:    894 / 1576 loss=7.639, nll_loss=5.977, ppl=62.98, wps=11561.1, ups=0.18, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.488, loss_scale=16, train_wall=556, gb_free=8.8, wall=31711
2022-02-12 02:37:30 | INFO | train_inner | epoch 004:    994 / 1576 loss=7.608, nll_loss=5.942, ppl=61.49, wps=11675.1, ups=0.18, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.475, loss_scale=16, train_wall=551, gb_free=8.8, wall=32272
2022-02-12 02:45:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:46:57 | INFO | train_inner | epoch 004:   1095 / 1576 loss=7.616, nll_loss=5.952, ppl=61.9, wps=11561.7, ups=0.18, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.475, loss_scale=16, train_wall=556, gb_free=8.8, wall=32839
2022-02-12 02:56:18 | INFO | train_inner | epoch 004:   1195 / 1576 loss=7.594, nll_loss=5.927, ppl=60.86, wps=11679.5, ups=0.18, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.475, loss_scale=16, train_wall=550, gb_free=8.8, wall=33400
2022-02-12 03:05:39 | INFO | train_inner | epoch 004:   1295 / 1576 loss=7.59, nll_loss=5.922, ppl=60.62, wps=11683.8, ups=0.18, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.496, loss_scale=16, train_wall=550, gb_free=8.8, wall=33961
2022-02-12 03:14:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:15:06 | INFO | train_inner | epoch 004:   1396 / 1576 loss=7.569, nll_loss=5.899, ppl=59.69, wps=11559.9, ups=0.18, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.46, loss_scale=16, train_wall=556, gb_free=8.8, wall=34528
2022-02-12 03:24:27 | INFO | train_inner | epoch 004:   1496 / 1576 loss=7.567, nll_loss=5.897, ppl=59.57, wps=11679.8, ups=0.18, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.473, loss_scale=16, train_wall=550, gb_free=8.8, wall=35089
2022-02-12 03:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 03:31:58 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.431 | nll_loss 5.746 | ppl 53.65 | wps 32048.9 | wpb 1021.8 | bsz 2 | num_updates 6280 | best_loss 7.431
2022-02-12 03:31:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6280 updates
2022-02-12 03:31:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint4.pt
2022-02-12 03:32:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint4.pt
2022-02-12 03:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint4.pt (epoch 4 @ 6280 updates, score 7.431) (writing took 32.92728643817827 seconds)
2022-02-12 03:32:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-12 03:32:31 | INFO | train | epoch 004 | loss 7.642 | nll_loss 5.98 | ppl 63.12 | wps 11585.8 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 6280 | lr 0.000399043 | gnorm 0.49 | loss_scale 16 | train_wall 8673 | gb_free 8.8 | wall 35573
2022-02-12 03:32:31 | INFO | fairseq.trainer | begin training epoch 5
2022-02-12 03:32:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 03:34:23 | INFO | train_inner | epoch 005:     20 / 1576 loss=7.547, nll_loss=5.874, ppl=58.65, wps=10899.9, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.51, loss_scale=16, train_wall=545, gb_free=8.8, wall=35685
2022-02-12 03:43:44 | INFO | train_inner | epoch 005:    120 / 1576 loss=7.449, nll_loss=5.764, ppl=54.33, wps=11668.5, ups=0.18, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.46, loss_scale=32, train_wall=551, gb_free=8.8, wall=36247
2022-02-12 03:52:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:53:12 | INFO | train_inner | epoch 005:    221 / 1576 loss=7.459, nll_loss=5.775, ppl=54.74, wps=11550, ups=0.18, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.462, loss_scale=16, train_wall=557, gb_free=8.8, wall=36814
2022-02-12 04:02:33 | INFO | train_inner | epoch 005:    321 / 1576 loss=7.452, nll_loss=5.766, ppl=54.43, wps=11670.1, ups=0.18, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.479, loss_scale=16, train_wall=551, gb_free=8.8, wall=37376
2022-02-12 04:11:55 | INFO | train_inner | epoch 005:    421 / 1576 loss=7.454, nll_loss=5.77, ppl=54.57, wps=11678.8, ups=0.18, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.459, loss_scale=16, train_wall=550, gb_free=8.8, wall=37937
2022-02-12 04:21:16 | INFO | train_inner | epoch 005:    521 / 1576 loss=7.439, nll_loss=5.752, ppl=53.9, wps=11668, ups=0.18, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.471, loss_scale=32, train_wall=551, gb_free=8.8, wall=38499
2022-02-12 04:22:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:30:43 | INFO | train_inner | epoch 005:    622 / 1576 loss=7.441, nll_loss=5.755, ppl=54.01, wps=11558.1, ups=0.18, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.487, loss_scale=16, train_wall=556, gb_free=8.8, wall=39066
2022-02-12 04:40:05 | INFO | train_inner | epoch 005:    722 / 1576 loss=7.449, nll_loss=5.764, ppl=54.36, wps=11671.7, ups=0.18, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.456, loss_scale=16, train_wall=551, gb_free=8.8, wall=39627
2022-02-12 04:48:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:49:32 | INFO | train_inner | epoch 005:    823 / 1576 loss=7.434, nll_loss=5.747, ppl=53.72, wps=11554.1, ups=0.18, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.471, loss_scale=16, train_wall=556, gb_free=8.8, wall=40194
2022-02-12 04:58:54 | INFO | train_inner | epoch 005:    923 / 1576 loss=7.446, nll_loss=5.761, ppl=54.21, wps=11670.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.47, loss_scale=16, train_wall=551, gb_free=8.8, wall=40756
2022-02-12 05:08:15 | INFO | train_inner | epoch 005:   1023 / 1576 loss=7.426, nll_loss=5.738, ppl=53.36, wps=11670.3, ups=0.18, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.475, loss_scale=16, train_wall=551, gb_free=8.8, wall=41317
2022-02-12 05:17:37 | INFO | train_inner | epoch 005:   1123 / 1576 loss=7.428, nll_loss=5.741, ppl=53.49, wps=11666.2, ups=0.18, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.449, loss_scale=32, train_wall=551, gb_free=8.8, wall=41879
2022-02-12 05:23:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:27:04 | INFO | train_inner | epoch 005:   1224 / 1576 loss=7.414, nll_loss=5.725, ppl=52.88, wps=11546.6, ups=0.18, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.464, loss_scale=16, train_wall=557, gb_free=8.8, wall=42447
2022-02-12 05:36:26 | INFO | train_inner | epoch 005:   1324 / 1576 loss=7.409, nll_loss=5.72, ppl=52.71, wps=11669.1, ups=0.18, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.45, loss_scale=16, train_wall=551, gb_free=8.8, wall=43008
2022-02-12 05:45:48 | INFO | train_inner | epoch 005:   1424 / 1576 loss=7.408, nll_loss=5.719, ppl=52.66, wps=11670.6, ups=0.18, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.475, loss_scale=16, train_wall=551, gb_free=8.8, wall=43570
2022-02-12 05:54:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:55:15 | INFO | train_inner | epoch 005:   1525 / 1576 loss=7.402, nll_loss=5.712, ppl=52.41, wps=11552.1, ups=0.18, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.455, loss_scale=16, train_wall=556, gb_free=8.8, wall=44137
2022-02-12 05:59:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 06:00:03 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.297 | nll_loss 5.555 | ppl 47.01 | wps 32186.1 | wpb 1021.8 | bsz 2 | num_updates 7851 | best_loss 7.297
2022-02-12 06:00:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7851 updates
2022-02-12 06:00:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint5.pt
2022-02-12 06:00:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint5.pt
2022-02-12 06:00:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint5.pt (epoch 5 @ 7851 updates, score 7.297) (writing took 32.82031698804349 seconds)
2022-02-12 06:00:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-12 06:00:36 | INFO | train | epoch 005 | loss 7.433 | nll_loss 5.747 | ppl 53.69 | wps 11580.6 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 7851 | lr 0.000356893 | gnorm 0.467 | loss_scale 16 | train_wall 8676 | gb_free 8.8 | wall 44458
2022-02-12 06:00:36 | INFO | fairseq.trainer | begin training epoch 6
2022-02-12 06:00:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 06:05:11 | INFO | train_inner | epoch 006:     49 / 1576 loss=7.347, nll_loss=5.65, ppl=50.21, wps=10892.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.49, loss_scale=16, train_wall=546, gb_free=8.8, wall=44734
2022-02-12 06:14:33 | INFO | train_inner | epoch 006:    149 / 1576 loss=7.3, nll_loss=5.597, ppl=48.4, wps=11674.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.46, loss_scale=16, train_wall=550, gb_free=8.8, wall=45295
2022-02-12 06:23:54 | INFO | train_inner | epoch 006:    249 / 1576 loss=7.313, nll_loss=5.611, ppl=48.87, wps=11673.2, ups=0.18, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.473, loss_scale=32, train_wall=551, gb_free=8.8, wall=45856
2022-02-12 06:30:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 06:33:21 | INFO | train_inner | epoch 006:    350 / 1576 loss=7.3, nll_loss=5.597, ppl=48.4, wps=11556.2, ups=0.18, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.462, loss_scale=16, train_wall=556, gb_free=8.8, wall=46424
2022-02-12 06:42:42 | INFO | train_inner | epoch 006:    450 / 1576 loss=7.315, nll_loss=5.614, ppl=48.97, wps=11677.4, ups=0.18, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.474, loss_scale=16, train_wall=550, gb_free=8.8, wall=46985
2022-02-12 06:52:04 | INFO | train_inner | epoch 006:    550 / 1576 loss=7.317, nll_loss=5.616, ppl=49.04, wps=11676.1, ups=0.18, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.468, loss_scale=16, train_wall=551, gb_free=8.8, wall=47546
2022-02-12 06:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:01:30 | INFO | train_inner | epoch 006:    651 / 1576 loss=7.302, nll_loss=5.599, ppl=48.48, wps=11563.9, ups=0.18, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.468, loss_scale=16, train_wall=556, gb_free=8.8, wall=48113
2022-02-12 07:10:52 | INFO | train_inner | epoch 006:    751 / 1576 loss=7.306, nll_loss=5.604, ppl=48.62, wps=11674.1, ups=0.18, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.468, loss_scale=16, train_wall=551, gb_free=8.8, wall=48674
2022-02-12 07:20:13 | INFO | train_inner | epoch 006:    851 / 1576 loss=7.312, nll_loss=5.611, ppl=48.86, wps=11675.2, ups=0.18, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.458, loss_scale=32, train_wall=551, gb_free=8.8, wall=49235
2022-02-12 07:22:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:29:40 | INFO | train_inner | epoch 006:    952 / 1576 loss=7.316, nll_loss=5.615, ppl=49.01, wps=11563.1, ups=0.18, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.48, loss_scale=16, train_wall=556, gb_free=8.8, wall=49802
2022-02-12 07:39:01 | INFO | train_inner | epoch 006:   1052 / 1576 loss=7.294, nll_loss=5.59, ppl=48.17, wps=11675.5, ups=0.18, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.46, loss_scale=16, train_wall=551, gb_free=8.8, wall=50364
2022-02-12 07:48:23 | INFO | train_inner | epoch 006:   1152 / 1576 loss=7.305, nll_loss=5.604, ppl=48.62, wps=11671.5, ups=0.18, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.462, loss_scale=32, train_wall=551, gb_free=8.8, wall=50925
2022-02-12 07:49:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:57:49 | INFO | train_inner | epoch 006:   1253 / 1576 loss=7.304, nll_loss=5.602, ppl=48.56, wps=11563.2, ups=0.18, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.472, loss_scale=16, train_wall=556, gb_free=8.8, wall=51492
2022-02-12 08:07:11 | INFO | train_inner | epoch 006:   1353 / 1576 loss=7.313, nll_loss=5.612, ppl=48.91, wps=11678.6, ups=0.18, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.47, loss_scale=16, train_wall=550, gb_free=8.8, wall=52053
2022-02-12 08:15:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:16:37 | INFO | train_inner | epoch 006:   1454 / 1576 loss=7.297, nll_loss=5.595, ppl=48.34, wps=11561.3, ups=0.18, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.452, loss_scale=16, train_wall=556, gb_free=8.8, wall=52620
2022-02-12 08:25:59 | INFO | train_inner | epoch 006:   1554 / 1576 loss=7.295, nll_loss=5.592, ppl=48.23, wps=11676.2, ups=0.18, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.465, loss_scale=16, train_wall=551, gb_free=8.8, wall=53181
2022-02-12 08:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 08:28:04 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.208 | nll_loss 5.465 | ppl 44.17 | wps 32308.3 | wpb 1021.8 | bsz 2 | num_updates 9422 | best_loss 7.208
2022-02-12 08:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9422 updates
2022-02-12 08:28:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint6.pt
2022-02-12 08:28:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint6.pt
2022-02-12 08:28:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint6.pt (epoch 6 @ 9422 updates, score 7.208) (writing took 33.07668070914224 seconds)
2022-02-12 08:28:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-12 08:28:37 | INFO | train | epoch 006 | loss 7.305 | nll_loss 5.603 | ppl 48.6 | wps 11586.2 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 9422 | lr 0.000325783 | gnorm 0.467 | loss_scale 16 | train_wall 8672 | gb_free 8.8 | wall 53340
2022-02-12 08:28:37 | INFO | fairseq.trainer | begin training epoch 7
2022-02-12 08:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 08:35:55 | INFO | train_inner | epoch 007:     78 / 1576 loss=7.202, nll_loss=5.487, ppl=44.86, wps=10887, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.453, loss_scale=16, train_wall=546, gb_free=8.8, wall=53778
2022-02-12 08:42:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 08:45:23 | INFO | train_inner | epoch 007:    179 / 1576 loss=7.201, nll_loss=5.485, ppl=44.8, wps=11552.3, ups=0.18, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.494, loss_scale=16, train_wall=556, gb_free=8.8, wall=54345
2022-02-12 08:54:44 | INFO | train_inner | epoch 007:    279 / 1576 loss=7.199, nll_loss=5.483, ppl=44.74, wps=11678.9, ups=0.18, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.452, loss_scale=16, train_wall=550, gb_free=8.8, wall=54906
2022-02-12 09:04:05 | INFO | train_inner | epoch 007:    379 / 1576 loss=7.21, nll_loss=5.495, ppl=45.11, wps=11677.4, ups=0.18, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.477, loss_scale=16, train_wall=550, gb_free=8.8, wall=55467
2022-02-12 09:06:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 09:13:32 | INFO | train_inner | epoch 007:    480 / 1576 loss=7.214, nll_loss=5.5, ppl=45.26, wps=11555.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.478, loss_scale=16, train_wall=556, gb_free=8.8, wall=56035
2022-02-12 09:22:54 | INFO | train_inner | epoch 007:    580 / 1576 loss=7.205, nll_loss=5.491, ppl=44.96, wps=11664.9, ups=0.18, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.457, loss_scale=16, train_wall=551, gb_free=8.8, wall=56596
2022-02-12 09:32:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 09:32:21 | INFO | train_inner | epoch 007:    681 / 1576 loss=7.224, nll_loss=5.512, ppl=45.64, wps=11553.9, ups=0.18, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.46, loss_scale=16, train_wall=556, gb_free=8.8, wall=57164
2022-02-12 09:41:43 | INFO | train_inner | epoch 007:    781 / 1576 loss=7.226, nll_loss=5.513, ppl=45.66, wps=11676.9, ups=0.18, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.499, loss_scale=16, train_wall=551, gb_free=8.8, wall=57725
2022-02-12 09:51:04 | INFO | train_inner | epoch 007:    881 / 1576 loss=7.212, nll_loss=5.498, ppl=45.21, wps=11677.6, ups=0.18, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.452, loss_scale=16, train_wall=550, gb_free=8.8, wall=58286
2022-02-12 09:57:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 10:00:31 | INFO | train_inner | epoch 007:    982 / 1576 loss=7.217, nll_loss=5.504, ppl=45.37, wps=11559.2, ups=0.18, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.477, loss_scale=16, train_wall=556, gb_free=8.8, wall=58853
2022-02-12 10:09:52 | INFO | train_inner | epoch 007:   1082 / 1576 loss=7.23, nll_loss=5.518, ppl=45.84, wps=11675.5, ups=0.18, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.469, loss_scale=16, train_wall=551, gb_free=8.8, wall=59414
2022-02-12 10:19:14 | INFO | train_inner | epoch 007:   1182 / 1576 loss=7.234, nll_loss=5.523, ppl=45.97, wps=11666.7, ups=0.18, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.466, loss_scale=16, train_wall=551, gb_free=8.8, wall=59976
2022-02-12 10:24:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 10:28:41 | INFO | train_inner | epoch 007:   1283 / 1576 loss=7.22, nll_loss=5.507, ppl=45.48, wps=11558, ups=0.18, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.477, loss_scale=16, train_wall=556, gb_free=8.8, wall=60543
2022-02-12 10:38:02 | INFO | train_inner | epoch 007:   1383 / 1576 loss=7.215, nll_loss=5.502, ppl=45.31, wps=11671.3, ups=0.18, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.471, loss_scale=16, train_wall=551, gb_free=8.8, wall=61105
2022-02-12 10:47:24 | INFO | train_inner | epoch 007:   1483 / 1576 loss=7.226, nll_loss=5.514, ppl=45.7, wps=11668.5, ups=0.18, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.461, loss_scale=16, train_wall=551, gb_free=8.8, wall=61666
2022-02-12 10:48:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 10:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 10:56:08 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.151 | nll_loss 5.409 | ppl 42.49 | wps 32214.3 | wpb 1021.8 | bsz 2 | num_updates 10992 | best_loss 7.151
2022-02-12 10:56:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10992 updates
2022-02-12 10:56:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint7.pt
2022-02-12 10:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint7.pt
2022-02-12 10:56:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint7.pt (epoch 7 @ 10992 updates, score 7.151) (writing took 35.45277168508619 seconds)
2022-02-12 10:56:44 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-12 10:56:44 | INFO | train | epoch 007 | loss 7.216 | nll_loss 5.503 | ppl 45.34 | wps 11572.3 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 10992 | lr 0.000301621 | gnorm 0.472 | loss_scale 16 | train_wall 8674 | gb_free 8.8 | wall 62226
2022-02-12 10:56:44 | INFO | fairseq.trainer | begin training epoch 8
2022-02-12 10:56:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 10:57:29 | INFO | train_inner | epoch 008:      8 / 1576 loss=7.227, nll_loss=5.516, ppl=45.75, wps=10743.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.503, loss_scale=16, train_wall=551, gb_free=8.8, wall=62271
2022-02-12 11:06:50 | INFO | train_inner | epoch 008:    108 / 1576 loss=7.114, nll_loss=5.388, ppl=41.88, wps=11666, ups=0.18, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.456, loss_scale=16, train_wall=551, gb_free=8.8, wall=62833
2022-02-12 11:13:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 11:16:18 | INFO | train_inner | epoch 008:    209 / 1576 loss=7.124, nll_loss=5.399, ppl=42.19, wps=11555.2, ups=0.18, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.474, loss_scale=16, train_wall=556, gb_free=8.8, wall=63400
2022-02-12 11:25:39 | INFO | train_inner | epoch 008:    309 / 1576 loss=7.126, nll_loss=5.401, ppl=42.25, wps=11674.9, ups=0.18, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.46, loss_scale=16, train_wall=551, gb_free=8.8, wall=63961
2022-02-12 11:34:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 11:35:06 | INFO | train_inner | epoch 008:    410 / 1576 loss=7.161, nll_loss=5.44, ppl=43.42, wps=11559.8, ups=0.18, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.464, loss_scale=8, train_wall=556, gb_free=8.8, wall=64528
2022-02-12 11:44:27 | INFO | train_inner | epoch 008:    510 / 1576 loss=7.153, nll_loss=5.432, ppl=43.17, wps=11683.5, ups=0.18, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.504, loss_scale=8, train_wall=550, gb_free=8.8, wall=65089
2022-02-12 11:53:47 | INFO | train_inner | epoch 008:    610 / 1576 loss=7.144, nll_loss=5.422, ppl=42.86, wps=11688.9, ups=0.18, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.456, loss_scale=8, train_wall=550, gb_free=8.8, wall=65650
2022-02-12 11:58:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 12:03:14 | INFO | train_inner | epoch 008:    711 / 1576 loss=7.161, nll_loss=5.441, ppl=43.43, wps=11561.4, ups=0.18, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.466, loss_scale=8, train_wall=556, gb_free=8.8, wall=66217
2022-02-12 12:12:35 | INFO | train_inner | epoch 008:    811 / 1576 loss=7.149, nll_loss=5.427, ppl=43.03, wps=11685.7, ups=0.18, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.458, loss_scale=8, train_wall=550, gb_free=8.8, wall=66777
2022-02-12 12:21:56 | INFO | train_inner | epoch 008:    911 / 1576 loss=7.151, nll_loss=5.43, ppl=43.12, wps=11692.4, ups=0.18, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.485, loss_scale=8, train_wall=550, gb_free=8.8, wall=67338
2022-02-12 12:30:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 12:31:22 | INFO | train_inner | epoch 008:   1012 / 1576 loss=7.161, nll_loss=5.441, ppl=43.44, wps=11569, ups=0.18, wpb=65532.3, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.486, loss_scale=8, train_wall=556, gb_free=8.8, wall=67904
2022-02-12 12:40:43 | INFO | train_inner | epoch 008:   1112 / 1576 loss=7.159, nll_loss=5.439, ppl=43.38, wps=11681, ups=0.18, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.495, loss_scale=8, train_wall=550, gb_free=8.8, wall=68465
2022-02-12 12:50:04 | INFO | train_inner | epoch 008:   1212 / 1576 loss=7.158, nll_loss=5.438, ppl=43.36, wps=11680.5, ups=0.18, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.476, loss_scale=8, train_wall=550, gb_free=8.8, wall=69026
2022-02-12 12:59:26 | INFO | train_inner | epoch 008:   1312 / 1576 loss=7.163, nll_loss=5.443, ppl=43.5, wps=11672.8, ups=0.18, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.469, loss_scale=16, train_wall=551, gb_free=8.8, wall=69588
2022-02-12 13:04:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:08:53 | INFO | train_inner | epoch 008:   1413 / 1576 loss=7.164, nll_loss=5.445, ppl=43.55, wps=11560, ups=0.18, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.468, loss_scale=8, train_wall=556, gb_free=8.8, wall=70155
2022-02-12 13:18:14 | INFO | train_inner | epoch 008:   1513 / 1576 loss=7.158, nll_loss=5.438, ppl=43.36, wps=11672.7, ups=0.18, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.486, loss_scale=8, train_wall=551, gb_free=8.8, wall=70716
2022-02-12 13:24:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 13:24:09 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.11 | nll_loss 5.369 | ppl 41.33 | wps 32274.8 | wpb 1021.8 | bsz 2 | num_updates 12563 | best_loss 7.11
2022-02-12 13:24:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12563 updates
2022-02-12 13:24:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint8.pt
2022-02-12 13:24:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint8.pt
2022-02-12 13:24:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint8.pt (epoch 8 @ 12563 updates, score 7.11) (writing took 36.10752788325772 seconds)
2022-02-12 13:24:46 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-12 13:24:46 | INFO | train | epoch 008 | loss 7.15 | nll_loss 5.429 | ppl 43.07 | wps 11585.1 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 12563 | lr 0.000282133 | gnorm 0.474 | loss_scale 8 | train_wall 8670 | gb_free 8.8 | wall 71108
2022-02-12 13:24:46 | INFO | fairseq.trainer | begin training epoch 9
2022-02-12 13:24:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 13:28:13 | INFO | train_inner | epoch 009:     37 / 1576 loss=7.126, nll_loss=5.402, ppl=42.27, wps=10845.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.469, loss_scale=8, train_wall=545, gb_free=8.8, wall=71315
2022-02-12 13:31:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:37:40 | INFO | train_inner | epoch 009:    138 / 1576 loss=7.064, nll_loss=5.332, ppl=40.27, wps=11563.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.47, loss_scale=8, train_wall=556, gb_free=8.8, wall=71882
2022-02-12 13:47:01 | INFO | train_inner | epoch 009:    238 / 1576 loss=7.076, nll_loss=5.346, ppl=40.66, wps=11681.5, ups=0.18, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.485, loss_scale=8, train_wall=550, gb_free=8.8, wall=72443
2022-02-12 13:55:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:56:27 | INFO | train_inner | epoch 009:    339 / 1576 loss=7.079, nll_loss=5.348, ppl=40.74, wps=11565.9, ups=0.18, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.471, loss_scale=8, train_wall=556, gb_free=8.8, wall=73010
2022-02-12 14:05:49 | INFO | train_inner | epoch 009:    439 / 1576 loss=7.079, nll_loss=5.348, ppl=40.73, wps=11678.1, ups=0.18, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.498, loss_scale=8, train_wall=550, gb_free=8.8, wall=73571
2022-02-12 14:15:09 | INFO | train_inner | epoch 009:    539 / 1576 loss=7.093, nll_loss=5.364, ppl=41.19, wps=11687.4, ups=0.18, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.474, loss_scale=8, train_wall=550, gb_free=8.8, wall=74132
2022-02-12 14:22:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 14:24:36 | INFO | train_inner | epoch 009:    640 / 1576 loss=7.102, nll_loss=5.374, ppl=41.48, wps=11569.5, ups=0.18, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.481, loss_scale=8, train_wall=556, gb_free=8.8, wall=74698
2022-02-12 14:33:57 | INFO | train_inner | epoch 009:    740 / 1576 loss=7.093, nll_loss=5.364, ppl=41.17, wps=11676.7, ups=0.18, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.475, loss_scale=8, train_wall=551, gb_free=8.8, wall=75259
2022-02-12 14:43:18 | INFO | train_inner | epoch 009:    840 / 1576 loss=7.105, nll_loss=5.378, ppl=41.58, wps=11684.5, ups=0.18, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.466, loss_scale=8, train_wall=550, gb_free=8.8, wall=75820
2022-02-12 14:46:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 14:52:44 | INFO | train_inner | epoch 009:    941 / 1576 loss=7.114, nll_loss=5.388, ppl=41.87, wps=11569.6, ups=0.18, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.486, loss_scale=8, train_wall=556, gb_free=8.8, wall=76387
2022-02-12 15:02:05 | INFO | train_inner | epoch 009:   1041 / 1576 loss=7.106, nll_loss=5.38, ppl=41.63, wps=11679.1, ups=0.18, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.466, loss_scale=8, train_wall=550, gb_free=8.8, wall=76948
2022-02-12 15:11:27 | INFO | train_inner | epoch 009:   1141 / 1576 loss=7.109, nll_loss=5.383, ppl=41.73, wps=11672.7, ups=0.18, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.493, loss_scale=16, train_wall=551, gb_free=8.8, wall=77509
2022-02-12 15:14:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:20:54 | INFO | train_inner | epoch 009:   1242 / 1576 loss=7.109, nll_loss=5.382, ppl=41.71, wps=11561.9, ups=0.18, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.462, loss_scale=8, train_wall=556, gb_free=8.8, wall=78076
2022-02-12 15:30:15 | INFO | train_inner | epoch 009:   1342 / 1576 loss=7.114, nll_loss=5.388, ppl=41.88, wps=11683.1, ups=0.18, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.489, loss_scale=8, train_wall=550, gb_free=8.8, wall=78637
2022-02-12 15:38:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:39:41 | INFO | train_inner | epoch 009:   1443 / 1576 loss=7.117, nll_loss=5.392, ppl=41.98, wps=11576.5, ups=0.18, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.496, loss_scale=8, train_wall=555, gb_free=8.8, wall=79203
2022-02-12 15:49:01 | INFO | train_inner | epoch 009:   1543 / 1576 loss=7.119, nll_loss=5.394, ppl=42.04, wps=11687.8, ups=0.18, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.48, loss_scale=8, train_wall=550, gb_free=8.8, wall=79764
2022-02-12 15:52:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 15:52:09 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.074 | nll_loss 5.335 | ppl 40.35 | wps 32080.8 | wpb 1021.8 | bsz 2 | num_updates 14133 | best_loss 7.074
2022-02-12 15:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14133 updates
2022-02-12 15:52:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint9.pt
2022-02-12 15:52:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint9.pt
2022-02-12 15:52:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint9.pt (epoch 9 @ 14133 updates, score 7.074) (writing took 32.73785193497315 seconds)
2022-02-12 15:52:41 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-12 15:52:41 | INFO | train | epoch 009 | loss 7.098 | nll_loss 5.371 | ppl 41.37 | wps 11585.9 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 14133 | lr 0.000266001 | gnorm 0.479 | loss_scale 8 | train_wall 8667 | gb_free 8.8 | wall 79984
2022-02-12 15:52:41 | INFO | fairseq.trainer | begin training epoch 10
2022-02-12 15:52:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 15:58:57 | INFO | train_inner | epoch 010:     67 / 1576 loss=7.05, nll_loss=5.316, ppl=39.83, wps=10908.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=14200, lr=0.000265372, gnorm=0.485, loss_scale=8, train_wall=545, gb_free=8.8, wall=80359
2022-02-12 16:05:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 16:08:24 | INFO | train_inner | epoch 010:    168 / 1576 loss=7.026, nll_loss=5.289, ppl=39.1, wps=11567.9, ups=0.18, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.511, loss_scale=8, train_wall=556, gb_free=8.8, wall=80926
2022-02-12 16:17:44 | INFO | train_inner | epoch 010:    268 / 1576 loss=7.03, nll_loss=5.294, ppl=39.22, wps=11692.1, ups=0.18, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.462, loss_scale=8, train_wall=550, gb_free=8.8, wall=81486
2022-02-12 16:21:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 16:27:10 | INFO | train_inner | epoch 010:    369 / 1576 loss=7.036, nll_loss=5.299, ppl=39.38, wps=11587.5, ups=0.18, wpb=65532.3, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.482, loss_scale=4, train_wall=555, gb_free=8.8, wall=82052
2022-02-12 16:36:30 | INFO | train_inner | epoch 010:    469 / 1576 loss=7.051, nll_loss=5.317, ppl=39.87, wps=11700.7, ups=0.18, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.487, loss_scale=4, train_wall=549, gb_free=8.8, wall=82612
2022-02-12 16:45:50 | INFO | train_inner | epoch 010:    569 / 1576 loss=7.055, nll_loss=5.321, ppl=39.98, wps=11705.8, ups=0.18, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.503, loss_scale=8, train_wall=549, gb_free=8.8, wall=83172
2022-02-12 16:55:10 | INFO | train_inner | epoch 010:    669 / 1576 loss=7.047, nll_loss=5.313, ppl=39.75, wps=11688.1, ups=0.18, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.478, loss_scale=8, train_wall=550, gb_free=8.8, wall=83733
2022-02-12 17:04:31 | INFO | train_inner | epoch 010:    769 / 1576 loss=7.065, nll_loss=5.333, ppl=40.31, wps=11686.6, ups=0.18, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.487, loss_scale=8, train_wall=550, gb_free=8.8, wall=84293
2022-02-12 17:09:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 17:10:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 17:14:03 | INFO | train_inner | epoch 010:    871 / 1576 loss=7.058, nll_loss=5.325, ppl=40.08, wps=11468.4, ups=0.17, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.513, loss_scale=4, train_wall=561, gb_free=8.8, wall=84865
2022-02-12 17:23:23 | INFO | train_inner | epoch 010:    971 / 1576 loss=7.07, nll_loss=5.339, ppl=40.47, wps=11693.6, ups=0.18, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.489, loss_scale=4, train_wall=550, gb_free=8.8, wall=85425
2022-02-12 17:32:43 | INFO | train_inner | epoch 010:   1071 / 1576 loss=7.074, nll_loss=5.343, ppl=40.59, wps=11693.6, ups=0.18, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.488, loss_scale=4, train_wall=550, gb_free=8.8, wall=85986
2022-02-12 17:40:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 17:42:10 | INFO | train_inner | epoch 010:   1172 / 1576 loss=7.073, nll_loss=5.342, ppl=40.56, wps=11570.4, ups=0.18, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.507, loss_scale=4, train_wall=556, gb_free=8.8, wall=86552
2022-02-12 17:51:30 | INFO | train_inner | epoch 010:   1272 / 1576 loss=7.066, nll_loss=5.334, ppl=40.33, wps=11695.3, ups=0.18, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.462, loss_scale=4, train_wall=550, gb_free=8.8, wall=87113
2022-02-12 18:00:50 | INFO | train_inner | epoch 010:   1372 / 1576 loss=7.079, nll_loss=5.349, ppl=40.76, wps=11696.4, ups=0.18, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.498, loss_scale=4, train_wall=550, gb_free=8.8, wall=87673
2022-02-12 18:10:11 | INFO | train_inner | epoch 010:   1472 / 1576 loss=7.071, nll_loss=5.34, ppl=40.5, wps=11698.2, ups=0.18, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.462, loss_scale=8, train_wall=550, gb_free=8.8, wall=88233
2022-02-12 18:16:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 18:19:37 | INFO | train_inner | epoch 010:   1573 / 1576 loss=7.076, nll_loss=5.345, ppl=40.65, wps=11578.1, ups=0.18, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.493, loss_scale=4, train_wall=555, gb_free=8.8, wall=88799
2022-02-12 18:19:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 18:19:56 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.053 | nll_loss 5.29 | ppl 39.12 | wps 32027 | wpb 1021.8 | bsz 2 | num_updates 15703 | best_loss 7.053
2022-02-12 18:19:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15703 updates
2022-02-12 18:19:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint10.pt
2022-02-12 18:20:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint10.pt
2022-02-12 18:20:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint10.pt (epoch 10 @ 15703 updates, score 7.053) (writing took 34.44939082022756 seconds)
2022-02-12 18:20:30 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-12 18:20:30 | INFO | train | epoch 010 | loss 7.057 | nll_loss 5.323 | ppl 40.04 | wps 11595.2 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 15703 | lr 0.000252353 | gnorm 0.488 | loss_scale 4 | train_wall 8659 | gb_free 8.8 | wall 88852
2022-02-12 18:20:30 | INFO | fairseq.trainer | begin training epoch 11
2022-02-12 18:20:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 18:29:34 | INFO | train_inner | epoch 011:     97 / 1576 loss=6.988, nll_loss=5.246, ppl=37.96, wps=10885.9, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=15800, lr=0.000251577, gnorm=0.475, loss_scale=4, train_wall=544, gb_free=8.8, wall=89396
2022-02-12 18:38:54 | INFO | train_inner | epoch 011:    197 / 1576 loss=6.99, nll_loss=5.248, ppl=38.01, wps=11702.7, ups=0.18, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.486, loss_scale=4, train_wall=549, gb_free=8.8, wall=89956
2022-02-12 18:45:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 18:48:19 | INFO | train_inner | epoch 011:    298 / 1576 loss=6.997, nll_loss=5.256, ppl=38.2, wps=11582.4, ups=0.18, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.517, loss_scale=4, train_wall=555, gb_free=8.8, wall=90522
2022-02-12 18:57:39 | INFO | train_inner | epoch 011:    398 / 1576 loss=7.004, nll_loss=5.264, ppl=38.43, wps=11701.2, ups=0.18, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.471, loss_scale=4, train_wall=549, gb_free=8.8, wall=91082
2022-02-12 19:06:59 | INFO | train_inner | epoch 011:    498 / 1576 loss=7.025, nll_loss=5.287, ppl=39.05, wps=11707.1, ups=0.18, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.493, loss_scale=4, train_wall=549, gb_free=8.8, wall=91642
2022-02-12 19:12:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 19:16:25 | INFO | train_inner | epoch 011:    599 / 1576 loss=7.016, nll_loss=5.277, ppl=38.77, wps=11592.8, ups=0.18, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.508, loss_scale=4, train_wall=555, gb_free=8.8, wall=92207
2022-02-12 19:25:44 | INFO | train_inner | epoch 011:    699 / 1576 loss=7.019, nll_loss=5.281, ppl=38.88, wps=11710, ups=0.18, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.484, loss_scale=4, train_wall=549, gb_free=8.8, wall=92767
2022-02-12 19:35:04 | INFO | train_inner | epoch 011:    799 / 1576 loss=7.02, nll_loss=5.282, ppl=38.9, wps=11702.5, ups=0.18, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.482, loss_scale=4, train_wall=549, gb_free=8.8, wall=93327
2022-02-12 19:36:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 19:44:30 | INFO | train_inner | epoch 011:    900 / 1576 loss=7.031, nll_loss=5.295, ppl=39.25, wps=11586.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.506, loss_scale=4, train_wall=555, gb_free=8.8, wall=93892
2022-02-12 19:53:50 | INFO | train_inner | epoch 011:   1000 / 1576 loss=7.03, nll_loss=5.293, ppl=39.22, wps=11698.4, ups=0.18, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.466, loss_scale=4, train_wall=550, gb_free=8.8, wall=94452
2022-02-12 20:03:10 | INFO | train_inner | epoch 011:   1100 / 1576 loss=7.03, nll_loss=5.293, ppl=39.21, wps=11707.8, ups=0.18, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.498, loss_scale=8, train_wall=549, gb_free=8.8, wall=95012
2022-02-12 20:05:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 20:12:35 | INFO | train_inner | epoch 011:   1201 / 1576 loss=7.039, nll_loss=5.304, ppl=39.52, wps=11594.4, ups=0.18, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.498, loss_scale=4, train_wall=555, gb_free=8.8, wall=95577
2022-02-12 20:21:55 | INFO | train_inner | epoch 011:   1301 / 1576 loss=7.047, nll_loss=5.313, ppl=39.74, wps=11705.8, ups=0.18, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.496, loss_scale=4, train_wall=549, gb_free=8.8, wall=96137
2022-02-12 20:31:15 | INFO | train_inner | epoch 011:   1401 / 1576 loss=7.037, nll_loss=5.302, ppl=39.45, wps=11704.5, ups=0.18, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.477, loss_scale=8, train_wall=549, gb_free=8.8, wall=96697
2022-02-12 20:36:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 20:40:40 | INFO | train_inner | epoch 011:   1502 / 1576 loss=7.044, nll_loss=5.31, ppl=39.66, wps=11587.1, ups=0.18, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.527, loss_scale=4, train_wall=555, gb_free=8.8, wall=97263
2022-02-12 20:47:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 20:47:37 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.038 | nll_loss 5.272 | ppl 38.63 | wps 32276 | wpb 1021.8 | bsz 2 | num_updates 17274 | best_loss 7.038
2022-02-12 20:47:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17274 updates
2022-02-12 20:47:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint11.pt
2022-02-12 20:47:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint11.pt
2022-02-12 20:48:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint11.pt (epoch 11 @ 17274 updates, score 7.038) (writing took 32.699868319090456 seconds)
2022-02-12 20:48:09 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-12 20:48:09 | INFO | train | epoch 011 | loss 7.022 | nll_loss 5.285 | ppl 38.99 | wps 11614.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 17274 | lr 0.000240604 | gnorm 0.492 | loss_scale 4 | train_wall 8651 | gb_free 8.8 | wall 97712
2022-02-12 20:48:10 | INFO | fairseq.trainer | begin training epoch 12
2022-02-12 20:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 20:50:35 | INFO | train_inner | epoch 012:     26 / 1576 loss=7.027, nll_loss=5.29, ppl=39.13, wps=10927.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=17300, lr=0.000240424, gnorm=0.491, loss_scale=4, train_wall=544, gb_free=8.8, wall=97857
2022-02-12 20:59:55 | INFO | train_inner | epoch 012:    126 / 1576 loss=6.945, nll_loss=5.198, ppl=36.7, wps=11705.3, ups=0.18, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.492, loss_scale=4, train_wall=549, gb_free=8.8, wall=98417
2022-02-12 21:06:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 21:09:21 | INFO | train_inner | epoch 012:    227 / 1576 loss=6.965, nll_loss=5.22, ppl=37.28, wps=11582.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.486, loss_scale=4, train_wall=555, gb_free=8.8, wall=98983
2022-02-12 21:18:40 | INFO | train_inner | epoch 012:    327 / 1576 loss=6.974, nll_loss=5.23, ppl=37.54, wps=11710.1, ups=0.18, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.483, loss_scale=4, train_wall=549, gb_free=8.8, wall=99543
2022-02-12 21:28:00 | INFO | train_inner | epoch 012:    427 / 1576 loss=6.977, nll_loss=5.233, ppl=37.61, wps=11706.2, ups=0.18, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.483, loss_scale=4, train_wall=549, gb_free=8.8, wall=100102
2022-02-12 21:37:20 | INFO | train_inner | epoch 012:    527 / 1576 loss=6.984, nll_loss=5.242, ppl=37.85, wps=11699.7, ups=0.18, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.485, loss_scale=8, train_wall=550, gb_free=8.8, wall=100663
2022-02-12 21:38:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 21:46:45 | INFO | train_inner | epoch 012:    628 / 1576 loss=6.995, nll_loss=5.253, ppl=38.14, wps=11597.1, ups=0.18, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.548, loss_scale=4, train_wall=554, gb_free=8.8, wall=101228
2022-02-12 21:56:05 | INFO | train_inner | epoch 012:    728 / 1576 loss=6.989, nll_loss=5.247, ppl=37.99, wps=11706.3, ups=0.18, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.502, loss_scale=4, train_wall=549, gb_free=8.8, wall=101788
2022-02-12 22:05:25 | INFO | train_inner | epoch 012:    828 / 1576 loss=7.003, nll_loss=5.263, ppl=38.4, wps=11706.9, ups=0.18, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.483, loss_scale=8, train_wall=549, gb_free=8.8, wall=102347
2022-02-12 22:14:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 22:14:50 | INFO | train_inner | epoch 012:    929 / 1576 loss=7.004, nll_loss=5.265, ppl=38.45, wps=11591, ups=0.18, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.49, loss_scale=4, train_wall=555, gb_free=8.8, wall=102913
2022-02-12 22:24:10 | INFO | train_inner | epoch 012:   1029 / 1576 loss=7.004, nll_loss=5.264, ppl=38.43, wps=11709.8, ups=0.18, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.492, loss_scale=4, train_wall=549, gb_free=8.8, wall=103472
2022-02-12 22:33:30 | INFO | train_inner | epoch 012:   1129 / 1576 loss=7.006, nll_loss=5.266, ppl=38.49, wps=11714.2, ups=0.18, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.488, loss_scale=4, train_wall=549, gb_free=8.8, wall=104032
2022-02-12 22:42:49 | INFO | train_inner | epoch 012:   1229 / 1576 loss=7.001, nll_loss=5.261, ppl=38.34, wps=11707, ups=0.18, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.497, loss_scale=8, train_wall=549, gb_free=8.8, wall=104592
2022-02-12 22:52:10 | INFO | train_inner | epoch 012:   1329 / 1576 loss=7.017, nll_loss=5.279, ppl=38.84, wps=11694.7, ups=0.18, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.494, loss_scale=8, train_wall=550, gb_free=8.8, wall=105152
2022-02-12 23:01:30 | INFO | train_inner | epoch 012:   1429 / 1576 loss=7.024, nll_loss=5.287, ppl=39.05, wps=11702.7, ups=0.18, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.489, loss_scale=8, train_wall=549, gb_free=8.8, wall=105712
2022-02-12 23:03:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 23:10:56 | INFO | train_inner | epoch 012:   1530 / 1576 loss=7.006, nll_loss=5.266, ppl=38.49, wps=11582.7, ups=0.18, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.495, loss_scale=8, train_wall=555, gb_free=8.8, wall=106278
2022-02-12 23:15:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 23:15:15 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.024 | nll_loss 5.248 | ppl 37.99 | wps 32021.2 | wpb 1021.8 | bsz 2 | num_updates 18846 | best_loss 7.024
2022-02-12 23:15:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18846 updates
2022-02-12 23:15:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint12.pt
2022-02-12 23:15:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint12.pt
2022-02-12 23:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint12.pt (epoch 12 @ 18846 updates, score 7.024) (writing took 32.35109971696511 seconds)
2022-02-12 23:15:48 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-12 23:15:48 | INFO | train | epoch 012 | loss 6.993 | nll_loss 5.252 | ppl 38.11 | wps 11623.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 18846 | lr 0.000230351 | gnorm 0.494 | loss_scale 8 | train_wall 8652 | gb_free 8.8 | wall 106570
2022-02-12 23:15:48 | INFO | fairseq.trainer | begin training epoch 13
2022-02-12 23:15:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 23:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 23:20:55 | INFO | train_inner | epoch 013:     55 / 1576 loss=6.972, nll_loss=5.228, ppl=37.47, wps=10832.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=18900, lr=0.000230022, gnorm=0.523, loss_scale=4, train_wall=550, gb_free=8.8, wall=106878
2022-02-12 23:30:15 | INFO | train_inner | epoch 013:    155 / 1576 loss=6.928, nll_loss=5.178, ppl=36.21, wps=11703.1, ups=0.18, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.486, loss_scale=4, train_wall=549, gb_free=8.8, wall=107438
2022-02-12 23:39:35 | INFO | train_inner | epoch 013:    255 / 1576 loss=6.941, nll_loss=5.194, ppl=36.6, wps=11705.9, ups=0.18, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.494, loss_scale=4, train_wall=549, gb_free=8.8, wall=107997
2022-02-12 23:48:55 | INFO | train_inner | epoch 013:    355 / 1576 loss=6.946, nll_loss=5.198, ppl=36.72, wps=11696.2, ups=0.18, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.485, loss_scale=8, train_wall=550, gb_free=8.8, wall=108558
2022-02-12 23:58:16 | INFO | train_inner | epoch 013:    455 / 1576 loss=6.968, nll_loss=5.223, ppl=37.35, wps=11693, ups=0.18, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.479, loss_scale=8, train_wall=550, gb_free=8.8, wall=109118
2022-02-13 00:01:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 00:07:42 | INFO | train_inner | epoch 013:    556 / 1576 loss=6.965, nll_loss=5.219, ppl=37.26, wps=11580.4, ups=0.18, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.543, loss_scale=4, train_wall=555, gb_free=8.8, wall=109684
2022-02-13 00:17:02 | INFO | train_inner | epoch 013:    656 / 1576 loss=6.966, nll_loss=5.221, ppl=37.31, wps=11702.2, ups=0.18, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.527, loss_scale=4, train_wall=549, gb_free=8.8, wall=110244
2022-02-13 00:26:22 | INFO | train_inner | epoch 013:    756 / 1576 loss=6.967, nll_loss=5.223, ppl=37.35, wps=11701.2, ups=0.18, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.474, loss_scale=8, train_wall=549, gb_free=8.8, wall=110804
2022-02-13 00:28:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 00:35:48 | INFO | train_inner | epoch 013:    857 / 1576 loss=6.99, nll_loss=5.249, ppl=38.02, wps=11584, ups=0.18, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.514, loss_scale=4, train_wall=555, gb_free=8.8, wall=111370
2022-02-13 00:45:07 | INFO | train_inner | epoch 013:    957 / 1576 loss=6.973, nll_loss=5.229, ppl=37.5, wps=11707.3, ups=0.18, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.502, loss_scale=4, train_wall=549, gb_free=8.8, wall=111930
2022-02-13 00:54:28 | INFO | train_inner | epoch 013:   1057 / 1576 loss=6.978, nll_loss=5.235, ppl=37.66, wps=11698.4, ups=0.18, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.5, loss_scale=8, train_wall=550, gb_free=8.8, wall=112490
2022-02-13 01:03:48 | INFO | train_inner | epoch 013:   1157 / 1576 loss=6.978, nll_loss=5.235, ppl=37.66, wps=11687.7, ups=0.18, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.49, loss_scale=8, train_wall=550, gb_free=8.8, wall=113051
2022-02-13 01:09:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 01:13:14 | INFO | train_inner | epoch 013:   1258 / 1576 loss=6.99, nll_loss=5.249, ppl=38.02, wps=11584.4, ups=0.18, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.523, loss_scale=4, train_wall=555, gb_free=8.8, wall=113616
2022-02-13 01:22:34 | INFO | train_inner | epoch 013:   1358 / 1576 loss=6.974, nll_loss=5.231, ppl=37.56, wps=11695.3, ups=0.18, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.476, loss_scale=4, train_wall=550, gb_free=8.8, wall=114177
2022-02-13 01:31:54 | INFO | train_inner | epoch 013:   1458 / 1576 loss=6.989, nll_loss=5.248, ppl=37.99, wps=11712.1, ups=0.18, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.485, loss_scale=4, train_wall=549, gb_free=8.8, wall=114736
2022-02-13 01:41:14 | INFO | train_inner | epoch 013:   1558 / 1576 loss=6.982, nll_loss=5.24, ppl=37.79, wps=11697.5, ups=0.18, wpb=65532.3, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.486, loss_scale=8, train_wall=550, gb_free=8.8, wall=115297
2022-02-13 01:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 01:42:57 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.007 | nll_loss 5.258 | ppl 38.27 | wps 32439.6 | wpb 1021.8 | bsz 2 | num_updates 20418 | best_loss 7.007
2022-02-13 01:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20418 updates
2022-02-13 01:42:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint13.pt
2022-02-13 01:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint13.pt
2022-02-13 01:43:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint13.pt (epoch 13 @ 20418 updates, score 7.007) (writing took 32.15681207599118 seconds)
2022-02-13 01:43:29 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-13 01:43:29 | INFO | train | epoch 013 | loss 6.968 | nll_loss 5.224 | ppl 37.36 | wps 11619.2 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 20418 | lr 0.000221306 | gnorm 0.5 | loss_scale 8 | train_wall 8655 | gb_free 8.8 | wall 115432
2022-02-13 01:43:29 | INFO | fairseq.trainer | begin training epoch 14
2022-02-13 01:43:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 01:51:08 | INFO | train_inner | epoch 014:     82 / 1576 loss=6.909, nll_loss=5.157, ppl=35.69, wps=10934.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=20500, lr=0.000220863, gnorm=0.505, loss_scale=8, train_wall=544, gb_free=8.8, wall=115891
2022-02-13 01:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:00:34 | INFO | train_inner | epoch 014:    183 / 1576 loss=6.915, nll_loss=5.164, ppl=35.85, wps=11578, ups=0.18, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.496, loss_scale=8, train_wall=555, gb_free=8.8, wall=116457
2022-02-13 02:04:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 02:10:00 | INFO | train_inner | epoch 014:    284 / 1576 loss=6.926, nll_loss=5.176, ppl=36.16, wps=11579.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.499, loss_scale=4, train_wall=555, gb_free=8.8, wall=117023
2022-02-13 02:19:20 | INFO | train_inner | epoch 014:    384 / 1576 loss=6.924, nll_loss=5.174, ppl=36.11, wps=11705.8, ups=0.18, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.482, loss_scale=4, train_wall=549, gb_free=8.8, wall=117583
2022-02-13 02:28:40 | INFO | train_inner | epoch 014:    484 / 1576 loss=6.932, nll_loss=5.183, ppl=36.33, wps=11702.7, ups=0.18, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.499, loss_scale=8, train_wall=549, gb_free=8.8, wall=118143
2022-02-13 02:38:01 | INFO | train_inner | epoch 014:    584 / 1576 loss=6.938, nll_loss=5.19, ppl=36.5, wps=11689.4, ups=0.18, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.493, loss_scale=8, train_wall=550, gb_free=8.8, wall=118703
2022-02-13 02:47:22 | INFO | train_inner | epoch 014:    684 / 1576 loss=6.947, nll_loss=5.199, ppl=36.74, wps=11689.3, ups=0.18, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.486, loss_scale=8, train_wall=550, gb_free=8.8, wall=119264
2022-02-13 02:52:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:56:48 | INFO | train_inner | epoch 014:    785 / 1576 loss=6.948, nll_loss=5.201, ppl=36.77, wps=11574.2, ups=0.18, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.489, loss_scale=8, train_wall=555, gb_free=8.8, wall=119830
2022-02-13 02:57:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 03:06:14 | INFO | train_inner | epoch 014:    886 / 1576 loss=6.949, nll_loss=5.202, ppl=36.81, wps=11579.1, ups=0.18, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.515, loss_scale=4, train_wall=555, gb_free=8.8, wall=120396
2022-02-13 03:15:34 | INFO | train_inner | epoch 014:    986 / 1576 loss=6.962, nll_loss=5.216, ppl=37.18, wps=11699.4, ups=0.18, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.483, loss_scale=4, train_wall=550, gb_free=8.8, wall=120956
2022-02-13 03:24:54 | INFO | train_inner | epoch 014:   1086 / 1576 loss=6.958, nll_loss=5.213, ppl=37.08, wps=11692.2, ups=0.18, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.516, loss_scale=8, train_wall=550, gb_free=8.8, wall=121517
2022-02-13 03:34:15 | INFO | train_inner | epoch 014:   1186 / 1576 loss=6.961, nll_loss=5.216, ppl=37.16, wps=11690.1, ups=0.18, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.487, loss_scale=8, train_wall=550, gb_free=8.8, wall=122077
2022-02-13 03:43:36 | INFO | train_inner | epoch 014:   1286 / 1576 loss=6.967, nll_loss=5.223, ppl=37.34, wps=11690.1, ups=0.18, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.494, loss_scale=8, train_wall=550, gb_free=8.8, wall=122638
2022-02-13 03:45:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:53:02 | INFO | train_inner | epoch 014:   1387 / 1576 loss=6.962, nll_loss=5.217, ppl=37.19, wps=11576.9, ups=0.18, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.501, loss_scale=8, train_wall=555, gb_free=8.8, wall=123204
2022-02-13 04:02:22 | INFO | train_inner | epoch 014:   1487 / 1576 loss=6.97, nll_loss=5.226, ppl=37.42, wps=11692.3, ups=0.18, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.483, loss_scale=8, train_wall=550, gb_free=8.8, wall=123765
2022-02-13 04:03:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 04:10:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 04:10:43 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.992 | nll_loss 5.222 | ppl 37.31 | wps 31845.5 | wpb 1021.8 | bsz 2 | num_updates 21988 | best_loss 6.992
2022-02-13 04:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21988 updates
2022-02-13 04:10:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint14.pt
2022-02-13 04:10:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint14.pt
2022-02-13 04:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint14.pt (epoch 14 @ 21988 updates, score 6.992) (writing took 32.46001547342166 seconds)
2022-02-13 04:11:15 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-13 04:11:16 | INFO | train | epoch 014 | loss 6.945 | nll_loss 5.198 | ppl 36.71 | wps 11598.3 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 21988 | lr 0.000213259 | gnorm 0.494 | loss_scale 4 | train_wall 8659 | gb_free 8.8 | wall 124298
2022-02-13 04:11:16 | INFO | fairseq.trainer | begin training epoch 15
2022-02-13 04:11:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 04:12:23 | INFO | train_inner | epoch 015:     12 / 1576 loss=6.955, nll_loss=5.209, ppl=36.98, wps=10819.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=22000, lr=0.000213201, gnorm=0.497, loss_scale=4, train_wall=550, gb_free=8.8, wall=124365
2022-02-13 04:21:42 | INFO | train_inner | epoch 015:    112 / 1576 loss=6.882, nll_loss=5.127, ppl=34.94, wps=11708.5, ups=0.18, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.49, loss_scale=4, train_wall=549, gb_free=8.8, wall=124925
2022-02-13 04:31:03 | INFO | train_inner | epoch 015:    212 / 1576 loss=6.891, nll_loss=5.136, ppl=35.16, wps=11696.8, ups=0.18, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.52, loss_scale=8, train_wall=550, gb_free=8.8, wall=125485
2022-02-13 04:40:23 | INFO | train_inner | epoch 015:    312 / 1576 loss=6.904, nll_loss=5.151, ppl=35.53, wps=11688.2, ups=0.18, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.493, loss_scale=8, train_wall=550, gb_free=8.8, wall=126046
2022-02-13 04:44:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 04:49:49 | INFO | train_inner | epoch 015:    413 / 1576 loss=6.917, nll_loss=5.166, ppl=35.89, wps=11578.2, ups=0.18, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.504, loss_scale=4, train_wall=555, gb_free=8.8, wall=126612
2022-02-13 04:59:10 | INFO | train_inner | epoch 015:    513 / 1576 loss=6.918, nll_loss=5.167, ppl=35.92, wps=11695.8, ups=0.18, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.505, loss_scale=4, train_wall=550, gb_free=8.8, wall=127172
2022-02-13 05:08:30 | INFO | train_inner | epoch 015:    613 / 1576 loss=6.918, nll_loss=5.167, ppl=35.93, wps=11695.3, ups=0.18, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.502, loss_scale=8, train_wall=550, gb_free=8.8, wall=127732
2022-02-13 05:17:51 | INFO | train_inner | epoch 015:    713 / 1576 loss=6.92, nll_loss=5.169, ppl=35.98, wps=11687.6, ups=0.18, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.493, loss_scale=8, train_wall=550, gb_free=8.8, wall=128293
2022-02-13 05:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 05:27:17 | INFO | train_inner | epoch 015:    814 / 1576 loss=6.932, nll_loss=5.182, ppl=36.3, wps=11579.8, ups=0.18, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.534, loss_scale=4, train_wall=555, gb_free=8.8, wall=128859
2022-02-13 05:36:37 | INFO | train_inner | epoch 015:    914 / 1576 loss=6.928, nll_loss=5.178, ppl=36.2, wps=11696.7, ups=0.18, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.522, loss_scale=4, train_wall=550, gb_free=8.8, wall=129419
2022-02-13 05:45:58 | INFO | train_inner | epoch 015:   1014 / 1576 loss=6.941, nll_loss=5.193, ppl=36.59, wps=11688.6, ups=0.18, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.5, loss_scale=8, train_wall=550, gb_free=8.8, wall=129980
2022-02-13 05:55:19 | INFO | train_inner | epoch 015:   1114 / 1576 loss=6.941, nll_loss=5.194, ppl=36.6, wps=11684.4, ups=0.18, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.493, loss_scale=8, train_wall=550, gb_free=8.8, wall=130541
2022-02-13 06:04:40 | INFO | train_inner | epoch 015:   1214 / 1576 loss=6.936, nll_loss=5.188, ppl=36.44, wps=11682, ups=0.18, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.513, loss_scale=8, train_wall=550, gb_free=8.8, wall=131102
2022-02-13 06:07:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 06:14:06 | INFO | train_inner | epoch 015:   1315 / 1576 loss=6.952, nll_loss=5.206, ppl=36.9, wps=11570.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.497, loss_scale=8, train_wall=556, gb_free=8.8, wall=131668
2022-02-13 06:23:27 | INFO | train_inner | epoch 015:   1415 / 1576 loss=6.94, nll_loss=5.193, ppl=36.57, wps=11684.4, ups=0.18, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.488, loss_scale=8, train_wall=550, gb_free=8.8, wall=132229
2022-02-13 06:27:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 06:32:53 | INFO | train_inner | epoch 015:   1516 / 1576 loss=6.956, nll_loss=5.211, ppl=37.03, wps=11577, ups=0.18, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.514, loss_scale=4, train_wall=555, gb_free=8.8, wall=132795
2022-02-13 06:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 06:38:31 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.98 | nll_loss 5.212 | ppl 37.08 | wps 32056.5 | wpb 1021.8 | bsz 2 | num_updates 23560 | best_loss 6.98
2022-02-13 06:38:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23560 updates
2022-02-13 06:38:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint15.pt
2022-02-13 06:38:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint15.pt
2022-02-13 06:39:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint15.pt (epoch 15 @ 23560 updates, score 6.98) (writing took 32.86513953981921 seconds)
2022-02-13 06:39:04 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-13 06:39:04 | INFO | train | epoch 015 | loss 6.926 | nll_loss 5.176 | ppl 36.14 | wps 11610.2 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 23560 | lr 0.000206021 | gnorm 0.506 | loss_scale 4 | train_wall 8661 | gb_free 8.8 | wall 133166
2022-02-13 06:39:04 | INFO | fairseq.trainer | begin training epoch 16
2022-02-13 06:39:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 06:42:48 | INFO | train_inner | epoch 016:     40 / 1576 loss=6.906, nll_loss=5.154, ppl=35.6, wps=10921.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=23600, lr=0.000205847, gnorm=0.513, loss_scale=4, train_wall=544, gb_free=8.8, wall=133390
2022-02-13 06:52:08 | INFO | train_inner | epoch 016:    140 / 1576 loss=6.853, nll_loss=5.094, ppl=34.15, wps=11700.7, ups=0.18, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.497, loss_scale=4, train_wall=549, gb_free=8.8, wall=133950
2022-02-13 07:01:29 | INFO | train_inner | epoch 016:    240 / 1576 loss=6.878, nll_loss=5.121, ppl=34.81, wps=11684.7, ups=0.18, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.503, loss_scale=8, train_wall=550, gb_free=8.8, wall=134511
2022-02-13 07:10:50 | INFO | train_inner | epoch 016:    340 / 1576 loss=6.886, nll_loss=5.13, ppl=35.02, wps=11686.5, ups=0.18, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.521, loss_scale=8, train_wall=550, gb_free=8.8, wall=135072
2022-02-13 07:17:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:20:16 | INFO | train_inner | epoch 016:    441 / 1576 loss=6.896, nll_loss=5.142, ppl=35.32, wps=11576.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.502, loss_scale=8, train_wall=555, gb_free=8.8, wall=135638
2022-02-13 07:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 07:29:42 | INFO | train_inner | epoch 016:    542 / 1576 loss=6.899, nll_loss=5.146, ppl=35.4, wps=11577.4, ups=0.18, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.507, loss_scale=4, train_wall=555, gb_free=8.8, wall=136204
2022-02-13 07:39:02 | INFO | train_inner | epoch 016:    642 / 1576 loss=6.909, nll_loss=5.157, ppl=35.68, wps=11697.8, ups=0.18, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.484, loss_scale=4, train_wall=550, gb_free=8.8, wall=136764
2022-02-13 07:48:22 | INFO | train_inner | epoch 016:    742 / 1576 loss=6.898, nll_loss=5.144, ppl=35.36, wps=11695.9, ups=0.18, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.497, loss_scale=8, train_wall=550, gb_free=8.8, wall=137325
2022-02-13 07:57:43 | INFO | train_inner | epoch 016:    842 / 1576 loss=6.92, nll_loss=5.17, ppl=35.99, wps=11691.7, ups=0.18, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.522, loss_scale=8, train_wall=550, gb_free=8.8, wall=137885
2022-02-13 08:07:03 | INFO | train_inner | epoch 016:    942 / 1576 loss=6.924, nll_loss=5.174, ppl=36.1, wps=11690.4, ups=0.18, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.505, loss_scale=8, train_wall=550, gb_free=8.8, wall=138446
2022-02-13 08:09:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:16:29 | INFO | train_inner | epoch 016:   1043 / 1576 loss=6.929, nll_loss=5.18, ppl=36.25, wps=11579.4, ups=0.18, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.501, loss_scale=8, train_wall=555, gb_free=8.8, wall=139012
2022-02-13 08:25:50 | INFO | train_inner | epoch 016:   1143 / 1576 loss=6.915, nll_loss=5.163, ppl=35.83, wps=11695.2, ups=0.18, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.484, loss_scale=8, train_wall=550, gb_free=8.8, wall=139572
2022-02-13 08:35:10 | INFO | train_inner | epoch 016:   1243 / 1576 loss=6.936, nll_loss=5.187, ppl=36.44, wps=11693.5, ups=0.18, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.488, loss_scale=16, train_wall=550, gb_free=8.8, wall=140133
2022-02-13 08:35:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:43:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 08:44:42 | INFO | train_inner | epoch 016:   1345 / 1576 loss=6.923, nll_loss=5.173, ppl=36.08, wps=11471.3, ups=0.18, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.525, loss_scale=4, train_wall=560, gb_free=8.8, wall=140704
2022-02-13 08:54:02 | INFO | train_inner | epoch 016:   1445 / 1576 loss=6.932, nll_loss=5.183, ppl=36.33, wps=11697.8, ups=0.18, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.501, loss_scale=4, train_wall=550, gb_free=8.8, wall=141264
2022-02-13 09:03:22 | INFO | train_inner | epoch 016:   1545 / 1576 loss=6.93, nll_loss=5.18, ppl=36.26, wps=11696.3, ups=0.18, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.487, loss_scale=4, train_wall=550, gb_free=8.8, wall=141824
2022-02-13 09:06:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 09:06:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.972 | nll_loss 5.198 | ppl 36.7 | wps 32570.7 | wpb 1021.8 | bsz 2 | num_updates 25131 | best_loss 6.972
2022-02-13 09:06:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 25131 updates
2022-02-13 09:06:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint16.pt
2022-02-13 09:06:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint16.pt
2022-02-13 09:06:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint16.pt (epoch 16 @ 25131 updates, score 6.972) (writing took 32.511912495829165 seconds)
2022-02-13 09:06:50 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-13 09:06:50 | INFO | train | epoch 016 | loss 6.908 | nll_loss 5.155 | ppl 35.63 | wps 11605.8 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 25131 | lr 0.000199478 | gnorm 0.502 | loss_scale 4 | train_wall 8659 | gb_free 8.8 | wall 142033
2022-02-13 09:06:50 | INFO | fairseq.trainer | begin training epoch 17
2022-02-13 09:06:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 09:08:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 09:13:22 | INFO | train_inner | epoch 017:     70 / 1576 loss=6.874, nll_loss=5.117, ppl=34.7, wps=10826.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=25200, lr=0.000199205, gnorm=0.511, loss_scale=4, train_wall=550, gb_free=8.8, wall=142424
2022-02-13 09:22:42 | INFO | train_inner | epoch 017:    170 / 1576 loss=6.853, nll_loss=5.094, ppl=34.15, wps=11705.1, ups=0.18, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.507, loss_scale=4, train_wall=549, gb_free=8.8, wall=142984
2022-02-13 09:32:02 | INFO | train_inner | epoch 017:    270 / 1576 loss=6.854, nll_loss=5.094, ppl=34.16, wps=11701.9, ups=0.18, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.497, loss_scale=4, train_wall=549, gb_free=8.8, wall=143544
2022-02-13 09:41:23 | INFO | train_inner | epoch 017:    370 / 1576 loss=6.876, nll_loss=5.119, ppl=34.76, wps=11689.4, ups=0.18, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.51, loss_scale=8, train_wall=550, gb_free=8.8, wall=144105
2022-02-13 09:50:43 | INFO | train_inner | epoch 017:    470 / 1576 loss=6.882, nll_loss=5.126, ppl=34.92, wps=11688.8, ups=0.18, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.492, loss_scale=8, train_wall=550, gb_free=8.8, wall=144666
2022-02-13 09:59:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:00:09 | INFO | train_inner | epoch 017:    571 / 1576 loss=6.884, nll_loss=5.128, ppl=34.98, wps=11595, ups=0.18, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.497, loss_scale=8, train_wall=555, gb_free=8.8, wall=145231
2022-02-13 10:09:27 | INFO | train_inner | epoch 017:    671 / 1576 loss=6.886, nll_loss=5.13, ppl=35.02, wps=11727.9, ups=0.18, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.518, loss_scale=8, train_wall=548, gb_free=8.8, wall=145790
2022-02-13 10:18:46 | INFO | train_inner | epoch 017:    771 / 1576 loss=6.891, nll_loss=5.137, ppl=35.18, wps=11731, ups=0.18, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.518, loss_scale=8, train_wall=548, gb_free=8.8, wall=146348
2022-02-13 10:23:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:28:10 | INFO | train_inner | epoch 017:    872 / 1576 loss=6.886, nll_loss=5.13, ppl=35.03, wps=11617.5, ups=0.18, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.497, loss_scale=8, train_wall=554, gb_free=8.8, wall=146913
2022-02-13 10:37:29 | INFO | train_inner | epoch 017:    972 / 1576 loss=6.903, nll_loss=5.15, ppl=35.5, wps=11727.2, ups=0.18, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.509, loss_scale=8, train_wall=548, gb_free=8.8, wall=147471
2022-02-13 10:46:47 | INFO | train_inner | epoch 017:   1072 / 1576 loss=6.92, nll_loss=5.169, ppl=35.98, wps=11734, ups=0.18, wpb=65532.3, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.513, loss_scale=8, train_wall=548, gb_free=8.8, wall=148030
2022-02-13 10:47:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:56:12 | INFO | train_inner | epoch 017:   1173 / 1576 loss=6.911, nll_loss=5.159, ppl=35.72, wps=11616.3, ups=0.18, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.498, loss_scale=8, train_wall=554, gb_free=8.8, wall=148594
2022-02-13 11:05:30 | INFO | train_inner | epoch 017:   1273 / 1576 loss=6.905, nll_loss=5.152, ppl=35.55, wps=11728.8, ups=0.18, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.503, loss_scale=8, train_wall=548, gb_free=8.8, wall=149153
2022-02-13 11:13:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:14:55 | INFO | train_inner | epoch 017:   1374 / 1576 loss=6.909, nll_loss=5.157, ppl=35.67, wps=11617.2, ups=0.18, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.501, loss_scale=8, train_wall=554, gb_free=8.8, wall=149717
2022-02-13 11:24:13 | INFO | train_inner | epoch 017:   1474 / 1576 loss=6.919, nll_loss=5.168, ppl=35.96, wps=11732.1, ups=0.18, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.489, loss_scale=8, train_wall=548, gb_free=8.8, wall=150275
2022-02-13 11:33:32 | INFO | train_inner | epoch 017:   1574 / 1576 loss=6.926, nll_loss=5.176, ppl=36.16, wps=11735.9, ups=0.18, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.527, loss_scale=8, train_wall=548, gb_free=8.8, wall=150834
2022-02-13 11:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 11:33:45 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.962 | nll_loss 5.183 | ppl 36.32 | wps 32230.2 | wpb 1021.8 | bsz 2 | num_updates 26702 | best_loss 6.962
2022-02-13 11:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26702 updates
2022-02-13 11:33:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint17.pt
2022-02-13 11:33:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint17.pt
2022-02-13 11:34:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint17.pt (epoch 17 @ 26702 updates, score 6.962) (writing took 27.663595740217716 seconds)
2022-02-13 11:34:12 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-13 11:34:12 | INFO | train | epoch 017 | loss 6.891 | nll_loss 5.137 | ppl 35.18 | wps 11637.3 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 26702 | lr 0.000193521 | gnorm 0.506 | loss_scale 8 | train_wall 8641 | gb_free 8.8 | wall 150875
2022-02-13 11:34:13 | INFO | fairseq.trainer | begin training epoch 18
2022-02-13 11:34:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 11:38:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:43:25 | INFO | train_inner | epoch 018:     99 / 1576 loss=6.839, nll_loss=5.077, ppl=33.75, wps=10948.7, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=26800, lr=0.000193167, gnorm=0.529, loss_scale=8, train_wall=548, gb_free=8.8, wall=151427
2022-02-13 11:52:43 | INFO | train_inner | epoch 018:    199 / 1576 loss=6.848, nll_loss=5.087, ppl=33.99, wps=11734.1, ups=0.18, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.517, loss_scale=8, train_wall=548, gb_free=8.8, wall=151986
2022-02-13 12:02:02 | INFO | train_inner | epoch 018:    299 / 1576 loss=6.859, nll_loss=5.1, ppl=34.29, wps=11735.2, ups=0.18, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.503, loss_scale=16, train_wall=548, gb_free=8.8, wall=152544
2022-02-13 12:05:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:11:26 | INFO | train_inner | epoch 018:    400 / 1576 loss=6.855, nll_loss=5.095, ppl=34.19, wps=11615.5, ups=0.18, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.506, loss_scale=8, train_wall=554, gb_free=8.8, wall=153108
2022-02-13 12:20:45 | INFO | train_inner | epoch 018:    500 / 1576 loss=6.859, nll_loss=5.1, ppl=34.3, wps=11730.5, ups=0.18, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.507, loss_scale=8, train_wall=548, gb_free=8.8, wall=153667
2022-02-13 12:29:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:30:09 | INFO | train_inner | epoch 018:    601 / 1576 loss=6.869, nll_loss=5.112, ppl=34.58, wps=11617.9, ups=0.18, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.497, loss_scale=8, train_wall=553, gb_free=8.8, wall=154231
2022-02-13 12:39:27 | INFO | train_inner | epoch 018:    701 / 1576 loss=6.879, nll_loss=5.123, ppl=34.85, wps=11732.4, ups=0.18, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.512, loss_scale=8, train_wall=548, gb_free=8.8, wall=154790
2022-02-13 12:48:46 | INFO | train_inner | epoch 018:    801 / 1576 loss=6.883, nll_loss=5.127, ppl=34.95, wps=11731.3, ups=0.18, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.503, loss_scale=8, train_wall=548, gb_free=8.8, wall=155348
2022-02-13 12:57:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:58:10 | INFO | train_inner | epoch 018:    902 / 1576 loss=6.88, nll_loss=5.124, ppl=34.87, wps=11616.4, ups=0.18, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.505, loss_scale=8, train_wall=554, gb_free=8.8, wall=155913
2022-02-13 13:07:29 | INFO | train_inner | epoch 018:   1002 / 1576 loss=6.887, nll_loss=5.132, ppl=35.06, wps=11730.7, ups=0.18, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.513, loss_scale=8, train_wall=548, gb_free=8.8, wall=156471
2022-02-13 13:16:48 | INFO | train_inner | epoch 018:   1102 / 1576 loss=6.884, nll_loss=5.128, ppl=34.97, wps=11731.8, ups=0.18, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.51, loss_scale=8, train_wall=548, gb_free=8.8, wall=157030
2022-02-13 13:21:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:26:12 | INFO | train_inner | epoch 018:   1203 / 1576 loss=6.891, nll_loss=5.136, ppl=35.16, wps=11618.8, ups=0.18, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.531, loss_scale=8, train_wall=553, gb_free=8.8, wall=157594
2022-02-13 13:35:30 | INFO | train_inner | epoch 018:   1303 / 1576 loss=6.896, nll_loss=5.143, ppl=35.32, wps=11736.6, ups=0.18, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.492, loss_scale=8, train_wall=548, gb_free=8.8, wall=158152
2022-02-13 13:44:48 | INFO | train_inner | epoch 018:   1403 / 1576 loss=6.897, nll_loss=5.143, ppl=35.34, wps=11736, ups=0.18, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.51, loss_scale=8, train_wall=548, gb_free=8.8, wall=158711
2022-02-13 13:45:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:54:12 | INFO | train_inner | epoch 018:   1504 / 1576 loss=6.901, nll_loss=5.147, ppl=35.44, wps=11620.4, ups=0.18, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.496, loss_scale=8, train_wall=553, gb_free=8.8, wall=159275
2022-02-13 14:00:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 14:00:56 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.954 | nll_loss 5.184 | ppl 36.35 | wps 32359.2 | wpb 1021.8 | bsz 2 | num_updates 28272 | best_loss 6.954
2022-02-13 14:00:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 28272 updates
2022-02-13 14:00:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint18.pt
2022-02-13 14:01:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint18.pt
2022-02-13 14:01:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint18.pt (epoch 18 @ 28272 updates, score 6.954) (writing took 27.231291657779366 seconds)
2022-02-13 14:01:24 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-13 14:01:24 | INFO | train | epoch 018 | loss 6.876 | nll_loss 5.12 | ppl 34.77 | wps 11644.5 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 28272 | lr 0.000188071 | gnorm 0.509 | loss_scale 8 | train_wall 8631 | gb_free 8.8 | wall 159706
2022-02-13 14:01:24 | INFO | fairseq.trainer | begin training epoch 19
2022-02-13 14:01:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 14:04:00 | INFO | train_inner | epoch 019:     28 / 1576 loss=6.881, nll_loss=5.125, ppl=34.89, wps=11059, ups=0.17, wpb=64958.8, bsz=126.9, num_updates=28300, lr=0.000187978, gnorm=0.518, loss_scale=8, train_wall=543, gb_free=8.8, wall=159862
2022-02-13 14:11:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 14:13:23 | INFO | train_inner | epoch 019:    129 / 1576 loss=6.817, nll_loss=5.053, ppl=33.2, wps=11634.5, ups=0.18, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.513, loss_scale=8, train_wall=553, gb_free=8.8, wall=160425
2022-02-13 14:22:41 | INFO | train_inner | epoch 019:    229 / 1576 loss=6.827, nll_loss=5.064, ppl=33.45, wps=11742.3, ups=0.18, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.502, loss_scale=8, train_wall=548, gb_free=8.8, wall=160984
2022-02-13 14:31:59 | INFO | train_inner | epoch 019:    329 / 1576 loss=6.83, nll_loss=5.068, ppl=33.53, wps=11748.1, ups=0.18, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.505, loss_scale=8, train_wall=547, gb_free=8.8, wall=161541
2022-02-13 14:36:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 14:41:22 | INFO | train_inner | epoch 019:    430 / 1576 loss=6.84, nll_loss=5.078, ppl=33.79, wps=11635.5, ups=0.18, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.515, loss_scale=8, train_wall=553, gb_free=8.8, wall=162105
2022-02-13 14:50:40 | INFO | train_inner | epoch 019:    530 / 1576 loss=6.858, nll_loss=5.099, ppl=34.28, wps=11749.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.507, loss_scale=8, train_wall=547, gb_free=8.8, wall=162662
2022-02-13 14:59:58 | INFO | train_inner | epoch 019:    630 / 1576 loss=6.866, nll_loss=5.108, ppl=34.48, wps=11745.7, ups=0.18, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.505, loss_scale=8, train_wall=547, gb_free=8.8, wall=163220
2022-02-13 15:05:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:09:22 | INFO | train_inner | epoch 019:    731 / 1576 loss=6.868, nll_loss=5.111, ppl=34.56, wps=11626.5, ups=0.18, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.497, loss_scale=8, train_wall=553, gb_free=8.8, wall=163784
2022-02-13 15:18:40 | INFO | train_inner | epoch 019:    831 / 1576 loss=6.866, nll_loss=5.108, ppl=34.5, wps=11746.9, ups=0.18, wpb=65536, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.495, loss_scale=8, train_wall=547, gb_free=8.8, wall=164342
2022-02-13 15:27:57 | INFO | train_inner | epoch 019:    931 / 1576 loss=6.874, nll_loss=5.117, ppl=34.71, wps=11747.2, ups=0.18, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.504, loss_scale=8, train_wall=547, gb_free=8.8, wall=164900
2022-02-13 15:30:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:37:21 | INFO | train_inner | epoch 019:   1032 / 1576 loss=6.868, nll_loss=5.11, ppl=34.54, wps=11634.8, ups=0.18, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.496, loss_scale=8, train_wall=553, gb_free=8.8, wall=165463
2022-02-13 15:46:39 | INFO | train_inner | epoch 019:   1132 / 1576 loss=6.881, nll_loss=5.124, ppl=34.88, wps=11748.6, ups=0.18, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.503, loss_scale=8, train_wall=547, gb_free=8.8, wall=166021
2022-02-13 15:55:56 | INFO | train_inner | epoch 019:   1232 / 1576 loss=6.886, nll_loss=5.131, ppl=35.04, wps=11748.3, ups=0.18, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.51, loss_scale=16, train_wall=547, gb_free=8.8, wall=166579
2022-02-13 15:56:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:05:20 | INFO | train_inner | epoch 019:   1333 / 1576 loss=6.885, nll_loss=5.13, ppl=35.01, wps=11633.1, ups=0.18, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.515, loss_scale=8, train_wall=553, gb_free=8.8, wall=167142
2022-02-13 16:14:37 | INFO | train_inner | epoch 019:   1433 / 1576 loss=6.892, nll_loss=5.138, ppl=35.21, wps=11751.2, ups=0.18, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.505, loss_scale=8, train_wall=547, gb_free=8.8, wall=167700
2022-02-13 16:23:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:24:01 | INFO | train_inner | epoch 019:   1534 / 1576 loss=6.886, nll_loss=5.131, ppl=35.03, wps=11629.3, ups=0.18, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.495, loss_scale=8, train_wall=553, gb_free=8.8, wall=168263
2022-02-13 16:27:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 16:27:57 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.951 | nll_loss 5.165 | ppl 35.87 | wps 32320.1 | wpb 1021.8 | bsz 2 | num_updates 29842 | best_loss 6.951
2022-02-13 16:27:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 29842 updates
2022-02-13 16:27:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint19.pt
2022-02-13 16:28:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint19.pt
2022-02-13 16:28:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint19.pt (epoch 19 @ 29842 updates, score 6.951) (writing took 27.398017161060125 seconds)
2022-02-13 16:28:25 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-13 16:28:25 | INFO | train | epoch 019 | loss 6.863 | nll_loss 5.105 | ppl 34.41 | wps 11657.7 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 29842 | lr 0.000183057 | gnorm 0.505 | loss_scale 8 | train_wall 8621 | gb_free 8.8 | wall 168527
2022-02-13 16:28:25 | INFO | fairseq.trainer | begin training epoch 20
2022-02-13 16:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 16:33:48 | INFO | train_inner | epoch 020:     58 / 1576 loss=6.831, nll_loss=5.068, ppl=33.54, wps=11063.1, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=29900, lr=0.000182879, gnorm=0.513, loss_scale=8, train_wall=542, gb_free=8.8, wall=168851
2022-02-13 16:43:06 | INFO | train_inner | epoch 020:    158 / 1576 loss=6.802, nll_loss=5.035, ppl=32.79, wps=11752.7, ups=0.18, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.508, loss_scale=8, train_wall=547, gb_free=8.8, wall=169408
2022-02-13 16:47:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:52:29 | INFO | train_inner | epoch 020:    259 / 1576 loss=6.823, nll_loss=5.06, ppl=33.35, wps=11632.2, ups=0.18, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.511, loss_scale=8, train_wall=553, gb_free=8.8, wall=169972
2022-02-13 17:01:47 | INFO | train_inner | epoch 020:    359 / 1576 loss=6.83, nll_loss=5.067, ppl=33.51, wps=11747.1, ups=0.18, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.515, loss_scale=8, train_wall=547, gb_free=8.8, wall=170529
2022-02-13 17:11:05 | INFO | train_inner | epoch 020:    459 / 1576 loss=6.83, nll_loss=5.067, ppl=33.52, wps=11748.2, ups=0.18, wpb=65536, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.504, loss_scale=8, train_wall=547, gb_free=8.8, wall=171087
2022-02-13 17:13:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:20:29 | INFO | train_inner | epoch 020:    560 / 1576 loss=6.832, nll_loss=5.07, ppl=33.58, wps=11628.5, ups=0.18, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.509, loss_scale=8, train_wall=553, gb_free=8.8, wall=171651
2022-02-13 17:29:46 | INFO | train_inner | epoch 020:    660 / 1576 loss=6.846, nll_loss=5.086, ppl=33.96, wps=11747.3, ups=0.18, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.514, loss_scale=8, train_wall=547, gb_free=8.8, wall=172209
2022-02-13 17:38:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:39:10 | INFO | train_inner | epoch 020:    761 / 1576 loss=6.857, nll_loss=5.098, ppl=34.24, wps=11632.3, ups=0.18, wpb=65536, bsz=128, num_updates=30600, lr=0.000180775, gnorm=0.505, loss_scale=8, train_wall=553, gb_free=8.8, wall=172772
2022-02-13 17:48:27 | INFO | train_inner | epoch 020:    861 / 1576 loss=6.858, nll_loss=5.098, ppl=34.26, wps=11752.6, ups=0.18, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.505, loss_scale=8, train_wall=547, gb_free=8.8, wall=173330
2022-02-13 17:57:45 | INFO | train_inner | epoch 020:    961 / 1576 loss=6.861, nll_loss=5.102, ppl=34.35, wps=11751.9, ups=0.18, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.516, loss_scale=8, train_wall=547, gb_free=8.8, wall=173887
2022-02-13 18:02:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:03:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 18:07:14 | INFO | train_inner | epoch 020:   1063 / 1576 loss=6.877, nll_loss=5.12, ppl=34.78, wps=11520, ups=0.18, wpb=65532.3, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.533, loss_scale=4, train_wall=558, gb_free=8.8, wall=174456
2022-02-13 18:16:31 | INFO | train_inner | epoch 020:   1163 / 1576 loss=6.86, nll_loss=5.101, ppl=34.32, wps=11760.6, ups=0.18, wpb=65536, bsz=128, num_updates=31000, lr=0.000179605, gnorm=0.503, loss_scale=4, train_wall=547, gb_free=8.8, wall=175014
2022-02-13 18:25:49 | INFO | train_inner | epoch 020:   1263 / 1576 loss=6.874, nll_loss=5.118, ppl=34.72, wps=11758.2, ups=0.18, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.513, loss_scale=4, train_wall=547, gb_free=8.8, wall=175571
2022-02-13 18:35:07 | INFO | train_inner | epoch 020:   1363 / 1576 loss=6.874, nll_loss=5.117, ppl=34.7, wps=11742.7, ups=0.18, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.504, loss_scale=8, train_wall=548, gb_free=8.8, wall=176129
2022-02-13 18:44:25 | INFO | train_inner | epoch 020:   1463 / 1576 loss=6.876, nll_loss=5.12, ppl=34.77, wps=11746.2, ups=0.18, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.507, loss_scale=8, train_wall=547, gb_free=8.8, wall=176687
2022-02-13 18:52:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:53:48 | INFO | train_inner | epoch 020:   1564 / 1576 loss=6.883, nll_loss=5.127, ppl=34.95, wps=11627.1, ups=0.18, wpb=65536, bsz=128, num_updates=31400, lr=0.000178458, gnorm=0.513, loss_scale=8, train_wall=553, gb_free=8.8, wall=177251
2022-02-13 18:54:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 18:54:57 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.941 | nll_loss 5.158 | ppl 35.69 | wps 32504.7 | wpb 1021.8 | bsz 2 | num_updates 31412 | best_loss 6.941
2022-02-13 18:54:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 31412 updates
2022-02-13 18:54:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint20.pt
2022-02-13 18:55:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint20.pt
2022-02-13 18:55:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint20.pt (epoch 20 @ 31412 updates, score 6.941) (writing took 27.525566613767296 seconds)
2022-02-13 18:55:25 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-13 18:55:25 | INFO | train | epoch 020 | loss 6.85 | nll_loss 5.09 | ppl 34.07 | wps 11659.2 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 31412 | lr 0.000178424 | gnorm 0.511 | loss_scale 8 | train_wall 8620 | gb_free 8.8 | wall 177347
2022-02-13 18:55:25 | INFO | fairseq.trainer | begin training epoch 21
2022-02-13 18:55:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 19:03:36 | INFO | train_inner | epoch 021:     88 / 1576 loss=6.803, nll_loss=5.036, ppl=32.82, wps=11062.3, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=31500, lr=0.000178174, gnorm=0.519, loss_scale=8, train_wall=542, gb_free=8.8, wall=177838
2022-02-13 19:12:53 | INFO | train_inner | epoch 021:    188 / 1576 loss=6.789, nll_loss=5.021, ppl=32.47, wps=11749.9, ups=0.18, wpb=65536, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.519, loss_scale=8, train_wall=547, gb_free=8.8, wall=178396
2022-02-13 19:17:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:22:16 | INFO | train_inner | epoch 021:    289 / 1576 loss=6.813, nll_loss=5.047, ppl=33.07, wps=11636.7, ups=0.18, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.505, loss_scale=8, train_wall=553, gb_free=8.8, wall=178959
2022-02-13 19:31:34 | INFO | train_inner | epoch 021:    389 / 1576 loss=6.822, nll_loss=5.058, ppl=33.32, wps=11753.6, ups=0.18, wpb=65536, bsz=128, num_updates=31800, lr=0.000177332, gnorm=0.508, loss_scale=8, train_wall=547, gb_free=8.8, wall=179516
2022-02-13 19:40:52 | INFO | train_inner | epoch 021:    489 / 1576 loss=6.83, nll_loss=5.067, ppl=33.52, wps=11753.8, ups=0.18, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.517, loss_scale=8, train_wall=547, gb_free=8.8, wall=180074
2022-02-13 19:43:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:50:15 | INFO | train_inner | epoch 021:    590 / 1576 loss=6.842, nll_loss=5.081, ppl=33.84, wps=11631.7, ups=0.18, wpb=65536, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.508, loss_scale=8, train_wall=553, gb_free=8.8, wall=180637
2022-02-13 19:59:33 | INFO | train_inner | epoch 021:    690 / 1576 loss=6.843, nll_loss=5.082, ppl=33.87, wps=11751.4, ups=0.18, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.505, loss_scale=8, train_wall=547, gb_free=8.8, wall=181195
2022-02-13 20:08:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:08:56 | INFO | train_inner | epoch 021:    791 / 1576 loss=6.837, nll_loss=5.076, ppl=33.72, wps=11631.3, ups=0.18, wpb=65536, bsz=128, num_updates=32200, lr=0.000176227, gnorm=0.51, loss_scale=8, train_wall=553, gb_free=8.8, wall=181759
2022-02-13 20:18:14 | INFO | train_inner | epoch 021:    891 / 1576 loss=6.849, nll_loss=5.089, ppl=34.04, wps=11748, ups=0.18, wpb=65532.3, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.499, loss_scale=8, train_wall=547, gb_free=8.8, wall=182316
2022-02-13 20:21:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 20:27:37 | INFO | train_inner | epoch 021:    992 / 1576 loss=6.85, nll_loss=5.089, ppl=34.04, wps=11640.2, ups=0.18, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.518, loss_scale=4, train_wall=552, gb_free=8.8, wall=182879
2022-02-13 20:36:55 | INFO | train_inner | epoch 021:   1092 / 1576 loss=6.85, nll_loss=5.09, ppl=34.07, wps=11753.3, ups=0.18, wpb=65536, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.515, loss_scale=4, train_wall=547, gb_free=8.8, wall=183437
2022-02-13 20:46:12 | INFO | train_inner | epoch 021:   1192 / 1576 loss=6.857, nll_loss=5.098, ppl=34.26, wps=11756.5, ups=0.18, wpb=65536, bsz=128, num_updates=32600, lr=0.000175142, gnorm=0.494, loss_scale=8, train_wall=547, gb_free=8.8, wall=183994
2022-02-13 20:55:30 | INFO | train_inner | epoch 021:   1292 / 1576 loss=6.86, nll_loss=5.101, ppl=34.33, wps=11741.6, ups=0.18, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.51, loss_scale=8, train_wall=548, gb_free=8.8, wall=184553
2022-02-13 21:01:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 21:04:54 | INFO | train_inner | epoch 021:   1393 / 1576 loss=6.864, nll_loss=5.106, ppl=34.44, wps=11632.8, ups=0.18, wpb=65536, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.508, loss_scale=4, train_wall=553, gb_free=8.8, wall=185116
2022-02-13 21:14:11 | INFO | train_inner | epoch 021:   1493 / 1576 loss=6.86, nll_loss=5.102, ppl=34.33, wps=11761.2, ups=0.18, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.504, loss_scale=4, train_wall=547, gb_free=8.8, wall=185673
2022-02-13 21:21:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 21:21:56 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.937 | nll_loss 5.157 | ppl 35.67 | wps 32198.2 | wpb 1021.8 | bsz 2 | num_updates 32983 | best_loss 6.937
2022-02-13 21:21:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 32983 updates
2022-02-13 21:21:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint21.pt
2022-02-13 21:22:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint21.pt
2022-02-13 21:22:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint21.pt (epoch 21 @ 32983 updates, score 6.937) (writing took 27.345944168046117 seconds)
2022-02-13 21:22:23 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-13 21:22:23 | INFO | train | epoch 021 | loss 6.839 | nll_loss 5.077 | ppl 33.76 | wps 11668.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 32983 | lr 0.000174123 | gnorm 0.509 | loss_scale 4 | train_wall 8619 | gb_free 8.8 | wall 186165
2022-02-13 21:22:23 | INFO | fairseq.trainer | begin training epoch 22
2022-02-13 21:22:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 21:23:58 | INFO | train_inner | epoch 022:     17 / 1576 loss=6.853, nll_loss=5.094, ppl=34.15, wps=11066.1, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=33000, lr=0.000174078, gnorm=0.516, loss_scale=4, train_wall=542, gb_free=8.8, wall=186260
2022-02-13 21:33:16 | INFO | train_inner | epoch 022:    117 / 1576 loss=6.774, nll_loss=5.004, ppl=32.09, wps=11743.2, ups=0.18, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.51, loss_scale=8, train_wall=547, gb_free=8.8, wall=186818
2022-02-13 21:42:34 | INFO | train_inner | epoch 022:    217 / 1576 loss=6.788, nll_loss=5.019, ppl=32.43, wps=11744.1, ups=0.18, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.503, loss_scale=8, train_wall=547, gb_free=8.8, wall=187376
2022-02-13 21:50:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:51:58 | INFO | train_inner | epoch 022:    318 / 1576 loss=6.806, nll_loss=5.04, ppl=32.9, wps=11624.2, ups=0.18, wpb=65536, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.502, loss_scale=8, train_wall=553, gb_free=8.8, wall=187940
2022-02-13 22:01:16 | INFO | train_inner | epoch 022:    418 / 1576 loss=6.812, nll_loss=5.046, ppl=33.04, wps=11744.6, ups=0.18, wpb=65536, bsz=128, num_updates=33400, lr=0.000173032, gnorm=0.517, loss_scale=8, train_wall=547, gb_free=8.8, wall=188498
2022-02-13 22:10:34 | INFO | train_inner | epoch 022:    518 / 1576 loss=6.809, nll_loss=5.044, ppl=32.98, wps=11737.6, ups=0.18, wpb=65536, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.519, loss_scale=8, train_wall=548, gb_free=8.8, wall=189056
2022-02-13 22:19:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:19:58 | INFO | train_inner | epoch 022:    619 / 1576 loss=6.822, nll_loss=5.058, ppl=33.31, wps=11623.8, ups=0.18, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.499, loss_scale=8, train_wall=553, gb_free=8.8, wall=189620
2022-02-13 22:29:16 | INFO | train_inner | epoch 022:    719 / 1576 loss=6.832, nll_loss=5.069, ppl=33.58, wps=11745.9, ups=0.18, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.514, loss_scale=8, train_wall=547, gb_free=8.8, wall=190178
2022-02-13 22:38:34 | INFO | train_inner | epoch 022:    819 / 1576 loss=6.837, nll_loss=5.075, ppl=33.71, wps=11743.5, ups=0.18, wpb=65536, bsz=128, num_updates=33800, lr=0.000172005, gnorm=0.513, loss_scale=8, train_wall=548, gb_free=8.8, wall=190736
2022-02-13 22:42:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:47:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 22:48:03 | INFO | train_inner | epoch 022:    921 / 1576 loss=6.839, nll_loss=5.077, ppl=33.75, wps=11517.6, ups=0.18, wpb=65536, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.523, loss_scale=4, train_wall=558, gb_free=8.8, wall=191305
2022-02-13 22:57:21 | INFO | train_inner | epoch 022:   1021 / 1576 loss=6.844, nll_loss=5.083, ppl=33.89, wps=11746.5, ups=0.18, wpb=65532.3, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.497, loss_scale=4, train_wall=547, gb_free=8.8, wall=191863
2022-02-13 23:06:39 | INFO | train_inner | epoch 022:   1121 / 1576 loss=6.848, nll_loss=5.088, ppl=34, wps=11750.7, ups=0.18, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=0.509, loss_scale=4, train_wall=547, gb_free=8.8, wall=192421
2022-02-13 23:15:57 | INFO | train_inner | epoch 022:   1221 / 1576 loss=6.846, nll_loss=5.086, ppl=33.96, wps=11744.4, ups=0.18, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=0.522, loss_scale=8, train_wall=548, gb_free=8.8, wall=192979
2022-02-13 23:25:15 | INFO | train_inner | epoch 022:   1321 / 1576 loss=6.851, nll_loss=5.09, ppl=34.07, wps=11740.9, ups=0.18, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.531, loss_scale=8, train_wall=548, gb_free=8.8, wall=193537
2022-02-13 23:27:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 23:34:38 | INFO | train_inner | epoch 022:   1422 / 1576 loss=6.859, nll_loss=5.1, ppl=34.29, wps=11630.9, ups=0.18, wpb=65536, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.515, loss_scale=4, train_wall=553, gb_free=8.8, wall=194101
2022-02-13 23:43:56 | INFO | train_inner | epoch 022:   1522 / 1576 loss=6.858, nll_loss=5.099, ppl=34.28, wps=11746.4, ups=0.18, wpb=65536, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.504, loss_scale=4, train_wall=547, gb_free=8.8, wall=194658
2022-02-13 23:48:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 23:49:00 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.937 | nll_loss 5.15 | ppl 35.5 | wps 32284.2 | wpb 1021.8 | bsz 2 | num_updates 34554 | best_loss 6.937
2022-02-13 23:49:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 34554 updates
2022-02-13 23:49:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint22.pt
2022-02-13 23:49:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint22.pt
2022-02-13 23:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint22.pt (epoch 22 @ 34554 updates, score 6.937) (writing took 27.44304352486506 seconds)
2022-02-13 23:49:27 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-13 23:49:27 | INFO | train | epoch 022 | loss 6.829 | nll_loss 5.066 | ppl 33.49 | wps 11661.1 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 34554 | lr 0.000170118 | gnorm 0.512 | loss_scale 4 | train_wall 8624 | gb_free 8.8 | wall 194990
2022-02-13 23:49:27 | INFO | fairseq.trainer | begin training epoch 23
2022-02-13 23:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 23:53:44 | INFO | train_inner | epoch 023:     46 / 1576 loss=6.81, nll_loss=5.045, ppl=33.01, wps=11055.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=34600, lr=0.000170005, gnorm=0.512, loss_scale=8, train_wall=543, gb_free=8.8, wall=195246
2022-02-14 00:03:03 | INFO | train_inner | epoch 023:    146 / 1576 loss=6.775, nll_loss=5.004, ppl=32.1, wps=11719.4, ups=0.18, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.524, loss_scale=8, train_wall=549, gb_free=8.8, wall=195805
2022-02-14 00:12:22 | INFO | train_inner | epoch 023:    246 / 1576 loss=6.79, nll_loss=5.022, ppl=32.49, wps=11720.6, ups=0.18, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.52, loss_scale=8, train_wall=549, gb_free=8.8, wall=196364
2022-02-14 00:15:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 00:21:46 | INFO | train_inner | epoch 023:    347 / 1576 loss=6.794, nll_loss=5.026, ppl=32.57, wps=11616.6, ups=0.18, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.53, loss_scale=4, train_wall=554, gb_free=8.8, wall=196929
2022-02-14 00:31:05 | INFO | train_inner | epoch 023:    447 / 1576 loss=6.798, nll_loss=5.031, ppl=32.7, wps=11731.7, ups=0.18, wpb=65536, bsz=128, num_updates=35000, lr=0.000169031, gnorm=0.518, loss_scale=4, train_wall=548, gb_free=8.8, wall=197487
2022-02-14 00:40:24 | INFO | train_inner | epoch 023:    547 / 1576 loss=6.806, nll_loss=5.04, ppl=32.9, wps=11729.8, ups=0.18, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.532, loss_scale=8, train_wall=548, gb_free=8.8, wall=198046
2022-02-14 00:49:43 | INFO | train_inner | epoch 023:    647 / 1576 loss=6.824, nll_loss=5.06, ppl=33.36, wps=11715, ups=0.18, wpb=65536, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.506, loss_scale=8, train_wall=549, gb_free=8.8, wall=198605
2022-02-14 00:59:02 | INFO | train_inner | epoch 023:    747 / 1576 loss=6.821, nll_loss=5.057, ppl=33.29, wps=11718.7, ups=0.18, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.512, loss_scale=8, train_wall=549, gb_free=8.8, wall=199165
2022-02-14 01:03:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:08:27 | INFO | train_inner | epoch 023:    848 / 1576 loss=6.828, nll_loss=5.065, ppl=33.48, wps=11608.1, ups=0.18, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=0.524, loss_scale=8, train_wall=554, gb_free=8.8, wall=199729
2022-02-14 01:17:46 | INFO | train_inner | epoch 023:    948 / 1576 loss=6.829, nll_loss=5.066, ppl=33.5, wps=11719.4, ups=0.18, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.505, loss_scale=8, train_wall=549, gb_free=8.8, wall=200288
2022-02-14 01:25:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 01:27:11 | INFO | train_inner | epoch 023:   1049 / 1576 loss=6.829, nll_loss=5.066, ppl=33.5, wps=11609.1, ups=0.18, wpb=65536, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.528, loss_scale=4, train_wall=554, gb_free=8.8, wall=200853
2022-02-14 01:36:29 | INFO | train_inner | epoch 023:   1149 / 1576 loss=6.844, nll_loss=5.083, ppl=33.9, wps=11738.8, ups=0.18, wpb=65536, bsz=128, num_updates=35700, lr=0.000167365, gnorm=0.515, loss_scale=4, train_wall=548, gb_free=8.8, wall=201411
2022-02-14 01:45:47 | INFO | train_inner | epoch 023:   1249 / 1576 loss=6.837, nll_loss=5.075, ppl=33.7, wps=11739, ups=0.18, wpb=65536, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.512, loss_scale=4, train_wall=548, gb_free=8.8, wall=201969
2022-02-14 01:55:06 | INFO | train_inner | epoch 023:   1349 / 1576 loss=6.844, nll_loss=5.083, ppl=33.9, wps=11730.6, ups=0.18, wpb=65536, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.515, loss_scale=8, train_wall=548, gb_free=8.8, wall=202528
2022-02-14 02:02:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 02:04:30 | INFO | train_inner | epoch 023:   1450 / 1576 loss=6.839, nll_loss=5.077, ppl=33.75, wps=11610.8, ups=0.18, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.513, loss_scale=4, train_wall=554, gb_free=8.8, wall=203093
2022-02-14 02:13:49 | INFO | train_inner | epoch 023:   1550 / 1576 loss=6.852, nll_loss=5.093, ppl=34.12, wps=11734.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=36100, lr=0.000166436, gnorm=0.504, loss_scale=4, train_wall=548, gb_free=8.8, wall=203651
2022-02-14 02:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 02:16:16 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.931 | nll_loss 5.151 | ppl 35.53 | wps 32211.4 | wpb 1021.8 | bsz 2 | num_updates 36126 | best_loss 6.931
2022-02-14 02:16:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 36126 updates
2022-02-14 02:16:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint23.pt
2022-02-14 02:16:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint23.pt
2022-02-14 02:16:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint23.pt (epoch 23 @ 36126 updates, score 6.931) (writing took 27.28115601092577 seconds)
2022-02-14 02:16:43 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-14 02:16:43 | INFO | train | epoch 023 | loss 6.819 | nll_loss 5.055 | ppl 33.24 | wps 11652.8 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 36126 | lr 0.000166376 | gnorm 0.517 | loss_scale 4 | train_wall 8637 | gb_free 8.8 | wall 203826
2022-02-14 02:16:43 | INFO | fairseq.trainer | begin training epoch 24
2022-02-14 02:16:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 02:23:36 | INFO | train_inner | epoch 024:     74 / 1576 loss=6.781, nll_loss=5.012, ppl=32.26, wps=11066.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=36200, lr=0.000166206, gnorm=0.516, loss_scale=4, train_wall=542, gb_free=8.8, wall=204238
2022-02-14 02:32:55 | INFO | train_inner | epoch 024:    174 / 1576 loss=6.761, nll_loss=4.989, ppl=31.75, wps=11726.2, ups=0.18, wpb=65536, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.512, loss_scale=8, train_wall=548, gb_free=8.8, wall=204797
2022-02-14 02:42:14 | INFO | train_inner | epoch 024:    274 / 1576 loss=6.769, nll_loss=4.998, ppl=31.96, wps=11719.8, ups=0.18, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.505, loss_scale=8, train_wall=549, gb_free=8.8, wall=205356
2022-02-14 02:51:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:51:38 | INFO | train_inner | epoch 024:    375 / 1576 loss=6.787, nll_loss=5.018, ppl=32.41, wps=11606.8, ups=0.18, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=0.515, loss_scale=8, train_wall=554, gb_free=8.8, wall=205921
2022-02-14 03:00:58 | INFO | train_inner | epoch 024:    475 / 1576 loss=6.787, nll_loss=5.018, ppl=32.41, wps=11722.3, ups=0.18, wpb=65536, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.504, loss_scale=8, train_wall=549, gb_free=8.8, wall=206480
2022-02-14 03:08:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 03:10:22 | INFO | train_inner | epoch 024:    576 / 1576 loss=6.807, nll_loss=5.041, ppl=32.93, wps=11601, ups=0.18, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.533, loss_scale=4, train_wall=554, gb_free=8.8, wall=207045
2022-02-14 03:19:41 | INFO | train_inner | epoch 024:    676 / 1576 loss=6.818, nll_loss=5.054, ppl=33.22, wps=11731.8, ups=0.18, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.519, loss_scale=4, train_wall=548, gb_free=8.8, wall=207603
2022-02-14 03:29:00 | INFO | train_inner | epoch 024:    776 / 1576 loss=6.812, nll_loss=5.047, ppl=33.06, wps=11726.4, ups=0.18, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=0.507, loss_scale=4, train_wall=548, gb_free=8.8, wall=208162
2022-02-14 03:38:19 | INFO | train_inner | epoch 024:    876 / 1576 loss=6.806, nll_loss=5.04, ppl=32.9, wps=11726.3, ups=0.18, wpb=65536, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.525, loss_scale=8, train_wall=548, gb_free=8.8, wall=208721
2022-02-14 03:47:38 | INFO | train_inner | epoch 024:    976 / 1576 loss=6.827, nll_loss=5.064, ppl=33.45, wps=11719.2, ups=0.18, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.51, loss_scale=8, train_wall=549, gb_free=8.8, wall=209280
2022-02-14 03:56:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:57:03 | INFO | train_inner | epoch 024:   1077 / 1576 loss=6.826, nll_loss=5.063, ppl=33.42, wps=11608.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=37200, lr=0.000163956, gnorm=0.515, loss_scale=8, train_wall=554, gb_free=8.8, wall=209845
2022-02-14 04:06:21 | INFO | train_inner | epoch 024:   1177 / 1576 loss=6.833, nll_loss=5.07, ppl=33.6, wps=11727.3, ups=0.18, wpb=65536, bsz=128, num_updates=37300, lr=0.000163737, gnorm=0.52, loss_scale=8, train_wall=548, gb_free=8.8, wall=210404
2022-02-14 04:12:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 04:15:46 | INFO | train_inner | epoch 024:   1278 / 1576 loss=6.838, nll_loss=5.076, ppl=33.73, wps=11611.4, ups=0.18, wpb=65536, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.522, loss_scale=4, train_wall=554, gb_free=8.8, wall=210968
2022-02-14 04:25:04 | INFO | train_inner | epoch 024:   1378 / 1576 loss=6.835, nll_loss=5.074, ppl=33.67, wps=11734.4, ups=0.18, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.509, loss_scale=4, train_wall=548, gb_free=8.8, wall=211527
2022-02-14 04:34:23 | INFO | train_inner | epoch 024:   1478 / 1576 loss=6.835, nll_loss=5.073, ppl=33.67, wps=11737.3, ups=0.18, wpb=65536, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.516, loss_scale=4, train_wall=548, gb_free=8.8, wall=212085
2022-02-14 04:43:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 04:43:32 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.924 | nll_loss 5.156 | ppl 35.66 | wps 32285.3 | wpb 1021.8 | bsz 2 | num_updates 37698 | best_loss 6.924
2022-02-14 04:43:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 37698 updates
2022-02-14 04:43:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint24.pt
2022-02-14 04:43:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint24.pt
2022-02-14 04:44:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint24.pt (epoch 24 @ 37698 updates, score 6.924) (writing took 27.605166020337492 seconds)
2022-02-14 04:44:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-14 04:44:00 | INFO | train | epoch 024 | loss 6.81 | nll_loss 5.045 | ppl 33.01 | wps 11652 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 37698 | lr 0.00016287 | gnorm 0.515 | loss_scale 8 | train_wall 8637 | gb_free 8.8 | wall 212662
2022-02-14 04:44:00 | INFO | fairseq.trainer | begin training epoch 25
2022-02-14 04:44:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 04:44:11 | INFO | train_inner | epoch 025:      2 / 1576 loss=6.848, nll_loss=5.088, ppl=34.01, wps=11038.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=37700, lr=0.000162866, gnorm=0.528, loss_scale=8, train_wall=544, gb_free=8.8, wall=212674
2022-02-14 04:53:30 | INFO | train_inner | epoch 025:    102 / 1576 loss=6.76, nll_loss=4.988, ppl=31.74, wps=11729, ups=0.18, wpb=65536, bsz=128, num_updates=37800, lr=0.00016265, gnorm=0.511, loss_scale=8, train_wall=548, gb_free=8.8, wall=213232
2022-02-14 05:01:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:02:55 | INFO | train_inner | epoch 025:    203 / 1576 loss=6.754, nll_loss=4.981, ppl=31.59, wps=11599, ups=0.18, wpb=65536, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.517, loss_scale=8, train_wall=554, gb_free=8.8, wall=213797
2022-02-14 05:10:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 05:12:20 | INFO | train_inner | epoch 025:    304 / 1576 loss=6.776, nll_loss=5.006, ppl=32.13, wps=11599.8, ups=0.18, wpb=65536, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.511, loss_scale=4, train_wall=554, gb_free=8.8, wall=214362
2022-02-14 05:21:39 | INFO | train_inner | epoch 025:    404 / 1576 loss=6.776, nll_loss=5.006, ppl=32.12, wps=11730.3, ups=0.18, wpb=65536, bsz=128, num_updates=38100, lr=0.000162008, gnorm=0.52, loss_scale=4, train_wall=548, gb_free=8.8, wall=214921
2022-02-14 05:30:57 | INFO | train_inner | epoch 025:    504 / 1576 loss=6.795, nll_loss=5.028, ppl=32.62, wps=11727.5, ups=0.18, wpb=65536, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.518, loss_scale=4, train_wall=548, gb_free=8.8, wall=215480
2022-02-14 05:40:17 | INFO | train_inner | epoch 025:    604 / 1576 loss=6.798, nll_loss=5.031, ppl=32.68, wps=11717.9, ups=0.18, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.519, loss_scale=8, train_wall=549, gb_free=8.8, wall=216039
2022-02-14 05:40:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 05:49:41 | INFO | train_inner | epoch 025:    705 / 1576 loss=6.796, nll_loss=5.028, ppl=32.63, wps=11615.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.541, loss_scale=4, train_wall=554, gb_free=8.8, wall=216603
2022-02-14 05:59:00 | INFO | train_inner | epoch 025:    805 / 1576 loss=6.81, nll_loss=5.045, ppl=33.01, wps=11729.8, ups=0.18, wpb=65536, bsz=128, num_updates=38500, lr=0.000161165, gnorm=0.519, loss_scale=4, train_wall=548, gb_free=8.8, wall=217162
2022-02-14 06:08:18 | INFO | train_inner | epoch 025:    905 / 1576 loss=6.812, nll_loss=5.047, ppl=33.05, wps=11727.3, ups=0.18, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.511, loss_scale=8, train_wall=548, gb_free=8.8, wall=217721
2022-02-14 06:17:38 | INFO | train_inner | epoch 025:   1005 / 1576 loss=6.805, nll_loss=5.039, ppl=32.87, wps=11720.6, ups=0.18, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.525, loss_scale=8, train_wall=549, gb_free=8.8, wall=218280
2022-02-14 06:26:57 | INFO | train_inner | epoch 025:   1105 / 1576 loss=6.819, nll_loss=5.055, ppl=33.24, wps=11720.3, ups=0.18, wpb=65536, bsz=128, num_updates=38800, lr=0.00016054, gnorm=0.508, loss_scale=8, train_wall=549, gb_free=8.8, wall=218839
2022-02-14 06:28:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:33:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 06:36:27 | INFO | train_inner | epoch 025:   1207 / 1576 loss=6.821, nll_loss=5.058, ppl=33.3, wps=11491.9, ups=0.18, wpb=65536, bsz=128, num_updates=38900, lr=0.000160334, gnorm=0.515, loss_scale=4, train_wall=560, gb_free=8.8, wall=219409
2022-02-14 06:45:46 | INFO | train_inner | epoch 025:   1307 / 1576 loss=6.825, nll_loss=5.062, ppl=33.4, wps=11730, ups=0.18, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.509, loss_scale=4, train_wall=548, gb_free=8.8, wall=219968
2022-02-14 06:55:04 | INFO | train_inner | epoch 025:   1407 / 1576 loss=6.83, nll_loss=5.068, ppl=33.53, wps=11729.7, ups=0.18, wpb=65536, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.512, loss_scale=4, train_wall=548, gb_free=8.8, wall=220527
2022-02-14 07:04:23 | INFO | train_inner | epoch 025:   1507 / 1576 loss=6.832, nll_loss=5.069, ppl=33.57, wps=11724.7, ups=0.18, wpb=65536, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.514, loss_scale=8, train_wall=549, gb_free=8.8, wall=221086
2022-02-14 07:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 07:10:51 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.922 | nll_loss 5.154 | ppl 35.6 | wps 32416.2 | wpb 1021.8 | bsz 2 | num_updates 39269 | best_loss 6.922
2022-02-14 07:10:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 39269 updates
2022-02-14 07:10:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint25.pt
2022-02-14 07:11:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint25.pt
2022-02-14 07:11:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint25.pt (epoch 25 @ 39269 updates, score 6.922) (writing took 27.597012270241976 seconds)
2022-02-14 07:11:19 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-14 07:11:19 | INFO | train | epoch 025 | loss 6.802 | nll_loss 5.035 | ppl 32.8 | wps 11641.6 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 39269 | lr 0.000159579 | gnorm 0.517 | loss_scale 8 | train_wall 8639 | gb_free 8.8 | wall 221501
2022-02-14 07:11:19 | INFO | fairseq.trainer | begin training epoch 26
2022-02-14 07:11:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 07:14:12 | INFO | train_inner | epoch 026:     31 / 1576 loss=6.806, nll_loss=5.041, ppl=32.92, wps=11038.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=39300, lr=0.000159516, gnorm=0.531, loss_scale=8, train_wall=544, gb_free=8.8, wall=221674
2022-02-14 07:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:23:37 | INFO | train_inner | epoch 026:    132 / 1576 loss=6.752, nll_loss=4.98, ppl=31.55, wps=11604.5, ups=0.18, wpb=65536, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.505, loss_scale=8, train_wall=554, gb_free=8.8, wall=222239
2022-02-14 07:32:56 | INFO | train_inner | epoch 026:    232 / 1576 loss=6.765, nll_loss=4.993, ppl=31.86, wps=11714.2, ups=0.18, wpb=65536, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.522, loss_scale=8, train_wall=549, gb_free=8.8, wall=222798
2022-02-14 07:33:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 07:42:21 | INFO | train_inner | epoch 026:    333 / 1576 loss=6.764, nll_loss=4.993, ppl=31.84, wps=11605.5, ups=0.18, wpb=65536, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.554, loss_scale=4, train_wall=554, gb_free=8.8, wall=223363
2022-02-14 07:51:40 | INFO | train_inner | epoch 026:    433 / 1576 loss=6.773, nll_loss=5.003, ppl=32.07, wps=11727.4, ups=0.18, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=0.512, loss_scale=4, train_wall=548, gb_free=8.8, wall=223922
2022-02-14 08:00:59 | INFO | train_inner | epoch 026:    533 / 1576 loss=6.775, nll_loss=5.005, ppl=32.12, wps=11723.7, ups=0.18, wpb=65536, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.514, loss_scale=8, train_wall=549, gb_free=8.8, wall=224481
2022-02-14 08:10:18 | INFO | train_inner | epoch 026:    633 / 1576 loss=6.788, nll_loss=5.02, ppl=32.46, wps=11721.4, ups=0.18, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.515, loss_scale=8, train_wall=549, gb_free=8.8, wall=225040
2022-02-14 08:19:37 | INFO | train_inner | epoch 026:    733 / 1576 loss=6.794, nll_loss=5.027, ppl=32.6, wps=11719.4, ups=0.18, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.519, loss_scale=8, train_wall=549, gb_free=8.8, wall=225599
2022-02-14 08:21:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:29:02 | INFO | train_inner | epoch 026:    834 / 1576 loss=6.8, nll_loss=5.034, ppl=32.75, wps=11605.1, ups=0.18, wpb=65536, bsz=128, num_updates=40100, lr=0.000157917, gnorm=0.523, loss_scale=8, train_wall=554, gb_free=8.8, wall=226164
2022-02-14 08:38:21 | INFO | train_inner | epoch 026:    934 / 1576 loss=6.807, nll_loss=5.041, ppl=32.92, wps=11721.7, ups=0.18, wpb=65536, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.513, loss_scale=8, train_wall=549, gb_free=8.8, wall=226723
2022-02-14 08:46:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:47:46 | INFO | train_inner | epoch 026:   1035 / 1576 loss=6.799, nll_loss=5.033, ppl=32.73, wps=11600.1, ups=0.18, wpb=65536, bsz=128, num_updates=40300, lr=0.000157524, gnorm=0.516, loss_scale=8, train_wall=554, gb_free=8.8, wall=227288
2022-02-14 08:57:05 | INFO | train_inner | epoch 026:   1135 / 1576 loss=6.81, nll_loss=5.045, ppl=33.01, wps=11715.4, ups=0.18, wpb=65536, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.509, loss_scale=8, train_wall=549, gb_free=8.8, wall=227848
2022-02-14 09:06:25 | INFO | train_inner | epoch 026:   1235 / 1576 loss=6.819, nll_loss=5.055, ppl=33.24, wps=11714.3, ups=0.18, wpb=65536, bsz=128, num_updates=40500, lr=0.000157135, gnorm=0.518, loss_scale=8, train_wall=549, gb_free=8.8, wall=228407
2022-02-14 09:13:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:15:49 | INFO | train_inner | epoch 026:   1336 / 1576 loss=6.824, nll_loss=5.061, ppl=33.38, wps=11603.5, ups=0.18, wpb=65536, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.505, loss_scale=8, train_wall=554, gb_free=8.8, wall=228972
2022-02-14 09:17:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 09:25:14 | INFO | train_inner | epoch 026:   1437 / 1576 loss=6.81, nll_loss=5.045, ppl=33.01, wps=11610, ups=0.18, wpb=65532.3, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.526, loss_scale=4, train_wall=554, gb_free=8.8, wall=229536
2022-02-14 09:34:33 | INFO | train_inner | epoch 026:   1537 / 1576 loss=6.828, nll_loss=5.065, ppl=33.48, wps=11723.3, ups=0.18, wpb=65536, bsz=128, num_updates=40800, lr=0.000156556, gnorm=0.53, loss_scale=4, train_wall=549, gb_free=8.8, wall=230095
2022-02-14 09:38:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 09:38:13 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.919 | nll_loss 5.147 | ppl 35.42 | wps 32181 | wpb 1021.8 | bsz 2 | num_updates 40839 | best_loss 6.919
2022-02-14 09:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 40839 updates
2022-02-14 09:38:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint26.pt
2022-02-14 09:38:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint26.pt
2022-02-14 09:38:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint26.pt (epoch 26 @ 40839 updates, score 6.919) (writing took 27.40743931522593 seconds)
2022-02-14 09:38:40 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-14 09:38:40 | INFO | train | epoch 026 | loss 6.794 | nll_loss 5.027 | ppl 32.6 | wps 11630.9 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 40839 | lr 0.000156481 | gnorm 0.52 | loss_scale 4 | train_wall 8642 | gb_free 8.8 | wall 230343
2022-02-14 09:38:40 | INFO | fairseq.trainer | begin training epoch 27
2022-02-14 09:38:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 09:44:21 | INFO | train_inner | epoch 027:     61 / 1576 loss=6.773, nll_loss=5.003, ppl=32.07, wps=11048.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=40900, lr=0.000156365, gnorm=0.522, loss_scale=8, train_wall=543, gb_free=8.8, wall=230683
2022-02-14 09:53:40 | INFO | train_inner | epoch 027:    161 / 1576 loss=6.749, nll_loss=4.976, ppl=31.46, wps=11721.7, ups=0.18, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.529, loss_scale=8, train_wall=549, gb_free=8.8, wall=231242
2022-02-14 10:02:59 | INFO | train_inner | epoch 027:    261 / 1576 loss=6.759, nll_loss=4.986, ppl=31.7, wps=11722.3, ups=0.18, wpb=65536, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.511, loss_scale=8, train_wall=549, gb_free=8.8, wall=231801
2022-02-14 10:05:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:12:24 | INFO | train_inner | epoch 027:    362 / 1576 loss=6.75, nll_loss=4.977, ppl=31.5, wps=11603.1, ups=0.18, wpb=65536, bsz=128, num_updates=41200, lr=0.000155794, gnorm=0.519, loss_scale=8, train_wall=554, gb_free=8.8, wall=232366
2022-02-14 10:21:43 | INFO | train_inner | epoch 027:    462 / 1576 loss=6.766, nll_loss=4.995, ppl=31.88, wps=11722.3, ups=0.18, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.522, loss_scale=8, train_wall=549, gb_free=8.8, wall=232925
2022-02-14 10:31:02 | INFO | train_inner | epoch 027:    562 / 1576 loss=6.777, nll_loss=5.008, ppl=32.17, wps=11720.5, ups=0.18, wpb=65536, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.526, loss_scale=16, train_wall=549, gb_free=8.8, wall=233484
2022-02-14 10:31:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:40:27 | INFO | train_inner | epoch 027:    663 / 1576 loss=6.782, nll_loss=5.014, ppl=32.31, wps=11602.3, ups=0.18, wpb=65536, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.52, loss_scale=8, train_wall=554, gb_free=8.8, wall=234049
2022-02-14 10:49:46 | INFO | train_inner | epoch 027:    763 / 1576 loss=6.796, nll_loss=5.029, ppl=32.66, wps=11719.8, ups=0.18, wpb=65536, bsz=128, num_updates=41600, lr=0.000155043, gnorm=0.506, loss_scale=8, train_wall=549, gb_free=8.8, wall=234608
2022-02-14 10:57:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:59:11 | INFO | train_inner | epoch 027:    864 / 1576 loss=6.794, nll_loss=5.027, ppl=32.61, wps=11597, ups=0.18, wpb=65536, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.515, loss_scale=8, train_wall=554, gb_free=8.8, wall=235174
2022-02-14 11:08:31 | INFO | train_inner | epoch 027:    964 / 1576 loss=6.8, nll_loss=5.033, ppl=32.75, wps=11718, ups=0.18, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.525, loss_scale=8, train_wall=549, gb_free=8.8, wall=235733
2022-02-14 11:17:50 | INFO | train_inner | epoch 027:   1064 / 1576 loss=6.799, nll_loss=5.032, ppl=32.73, wps=11718.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=41900, lr=0.000154487, gnorm=0.519, loss_scale=8, train_wall=549, gb_free=8.8, wall=236292
2022-02-14 11:23:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:27:15 | INFO | train_inner | epoch 027:   1165 / 1576 loss=6.807, nll_loss=5.042, ppl=32.94, wps=11603.2, ups=0.18, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=0.513, loss_scale=8, train_wall=554, gb_free=8.8, wall=236857
2022-02-14 11:36:34 | INFO | train_inner | epoch 027:   1265 / 1576 loss=6.813, nll_loss=5.048, ppl=33.09, wps=11717.7, ups=0.18, wpb=65536, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.513, loss_scale=8, train_wall=549, gb_free=8.8, wall=237416
2022-02-14 11:45:54 | INFO | train_inner | epoch 027:   1365 / 1576 loss=6.811, nll_loss=5.046, ppl=33.04, wps=11707.8, ups=0.18, wpb=65536, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.51, loss_scale=8, train_wall=549, gb_free=8.8, wall=237976
2022-02-14 11:47:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:55:18 | INFO | train_inner | epoch 027:   1466 / 1576 loss=6.813, nll_loss=5.048, ppl=33.09, wps=11605.3, ups=0.18, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.518, loss_scale=8, train_wall=554, gb_free=8.8, wall=238541
2022-02-14 12:04:38 | INFO | train_inner | epoch 027:   1566 / 1576 loss=6.818, nll_loss=5.054, ppl=33.23, wps=11719.3, ups=0.18, wpb=65536, bsz=128, num_updates=42400, lr=0.000153574, gnorm=0.524, loss_scale=8, train_wall=549, gb_free=8.8, wall=239100
2022-02-14 12:05:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 12:05:35 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.92 | nll_loss 5.13 | ppl 35.02 | wps 32228.5 | wpb 1021.8 | bsz 2 | num_updates 42410 | best_loss 6.919
2022-02-14 12:05:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 42410 updates
2022-02-14 12:05:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint27.pt
2022-02-14 12:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint27.pt
2022-02-14 12:05:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint27.pt (epoch 27 @ 42410 updates, score 6.92) (writing took 18.145876810885966 seconds)
2022-02-14 12:05:54 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-14 12:05:54 | INFO | train | epoch 027 | loss 6.787 | nll_loss 5.019 | ppl 32.42 | wps 11649.1 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 42410 | lr 0.000153556 | gnorm 0.518 | loss_scale 8 | train_wall 8643 | gb_free 8.8 | wall 239176
2022-02-14 12:05:54 | INFO | fairseq.trainer | begin training epoch 28
2022-02-14 12:05:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 12:11:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 12:14:22 | INFO | train_inner | epoch 028:     91 / 1576 loss=6.734, nll_loss=4.959, ppl=31.09, wps=11111.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=42500, lr=0.000153393, gnorm=0.522, loss_scale=8, train_wall=549, gb_free=8.8, wall=239685
2022-02-14 12:23:41 | INFO | train_inner | epoch 028:    191 / 1576 loss=6.744, nll_loss=4.97, ppl=31.33, wps=11725.5, ups=0.18, wpb=65536, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.516, loss_scale=8, train_wall=548, gb_free=8.8, wall=240243
2022-02-14 12:33:00 | INFO | train_inner | epoch 028:    291 / 1576 loss=6.743, nll_loss=4.969, ppl=31.32, wps=11722.1, ups=0.18, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.522, loss_scale=8, train_wall=549, gb_free=8.8, wall=240803
2022-02-14 12:35:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 12:42:25 | INFO | train_inner | epoch 028:    392 / 1576 loss=6.754, nll_loss=4.982, ppl=31.59, wps=11608.4, ups=0.18, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=0.517, loss_scale=8, train_wall=554, gb_free=8.8, wall=241367
2022-02-14 12:51:44 | INFO | train_inner | epoch 028:    492 / 1576 loss=6.764, nll_loss=4.993, ppl=31.84, wps=11725.3, ups=0.18, wpb=65536, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.518, loss_scale=8, train_wall=548, gb_free=8.8, wall=241926
2022-02-14 13:00:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 13:01:08 | INFO | train_inner | epoch 028:    593 / 1576 loss=6.77, nll_loss=4.999, ppl=31.99, wps=11605.4, ups=0.18, wpb=65536, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.512, loss_scale=8, train_wall=554, gb_free=8.8, wall=242491
2022-02-14 13:01:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 13:10:33 | INFO | train_inner | epoch 028:    694 / 1576 loss=6.786, nll_loss=5.017, ppl=32.39, wps=11615.7, ups=0.18, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.52, loss_scale=4, train_wall=554, gb_free=8.8, wall=243055
2022-02-14 13:19:51 | INFO | train_inner | epoch 028:    794 / 1576 loss=6.784, nll_loss=5.015, ppl=32.34, wps=11727.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=43200, lr=0.000152145, gnorm=0.52, loss_scale=4, train_wall=548, gb_free=8.8, wall=243614
2022-02-14 13:29:10 | INFO | train_inner | epoch 028:    894 / 1576 loss=6.796, nll_loss=5.029, ppl=32.66, wps=11730.3, ups=0.18, wpb=65536, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.528, loss_scale=8, train_wall=548, gb_free=8.8, wall=244172
2022-02-14 13:38:29 | INFO | train_inner | epoch 028:    994 / 1576 loss=6.791, nll_loss=5.024, ppl=32.53, wps=11723.7, ups=0.18, wpb=65536, bsz=128, num_updates=43400, lr=0.000151794, gnorm=0.518, loss_scale=8, train_wall=548, gb_free=8.8, wall=244731
2022-02-14 13:47:48 | INFO | train_inner | epoch 028:   1094 / 1576 loss=6.79, nll_loss=5.023, ppl=32.51, wps=11721.4, ups=0.18, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.512, loss_scale=8, train_wall=549, gb_free=8.8, wall=245291
2022-02-14 13:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 13:57:13 | INFO | train_inner | epoch 028:   1195 / 1576 loss=6.807, nll_loss=5.042, ppl=32.93, wps=11605.8, ups=0.18, wpb=65536, bsz=128, num_updates=43600, lr=0.000151446, gnorm=0.514, loss_scale=8, train_wall=554, gb_free=8.8, wall=245855
2022-02-14 14:06:32 | INFO | train_inner | epoch 028:   1295 / 1576 loss=6.808, nll_loss=5.043, ppl=32.98, wps=11720.2, ups=0.18, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.519, loss_scale=8, train_wall=549, gb_free=8.8, wall=246414
2022-02-14 14:14:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 14:15:57 | INFO | train_inner | epoch 028:   1396 / 1576 loss=6.812, nll_loss=5.048, ppl=33.08, wps=11606.5, ups=0.18, wpb=65536, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.513, loss_scale=8, train_wall=554, gb_free=8.8, wall=246979
2022-02-14 14:25:16 | INFO | train_inner | epoch 028:   1496 / 1576 loss=6.793, nll_loss=5.026, ppl=32.59, wps=11723.2, ups=0.18, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.521, loss_scale=8, train_wall=549, gb_free=8.8, wall=247538
2022-02-14 14:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 14:32:45 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.915 | nll_loss 5.154 | ppl 35.6 | wps 32373.5 | wpb 1021.8 | bsz 2 | num_updates 43980 | best_loss 6.915
2022-02-14 14:32:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 43980 updates
2022-02-14 14:32:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint28.pt
2022-02-14 14:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint28.pt
2022-02-14 14:33:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint28.pt (epoch 28 @ 43980 updates, score 6.915) (writing took 27.414367839228362 seconds)
2022-02-14 14:33:12 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-14 14:33:12 | INFO | train | epoch 028 | loss 6.78 | nll_loss 5.012 | ppl 32.26 | wps 11634.4 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 43980 | lr 0.00015079 | gnorm 0.517 | loss_scale 8 | train_wall 8638 | gb_free 8.8 | wall 248015
2022-02-14 14:33:12 | INFO | fairseq.trainer | begin training epoch 29
2022-02-14 14:33:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 14:33:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 14:35:09 | INFO | train_inner | epoch 029:     21 / 1576 loss=6.802, nll_loss=5.036, ppl=32.81, wps=10942.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=44000, lr=0.000150756, gnorm=0.527, loss_scale=4, train_wall=549, gb_free=8.8, wall=248132
2022-02-14 14:44:28 | INFO | train_inner | epoch 029:    121 / 1576 loss=6.738, nll_loss=4.963, ppl=31.19, wps=11732.8, ups=0.18, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.51, loss_scale=4, train_wall=548, gb_free=8.8, wall=248690
2022-02-14 14:53:47 | INFO | train_inner | epoch 029:    221 / 1576 loss=6.732, nll_loss=4.957, ppl=31.06, wps=11728.2, ups=0.18, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.515, loss_scale=4, train_wall=548, gb_free=8.8, wall=249249
2022-02-14 15:03:06 | INFO | train_inner | epoch 029:    321 / 1576 loss=6.741, nll_loss=4.966, ppl=31.26, wps=11721.1, ups=0.18, wpb=65536, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.527, loss_scale=8, train_wall=549, gb_free=8.8, wall=249808
2022-02-14 15:12:25 | INFO | train_inner | epoch 029:    421 / 1576 loss=6.749, nll_loss=4.976, ppl=31.48, wps=11725.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=44400, lr=0.000150075, gnorm=0.516, loss_scale=8, train_wall=548, gb_free=8.8, wall=250367
2022-02-14 15:21:44 | INFO | train_inner | epoch 029:    521 / 1576 loss=6.754, nll_loss=4.982, ppl=31.6, wps=11724.2, ups=0.18, wpb=65536, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.51, loss_scale=16, train_wall=548, gb_free=8.8, wall=250926
2022-02-14 15:21:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 15:31:08 | INFO | train_inner | epoch 029:    622 / 1576 loss=6.771, nll_loss=5.001, ppl=32.02, wps=11609.2, ups=0.18, wpb=65536, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.521, loss_scale=8, train_wall=554, gb_free=8.8, wall=251491
2022-02-14 15:40:27 | INFO | train_inner | epoch 029:    722 / 1576 loss=6.77, nll_loss=5, ppl=32, wps=11721.6, ups=0.18, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.516, loss_scale=8, train_wall=549, gb_free=8.8, wall=252050
2022-02-14 15:48:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 15:49:52 | INFO | train_inner | epoch 029:    823 / 1576 loss=6.784, nll_loss=5.015, ppl=32.34, wps=11604.3, ups=0.18, wpb=65536, bsz=128, num_updates=44800, lr=0.000149404, gnorm=0.525, loss_scale=8, train_wall=554, gb_free=8.8, wall=252614
2022-02-14 15:59:11 | INFO | train_inner | epoch 029:    923 / 1576 loss=6.785, nll_loss=5.017, ppl=32.39, wps=11718.8, ups=0.18, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.512, loss_scale=8, train_wall=549, gb_free=8.8, wall=253174
2022-02-14 16:08:31 | INFO | train_inner | epoch 029:   1023 / 1576 loss=6.784, nll_loss=5.015, ppl=32.34, wps=11712.9, ups=0.18, wpb=65536, bsz=128, num_updates=45000, lr=0.000149071, gnorm=0.507, loss_scale=8, train_wall=549, gb_free=8.8, wall=253733
2022-02-14 16:12:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 16:17:56 | INFO | train_inner | epoch 029:   1124 / 1576 loss=6.798, nll_loss=5.031, ppl=32.7, wps=11606.8, ups=0.18, wpb=65536, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.528, loss_scale=8, train_wall=554, gb_free=8.8, wall=254298
2022-02-14 16:27:15 | INFO | train_inner | epoch 029:   1224 / 1576 loss=6.794, nll_loss=5.028, ppl=32.62, wps=11715.7, ups=0.18, wpb=65536, bsz=128, num_updates=45200, lr=0.000148741, gnorm=0.51, loss_scale=8, train_wall=549, gb_free=8.8, wall=254857
2022-02-14 16:36:34 | INFO | train_inner | epoch 029:   1324 / 1576 loss=6.8, nll_loss=5.034, ppl=32.76, wps=11718.9, ups=0.18, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.515, loss_scale=16, train_wall=549, gb_free=8.8, wall=255416
2022-02-14 16:39:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 16:45:59 | INFO | train_inner | epoch 029:   1425 / 1576 loss=6.799, nll_loss=5.033, ppl=32.75, wps=11609.7, ups=0.18, wpb=65536, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.52, loss_scale=8, train_wall=554, gb_free=8.8, wall=255981
2022-02-14 16:55:18 | INFO | train_inner | epoch 029:   1525 / 1576 loss=6.805, nll_loss=5.04, ppl=32.91, wps=11724.8, ups=0.18, wpb=65536, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.509, loss_scale=8, train_wall=548, gb_free=8.8, wall=256540
2022-02-14 16:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 17:00:05 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.91 | nll_loss 5.146 | ppl 35.42 | wps 32254.7 | wpb 1021.8 | bsz 2 | num_updates 45551 | best_loss 6.91
2022-02-14 17:00:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 45551 updates
2022-02-14 17:00:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint29.pt
2022-02-14 17:00:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint29.pt
2022-02-14 17:00:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint29.pt (epoch 29 @ 45551 updates, score 6.91) (writing took 27.54255624115467 seconds)
2022-02-14 17:00:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-14 17:00:32 | INFO | train | epoch 029 | loss 6.774 | nll_loss 5.005 | ppl 32.11 | wps 11640.5 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 45551 | lr 0.000148167 | gnorm 0.517 | loss_scale 8 | train_wall 8639 | gb_free 8.8 | wall 256854
2022-02-14 17:00:32 | INFO | fairseq.trainer | begin training epoch 30
2022-02-14 17:00:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 17:04:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 17:05:11 | INFO | train_inner | epoch 030:     50 / 1576 loss=6.764, nll_loss=4.993, ppl=31.84, wps=10946.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=45600, lr=0.000148087, gnorm=0.534, loss_scale=8, train_wall=548, gb_free=8.8, wall=257133
2022-02-14 17:14:30 | INFO | train_inner | epoch 030:    150 / 1576 loss=6.717, nll_loss=4.94, ppl=30.69, wps=11732.8, ups=0.18, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.521, loss_scale=8, train_wall=548, gb_free=8.8, wall=257692
2022-02-14 17:23:48 | INFO | train_inner | epoch 030:    250 / 1576 loss=6.736, nll_loss=4.961, ppl=31.15, wps=11737.5, ups=0.18, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.518, loss_scale=8, train_wall=548, gb_free=8.8, wall=258250
2022-02-14 17:33:06 | INFO | train_inner | epoch 030:    350 / 1576 loss=6.752, nll_loss=4.979, ppl=31.54, wps=11734, ups=0.18, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=0.516, loss_scale=16, train_wall=548, gb_free=8.8, wall=258809
2022-02-14 17:34:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 17:42:31 | INFO | train_inner | epoch 030:    451 / 1576 loss=6.755, nll_loss=4.982, ppl=31.61, wps=11619.5, ups=0.18, wpb=65536, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.525, loss_scale=8, train_wall=553, gb_free=8.8, wall=259373
2022-02-14 17:51:49 | INFO | train_inner | epoch 030:    551 / 1576 loss=6.757, nll_loss=4.985, ppl=31.66, wps=11723.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.519, loss_scale=8, train_wall=548, gb_free=8.8, wall=259932
2022-02-14 18:00:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:01:14 | INFO | train_inner | epoch 030:    652 / 1576 loss=6.766, nll_loss=4.995, ppl=31.9, wps=11603.7, ups=0.18, wpb=65536, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.513, loss_scale=8, train_wall=554, gb_free=8.8, wall=260497
2022-02-14 18:10:34 | INFO | train_inner | epoch 030:    752 / 1576 loss=6.761, nll_loss=4.989, ppl=31.77, wps=11716.5, ups=0.18, wpb=65536, bsz=128, num_updates=46300, lr=0.000146964, gnorm=0.518, loss_scale=8, train_wall=549, gb_free=8.8, wall=261056
2022-02-14 18:19:53 | INFO | train_inner | epoch 030:    852 / 1576 loss=6.762, nll_loss=4.99, ppl=31.79, wps=11720.9, ups=0.18, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.531, loss_scale=8, train_wall=549, gb_free=8.8, wall=261615
2022-02-14 18:26:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:29:18 | INFO | train_inner | epoch 030:    953 / 1576 loss=6.782, nll_loss=5.014, ppl=32.31, wps=11604.1, ups=0.18, wpb=65536, bsz=128, num_updates=46500, lr=0.000146647, gnorm=0.522, loss_scale=8, train_wall=554, gb_free=8.8, wall=262180
2022-02-14 18:38:37 | INFO | train_inner | epoch 030:   1053 / 1576 loss=6.785, nll_loss=5.017, ppl=32.37, wps=11719.2, ups=0.18, wpb=65536, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.527, loss_scale=8, train_wall=549, gb_free=8.8, wall=262739
2022-02-14 18:47:56 | INFO | train_inner | epoch 030:   1153 / 1576 loss=6.78, nll_loss=5.011, ppl=32.24, wps=11718.5, ups=0.18, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=0.518, loss_scale=8, train_wall=549, gb_free=8.8, wall=263298
2022-02-14 18:55:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:57:21 | INFO | train_inner | epoch 030:   1254 / 1576 loss=6.793, nll_loss=5.026, ppl=32.59, wps=11605.3, ups=0.18, wpb=65536, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.523, loss_scale=8, train_wall=554, gb_free=8.8, wall=263863
2022-02-14 19:06:40 | INFO | train_inner | epoch 030:   1354 / 1576 loss=6.797, nll_loss=5.031, ppl=32.69, wps=11720.5, ups=0.18, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.509, loss_scale=8, train_wall=549, gb_free=8.8, wall=264422
2022-02-14 19:15:59 | INFO | train_inner | epoch 030:   1454 / 1576 loss=6.787, nll_loss=5.019, ppl=32.43, wps=11717.9, ups=0.18, wpb=65536, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.518, loss_scale=8, train_wall=549, gb_free=8.8, wall=264981
2022-02-14 19:20:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 19:25:24 | INFO | train_inner | epoch 030:   1555 / 1576 loss=6.809, nll_loss=5.045, ppl=33, wps=11604.3, ups=0.18, wpb=65536, bsz=128, num_updates=47100, lr=0.00014571, gnorm=0.519, loss_scale=8, train_wall=554, gb_free=8.8, wall=265546
2022-02-14 19:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 19:27:23 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.907 | nll_loss 5.131 | ppl 35.03 | wps 32211.4 | wpb 1021.8 | bsz 2 | num_updates 47121 | best_loss 6.907
2022-02-14 19:27:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 47121 updates
2022-02-14 19:27:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint30.pt
2022-02-14 19:27:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint30.pt
2022-02-14 19:27:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint30.pt (epoch 30 @ 47121 updates, score 6.907) (writing took 32.68325221678242 seconds)
2022-02-14 19:27:56 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-14 19:27:56 | INFO | train | epoch 030 | loss 6.768 | nll_loss 4.998 | ppl 31.95 | wps 11627.6 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 47121 | lr 0.000145678 | gnorm 0.521 | loss_scale 8 | train_wall 8638 | gb_free 8.8 | wall 265698
2022-02-14 19:27:56 | INFO | fairseq.trainer | begin training epoch 31
2022-02-14 19:27:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 19:35:17 | INFO | train_inner | epoch 031:     79 / 1576 loss=6.73, nll_loss=4.954, ppl=30.99, wps=10948.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=47200, lr=0.000145556, gnorm=0.528, loss_scale=8, train_wall=543, gb_free=8.8, wall=266140
2022-02-14 19:44:37 | INFO | train_inner | epoch 031:    179 / 1576 loss=6.722, nll_loss=4.946, ppl=30.82, wps=11714.7, ups=0.18, wpb=65536, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.526, loss_scale=8, train_wall=549, gb_free=8.8, wall=266699
2022-02-14 19:53:56 | INFO | train_inner | epoch 031:    279 / 1576 loss=6.727, nll_loss=4.951, ppl=30.93, wps=11719.8, ups=0.18, wpb=65536, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.511, loss_scale=16, train_wall=549, gb_free=8.8, wall=267258
2022-02-14 20:03:15 | INFO | train_inner | epoch 031:    379 / 1576 loss=6.733, nll_loss=4.958, ppl=31.08, wps=11713.7, ups=0.18, wpb=65536, bsz=128, num_updates=47500, lr=0.000145095, gnorm=0.507, loss_scale=16, train_wall=549, gb_free=8.8, wall=267818
2022-02-14 20:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-14 20:09:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 20:12:46 | INFO | train_inner | epoch 031:    481 / 1576 loss=6.751, nll_loss=4.979, ppl=31.53, wps=11489.1, ups=0.18, wpb=65536, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.518, loss_scale=8, train_wall=560, gb_free=8.8, wall=268388
2022-02-14 20:22:05 | INFO | train_inner | epoch 031:    581 / 1576 loss=6.757, nll_loss=4.985, ppl=31.68, wps=11717, ups=0.18, wpb=65536, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.532, loss_scale=8, train_wall=549, gb_free=8.8, wall=268947
2022-02-14 20:31:24 | INFO | train_inner | epoch 031:    681 / 1576 loss=6.75, nll_loss=4.977, ppl=31.49, wps=11717.1, ups=0.18, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.525, loss_scale=8, train_wall=549, gb_free=8.8, wall=269507
2022-02-14 20:40:44 | INFO | train_inner | epoch 031:    781 / 1576 loss=6.78, nll_loss=5.012, ppl=32.26, wps=11714.5, ups=0.18, wpb=65536, bsz=128, num_updates=47900, lr=0.000144488, gnorm=0.509, loss_scale=16, train_wall=549, gb_free=8.8, wall=270066
2022-02-14 20:41:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 20:50:09 | INFO | train_inner | epoch 031:    882 / 1576 loss=6.772, nll_loss=5.002, ppl=32.05, wps=11604.5, ups=0.18, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.522, loss_scale=8, train_wall=554, gb_free=8.8, wall=270631
2022-02-14 20:59:28 | INFO | train_inner | epoch 031:    982 / 1576 loss=6.768, nll_loss=4.997, ppl=31.94, wps=11720.2, ups=0.18, wpb=65536, bsz=128, num_updates=48100, lr=0.000144187, gnorm=0.528, loss_scale=8, train_wall=549, gb_free=8.8, wall=271190
2022-02-14 21:08:47 | INFO | train_inner | epoch 031:   1082 / 1576 loss=6.782, nll_loss=5.013, ppl=32.29, wps=11717.2, ups=0.18, wpb=65536, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.52, loss_scale=16, train_wall=549, gb_free=8.8, wall=271749
2022-02-14 21:10:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 21:18:12 | INFO | train_inner | epoch 031:   1183 / 1576 loss=6.786, nll_loss=5.018, ppl=32.4, wps=11601.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=48300, lr=0.000143889, gnorm=0.524, loss_scale=8, train_wall=554, gb_free=8.8, wall=272314
2022-02-14 21:27:31 | INFO | train_inner | epoch 031:   1283 / 1576 loss=6.783, nll_loss=5.014, ppl=32.32, wps=11716.8, ups=0.18, wpb=65536, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.517, loss_scale=8, train_wall=549, gb_free=8.8, wall=272874
2022-02-14 21:36:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 21:36:56 | INFO | train_inner | epoch 031:   1384 / 1576 loss=6.779, nll_loss=5.01, ppl=32.22, wps=11599.3, ups=0.18, wpb=65536, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.518, loss_scale=8, train_wall=554, gb_free=8.8, wall=273439
2022-02-14 21:46:16 | INFO | train_inner | epoch 031:   1484 / 1576 loss=6.787, nll_loss=5.019, ppl=32.42, wps=11714.8, ups=0.18, wpb=65536, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.514, loss_scale=8, train_wall=549, gb_free=8.8, wall=273998
2022-02-14 21:54:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 21:54:52 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.904 | nll_loss 5.128 | ppl 34.97 | wps 32356.1 | wpb 1021.8 | bsz 2 | num_updates 48692 | best_loss 6.904
2022-02-14 21:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 48692 updates
2022-02-14 21:54:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint31.pt
2022-02-14 21:55:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint31.pt
2022-02-14 21:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint31.pt (epoch 31 @ 48692 updates, score 6.904) (writing took 27.376247309613973 seconds)
2022-02-14 21:55:20 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-14 21:55:20 | INFO | train | epoch 031 | loss 6.762 | nll_loss 4.991 | ppl 31.8 | wps 11635.3 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 48692 | lr 0.000143308 | gnorm 0.52 | loss_scale 8 | train_wall 8644 | gb_free 8.8 | wall 274542
2022-02-14 21:55:20 | INFO | fairseq.trainer | begin training epoch 32
2022-02-14 21:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 21:56:05 | INFO | train_inner | epoch 032:      8 / 1576 loss=6.792, nll_loss=5.025, ppl=32.57, wps=11032.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=48700, lr=0.000143296, gnorm=0.526, loss_scale=8, train_wall=544, gb_free=8.8, wall=274587
2022-02-14 22:03:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 22:05:29 | INFO | train_inner | epoch 032:    109 / 1576 loss=6.709, nll_loss=4.931, ppl=30.5, wps=11610.4, ups=0.18, wpb=65536, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.519, loss_scale=8, train_wall=554, gb_free=8.8, wall=275151
2022-02-14 22:14:48 | INFO | train_inner | epoch 032:    209 / 1576 loss=6.714, nll_loss=4.936, ppl=30.61, wps=11719.8, ups=0.18, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.512, loss_scale=8, train_wall=549, gb_free=8.8, wall=275711
2022-02-14 22:24:08 | INFO | train_inner | epoch 032:    309 / 1576 loss=6.72, nll_loss=4.943, ppl=30.76, wps=11716.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.521, loss_scale=8, train_wall=549, gb_free=8.8, wall=276270
2022-02-14 22:27:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 22:33:32 | INFO | train_inner | epoch 032:    410 / 1576 loss=6.735, nll_loss=4.96, ppl=31.13, wps=11602, ups=0.18, wpb=65536, bsz=128, num_updates=49100, lr=0.000142712, gnorm=0.532, loss_scale=8, train_wall=554, gb_free=8.8, wall=276835
2022-02-14 22:42:52 | INFO | train_inner | epoch 032:    510 / 1576 loss=6.738, nll_loss=4.963, ppl=31.2, wps=11709.6, ups=0.18, wpb=65536, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.523, loss_scale=8, train_wall=549, gb_free=8.8, wall=277394
2022-02-14 22:52:11 | INFO | train_inner | epoch 032:    610 / 1576 loss=6.747, nll_loss=4.973, ppl=31.41, wps=11720.8, ups=0.18, wpb=65536, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.521, loss_scale=16, train_wall=549, gb_free=8.8, wall=277954
2022-02-14 22:55:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 23:01:36 | INFO | train_inner | epoch 032:    711 / 1576 loss=6.758, nll_loss=4.986, ppl=31.69, wps=11605.6, ups=0.18, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.525, loss_scale=8, train_wall=554, gb_free=8.8, wall=278518
2022-02-14 23:10:55 | INFO | train_inner | epoch 032:    811 / 1576 loss=6.76, nll_loss=4.989, ppl=31.75, wps=11719.2, ups=0.18, wpb=65536, bsz=128, num_updates=49500, lr=0.000142134, gnorm=0.522, loss_scale=8, train_wall=549, gb_free=8.8, wall=279077
2022-02-14 23:20:14 | INFO | train_inner | epoch 032:    911 / 1576 loss=6.764, nll_loss=4.993, ppl=31.85, wps=11721.9, ups=0.18, wpb=65536, bsz=128, num_updates=49600, lr=0.00014199, gnorm=0.518, loss_scale=16, train_wall=549, gb_free=8.8, wall=279637
2022-02-14 23:23:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 23:29:39 | INFO | train_inner | epoch 032:   1012 / 1576 loss=6.772, nll_loss=5.003, ppl=32.06, wps=11602.8, ups=0.18, wpb=65536, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.531, loss_scale=8, train_wall=554, gb_free=8.8, wall=280201
2022-02-14 23:38:58 | INFO | train_inner | epoch 032:   1112 / 1576 loss=6.772, nll_loss=5.003, ppl=32.06, wps=11720.7, ups=0.18, wpb=65536, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.523, loss_scale=8, train_wall=549, gb_free=8.8, wall=280761
2022-02-14 23:48:17 | INFO | train_inner | epoch 032:   1212 / 1576 loss=6.781, nll_loss=5.012, ppl=32.27, wps=11721.6, ups=0.18, wpb=65536, bsz=128, num_updates=49900, lr=0.000141563, gnorm=0.524, loss_scale=16, train_wall=549, gb_free=8.8, wall=281320
2022-02-14 23:48:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 23:57:42 | INFO | train_inner | epoch 032:   1313 / 1576 loss=6.781, nll_loss=5.013, ppl=32.28, wps=11608.2, ups=0.18, wpb=65536, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.524, loss_scale=8, train_wall=554, gb_free=8.8, wall=281884
2022-02-14 23:57:42 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2022-02-14 23:57:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 23:57:49 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.906 | nll_loss 5.131 | ppl 35.04 | wps 32359 | wpb 1021.8 | bsz 2 | num_updates 50000 | best_loss 6.904
2022-02-14 23:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 50000 updates
2022-02-14 23:57:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-14 23:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt
2022-02-14 23:57:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#2/checkpoint_last.pt (epoch 32 @ 50000 updates, score 6.906) (writing took 8.466618542093784 seconds)
2022-02-14 23:57:57 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-14 23:57:57 | INFO | train | epoch 032 | loss 6.75 | nll_loss 4.977 | ppl 31.49 | wps 11650.9 | ups 0.18 | wpb 65535.7 | bsz 128 | num_updates 50000 | lr 0.000141421 | gnorm 0.523 | loss_scale 8 | train_wall 7204 | gb_free 8.8 | wall 281899
2022-02-14 23:57:57 | INFO | fairseq_cli.train | done training in 281895.6 seconds
