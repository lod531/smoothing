Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 207526052: <w103_fp16_size_0.125_jelinek_0.0_0.1_0.9_#3> in cluster <euler> Exited

Job <w103_fp16_size_0.125_jelinek_0.0_0.1_0.9_#3> was submitted from host <eu-login-27> by user <andriusb> in cluster <euler> at Tue Mar  8 08:11:00 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Tue Mar  8 08:11:15 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar  8 08:11:15 2022
Terminated at Tue Mar  8 16:28:06 2022
Results reported at Tue Mar  8 16:28:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.0, 0.1, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --seed 1321673 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   29792.31 sec.
    Max Memory :                                 6776 MB
    Average Memory :                             4034.30 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13224.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                16
    Run time :                                   29810 sec.
    Turnaround time :                            29826 sec.

The output (if any) follows:

2022-03-08 08:11:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321673, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321673, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.0, 0.1, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-08 08:11:22 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-08 08:11:25 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
Calculating frequency stats:
  0%|          | 0/225169 [00:00<?, ?it/s]  0%|          | 661/225169 [00:00<00:33, 6603.30it/s]  1%|          | 1322/225169 [00:00<00:38, 5788.29it/s]  1%|          | 1908/225169 [00:00<00:39, 5627.94it/s]  1%|          | 2474/225169 [00:00<00:40, 5552.12it/s]  1%|▏         | 3196/225169 [00:00<00:36, 6116.00it/s]  2%|▏         | 3812/225169 [00:00<00:36, 6011.41it/s]  2%|▏         | 4519/225169 [00:00<00:34, 6341.68it/s]  2%|▏         | 5235/225169 [00:00<00:33, 6588.87it/s]  3%|▎         | 5922/225169 [00:00<00:32, 6670.28it/s]  3%|▎         | 6591/225169 [00:01<00:36, 6046.82it/s]  3%|▎         | 7208/225169 [00:01<00:35, 6075.47it/s]  3%|▎         | 7824/225169 [00:01<00:36, 6036.61it/s]  4%|▎         | 8434/225169 [00:01<00:37, 5841.38it/s]  4%|▍         | 9087/225169 [00:01<00:35, 6022.11it/s]  4%|▍         | 9694/225169 [00:01<00:36, 5895.94it/s]  5%|▍         | 10327/225169 [00:01<00:35, 6017.65it/s]  5%|▍         | 10932/225169 [00:01<00:36, 5925.23it/s]  5%|▌         | 11528/225169 [00:01<00:36, 5933.21it/s]  5%|▌         | 12153/225169 [00:02<00:35, 6024.12it/s]  6%|▌         | 12757/225169 [00:02<00:36, 5867.81it/s]  6%|▌         | 13382/225169 [00:02<00:35, 5973.32it/s]  6%|▌         | 14037/225169 [00:02<00:34, 6139.09it/s]  7%|▋         | 14653/225169 [00:02<00:34, 6132.90it/s]  7%|▋         | 15313/225169 [00:02<00:33, 6268.54it/s]  7%|▋         | 15941/225169 [00:02<00:35, 5920.71it/s]  7%|▋         | 16538/225169 [00:02<00:35, 5847.29it/s]  8%|▊         | 17155/225169 [00:02<00:35, 5933.24it/s]  8%|▊         | 17751/225169 [00:02<00:34, 5928.97it/s]  8%|▊         | 18346/225169 [00:03<00:34, 5919.81it/s]  8%|▊         | 19053/225169 [00:03<00:32, 6258.46it/s]  9%|▊         | 19683/225169 [00:03<00:32, 6258.18it/s]  9%|▉         | 20310/225169 [00:03<00:34, 5920.56it/s]  9%|▉         | 20942/225169 [00:03<00:33, 6033.08it/s] 10%|▉         | 21549/225169 [00:03<00:35, 5710.36it/s] 10%|▉         | 22168/225169 [00:03<00:34, 5844.01it/s] 10%|█         | 22849/225169 [00:03<00:33, 6114.54it/s] 10%|█         | 23488/225169 [00:03<00:32, 6192.75it/s] 11%|█         | 24226/225169 [00:03<00:30, 6527.10it/s] 11%|█         | 24932/225169 [00:04<00:29, 6678.68it/s] 11%|█▏        | 25603/225169 [00:04<00:30, 6638.58it/s] 12%|█▏        | 26269/225169 [00:04<00:31, 6237.18it/s] 12%|█▏        | 26899/225169 [00:04<00:33, 5999.11it/s] 12%|█▏        | 27504/225169 [00:04<00:34, 5769.61it/s] 12%|█▏        | 28140/225169 [00:04<00:33, 5927.04it/s] 13%|█▎        | 28824/225169 [00:04<00:31, 6179.43it/s] 13%|█▎        | 29447/225169 [00:04<00:33, 5861.11it/s] 13%|█▎        | 30119/225169 [00:04<00:32, 6094.00it/s] 14%|█▎        | 30734/225169 [00:05<00:33, 5795.39it/s] 14%|█▍        | 31320/225169 [00:05<00:33, 5744.91it/s] 14%|█▍        | 31899/225169 [00:05<00:33, 5712.26it/s] 14%|█▍        | 32473/225169 [00:05<00:33, 5714.04it/s] 15%|█▍        | 33047/225169 [00:05<00:34, 5574.99it/s] 15%|█▍        | 33622/225169 [00:05<00:34, 5624.53it/s] 15%|█▌        | 34207/225169 [00:05<00:33, 5685.34it/s] 16%|█▌        | 34916/225169 [00:05<00:31, 6091.82it/s] 16%|█▌        | 35527/225169 [00:05<00:32, 5897.90it/s] 16%|█▌        | 36125/225169 [00:06<00:31, 5919.39it/s] 16%|█▋        | 36729/225169 [00:06<00:31, 5954.11it/s] 17%|█▋        | 37326/225169 [00:06<00:33, 5686.85it/s] 17%|█▋        | 37898/225169 [00:06<00:33, 5594.85it/s] 17%|█▋        | 38480/225169 [00:06<00:32, 5659.00it/s] 17%|█▋        | 39054/225169 [00:06<00:32, 5681.38it/s] 18%|█▊        | 39648/225169 [00:06<00:32, 5756.87it/s] 18%|█▊        | 40277/225169 [00:06<00:31, 5912.91it/s] 18%|█▊        | 40943/225169 [00:06<00:30, 6128.60it/s] 18%|█▊        | 41557/225169 [00:06<00:30, 5954.17it/s] 19%|█▊        | 42155/225169 [00:07<00:32, 5661.44it/s] 19%|█▉        | 42725/225169 [00:07<00:32, 5638.97it/s] 19%|█▉        | 43292/225169 [00:07<00:32, 5577.52it/s] 20%|█▉        | 43962/225169 [00:07<00:30, 5898.61it/s] 20%|█▉        | 44605/225169 [00:07<00:29, 6051.36it/s] 20%|██        | 45213/225169 [00:07<00:30, 5883.80it/s] 20%|██        | 45881/225169 [00:07<00:29, 6112.47it/s] 21%|██        | 46555/225169 [00:07<00:28, 6295.13it/s] 21%|██        | 47280/225169 [00:07<00:27, 6574.63it/s] 21%|██▏       | 48133/225169 [00:07<00:24, 7150.97it/s] 22%|██▏       | 48851/225169 [00:08<00:25, 6847.37it/s] 22%|██▏       | 49585/225169 [00:08<00:25, 6985.63it/s] 22%|██▏       | 50287/225169 [00:08<00:27, 6411.81it/s] 23%|██▎       | 50939/225169 [00:08<00:28, 6216.74it/s] 23%|██▎       | 51569/225169 [00:08<00:28, 6111.55it/s] 23%|██▎       | 52437/225169 [00:08<00:25, 6827.07it/s] 24%|██▎       | 53129/225169 [00:08<00:26, 6495.12it/s] 24%|██▍       | 53788/225169 [00:08<00:26, 6373.48it/s] 24%|██▍       | 54432/225169 [00:09<00:28, 5989.37it/s] 24%|██▍       | 55038/225169 [00:09<00:28, 5946.23it/s] 25%|██▍       | 55737/225169 [00:09<00:27, 6235.65it/s] 25%|██▌       | 56367/225169 [00:09<00:27, 6135.01it/s] 25%|██▌       | 56985/225169 [00:09<00:29, 5787.39it/s] 26%|██▌       | 57570/225169 [00:09<00:29, 5769.86it/s] 26%|██▌       | 58179/225169 [00:09<00:28, 5856.85it/s] 26%|██▌       | 58894/225169 [00:09<00:26, 6225.91it/s] 26%|██▋       | 59521/225169 [00:09<00:27, 5916.92it/s] 27%|██▋       | 60118/225169 [00:09<00:28, 5886.46it/s] 27%|██▋       | 60781/225169 [00:10<00:26, 6098.85it/s] 27%|██▋       | 61513/225169 [00:10<00:25, 6450.34it/s] 28%|██▊       | 62162/225169 [00:10<00:27, 6003.96it/s] 28%|██▊       | 62771/225169 [00:10<00:26, 6015.52it/s] 28%|██▊       | 63379/225169 [00:10<00:27, 5870.57it/s] 28%|██▊       | 63999/225169 [00:10<00:27, 5962.62it/s] 29%|██▊       | 64599/225169 [00:10<00:27, 5914.01it/s] 29%|██▉       | 65193/225169 [00:10<00:27, 5916.27it/s] 29%|██▉       | 65825/225169 [00:10<00:26, 6031.99it/s] 30%|██▉       | 66430/225169 [00:11<00:26, 5978.68it/s] 30%|██▉       | 67056/225169 [00:11<00:26, 6055.63it/s] 30%|███       | 67663/225169 [00:11<00:26, 5868.01it/s] 30%|███       | 68252/225169 [00:11<00:27, 5722.45it/s] 31%|███       | 68985/225169 [00:11<00:25, 6181.88it/s] 31%|███       | 69611/225169 [00:11<00:25, 6201.12it/s] 31%|███       | 70241/225169 [00:11<00:24, 6217.83it/s] 31%|███▏      | 70865/225169 [00:11<00:25, 5950.25it/s] 32%|███▏      | 71494/225169 [00:11<00:25, 6045.24it/s] 32%|███▏      | 72124/225169 [00:11<00:25, 6114.86it/s] 32%|███▏      | 72808/225169 [00:12<00:24, 6324.45it/s] 33%|███▎      | 73542/225169 [00:12<00:22, 6622.68it/s] 33%|███▎      | 74337/225169 [00:12<00:21, 7015.91it/s] 33%|███▎      | 75041/225169 [00:12<00:22, 6727.72it/s] 34%|███▎      | 75718/225169 [00:12<00:23, 6327.62it/s] 34%|███▍      | 76408/225169 [00:12<00:22, 6481.90it/s] 34%|███▍      | 77062/225169 [00:12<00:23, 6251.68it/s] 35%|███▍      | 77721/225169 [00:12<00:23, 6346.28it/s] 35%|███▍      | 78360/225169 [00:12<00:24, 6102.60it/s] 35%|███▌      | 78975/225169 [00:13<00:25, 5746.73it/s] 35%|███▌      | 79556/225169 [00:13<00:25, 5691.40it/s] 36%|███▌      | 80129/225169 [00:13<00:25, 5643.50it/s] 36%|███▌      | 80752/225169 [00:13<00:24, 5809.27it/s] 36%|███▌      | 81447/225169 [00:13<00:23, 6137.13it/s] 36%|███▋      | 82064/225169 [00:13<00:23, 5964.18it/s] 37%|███▋      | 82703/225169 [00:13<00:23, 6084.00it/s] 37%|███▋      | 83358/225169 [00:13<00:22, 6216.45it/s] 37%|███▋      | 84006/225169 [00:13<00:22, 6290.42it/s] 38%|███▊      | 84637/225169 [00:13<00:22, 6157.91it/s] 38%|███▊      | 85255/225169 [00:14<00:23, 5924.48it/s] 38%|███▊      | 85929/225169 [00:14<00:22, 6152.88it/s] 38%|███▊      | 86548/225169 [00:14<00:22, 6066.62it/s] 39%|███▊      | 87236/225169 [00:14<00:21, 6302.19it/s] 39%|███▉      | 87869/225169 [00:14<00:21, 6281.39it/s] 39%|███▉      | 88499/225169 [00:14<00:22, 6053.29it/s] 40%|███▉      | 89107/225169 [00:14<00:23, 5876.58it/s] 40%|███▉      | 89732/225169 [00:14<00:22, 5980.60it/s] 40%|████      | 90333/225169 [00:14<00:23, 5724.95it/s] 40%|████      | 90909/225169 [00:15<00:23, 5631.51it/s] 41%|████      | 91623/225169 [00:15<00:22, 6057.56it/s] 41%|████      | 92233/225169 [00:15<00:22, 6015.96it/s] 41%|████      | 92844/225169 [00:15<00:21, 6041.45it/s] 42%|████▏     | 93451/225169 [00:15<00:22, 5882.07it/s] 42%|████▏     | 94042/225169 [00:15<00:22, 5827.85it/s] 42%|████▏     | 94738/225169 [00:15<00:21, 6153.17it/s] 42%|████▏     | 95356/225169 [00:15<00:21, 6047.12it/s] 43%|████▎     | 95963/225169 [00:15<00:21, 5978.69it/s] 43%|████▎     | 96830/225169 [00:15<00:18, 6761.34it/s] 43%|████▎     | 97510/225169 [00:16<00:20, 6375.20it/s] 44%|████▎     | 98154/225169 [00:16<00:20, 6228.16it/s] 44%|████▍     | 98782/225169 [00:16<00:21, 6007.01it/s] 44%|████▍     | 99387/225169 [00:16<00:21, 5838.16it/s] 44%|████▍     | 99996/225169 [00:16<00:21, 5907.30it/s] 45%|████▍     | 100590/225169 [00:16<00:21, 5685.65it/s] 45%|████▍     | 101255/225169 [00:16<00:20, 5955.32it/s] 45%|████▌     | 101855/225169 [00:16<00:20, 5957.32it/s] 46%|████▌     | 102454/225169 [00:16<00:20, 5937.05it/s] 46%|████▌     | 103087/225169 [00:17<00:20, 6048.05it/s] 46%|████▌     | 103694/225169 [00:17<00:20, 5905.12it/s] 46%|████▋     | 104287/225169 [00:17<00:21, 5755.35it/s] 47%|████▋     | 104947/225169 [00:17<00:20, 5993.81it/s] 47%|████▋     | 105549/225169 [00:17<00:19, 5996.32it/s] 47%|████▋     | 106164/225169 [00:17<00:19, 6036.85it/s] 47%|████▋     | 106769/225169 [00:17<00:20, 5902.82it/s] 48%|████▊     | 107361/225169 [00:17<00:20, 5762.79it/s] 48%|████▊     | 108001/225169 [00:17<00:19, 5943.25it/s] 48%|████▊     | 108597/225169 [00:17<00:20, 5752.01it/s] 48%|████▊     | 109175/225169 [00:18<00:20, 5681.29it/s] 49%|████▊     | 109763/225169 [00:18<00:20, 5735.05it/s] 49%|████▉     | 110404/225169 [00:18<00:19, 5919.93it/s] 49%|████▉     | 111130/225169 [00:18<00:18, 6313.20it/s] 50%|████▉     | 111764/225169 [00:18<00:18, 6083.92it/s] 50%|████▉     | 112376/225169 [00:18<00:18, 6015.02it/s] 50%|█████     | 113011/225169 [00:18<00:18, 6110.71it/s] 50%|█████     | 113624/225169 [00:18<00:19, 5792.22it/s] 51%|█████     | 114281/225169 [00:18<00:18, 6004.50it/s] 51%|█████     | 114886/225169 [00:19<00:18, 5925.88it/s] 51%|█████▏    | 115482/225169 [00:19<00:18, 5813.35it/s] 52%|█████▏    | 116066/225169 [00:19<00:19, 5724.77it/s] 52%|█████▏    | 116640/225169 [00:19<00:19, 5644.34it/s] 52%|█████▏    | 117206/225169 [00:19<00:19, 5610.16it/s] 52%|█████▏    | 117772/225169 [00:19<00:19, 5622.69it/s] 53%|█████▎    | 118449/225169 [00:19<00:17, 5957.21it/s] 53%|█████▎    | 119046/225169 [00:19<00:18, 5825.45it/s] 53%|█████▎    | 119631/225169 [00:19<00:18, 5826.51it/s] 53%|█████▎    | 120390/225169 [00:19<00:16, 6338.75it/s] 54%|█████▎    | 121026/225169 [00:20<00:16, 6156.07it/s] 54%|█████▍    | 121644/225169 [00:20<00:17, 5840.65it/s] 54%|█████▍    | 122284/225169 [00:20<00:17, 5997.40it/s] 55%|█████▍    | 122888/225169 [00:20<00:17, 5979.90it/s] 55%|█████▍    | 123489/225169 [00:20<00:17, 5808.76it/s] 55%|█████▌    | 124081/225169 [00:20<00:17, 5832.29it/s] 55%|█████▌    | 124667/225169 [00:20<00:17, 5806.89it/s] 56%|█████▌    | 125274/225169 [00:20<00:16, 5881.60it/s] 56%|█████▌    | 125943/225169 [00:20<00:16, 6114.00it/s] 56%|█████▌    | 126583/225169 [00:20<00:15, 6195.10it/s] 56%|█████▋    | 127210/225169 [00:21<00:15, 6207.06it/s] 57%|█████▋    | 127832/225169 [00:21<00:15, 6121.57it/s] 57%|█████▋    | 128445/225169 [00:21<00:16, 5931.26it/s] 57%|█████▋    | 129077/225169 [00:21<00:15, 6042.82it/s] 58%|█████▊    | 129685/225169 [00:21<00:15, 6048.05it/s] 58%|█████▊    | 130291/225169 [00:21<00:16, 5773.22it/s] 58%|█████▊    | 130872/225169 [00:21<00:16, 5726.27it/s] 58%|█████▊    | 131472/225169 [00:21<00:16, 5793.52it/s] 59%|█████▊    | 132053/225169 [00:21<00:17, 5357.13it/s] 59%|█████▉    | 132728/225169 [00:22<00:16, 5738.09it/s] 59%|█████▉    | 133356/225169 [00:22<00:15, 5882.71it/s] 60%|█████▉    | 134009/225169 [00:22<00:15, 6060.89it/s] 60%|█████▉    | 134695/225169 [00:22<00:14, 6290.77it/s] 60%|██████    | 135435/225169 [00:22<00:13, 6608.01it/s] 60%|██████    | 136100/225169 [00:22<00:13, 6449.60it/s] 61%|██████    | 136811/225169 [00:22<00:13, 6640.59it/s] 61%|██████    | 137506/225169 [00:22<00:13, 6724.64it/s] 61%|██████▏   | 138181/225169 [00:22<00:13, 6276.84it/s] 62%|██████▏   | 138831/225169 [00:22<00:13, 6333.42it/s] 62%|██████▏   | 139470/225169 [00:23<00:13, 6181.57it/s] 62%|██████▏   | 140093/225169 [00:23<00:13, 6130.00it/s] 62%|██████▏   | 140709/225169 [00:23<00:14, 5967.36it/s] 63%|██████▎   | 141309/225169 [00:23<00:14, 5724.31it/s] 63%|██████▎   | 141933/225169 [00:23<00:14, 5867.40it/s] 63%|██████▎   | 142530/225169 [00:23<00:14, 5891.87it/s] 64%|██████▎   | 143122/225169 [00:23<00:14, 5578.92it/s] 64%|██████▍   | 143924/225169 [00:23<00:12, 6263.90it/s] 64%|██████▍   | 144558/225169 [00:23<00:13, 6192.10it/s] 65%|██████▍   | 145237/225169 [00:24<00:12, 6356.02it/s] 65%|██████▍   | 145904/225169 [00:24<00:12, 6445.30it/s] 65%|██████▌   | 146552/225169 [00:24<00:12, 6206.07it/s] 65%|██████▌   | 147177/225169 [00:24<00:12, 6053.10it/s] 66%|██████▌   | 147786/225169 [00:24<00:13, 5686.59it/s] 66%|██████▌   | 148382/225169 [00:24<00:13, 5762.15it/s] 66%|██████▌   | 149053/225169 [00:24<00:12, 6027.66it/s] 66%|██████▋   | 149661/225169 [00:24<00:13, 5757.26it/s] 67%|██████▋   | 150242/225169 [00:24<00:13, 5677.89it/s] 67%|██████▋   | 150831/225169 [00:25<00:12, 5733.90it/s] 67%|██████▋   | 151437/225169 [00:25<00:12, 5824.44it/s] 68%|██████▊   | 152022/225169 [00:25<00:12, 5726.35it/s] 68%|██████▊   | 152597/225169 [00:25<00:12, 5695.60it/s] 68%|██████▊   | 153331/225169 [00:25<00:11, 6166.98it/s] 68%|██████▊   | 153950/225169 [00:25<00:12, 5798.04it/s] 69%|██████▊   | 154536/225169 [00:25<00:12, 5778.97it/s] 69%|██████▉   | 155164/225169 [00:25<00:11, 5917.55it/s] 69%|██████▉   | 155795/225169 [00:25<00:11, 6025.86it/s] 69%|██████▉   | 156434/225169 [00:25<00:11, 6127.13it/s] 70%|██████▉   | 157064/225169 [00:26<00:11, 6174.72it/s] 70%|███████   | 157707/225169 [00:26<00:10, 6250.02it/s] 70%|███████   | 158408/225169 [00:26<00:10, 6473.84it/s] 71%|███████   | 159057/225169 [00:26<00:11, 5988.37it/s] 71%|███████   | 159671/225169 [00:26<00:10, 6030.41it/s] 71%|███████   | 160296/225169 [00:26<00:10, 6080.48it/s] 71%|███████▏  | 160974/225169 [00:26<00:10, 6283.08it/s] 72%|███████▏  | 161606/225169 [00:26<00:10, 6030.90it/s] 72%|███████▏  | 162234/225169 [00:26<00:10, 6098.94it/s] 72%|███████▏  | 162878/225169 [00:27<00:10, 6193.62it/s] 73%|███████▎  | 163500/225169 [00:27<00:10, 6091.84it/s] 73%|███████▎  | 164112/225169 [00:27<00:10, 5887.33it/s] 73%|███████▎  | 164779/225169 [00:27<00:09, 6110.58it/s] 73%|███████▎  | 165452/225169 [00:27<00:09, 6286.45it/s] 74%|███████▍  | 166084/225169 [00:27<00:09, 6289.52it/s] 74%|███████▍  | 166783/225169 [00:27<00:08, 6493.73it/s] 74%|███████▍  | 167434/225169 [00:27<00:09, 6337.25it/s] 75%|███████▍  | 168070/225169 [00:27<00:09, 5914.44it/s] 75%|███████▍  | 168773/225169 [00:27<00:09, 6224.74it/s] 75%|███████▌  | 169403/225169 [00:28<00:09, 6058.90it/s] 76%|███████▌  | 170014/225169 [00:28<00:09, 5880.65it/s] 76%|███████▌  | 170606/225169 [00:28<00:09, 5715.14it/s] 76%|███████▌  | 171181/225169 [00:28<00:09, 5650.61it/s] 76%|███████▋  | 171836/225169 [00:28<00:09, 5905.59it/s] 77%|███████▋  | 172430/225169 [00:28<00:09, 5788.96it/s] 77%|███████▋  | 173035/225169 [00:28<00:08, 5862.65it/s] 77%|███████▋  | 173626/225169 [00:28<00:08, 5875.80it/s] 77%|███████▋  | 174215/225169 [00:28<00:08, 5786.32it/s] 78%|███████▊  | 174948/225169 [00:29<00:08, 6230.37it/s] 78%|███████▊  | 175573/225169 [00:29<00:08, 5856.21it/s] 78%|███████▊  | 176165/225169 [00:29<00:08, 5734.96it/s] 78%|███████▊  | 176750/225169 [00:29<00:08, 5761.79it/s] 79%|███████▉  | 177329/225169 [00:29<00:08, 5766.95it/s] 79%|███████▉  | 177908/225169 [00:29<00:08, 5383.88it/s] 79%|███████▉  | 178559/225169 [00:29<00:08, 5692.87it/s] 80%|███████▉  | 179156/225169 [00:29<00:07, 5771.60it/s] 80%|███████▉  | 179738/225169 [00:29<00:07, 5710.48it/s] 80%|████████  | 180346/225169 [00:29<00:07, 5817.29it/s] 80%|████████  | 180931/225169 [00:30<00:07, 5740.87it/s] 81%|████████  | 181508/225169 [00:30<00:07, 5658.29it/s] 81%|████████  | 182107/225169 [00:30<00:07, 5747.82it/s] 81%|████████  | 182687/225169 [00:30<00:07, 5762.92it/s] 81%|████████▏ | 183265/225169 [00:30<00:07, 5691.04it/s] 82%|████████▏ | 183835/225169 [00:30<00:07, 5369.02it/s] 82%|████████▏ | 184405/225169 [00:30<00:07, 5459.23it/s] 82%|████████▏ | 185037/225169 [00:30<00:07, 5701.27it/s] 82%|████████▏ | 185611/225169 [00:30<00:07, 5408.56it/s] 83%|████████▎ | 186211/225169 [00:31<00:06, 5574.28it/s] 83%|████████▎ | 186773/225169 [00:31<00:06, 5572.99it/s] 83%|████████▎ | 187398/225169 [00:31<00:06, 5768.62it/s] 83%|████████▎ | 187978/225169 [00:31<00:06, 5718.65it/s] 84%|████████▍ | 188697/225169 [00:31<00:05, 6149.88it/s] 84%|████████▍ | 189315/225169 [00:31<00:06, 5890.36it/s] 84%|████████▍ | 189973/225169 [00:31<00:05, 6081.89it/s] 85%|████████▍ | 190601/225169 [00:31<00:05, 6136.61it/s] 85%|████████▍ | 191218/225169 [00:31<00:05, 6077.18it/s] 85%|████████▌ | 191828/225169 [00:31<00:05, 6007.84it/s] 85%|████████▌ | 192454/225169 [00:32<00:05, 6074.66it/s] 86%|████████▌ | 193063/225169 [00:32<00:05, 6052.56it/s] 86%|████████▌ | 193685/225169 [00:32<00:05, 6098.00it/s] 86%|████████▋ | 194296/225169 [00:32<00:05, 6053.62it/s] 87%|████████▋ | 194902/225169 [00:32<00:05, 6052.63it/s] 87%|████████▋ | 195519/225169 [00:32<00:04, 6087.36it/s] 87%|████████▋ | 196128/225169 [00:32<00:04, 5920.65it/s] 87%|████████▋ | 196722/225169 [00:32<00:04, 5813.95it/s] 88%|████████▊ | 197686/225169 [00:32<00:03, 6923.64it/s] 88%|████████▊ | 198383/225169 [00:33<00:04, 6277.11it/s] 88%|████████▊ | 199025/225169 [00:33<00:04, 6266.34it/s] 89%|████████▊ | 199662/225169 [00:33<00:04, 6064.23it/s] 89%|████████▉ | 200276/225169 [00:33<00:04, 5953.28it/s] 89%|████████▉ | 200877/225169 [00:33<00:04, 5826.07it/s] 89%|████████▉ | 201463/225169 [00:33<00:04, 5587.04it/s] 90%|████████▉ | 202080/225169 [00:33<00:04, 5738.31it/s] 90%|█████████ | 202666/225169 [00:33<00:03, 5772.14it/s] 90%|█████████ | 203329/225169 [00:33<00:03, 6016.42it/s] 91%|█████████ | 203993/225169 [00:33<00:03, 6190.63it/s] 91%|█████████ | 204615/225169 [00:34<00:03, 5928.21it/s] 91%|█████████ | 205212/225169 [00:34<00:03, 5811.57it/s] 91%|█████████▏| 205796/225169 [00:34<00:03, 5730.03it/s] 92%|█████████▏| 206471/225169 [00:34<00:03, 6020.82it/s] 92%|█████████▏| 207076/225169 [00:34<00:03, 5847.44it/s] 92%|█████████▏| 207664/225169 [00:34<00:03, 5796.17it/s] 92%|█████████▏| 208246/225169 [00:34<00:02, 5737.92it/s] 93%|█████████▎| 208821/225169 [00:34<00:02, 5461.78it/s] 93%|█████████▎| 209394/225169 [00:34<00:02, 5524.34it/s] 93%|█████████▎| 210087/225169 [00:35<00:02, 5922.80it/s] 94%|█████████▎| 210716/225169 [00:35<00:02, 6028.77it/s] 94%|█████████▍| 211322/225169 [00:35<00:02, 5969.22it/s] 94%|█████████▍| 211934/225169 [00:35<00:02, 6012.66it/s] 94%|█████████▍| 212565/225169 [00:35<00:02, 6097.62it/s] 95%|█████████▍| 213218/225169 [00:35<00:01, 6223.20it/s] 95%|█████████▍| 213842/225169 [00:35<00:01, 5780.54it/s] 95%|█████████▌| 214427/225169 [00:35<00:01, 5786.71it/s] 95%|█████████▌| 215011/225169 [00:35<00:01, 5465.76it/s] 96%|█████████▌| 215564/225169 [00:35<00:01, 5473.31it/s] 96%|█████████▌| 216163/225169 [00:36<00:01, 5618.64it/s] 96%|█████████▋| 216729/225169 [00:36<00:01, 5628.47it/s] 97%|█████████▋| 217314/225169 [00:36<00:01, 5692.77it/s] 97%|█████████▋| 217965/225169 [00:36<00:01, 5933.30it/s] 97%|█████████▋| 218582/225169 [00:36<00:01, 6001.79it/s] 97%|█████████▋| 219234/225169 [00:36<00:00, 6151.65it/s] 98%|█████████▊| 219851/225169 [00:36<00:00, 5860.44it/s] 98%|█████████▊| 220441/225169 [00:36<00:00, 5817.26it/s] 98%|█████████▊| 221093/225169 [00:36<00:00, 6013.25it/s] 98%|█████████▊| 221697/225169 [00:37<00:00, 5860.02it/s] 99%|█████████▊| 222286/225169 [00:37<00:00, 5561.72it/s] 99%|█████████▉| 222911/225169 [00:37<00:00, 5752.40it/s] 99%|█████████▉| 223491/225169 [00:37<00:00, 5447.37it/s]100%|█████████▉| 224115/225169 [00:37<00:00, 5659.98it/s]100%|█████████▉| 224790/225169 [00:37<00:00, 5969.73it/s]100%|██████████| 225169/225169 [00:37<00:00, 5985.58it/s]

gathering stats for n=1
  0%|          | 0/225169 [00:00<?, ?it/s]  1%|          | 1851/225169 [00:00<00:12, 18495.77it/s]  2%|▏         | 3914/225169 [00:00<00:11, 19748.52it/s]  3%|▎         | 6162/225169 [00:00<00:10, 20986.71it/s]  4%|▎         | 8261/225169 [00:00<00:10, 20138.42it/s]  5%|▍         | 10306/225169 [00:00<00:10, 20242.89it/s]  5%|▌         | 12334/225169 [00:00<00:10, 19966.80it/s]  6%|▋         | 14415/225169 [00:00<00:10, 20225.34it/s]  7%|▋         | 16440/225169 [00:00<00:10, 20058.84it/s]  8%|▊         | 18448/225169 [00:00<00:10, 20062.16it/s]  9%|▉         | 20552/225169 [00:01<00:10, 20355.76it/s] 10%|█         | 22589/225169 [00:01<00:10, 20245.96it/s] 11%|█         | 24876/225169 [00:01<00:09, 21035.32it/s] 12%|█▏        | 26981/225169 [00:01<00:09, 20499.98it/s] 13%|█▎        | 29072/225169 [00:01<00:09, 20609.32it/s] 14%|█▍        | 31136/225169 [00:01<00:09, 19903.63it/s] 15%|█▍        | 33133/225169 [00:01<00:09, 19696.82it/s] 16%|█▌        | 35185/225169 [00:01<00:09, 19935.51it/s] 17%|█▋        | 37183/225169 [00:01<00:09, 19852.93it/s] 17%|█▋        | 39171/225169 [00:01<00:09, 19753.00it/s] 18%|█▊        | 41279/225169 [00:02<00:09, 20139.21it/s] 19%|█▉        | 43295/225169 [00:02<00:09, 19456.53it/s] 20%|██        | 45398/225169 [00:02<00:09, 19904.65it/s] 21%|██        | 47826/225169 [00:02<00:08, 21176.10it/s] 22%|██▏       | 50014/225169 [00:02<00:08, 21383.52it/s] 23%|██▎       | 52158/225169 [00:02<00:08, 21073.71it/s] 24%|██▍       | 54270/225169 [00:02<00:08, 20987.27it/s] 25%|██▌       | 56372/225169 [00:02<00:08, 20890.23it/s] 26%|██▌       | 58464/225169 [00:02<00:08, 20562.72it/s] 27%|██▋       | 60523/225169 [00:02<00:08, 20313.30it/s] 28%|██▊       | 62685/225169 [00:03<00:07, 20695.14it/s] 29%|██▉       | 64757/225169 [00:03<00:07, 20456.71it/s] 30%|██▉       | 66880/225169 [00:03<00:07, 20682.05it/s] 31%|███       | 68950/225169 [00:03<00:07, 20410.29it/s] 32%|███▏      | 71001/225169 [00:03<00:07, 20436.80it/s] 33%|███▎      | 73248/225169 [00:03<00:07, 21035.77it/s] 34%|███▎      | 75482/225169 [00:03<00:06, 21421.64it/s] 34%|███▍      | 77645/225169 [00:03<00:06, 21482.03it/s] 35%|███▌      | 79795/225169 [00:03<00:07, 20424.23it/s] 36%|███▋      | 81873/225169 [00:04<00:06, 20523.78it/s] 37%|███▋      | 84032/225169 [00:04<00:06, 20833.51it/s] 38%|███▊      | 86123/225169 [00:04<00:06, 20536.45it/s] 39%|███▉      | 88304/225169 [00:04<00:06, 20900.41it/s] 40%|████      | 90399/225169 [00:04<00:06, 20213.63it/s] 41%|████      | 92525/225169 [00:04<00:06, 20514.42it/s] 42%|████▏     | 94583/225169 [00:04<00:06, 20343.29it/s] 43%|████▎     | 96878/225169 [00:04<00:06, 21101.11it/s] 44%|████▍     | 98993/225169 [00:04<00:06, 20644.32it/s] 45%|████▍     | 101063/225169 [00:04<00:06, 20269.00it/s] 46%|████▌     | 103103/225169 [00:05<00:06, 20301.79it/s] 47%|████▋     | 105137/225169 [00:05<00:05, 20085.61it/s] 48%|████▊     | 107148/225169 [00:05<00:05, 20077.00it/s] 48%|████▊     | 109158/225169 [00:05<00:05, 19844.50it/s] 49%|████▉     | 111274/225169 [00:05<00:05, 20230.75it/s] 50%|█████     | 113300/225169 [00:05<00:05, 20239.04it/s] 51%|█████     | 115326/225169 [00:05<00:05, 20239.96it/s] 52%|█████▏    | 117351/225169 [00:05<00:05, 19637.88it/s] 53%|█████▎    | 119369/225169 [00:05<00:05, 19795.77it/s] 54%|█████▍    | 121479/225169 [00:05<00:05, 20179.30it/s] 55%|█████▍    | 123500/225169 [00:06<00:05, 20091.57it/s] 56%|█████▌    | 125512/225169 [00:06<00:04, 20064.31it/s] 57%|█████▋    | 127628/225169 [00:06<00:04, 20386.48it/s] 58%|█████▊    | 129681/225169 [00:06<00:04, 20426.59it/s] 59%|█████▊    | 131725/225169 [00:06<00:04, 19609.43it/s] 59%|█████▉    | 133836/225169 [00:06<00:04, 20044.76it/s] 60%|██████    | 136113/225169 [00:06<00:04, 20842.20it/s] 61%|██████▏   | 138243/225169 [00:06<00:04, 20974.73it/s] 62%|██████▏   | 140346/225169 [00:06<00:04, 20826.98it/s] 63%|██████▎   | 142433/225169 [00:06<00:04, 20591.57it/s] 64%|██████▍   | 144540/225169 [00:07<00:03, 20728.36it/s] 65%|██████▌   | 146648/225169 [00:07<00:03, 20831.77it/s] 66%|██████▌   | 148733/225169 [00:07<00:03, 20334.97it/s] 67%|██████▋   | 150770/225169 [00:07<00:03, 20107.60it/s] 68%|██████▊   | 152784/225169 [00:07<00:03, 19901.61it/s] 69%|██████▊   | 154776/225169 [00:07<00:03, 19886.70it/s] 70%|██████▉   | 156883/225169 [00:07<00:03, 20231.35it/s] 71%|███████   | 158983/225169 [00:07<00:03, 20457.33it/s] 72%|███████▏  | 161104/225169 [00:07<00:03, 20677.92it/s] 72%|███████▏  | 163173/225169 [00:08<00:03, 20611.56it/s] 73%|███████▎  | 165235/225169 [00:08<00:02, 20517.98it/s] 74%|███████▍  | 167395/225169 [00:08<00:02, 20837.72it/s] 75%|███████▌  | 169480/225169 [00:08<00:02, 20471.93it/s] 76%|███████▌  | 171529/225169 [00:08<00:02, 20030.48it/s] 77%|███████▋  | 173535/225169 [00:08<00:02, 19903.42it/s] 78%|███████▊  | 175528/225169 [00:08<00:02, 19837.35it/s] 79%|███████▉  | 177513/225169 [00:08<00:02, 19220.23it/s] 80%|███████▉  | 179562/225169 [00:08<00:02, 19583.19it/s] 81%|████████  | 181525/225169 [00:08<00:02, 19295.97it/s] 81%|████████▏ | 183479/225169 [00:09<00:02, 19366.76it/s] 82%|████████▏ | 185419/225169 [00:09<00:02, 18869.49it/s] 83%|████████▎ | 187372/225169 [00:09<00:01, 19056.53it/s] 84%|████████▍ | 189421/225169 [00:09<00:01, 19466.87it/s] 85%|████████▌ | 191424/225169 [00:09<00:01, 19626.75it/s] 86%|████████▌ | 193506/225169 [00:09<00:01, 19976.78it/s] 87%|████████▋ | 195542/225169 [00:09<00:01, 20088.81it/s] 88%|████████▊ | 197796/225169 [00:09<00:01, 20818.10it/s] 89%|████████▉ | 199880/225169 [00:09<00:01, 20238.53it/s] 90%|████████▉ | 201909/225169 [00:09<00:01, 19890.36it/s] 91%|█████████ | 204001/225169 [00:10<00:01, 20188.32it/s] 91%|█████████▏| 206024/225169 [00:10<00:00, 19802.68it/s] 92%|█████████▏| 208008/225169 [00:10<00:00, 19518.96it/s] 93%|█████████▎| 209963/225169 [00:10<00:00, 19336.74it/s] 94%|█████████▍| 211982/225169 [00:10<00:00, 19584.43it/s] 95%|█████████▌| 213971/225169 [00:10<00:00, 19673.16it/s] 96%|█████████▌| 215940/225169 [00:10<00:00, 19474.77it/s] 97%|█████████▋| 217920/225169 [00:10<00:00, 19570.32it/s] 98%|█████████▊| 219938/225169 [00:10<00:00, 19750.42it/s] 99%|█████████▊| 221914/225169 [00:10<00:00, 19672.44it/s] 99%|█████████▉| 223882/225169 [00:11<00:00, 19200.11it/s]100%|██████████| 225169/225169 [00:11<00:00, 20196.62it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 63.27it/s]2022-03-08 08:12:20 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-08 08:12:20 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-08 08:12:20 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-08 08:12:20 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-08 08:12:20 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-08 08:12:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-08 08:12:20 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-08 08:12:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-08 08:12:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 08:12:20 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-08 08:12:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 08:12:20 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-08 08:12:20 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-08 08:12:20 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 08:12:20 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 08:12:20 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-08 08:12:20 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-08 08:12:20 | INFO | fairseq.trainer | begin training epoch 1
2022-03-08 08:12:20 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-08 08:12:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-08 08:12:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 08:12:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 08:12:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 08:14:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-08 08:18:04 | INFO | train_inner | epoch 001:    105 / 196 loss=16.309, ppl=81194.8, wps=20197, ups=0.31, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.648, loss_scale=4, train_wall=338, gb_free=14.1, wall=343
2022-03-08 08:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:23:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.086 | ppl 8694.79 | wps 46734.2 | wpb 2034.1 | bsz 4 | num_updates 191
2022-03-08 08:23:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-08 08:23:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:23:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.086) (writing took 7.076973635703325 seconds)
2022-03-08 08:23:07 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-08 08:23:07 | INFO | train | epoch 001 | loss 15.236 | ppl 38580.6 | wps 19908.6 | ups 0.3 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.661 | loss_scale 4 | train_wall 625 | gb_free 14.1 | wall 647
2022-03-08 08:23:07 | INFO | fairseq.trainer | begin training epoch 2
2022-03-08 08:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:23:36 | INFO | train_inner | epoch 002:      9 / 196 loss=13.962, ppl=15962.3, wps=19666.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.555, loss_scale=4, train_wall=316, gb_free=14.1, wall=676
2022-03-08 08:28:57 | INFO | train_inner | epoch 002:    109 / 196 loss=11.996, ppl=4084.97, wps=20396.4, ups=0.31, wpb=65527.3, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.016, loss_scale=4, train_wall=316, gb_free=14.1, wall=997
2022-03-08 08:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:33:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.449 | ppl 1398.05 | wps 46880 | wpb 2034.1 | bsz 4 | num_updates 387 | best_loss 10.449
2022-03-08 08:33:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-08 08:33:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:33:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:33:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.449) (writing took 7.0095404256135225 seconds)
2022-03-08 08:33:48 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-08 08:33:48 | INFO | train | epoch 002 | loss 11.49 | ppl 2876.64 | wps 20026.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.846 | loss_scale 4 | train_wall 619 | gb_free 14.1 | wall 1287
2022-03-08 08:33:48 | INFO | fairseq.trainer | begin training epoch 3
2022-03-08 08:33:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:34:29 | INFO | train_inner | epoch 003:     13 / 196 loss=10.701, ppl=1665.09, wps=19684.9, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.578, loss_scale=4, train_wall=315, gb_free=14.1, wall=1329
2022-03-08 08:39:51 | INFO | train_inner | epoch 003:    113 / 196 loss=10.212, ppl=1186.18, wps=20407.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.447, loss_scale=4, train_wall=316, gb_free=14.1, wall=1650
2022-03-08 08:44:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:44:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.868 | ppl 934.26 | wps 46568.2 | wpb 2034.1 | bsz 4 | num_updates 583 | best_loss 9.868
2022-03-08 08:44:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 583 updates
2022-03-08 08:44:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:44:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:44:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 3 @ 583 updates, score 9.868) (writing took 7.881997701711953 seconds)
2022-03-08 08:44:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-08 08:44:29 | INFO | train | epoch 003 | loss 10.12 | ppl 1112.92 | wps 20004.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 583 | lr 7.29604e-05 | gnorm 0.464 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 1929
2022-03-08 08:44:29 | INFO | fairseq.trainer | begin training epoch 4
2022-03-08 08:44:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:45:24 | INFO | train_inner | epoch 004:     17 / 196 loss=9.937, ppl=980.5, wps=19631.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.494, loss_scale=8, train_wall=315, gb_free=14.1, wall=1983
2022-03-08 08:50:45 | INFO | train_inner | epoch 004:    117 / 196 loss=9.692, ppl=827.13, wps=20406.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.543, loss_scale=8, train_wall=316, gb_free=14.1, wall=2304
2022-03-08 08:54:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:55:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.466 | ppl 707.4 | wps 46955.5 | wpb 2034.1 | bsz 4 | num_updates 779 | best_loss 9.466
2022-03-08 08:55:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 779 updates
2022-03-08 08:55:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:55:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 08:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 4 @ 779 updates, score 9.466) (writing took 7.235539906658232 seconds)
2022-03-08 08:55:09 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-08 08:55:09 | INFO | train | epoch 004 | loss 9.627 | ppl 790.92 | wps 20024 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 779 | lr 9.74555e-05 | gnorm 0.572 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 2569
2022-03-08 08:55:09 | INFO | fairseq.trainer | begin training epoch 5
2022-03-08 08:55:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:56:17 | INFO | train_inner | epoch 005:     21 / 196 loss=9.479, ppl=713.86, wps=19668.1, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.626, loss_scale=8, train_wall=315, gb_free=14.1, wall=2637
2022-03-08 09:01:38 | INFO | train_inner | epoch 005:    121 / 196 loss=9.278, ppl=620.72, wps=20403.8, ups=0.31, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.685, loss_scale=8, train_wall=316, gb_free=14.1, wall=2958
2022-03-08 09:05:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:05:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.133 | ppl 561.56 | wps 46693 | wpb 2034.1 | bsz 4 | num_updates 975 | best_loss 9.133
2022-03-08 09:05:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 975 updates
2022-03-08 09:05:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:05:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:05:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 5 @ 975 updates, score 9.133) (writing took 7.085361544974148 seconds)
2022-03-08 09:05:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-08 09:05:50 | INFO | train | epoch 005 | loss 9.235 | ppl 602.44 | wps 20023 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 975 | lr 0.000121951 | gnorm 0.717 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 3210
2022-03-08 09:05:50 | INFO | fairseq.trainer | begin training epoch 6
2022-03-08 09:05:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:07:11 | INFO | train_inner | epoch 006:     25 / 196 loss=9.104, ppl=550.2, wps=19670, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.774, loss_scale=8, train_wall=316, gb_free=14.1, wall=3290
2022-03-08 09:12:32 | INFO | train_inner | epoch 006:    125 / 196 loss=8.927, ppl=486.88, wps=20405.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.762, loss_scale=16, train_wall=316, gb_free=14.1, wall=3611
2022-03-08 09:16:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:16:24 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.845 | ppl 459.97 | wps 46812.1 | wpb 2034.1 | bsz 4 | num_updates 1171 | best_loss 8.845
2022-03-08 09:16:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1171 updates
2022-03-08 09:16:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:16:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:16:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 6 @ 1171 updates, score 8.845) (writing took 7.127186954021454 seconds)
2022-03-08 09:16:31 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-08 09:16:31 | INFO | train | epoch 006 | loss 8.9 | ppl 477.78 | wps 20020.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1171 | lr 0.000146446 | gnorm 0.769 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 3851
2022-03-08 09:16:31 | INFO | fairseq.trainer | begin training epoch 7
2022-03-08 09:16:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:18:04 | INFO | train_inner | epoch 007:     29 / 196 loss=8.792, ppl=443.12, wps=19658.9, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.807, loss_scale=16, train_wall=316, gb_free=14.1, wall=3944
2022-03-08 09:23:25 | INFO | train_inner | epoch 007:    129 / 196 loss=8.638, ppl=398.5, wps=20400.3, ups=0.31, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.805, loss_scale=16, train_wall=316, gb_free=14.1, wall=4265
2022-03-08 09:27:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:27:05 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.607 | ppl 389.95 | wps 46939.8 | wpb 2034.1 | bsz 4 | num_updates 1367 | best_loss 8.607
2022-03-08 09:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1367 updates
2022-03-08 09:27:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:27:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 7 @ 1367 updates, score 8.607) (writing took 7.002326400950551 seconds)
2022-03-08 09:27:12 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-08 09:27:12 | INFO | train | epoch 007 | loss 8.619 | ppl 393.18 | wps 20021.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1367 | lr 0.000170941 | gnorm 0.812 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 4491
2022-03-08 09:27:12 | INFO | fairseq.trainer | begin training epoch 8
2022-03-08 09:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:28:58 | INFO | train_inner | epoch 008:     33 / 196 loss=8.514, ppl=365.55, wps=19674.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.806, loss_scale=16, train_wall=316, gb_free=14.1, wall=4597
2022-03-08 09:34:19 | INFO | train_inner | epoch 008:    133 / 196 loss=8.374, ppl=331.69, wps=20406.3, ups=0.31, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.828, loss_scale=16, train_wall=316, gb_free=14.1, wall=4919
2022-03-08 09:37:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:37:45 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.403 | ppl 338.43 | wps 46874.2 | wpb 2034.1 | bsz 4 | num_updates 1563 | best_loss 8.403
2022-03-08 09:37:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1563 updates
2022-03-08 09:37:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:37:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:37:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 8 @ 1563 updates, score 8.403) (writing took 7.062146885320544 seconds)
2022-03-08 09:37:52 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-08 09:37:52 | INFO | train | epoch 008 | loss 8.365 | ppl 329.66 | wps 20025.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1563 | lr 0.000195436 | gnorm 0.827 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 5132
2022-03-08 09:37:52 | INFO | fairseq.trainer | begin training epoch 9
2022-03-08 09:37:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:39:51 | INFO | train_inner | epoch 009:     37 / 196 loss=8.264, ppl=307.49, wps=19670.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.836, loss_scale=32, train_wall=316, gb_free=14.1, wall=5251
2022-03-08 09:45:12 | INFO | train_inner | epoch 009:    137 / 196 loss=8.138, ppl=281.66, wps=20397.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.827, loss_scale=32, train_wall=316, gb_free=14.1, wall=5572
2022-03-08 09:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:48:26 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.221 | ppl 298.41 | wps 46724.5 | wpb 2034.1 | bsz 4 | num_updates 1759 | best_loss 8.221
2022-03-08 09:48:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1759 updates
2022-03-08 09:48:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:48:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:48:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 9 @ 1759 updates, score 8.221) (writing took 7.0857260739430785 seconds)
2022-03-08 09:48:33 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-08 09:48:33 | INFO | train | epoch 009 | loss 8.129 | ppl 279.97 | wps 20015.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1759 | lr 0.000219931 | gnorm 0.825 | loss_scale 32 | train_wall 619 | gb_free 14.1 | wall 5773
2022-03-08 09:48:33 | INFO | fairseq.trainer | begin training epoch 10
2022-03-08 09:48:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:50:45 | INFO | train_inner | epoch 010:     41 / 196 loss=8.027, ppl=260.83, wps=19666.4, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.805, loss_scale=32, train_wall=316, gb_free=14.1, wall=5905
2022-03-08 09:56:06 | INFO | train_inner | epoch 010:    141 / 196 loss=7.912, ppl=240.9, wps=20390.9, ups=0.31, wpb=65527.3, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.806, loss_scale=32, train_wall=316, gb_free=14.1, wall=6226
2022-03-08 09:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:59:07 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.08 | ppl 270.51 | wps 46939.5 | wpb 2034.1 | bsz 4 | num_updates 1955 | best_loss 8.08
2022-03-08 09:59:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1955 updates
2022-03-08 09:59:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:59:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 09:59:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 10 @ 1955 updates, score 8.08) (writing took 7.165040358901024 seconds)
2022-03-08 09:59:14 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-08 09:59:14 | INFO | train | epoch 010 | loss 7.907 | ppl 240.09 | wps 20016.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1955 | lr 0.000244426 | gnorm 0.801 | loss_scale 32 | train_wall 619 | gb_free 14.1 | wall 6414
2022-03-08 09:59:14 | INFO | fairseq.trainer | begin training epoch 11
2022-03-08 09:59:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:01:39 | INFO | train_inner | epoch 011:     45 / 196 loss=7.8, ppl=222.88, wps=19663.4, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.799, loss_scale=32, train_wall=316, gb_free=14.1, wall=6558
2022-03-08 10:05:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 10:07:03 | INFO | train_inner | epoch 011:    146 / 196 loss=7.704, ppl=208.52, wps=20195.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.786, loss_scale=32, train_wall=320, gb_free=14.1, wall=6883
2022-03-08 10:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:09:48 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.952 | ppl 247.7 | wps 46899.8 | wpb 2034.1 | bsz 4 | num_updates 2150 | best_loss 7.952
2022-03-08 10:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2150 updates
2022-03-08 10:09:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:09:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:09:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 11 @ 2150 updates, score 7.952) (writing took 7.116003549657762 seconds)
2022-03-08 10:09:55 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-08 10:09:55 | INFO | train | epoch 011 | loss 7.7 | ppl 207.95 | wps 19914 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2150 | lr 0.000268796 | gnorm 0.789 | loss_scale 32 | train_wall 619 | gb_free 14.1 | wall 7054
2022-03-08 10:09:55 | INFO | fairseq.trainer | begin training epoch 12
2022-03-08 10:09:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:12:35 | INFO | train_inner | epoch 012:     50 / 196 loss=7.606, ppl=194.8, wps=19664.8, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.774, loss_scale=32, train_wall=316, gb_free=14.1, wall=7215
2022-03-08 10:17:57 | INFO | train_inner | epoch 012:    150 / 196 loss=7.5, ppl=181.08, wps=20390, ups=0.31, wpb=65530.9, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.779, loss_scale=32, train_wall=316, gb_free=14.1, wall=7537
2022-03-08 10:20:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:20:29 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.846 | ppl 230.07 | wps 46678.7 | wpb 2034.1 | bsz 4 | num_updates 2346 | best_loss 7.846
2022-03-08 10:20:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2346 updates
2022-03-08 10:20:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:20:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:20:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 12 @ 2346 updates, score 7.846) (writing took 7.059948829002678 seconds)
2022-03-08 10:20:36 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-08 10:20:36 | INFO | train | epoch 012 | loss 7.507 | ppl 181.95 | wps 20013 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 2346 | lr 0.000293291 | gnorm 0.778 | loss_scale 32 | train_wall 619 | gb_free 14.1 | wall 7695
2022-03-08 10:20:36 | INFO | fairseq.trainer | begin training epoch 13
2022-03-08 10:20:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:23:29 | INFO | train_inner | epoch 013:     54 / 196 loss=7.406, ppl=169.61, wps=19661.6, ups=0.3, wpb=65365, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.777, loss_scale=32, train_wall=316, gb_free=14.1, wall=7869
2022-03-08 10:28:51 | INFO | train_inner | epoch 013:    154 / 196 loss=7.333, ppl=161.27, wps=20390.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.767, loss_scale=32, train_wall=316, gb_free=14.1, wall=8190
2022-03-08 10:31:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:31:10 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.773 | ppl 218.66 | wps 46730.2 | wpb 2034.1 | bsz 4 | num_updates 2542 | best_loss 7.773
2022-03-08 10:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2542 updates
2022-03-08 10:31:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:31:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:31:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 13 @ 2542 updates, score 7.773) (writing took 7.024627179838717 seconds)
2022-03-08 10:31:17 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-08 10:31:17 | INFO | train | epoch 013 | loss 7.33 | ppl 160.86 | wps 20013.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 2542 | lr 0.000317786 | gnorm 0.783 | loss_scale 32 | train_wall 620 | gb_free 14.1 | wall 8336
2022-03-08 10:31:17 | INFO | fairseq.trainer | begin training epoch 14
2022-03-08 10:31:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 10:34:26 | INFO | train_inner | epoch 014:     59 / 196 loss=7.225, ppl=149.65, wps=19483.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.751, loss_scale=32, train_wall=319, gb_free=14.1, wall=8526
2022-03-08 10:39:48 | INFO | train_inner | epoch 014:    159 / 196 loss=7.166, ppl=143.6, wps=20386.4, ups=0.31, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.771, loss_scale=32, train_wall=317, gb_free=14.1, wall=8847
2022-03-08 10:41:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:41:50 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.701 | ppl 208.08 | wps 46883 | wpb 2034.1 | bsz 4 | num_updates 2737 | best_loss 7.701
2022-03-08 10:41:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2737 updates
2022-03-08 10:41:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:41:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:41:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 14 @ 2737 updates, score 7.701) (writing took 7.147910135798156 seconds)
2022-03-08 10:41:58 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-08 10:41:58 | INFO | train | epoch 014 | loss 7.163 | ppl 143.35 | wps 19910.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2737 | lr 0.000342157 | gnorm 0.735 | loss_scale 32 | train_wall 619 | gb_free 14.1 | wall 8977
2022-03-08 10:41:58 | INFO | fairseq.trainer | begin training epoch 15
2022-03-08 10:41:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:44:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 10:45:23 | INFO | train_inner | epoch 015:     64 / 196 loss=7.062, ppl=133.63, wps=19476, ups=0.3, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.746, loss_scale=16, train_wall=319, gb_free=14.1, wall=9183
2022-03-08 10:50:45 | INFO | train_inner | epoch 015:    164 / 196 loss=7.016, ppl=129.46, wps=20398.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.751, loss_scale=16, train_wall=316, gb_free=14.1, wall=9504
2022-03-08 10:52:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:52:31 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.645 | ppl 200.2 | wps 46817.9 | wpb 2034.1 | bsz 4 | num_updates 2932 | best_loss 7.645
2022-03-08 10:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2932 updates
2022-03-08 10:52:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 10:52:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 15 @ 2932 updates, score 7.645) (writing took 7.053982622921467 seconds)
2022-03-08 10:52:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-08 10:52:38 | INFO | train | epoch 015 | loss 7.015 | ppl 129.36 | wps 19919.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2932 | lr 0.000366527 | gnorm 0.755 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 9618
2022-03-08 10:52:38 | INFO | fairseq.trainer | begin training epoch 16
2022-03-08 10:52:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:56:17 | INFO | train_inner | epoch 016:     68 / 196 loss=6.912, ppl=120.44, wps=19673.7, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.74, loss_scale=16, train_wall=316, gb_free=14.1, wall=9837
2022-03-08 11:01:38 | INFO | train_inner | epoch 016:    168 / 196 loss=6.882, ppl=117.95, wps=20392.8, ups=0.31, wpb=65530.9, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.741, loss_scale=16, train_wall=316, gb_free=14.1, wall=10158
2022-03-08 11:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:03:12 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.602 | ppl 194.33 | wps 46835.5 | wpb 2034.1 | bsz 4 | num_updates 3128 | best_loss 7.602
2022-03-08 11:03:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3128 updates
2022-03-08 11:03:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 11:03:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-08 11:03:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 16 @ 3128 updates, score 7.602) (writing took 7.106521124951541 seconds)
2022-03-08 11:03:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-08 11:03:19 | INFO | train | epoch 016 | loss 6.873 | ppl 117.25 | wps 20018.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3128 | lr 0.000391022 | gnorm 0.743 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 10259
2022-03-08 11:03:19 | INFO | fairseq.trainer | begin training epoch 17
2022-03-08 11:03:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:07:11 | INFO | train_inner | epoch 017:     72 / 196 loss=6.767, ppl=108.92, wps=19664.4, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.741, loss_scale=16, train_wall=316, gb_free=14.1, wall=10490
2022-03-08 11:12:32 | INFO | train_inner | epoch 017:    172 / 196 loss=6.75, ppl=107.67, wps=20388.4, ups=0.31, wpb=65527.3, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.74, loss_scale=32, train_wall=316, gb_free=14.1, wall=10812
2022-03-08 11:13:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 11:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:13:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.626 | ppl 197.58 | wps 46818.3 | wpb 2034.1 | bsz 4 | num_updates 3323 | best_loss 7.602
2022-03-08 11:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3323 updates
2022-03-08 11:13:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 17 @ 3323 updates, score 7.626) (writing took 3.4663784001022577 seconds)
2022-03-08 11:13:56 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-08 11:13:56 | INFO | train | epoch 017 | loss 6.743 | ppl 107.09 | wps 20023.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3323 | lr 0.000415392 | gnorm 0.744 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 10896
2022-03-08 11:13:56 | INFO | fairseq.trainer | begin training epoch 18
2022-03-08 11:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:18:04 | INFO | train_inner | epoch 018:     77 / 196 loss=6.641, ppl=99.78, wps=19685.7, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.741, loss_scale=16, train_wall=319, gb_free=14.1, wall=11144
2022-03-08 11:23:25 | INFO | train_inner | epoch 018:    177 / 196 loss=6.629, ppl=99.01, wps=20397.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.735, loss_scale=16, train_wall=316, gb_free=14.1, wall=11465
2022-03-08 11:24:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:24:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.604 | ppl 194.59 | wps 46772.2 | wpb 2034.1 | bsz 4 | num_updates 3519 | best_loss 7.602
2022-03-08 11:24:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3519 updates
2022-03-08 11:24:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:24:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 18 @ 3519 updates, score 7.604) (writing took 3.474126771092415 seconds)
2022-03-08 11:24:34 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-08 11:24:34 | INFO | train | epoch 018 | loss 6.62 | ppl 98.39 | wps 20128.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3519 | lr 0.000439887 | gnorm 0.74 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 11534
2022-03-08 11:24:34 | INFO | fairseq.trainer | begin training epoch 19
2022-03-08 11:24:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:28:54 | INFO | train_inner | epoch 019:     81 / 196 loss=6.509, ppl=91.05, wps=19879.6, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.74, loss_scale=16, train_wall=316, gb_free=14.1, wall=11794
2022-03-08 11:34:15 | INFO | train_inner | epoch 019:    181 / 196 loss=6.519, ppl=91.68, wps=20394.7, ups=0.31, wpb=65530.9, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.738, loss_scale=16, train_wall=316, gb_free=14.1, wall=12115
2022-03-08 11:35:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:35:08 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.642 | ppl 199.68 | wps 46790.8 | wpb 2034.1 | bsz 4 | num_updates 3715 | best_loss 7.602
2022-03-08 11:35:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3715 updates
2022-03-08 11:35:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:35:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:35:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 19 @ 3715 updates, score 7.642) (writing took 3.4622717425227165 seconds)
2022-03-08 11:35:11 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-08 11:35:11 | INFO | train | epoch 019 | loss 6.502 | ppl 90.66 | wps 20130.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3715 | lr 0.000464382 | gnorm 0.732 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 12171
2022-03-08 11:35:11 | INFO | fairseq.trainer | begin training epoch 20
2022-03-08 11:35:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:39:44 | INFO | train_inner | epoch 020:     85 / 196 loss=6.389, ppl=83.8, wps=19882.2, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=3800, lr=0.000475005, gnorm=0.74, loss_scale=16, train_wall=316, gb_free=14.1, wall=12444
2022-03-08 11:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 11:45:09 | INFO | train_inner | epoch 020:    186 / 196 loss=6.415, ppl=85.35, wps=20187.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.754, loss_scale=16, train_wall=320, gb_free=14.1, wall=12769
2022-03-08 11:45:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:45:45 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.635 | ppl 198.75 | wps 46818.6 | wpb 2034.1 | bsz 4 | num_updates 3910 | best_loss 7.602
2022-03-08 11:45:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3910 updates
2022-03-08 11:45:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:45:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 20 @ 3910 updates, score 7.635) (writing took 3.465544047765434 seconds)
2022-03-08 11:45:48 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-08 11:45:48 | INFO | train | epoch 020 | loss 6.393 | ppl 84.06 | wps 20025.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3910 | lr 0.000488752 | gnorm 0.744 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 12808
2022-03-08 11:45:48 | INFO | fairseq.trainer | begin training epoch 21
2022-03-08 11:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:50:37 | INFO | train_inner | epoch 021:     90 / 196 loss=6.271, ppl=77.23, wps=19892.7, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=4000, lr=0.0005, gnorm=0.731, loss_scale=16, train_wall=315, gb_free=14.1, wall=13097
2022-03-08 11:55:59 | INFO | train_inner | epoch 021:    190 / 196 loss=6.312, ppl=79.43, wps=20383.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.735, loss_scale=16, train_wall=317, gb_free=14.1, wall=13419
2022-03-08 11:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:56:22 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.715 | ppl 210.09 | wps 46552.2 | wpb 2034.1 | bsz 4 | num_updates 4106 | best_loss 7.602
2022-03-08 11:56:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4106 updates
2022-03-08 11:56:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:56:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 11:56:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 21 @ 4106 updates, score 7.715) (writing took 3.4244732884690166 seconds)
2022-03-08 11:56:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-08 11:56:26 | INFO | train | epoch 021 | loss 6.286 | ppl 78.05 | wps 20130.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4106 | lr 0.000493504 | gnorm 0.736 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 13445
2022-03-08 11:56:26 | INFO | fairseq.trainer | begin training epoch 22
2022-03-08 11:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:01:28 | INFO | train_inner | epoch 022:     94 / 196 loss=6.157, ppl=71.36, wps=19883.2, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=4200, lr=0.00048795, gnorm=0.716, loss_scale=16, train_wall=316, gb_free=14.1, wall=13747
2022-03-08 12:06:49 | INFO | train_inner | epoch 022:    194 / 196 loss=6.199, ppl=73.47, wps=20398.9, ups=0.31, wpb=65530.9, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.711, loss_scale=16, train_wall=316, gb_free=14.1, wall=14069
2022-03-08 12:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:06:59 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.803 | ppl 223.28 | wps 46783.3 | wpb 2034.1 | bsz 4 | num_updates 4302 | best_loss 7.602
2022-03-08 12:06:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4302 updates
2022-03-08 12:06:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 22 @ 4302 updates, score 7.803) (writing took 3.4781309831887484 seconds)
2022-03-08 12:07:03 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-08 12:07:03 | INFO | train | epoch 022 | loss 6.174 | ppl 72.22 | wps 20131.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4302 | lr 0.000482131 | gnorm 0.713 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 14083
2022-03-08 12:07:03 | INFO | fairseq.trainer | begin training epoch 23
2022-03-08 12:07:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:10:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 12:12:21 | INFO | train_inner | epoch 023:     99 / 196 loss=6.041, ppl=65.84, wps=19680.9, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=4400, lr=0.000476731, gnorm=0.692, loss_scale=16, train_wall=319, gb_free=14.1, wall=14401
2022-03-08 12:17:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:17:37 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.812 | ppl 224.71 | wps 46733.8 | wpb 2034.1 | bsz 4 | num_updates 4497 | best_loss 7.602
2022-03-08 12:17:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4497 updates
2022-03-08 12:17:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 23 @ 4497 updates, score 7.812) (writing took 3.374738512560725 seconds)
2022-03-08 12:17:40 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-08 12:17:40 | INFO | train | epoch 023 | loss 6.066 | ppl 67 | wps 20023.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4497 | lr 0.000471562 | gnorm 0.691 | loss_scale 16 | train_wall 620 | gb_free 14.1 | wall 14720
2022-03-08 12:17:40 | INFO | fairseq.trainer | begin training epoch 24
2022-03-08 12:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:17:50 | INFO | train_inner | epoch 024:      3 / 196 loss=6.09, ppl=68.11, wps=19882.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.691, loss_scale=16, train_wall=316, gb_free=14.1, wall=14730
2022-03-08 12:23:11 | INFO | train_inner | epoch 024:    103 / 196 loss=5.942, ppl=61.46, wps=20387.9, ups=0.31, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.701, loss_scale=16, train_wall=317, gb_free=14.1, wall=15051
2022-03-08 12:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:28:14 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.9 | ppl 238.89 | wps 47034.4 | wpb 2034.1 | bsz 4 | num_updates 4693 | best_loss 7.602
2022-03-08 12:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4693 updates
2022-03-08 12:28:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:28:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 24 @ 4693 updates, score 7.9) (writing took 3.399430250748992 seconds)
2022-03-08 12:28:17 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-08 12:28:17 | INFO | train | epoch 024 | loss 5.971 | ppl 62.71 | wps 20132 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4693 | lr 0.000461609 | gnorm 0.696 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 15357
2022-03-08 12:28:17 | INFO | fairseq.trainer | begin training epoch 25
2022-03-08 12:28:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:28:40 | INFO | train_inner | epoch 025:      7 / 196 loss=5.991, ppl=63.6, wps=19889.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.691, loss_scale=16, train_wall=316, gb_free=14.1, wall=15380
2022-03-08 12:34:01 | INFO | train_inner | epoch 025:    107 / 196 loss=5.851, ppl=57.73, wps=20396, ups=0.31, wpb=65527.3, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.697, loss_scale=16, train_wall=316, gb_free=14.1, wall=15701
2022-03-08 12:38:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:38:51 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.881 | ppl 235.8 | wps 46684.1 | wpb 2034.1 | bsz 4 | num_updates 4889 | best_loss 7.602
2022-03-08 12:38:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4889 updates
2022-03-08 12:38:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:38:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 25 @ 4889 updates, score 7.881) (writing took 3.4744229279458523 seconds)
2022-03-08 12:38:55 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-08 12:38:55 | INFO | train | epoch 025 | loss 5.878 | ppl 58.79 | wps 20121.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4889 | lr 0.000452262 | gnorm 0.686 | loss_scale 32 | train_wall 620 | gb_free 14.1 | wall 15995
2022-03-08 12:38:55 | INFO | fairseq.trainer | begin training epoch 26
2022-03-08 12:38:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:38:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 12:39:33 | INFO | train_inner | epoch 026:     12 / 196 loss=5.892, ppl=59.39, wps=19674.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.677, loss_scale=16, train_wall=319, gb_free=14.1, wall=16033
2022-03-08 12:44:55 | INFO | train_inner | epoch 026:    112 / 196 loss=5.759, ppl=54.17, wps=20405.2, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.684, loss_scale=16, train_wall=316, gb_free=14.1, wall=16354
2022-03-08 12:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:49:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.98 | ppl 252.5 | wps 46836.3 | wpb 2034.1 | bsz 4 | num_updates 5084 | best_loss 7.602
2022-03-08 12:49:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5084 updates
2022-03-08 12:49:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:49:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 12:49:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 26 @ 5084 updates, score 7.98) (writing took 3.500024282373488 seconds)
2022-03-08 12:49:32 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-08 12:49:32 | INFO | train | epoch 026 | loss 5.79 | ppl 55.34 | wps 20031.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5084 | lr 0.000443504 | gnorm 0.688 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 16632
2022-03-08 12:49:32 | INFO | fairseq.trainer | begin training epoch 27
2022-03-08 12:49:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:50:23 | INFO | train_inner | epoch 027:     16 / 196 loss=5.803, ppl=55.83, wps=19881.6, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.701, loss_scale=16, train_wall=316, gb_free=14.1, wall=16683
2022-03-08 12:55:45 | INFO | train_inner | epoch 027:    116 / 196 loss=5.686, ppl=51.48, wps=20398.1, ups=0.31, wpb=65527.3, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.693, loss_scale=16, train_wall=316, gb_free=14.1, wall=17004
2022-03-08 13:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:00:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.955 | ppl 248.1 | wps 46770.1 | wpb 2034.1 | bsz 4 | num_updates 5280 | best_loss 7.602
2022-03-08 13:00:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5280 updates
2022-03-08 13:00:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:00:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 27 @ 5280 updates, score 7.955) (writing took 3.486697157844901 seconds)
2022-03-08 13:00:09 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-08 13:00:09 | INFO | train | epoch 027 | loss 5.71 | ppl 52.36 | wps 20133.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5280 | lr 0.000435194 | gnorm 0.697 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 17269
2022-03-08 13:00:09 | INFO | fairseq.trainer | begin training epoch 28
2022-03-08 13:00:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:01:13 | INFO | train_inner | epoch 028:     20 / 196 loss=5.715, ppl=52.51, wps=19885.1, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.676, loss_scale=16, train_wall=316, gb_free=14.1, wall=17333
2022-03-08 13:06:35 | INFO | train_inner | epoch 028:    120 / 196 loss=5.616, ppl=49.06, wps=20399.7, ups=0.31, wpb=65527.3, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.688, loss_scale=16, train_wall=316, gb_free=14.1, wall=17654
2022-03-08 13:07:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 13:10:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:10:43 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.023 | ppl 260.19 | wps 46775.6 | wpb 2034.1 | bsz 4 | num_updates 5475 | best_loss 7.602
2022-03-08 13:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5475 updates
2022-03-08 13:10:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:10:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:10:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 28 @ 5475 updates, score 8.023) (writing took 3.5354339657351375 seconds)
2022-03-08 13:10:46 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-08 13:10:46 | INFO | train | epoch 028 | loss 5.633 | ppl 49.63 | wps 20032 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5475 | lr 0.000427374 | gnorm 0.678 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 17906
2022-03-08 13:10:46 | INFO | fairseq.trainer | begin training epoch 29
2022-03-08 13:10:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:12:07 | INFO | train_inner | epoch 029:     25 / 196 loss=5.637, ppl=49.75, wps=19693.2, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.713, loss_scale=16, train_wall=319, gb_free=14.1, wall=17986
2022-03-08 13:17:28 | INFO | train_inner | epoch 029:    125 / 196 loss=5.544, ppl=46.67, wps=20406.9, ups=0.31, wpb=65527.3, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.695, loss_scale=16, train_wall=316, gb_free=14.1, wall=18307
2022-03-08 13:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:21:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.157 | ppl 285.45 | wps 46754 | wpb 2034.1 | bsz 4 | num_updates 5671 | best_loss 7.602
2022-03-08 13:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5671 updates
2022-03-08 13:21:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:21:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 29 @ 5671 updates, score 8.157) (writing took 3.474115992896259 seconds)
2022-03-08 13:21:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-08 13:21:23 | INFO | train | epoch 029 | loss 5.562 | ppl 47.24 | wps 20138.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5671 | lr 0.000419923 | gnorm 0.703 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 18543
2022-03-08 13:21:23 | INFO | fairseq.trainer | begin training epoch 30
2022-03-08 13:21:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:22:56 | INFO | train_inner | epoch 030:     29 / 196 loss=5.551, ppl=46.89, wps=19888.3, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.685, loss_scale=16, train_wall=316, gb_free=14.1, wall=18636
2022-03-08 13:28:18 | INFO | train_inner | epoch 030:    129 / 196 loss=5.482, ppl=44.7, wps=20404, ups=0.31, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.689, loss_scale=16, train_wall=316, gb_free=14.1, wall=18957
2022-03-08 13:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:31:57 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.106 | ppl 275.45 | wps 46685.4 | wpb 2034.1 | bsz 4 | num_updates 5867 | best_loss 7.602
2022-03-08 13:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5867 updates
2022-03-08 13:31:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:32:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:32:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 30 @ 5867 updates, score 8.106) (writing took 3.4750097040086985 seconds)
2022-03-08 13:32:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-08 13:32:00 | INFO | train | epoch 030 | loss 5.494 | ppl 45.05 | wps 20141.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5867 | lr 0.00041285 | gnorm 0.69 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 19180
2022-03-08 13:32:00 | INFO | fairseq.trainer | begin training epoch 31
2022-03-08 13:32:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:33:46 | INFO | train_inner | epoch 031:     33 / 196 loss=5.478, ppl=44.56, wps=19895.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.704, loss_scale=16, train_wall=315, gb_free=14.1, wall=19286
2022-03-08 13:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 13:35:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 13:39:13 | INFO | train_inner | epoch 031:    135 / 196 loss=5.42, ppl=42.81, wps=20019.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.703, loss_scale=8, train_wall=322, gb_free=14.1, wall=19613
2022-03-08 13:42:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:42:33 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.166 | ppl 287.25 | wps 46837.5 | wpb 2034.1 | bsz 4 | num_updates 6061 | best_loss 7.602
2022-03-08 13:42:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6061 updates
2022-03-08 13:42:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 31 @ 6061 updates, score 8.166) (writing took 3.4888020865619183 seconds)
2022-03-08 13:42:37 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-08 13:42:37 | INFO | train | epoch 031 | loss 5.428 | ppl 43.05 | wps 19944.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 6061 | lr 0.000406189 | gnorm 0.702 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 19816
2022-03-08 13:42:37 | INFO | fairseq.trainer | begin training epoch 32
2022-03-08 13:42:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:44:42 | INFO | train_inner | epoch 032:     39 / 196 loss=5.41, ppl=42.53, wps=19898.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.7, loss_scale=8, train_wall=315, gb_free=14.1, wall=19942
2022-03-08 13:50:03 | INFO | train_inner | epoch 032:    139 / 196 loss=5.366, ppl=41.23, wps=20404.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.701, loss_scale=8, train_wall=316, gb_free=14.1, wall=20263
2022-03-08 13:53:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:53:10 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.25 | ppl 304.47 | wps 46679.5 | wpb 2034.1 | bsz 4 | num_updates 6257 | best_loss 7.602
2022-03-08 13:53:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6257 updates
2022-03-08 13:53:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:53:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 13:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 32 @ 6257 updates, score 8.25) (writing took 3.5063868453726172 seconds)
2022-03-08 13:53:14 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-08 13:53:14 | INFO | train | epoch 032 | loss 5.369 | ppl 41.32 | wps 20139 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6257 | lr 0.000399776 | gnorm 0.722 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 20453
2022-03-08 13:53:14 | INFO | fairseq.trainer | begin training epoch 33
2022-03-08 13:53:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:55:32 | INFO | train_inner | epoch 033:     43 / 196 loss=5.343, ppl=40.58, wps=19888.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.725, loss_scale=8, train_wall=315, gb_free=14.1, wall=20592
2022-03-08 14:00:53 | INFO | train_inner | epoch 033:    143 / 196 loss=5.309, ppl=39.64, wps=20406.1, ups=0.31, wpb=65530.9, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.699, loss_scale=8, train_wall=316, gb_free=14.1, wall=20913
2022-03-08 14:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:03:47 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.232 | ppl 300.64 | wps 46634.4 | wpb 2034.1 | bsz 4 | num_updates 6453 | best_loss 7.602
2022-03-08 14:03:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6453 updates
2022-03-08 14:03:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:03:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:03:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 33 @ 6453 updates, score 8.232) (writing took 3.473141113296151 seconds)
2022-03-08 14:03:51 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-08 14:03:51 | INFO | train | epoch 033 | loss 5.309 | ppl 39.65 | wps 20138.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6453 | lr 0.000393658 | gnorm 0.706 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 21090
2022-03-08 14:03:51 | INFO | fairseq.trainer | begin training epoch 34
2022-03-08 14:03:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:06:22 | INFO | train_inner | epoch 034:     47 / 196 loss=5.282, ppl=38.9, wps=19887.6, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.703, loss_scale=16, train_wall=316, gb_free=14.1, wall=21241
2022-03-08 14:11:43 | INFO | train_inner | epoch 034:    147 / 196 loss=5.254, ppl=38.17, wps=20393, ups=0.31, wpb=65527.3, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.715, loss_scale=16, train_wall=316, gb_free=14.1, wall=21563
2022-03-08 14:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:14:25 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.298 | ppl 314.64 | wps 45514.1 | wpb 2034.1 | bsz 4 | num_updates 6649 | best_loss 7.602
2022-03-08 14:14:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6649 updates
2022-03-08 14:14:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:14:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:14:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 34 @ 6649 updates, score 8.298) (writing took 3.144029955379665 seconds)
2022-03-08 14:14:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-08 14:14:28 | INFO | train | epoch 034 | loss 5.254 | ppl 38.17 | wps 20132.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6649 | lr 0.000387813 | gnorm 0.711 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 21728
2022-03-08 14:14:28 | INFO | fairseq.trainer | begin training epoch 35
2022-03-08 14:14:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:17:13 | INFO | train_inner | epoch 035:     51 / 196 loss=5.228, ppl=37.47, wps=19791.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.702, loss_scale=16, train_wall=317, gb_free=14.1, wall=21893
2022-03-08 14:22:36 | INFO | train_inner | epoch 035:    151 / 196 loss=5.208, ppl=36.96, wps=20318, ups=0.31, wpb=65532.4, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.715, loss_scale=16, train_wall=318, gb_free=14.1, wall=22215
2022-03-08 14:24:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 14:24:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:25:04 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.355 | ppl 327.31 | wps 46810.3 | wpb 2034.1 | bsz 4 | num_updates 6844 | best_loss 7.602
2022-03-08 14:25:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6844 updates
2022-03-08 14:25:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:25:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 35 @ 6844 updates, score 8.355) (writing took 3.572472468018532 seconds)
2022-03-08 14:25:08 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-08 14:25:08 | INFO | train | epoch 035 | loss 5.2 | ppl 36.77 | wps 19941.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6844 | lr 0.000382248 | gnorm 0.697 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 22367
2022-03-08 14:25:08 | INFO | fairseq.trainer | begin training epoch 36
2022-03-08 14:25:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:28:08 | INFO | train_inner | epoch 036:     56 / 196 loss=5.166, ppl=35.89, wps=19695.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.733, loss_scale=8, train_wall=319, gb_free=14.1, wall=22547
2022-03-08 14:33:29 | INFO | train_inner | epoch 036:    156 / 196 loss=5.161, ppl=35.77, wps=20406.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.699, loss_scale=8, train_wall=316, gb_free=14.1, wall=22869
2022-03-08 14:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:35:41 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.368 | ppl 330.49 | wps 46779.5 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 7.602
2022-03-08 14:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7040 updates
2022-03-08 14:35:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:35:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 36 @ 7040 updates, score 8.368) (writing took 3.486354911699891 seconds)
2022-03-08 14:35:45 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-08 14:35:45 | INFO | train | epoch 036 | loss 5.152 | ppl 35.54 | wps 20142.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.723 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 23004
2022-03-08 14:35:45 | INFO | fairseq.trainer | begin training epoch 37
2022-03-08 14:35:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:38:57 | INFO | train_inner | epoch 037:     60 / 196 loss=5.111, ppl=34.56, wps=19895.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.734, loss_scale=8, train_wall=315, gb_free=14.1, wall=23197
2022-03-08 14:44:18 | INFO | train_inner | epoch 037:    160 / 196 loss=5.116, ppl=34.69, wps=20403.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.712, loss_scale=8, train_wall=316, gb_free=14.1, wall=23518
2022-03-08 14:46:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:46:18 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.409 | ppl 340.03 | wps 46643.8 | wpb 2034.1 | bsz 4 | num_updates 7236 | best_loss 7.602
2022-03-08 14:46:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7236 updates
2022-03-08 14:46:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:46:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:46:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 37 @ 7236 updates, score 8.409) (writing took 3.5002795914188027 seconds)
2022-03-08 14:46:21 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-08 14:46:21 | INFO | train | epoch 037 | loss 5.103 | ppl 34.37 | wps 20141.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7236 | lr 0.00037175 | gnorm 0.728 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 23641
2022-03-08 14:46:21 | INFO | fairseq.trainer | begin training epoch 38
2022-03-08 14:46:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:49:47 | INFO | train_inner | epoch 038:     64 / 196 loss=5.058, ppl=33.31, wps=19894.9, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.719, loss_scale=8, train_wall=315, gb_free=14.1, wall=23847
2022-03-08 14:55:08 | INFO | train_inner | epoch 038:    164 / 196 loss=5.074, ppl=33.68, wps=20406.9, ups=0.31, wpb=65530.9, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.726, loss_scale=16, train_wall=316, gb_free=14.1, wall=24168
2022-03-08 14:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:56:55 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.448 | ppl 349.19 | wps 46651.1 | wpb 2034.1 | bsz 4 | num_updates 7432 | best_loss 7.602
2022-03-08 14:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7432 updates
2022-03-08 14:56:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:56:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 14:56:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 38 @ 7432 updates, score 8.448) (writing took 3.5037938924506307 seconds)
2022-03-08 14:56:58 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-08 14:56:58 | INFO | train | epoch 038 | loss 5.056 | ppl 33.28 | wps 20139.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7432 | lr 0.000366815 | gnorm 0.716 | loss_scale 16 | train_wall 619 | gb_free 14.1 | wall 24278
2022-03-08 14:56:58 | INFO | fairseq.trainer | begin training epoch 39
2022-03-08 14:56:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:00:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 15:00:40 | INFO | train_inner | epoch 039:     69 / 196 loss=5.009, ppl=32.19, wps=19693, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=7500, lr=0.000365148, gnorm=0.715, loss_scale=8, train_wall=319, gb_free=14.1, wall=24500
2022-03-08 15:06:01 | INFO | train_inner | epoch 039:    169 / 196 loss=5.031, ppl=32.7, wps=20407.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.716, loss_scale=8, train_wall=316, gb_free=14.1, wall=24821
2022-03-08 15:07:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:07:32 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.458 | ppl 351.59 | wps 46713.7 | wpb 2034.1 | bsz 4 | num_updates 7627 | best_loss 7.602
2022-03-08 15:07:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7627 updates
2022-03-08 15:07:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:07:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 39 @ 7627 updates, score 8.458) (writing took 3.470933008939028 seconds)
2022-03-08 15:07:35 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-08 15:07:35 | INFO | train | epoch 039 | loss 5.013 | ppl 32.28 | wps 20036.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7627 | lr 0.000362095 | gnorm 0.719 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 24915
2022-03-08 15:07:35 | INFO | fairseq.trainer | begin training epoch 40
2022-03-08 15:07:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:11:30 | INFO | train_inner | epoch 040:     73 / 196 loss=4.961, ppl=31.15, wps=19900.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7700, lr=0.000360375, gnorm=0.729, loss_scale=8, train_wall=315, gb_free=14.1, wall=25149
2022-03-08 15:16:51 | INFO | train_inner | epoch 040:    173 / 196 loss=4.993, ppl=31.85, wps=20402.7, ups=0.31, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.74, loss_scale=8, train_wall=316, gb_free=14.1, wall=25471
2022-03-08 15:18:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:18:09 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.494 | ppl 360.6 | wps 46825.4 | wpb 2034.1 | bsz 4 | num_updates 7823 | best_loss 7.602
2022-03-08 15:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7823 updates
2022-03-08 15:18:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:18:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 40 @ 7823 updates, score 8.494) (writing took 3.4960669158026576 seconds)
2022-03-08 15:18:12 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-08 15:18:12 | INFO | train | epoch 040 | loss 4.971 | ppl 31.35 | wps 20143.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7823 | lr 0.000357531 | gnorm 0.728 | loss_scale 8 | train_wall 619 | gb_free 14.1 | wall 25552
2022-03-08 15:18:12 | INFO | fairseq.trainer | begin training epoch 41
2022-03-08 15:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:22:21 | INFO | train_inner | epoch 041:     77 / 196 loss=4.918, ppl=30.23, wps=19801.9, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=7900, lr=0.000355784, gnorm=0.718, loss_scale=8, train_wall=317, gb_free=14.1, wall=25801
2022-03-08 15:27:46 | INFO | train_inner | epoch 041:    177 / 196 loss=4.954, ppl=31, wps=20170.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.73, loss_scale=8, train_wall=320, gb_free=14.1, wall=26126
2022-03-08 15:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:28:52 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.57 | ppl 380.17 | wps 45637.3 | wpb 2034.1 | bsz 4 | num_updates 8019 | best_loss 7.602
2022-03-08 15:28:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 8019 updates
2022-03-08 15:28:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:28:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:28:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 41 @ 8019 updates, score 8.57) (writing took 3.5535314539447427 seconds)
2022-03-08 15:28:55 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-08 15:28:55 | INFO | train | epoch 041 | loss 4.93 | ppl 30.48 | wps 19949.5 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 8019 | lr 0.000353134 | gnorm 0.727 | loss_scale 16 | train_wall 624 | gb_free 14.1 | wall 26195
2022-03-08 15:28:55 | INFO | fairseq.trainer | begin training epoch 42
2022-03-08 15:28:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:29:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 15:33:22 | INFO | train_inner | epoch 042:     82 / 196 loss=4.871, ppl=29.25, wps=19472.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8100, lr=0.000351364, gnorm=0.734, loss_scale=8, train_wall=322, gb_free=14.1, wall=26461
2022-03-08 15:38:46 | INFO | train_inner | epoch 042:    182 / 196 loss=4.921, ppl=30.29, wps=20182, ups=0.31, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.727, loss_scale=8, train_wall=320, gb_free=14.1, wall=26786
2022-03-08 15:39:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:39:36 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.577 | ppl 381.93 | wps 45770.5 | wpb 2034.1 | bsz 4 | num_updates 8214 | best_loss 7.602
2022-03-08 15:39:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8214 updates
2022-03-08 15:39:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:39:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:39:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 42 @ 8214 updates, score 8.577) (writing took 3.5444526355713606 seconds)
2022-03-08 15:39:39 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-08 15:39:39 | INFO | train | epoch 042 | loss 4.89 | ppl 29.65 | wps 19811.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8214 | lr 0.000348917 | gnorm 0.728 | loss_scale 8 | train_wall 626 | gb_free 14.1 | wall 26839
2022-03-08 15:39:39 | INFO | fairseq.trainer | begin training epoch 43
2022-03-08 15:39:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:44:19 | INFO | train_inner | epoch 043:     86 / 196 loss=4.826, ppl=28.36, wps=19664.6, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=8300, lr=0.000347105, gnorm=0.739, loss_scale=8, train_wall=319, gb_free=14.1, wall=27119
2022-03-08 15:49:44 | INFO | train_inner | epoch 043:    186 / 196 loss=4.89, ppl=29.65, wps=20171.6, ups=0.31, wpb=65527.3, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.737, loss_scale=8, train_wall=320, gb_free=14.1, wall=27443
2022-03-08 15:50:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:50:20 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.565 | ppl 378.75 | wps 45526.6 | wpb 2034.1 | bsz 4 | num_updates 8410 | best_loss 7.602
2022-03-08 15:50:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8410 updates
2022-03-08 15:50:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:50:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 15:50:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 43 @ 8410 updates, score 8.565) (writing took 3.5009611640125513 seconds)
2022-03-08 15:50:24 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-08 15:50:24 | INFO | train | epoch 043 | loss 4.854 | ppl 28.92 | wps 19912.7 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 8410 | lr 0.000344828 | gnorm 0.744 | loss_scale 8 | train_wall 626 | gb_free 14.1 | wall 27483
2022-03-08 15:50:24 | INFO | fairseq.trainer | begin training epoch 44
2022-03-08 15:50:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:55:16 | INFO | train_inner | epoch 044:     90 / 196 loss=4.783, ppl=27.54, wps=19659.7, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=8500, lr=0.000342997, gnorm=0.723, loss_scale=8, train_wall=319, gb_free=14.1, wall=27776
2022-03-08 16:00:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 16:00:44 | INFO | train_inner | epoch 044:    191 / 196 loss=4.859, ppl=29.01, wps=19962.7, ups=0.3, wpb=65527.3, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.742, loss_scale=8, train_wall=323, gb_free=14.1, wall=28104
2022-03-08 16:01:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:01:05 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.666 | ppl 406.2 | wps 45493.7 | wpb 2034.1 | bsz 4 | num_updates 8605 | best_loss 7.602
2022-03-08 16:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8605 updates
2022-03-08 16:01:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 16:01:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 16:01:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 44 @ 8605 updates, score 8.666) (writing took 3.479620832949877 seconds)
2022-03-08 16:01:08 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-08 16:01:08 | INFO | train | epoch 044 | loss 4.817 | ppl 28.18 | wps 19799.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8605 | lr 0.000340898 | gnorm 0.731 | loss_scale 8 | train_wall 626 | gb_free 14.1 | wall 28128
2022-03-08 16:01:08 | INFO | fairseq.trainer | begin training epoch 45
2022-03-08 16:01:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:06:17 | INFO | train_inner | epoch 045:     95 / 196 loss=4.747, ppl=26.86, wps=19658.1, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=8700, lr=0.000339032, gnorm=0.728, loss_scale=8, train_wall=319, gb_free=14.1, wall=28437
2022-03-08 16:11:42 | INFO | train_inner | epoch 045:    195 / 196 loss=4.823, ppl=28.3, wps=20165.6, ups=0.31, wpb=65530.9, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.755, loss_scale=8, train_wall=320, gb_free=14.1, wall=28762
2022-03-08 16:11:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:11:49 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.657 | ppl 403.63 | wps 45977.2 | wpb 2034.1 | bsz 4 | num_updates 8801 | best_loss 7.602
2022-03-08 16:11:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8801 updates
2022-03-08 16:11:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 16:11:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 16:11:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 45 @ 8801 updates, score 8.657) (writing took 3.4986932668834925 seconds)
2022-03-08 16:11:53 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-08 16:11:53 | INFO | train | epoch 045 | loss 4.783 | ppl 27.54 | wps 19905.2 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 8801 | lr 0.000337081 | gnorm 0.741 | loss_scale 8 | train_wall 626 | gb_free 14.1 | wall 28772
2022-03-08 16:11:53 | INFO | fairseq.trainer | begin training epoch 46
2022-03-08 16:11:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:17:14 | INFO | train_inner | epoch 046:     99 / 196 loss=4.709, ppl=26.15, wps=19653.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8900, lr=0.000335201, gnorm=0.728, loss_scale=8, train_wall=319, gb_free=14.1, wall=29094
2022-03-08 16:22:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:22:34 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.751 | ppl 430.81 | wps 45785.5 | wpb 2034.1 | bsz 4 | num_updates 8997 | best_loss 7.602
2022-03-08 16:22:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8997 updates
2022-03-08 16:22:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 16:22:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-08 16:22:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt (epoch 46 @ 8997 updates, score 8.751) (writing took 3.5436085863038898 seconds)
2022-03-08 16:22:37 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-08 16:22:37 | INFO | train | epoch 046 | loss 4.749 | ppl 26.89 | wps 19903.3 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 8997 | lr 0.000333389 | gnorm 0.738 | loss_scale 8 | train_wall 626 | gb_free 14.1 | wall 29417
2022-03-08 16:22:37 | INFO | fairseq.trainer | begin training epoch 47
2022-03-08 16:22:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:22:47 | INFO | train_inner | epoch 047:      3 / 196 loss=4.787, ppl=27.61, wps=19658.8, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.748, loss_scale=8, train_wall=319, gb_free=14.1, wall=29427
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
