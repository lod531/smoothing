Sender: LSF System <lsfadmin@eu-g3-072>
Subject: Job 207133292: <w103_size_0.0625_fp16_label_smoothing_0.06_#1> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_label_smoothing_0.06_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:38:36 2022
Job was executed on host(s) <eu-g3-072>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 13:22:00 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 13:22:00 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.06 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   79336.92 sec.
    Max Memory :                                 8192 MB
    Average Memory :                             4191.06 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11808.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   79401 sec.
    Turnaround time :                            92806 sec.

The output (if any) follows:

2022-03-04 13:22:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.06, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 13:22:06 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 13:22:08 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 13:22:08 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 13:22:08 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 13:22:08 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 13:22:08 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 13:22:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 13:22:08 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 13:22:11 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 13:22:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 13:22:11 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-04 13:22:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 13:22:11 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 13:22:11 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 13:22:11 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 13:22:11 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 13:22:11 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 13:22:11 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 13:22:11 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 13:22:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:22:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 13:22:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:22:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:22:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:26:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:26:40 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.663 | nll_loss 14.461 | ppl 22547.1 | wps 45189.8 | wpb 510.9 | bsz 1 | num_updates 93
2022-03-04 13:26:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 93 updates
2022-03-04 13:26:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:26:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 1 @ 93 updates, score 14.663) (writing took 4.83368225209415 seconds)
2022-03-04 13:26:45 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 13:26:45 | INFO | train | epoch 001 | loss 16.036 | nll_loss 15.92 | ppl 62019.1 | wps 24545.3 | ups 0.37 | wpb 65489.7 | bsz 127.9 | num_updates 93 | lr 1.17227e-05 | gnorm 3.305 | loss_scale 8 | train_wall 243 | gb_free 21 | wall 275
2022-03-04 13:26:45 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 13:26:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:27:03 | INFO | train_inner | epoch 002:      7 / 97 loss=15.942, nll_loss=15.82, ppl=57848.5, wps=24604.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.188, loss_scale=8, train_wall=260, gb_free=21, wall=293
2022-03-04 13:28:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:30:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:30:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.123 | nll_loss 12.82 | ppl 7231.67 | wps 45033.2 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 13.123
2022-03-04 13:30:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 13:30:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:31:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 2 @ 189 updates, score 13.123) (writing took 5.185365287587047 seconds)
2022-03-04 13:31:04 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 13:31:04 | INFO | train | epoch 002 | loss 14.024 | nll_loss 13.784 | ppl 14102.4 | wps 24303.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.502 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 533
2022-03-04 13:31:04 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 13:31:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:31:32 | INFO | train_inner | epoch 003:     11 / 97 loss=13.875, nll_loss=13.624, ppl=12625, wps=24355.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.466, loss_scale=8, train_wall=236, gb_free=21, wall=562
2022-03-04 13:35:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:35:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.548 | nll_loss 11.115 | ppl 2218.04 | wps 45266.5 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.548
2022-03-04 13:35:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 13:35:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:35:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:35:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.548) (writing took 5.181021686643362 seconds)
2022-03-04 13:35:22 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 13:35:22 | INFO | train | epoch 003 | loss 12.32 | nll_loss 11.958 | ppl 3977.23 | wps 24582.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 1.054 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 792
2022-03-04 13:35:22 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 13:35:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:35:58 | INFO | train_inner | epoch 004:     14 / 97 loss=12.117, nll_loss=11.737, ppl=3412.52, wps=24610.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=0.985, loss_scale=16, train_wall=234, gb_free=21, wall=828
2022-03-04 13:39:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:39:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.815 | nll_loss 10.28 | ppl 1243.58 | wps 44411.3 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 10.815
2022-03-04 13:39:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 13:39:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:39:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:39:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 4 @ 383 updates, score 10.815) (writing took 5.2121557760983706 seconds)
2022-03-04 13:39:41 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 13:39:41 | INFO | train | epoch 004 | loss 11.106 | nll_loss 10.621 | ppl 1574.62 | wps 24567.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.556 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 1050
2022-03-04 13:39:41 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 13:39:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:40:25 | INFO | train_inner | epoch 005:     17 / 97 loss=10.996, nll_loss=10.495, ppl=1443.37, wps=24594.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.518, loss_scale=32, train_wall=234, gb_free=21, wall=1094
2022-03-04 13:43:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:43:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.471 | nll_loss 9.883 | ppl 944.01 | wps 45156.6 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 10.471
2022-03-04 13:43:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 13:43:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:43:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:43:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 5 @ 480 updates, score 10.471) (writing took 5.166188484989107 seconds)
2022-03-04 13:43:59 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 13:43:59 | INFO | train | epoch 005 | loss 10.616 | nll_loss 10.054 | ppl 1062.78 | wps 24574.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.468 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 1309
2022-03-04 13:43:59 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 13:43:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:44:51 | INFO | train_inner | epoch 006:     20 / 97 loss=10.554, nll_loss=9.982, ppl=1011.02, wps=24601.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.478, loss_scale=32, train_wall=234, gb_free=21, wall=1360
2022-03-04 13:45:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:48:13 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.185 | nll_loss 9.566 | ppl 758.14 | wps 45192.9 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 10.185
2022-03-04 13:48:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 13:48:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:48:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:48:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 6 @ 576 updates, score 10.185) (writing took 5.282091950066388 seconds)
2022-03-04 13:48:18 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 13:48:18 | INFO | train | epoch 006 | loss 10.308 | nll_loss 9.706 | ppl 835.16 | wps 24300.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.529 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 1568
2022-03-04 13:48:18 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 13:48:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:49:20 | INFO | train_inner | epoch 007:     24 / 97 loss=10.24, nll_loss=9.631, ppl=792.77, wps=24348.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.541, loss_scale=32, train_wall=236, gb_free=21, wall=1629
2022-03-04 13:51:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:52:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:52:32 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.946 | nll_loss 9.299 | ppl 629.77 | wps 45084.8 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 9.946
2022-03-04 13:52:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 13:52:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:52:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:52:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 7 @ 672 updates, score 9.946) (writing took 5.245752635411918 seconds)
2022-03-04 13:52:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 13:52:37 | INFO | train | epoch 007 | loss 10.037 | nll_loss 9.409 | ppl 679.74 | wps 24299.9 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.599 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 1826
2022-03-04 13:52:37 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 13:52:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:53:49 | INFO | train_inner | epoch 008:     28 / 97 loss=9.971, nll_loss=9.336, ppl=646.36, wps=24349.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.633, loss_scale=32, train_wall=236, gb_free=21, wall=1898
2022-03-04 13:56:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:56:50 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.735 | nll_loss 9.069 | ppl 536.94 | wps 45196.4 | wpb 510.9 | bsz 1 | num_updates 769 | best_loss 9.735
2022-03-04 13:56:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 769 updates
2022-03-04 13:56:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:56:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 13:56:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 8 @ 769 updates, score 9.735) (writing took 5.084346493706107 seconds)
2022-03-04 13:56:55 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 13:56:55 | INFO | train | epoch 008 | loss 9.795 | nll_loss 9.144 | ppl 565.7 | wps 24570.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 769 | lr 9.62058e-05 | gnorm 0.733 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 2085
2022-03-04 13:56:55 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 13:56:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:58:15 | INFO | train_inner | epoch 009:     31 / 97 loss=9.726, nll_loss=9.069, ppl=537.04, wps=24599.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.768, loss_scale=64, train_wall=234, gb_free=21, wall=2164
2022-03-04 13:58:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:01:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:01:09 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.536 | nll_loss 8.856 | ppl 463.36 | wps 44413.1 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 9.536
2022-03-04 14:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 14:01:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:01:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 9 @ 865 updates, score 9.536) (writing took 5.21748929657042 seconds)
2022-03-04 14:01:14 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 14:01:14 | INFO | train | epoch 009 | loss 9.573 | nll_loss 8.9 | ppl 477.87 | wps 24278.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.799 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 2344
2022-03-04 14:01:14 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 14:01:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:02:44 | INFO | train_inner | epoch 010:     35 / 97 loss=9.496, nll_loss=8.817, ppl=450.93, wps=24333.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.802, loss_scale=32, train_wall=236, gb_free=21, wall=2434
2022-03-04 14:04:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:05:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:05:28 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.367 | nll_loss 8.668 | ppl 406.63 | wps 45095.8 | wpb 510.9 | bsz 1 | num_updates 961 | best_loss 9.367
2022-03-04 14:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 961 updates
2022-03-04 14:05:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:05:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 10 @ 961 updates, score 9.367) (writing took 5.182024071924388 seconds)
2022-03-04 14:05:33 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 14:05:33 | INFO | train | epoch 010 | loss 9.362 | nll_loss 8.67 | ppl 407.34 | wps 24313.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 961 | lr 0.000120201 | gnorm 0.831 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 2602
2022-03-04 14:05:33 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 14:05:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:07:13 | INFO | train_inner | epoch 011:     39 / 97 loss=9.281, nll_loss=8.581, ppl=383.02, wps=24355, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.833, loss_scale=32, train_wall=236, gb_free=21, wall=2702
2022-03-04 14:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:09:47 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.202 | nll_loss 8.484 | ppl 358.11 | wps 44980.9 | wpb 510.9 | bsz 1 | num_updates 1058 | best_loss 9.202
2022-03-04 14:09:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1058 updates
2022-03-04 14:09:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:09:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:09:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 11 @ 1058 updates, score 9.202) (writing took 5.23728761728853 seconds)
2022-03-04 14:09:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 14:09:52 | INFO | train | epoch 011 | loss 9.171 | nll_loss 8.46 | ppl 352.18 | wps 24539.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1058 | lr 0.000132324 | gnorm 0.845 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 2861
2022-03-04 14:09:52 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 14:09:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:11:42 | INFO | train_inner | epoch 012:     43 / 97 loss=9.097, nll_loss=8.379, ppl=333.02, wps=24339.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.85, loss_scale=32, train_wall=236, gb_free=21, wall=2972
2022-03-04 14:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:14:05 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.069 | nll_loss 8.336 | ppl 323.12 | wps 45122.2 | wpb 510.9 | bsz 1 | num_updates 1154 | best_loss 9.069
2022-03-04 14:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1154 updates
2022-03-04 14:14:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:14:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 12 @ 1154 updates, score 9.069) (writing took 5.25051888730377 seconds)
2022-03-04 14:14:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 14:14:11 | INFO | train | epoch 012 | loss 8.996 | nll_loss 8.269 | ppl 308.41 | wps 24292 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1154 | lr 0.000144321 | gnorm 0.867 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 3120
2022-03-04 14:14:11 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 14:14:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:16:09 | INFO | train_inner | epoch 013:     46 / 97 loss=8.918, nll_loss=8.184, ppl=290.75, wps=24551.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.878, loss_scale=32, train_wall=234, gb_free=21, wall=3238
2022-03-04 14:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:18:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:18:24 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.952 | nll_loss 8.204 | ppl 294.89 | wps 44763.4 | wpb 510.9 | bsz 1 | num_updates 1250 | best_loss 8.952
2022-03-04 14:18:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1250 updates
2022-03-04 14:18:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:18:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:18:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 13 @ 1250 updates, score 8.952) (writing took 5.34956013597548 seconds)
2022-03-04 14:18:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 14:18:30 | INFO | train | epoch 013 | loss 8.834 | nll_loss 8.091 | ppl 272.65 | wps 24268.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1250 | lr 0.000156319 | gnorm 0.883 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 3379
2022-03-04 14:18:30 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 14:18:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:20:38 | INFO | train_inner | epoch 014:     50 / 97 loss=8.754, nll_loss=8.004, ppl=256.62, wps=24312, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.904, loss_scale=32, train_wall=237, gb_free=21, wall=3508
2022-03-04 14:22:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:22:44 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.846 | nll_loss 8.089 | ppl 272.22 | wps 45092.1 | wpb 510.9 | bsz 1 | num_updates 1347 | best_loss 8.846
2022-03-04 14:22:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1347 updates
2022-03-04 14:22:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:22:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 14 @ 1347 updates, score 8.846) (writing took 5.338043085299432 seconds)
2022-03-04 14:22:49 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 14:22:49 | INFO | train | epoch 014 | loss 8.68 | nll_loss 7.922 | ppl 242.53 | wps 24515.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1347 | lr 0.000168441 | gnorm 0.9 | loss_scale 64 | train_wall 227 | gb_free 21 | wall 3638
2022-03-04 14:22:49 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 14:22:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:23:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:25:08 | INFO | train_inner | epoch 015:     54 / 97 loss=8.605, nll_loss=7.839, ppl=229.03, wps=24328.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=0.919, loss_scale=32, train_wall=236, gb_free=21, wall=3777
2022-03-04 14:26:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:27:03 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.743 | nll_loss 7.979 | ppl 252.27 | wps 45310.4 | wpb 510.9 | bsz 1 | num_updates 1443 | best_loss 8.743
2022-03-04 14:27:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1443 updates
2022-03-04 14:27:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:27:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:27:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 15 @ 1443 updates, score 8.743) (writing took 5.301126281730831 seconds)
2022-03-04 14:27:08 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 14:27:08 | INFO | train | epoch 015 | loss 8.529 | nll_loss 7.757 | ppl 216.27 | wps 24275.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1443 | lr 0.000180439 | gnorm 0.918 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 3897
2022-03-04 14:27:08 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 14:27:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:29:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:29:37 | INFO | train_inner | epoch 016:     58 / 97 loss=8.438, nll_loss=7.657, ppl=201.82, wps=24321.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=0.925, loss_scale=32, train_wall=237, gb_free=21, wall=4046
2022-03-04 14:31:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:31:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.645 | nll_loss 7.855 | ppl 231.48 | wps 45066.1 | wpb 510.9 | bsz 1 | num_updates 1539 | best_loss 8.645
2022-03-04 14:31:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1539 updates
2022-03-04 14:31:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:31:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 16 @ 1539 updates, score 8.645) (writing took 5.391624967567623 seconds)
2022-03-04 14:31:27 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 14:31:27 | INFO | train | epoch 016 | loss 8.384 | nll_loss 7.597 | ppl 193.66 | wps 24251.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1539 | lr 0.000192437 | gnorm 0.946 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 4156
2022-03-04 14:31:27 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 14:31:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:34:04 | INFO | train_inner | epoch 017:     61 / 97 loss=8.303, nll_loss=7.509, ppl=182.1, wps=24525.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=0.945, loss_scale=32, train_wall=234, gb_free=21, wall=4313
2022-03-04 14:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:35:41 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.562 | nll_loss 7.766 | ppl 217.71 | wps 44130.2 | wpb 510.9 | bsz 1 | num_updates 1635 | best_loss 8.562
2022-03-04 14:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1635 updates
2022-03-04 14:35:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:35:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:35:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 17 @ 1635 updates, score 8.562) (writing took 5.347503425553441 seconds)
2022-03-04 14:35:46 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 14:35:46 | INFO | train | epoch 017 | loss 8.238 | nll_loss 7.437 | ppl 173.34 | wps 24257.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1635 | lr 0.000204434 | gnorm 0.938 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 4416
2022-03-04 14:35:46 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 14:35:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:38:33 | INFO | train_inner | epoch 018:     65 / 97 loss=8.14, nll_loss=7.331, ppl=160.96, wps=24308.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=0.928, loss_scale=32, train_wall=236, gb_free=21, wall=4583
2022-03-04 14:39:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:40:00 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.484 | nll_loss 7.676 | ppl 204.44 | wps 45138.6 | wpb 510.9 | bsz 1 | num_updates 1732 | best_loss 8.484
2022-03-04 14:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1732 updates
2022-03-04 14:40:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:40:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:40:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 18 @ 1732 updates, score 8.484) (writing took 5.277522569522262 seconds)
2022-03-04 14:40:05 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 14:40:05 | INFO | train | epoch 018 | loss 8.097 | nll_loss 7.283 | ppl 155.71 | wps 24516.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1732 | lr 0.000216557 | gnorm 0.932 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 4675
2022-03-04 14:40:05 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 14:40:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:41:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:43:03 | INFO | train_inner | epoch 019:     69 / 97 loss=8.002, nll_loss=7.178, ppl=144.83, wps=24306.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.942, loss_scale=32, train_wall=237, gb_free=21, wall=4852
2022-03-04 14:44:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:44:19 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.421 | nll_loss 7.607 | ppl 195.01 | wps 45267.6 | wpb 510.9 | bsz 1 | num_updates 1828 | best_loss 8.421
2022-03-04 14:44:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1828 updates
2022-03-04 14:44:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:44:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:44:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 19 @ 1828 updates, score 8.421) (writing took 5.302562950178981 seconds)
2022-03-04 14:44:25 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 14:44:25 | INFO | train | epoch 019 | loss 7.957 | nll_loss 7.129 | ppl 140 | wps 24253.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1828 | lr 0.000228554 | gnorm 0.946 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 4934
2022-03-04 14:44:25 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 14:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:47:30 | INFO | train_inner | epoch 020:     72 / 97 loss=7.858, nll_loss=7.02, ppl=129.77, wps=24542.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=0.948, loss_scale=64, train_wall=234, gb_free=21, wall=5119
2022-03-04 14:47:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:48:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:48:38 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.374 | nll_loss 7.546 | ppl 186.94 | wps 44993 | wpb 510.9 | bsz 1 | num_updates 1924 | best_loss 8.374
2022-03-04 14:48:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1924 updates
2022-03-04 14:48:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:48:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:48:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 20 @ 1924 updates, score 8.374) (writing took 5.336306027136743 seconds)
2022-03-04 14:48:44 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 14:48:44 | INFO | train | epoch 020 | loss 7.821 | nll_loss 6.98 | ppl 126.24 | wps 24260 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1924 | lr 0.000240552 | gnorm 0.944 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 5193
2022-03-04 14:48:44 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 14:48:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:51:59 | INFO | train_inner | epoch 021:     76 / 97 loss=7.719, nll_loss=6.867, ppl=116.76, wps=24313.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.938, loss_scale=32, train_wall=237, gb_free=21, wall=5388
2022-03-04 14:52:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:52:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.311 | nll_loss 7.482 | ppl 178.73 | wps 45104 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 8.311
2022-03-04 14:52:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 14:52:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:53:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 21 @ 2021 updates, score 8.311) (writing took 5.251270632259548 seconds)
2022-03-04 14:53:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 14:53:03 | INFO | train | epoch 021 | loss 7.692 | nll_loss 6.837 | ppl 114.36 | wps 24530.8 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 0.948 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 5452
2022-03-04 14:53:03 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 14:53:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:53:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:56:28 | INFO | train_inner | epoch 022:     80 / 97 loss=7.59, nll_loss=6.725, ppl=105.79, wps=24341.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=0.933, loss_scale=32, train_wall=236, gb_free=21, wall=5657
2022-03-04 14:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:57:16 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.279 | nll_loss 7.443 | ppl 173.97 | wps 45171.7 | wpb 510.9 | bsz 1 | num_updates 2117 | best_loss 8.279
2022-03-04 14:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2117 updates
2022-03-04 14:57:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:57:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 14:57:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 22 @ 2117 updates, score 8.279) (writing took 5.554437613114715 seconds)
2022-03-04 14:57:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 14:57:22 | INFO | train | epoch 022 | loss 7.564 | nll_loss 6.697 | ppl 103.74 | wps 24267.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2117 | lr 0.000264672 | gnorm 0.929 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 5711
2022-03-04 14:57:22 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 14:57:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:00:57 | INFO | train_inner | epoch 023:     84 / 97 loss=7.465, nll_loss=6.587, ppl=96.15, wps=24311.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=0.941, loss_scale=32, train_wall=236, gb_free=21, wall=5927
2022-03-04 15:01:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:01:35 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.262 | nll_loss 7.418 | ppl 170.96 | wps 45019.2 | wpb 510.9 | bsz 1 | num_updates 2213 | best_loss 8.262
2022-03-04 15:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2213 updates
2022-03-04 15:01:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 15:01:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 15:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 23 @ 2213 updates, score 8.262) (writing took 5.373332723043859 seconds)
2022-03-04 15:01:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 15:01:41 | INFO | train | epoch 023 | loss 7.441 | nll_loss 6.561 | ppl 94.44 | wps 24275.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2213 | lr 0.00027667 | gnorm 0.941 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 5970
2022-03-04 15:01:41 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 15:01:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:05:24 | INFO | train_inner | epoch 024:     87 / 97 loss=7.335, nll_loss=6.444, ppl=87.07, wps=24561.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.928, loss_scale=64, train_wall=234, gb_free=21, wall=6193
2022-03-04 15:05:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:05:54 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.227 | nll_loss 7.379 | ppl 166.45 | wps 44987.8 | wpb 510.9 | bsz 1 | num_updates 2309 | best_loss 8.227
2022-03-04 15:05:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2309 updates
2022-03-04 15:05:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 15:05:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 15:06:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 24 @ 2309 updates, score 8.227) (writing took 5.3102952325716615 seconds)
2022-03-04 15:06:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 15:06:00 | INFO | train | epoch 024 | loss 7.323 | nll_loss 6.43 | ppl 86.25 | wps 24285.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2309 | lr 0.000288667 | gnorm 0.926 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 6229
2022-03-04 15:06:00 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 15:06:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:09:53 | INFO | train_inner | epoch 025:     91 / 97 loss=7.218, nll_loss=6.314, ppl=79.59, wps=24329.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=0.969, loss_scale=32, train_wall=236, gb_free=21, wall=6463
2022-03-04 15:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:10:13 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.224 | nll_loss 7.373 | ppl 165.82 | wps 44884.4 | wpb 510.9 | bsz 1 | num_updates 2406 | best_loss 8.224
2022-03-04 15:10:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2406 updates
2022-03-04 15:10:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 15:10:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt
2022-03-04 15:10:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_best.pt (epoch 25 @ 2406 updates, score 8.224) (writing took 5.279827891848981 seconds)
2022-03-04 15:10:19 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 15:10:19 | INFO | train | epoch 025 | loss 7.208 | nll_loss 6.304 | ppl 79.01 | wps 24536.4 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2406 | lr 0.00030079 | gnorm 0.976 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 6488
2022-03-04 15:10:19 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 15:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:11:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:14:22 | INFO | train_inner | epoch 026:     95 / 97 loss=7.101, nll_loss=6.185, ppl=72.77, wps=24338.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=0.933, loss_scale=32, train_wall=236, gb_free=21, wall=6732
2022-03-04 15:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:14:32 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.265 | nll_loss 7.419 | ppl 171.17 | wps 44580.7 | wpb 510.9 | bsz 1 | num_updates 2502 | best_loss 8.224
2022-03-04 15:14:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2502 updates
2022-03-04 15:14:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:14:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 26 @ 2502 updates, score 8.265) (writing took 2.336018725298345 seconds)
2022-03-04 15:14:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 15:14:35 | INFO | train | epoch 026 | loss 7.092 | nll_loss 6.176 | ppl 72.29 | wps 24563.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2502 | lr 0.000312787 | gnorm 0.927 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 6744
2022-03-04 15:14:35 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 15:14:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:17:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:17:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:18:48 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.27 | nll_loss 7.423 | ppl 171.56 | wps 45532.4 | wpb 510.9 | bsz 1 | num_updates 2597 | best_loss 8.224
2022-03-04 15:18:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2597 updates
2022-03-04 15:18:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:18:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 27 @ 2597 updates, score 8.27) (writing took 2.4340364234521985 seconds)
2022-03-04 15:18:51 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 15:18:51 | INFO | train | epoch 027 | loss 6.985 | nll_loss 6.057 | ppl 66.59 | wps 24299.4 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 2597 | lr 0.00032466 | gnorm 0.968 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 7000
2022-03-04 15:18:51 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 15:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:18:58 | INFO | train_inner | epoch 028:      3 / 97 loss=6.983, nll_loss=6.055, ppl=66.48, wps=23704.5, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=0.963, loss_scale=16, train_wall=239, gb_free=21, wall=7008
2022-03-04 15:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:23:04 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.275 | nll_loss 7.429 | ppl 172.38 | wps 45179.7 | wpb 510.9 | bsz 1 | num_updates 2694 | best_loss 8.224
2022-03-04 15:23:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2694 updates
2022-03-04 15:23:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 28 @ 2694 updates, score 8.275) (writing took 2.3676168136298656 seconds)
2022-03-04 15:23:07 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 15:23:07 | INFO | train | epoch 028 | loss 6.875 | nll_loss 5.936 | ppl 61.21 | wps 24829.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2694 | lr 0.000336783 | gnorm 0.933 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 7256
2022-03-04 15:23:07 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 15:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:23:22 | INFO | train_inner | epoch 029:      6 / 97 loss=6.869, nll_loss=5.929, ppl=60.92, wps=24850.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=0.944, loss_scale=32, train_wall=234, gb_free=21, wall=7271
2022-03-04 15:27:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:27:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.298 | nll_loss 7.435 | ppl 173.03 | wps 44950.5 | wpb 510.9 | bsz 1 | num_updates 2791 | best_loss 8.224
2022-03-04 15:27:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2791 updates
2022-03-04 15:27:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:27:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:27:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 29 @ 2791 updates, score 8.298) (writing took 2.355158298276365 seconds)
2022-03-04 15:27:22 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 15:27:22 | INFO | train | epoch 029 | loss 6.772 | nll_loss 5.821 | ppl 56.54 | wps 24818.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2791 | lr 0.000348905 | gnorm 0.981 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 7512
2022-03-04 15:27:22 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 15:27:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:27:46 | INFO | train_inner | epoch 030:      9 / 97 loss=6.762, nll_loss=5.81, ppl=56.1, wps=24840.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=0.973, loss_scale=32, train_wall=234, gb_free=21, wall=7535
2022-03-04 15:29:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:31:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:31:36 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.348 | nll_loss 7.497 | ppl 180.65 | wps 45032.8 | wpb 510.9 | bsz 1 | num_updates 2887 | best_loss 8.224
2022-03-04 15:31:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2887 updates
2022-03-04 15:31:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:31:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:31:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 30 @ 2887 updates, score 8.348) (writing took 2.334192172624171 seconds)
2022-03-04 15:31:38 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 15:31:38 | INFO | train | epoch 030 | loss 6.666 | nll_loss 5.704 | ppl 52.14 | wps 24569.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2887 | lr 0.000360903 | gnorm 0.968 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 7768
2022-03-04 15:31:38 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 15:31:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:32:12 | INFO | train_inner | epoch 031:     13 / 97 loss=6.649, nll_loss=5.685, ppl=51.45, wps=24604.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=0.962, loss_scale=32, train_wall=236, gb_free=21, wall=7801
2022-03-04 15:34:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:35:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:35:52 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.35 | nll_loss 7.486 | ppl 179.26 | wps 45181.8 | wpb 510.9 | bsz 1 | num_updates 2983 | best_loss 8.224
2022-03-04 15:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2983 updates
2022-03-04 15:35:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 31 @ 2983 updates, score 8.35) (writing took 2.3402695786207914 seconds)
2022-03-04 15:35:54 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 15:35:54 | INFO | train | epoch 031 | loss 6.563 | nll_loss 5.589 | ppl 48.15 | wps 24574.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2983 | lr 0.0003729 | gnorm 0.956 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 8024
2022-03-04 15:35:54 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 15:35:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:36:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:36:40 | INFO | train_inner | epoch 032:     18 / 97 loss=6.548, nll_loss=5.573, ppl=47.6, wps=24384.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=1.006, loss_scale=16, train_wall=239, gb_free=21, wall=8070
2022-03-04 15:40:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:40:08 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.389 | nll_loss 7.532 | ppl 185.06 | wps 45135.2 | wpb 510.9 | bsz 1 | num_updates 3079 | best_loss 8.224
2022-03-04 15:40:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3079 updates
2022-03-04 15:40:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:40:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:40:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 32 @ 3079 updates, score 8.389) (writing took 2.370816906914115 seconds)
2022-03-04 15:40:10 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 15:40:10 | INFO | train | epoch 032 | loss 6.469 | nll_loss 5.485 | ppl 44.77 | wps 24575.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3079 | lr 0.000384898 | gnorm 1.01 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 8279
2022-03-04 15:40:10 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 15:40:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:41:04 | INFO | train_inner | epoch 033:     21 / 97 loss=6.444, nll_loss=5.457, ppl=43.94, wps=24849.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=0.965, loss_scale=16, train_wall=234, gb_free=21, wall=8333
2022-03-04 15:44:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:44:24 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.456 | nll_loss 7.594 | ppl 193.15 | wps 44953.7 | wpb 510.9 | bsz 1 | num_updates 3176 | best_loss 8.224
2022-03-04 15:44:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3176 updates
2022-03-04 15:44:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:44:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:44:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 33 @ 3176 updates, score 8.456) (writing took 2.387667221017182 seconds)
2022-03-04 15:44:26 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 15:44:26 | INFO | train | epoch 033 | loss 6.369 | nll_loss 5.374 | ppl 41.46 | wps 24787.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3176 | lr 0.000397021 | gnorm 0.984 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 8536
2022-03-04 15:44:26 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 15:44:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:45:28 | INFO | train_inner | epoch 034:     24 / 97 loss=6.343, nll_loss=5.345, ppl=40.65, wps=24803.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=1.003, loss_scale=32, train_wall=234, gb_free=21, wall=8597
2022-03-04 15:47:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:48:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:48:40 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.488 | nll_loss 7.634 | ppl 198.57 | wps 45082 | wpb 510.9 | bsz 1 | num_updates 3272 | best_loss 8.224
2022-03-04 15:48:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3272 updates
2022-03-04 15:48:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:48:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 34 @ 3272 updates, score 8.488) (writing took 2.3247806848958135 seconds)
2022-03-04 15:48:42 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 15:48:42 | INFO | train | epoch 034 | loss 6.271 | nll_loss 5.265 | ppl 38.44 | wps 24557.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3272 | lr 0.000409018 | gnorm 1.022 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 8792
2022-03-04 15:48:42 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 15:48:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:49:54 | INFO | train_inner | epoch 035:     28 / 97 loss=6.248, nll_loss=5.24, ppl=37.78, wps=24594, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=1.022, loss_scale=16, train_wall=237, gb_free=21, wall=8864
2022-03-04 15:52:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:52:56 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.568 | nll_loss 7.718 | ppl 210.54 | wps 45051.7 | wpb 510.9 | bsz 1 | num_updates 3369 | best_loss 8.224
2022-03-04 15:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3369 updates
2022-03-04 15:52:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:52:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 35 @ 3369 updates, score 8.568) (writing took 2.3464631401002407 seconds)
2022-03-04 15:52:58 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 15:52:58 | INFO | train | epoch 035 | loss 6.176 | nll_loss 5.16 | ppl 35.74 | wps 24818.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3369 | lr 0.000421141 | gnorm 1.012 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 9048
2022-03-04 15:52:58 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 15:52:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:54:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:54:20 | INFO | train_inner | epoch 036:     32 / 97 loss=6.14, nll_loss=5.119, ppl=34.75, wps=24604.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=1.02, loss_scale=16, train_wall=236, gb_free=21, wall=9130
2022-03-04 15:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:57:12 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.581 | nll_loss 7.717 | ppl 210.39 | wps 44289.9 | wpb 510.9 | bsz 1 | num_updates 3465 | best_loss 8.224
2022-03-04 15:57:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3465 updates
2022-03-04 15:57:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:57:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 15:57:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 36 @ 3465 updates, score 8.581) (writing took 2.3458084044978023 seconds)
2022-03-04 15:57:15 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 15:57:15 | INFO | train | epoch 036 | loss 6.084 | nll_loss 5.056 | ppl 33.27 | wps 24524.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3465 | lr 0.000433138 | gnorm 1.057 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 9304
2022-03-04 15:57:15 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 15:57:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:58:45 | INFO | train_inner | epoch 037:     35 / 97 loss=6.051, nll_loss=5.02, ppl=32.44, wps=24800.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=1.046, loss_scale=16, train_wall=234, gb_free=21, wall=9394
2022-03-04 16:01:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:01:28 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.622 | nll_loss 7.757 | ppl 216.26 | wps 44546.3 | wpb 510.9 | bsz 1 | num_updates 3562 | best_loss 8.224
2022-03-04 16:01:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3562 updates
2022-03-04 16:01:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:01:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 37 @ 3562 updates, score 8.622) (writing took 2.302503764629364 seconds)
2022-03-04 16:01:31 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 16:01:31 | INFO | train | epoch 037 | loss 5.988 | nll_loss 4.949 | ppl 30.88 | wps 24809.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3562 | lr 0.000445261 | gnorm 1.048 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 9560
2022-03-04 16:01:31 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 16:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:03:08 | INFO | train_inner | epoch 038:     38 / 97 loss=5.951, nll_loss=4.908, ppl=30.03, wps=24832.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=1.046, loss_scale=32, train_wall=234, gb_free=21, wall=9658
2022-03-04 16:03:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:05:44 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.729 | nll_loss 7.882 | ppl 235.93 | wps 44937.9 | wpb 510.9 | bsz 1 | num_updates 3658 | best_loss 8.224
2022-03-04 16:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3658 updates
2022-03-04 16:05:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:05:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:05:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 38 @ 3658 updates, score 8.729) (writing took 2.3120320178568363 seconds)
2022-03-04 16:05:47 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 16:05:47 | INFO | train | epoch 038 | loss 5.894 | nll_loss 4.844 | ppl 28.72 | wps 24566.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3658 | lr 0.000457259 | gnorm 1.033 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 9816
2022-03-04 16:05:47 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 16:05:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:07:34 | INFO | train_inner | epoch 039:     42 / 97 loss=5.858, nll_loss=4.804, ppl=27.93, wps=24614.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.057, loss_scale=16, train_wall=236, gb_free=21, wall=9924
2022-03-04 16:09:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:10:00 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.741 | nll_loss 7.872 | ppl 234.23 | wps 44964.7 | wpb 510.9 | bsz 1 | num_updates 3755 | best_loss 8.224
2022-03-04 16:10:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3755 updates
2022-03-04 16:10:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:10:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:10:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 39 @ 3755 updates, score 8.741) (writing took 2.323270780965686 seconds)
2022-03-04 16:10:03 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 16:10:03 | INFO | train | epoch 039 | loss 5.808 | nll_loss 4.748 | ppl 26.87 | wps 24824.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3755 | lr 0.000469381 | gnorm 1.086 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 10072
2022-03-04 16:10:03 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 16:10:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:11:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:12:01 | INFO | train_inner | epoch 040:     46 / 97 loss=5.763, nll_loss=4.698, ppl=25.95, wps=24592.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.086, loss_scale=16, train_wall=236, gb_free=21, wall=10190
2022-03-04 16:14:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:14:16 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.758 | nll_loss 7.875 | ppl 234.71 | wps 44947.6 | wpb 510.9 | bsz 1 | num_updates 3851 | best_loss 8.224
2022-03-04 16:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3851 updates
2022-03-04 16:14:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:14:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 40 @ 3851 updates, score 8.758) (writing took 2.3553686877712607 seconds)
2022-03-04 16:14:19 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:14:19 | INFO | train | epoch 040 | loss 5.719 | nll_loss 4.648 | ppl 25.07 | wps 24539.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3851 | lr 0.000481379 | gnorm 1.085 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 10328
2022-03-04 16:14:19 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:16:24 | INFO | train_inner | epoch 041:     49 / 97 loss=5.672, nll_loss=4.596, ppl=24.18, wps=24827.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.078, loss_scale=16, train_wall=234, gb_free=21, wall=10454
2022-03-04 16:17:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:18:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:18:32 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.82 | nll_loss 7.935 | ppl 244.8 | wps 45174.8 | wpb 510.9 | bsz 1 | num_updates 3947 | best_loss 8.224
2022-03-04 16:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3947 updates
2022-03-04 16:18:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:18:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:18:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 41 @ 3947 updates, score 8.82) (writing took 2.4160622833296657 seconds)
2022-03-04 16:18:35 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:18:35 | INFO | train | epoch 041 | loss 5.631 | nll_loss 4.549 | ppl 23.41 | wps 24567.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3947 | lr 0.000493376 | gnorm 1.086 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 10584
2022-03-04 16:18:35 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:18:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:20:51 | INFO | train_inner | epoch 042:     53 / 97 loss=5.587, nll_loss=4.5, ppl=22.62, wps=24615.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.106, loss_scale=16, train_wall=236, gb_free=21, wall=10720
2022-03-04 16:22:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:22:48 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.888 | nll_loss 8.026 | ppl 260.64 | wps 45006 | wpb 510.9 | bsz 1 | num_updates 4044 | best_loss 8.224
2022-03-04 16:22:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4044 updates
2022-03-04 16:22:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:22:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:22:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 42 @ 4044 updates, score 8.888) (writing took 2.4065833622589707 seconds)
2022-03-04 16:22:50 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 16:22:50 | INFO | train | epoch 042 | loss 5.546 | nll_loss 4.454 | ppl 21.92 | wps 24839.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4044 | lr 0.000497272 | gnorm 1.114 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 10840
2022-03-04 16:22:50 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 16:22:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:24:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:25:17 | INFO | train_inner | epoch 043:     57 / 97 loss=5.493, nll_loss=4.395, ppl=21.04, wps=24609.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.077, loss_scale=16, train_wall=236, gb_free=21, wall=10986
2022-03-04 16:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:27:04 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.987 | nll_loss 8.122 | ppl 278.65 | wps 45028.9 | wpb 510.9 | bsz 1 | num_updates 4140 | best_loss 8.224
2022-03-04 16:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4140 updates
2022-03-04 16:27:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:27:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 43 @ 4140 updates, score 8.987) (writing took 2.3739798767492175 seconds)
2022-03-04 16:27:06 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 16:27:06 | INFO | train | epoch 043 | loss 5.45 | nll_loss 4.347 | ppl 20.35 | wps 24560.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4140 | lr 0.000491473 | gnorm 1.065 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 11096
2022-03-04 16:27:06 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 16:27:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:29:40 | INFO | train_inner | epoch 044:     60 / 97 loss=5.399, nll_loss=4.289, ppl=19.55, wps=24839.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.096, loss_scale=32, train_wall=234, gb_free=21, wall=11250
2022-03-04 16:30:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:31:20 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.031 | nll_loss 8.144 | ppl 282.8 | wps 45107.6 | wpb 510.9 | bsz 1 | num_updates 4236 | best_loss 8.224
2022-03-04 16:31:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4236 updates
2022-03-04 16:31:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:31:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:31:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 44 @ 4236 updates, score 9.031) (writing took 2.3626545248553157 seconds)
2022-03-04 16:31:22 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 16:31:22 | INFO | train | epoch 044 | loss 5.362 | nll_loss 4.248 | ppl 19 | wps 24568.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4236 | lr 0.000485872 | gnorm 1.079 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 11352
2022-03-04 16:31:22 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 16:31:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:34:06 | INFO | train_inner | epoch 045:     64 / 97 loss=5.307, nll_loss=4.186, ppl=18.21, wps=24605.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=1.059, loss_scale=16, train_wall=236, gb_free=21, wall=11516
2022-03-04 16:35:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:35:36 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.087 | nll_loss 8.201 | ppl 294.33 | wps 45139.2 | wpb 510.9 | bsz 1 | num_updates 4333 | best_loss 8.224
2022-03-04 16:35:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4333 updates
2022-03-04 16:35:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:35:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 45 @ 4333 updates, score 9.087) (writing took 2.3141086464747787 seconds)
2022-03-04 16:35:38 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 16:35:38 | INFO | train | epoch 045 | loss 5.272 | nll_loss 4.147 | ppl 17.72 | wps 24827.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4333 | lr 0.000480403 | gnorm 1.06 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 11608
2022-03-04 16:35:38 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 16:35:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:36:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:38:33 | INFO | train_inner | epoch 046:     68 / 97 loss=5.213, nll_loss=4.081, ppl=16.92, wps=24617.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.061, loss_scale=16, train_wall=236, gb_free=21, wall=11782
2022-03-04 16:39:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:39:52 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.163 | nll_loss 8.288 | ppl 312.49 | wps 45170.6 | wpb 510.9 | bsz 1 | num_updates 4429 | best_loss 8.224
2022-03-04 16:39:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4429 updates
2022-03-04 16:39:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:39:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:39:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 46 @ 4429 updates, score 9.163) (writing took 2.33085223659873 seconds)
2022-03-04 16:39:54 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 16:39:54 | INFO | train | epoch 046 | loss 5.186 | nll_loss 4.051 | ppl 16.57 | wps 24587.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4429 | lr 0.000475168 | gnorm 1.087 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 11863
2022-03-04 16:39:54 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 16:39:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:42:56 | INFO | train_inner | epoch 047:     71 / 97 loss=5.126, nll_loss=3.983, ppl=15.81, wps=24855.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=1.056, loss_scale=32, train_wall=234, gb_free=21, wall=12045
2022-03-04 16:43:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:44:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:44:07 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.238 | nll_loss 8.367 | ppl 330.22 | wps 45156.3 | wpb 510.9 | bsz 1 | num_updates 4525 | best_loss 8.224
2022-03-04 16:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4525 updates
2022-03-04 16:44:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:44:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:44:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 47 @ 4525 updates, score 9.238) (writing took 2.289431158453226 seconds)
2022-03-04 16:44:10 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 16:44:10 | INFO | train | epoch 047 | loss 5.101 | nll_loss 3.955 | ppl 15.51 | wps 24575.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4525 | lr 0.0004701 | gnorm 1.049 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 12119
2022-03-04 16:44:10 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 16:44:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:47:22 | INFO | train_inner | epoch 048:     75 / 97 loss=5.043, nll_loss=3.89, ppl=14.83, wps=24614, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.078, loss_scale=16, train_wall=236, gb_free=21, wall=12311
2022-03-04 16:48:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:48:23 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.3 | nll_loss 8.423 | ppl 343.17 | wps 44310.2 | wpb 510.9 | bsz 1 | num_updates 4622 | best_loss 8.224
2022-03-04 16:48:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4622 updates
2022-03-04 16:48:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:48:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:48:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 48 @ 4622 updates, score 9.3) (writing took 2.3613916924223304 seconds)
2022-03-04 16:48:26 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 16:48:26 | INFO | train | epoch 048 | loss 5.025 | nll_loss 3.869 | ppl 14.61 | wps 24817.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4622 | lr 0.000465141 | gnorm 1.073 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 12375
2022-03-04 16:48:26 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 16:48:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:49:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:51:48 | INFO | train_inner | epoch 049:     79 / 97 loss=4.97, nll_loss=3.807, ppl=14, wps=24600.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=1.052, loss_scale=16, train_wall=236, gb_free=21, wall=12578
2022-03-04 16:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:52:39 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.413 | nll_loss 8.535 | ppl 370.88 | wps 44963.2 | wpb 510.9 | bsz 1 | num_updates 4718 | best_loss 8.224
2022-03-04 16:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4718 updates
2022-03-04 16:52:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:52:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 49 @ 4718 updates, score 9.413) (writing took 2.338321358896792 seconds)
2022-03-04 16:52:42 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 16:52:42 | INFO | train | epoch 049 | loss 4.944 | nll_loss 3.778 | ppl 13.72 | wps 24567.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4718 | lr 0.000460385 | gnorm 1.036 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 12631
2022-03-04 16:52:42 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 16:52:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:56:14 | INFO | train_inner | epoch 050:     83 / 97 loss=4.887, nll_loss=3.714, ppl=13.13, wps=24620.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=1.066, loss_scale=16, train_wall=236, gb_free=21, wall=12844
2022-03-04 16:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:56:55 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.447 | nll_loss 8.565 | ppl 378.74 | wps 44935.4 | wpb 510.9 | bsz 1 | num_updates 4814 | best_loss 8.224
2022-03-04 16:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4814 updates
2022-03-04 16:56:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 16:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 50 @ 4814 updates, score 9.447) (writing took 2.314891841262579 seconds)
2022-03-04 16:56:57 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 16:56:57 | INFO | train | epoch 050 | loss 4.874 | nll_loss 3.699 | ppl 12.99 | wps 24590.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4814 | lr 0.000455771 | gnorm 1.069 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 12887
2022-03-04 16:56:57 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 16:56:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:00:38 | INFO | train_inner | epoch 051:     86 / 97 loss=4.81, nll_loss=3.627, ppl=12.35, wps=24867.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.048, loss_scale=16, train_wall=234, gb_free=21, wall=13107
2022-03-04 17:01:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:01:11 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.54 | nll_loss 8.674 | ppl 408.42 | wps 45548.8 | wpb 510.9 | bsz 1 | num_updates 4911 | best_loss 8.224
2022-03-04 17:01:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4911 updates
2022-03-04 17:01:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:01:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 51 @ 4911 updates, score 9.54) (writing took 2.3410999979823828 seconds)
2022-03-04 17:01:13 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 17:01:13 | INFO | train | epoch 051 | loss 4.8 | nll_loss 3.616 | ppl 12.26 | wps 24846.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4911 | lr 0.000451248 | gnorm 1.049 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 13142
2022-03-04 17:01:13 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 17:01:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:02:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:05:04 | INFO | train_inner | epoch 052:     90 / 97 loss=4.74, nll_loss=3.549, ppl=11.7, wps=24619.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1.045, loss_scale=16, train_wall=236, gb_free=21, wall=13373
2022-03-04 17:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:05:26 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.657 | nll_loss 8.8 | ppl 445.78 | wps 45302.7 | wpb 510.9 | bsz 1 | num_updates 5007 | best_loss 8.224
2022-03-04 17:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5007 updates
2022-03-04 17:05:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:05:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 52 @ 5007 updates, score 9.657) (writing took 2.3558617373928428 seconds)
2022-03-04 17:05:29 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 17:05:29 | INFO | train | epoch 052 | loss 4.729 | nll_loss 3.537 | ppl 11.6 | wps 24579.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5007 | lr 0.000446901 | gnorm 1.046 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 13398
2022-03-04 17:05:29 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 17:05:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:09:28 | INFO | train_inner | epoch 053:     93 / 97 loss=4.671, nll_loss=3.47, ppl=11.08, wps=24818.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=1.059, loss_scale=32, train_wall=234, gb_free=21, wall=13637
2022-03-04 17:09:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:09:43 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.738 | nll_loss 8.864 | ppl 465.91 | wps 44889.2 | wpb 510.9 | bsz 1 | num_updates 5104 | best_loss 8.224
2022-03-04 17:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5104 updates
2022-03-04 17:09:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:09:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:09:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 53 @ 5104 updates, score 9.738) (writing took 2.334091487340629 seconds)
2022-03-04 17:09:45 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 17:09:45 | INFO | train | epoch 053 | loss 4.664 | nll_loss 3.463 | ppl 11.02 | wps 24787.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5104 | lr 0.000442634 | gnorm 1.06 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 13654
2022-03-04 17:09:45 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 17:09:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:10:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:13:54 | INFO | train_inner | epoch 054:     97 / 97 loss=4.604, nll_loss=3.395, ppl=10.52, wps=24588.2, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=5200, lr=0.000438529, gnorm=1.057, loss_scale=16, train_wall=236, gb_free=21, wall=13903
2022-03-04 17:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:13:59 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.807 | nll_loss 8.943 | ppl 492.25 | wps 44963.9 | wpb 510.9 | bsz 1 | num_updates 5200 | best_loss 8.224
2022-03-04 17:13:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5200 updates
2022-03-04 17:13:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:14:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:14:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 54 @ 5200 updates, score 9.807) (writing took 2.333815911784768 seconds)
2022-03-04 17:14:01 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 17:14:01 | INFO | train | epoch 054 | loss 4.598 | nll_loss 3.388 | ppl 10.47 | wps 24558.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5200 | lr 0.000438529 | gnorm 1.058 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 13910
2022-03-04 17:14:01 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 17:14:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:15:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:18:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:18:15 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.852 | nll_loss 8.987 | ppl 507.26 | wps 44955.3 | wpb 510.9 | bsz 1 | num_updates 5296 | best_loss 8.224
2022-03-04 17:18:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5296 updates
2022-03-04 17:18:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:18:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:18:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 55 @ 5296 updates, score 9.852) (writing took 2.338538204319775 seconds)
2022-03-04 17:18:17 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 17:18:17 | INFO | train | epoch 055 | loss 4.536 | nll_loss 3.319 | ppl 9.98 | wps 24574.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5296 | lr 0.000434536 | gnorm 1.064 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 14166
2022-03-04 17:18:17 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 17:18:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:18:27 | INFO | train_inner | epoch 056:      4 / 97 loss=4.53, nll_loss=3.311, ppl=9.93, wps=23951.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5300, lr=0.000434372, gnorm=1.063, loss_scale=16, train_wall=236, gb_free=21, wall=14177
2022-03-04 17:21:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:22:30 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.956 | nll_loss 9.108 | ppl 551.97 | wps 45367.2 | wpb 510.9 | bsz 1 | num_updates 5392 | best_loss 8.224
2022-03-04 17:22:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5392 updates
2022-03-04 17:22:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:22:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:22:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 56 @ 5392 updates, score 9.956) (writing took 2.4020368149504066 seconds)
2022-03-04 17:22:33 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 17:22:33 | INFO | train | epoch 056 | loss 4.477 | nll_loss 3.252 | ppl 9.53 | wps 24576.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5392 | lr 0.000430651 | gnorm 1.057 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 14422
2022-03-04 17:22:33 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 17:22:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:22:53 | INFO | train_inner | epoch 057:      8 / 97 loss=4.47, nll_loss=3.244, ppl=9.47, wps=24612.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=1.063, loss_scale=16, train_wall=236, gb_free=21, wall=14443
2022-03-04 17:26:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:26:46 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.996 | nll_loss 9.13 | ppl 560.21 | wps 45053.7 | wpb 510.9 | bsz 1 | num_updates 5489 | best_loss 8.224
2022-03-04 17:26:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5489 updates
2022-03-04 17:26:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:26:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:26:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 57 @ 5489 updates, score 9.996) (writing took 2.3463584864512086 seconds)
2022-03-04 17:26:49 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 17:26:49 | INFO | train | epoch 057 | loss 4.421 | nll_loss 3.188 | ppl 9.11 | wps 24816.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5489 | lr 0.000426828 | gnorm 1.054 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 14678
2022-03-04 17:26:49 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 17:26:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:27:17 | INFO | train_inner | epoch 058:     11 / 97 loss=4.41, nll_loss=3.177, ppl=9.04, wps=24843.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.045, loss_scale=16, train_wall=234, gb_free=21, wall=14706
2022-03-04 17:27:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:30:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:31:02 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 10.056 | nll_loss 9.209 | ppl 591.9 | wps 44473.6 | wpb 510.9 | bsz 1 | num_updates 5585 | best_loss 8.224
2022-03-04 17:31:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5585 updates
2022-03-04 17:31:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:31:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:31:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 58 @ 5585 updates, score 10.056) (writing took 2.2982367146760225 seconds)
2022-03-04 17:31:04 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 17:31:04 | INFO | train | epoch 058 | loss 4.362 | nll_loss 3.122 | ppl 8.71 | wps 24578.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5585 | lr 0.000423144 | gnorm 1.058 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 14934
2022-03-04 17:31:05 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 17:31:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:31:43 | INFO | train_inner | epoch 059:     15 / 97 loss=4.354, nll_loss=3.113, ppl=8.65, wps=24610.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.06, loss_scale=16, train_wall=236, gb_free=21, wall=14972
2022-03-04 17:33:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:35:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:35:18 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 10.168 | nll_loss 9.321 | ppl 639.69 | wps 45144.5 | wpb 510.9 | bsz 1 | num_updates 5681 | best_loss 8.224
2022-03-04 17:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5681 updates
2022-03-04 17:35:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:35:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:35:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 59 @ 5681 updates, score 10.168) (writing took 2.3694017259404063 seconds)
2022-03-04 17:35:20 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 17:35:20 | INFO | train | epoch 059 | loss 4.31 | nll_loss 3.063 | ppl 8.36 | wps 24583 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5681 | lr 0.000419554 | gnorm 1.07 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 15190
2022-03-04 17:35:20 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 17:35:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:36:09 | INFO | train_inner | epoch 060:     19 / 97 loss=4.297, nll_loss=3.049, ppl=8.28, wps=24628.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.069, loss_scale=16, train_wall=236, gb_free=21, wall=15238
2022-03-04 17:39:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:39:34 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 10.247 | nll_loss 9.395 | ppl 673.08 | wps 45135.7 | wpb 510.9 | bsz 1 | num_updates 5778 | best_loss 8.224
2022-03-04 17:39:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5778 updates
2022-03-04 17:39:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:39:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:39:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 60 @ 5778 updates, score 10.247) (writing took 2.341914089396596 seconds)
2022-03-04 17:39:36 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 17:39:36 | INFO | train | epoch 060 | loss 4.258 | nll_loss 3.004 | ppl 8.02 | wps 24842.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5778 | lr 0.000416017 | gnorm 1.07 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 15445
2022-03-04 17:39:36 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 17:39:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:40:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:40:35 | INFO | train_inner | epoch 061:     23 / 97 loss=4.245, nll_loss=2.99, ppl=7.94, wps=24621.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.069, loss_scale=16, train_wall=236, gb_free=21, wall=15504
2022-03-04 17:43:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:43:49 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 10.291 | nll_loss 9.441 | ppl 694.95 | wps 45040.3 | wpb 510.9 | bsz 1 | num_updates 5874 | best_loss 8.224
2022-03-04 17:43:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5874 updates
2022-03-04 17:43:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:43:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:43:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 61 @ 5874 updates, score 10.291) (writing took 2.342202827334404 seconds)
2022-03-04 17:43:52 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 17:43:52 | INFO | train | epoch 061 | loss 4.204 | nll_loss 2.943 | ppl 7.69 | wps 24575.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5874 | lr 0.000412604 | gnorm 1.067 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 15701
2022-03-04 17:43:52 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 17:43:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:44:59 | INFO | train_inner | epoch 062:     26 / 97 loss=4.19, nll_loss=2.928, ppl=7.61, wps=24853.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.064, loss_scale=16, train_wall=234, gb_free=21, wall=15768
2022-03-04 17:46:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:48:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:48:05 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 10.371 | nll_loss 9.524 | ppl 736.37 | wps 45192.5 | wpb 510.9 | bsz 1 | num_updates 5970 | best_loss 8.224
2022-03-04 17:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5970 updates
2022-03-04 17:48:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:48:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 62 @ 5970 updates, score 10.371) (writing took 2.365674418397248 seconds)
2022-03-04 17:48:08 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 17:48:08 | INFO | train | epoch 062 | loss 4.156 | nll_loss 2.89 | ppl 7.41 | wps 24568.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5970 | lr 0.000409273 | gnorm 1.05 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 15957
2022-03-04 17:48:08 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 17:48:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:49:25 | INFO | train_inner | epoch 063:     30 / 97 loss=4.139, nll_loss=2.87, ppl=7.31, wps=24609.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.061, loss_scale=16, train_wall=236, gb_free=21, wall=16034
2022-03-04 17:52:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:52:21 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 10.46 | nll_loss 9.623 | ppl 788.55 | wps 45122.8 | wpb 510.9 | bsz 1 | num_updates 6067 | best_loss 8.224
2022-03-04 17:52:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6067 updates
2022-03-04 17:52:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:52:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:52:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 63 @ 6067 updates, score 10.46) (writing took 2.2841127328574657 seconds)
2022-03-04 17:52:23 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 17:52:23 | INFO | train | epoch 063 | loss 4.111 | nll_loss 2.838 | ppl 7.15 | wps 24834.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6067 | lr 0.000405988 | gnorm 1.061 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 16213
2022-03-04 17:52:24 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 17:52:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:52:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:53:51 | INFO | train_inner | epoch 064:     34 / 97 loss=4.095, nll_loss=2.821, ppl=7.06, wps=24602, ups=0.38, wpb=65495, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.056, loss_scale=16, train_wall=237, gb_free=21, wall=16300
2022-03-04 17:56:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:56:37 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10.499 | nll_loss 9.65 | ppl 803.29 | wps 44643.4 | wpb 510.9 | bsz 1 | num_updates 6163 | best_loss 8.224
2022-03-04 17:56:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6163 updates
2022-03-04 17:56:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:56:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 17:56:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 64 @ 6163 updates, score 10.499) (writing took 2.2221900457516313 seconds)
2022-03-04 17:56:40 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 17:56:40 | INFO | train | epoch 064 | loss 4.066 | nll_loss 2.787 | ppl 6.9 | wps 24558 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6163 | lr 0.000402813 | gnorm 1.083 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 16469
2022-03-04 17:56:40 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 17:56:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:58:15 | INFO | train_inner | epoch 065:     37 / 97 loss=4.047, nll_loss=2.766, ppl=6.8, wps=24839.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.077, loss_scale=16, train_wall=234, gb_free=21, wall=16564
2022-03-04 17:59:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:00:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:00:53 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 10.52 | nll_loss 9.687 | ppl 824.14 | wps 44957.2 | wpb 510.9 | bsz 1 | num_updates 6259 | best_loss 8.224
2022-03-04 18:00:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6259 updates
2022-03-04 18:00:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:00:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:00:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 65 @ 6259 updates, score 10.52) (writing took 2.250291529111564 seconds)
2022-03-04 18:00:55 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 18:00:55 | INFO | train | epoch 065 | loss 4.02 | nll_loss 2.735 | ppl 6.66 | wps 24569.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6259 | lr 0.000399712 | gnorm 1.061 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 16725
2022-03-04 18:00:55 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 18:00:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:02:41 | INFO | train_inner | epoch 066:     41 / 97 loss=4.002, nll_loss=2.715, ppl=6.57, wps=24609.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.057, loss_scale=16, train_wall=236, gb_free=21, wall=16830
2022-03-04 18:04:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:05:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:05:09 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 10.645 | nll_loss 9.801 | ppl 891.78 | wps 45594.4 | wpb 510.9 | bsz 1 | num_updates 6355 | best_loss 8.224
2022-03-04 18:05:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6355 updates
2022-03-04 18:05:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:05:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 66 @ 6355 updates, score 10.645) (writing took 2.2423968063667417 seconds)
2022-03-04 18:05:11 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 18:05:11 | INFO | train | epoch 066 | loss 3.98 | nll_loss 2.69 | ppl 6.45 | wps 24576.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6355 | lr 0.000396682 | gnorm 1.062 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 16981
2022-03-04 18:05:11 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 18:05:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:07:07 | INFO | train_inner | epoch 067:     45 / 97 loss=3.962, nll_loss=2.67, ppl=6.37, wps=24623.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.082, loss_scale=16, train_wall=236, gb_free=21, wall=17096
2022-03-04 18:09:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:09:25 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.667 | nll_loss 9.818 | ppl 902.58 | wps 43038.6 | wpb 510.9 | bsz 1 | num_updates 6452 | best_loss 8.224
2022-03-04 18:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6452 updates
2022-03-04 18:09:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:09:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 67 @ 6452 updates, score 10.667) (writing took 2.2621360504999757 seconds)
2022-03-04 18:09:27 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 18:09:27 | INFO | train | epoch 067 | loss 3.94 | nll_loss 2.645 | ppl 6.26 | wps 24811.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6452 | lr 0.000393689 | gnorm 1.081 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 17237
2022-03-04 18:09:27 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 18:09:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:11:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:11:33 | INFO | train_inner | epoch 068:     49 / 97 loss=3.922, nll_loss=2.625, ppl=6.17, wps=24588.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1.056, loss_scale=16, train_wall=236, gb_free=21, wall=17362
2022-03-04 18:13:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:13:41 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.754 | nll_loss 9.922 | ppl 970.18 | wps 45269.5 | wpb 510.9 | bsz 1 | num_updates 6548 | best_loss 8.224
2022-03-04 18:13:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6548 updates
2022-03-04 18:13:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:13:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:13:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 68 @ 6548 updates, score 10.754) (writing took 2.4471491975709796 seconds)
2022-03-04 18:13:43 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 18:13:43 | INFO | train | epoch 068 | loss 3.898 | nll_loss 2.597 | ppl 6.05 | wps 24557.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6548 | lr 0.000390792 | gnorm 1.044 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 17493
2022-03-04 18:13:43 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 18:13:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:15:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:15:59 | INFO | train_inner | epoch 069:     53 / 97 loss=3.881, nll_loss=2.579, ppl=5.97, wps=24600.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.053, loss_scale=8, train_wall=236, gb_free=21, wall=17629
2022-03-04 18:17:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:17:57 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.811 | nll_loss 9.973 | ppl 1005.11 | wps 45113.5 | wpb 510.9 | bsz 1 | num_updates 6644 | best_loss 8.224
2022-03-04 18:17:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6644 updates
2022-03-04 18:17:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:17:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:17:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 69 @ 6644 updates, score 10.811) (writing took 2.364278845489025 seconds)
2022-03-04 18:17:59 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 18:17:59 | INFO | train | epoch 069 | loss 3.861 | nll_loss 2.556 | ppl 5.88 | wps 24562.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6644 | lr 0.000387958 | gnorm 1.062 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 17749
2022-03-04 18:17:59 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 18:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:20:23 | INFO | train_inner | epoch 070:     56 / 97 loss=3.838, nll_loss=2.529, ppl=5.77, wps=24830, ups=0.38, wpb=65495, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.071, loss_scale=8, train_wall=234, gb_free=21, wall=17892
2022-03-04 18:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:22:13 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.859 | nll_loss 10.014 | ppl 1033.79 | wps 44988 | wpb 510.9 | bsz 1 | num_updates 6741 | best_loss 8.224
2022-03-04 18:22:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6741 updates
2022-03-04 18:22:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:22:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:22:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 70 @ 6741 updates, score 10.859) (writing took 2.399060158059001 seconds)
2022-03-04 18:22:15 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 18:22:15 | INFO | train | epoch 070 | loss 3.825 | nll_loss 2.515 | ppl 5.72 | wps 24807.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6741 | lr 0.000385157 | gnorm 1.069 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 18005
2022-03-04 18:22:15 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 18:22:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:24:47 | INFO | train_inner | epoch 071:     59 / 97 loss=3.806, nll_loss=2.494, ppl=5.63, wps=24828.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.058, loss_scale=16, train_wall=234, gb_free=21, wall=18156
2022-03-04 18:26:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:26:29 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.9 | nll_loss 10.069 | ppl 1074.43 | wps 44226.6 | wpb 510.9 | bsz 1 | num_updates 6838 | best_loss 8.224
2022-03-04 18:26:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6838 updates
2022-03-04 18:26:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:26:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:26:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 71 @ 6838 updates, score 10.9) (writing took 2.3647934263572097 seconds)
2022-03-04 18:26:31 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 18:26:31 | INFO | train | epoch 071 | loss 3.787 | nll_loss 2.472 | ppl 5.55 | wps 24803.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6838 | lr 0.000382415 | gnorm 1.06 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 18261
2022-03-04 18:26:31 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 18:26:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:27:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:29:13 | INFO | train_inner | epoch 072:     63 / 97 loss=3.761, nll_loss=2.444, ppl=5.44, wps=24590.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.061, loss_scale=16, train_wall=236, gb_free=21, wall=18422
2022-03-04 18:29:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:30:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:30:45 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.955 | nll_loss 10.13 | ppl 1120.56 | wps 45005.8 | wpb 510.9 | bsz 1 | num_updates 6933 | best_loss 8.224
2022-03-04 18:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6933 updates
2022-03-04 18:30:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 72 @ 6933 updates, score 10.955) (writing took 2.4493695264682174 seconds)
2022-03-04 18:30:47 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 18:30:47 | INFO | train | epoch 072 | loss 3.751 | nll_loss 2.432 | ppl 5.39 | wps 24298.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 6933 | lr 0.000379786 | gnorm 1.084 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 18517
2022-03-04 18:30:48 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 18:30:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:33:40 | INFO | train_inner | epoch 073:     67 / 97 loss=3.729, nll_loss=2.407, ppl=5.3, wps=24568.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.078, loss_scale=8, train_wall=237, gb_free=21, wall=18689
2022-03-04 18:34:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:35:01 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 11.004 | nll_loss 10.175 | ppl 1155.88 | wps 45113.5 | wpb 510.9 | bsz 1 | num_updates 7030 | best_loss 8.224
2022-03-04 18:35:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7030 updates
2022-03-04 18:35:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:35:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:35:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 73 @ 7030 updates, score 11.004) (writing took 2.2777792923152447 seconds)
2022-03-04 18:35:04 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 18:35:04 | INFO | train | epoch 073 | loss 3.718 | nll_loss 2.395 | ppl 5.26 | wps 24792.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7030 | lr 0.000377157 | gnorm 1.058 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 18773
2022-03-04 18:35:04 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 18:35:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:38:03 | INFO | train_inner | epoch 074:     70 / 97 loss=3.704, nll_loss=2.378, ppl=5.2, wps=24841.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.064, loss_scale=16, train_wall=234, gb_free=21, wall=18953
2022-03-04 18:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:39:17 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 11.048 | nll_loss 10.221 | ppl 1193.11 | wps 45041.3 | wpb 510.9 | bsz 1 | num_updates 7127 | best_loss 8.224
2022-03-04 18:39:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7127 updates
2022-03-04 18:39:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:39:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 74 @ 7127 updates, score 11.048) (writing took 2.415064270608127 seconds)
2022-03-04 18:39:20 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 18:39:20 | INFO | train | epoch 074 | loss 3.687 | nll_loss 2.36 | ppl 5.13 | wps 24815.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7127 | lr 0.000374582 | gnorm 1.064 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 19029
2022-03-04 18:39:20 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 18:39:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:41:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:42:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:42:32 | INFO | train_inner | epoch 075:     75 / 97 loss=3.662, nll_loss=2.332, ppl=5.03, wps=24357, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.077, loss_scale=8, train_wall=239, gb_free=21, wall=19222
2022-03-04 18:43:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:43:33 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 11.107 | nll_loss 10.285 | ppl 1247.24 | wps 44951.3 | wpb 510.9 | bsz 1 | num_updates 7222 | best_loss 8.224
2022-03-04 18:43:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7222 updates
2022-03-04 18:43:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:43:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:43:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 75 @ 7222 updates, score 11.107) (writing took 2.350505749695003 seconds)
2022-03-04 18:43:36 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 18:43:36 | INFO | train | epoch 075 | loss 3.652 | nll_loss 2.321 | ppl 5 | wps 24302.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 7222 | lr 0.00037211 | gnorm 1.065 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 19285
2022-03-04 18:43:36 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 18:43:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:46:56 | INFO | train_inner | epoch 076:     78 / 97 loss=3.631, nll_loss=2.297, ppl=4.91, wps=24844.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.055, loss_scale=8, train_wall=234, gb_free=21, wall=19485
2022-03-04 18:47:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:47:49 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 11.126 | nll_loss 10.306 | ppl 1265.55 | wps 45110.1 | wpb 510.9 | bsz 1 | num_updates 7319 | best_loss 8.224
2022-03-04 18:47:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7319 updates
2022-03-04 18:47:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:47:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:47:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 76 @ 7319 updates, score 11.126) (writing took 2.3106673350557685 seconds)
2022-03-04 18:47:52 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 18:47:52 | INFO | train | epoch 076 | loss 3.624 | nll_loss 2.289 | ppl 4.89 | wps 24831.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7319 | lr 0.000369636 | gnorm 1.057 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 19541
2022-03-04 18:47:52 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 18:47:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:49:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:51:22 | INFO | train_inner | epoch 077:     82 / 97 loss=3.6, nll_loss=2.262, ppl=4.8, wps=24605.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.082, loss_scale=8, train_wall=236, gb_free=21, wall=19751
2022-03-04 18:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:52:05 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 11.251 | nll_loss 10.442 | ppl 1391.21 | wps 45042 | wpb 510.9 | bsz 1 | num_updates 7415 | best_loss 8.224
2022-03-04 18:52:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7415 updates
2022-03-04 18:52:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:52:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 77 @ 7415 updates, score 11.251) (writing took 2.3332699397578835 seconds)
2022-03-04 18:52:08 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 18:52:08 | INFO | train | epoch 077 | loss 3.595 | nll_loss 2.256 | ppl 4.78 | wps 24561.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7415 | lr 0.000367235 | gnorm 1.08 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 19797
2022-03-04 18:52:08 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 18:52:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:55:45 | INFO | train_inner | epoch 078:     85 / 97 loss=3.572, nll_loss=2.23, ppl=4.69, wps=24857.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.047, loss_scale=16, train_wall=234, gb_free=21, wall=20015
2022-03-04 18:56:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:56:21 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 11.31 | nll_loss 10.49 | ppl 1438.18 | wps 45020.6 | wpb 510.9 | bsz 1 | num_updates 7512 | best_loss 8.224
2022-03-04 18:56:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7512 updates
2022-03-04 18:56:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:56:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 18:56:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 78 @ 7512 updates, score 11.31) (writing took 2.315222254022956 seconds)
2022-03-04 18:56:23 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 18:56:23 | INFO | train | epoch 078 | loss 3.565 | nll_loss 2.222 | ppl 4.67 | wps 24837.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7512 | lr 0.000364857 | gnorm 1.06 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 20053
2022-03-04 18:56:23 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 18:56:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:00:09 | INFO | train_inner | epoch 079:     88 / 97 loss=3.543, nll_loss=2.197, ppl=4.59, wps=24850.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.081, loss_scale=16, train_wall=234, gb_free=21, wall=20278
2022-03-04 19:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:00:37 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 11.329 | nll_loss 10.511 | ppl 1458.81 | wps 44571.5 | wpb 510.9 | bsz 1 | num_updates 7609 | best_loss 8.224
2022-03-04 19:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7609 updates
2022-03-04 19:00:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 79 @ 7609 updates, score 11.329) (writing took 2.3136682398617268 seconds)
2022-03-04 19:00:39 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 19:00:39 | INFO | train | epoch 079 | loss 3.537 | nll_loss 2.191 | ppl 4.57 | wps 24824.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7609 | lr 0.000362524 | gnorm 1.069 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 20309
2022-03-04 19:00:39 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 19:00:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:01:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:04:35 | INFO | train_inner | epoch 080:     92 / 97 loss=3.514, nll_loss=2.165, ppl=4.48, wps=24605, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.057, loss_scale=16, train_wall=236, gb_free=21, wall=20545
2022-03-04 19:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:04:53 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 11.339 | nll_loss 10.526 | ppl 1474.68 | wps 44981.3 | wpb 510.9 | bsz 1 | num_updates 7705 | best_loss 8.224
2022-03-04 19:04:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7705 updates
2022-03-04 19:04:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 80 @ 7705 updates, score 11.339) (writing took 2.297464292496443 seconds)
2022-03-04 19:04:55 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 19:04:55 | INFO | train | epoch 080 | loss 3.509 | nll_loss 2.159 | ppl 4.47 | wps 24572.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7705 | lr 0.000360258 | gnorm 1.06 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 20564
2022-03-04 19:04:55 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 19:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:07:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:09:01 | INFO | train_inner | epoch 081:     96 / 97 loss=3.488, nll_loss=2.135, ppl=4.39, wps=24613.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7800, lr=0.000358057, gnorm=1.046, loss_scale=16, train_wall=236, gb_free=21, wall=20811
2022-03-04 19:09:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:09 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 11.326 | nll_loss 10.516 | ppl 1464.24 | wps 45030.2 | wpb 510.9 | bsz 1 | num_updates 7801 | best_loss 8.224
2022-03-04 19:09:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7801 updates
2022-03-04 19:09:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 81 @ 7801 updates, score 11.326) (writing took 2.3138847704976797 seconds)
2022-03-04 19:09:11 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 19:09:11 | INFO | train | epoch 081 | loss 3.483 | nll_loss 2.131 | ppl 4.38 | wps 24572.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7801 | lr 0.000358034 | gnorm 1.045 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 20820
2022-03-04 19:09:11 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 19:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:11:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:13:24 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 11.368 | nll_loss 10.555 | ppl 1504.41 | wps 45106.3 | wpb 510.9 | bsz 1 | num_updates 7897 | best_loss 8.224
2022-03-04 19:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7897 updates
2022-03-04 19:13:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:13:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 82 @ 7897 updates, score 11.368) (writing took 2.356753943488002 seconds)
2022-03-04 19:13:27 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 19:13:27 | INFO | train | epoch 082 | loss 3.458 | nll_loss 2.101 | ppl 4.29 | wps 24575.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 7897 | lr 0.000355852 | gnorm 1.056 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 21076
2022-03-04 19:13:27 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 19:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:35 | INFO | train_inner | epoch 083:      3 / 97 loss=3.456, nll_loss=2.1, ppl=4.29, wps=23951, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=7900, lr=0.000355784, gnorm=1.056, loss_scale=8, train_wall=236, gb_free=21, wall=21084
2022-03-04 19:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:17:40 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 11.484 | nll_loss 10.674 | ppl 1633.83 | wps 44901.4 | wpb 510.9 | bsz 1 | num_updates 7994 | best_loss 8.224
2022-03-04 19:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7994 updates
2022-03-04 19:17:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 83 @ 7994 updates, score 11.484) (writing took 2.338191925548017 seconds)
2022-03-04 19:17:43 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 19:17:43 | INFO | train | epoch 083 | loss 3.433 | nll_loss 2.074 | ppl 4.21 | wps 24824.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7994 | lr 0.000353686 | gnorm 1.06 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 21332
2022-03-04 19:17:43 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 19:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:17:58 | INFO | train_inner | epoch 084:      6 / 97 loss=3.429, nll_loss=2.069, ppl=4.2, wps=24845.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8000, lr=0.000353553, gnorm=1.06, loss_scale=16, train_wall=234, gb_free=21, wall=21348
2022-03-04 19:21:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:21:56 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 11.503 | nll_loss 10.7 | ppl 1662.93 | wps 45108.4 | wpb 510.9 | bsz 1 | num_updates 8091 | best_loss 8.224
2022-03-04 19:21:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8091 updates
2022-03-04 19:21:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:21:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 84 @ 8091 updates, score 11.503) (writing took 2.306958105415106 seconds)
2022-03-04 19:21:59 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 19:21:59 | INFO | train | epoch 084 | loss 3.409 | nll_loss 2.047 | ppl 4.13 | wps 24826.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8091 | lr 0.00035156 | gnorm 1.067 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 21588
2022-03-04 19:21:59 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 19:21:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:22:22 | INFO | train_inner | epoch 085:      9 / 97 loss=3.405, nll_loss=2.042, ppl=4.12, wps=24850.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.067, loss_scale=16, train_wall=234, gb_free=21, wall=21611
2022-03-04 19:22:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:26:12 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 11.565 | nll_loss 10.761 | ppl 1735.72 | wps 45057.5 | wpb 510.9 | bsz 1 | num_updates 8187 | best_loss 8.224
2022-03-04 19:26:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8187 updates
2022-03-04 19:26:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:26:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:26:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 85 @ 8187 updates, score 11.565) (writing took 2.289594255387783 seconds)
2022-03-04 19:26:14 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 19:26:14 | INFO | train | epoch 085 | loss 3.384 | nll_loss 2.019 | ppl 4.05 | wps 24572 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8187 | lr 0.000349492 | gnorm 1.062 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 21844
2022-03-04 19:26:14 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 19:26:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:26:48 | INFO | train_inner | epoch 086:     13 / 97 loss=3.378, nll_loss=2.013, ppl=4.04, wps=24611.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.062, loss_scale=16, train_wall=236, gb_free=21, wall=21877
2022-03-04 19:26:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:30:28 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 11.624 | nll_loss 10.828 | ppl 1817.92 | wps 44978.3 | wpb 510.9 | bsz 1 | num_updates 8283 | best_loss 8.224
2022-03-04 19:30:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8283 updates
2022-03-04 19:30:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:30:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 86 @ 8283 updates, score 11.624) (writing took 2.3854781398549676 seconds)
2022-03-04 19:30:30 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 19:30:30 | INFO | train | epoch 086 | loss 3.362 | nll_loss 1.994 | ppl 3.98 | wps 24572.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 8283 | lr 0.000347461 | gnorm 1.064 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 22100
2022-03-04 19:30:30 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 19:30:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:31:14 | INFO | train_inner | epoch 087:     17 / 97 loss=3.357, nll_loss=1.988, ppl=3.97, wps=24611.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.068, loss_scale=8, train_wall=236, gb_free=21, wall=22143
2022-03-04 19:34:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:34:44 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.622 | nll_loss 10.826 | ppl 1815.32 | wps 44898.2 | wpb 510.9 | bsz 1 | num_updates 8380 | best_loss 8.224
2022-03-04 19:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8380 updates
2022-03-04 19:34:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:34:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:34:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 87 @ 8380 updates, score 11.622) (writing took 2.362572206184268 seconds)
2022-03-04 19:34:46 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 19:34:46 | INFO | train | epoch 087 | loss 3.341 | nll_loss 1.971 | ppl 3.92 | wps 24822.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8380 | lr 0.000345444 | gnorm 1.075 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 22356
2022-03-04 19:34:46 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 19:34:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:35:38 | INFO | train_inner | epoch 088:     20 / 97 loss=3.334, nll_loss=1.963, ppl=3.9, wps=24839.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.069, loss_scale=16, train_wall=234, gb_free=21, wall=22407
2022-03-04 19:38:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:38:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:38:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:39:00 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.656 | nll_loss 10.852 | ppl 1848.38 | wps 44977.2 | wpb 510.9 | bsz 1 | num_updates 8475 | best_loss 8.224
2022-03-04 19:39:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8475 updates
2022-03-04 19:39:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:39:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:39:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 88 @ 8475 updates, score 11.656) (writing took 2.366969690658152 seconds)
2022-03-04 19:39:02 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 19:39:02 | INFO | train | epoch 088 | loss 3.316 | nll_loss 1.943 | ppl 3.84 | wps 24310.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 8475 | lr 0.000343503 | gnorm 1.054 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 22611
2022-03-04 19:39:02 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 19:39:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:40:06 | INFO | train_inner | epoch 089:     25 / 97 loss=3.31, nll_loss=1.936, ppl=3.83, wps=24361.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.048, loss_scale=8, train_wall=239, gb_free=21, wall=22676
2022-03-04 19:43:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:43:16 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.705 | nll_loss 10.906 | ppl 1919.37 | wps 43775 | wpb 510.9 | bsz 1 | num_updates 8572 | best_loss 8.224
2022-03-04 19:43:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8572 updates
2022-03-04 19:43:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:43:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:43:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 89 @ 8572 updates, score 11.705) (writing took 2.297507746145129 seconds)
2022-03-04 19:43:18 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 19:43:18 | INFO | train | epoch 089 | loss 3.298 | nll_loss 1.923 | ppl 3.79 | wps 24784.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8572 | lr 0.000341554 | gnorm 1.053 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 22868
2022-03-04 19:43:18 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 19:43:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:44:30 | INFO | train_inner | epoch 090:     28 / 97 loss=3.291, nll_loss=1.915, ppl=3.77, wps=24817.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.051, loss_scale=16, train_wall=234, gb_free=21, wall=22940
2022-03-04 19:47:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:47:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:47:32 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.653 | nll_loss 10.854 | ppl 1851.01 | wps 44466 | wpb 510.9 | bsz 1 | num_updates 8668 | best_loss 8.224
2022-03-04 19:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8668 updates
2022-03-04 19:47:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:47:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:47:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 90 @ 8668 updates, score 11.653) (writing took 2.3645325200632215 seconds)
2022-03-04 19:47:35 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 19:47:35 | INFO | train | epoch 090 | loss 3.277 | nll_loss 1.899 | ppl 3.73 | wps 24563.8 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 8668 | lr 0.000339657 | gnorm 1.048 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 23124
2022-03-04 19:47:35 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 19:47:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:48:57 | INFO | train_inner | epoch 091:     32 / 97 loss=3.269, nll_loss=1.891, ppl=3.71, wps=24599.4, ups=0.38, wpb=65533.9, bsz=128, num_updates=8700, lr=0.000339032, gnorm=1.057, loss_scale=8, train_wall=237, gb_free=21, wall=23206
2022-03-04 19:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:51:48 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.758 | nll_loss 10.963 | ppl 1995.96 | wps 44857.4 | wpb 510.9 | bsz 1 | num_updates 8765 | best_loss 8.224
2022-03-04 19:51:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8765 updates
2022-03-04 19:51:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:51:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:51:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 91 @ 8765 updates, score 11.758) (writing took 2.3081612624228 seconds)
2022-03-04 19:51:51 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 19:51:51 | INFO | train | epoch 091 | loss 3.258 | nll_loss 1.878 | ppl 3.68 | wps 24795.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8765 | lr 0.000337772 | gnorm 1.065 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 23380
2022-03-04 19:51:51 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 19:51:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:53:21 | INFO | train_inner | epoch 092:     35 / 97 loss=3.251, nll_loss=1.87, ppl=3.65, wps=24814.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.059, loss_scale=16, train_wall=234, gb_free=21, wall=23470
2022-03-04 19:56:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:56:05 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.776 | nll_loss 10.988 | ppl 2031.25 | wps 44585.5 | wpb 510.9 | bsz 1 | num_updates 8862 | best_loss 8.224
2022-03-04 19:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8862 updates
2022-03-04 19:56:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:56:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 19:56:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 92 @ 8862 updates, score 11.776) (writing took 2.2378042740747333 seconds)
2022-03-04 19:56:07 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 19:56:07 | INFO | train | epoch 092 | loss 3.239 | nll_loss 1.856 | ppl 3.62 | wps 24790.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8862 | lr 0.000335919 | gnorm 1.054 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 23636
2022-03-04 19:56:07 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 19:56:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:57:45 | INFO | train_inner | epoch 093:     38 / 97 loss=3.23, nll_loss=1.847, ppl=3.6, wps=24801.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.049, loss_scale=16, train_wall=235, gb_free=21, wall=23734
2022-03-04 19:58:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:00:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:00:21 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.819 | nll_loss 11.033 | ppl 2094.79 | wps 44974.1 | wpb 510.9 | bsz 1 | num_updates 8958 | best_loss 8.224
2022-03-04 20:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8958 updates
2022-03-04 20:00:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:00:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:00:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 93 @ 8958 updates, score 11.819) (writing took 2.2527120523154736 seconds)
2022-03-04 20:00:23 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 20:00:23 | INFO | train | epoch 093 | loss 3.218 | nll_loss 1.833 | ppl 3.56 | wps 24541.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8958 | lr 0.000334114 | gnorm 1.048 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 23893
2022-03-04 20:00:23 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 20:00:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:02:11 | INFO | train_inner | epoch 094:     42 / 97 loss=3.21, nll_loss=1.825, ppl=3.54, wps=24581.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.058, loss_scale=16, train_wall=237, gb_free=21, wall=24000
2022-03-04 20:04:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:04:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:04:37 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.845 | nll_loss 11.057 | ppl 2130.78 | wps 45068.8 | wpb 510.9 | bsz 1 | num_updates 9053 | best_loss 8.224
2022-03-04 20:04:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9053 updates
2022-03-04 20:04:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:04:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 94 @ 9053 updates, score 11.845) (writing took 2.2573719434440136 seconds)
2022-03-04 20:04:39 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 20:04:39 | INFO | train | epoch 094 | loss 3.201 | nll_loss 1.815 | ppl 3.52 | wps 24315.7 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 9053 | lr 0.000332356 | gnorm 1.057 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 24149
2022-03-04 20:04:39 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 20:04:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:06:40 | INFO | train_inner | epoch 095:     47 / 97 loss=3.194, nll_loss=1.807, ppl=3.5, wps=24373.9, ups=0.37, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=1.059, loss_scale=8, train_wall=239, gb_free=21, wall=24269
2022-03-04 20:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:08:53 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.899 | nll_loss 11.112 | ppl 2213.25 | wps 45100.3 | wpb 510.9 | bsz 1 | num_updates 9150 | best_loss 8.224
2022-03-04 20:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9150 updates
2022-03-04 20:08:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:08:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 95 @ 9150 updates, score 11.899) (writing took 2.4283535052090883 seconds)
2022-03-04 20:08:55 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 20:08:55 | INFO | train | epoch 095 | loss 3.184 | nll_loss 1.795 | ppl 3.47 | wps 24796.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9150 | lr 0.00033059 | gnorm 1.065 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 24405
2022-03-04 20:08:55 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 20:08:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:11:04 | INFO | train_inner | epoch 096:     50 / 97 loss=3.175, nll_loss=1.785, ppl=3.45, wps=24838.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.056, loss_scale=16, train_wall=234, gb_free=21, wall=24533
2022-03-04 20:13:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:13:09 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.913 | nll_loss 11.129 | ppl 2239.8 | wps 44948.3 | wpb 510.9 | bsz 1 | num_updates 9247 | best_loss 8.224
2022-03-04 20:13:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9247 updates
2022-03-04 20:13:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:13:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:13:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 96 @ 9247 updates, score 11.913) (writing took 2.4292363384738564 seconds)
2022-03-04 20:13:11 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 20:13:11 | INFO | train | epoch 096 | loss 3.165 | nll_loss 1.774 | ppl 3.42 | wps 24813.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9247 | lr 0.000328851 | gnorm 1.05 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 24661
2022-03-04 20:13:11 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 20:13:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:15:28 | INFO | train_inner | epoch 097:     53 / 97 loss=3.158, nll_loss=1.766, ppl=3.4, wps=24822.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.051, loss_scale=16, train_wall=234, gb_free=21, wall=24797
2022-03-04 20:15:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:17:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:17:25 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.915 | nll_loss 11.134 | ppl 2247.85 | wps 44845.5 | wpb 510.9 | bsz 1 | num_updates 9343 | best_loss 8.224
2022-03-04 20:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9343 updates
2022-03-04 20:17:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:17:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:17:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 97 @ 9343 updates, score 11.915) (writing took 2.423207932151854 seconds)
2022-03-04 20:17:28 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 20:17:28 | INFO | train | epoch 097 | loss 3.15 | nll_loss 1.758 | ppl 3.38 | wps 24543.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9343 | lr 0.000327157 | gnorm 1.06 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 24917
2022-03-04 20:17:28 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 20:17:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:19:54 | INFO | train_inner | epoch 098:     57 / 97 loss=3.139, nll_loss=1.745, ppl=3.35, wps=24573.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.057, loss_scale=16, train_wall=236, gb_free=21, wall=25063
2022-03-04 20:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:21:41 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.933 | nll_loss 11.152 | ppl 2275.11 | wps 44976.1 | wpb 510.9 | bsz 1 | num_updates 9440 | best_loss 8.224
2022-03-04 20:21:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9440 updates
2022-03-04 20:21:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:21:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:21:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 98 @ 9440 updates, score 11.933) (writing took 2.3300625337287784 seconds)
2022-03-04 20:21:44 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 20:21:44 | INFO | train | epoch 098 | loss 3.131 | nll_loss 1.737 | ppl 3.33 | wps 24794.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9440 | lr 0.000325472 | gnorm 1.035 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 25173
2022-03-04 20:21:44 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 20:21:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:24:20 | INFO | train_inner | epoch 099:     61 / 97 loss=3.124, nll_loss=1.729, ppl=3.31, wps=24596.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.042, loss_scale=16, train_wall=236, gb_free=21, wall=25330
2022-03-04 20:25:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:25:57 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 12.036 | nll_loss 11.267 | ppl 2464.95 | wps 44846.4 | wpb 510.9 | bsz 1 | num_updates 9536 | best_loss 8.224
2022-03-04 20:25:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9536 updates
2022-03-04 20:25:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:26:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:26:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 99 @ 9536 updates, score 12.036) (writing took 2.3737524300813675 seconds)
2022-03-04 20:26:00 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 20:26:00 | INFO | train | epoch 099 | loss 3.115 | nll_loss 1.719 | ppl 3.29 | wps 24562.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9536 | lr 0.00032383 | gnorm 1.059 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 25429
2022-03-04 20:26:00 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 20:26:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:27:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:28:47 | INFO | train_inner | epoch 100:     65 / 97 loss=3.105, nll_loss=1.707, ppl=3.27, wps=24600.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.041, loss_scale=16, train_wall=236, gb_free=21, wall=25596
2022-03-04 20:30:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:30:13 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 12.046 | nll_loss 11.274 | ppl 2477.19 | wps 45069.6 | wpb 510.9 | bsz 1 | num_updates 9632 | best_loss 8.224
2022-03-04 20:30:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9632 updates
2022-03-04 20:30:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:30:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:30:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 100 @ 9632 updates, score 12.046) (writing took 2.3878932362422347 seconds)
2022-03-04 20:30:16 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 20:30:16 | INFO | train | epoch 100 | loss 3.098 | nll_loss 1.7 | ppl 3.25 | wps 24563.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9632 | lr 0.000322212 | gnorm 1.024 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 25685
2022-03-04 20:30:16 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 20:30:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:33:10 | INFO | train_inner | epoch 101:     68 / 97 loss=3.09, nll_loss=1.691, ppl=3.23, wps=24837.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.044, loss_scale=16, train_wall=234, gb_free=21, wall=25860
2022-03-04 20:33:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:34:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:34:29 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 12.023 | nll_loss 11.252 | ppl 2438.38 | wps 44937.2 | wpb 510.9 | bsz 1 | num_updates 9728 | best_loss 8.224
2022-03-04 20:34:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9728 updates
2022-03-04 20:34:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:34:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:34:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 101 @ 9728 updates, score 12.023) (writing took 2.3814057018607855 seconds)
2022-03-04 20:34:32 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 20:34:32 | INFO | train | epoch 101 | loss 3.085 | nll_loss 1.685 | ppl 3.22 | wps 24559.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9728 | lr 0.000320618 | gnorm 1.054 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 25941
2022-03-04 20:34:32 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 20:34:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:37:37 | INFO | train_inner | epoch 102:     72 / 97 loss=3.076, nll_loss=1.676, ppl=3.19, wps=24592.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.051, loss_scale=16, train_wall=236, gb_free=21, wall=26126
2022-03-04 20:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:38:45 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 12.09 | nll_loss 11.314 | ppl 2546.38 | wps 45037.6 | wpb 510.9 | bsz 1 | num_updates 9825 | best_loss 8.224
2022-03-04 20:38:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9825 updates
2022-03-04 20:38:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:38:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:38:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 102 @ 9825 updates, score 12.09) (writing took 2.3496692487969995 seconds)
2022-03-04 20:38:48 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 20:38:48 | INFO | train | epoch 102 | loss 3.069 | nll_loss 1.668 | ppl 3.18 | wps 24807 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9825 | lr 0.000319032 | gnorm 1.054 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 26197
2022-03-04 20:38:48 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 20:38:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:42:03 | INFO | train_inner | epoch 103:     76 / 97 loss=3.057, nll_loss=1.655, ppl=3.15, wps=24596.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.053, loss_scale=16, train_wall=236, gb_free=21, wall=26392
2022-03-04 20:42:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:43:01 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 12.134 | nll_loss 11.359 | ppl 2626.65 | wps 44994.8 | wpb 510.9 | bsz 1 | num_updates 9921 | best_loss 8.224
2022-03-04 20:43:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9921 updates
2022-03-04 20:43:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:43:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:43:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 103 @ 9921 updates, score 12.134) (writing took 2.324963149614632 seconds)
2022-03-04 20:43:04 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 20:43:04 | INFO | train | epoch 103 | loss 3.055 | nll_loss 1.653 | ppl 3.14 | wps 24563.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 9921 | lr 0.000317484 | gnorm 1.046 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 26453
2022-03-04 20:43:04 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 20:43:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:44:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:46:29 | INFO | train_inner | epoch 104:     80 / 97 loss=3.047, nll_loss=1.644, ppl=3.12, wps=24609.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.038, loss_scale=16, train_wall=236, gb_free=21, wall=26658
2022-03-04 20:47:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:47:17 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 12.128 | nll_loss 11.367 | ppl 2641.15 | wps 45039.9 | wpb 510.9 | bsz 1 | num_updates 10017 | best_loss 8.224
2022-03-04 20:47:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10017 updates
2022-03-04 20:47:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:47:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:47:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 104 @ 10017 updates, score 12.128) (writing took 2.339061062783003 seconds)
2022-03-04 20:47:20 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 20:47:20 | INFO | train | epoch 104 | loss 3.04 | nll_loss 1.636 | ppl 3.11 | wps 24566.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 10017 | lr 0.000315959 | gnorm 1.041 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 26709
2022-03-04 20:47:20 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 20:47:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:50:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:50:55 | INFO | train_inner | epoch 105:     84 / 97 loss=3.03, nll_loss=1.624, ppl=3.08, wps=24577.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.041, loss_scale=16, train_wall=237, gb_free=21, wall=26925
2022-03-04 20:51:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:51:34 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 12.126 | nll_loss 11.357 | ppl 2622.56 | wps 44893.6 | wpb 510.9 | bsz 1 | num_updates 10113 | best_loss 8.224
2022-03-04 20:51:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10113 updates
2022-03-04 20:51:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:51:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:51:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 105 @ 10113 updates, score 12.126) (writing took 2.3716909447684884 seconds)
2022-03-04 20:51:36 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 20:51:36 | INFO | train | epoch 105 | loss 3.026 | nll_loss 1.62 | ppl 3.07 | wps 24536.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10113 | lr 0.000314456 | gnorm 1.039 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 26965
2022-03-04 20:51:36 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 20:51:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:55:19 | INFO | train_inner | epoch 106:     87 / 97 loss=3.016, nll_loss=1.609, ppl=3.05, wps=24824.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10200, lr=0.000313112, gnorm=1.054, loss_scale=16, train_wall=234, gb_free=21, wall=27189
2022-03-04 20:55:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:55:50 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 12.201 | nll_loss 11.44 | ppl 2777.86 | wps 44861 | wpb 510.9 | bsz 1 | num_updates 10210 | best_loss 8.224
2022-03-04 20:55:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10210 updates
2022-03-04 20:55:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:55:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 20:55:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 106 @ 10210 updates, score 12.201) (writing took 2.369808821938932 seconds)
2022-03-04 20:55:52 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 20:55:52 | INFO | train | epoch 106 | loss 3.014 | nll_loss 1.607 | ppl 3.05 | wps 24803.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10210 | lr 0.000312959 | gnorm 1.058 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 27221
2022-03-04 20:55:52 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 20:55:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:56:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:59:46 | INFO | train_inner | epoch 107:     91 / 97 loss=3.003, nll_loss=1.595, ppl=3.02, wps=24577.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.045, loss_scale=16, train_wall=237, gb_free=21, wall=27455
2022-03-04 21:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:00:06 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 12.165 | nll_loss 11.409 | ppl 2719.95 | wps 44649.4 | wpb 510.9 | bsz 1 | num_updates 10306 | best_loss 8.224
2022-03-04 21:00:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10306 updates
2022-03-04 21:00:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:00:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:00:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 107 @ 10306 updates, score 12.165) (writing took 2.348397088237107 seconds)
2022-03-04 21:00:08 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 21:00:08 | INFO | train | epoch 107 | loss 2.998 | nll_loss 1.589 | ppl 3.01 | wps 24538.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10306 | lr 0.000311498 | gnorm 1.04 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 27478
2022-03-04 21:00:08 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 21:00:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:02:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:04:12 | INFO | train_inner | epoch 108:     95 / 97 loss=2.988, nll_loss=1.579, ppl=2.99, wps=24586.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.029, loss_scale=16, train_wall=237, gb_free=21, wall=27721
2022-03-04 21:04:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:04:22 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 12.225 | nll_loss 11.466 | ppl 2828.79 | wps 45441.6 | wpb 510.9 | bsz 1 | num_updates 10402 | best_loss 8.224
2022-03-04 21:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10402 updates
2022-03-04 21:04:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:04:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 108 @ 10402 updates, score 12.225) (writing took 2.3444356517866254 seconds)
2022-03-04 21:04:24 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 21:04:24 | INFO | train | epoch 108 | loss 2.985 | nll_loss 1.575 | ppl 2.98 | wps 24553 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10402 | lr 0.000310057 | gnorm 1.028 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 27734
2022-03-04 21:04:24 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 21:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:08:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:08:38 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 12.197 | nll_loss 11.437 | ppl 2772.36 | wps 44989.7 | wpb 510.9 | bsz 1 | num_updates 10498 | best_loss 8.224
2022-03-04 21:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10498 updates
2022-03-04 21:08:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:08:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:08:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 109 @ 10498 updates, score 12.197) (writing took 2.358219902962446 seconds)
2022-03-04 21:08:40 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 21:08:40 | INFO | train | epoch 109 | loss 2.973 | nll_loss 1.562 | ppl 2.95 | wps 24552 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10498 | lr 0.000308636 | gnorm 1.047 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 27990
2022-03-04 21:08:40 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 21:08:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:08:46 | INFO | train_inner | epoch 110:      2 / 97 loss=2.973, nll_loss=1.562, ppl=2.95, wps=23927.9, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=10500, lr=0.000308607, gnorm=1.048, loss_scale=16, train_wall=236, gb_free=21, wall=27995
2022-03-04 21:12:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:12:54 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 12.186 | nll_loss 11.419 | ppl 2738.17 | wps 44939.7 | wpb 510.9 | bsz 1 | num_updates 10595 | best_loss 8.224
2022-03-04 21:12:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10595 updates
2022-03-04 21:12:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:12:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:12:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 110 @ 10595 updates, score 12.186) (writing took 2.321260578930378 seconds)
2022-03-04 21:12:56 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 21:12:56 | INFO | train | epoch 110 | loss 2.961 | nll_loss 1.548 | ppl 2.92 | wps 24818.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10595 | lr 0.00030722 | gnorm 1.029 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 28246
2022-03-04 21:12:56 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 21:12:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:13:09 | INFO | train_inner | epoch 111:      5 / 97 loss=2.959, nll_loss=1.546, ppl=2.92, wps=24841.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10600, lr=0.000307148, gnorm=1.028, loss_scale=16, train_wall=234, gb_free=21, wall=28259
2022-03-04 21:13:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:17:10 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 12.235 | nll_loss 11.485 | ppl 2865.42 | wps 45114.5 | wpb 510.9 | bsz 1 | num_updates 10691 | best_loss 8.224
2022-03-04 21:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10691 updates
2022-03-04 21:17:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:17:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:17:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 111 @ 10691 updates, score 12.235) (writing took 2.396173235028982 seconds)
2022-03-04 21:17:13 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 21:17:13 | INFO | train | epoch 111 | loss 2.947 | nll_loss 1.533 | ppl 2.89 | wps 24529.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10691 | lr 0.000305838 | gnorm 1.028 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 28502
2022-03-04 21:17:13 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 21:17:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:17:36 | INFO | train_inner | epoch 112:      9 / 97 loss=2.943, nll_loss=1.528, ppl=2.88, wps=24570.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.028, loss_scale=16, train_wall=237, gb_free=21, wall=28525
2022-03-04 21:19:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:21:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:21:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:21:26 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 12.247 | nll_loss 11.494 | ppl 2883.32 | wps 45024.8 | wpb 510.9 | bsz 1 | num_updates 10786 | best_loss 8.224
2022-03-04 21:21:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10786 updates
2022-03-04 21:21:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:21:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 112 @ 10786 updates, score 12.247) (writing took 2.320223934017122 seconds)
2022-03-04 21:21:29 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 21:21:29 | INFO | train | epoch 112 | loss 2.936 | nll_loss 1.521 | ppl 2.87 | wps 24318.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 10786 | lr 0.000304488 | gnorm 1.043 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 28758
2022-03-04 21:21:29 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 21:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:22:04 | INFO | train_inner | epoch 113:     14 / 97 loss=2.932, nll_loss=1.516, ppl=2.86, wps=24379.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.039, loss_scale=8, train_wall=239, gb_free=21, wall=28794
2022-03-04 21:25:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:25:42 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 12.357 | nll_loss 11.609 | ppl 3122.91 | wps 44908.3 | wpb 510.9 | bsz 1 | num_updates 10883 | best_loss 8.224
2022-03-04 21:25:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10883 updates
2022-03-04 21:25:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:25:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:25:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 113 @ 10883 updates, score 12.357) (writing took 2.3347118217498064 seconds)
2022-03-04 21:25:44 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 21:25:44 | INFO | train | epoch 113 | loss 2.926 | nll_loss 1.51 | ppl 2.85 | wps 24831.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10883 | lr 0.000303128 | gnorm 1.044 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 29014
2022-03-04 21:25:44 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 21:25:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:26:28 | INFO | train_inner | epoch 114:     17 / 97 loss=2.923, nll_loss=1.507, ppl=2.84, wps=24848, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.045, loss_scale=8, train_wall=234, gb_free=21, wall=29057
2022-03-04 21:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:29:58 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 12.294 | nll_loss 11.539 | ppl 2976.57 | wps 45158.5 | wpb 510.9 | bsz 1 | num_updates 10980 | best_loss 8.224
2022-03-04 21:29:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10980 updates
2022-03-04 21:29:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:30:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:30:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 114 @ 10980 updates, score 12.294) (writing took 2.3435248229652643 seconds)
2022-03-04 21:30:00 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 21:30:00 | INFO | train | epoch 114 | loss 2.913 | nll_loss 1.496 | ppl 2.82 | wps 24813.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10980 | lr 0.000301786 | gnorm 1.042 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 29270
2022-03-04 21:30:00 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 21:30:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:30:52 | INFO | train_inner | epoch 115:     20 / 97 loss=2.911, nll_loss=1.494, ppl=2.82, wps=24833.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.043, loss_scale=16, train_wall=234, gb_free=21, wall=29321
2022-03-04 21:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:34:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:34:14 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 12.318 | nll_loss 11.57 | ppl 3040.94 | wps 44751 | wpb 510.9 | bsz 1 | num_updates 11076 | best_loss 8.224
2022-03-04 21:34:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11076 updates
2022-03-04 21:34:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:34:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:34:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 115 @ 11076 updates, score 12.318) (writing took 2.320306117646396 seconds)
2022-03-04 21:34:16 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 21:34:16 | INFO | train | epoch 115 | loss 2.901 | nll_loss 1.482 | ppl 2.79 | wps 24565.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 11076 | lr 0.000300475 | gnorm 1.022 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 29526
2022-03-04 21:34:16 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 21:34:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:35:18 | INFO | train_inner | epoch 116:     24 / 97 loss=2.895, nll_loss=1.476, ppl=2.78, wps=24603.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.016, loss_scale=16, train_wall=236, gb_free=21, wall=29587
2022-03-04 21:38:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:38:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:38:30 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 12.28 | nll_loss 11.526 | ppl 2948.72 | wps 44107.9 | wpb 510.9 | bsz 1 | num_updates 11172 | best_loss 8.224
2022-03-04 21:38:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11172 updates
2022-03-04 21:38:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:38:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:38:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 116 @ 11172 updates, score 12.28) (writing took 2.3871450275182724 seconds)
2022-03-04 21:38:32 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 21:38:32 | INFO | train | epoch 116 | loss 2.891 | nll_loss 1.472 | ppl 2.77 | wps 24546.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11172 | lr 0.000299181 | gnorm 1.029 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 29782
2022-03-04 21:38:32 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 21:38:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:39:44 | INFO | train_inner | epoch 117:     28 / 97 loss=2.888, nll_loss=1.469, ppl=2.77, wps=24583.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.029, loss_scale=16, train_wall=236, gb_free=21, wall=29854
2022-03-04 21:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:42:46 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 12.415 | nll_loss 11.668 | ppl 3255 | wps 45167.7 | wpb 510.9 | bsz 1 | num_updates 11269 | best_loss 8.224
2022-03-04 21:42:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11269 updates
2022-03-04 21:42:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:42:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:42:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 117 @ 11269 updates, score 12.415) (writing took 2.353839256800711 seconds)
2022-03-04 21:42:49 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 21:42:49 | INFO | train | epoch 117 | loss 2.88 | nll_loss 1.46 | ppl 2.75 | wps 24808.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11269 | lr 0.000297891 | gnorm 1.032 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30038
2022-03-04 21:42:49 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 21:42:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:44:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:44:11 | INFO | train_inner | epoch 118:     32 / 97 loss=2.874, nll_loss=1.453, ppl=2.74, wps=24588.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.032, loss_scale=16, train_wall=236, gb_free=21, wall=30120
2022-03-04 21:46:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:47:02 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 12.339 | nll_loss 11.584 | ppl 3070.57 | wps 44975.5 | wpb 510.9 | bsz 1 | num_updates 11365 | best_loss 8.224
2022-03-04 21:47:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11365 updates
2022-03-04 21:47:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:47:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 118 @ 11365 updates, score 12.339) (writing took 2.4055888829752803 seconds)
2022-03-04 21:47:05 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 21:47:05 | INFO | train | epoch 118 | loss 2.869 | nll_loss 1.448 | ppl 2.73 | wps 24543.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11365 | lr 0.00029663 | gnorm 1.024 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30294
2022-03-04 21:47:05 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 21:47:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:48:35 | INFO | train_inner | epoch 119:     35 / 97 loss=2.865, nll_loss=1.443, ppl=2.72, wps=24816.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.021, loss_scale=16, train_wall=234, gb_free=21, wall=30384
2022-03-04 21:49:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:51:18 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 12.362 | nll_loss 11.614 | ppl 3134.85 | wps 44871.8 | wpb 510.9 | bsz 1 | num_updates 11461 | best_loss 8.224
2022-03-04 21:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11461 updates
2022-03-04 21:51:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:51:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 119 @ 11461 updates, score 12.362) (writing took 2.2093536695465446 seconds)
2022-03-04 21:51:21 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 21:51:21 | INFO | train | epoch 119 | loss 2.857 | nll_loss 1.434 | ppl 2.7 | wps 24561.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 11461 | lr 0.000295385 | gnorm 1.02 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30550
2022-03-04 21:51:21 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 21:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:53:01 | INFO | train_inner | epoch 120:     39 / 97 loss=2.853, nll_loss=1.431, ppl=2.7, wps=24611.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.018, loss_scale=16, train_wall=236, gb_free=21, wall=30650
2022-03-04 21:55:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:55:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:55:34 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 12.44 | nll_loss 11.706 | ppl 3339.94 | wps 45021.8 | wpb 510.9 | bsz 1 | num_updates 11557 | best_loss 8.224
2022-03-04 21:55:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11557 updates
2022-03-04 21:55:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:55:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:55:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 120 @ 11557 updates, score 12.44) (writing took 2.243968394584954 seconds)
2022-03-04 21:55:37 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 21:55:37 | INFO | train | epoch 120 | loss 2.849 | nll_loss 1.426 | ppl 2.69 | wps 24565.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 11557 | lr 0.000294156 | gnorm 1.025 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30806
2022-03-04 21:55:37 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 21:55:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:57:27 | INFO | train_inner | epoch 121:     43 / 97 loss=2.848, nll_loss=1.425, ppl=2.69, wps=24597, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.04, loss_scale=16, train_wall=237, gb_free=21, wall=30916
2022-03-04 21:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:59:51 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 12.378 | nll_loss 11.629 | ppl 3167.53 | wps 44850.6 | wpb 510.9 | bsz 1 | num_updates 11654 | best_loss 8.224
2022-03-04 21:59:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11654 updates
2022-03-04 21:59:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:59:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 21:59:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 121 @ 11654 updates, score 12.378) (writing took 2.269792528823018 seconds)
2022-03-04 21:59:53 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 21:59:53 | INFO | train | epoch 121 | loss 2.839 | nll_loss 1.415 | ppl 2.67 | wps 24793.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11654 | lr 0.000292929 | gnorm 1.036 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31062
2022-03-04 21:59:53 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 21:59:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:01:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:01:54 | INFO | train_inner | epoch 122:     47 / 97 loss=2.831, nll_loss=1.406, ppl=2.65, wps=24577.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=1.027, loss_scale=16, train_wall=237, gb_free=21, wall=31183
2022-03-04 22:04:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:07 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 12.43 | nll_loss 11.688 | ppl 3298.89 | wps 45049.4 | wpb 510.9 | bsz 1 | num_updates 11750 | best_loss 8.224
2022-03-04 22:04:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11750 updates
2022-03-04 22:04:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:04:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:04:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 122 @ 11750 updates, score 12.43) (writing took 2.263185285963118 seconds)
2022-03-04 22:04:09 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 22:04:09 | INFO | train | epoch 122 | loss 2.827 | nll_loss 1.402 | ppl 2.64 | wps 24541.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11750 | lr 0.00029173 | gnorm 1.022 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31318
2022-03-04 22:04:09 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 22:04:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:06:17 | INFO | train_inner | epoch 123:     50 / 97 loss=2.823, nll_loss=1.398, ppl=2.64, wps=24815.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.022, loss_scale=16, train_wall=234, gb_free=21, wall=31447
2022-03-04 22:06:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:08:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:08:23 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 12.452 | nll_loss 11.699 | ppl 3325.51 | wps 44070.6 | wpb 510.9 | bsz 1 | num_updates 11846 | best_loss 8.224
2022-03-04 22:08:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11846 updates
2022-03-04 22:08:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:08:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:08:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 123 @ 11846 updates, score 12.452) (writing took 2.532858293503523 seconds)
2022-03-04 22:08:25 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 22:08:25 | INFO | train | epoch 123 | loss 2.818 | nll_loss 1.392 | ppl 2.62 | wps 24524.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11846 | lr 0.000290545 | gnorm 1.02 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31575
2022-03-04 22:08:25 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 22:08:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:10:44 | INFO | train_inner | epoch 124:     54 / 97 loss=2.811, nll_loss=1.384, ppl=2.61, wps=24568.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.012, loss_scale=16, train_wall=236, gb_free=21, wall=31713
2022-03-04 22:12:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:12:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:12:39 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 12.491 | nll_loss 11.749 | ppl 3441.5 | wps 45104.1 | wpb 510.9 | bsz 1 | num_updates 11942 | best_loss 8.224
2022-03-04 22:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11942 updates
2022-03-04 22:12:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:12:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 124 @ 11942 updates, score 12.491) (writing took 2.3680167756974697 seconds)
2022-03-04 22:12:41 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 22:12:41 | INFO | train | epoch 124 | loss 2.808 | nll_loss 1.382 | ppl 2.61 | wps 24550.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11942 | lr 0.000289375 | gnorm 1.012 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31831
2022-03-04 22:12:41 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 22:12:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:15:10 | INFO | train_inner | epoch 125:     58 / 97 loss=2.806, nll_loss=1.379, ppl=2.6, wps=24596.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.021, loss_scale=16, train_wall=237, gb_free=21, wall=31980
2022-03-04 22:16:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:16:55 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 12.537 | nll_loss 11.806 | ppl 3580.91 | wps 44987.4 | wpb 510.9 | bsz 1 | num_updates 12039 | best_loss 8.224
2022-03-04 22:16:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12039 updates
2022-03-04 22:16:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:16:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 125 @ 12039 updates, score 12.537) (writing took 2.3898256812244654 seconds)
2022-03-04 22:16:57 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 22:16:57 | INFO | train | epoch 125 | loss 2.802 | nll_loss 1.375 | ppl 2.59 | wps 24820.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12039 | lr 0.000288207 | gnorm 1.02 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32087
2022-03-04 22:16:57 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 22:16:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:18:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:19:36 | INFO | train_inner | epoch 126:     62 / 97 loss=2.797, nll_loss=1.369, ppl=2.58, wps=24600.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=1.012, loss_scale=16, train_wall=236, gb_free=21, wall=32246
2022-03-04 22:21:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:21:11 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 12.445 | nll_loss 11.708 | ppl 3346.2 | wps 44890.6 | wpb 510.9 | bsz 1 | num_updates 12135 | best_loss 8.224
2022-03-04 22:21:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12135 updates
2022-03-04 22:21:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:21:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:21:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 126 @ 12135 updates, score 12.445) (writing took 2.4362297132611275 seconds)
2022-03-04 22:21:13 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 22:21:13 | INFO | train | epoch 126 | loss 2.793 | nll_loss 1.365 | ppl 2.58 | wps 24549.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12135 | lr 0.000287065 | gnorm 1.019 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32343
2022-03-04 22:21:14 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 22:21:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:23:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:24:03 | INFO | train_inner | epoch 127:     66 / 97 loss=2.786, nll_loss=1.358, ppl=2.56, wps=24590.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12200, lr=0.000286299, gnorm=1.015, loss_scale=16, train_wall=236, gb_free=21, wall=32512
2022-03-04 22:25:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:25:27 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 12.514 | nll_loss 11.777 | ppl 3510.51 | wps 44993.1 | wpb 510.9 | bsz 1 | num_updates 12231 | best_loss 8.224
2022-03-04 22:25:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12231 updates
2022-03-04 22:25:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:25:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:25:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 127 @ 12231 updates, score 12.514) (writing took 2.3400420621037483 seconds)
2022-03-04 22:25:29 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 22:25:29 | INFO | train | epoch 127 | loss 2.782 | nll_loss 1.353 | ppl 2.55 | wps 24562.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 12231 | lr 0.000285936 | gnorm 1.007 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32599
2022-03-04 22:25:29 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 22:25:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:28:27 | INFO | train_inner | epoch 128:     69 / 97 loss=2.778, nll_loss=1.349, ppl=2.55, wps=24831.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=1.003, loss_scale=16, train_wall=234, gb_free=21, wall=32776
2022-03-04 22:29:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:29:43 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 12.524 | nll_loss 11.78 | ppl 3516.81 | wps 45022.5 | wpb 510.9 | bsz 1 | num_updates 12327 | best_loss 8.224
2022-03-04 22:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12327 updates
2022-03-04 22:29:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:29:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 128 @ 12327 updates, score 12.524) (writing took 2.354927516542375 seconds)
2022-03-04 22:29:45 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 22:29:46 | INFO | train | epoch 128 | loss 2.774 | nll_loss 1.344 | ppl 2.54 | wps 24555.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12327 | lr 0.000284821 | gnorm 1.01 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32855
2022-03-04 22:29:46 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 22:29:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:32:53 | INFO | train_inner | epoch 129:     73 / 97 loss=2.77, nll_loss=1.341, ppl=2.53, wps=24597.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=1.032, loss_scale=16, train_wall=237, gb_free=21, wall=33042
2022-03-04 22:33:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:33:59 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 12.492 | nll_loss 11.755 | ppl 3456.29 | wps 44865.6 | wpb 510.9 | bsz 1 | num_updates 12424 | best_loss 8.224
2022-03-04 22:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12424 updates
2022-03-04 22:33:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:34:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 129 @ 12424 updates, score 12.492) (writing took 2.35220043361187 seconds)
2022-03-04 22:34:02 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 22:34:02 | INFO | train | epoch 129 | loss 2.766 | nll_loss 1.336 | ppl 2.53 | wps 24812.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12424 | lr 0.000283706 | gnorm 1.027 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33111
2022-03-04 22:34:02 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 22:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:35:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:37:19 | INFO | train_inner | epoch 130:     77 / 97 loss=2.758, nll_loss=1.328, ppl=2.51, wps=24599, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=1.012, loss_scale=16, train_wall=236, gb_free=21, wall=33308
2022-03-04 22:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:38:15 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 12.538 | nll_loss 11.807 | ppl 3581.91 | wps 45016.3 | wpb 510.9 | bsz 1 | num_updates 12520 | best_loss 8.224
2022-03-04 22:38:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12520 updates
2022-03-04 22:38:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:38:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:38:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 130 @ 12520 updates, score 12.538) (writing took 2.414333994500339 seconds)
2022-03-04 22:38:18 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 22:38:18 | INFO | train | epoch 130 | loss 2.755 | nll_loss 1.323 | ppl 2.5 | wps 24558.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12520 | lr 0.000282617 | gnorm 1.014 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33367
2022-03-04 22:38:18 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 22:38:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:41:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:41:46 | INFO | train_inner | epoch 131:     81 / 97 loss=2.751, nll_loss=1.32, ppl=2.5, wps=24573.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=1.017, loss_scale=16, train_wall=237, gb_free=21, wall=33575
2022-03-04 22:42:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:42:32 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 12.588 | nll_loss 11.862 | ppl 3722.61 | wps 44250.5 | wpb 510.9 | bsz 1 | num_updates 12616 | best_loss 8.224
2022-03-04 22:42:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12616 updates
2022-03-04 22:42:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:42:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:42:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 131 @ 12616 updates, score 12.588) (writing took 2.359436174854636 seconds)
2022-03-04 22:42:34 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 22:42:34 | INFO | train | epoch 131 | loss 2.748 | nll_loss 1.317 | ppl 2.49 | wps 24525.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12616 | lr 0.000281539 | gnorm 1.016 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33623
2022-03-04 22:42:34 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 22:42:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:45:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:46:12 | INFO | train_inner | epoch 132:     85 / 97 loss=2.742, nll_loss=1.31, ppl=2.48, wps=24589.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=1.017, loss_scale=8, train_wall=236, gb_free=21, wall=33841
2022-03-04 22:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:46:47 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 12.558 | nll_loss 11.829 | ppl 3637.64 | wps 44888.6 | wpb 510.9 | bsz 1 | num_updates 12712 | best_loss 8.224
2022-03-04 22:46:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12712 updates
2022-03-04 22:46:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:46:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:46:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 132 @ 12712 updates, score 12.558) (writing took 2.3807312780991197 seconds)
2022-03-04 22:46:50 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 22:46:50 | INFO | train | epoch 132 | loss 2.739 | nll_loss 1.307 | ppl 2.47 | wps 24561.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 12712 | lr 0.000280474 | gnorm 1.018 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 33879
2022-03-04 22:46:50 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 22:46:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:50:36 | INFO | train_inner | epoch 133:     88 / 97 loss=2.733, nll_loss=1.301, ppl=2.46, wps=24831.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=0.989, loss_scale=8, train_wall=234, gb_free=21, wall=34105
2022-03-04 22:50:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:51:04 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 12.594 | nll_loss 11.865 | ppl 3729.02 | wps 45076.6 | wpb 510.9 | bsz 1 | num_updates 12809 | best_loss 8.224
2022-03-04 22:51:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12809 updates
2022-03-04 22:51:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:51:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:51:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 133 @ 12809 updates, score 12.594) (writing took 2.350415783934295 seconds)
2022-03-04 22:51:06 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 22:51:06 | INFO | train | epoch 133 | loss 2.732 | nll_loss 1.299 | ppl 2.46 | wps 24812.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12809 | lr 0.00027941 | gnorm 0.986 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 34135
2022-03-04 22:51:06 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 22:51:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:54:59 | INFO | train_inner | epoch 134:     91 / 97 loss=2.727, nll_loss=1.294, ppl=2.45, wps=24844.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=1.02, loss_scale=16, train_wall=234, gb_free=21, wall=34369
2022-03-04 22:55:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:55:19 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 12.598 | nll_loss 11.882 | ppl 3775.29 | wps 45006.6 | wpb 510.9 | bsz 1 | num_updates 12906 | best_loss 8.224
2022-03-04 22:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12906 updates
2022-03-04 22:55:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:55:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 134 @ 12906 updates, score 12.598) (writing took 2.326381178572774 seconds)
2022-03-04 22:55:22 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 22:55:22 | INFO | train | epoch 134 | loss 2.726 | nll_loss 1.292 | ppl 2.45 | wps 24827.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12906 | lr 0.000278358 | gnorm 1.021 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34391
2022-03-04 22:55:22 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 22:55:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:57:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:59:25 | INFO | train_inner | epoch 135:     95 / 97 loss=2.72, nll_loss=1.286, ppl=2.44, wps=24606.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=1.008, loss_scale=16, train_wall=236, gb_free=21, wall=34635
2022-03-04 22:59:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:59:35 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 12.594 | nll_loss 11.87 | ppl 3743.05 | wps 45067.8 | wpb 510.9 | bsz 1 | num_updates 13002 | best_loss 8.224
2022-03-04 22:59:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13002 updates
2022-03-04 22:59:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:59:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 22:59:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 135 @ 13002 updates, score 12.594) (writing took 2.350200979039073 seconds)
2022-03-04 22:59:38 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 22:59:38 | INFO | train | epoch 135 | loss 2.716 | nll_loss 1.281 | ppl 2.43 | wps 24565.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13002 | lr 0.000277329 | gnorm 1.005 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34647
2022-03-04 22:59:38 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 22:59:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:03:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:03:51 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 12.603 | nll_loss 11.878 | ppl 3764.68 | wps 44959.8 | wpb 510.9 | bsz 1 | num_updates 13098 | best_loss 8.224
2022-03-04 23:03:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13098 updates
2022-03-04 23:03:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:03:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:03:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 136 @ 13098 updates, score 12.603) (writing took 2.35030525829643 seconds)
2022-03-04 23:03:54 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 23:03:54 | INFO | train | epoch 136 | loss 2.711 | nll_loss 1.276 | ppl 2.42 | wps 24561.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13098 | lr 0.000276311 | gnorm 1.016 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34903
2022-03-04 23:03:54 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 23:03:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:03:59 | INFO | train_inner | epoch 137:      2 / 97 loss=2.71, nll_loss=1.276, ppl=2.42, wps=23935.3, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=13100, lr=0.000276289, gnorm=1.015, loss_scale=16, train_wall=236, gb_free=21, wall=34908
2022-03-04 23:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:08:07 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 12.648 | nll_loss 11.926 | ppl 3890.44 | wps 45046.9 | wpb 510.9 | bsz 1 | num_updates 13195 | best_loss 8.224
2022-03-04 23:08:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13195 updates
2022-03-04 23:08:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:08:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:08:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 137 @ 13195 updates, score 12.648) (writing took 2.386640622280538 seconds)
2022-03-04 23:08:10 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 23:08:10 | INFO | train | epoch 137 | loss 2.703 | nll_loss 1.268 | ppl 2.41 | wps 24816.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13195 | lr 0.000275293 | gnorm 0.994 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35159
2022-03-04 23:08:10 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 23:08:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:08:23 | INFO | train_inner | epoch 138:      5 / 97 loss=2.701, nll_loss=1.266, ppl=2.4, wps=24840.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13200, lr=0.000275241, gnorm=0.995, loss_scale=16, train_wall=234, gb_free=21, wall=35172
2022-03-04 23:08:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:10:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:12:23 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 12.594 | nll_loss 11.873 | ppl 3751.54 | wps 44964.3 | wpb 510.9 | bsz 1 | num_updates 13290 | best_loss 8.224
2022-03-04 23:12:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13290 updates
2022-03-04 23:12:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:12:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:12:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 138 @ 13290 updates, score 12.594) (writing took 3.122346551157534 seconds)
2022-03-04 23:12:26 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 23:12:26 | INFO | train | epoch 138 | loss 2.694 | nll_loss 1.258 | ppl 2.39 | wps 24234 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 13290 | lr 0.000274307 | gnorm 1.016 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 35416
2022-03-04 23:12:26 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 23:12:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:12:52 | INFO | train_inner | epoch 139:     10 / 97 loss=2.691, nll_loss=1.255, ppl=2.39, wps=24293.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13300, lr=0.000274204, gnorm=1.02, loss_scale=8, train_wall=239, gb_free=21, wall=35442
2022-03-04 23:16:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:16:40 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 12.59 | nll_loss 11.87 | ppl 3741.82 | wps 45170.9 | wpb 510.9 | bsz 1 | num_updates 13387 | best_loss 8.224
2022-03-04 23:16:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13387 updates
2022-03-04 23:16:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:16:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 139 @ 13387 updates, score 12.59) (writing took 2.3452379293739796 seconds)
2022-03-04 23:16:42 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 23:16:42 | INFO | train | epoch 139 | loss 2.686 | nll_loss 1.25 | ppl 2.38 | wps 24823.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13387 | lr 0.000273312 | gnorm 1.002 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35672
2022-03-04 23:16:42 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 23:16:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:17:16 | INFO | train_inner | epoch 140:     13 / 97 loss=2.685, nll_loss=1.249, ppl=2.38, wps=24846.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13400, lr=0.000273179, gnorm=0.997, loss_scale=16, train_wall=234, gb_free=21, wall=35705
2022-03-04 23:20:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:20:56 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 12.646 | nll_loss 11.926 | ppl 3891.89 | wps 44796.7 | wpb 510.9 | bsz 1 | num_updates 13484 | best_loss 8.224
2022-03-04 23:20:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13484 updates
2022-03-04 23:20:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:20:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:20:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 140 @ 13484 updates, score 12.646) (writing took 2.410570066422224 seconds)
2022-03-04 23:20:58 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 23:20:58 | INFO | train | epoch 140 | loss 2.682 | nll_loss 1.245 | ppl 2.37 | wps 24822.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13484 | lr 0.000272327 | gnorm 1.008 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35928
2022-03-04 23:20:58 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 23:20:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:21:39 | INFO | train_inner | epoch 141:     16 / 97 loss=2.677, nll_loss=1.24, ppl=2.36, wps=24849, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1.004, loss_scale=16, train_wall=234, gb_free=21, wall=35969
2022-03-04 23:22:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:25:12 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 12.608 | nll_loss 11.883 | ppl 3777.29 | wps 44943.8 | wpb 510.9 | bsz 1 | num_updates 13580 | best_loss 8.224
2022-03-04 23:25:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13580 updates
2022-03-04 23:25:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:25:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:25:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 141 @ 13580 updates, score 12.608) (writing took 2.342815768904984 seconds)
2022-03-04 23:25:14 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 23:25:14 | INFO | train | epoch 141 | loss 2.672 | nll_loss 1.235 | ppl 2.35 | wps 24580.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13580 | lr 0.000271363 | gnorm 0.996 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 36183
2022-03-04 23:25:14 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 23:25:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:26:05 | INFO | train_inner | epoch 142:     20 / 97 loss=2.67, nll_loss=1.232, ppl=2.35, wps=24614.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=0.994, loss_scale=8, train_wall=236, gb_free=21, wall=36235
2022-03-04 23:29:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:29:28 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 12.685 | nll_loss 11.971 | ppl 4013.17 | wps 44955.7 | wpb 510.9 | bsz 1 | num_updates 13677 | best_loss 8.224
2022-03-04 23:29:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13677 updates
2022-03-04 23:29:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:29:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 142 @ 13677 updates, score 12.685) (writing took 2.347186205908656 seconds)
2022-03-04 23:29:30 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 23:29:30 | INFO | train | epoch 142 | loss 2.667 | nll_loss 1.229 | ppl 2.34 | wps 24830.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13677 | lr 0.000270399 | gnorm 0.997 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36439
2022-03-04 23:29:30 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 23:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:30:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:30:31 | INFO | train_inner | epoch 143:     24 / 97 loss=2.664, nll_loss=1.226, ppl=2.34, wps=24612.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1, loss_scale=8, train_wall=236, gb_free=21, wall=36501
2022-03-04 23:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:33:44 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 12.678 | nll_loss 11.961 | ppl 3985.9 | wps 44333.6 | wpb 510.9 | bsz 1 | num_updates 13773 | best_loss 8.224
2022-03-04 23:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13773 updates
2022-03-04 23:33:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:33:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:33:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 143 @ 13773 updates, score 12.678) (writing took 2.3397124791517854 seconds)
2022-03-04 23:33:46 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 23:33:46 | INFO | train | epoch 143 | loss 2.659 | nll_loss 1.222 | ppl 2.33 | wps 24554.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13773 | lr 0.000269455 | gnorm 1.001 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 36695
2022-03-04 23:33:46 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 23:33:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:34:55 | INFO | train_inner | epoch 144:     27 / 97 loss=2.657, nll_loss=1.219, ppl=2.33, wps=24832.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=0.998, loss_scale=8, train_wall=234, gb_free=21, wall=36765
2022-03-04 23:37:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:38:00 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 12.66 | nll_loss 11.947 | ppl 3947.09 | wps 45146.4 | wpb 510.9 | bsz 1 | num_updates 13870 | best_loss 8.224
2022-03-04 23:38:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13870 updates
2022-03-04 23:38:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:38:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:38:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 144 @ 13870 updates, score 12.66) (writing took 2.317391784861684 seconds)
2022-03-04 23:38:02 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 23:38:02 | INFO | train | epoch 144 | loss 2.653 | nll_loss 1.214 | ppl 2.32 | wps 24820.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13870 | lr 0.000268511 | gnorm 0.996 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36951
2022-03-04 23:38:02 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 23:38:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:39:19 | INFO | train_inner | epoch 145:     30 / 97 loss=2.651, nll_loss=1.212, ppl=2.32, wps=24840.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=0.99, loss_scale=16, train_wall=234, gb_free=21, wall=37028
2022-03-04 23:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:42:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:42:15 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 12.657 | nll_loss 11.937 | ppl 3920.84 | wps 45019 | wpb 510.9 | bsz 1 | num_updates 13966 | best_loss 8.224
2022-03-04 23:42:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13966 updates
2022-03-04 23:42:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:42:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 145 @ 13966 updates, score 12.657) (writing took 2.294095277786255 seconds)
2022-03-04 23:42:18 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 23:42:18 | INFO | train | epoch 145 | loss 2.645 | nll_loss 1.206 | ppl 2.31 | wps 24577.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13966 | lr 0.000267586 | gnorm 0.991 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37207
2022-03-04 23:42:18 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 23:42:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:43:45 | INFO | train_inner | epoch 146:     34 / 97 loss=2.644, nll_loss=1.205, ppl=2.31, wps=24616.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=1.003, loss_scale=16, train_wall=236, gb_free=21, wall=37294
2022-03-04 23:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:46:31 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 12.77 | nll_loss 12.057 | ppl 4262.55 | wps 45220.8 | wpb 510.9 | bsz 1 | num_updates 14063 | best_loss 8.224
2022-03-04 23:46:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14063 updates
2022-03-04 23:46:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:46:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:46:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 146 @ 14063 updates, score 12.77) (writing took 2.2856664154678583 seconds)
2022-03-04 23:46:34 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 23:46:34 | INFO | train | epoch 146 | loss 2.64 | nll_loss 1.2 | ppl 2.3 | wps 24823.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14063 | lr 0.000266662 | gnorm 0.993 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37463
2022-03-04 23:46:34 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 23:46:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:47:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:48:11 | INFO | train_inner | epoch 147:     38 / 97 loss=2.635, nll_loss=1.195, ppl=2.29, wps=24591.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=0.981, loss_scale=16, train_wall=237, gb_free=21, wall=37561
2022-03-04 23:49:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:50:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:50:47 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 12.646 | nll_loss 11.923 | ppl 3884.12 | wps 44894.6 | wpb 510.9 | bsz 1 | num_updates 14158 | best_loss 8.224
2022-03-04 23:50:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14158 updates
2022-03-04 23:50:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:50:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:50:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 147 @ 14158 updates, score 12.646) (writing took 2.278318924829364 seconds)
2022-03-04 23:50:50 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 23:50:50 | INFO | train | epoch 147 | loss 2.634 | nll_loss 1.194 | ppl 2.29 | wps 24285 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 14158 | lr 0.000265766 | gnorm 1.005 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 37719
2022-03-04 23:50:50 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 23:50:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:52:38 | INFO | train_inner | epoch 148:     42 / 97 loss=2.628, nll_loss=1.188, ppl=2.28, wps=24589, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=1.001, loss_scale=8, train_wall=237, gb_free=21, wall=37827
2022-03-04 23:54:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:55:03 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 12.716 | nll_loss 12.001 | ppl 4099.49 | wps 44951.1 | wpb 510.9 | bsz 1 | num_updates 14255 | best_loss 8.224
2022-03-04 23:55:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14255 updates
2022-03-04 23:55:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:55:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:55:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 148 @ 14255 updates, score 12.716) (writing took 2.281938921660185 seconds)
2022-03-04 23:55:06 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 23:55:06 | INFO | train | epoch 148 | loss 2.626 | nll_loss 1.185 | ppl 2.27 | wps 24822.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14255 | lr 0.00026486 | gnorm 0.986 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 37975
2022-03-04 23:55:06 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 23:55:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:57:01 | INFO | train_inner | epoch 149:     45 / 97 loss=2.626, nll_loss=1.186, ppl=2.28, wps=24832.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=0.997, loss_scale=16, train_wall=234, gb_free=21, wall=38091
2022-03-04 23:59:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:59:20 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 12.71 | nll_loss 11.998 | ppl 4089.5 | wps 44961.3 | wpb 510.9 | bsz 1 | num_updates 14352 | best_loss 8.224
2022-03-04 23:59:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14352 updates
2022-03-04 23:59:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-04 23:59:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 149 @ 14352 updates, score 12.71) (writing took 2.274584132246673 seconds)
2022-03-04 23:59:22 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-04 23:59:22 | INFO | train | epoch 149 | loss 2.622 | nll_loss 1.182 | ppl 2.27 | wps 24802.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14352 | lr 0.000263963 | gnorm 0.993 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38231
2022-03-04 23:59:22 | INFO | fairseq.trainer | begin training epoch 150
2022-03-04 23:59:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:59:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:01:28 | INFO | train_inner | epoch 150:     49 / 97 loss=2.619, nll_loss=1.178, ppl=2.26, wps=24571.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=0.992, loss_scale=8, train_wall=237, gb_free=21, wall=38357
2022-03-05 00:03:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:03:36 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 12.756 | nll_loss 12.045 | ppl 4224.96 | wps 44838.8 | wpb 510.9 | bsz 1 | num_updates 14448 | best_loss 8.224
2022-03-05 00:03:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14448 updates
2022-03-05 00:03:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:03:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 150 @ 14448 updates, score 12.756) (writing took 2.335227412171662 seconds)
2022-03-05 00:03:38 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-05 00:03:38 | INFO | train | epoch 150 | loss 2.615 | nll_loss 1.174 | ppl 2.26 | wps 24530.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14448 | lr 0.000263085 | gnorm 0.997 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 38487
2022-03-05 00:03:38 | INFO | fairseq.trainer | begin training epoch 151
2022-03-05 00:03:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:05:52 | INFO | train_inner | epoch 151:     52 / 97 loss=2.611, nll_loss=1.17, ppl=2.25, wps=24826.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=0.989, loss_scale=16, train_wall=234, gb_free=21, wall=38621
2022-03-05 00:07:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:07:52 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 12.773 | nll_loss 12.066 | ppl 4288.07 | wps 45033.2 | wpb 510.9 | bsz 1 | num_updates 14545 | best_loss 8.224
2022-03-05 00:07:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14545 updates
2022-03-05 00:07:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:07:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:07:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 151 @ 14545 updates, score 12.773) (writing took 2.4590730434283614 seconds)
2022-03-05 00:07:54 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-05 00:07:54 | INFO | train | epoch 151 | loss 2.609 | nll_loss 1.168 | ppl 2.25 | wps 24799.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14545 | lr 0.000262206 | gnorm 0.983 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38744
2022-03-05 00:07:54 | INFO | fairseq.trainer | begin training epoch 152
2022-03-05 00:07:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:10:16 | INFO | train_inner | epoch 152:     55 / 97 loss=2.606, nll_loss=1.164, ppl=2.24, wps=24821.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14600, lr=0.000261712, gnorm=0.986, loss_scale=16, train_wall=234, gb_free=21, wall=38885
2022-03-05 00:10:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:11:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:12:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:12:08 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 12.762 | nll_loss 12.059 | ppl 4266.77 | wps 44918 | wpb 510.9 | bsz 1 | num_updates 14640 | best_loss 8.224
2022-03-05 00:12:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14640 updates
2022-03-05 00:12:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:12:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:12:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 152 @ 14640 updates, score 12.762) (writing took 2.392795216292143 seconds)
2022-03-05 00:12:10 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-05 00:12:10 | INFO | train | epoch 152 | loss 2.602 | nll_loss 1.16 | ppl 2.23 | wps 24289.4 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 14640 | lr 0.000261354 | gnorm 0.988 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 39000
2022-03-05 00:12:10 | INFO | fairseq.trainer | begin training epoch 153
2022-03-05 00:12:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:14:44 | INFO | train_inner | epoch 153:     60 / 97 loss=2.6, nll_loss=1.159, ppl=2.23, wps=24367.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=0.99, loss_scale=8, train_wall=239, gb_free=21, wall=39154
2022-03-05 00:16:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:16:24 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 12.741 | nll_loss 12.035 | ppl 4195.45 | wps 44895.7 | wpb 510.9 | bsz 1 | num_updates 14737 | best_loss 8.224
2022-03-05 00:16:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14737 updates
2022-03-05 00:16:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:16:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 153 @ 14737 updates, score 12.741) (writing took 2.3403826663270593 seconds)
2022-03-05 00:16:26 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-05 00:16:26 | INFO | train | epoch 153 | loss 2.598 | nll_loss 1.156 | ppl 2.23 | wps 24828.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14737 | lr 0.000260493 | gnorm 1.002 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 39256
2022-03-05 00:16:26 | INFO | fairseq.trainer | begin training epoch 154
2022-03-05 00:16:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:19:08 | INFO | train_inner | epoch 154:     63 / 97 loss=2.595, nll_loss=1.153, ppl=2.22, wps=24844, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=0.994, loss_scale=16, train_wall=234, gb_free=21, wall=39417
2022-03-05 00:19:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:20:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:20:40 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 12.823 | nll_loss 12.12 | ppl 4452.68 | wps 45042.4 | wpb 510.9 | bsz 1 | num_updates 14833 | best_loss 8.224
2022-03-05 00:20:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14833 updates
2022-03-05 00:20:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:20:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:20:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 154 @ 14833 updates, score 12.823) (writing took 2.3230661200359464 seconds)
2022-03-05 00:20:42 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-05 00:20:42 | INFO | train | epoch 154 | loss 2.591 | nll_loss 1.149 | ppl 2.22 | wps 24564.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 14833 | lr 0.000259648 | gnorm 0.991 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 39512
2022-03-05 00:20:42 | INFO | fairseq.trainer | begin training epoch 155
2022-03-05 00:20:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:23:34 | INFO | train_inner | epoch 155:     67 / 97 loss=2.59, nll_loss=1.148, ppl=2.22, wps=24596.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=0.994, loss_scale=8, train_wall=237, gb_free=21, wall=39684
2022-03-05 00:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:24:56 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 12.761 | nll_loss 12.061 | ppl 4272.46 | wps 45134 | wpb 510.9 | bsz 1 | num_updates 14930 | best_loss 8.224
2022-03-05 00:24:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14930 updates
2022-03-05 00:24:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:24:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:24:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 155 @ 14930 updates, score 12.761) (writing took 2.36828123498708 seconds)
2022-03-05 00:24:58 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-05 00:24:58 | INFO | train | epoch 155 | loss 2.587 | nll_loss 1.145 | ppl 2.21 | wps 24811.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14930 | lr 0.000258803 | gnorm 0.985 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 39768
2022-03-05 00:24:58 | INFO | fairseq.trainer | begin training epoch 156
2022-03-05 00:24:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:25:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:28:00 | INFO | train_inner | epoch 156:     71 / 97 loss=2.584, nll_loss=1.141, ppl=2.2, wps=24602.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=0.992, loss_scale=8, train_wall=236, gb_free=21, wall=39950
2022-03-05 00:29:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:29:12 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 12.718 | nll_loss 12.01 | ppl 4123.06 | wps 45456.4 | wpb 510.9 | bsz 1 | num_updates 15026 | best_loss 8.224
2022-03-05 00:29:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15026 updates
2022-03-05 00:29:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:29:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:29:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 156 @ 15026 updates, score 12.718) (writing took 2.3888362273573875 seconds)
2022-03-05 00:29:14 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-05 00:29:14 | INFO | train | epoch 156 | loss 2.58 | nll_loss 1.137 | ppl 2.2 | wps 24572.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15026 | lr 0.000257975 | gnorm 0.994 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 40024
2022-03-05 00:29:14 | INFO | fairseq.trainer | begin training epoch 157
2022-03-05 00:29:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:32:24 | INFO | train_inner | epoch 157:     74 / 97 loss=2.577, nll_loss=1.133, ppl=2.19, wps=24850.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=0.982, loss_scale=16, train_wall=234, gb_free=21, wall=40213
2022-03-05 00:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:33:28 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 12.752 | nll_loss 12.051 | ppl 4243.86 | wps 45003 | wpb 510.9 | bsz 1 | num_updates 15123 | best_loss 8.224
2022-03-05 00:33:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15123 updates
2022-03-05 00:33:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:33:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:33:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 157 @ 15123 updates, score 12.752) (writing took 2.342198177240789 seconds)
2022-03-05 00:33:30 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-05 00:33:30 | INFO | train | epoch 157 | loss 2.576 | nll_loss 1.132 | ppl 2.19 | wps 24822.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15123 | lr 0.000257147 | gnorm 0.977 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40279
2022-03-05 00:33:30 | INFO | fairseq.trainer | begin training epoch 158
2022-03-05 00:33:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:34:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:36:50 | INFO | train_inner | epoch 158:     78 / 97 loss=2.571, nll_loss=1.128, ppl=2.19, wps=24608.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=0.97, loss_scale=8, train_wall=236, gb_free=21, wall=40479
2022-03-05 00:37:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:37:44 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 12.807 | nll_loss 12.109 | ppl 4416.46 | wps 45106.5 | wpb 510.9 | bsz 1 | num_updates 15219 | best_loss 8.224
2022-03-05 00:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15219 updates
2022-03-05 00:37:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:37:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:37:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 158 @ 15219 updates, score 12.807) (writing took 2.36683680023998 seconds)
2022-03-05 00:37:46 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-05 00:37:46 | INFO | train | epoch 158 | loss 2.568 | nll_loss 1.124 | ppl 2.18 | wps 24573.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15219 | lr 0.000256334 | gnorm 0.961 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 40535
2022-03-05 00:37:46 | INFO | fairseq.trainer | begin training epoch 159
2022-03-05 00:37:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:41:14 | INFO | train_inner | epoch 159:     81 / 97 loss=2.568, nll_loss=1.124, ppl=2.18, wps=24846.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=0.97, loss_scale=16, train_wall=234, gb_free=21, wall=40743
2022-03-05 00:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:42:00 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 12.786 | nll_loss 12.084 | ppl 4340.8 | wps 44910.3 | wpb 510.9 | bsz 1 | num_updates 15316 | best_loss 8.224
2022-03-05 00:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15316 updates
2022-03-05 00:42:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:42:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 159 @ 15316 updates, score 12.786) (writing took 2.3484518621116877 seconds)
2022-03-05 00:42:02 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-05 00:42:02 | INFO | train | epoch 159 | loss 2.565 | nll_loss 1.121 | ppl 2.18 | wps 24822.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15316 | lr 0.000255521 | gnorm 0.975 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40791
2022-03-05 00:42:02 | INFO | fairseq.trainer | begin training epoch 160
2022-03-05 00:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:45:37 | INFO | train_inner | epoch 160:     84 / 97 loss=2.561, nll_loss=1.117, ppl=2.17, wps=24849.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=0.969, loss_scale=16, train_wall=234, gb_free=21, wall=41007
2022-03-05 00:46:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:46:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:46:15 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 12.775 | nll_loss 12.072 | ppl 4304.48 | wps 44984.3 | wpb 510.9 | bsz 1 | num_updates 15412 | best_loss 8.224
2022-03-05 00:46:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15412 updates
2022-03-05 00:46:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:46:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:46:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 160 @ 15412 updates, score 12.775) (writing took 2.395369090139866 seconds)
2022-03-05 00:46:18 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-05 00:46:18 | INFO | train | epoch 160 | loss 2.559 | nll_loss 1.115 | ppl 2.17 | wps 24572.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15412 | lr 0.000254724 | gnorm 0.965 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41047
2022-03-05 00:46:18 | INFO | fairseq.trainer | begin training epoch 161
2022-03-05 00:46:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:50:03 | INFO | train_inner | epoch 161:     88 / 97 loss=2.556, nll_loss=1.111, ppl=2.16, wps=24600.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=0.982, loss_scale=16, train_wall=236, gb_free=21, wall=41273
2022-03-05 00:50:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:50:31 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 12.808 | nll_loss 12.11 | ppl 4419.73 | wps 44954.4 | wpb 510.9 | bsz 1 | num_updates 15509 | best_loss 8.224
2022-03-05 00:50:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15509 updates
2022-03-05 00:50:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:50:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:50:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 161 @ 15509 updates, score 12.808) (writing took 2.3558749509975314 seconds)
2022-03-05 00:50:34 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-05 00:50:34 | INFO | train | epoch 161 | loss 2.556 | nll_loss 1.111 | ppl 2.16 | wps 24821.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15509 | lr 0.000253927 | gnorm 0.987 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41303
2022-03-05 00:50:34 | INFO | fairseq.trainer | begin training epoch 162
2022-03-05 00:50:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:52:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:54:30 | INFO | train_inner | epoch 162:     92 / 97 loss=2.55, nll_loss=1.105, ppl=2.15, wps=24611.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=0.977, loss_scale=16, train_wall=236, gb_free=21, wall=41539
2022-03-05 00:54:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:54:47 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 12.772 | nll_loss 12.072 | ppl 4306.07 | wps 45209 | wpb 510.9 | bsz 1 | num_updates 15605 | best_loss 8.224
2022-03-05 00:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15605 updates
2022-03-05 00:54:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:54:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 162 @ 15605 updates, score 12.772) (writing took 2.341845448128879 seconds)
2022-03-05 00:54:49 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-05 00:54:49 | INFO | train | epoch 162 | loss 2.548 | nll_loss 1.103 | ppl 2.15 | wps 24575.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15605 | lr 0.000253144 | gnorm 0.976 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41559
2022-03-05 00:54:50 | INFO | fairseq.trainer | begin training epoch 163
2022-03-05 00:54:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:56:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:58:56 | INFO | train_inner | epoch 163:     96 / 97 loss=2.546, nll_loss=1.101, ppl=2.14, wps=24605.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15700, lr=0.000252377, gnorm=0.99, loss_scale=8, train_wall=236, gb_free=21, wall=41805
2022-03-05 00:58:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:59:03 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 12.816 | nll_loss 12.117 | ppl 4442.66 | wps 44924.2 | wpb 510.9 | bsz 1 | num_updates 15701 | best_loss 8.224
2022-03-05 00:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15701 updates
2022-03-05 00:59:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:59:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 00:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 163 @ 15701 updates, score 12.816) (writing took 2.316251807846129 seconds)
2022-03-05 00:59:05 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-05 00:59:05 | INFO | train | epoch 163 | loss 2.543 | nll_loss 1.098 | ppl 2.14 | wps 24562.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15701 | lr 0.000252369 | gnorm 0.99 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 41815
2022-03-05 00:59:05 | INFO | fairseq.trainer | begin training epoch 164
2022-03-05 00:59:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:03:19 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 12.771 | nll_loss 12.066 | ppl 4287.19 | wps 45118.1 | wpb 510.9 | bsz 1 | num_updates 15798 | best_loss 8.224
2022-03-05 01:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15798 updates
2022-03-05 01:03:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:03:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:03:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 164 @ 15798 updates, score 12.771) (writing took 2.37636436522007 seconds)
2022-03-05 01:03:21 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-05 01:03:21 | INFO | train | epoch 164 | loss 2.539 | nll_loss 1.094 | ppl 2.13 | wps 24811.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15798 | lr 0.000251593 | gnorm 0.978 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42071
2022-03-05 01:03:22 | INFO | fairseq.trainer | begin training epoch 165
2022-03-05 01:03:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:03:27 | INFO | train_inner | epoch 165:      2 / 97 loss=2.539, nll_loss=1.094, ppl=2.13, wps=24154.8, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=15800, lr=0.000251577, gnorm=0.979, loss_scale=16, train_wall=234, gb_free=21, wall=42076
2022-03-05 01:04:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:07:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:07:35 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 12.844 | nll_loss 12.147 | ppl 4534.52 | wps 44945.1 | wpb 510.9 | bsz 1 | num_updates 15894 | best_loss 8.224
2022-03-05 01:07:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15894 updates
2022-03-05 01:07:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:07:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:07:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 165 @ 15894 updates, score 12.844) (writing took 2.3576114550232887 seconds)
2022-03-05 01:07:37 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-05 01:07:37 | INFO | train | epoch 165 | loss 2.533 | nll_loss 1.087 | ppl 2.12 | wps 24568.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15894 | lr 0.000250832 | gnorm 0.972 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 42327
2022-03-05 01:07:37 | INFO | fairseq.trainer | begin training epoch 166
2022-03-05 01:07:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:07:53 | INFO | train_inner | epoch 166:      6 / 97 loss=2.533, nll_loss=1.087, ppl=2.12, wps=24608.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15900, lr=0.000250785, gnorm=0.973, loss_scale=8, train_wall=236, gb_free=21, wall=42342
2022-03-05 01:10:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:11:51 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 12.818 | nll_loss 12.124 | ppl 4464.96 | wps 44908.4 | wpb 510.9 | bsz 1 | num_updates 15990 | best_loss 8.224
2022-03-05 01:11:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15990 updates
2022-03-05 01:11:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:11:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:11:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 166 @ 15990 updates, score 12.818) (writing took 2.333715875633061 seconds)
2022-03-05 01:11:53 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-05 01:11:53 | INFO | train | epoch 166 | loss 2.528 | nll_loss 1.082 | ppl 2.12 | wps 24577.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 15990 | lr 0.000250078 | gnorm 0.977 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 42583
2022-03-05 01:11:53 | INFO | fairseq.trainer | begin training epoch 167
2022-03-05 01:11:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:12:19 | INFO | train_inner | epoch 167:     10 / 97 loss=2.526, nll_loss=1.079, ppl=2.11, wps=24615.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16000, lr=0.00025, gnorm=0.973, loss_scale=8, train_wall=236, gb_free=21, wall=42608
2022-03-05 01:16:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:16:07 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 12.813 | nll_loss 12.118 | ppl 4443.93 | wps 44911.5 | wpb 510.9 | bsz 1 | num_updates 16087 | best_loss 8.224
2022-03-05 01:16:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16087 updates
2022-03-05 01:16:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:16:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 167 @ 16087 updates, score 12.813) (writing took 2.3162976559251547 seconds)
2022-03-05 01:16:09 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-05 01:16:09 | INFO | train | epoch 167 | loss 2.525 | nll_loss 1.079 | ppl 2.11 | wps 24826.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16087 | lr 0.000249323 | gnorm 0.973 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42838
2022-03-05 01:16:09 | INFO | fairseq.trainer | begin training epoch 168
2022-03-05 01:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:16:43 | INFO | train_inner | epoch 168:     13 / 97 loss=2.522, nll_loss=1.076, ppl=2.11, wps=24847.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=0.975, loss_scale=16, train_wall=234, gb_free=21, wall=42872
2022-03-05 01:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:20:23 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 12.908 | nll_loss 12.217 | ppl 4762.29 | wps 45033.1 | wpb 510.9 | bsz 1 | num_updates 16184 | best_loss 8.224
2022-03-05 01:20:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16184 updates
2022-03-05 01:20:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:20:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:20:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 168 @ 16184 updates, score 12.908) (writing took 2.3137298822402954 seconds)
2022-03-05 01:20:25 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-05 01:20:25 | INFO | train | epoch 168 | loss 2.52 | nll_loss 1.074 | ppl 2.11 | wps 24831 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16184 | lr 0.000248575 | gnorm 0.976 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43094
2022-03-05 01:20:25 | INFO | fairseq.trainer | begin training epoch 169
2022-03-05 01:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:21:06 | INFO | train_inner | epoch 169:     16 / 97 loss=2.521, nll_loss=1.074, ppl=2.11, wps=24851.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=0.978, loss_scale=16, train_wall=234, gb_free=21, wall=43135
2022-03-05 01:21:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:24:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:24:39 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 12.845 | nll_loss 12.15 | ppl 4544.57 | wps 45073.3 | wpb 510.9 | bsz 1 | num_updates 16280 | best_loss 8.224
2022-03-05 01:24:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16280 updates
2022-03-05 01:24:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:24:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:24:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 169 @ 16280 updates, score 12.845) (writing took 2.3852319326251745 seconds)
2022-03-05 01:24:41 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-05 01:24:41 | INFO | train | epoch 169 | loss 2.514 | nll_loss 1.067 | ppl 2.1 | wps 24539.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16280 | lr 0.000247841 | gnorm 0.974 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43350
2022-03-05 01:24:41 | INFO | fairseq.trainer | begin training epoch 170
2022-03-05 01:24:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:25:32 | INFO | train_inner | epoch 170:     20 / 97 loss=2.511, nll_loss=1.064, ppl=2.09, wps=24580.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=0.964, loss_scale=16, train_wall=237, gb_free=21, wall=43402
2022-03-05 01:27:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:28:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:28:55 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 12.869 | nll_loss 12.169 | ppl 4606.26 | wps 44945.9 | wpb 510.9 | bsz 1 | num_updates 16376 | best_loss 8.224
2022-03-05 01:28:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16376 updates
2022-03-05 01:28:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:28:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:28:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 170 @ 16376 updates, score 12.869) (writing took 2.348514028824866 seconds)
2022-03-05 01:28:57 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-05 01:28:57 | INFO | train | epoch 170 | loss 2.511 | nll_loss 1.064 | ppl 2.09 | wps 24571.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 16376 | lr 0.000247113 | gnorm 0.965 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43606
2022-03-05 01:28:57 | INFO | fairseq.trainer | begin training epoch 171
2022-03-05 01:28:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:29:59 | INFO | train_inner | epoch 171:     24 / 97 loss=2.509, nll_loss=1.062, ppl=2.09, wps=24605.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=0.965, loss_scale=16, train_wall=236, gb_free=21, wall=43668
2022-03-05 01:32:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:33:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:33:11 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 12.836 | nll_loss 12.146 | ppl 4530.8 | wps 45238.5 | wpb 510.9 | bsz 1 | num_updates 16472 | best_loss 8.224
2022-03-05 01:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16472 updates
2022-03-05 01:33:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:33:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 171 @ 16472 updates, score 12.836) (writing took 2.407958770170808 seconds)
2022-03-05 01:33:13 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-05 01:33:13 | INFO | train | epoch 171 | loss 2.507 | nll_loss 1.06 | ppl 2.08 | wps 24564.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 16472 | lr 0.000246392 | gnorm 0.963 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43862
2022-03-05 01:33:13 | INFO | fairseq.trainer | begin training epoch 172
2022-03-05 01:33:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:25 | INFO | train_inner | epoch 172:     28 / 97 loss=2.505, nll_loss=1.058, ppl=2.08, wps=24607.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=0.962, loss_scale=16, train_wall=236, gb_free=21, wall=43934
2022-03-05 01:37:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:37:27 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 12.891 | nll_loss 12.208 | ppl 4729.62 | wps 44887.6 | wpb 510.9 | bsz 1 | num_updates 16569 | best_loss 8.224
2022-03-05 01:37:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16569 updates
2022-03-05 01:37:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:37:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 172 @ 16569 updates, score 12.891) (writing took 2.4153495645150542 seconds)
2022-03-05 01:37:29 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-05 01:37:29 | INFO | train | epoch 172 | loss 2.502 | nll_loss 1.055 | ppl 2.08 | wps 24811.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16569 | lr 0.00024567 | gnorm 0.958 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44118
2022-03-05 01:37:29 | INFO | fairseq.trainer | begin training epoch 173
2022-03-05 01:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:38:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:38:51 | INFO | train_inner | epoch 173:     32 / 97 loss=2.502, nll_loss=1.055, ppl=2.08, wps=24591.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=0.964, loss_scale=16, train_wall=236, gb_free=21, wall=44200
2022-03-05 01:38:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:41:43 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 12.826 | nll_loss 12.135 | ppl 4498.12 | wps 44970.4 | wpb 510.9 | bsz 1 | num_updates 16664 | best_loss 8.224
2022-03-05 01:41:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16664 updates
2022-03-05 01:41:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:41:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:41:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 173 @ 16664 updates, score 12.826) (writing took 2.387060744687915 seconds)
2022-03-05 01:41:45 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-05 01:41:45 | INFO | train | epoch 173 | loss 2.498 | nll_loss 1.05 | ppl 2.07 | wps 24301.8 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 16664 | lr 0.000244969 | gnorm 0.967 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 44374
2022-03-05 01:41:45 | INFO | fairseq.trainer | begin training epoch 174
2022-03-05 01:41:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:43:18 | INFO | train_inner | epoch 174:     36 / 97 loss=2.495, nll_loss=1.047, ppl=2.07, wps=24580.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=0.966, loss_scale=8, train_wall=237, gb_free=21, wall=44467
2022-03-05 01:45:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:45:59 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 12.875 | nll_loss 12.189 | ppl 4667.74 | wps 44997.3 | wpb 510.9 | bsz 1 | num_updates 16761 | best_loss 8.224
2022-03-05 01:45:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16761 updates
2022-03-05 01:45:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:46:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:46:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 174 @ 16761 updates, score 12.875) (writing took 2.242443660274148 seconds)
2022-03-05 01:46:01 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-05 01:46:01 | INFO | train | epoch 174 | loss 2.494 | nll_loss 1.047 | ppl 2.07 | wps 24814.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16761 | lr 0.000244259 | gnorm 0.972 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44630
2022-03-05 01:46:01 | INFO | fairseq.trainer | begin training epoch 175
2022-03-05 01:46:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:47:41 | INFO | train_inner | epoch 175:     39 / 97 loss=2.492, nll_loss=1.044, ppl=2.06, wps=24845, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=0.977, loss_scale=16, train_wall=234, gb_free=21, wall=44731
2022-03-05 01:50:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:50:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:50:15 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 12.845 | nll_loss 12.157 | ppl 4567.51 | wps 44839.6 | wpb 510.9 | bsz 1 | num_updates 16857 | best_loss 8.224
2022-03-05 01:50:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16857 updates
2022-03-05 01:50:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:50:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:50:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 175 @ 16857 updates, score 12.845) (writing took 2.2825490813702345 seconds)
2022-03-05 01:50:17 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-05 01:50:17 | INFO | train | epoch 175 | loss 2.488 | nll_loss 1.04 | ppl 2.06 | wps 24574 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 16857 | lr 0.000243562 | gnorm 0.969 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44886
2022-03-05 01:50:17 | INFO | fairseq.trainer | begin training epoch 176
2022-03-05 01:50:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:52:07 | INFO | train_inner | epoch 176:     43 / 97 loss=2.485, nll_loss=1.037, ppl=2.05, wps=24612.7, ups=0.38, wpb=65531.7, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.952, loss_scale=16, train_wall=237, gb_free=21, wall=44997
2022-03-05 01:53:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:54:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:54:31 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 12.847 | nll_loss 12.157 | ppl 4566.6 | wps 45090.9 | wpb 510.9 | bsz 1 | num_updates 16953 | best_loss 8.224
2022-03-05 01:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16953 updates
2022-03-05 01:54:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:54:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 176 @ 16953 updates, score 12.847) (writing took 2.2986534694209695 seconds)
2022-03-05 01:54:33 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-05 01:54:33 | INFO | train | epoch 176 | loss 2.485 | nll_loss 1.037 | ppl 2.05 | wps 24566 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 16953 | lr 0.000242872 | gnorm 0.961 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 45142
2022-03-05 01:54:33 | INFO | fairseq.trainer | begin training epoch 177
2022-03-05 01:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:56:34 | INFO | train_inner | epoch 177:     47 / 97 loss=2.483, nll_loss=1.035, ppl=2.05, wps=24606.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=0.962, loss_scale=8, train_wall=236, gb_free=21, wall=45263
2022-03-05 01:58:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:58:47 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 12.846 | nll_loss 12.153 | ppl 4553.15 | wps 45173.3 | wpb 510.9 | bsz 1 | num_updates 17050 | best_loss 8.224
2022-03-05 01:58:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17050 updates
2022-03-05 01:58:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:58:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 01:58:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 177 @ 17050 updates, score 12.846) (writing took 2.3459647996351123 seconds)
2022-03-05 01:58:49 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-05 01:58:49 | INFO | train | epoch 177 | loss 2.481 | nll_loss 1.033 | ppl 2.05 | wps 24801.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17050 | lr 0.00024218 | gnorm 0.959 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 45398
2022-03-05 01:58:49 | INFO | fairseq.trainer | begin training epoch 178
2022-03-05 01:58:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:00:57 | INFO | train_inner | epoch 178:     50 / 97 loss=2.477, nll_loss=1.029, ppl=2.04, wps=24829.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=0.959, loss_scale=16, train_wall=234, gb_free=21, wall=45527
2022-03-05 02:02:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:03:03 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 12.888 | nll_loss 12.199 | ppl 4700.36 | wps 44823 | wpb 510.9 | bsz 1 | num_updates 17147 | best_loss 8.224
2022-03-05 02:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17147 updates
2022-03-05 02:03:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:03:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:03:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 178 @ 17147 updates, score 12.888) (writing took 2.4262982150539756 seconds)
2022-03-05 02:03:05 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-05 02:03:05 | INFO | train | epoch 178 | loss 2.477 | nll_loss 1.029 | ppl 2.04 | wps 24814.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17147 | lr 0.000241494 | gnorm 0.959 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 45654
2022-03-05 02:03:05 | INFO | fairseq.trainer | begin training epoch 179
2022-03-05 02:03:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:05:24 | INFO | train_inner | epoch 179:     54 / 97 loss=2.478, nll_loss=1.03, ppl=2.04, wps=24583.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=0.962, loss_scale=8, train_wall=236, gb_free=21, wall=45793
2022-03-05 02:07:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:07:19 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 12.849 | nll_loss 12.162 | ppl 4583.71 | wps 45091.4 | wpb 510.9 | bsz 1 | num_updates 17243 | best_loss 8.224
2022-03-05 02:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17243 updates
2022-03-05 02:07:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:07:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 179 @ 17243 updates, score 12.849) (writing took 2.4422092139720917 seconds)
2022-03-05 02:07:21 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-05 02:07:21 | INFO | train | epoch 179 | loss 2.472 | nll_loss 1.024 | ppl 2.03 | wps 24539.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17243 | lr 0.000240821 | gnorm 0.947 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 45911
2022-03-05 02:07:21 | INFO | fairseq.trainer | begin training epoch 180
2022-03-05 02:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:09:48 | INFO | train_inner | epoch 180:     57 / 97 loss=2.47, nll_loss=1.021, ppl=2.03, wps=24815.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=0.955, loss_scale=8, train_wall=234, gb_free=21, wall=46057
2022-03-05 02:11:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:11:35 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 12.884 | nll_loss 12.194 | ppl 4684.36 | wps 45066.7 | wpb 510.9 | bsz 1 | num_updates 17340 | best_loss 8.224
2022-03-05 02:11:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17340 updates
2022-03-05 02:11:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:11:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:11:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 180 @ 17340 updates, score 12.884) (writing took 2.3917255653068423 seconds)
2022-03-05 02:11:38 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-05 02:11:38 | INFO | train | epoch 180 | loss 2.469 | nll_loss 1.021 | ppl 2.03 | wps 24787 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17340 | lr 0.000240146 | gnorm 0.97 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46167
2022-03-05 02:11:38 | INFO | fairseq.trainer | begin training epoch 181
2022-03-05 02:11:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:14:12 | INFO | train_inner | epoch 181:     60 / 97 loss=2.464, nll_loss=1.015, ppl=2.02, wps=24814.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=0.961, loss_scale=16, train_wall=234, gb_free=21, wall=46321
2022-03-05 02:15:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:15:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:15:51 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 12.904 | nll_loss 12.216 | ppl 4757.82 | wps 44998.5 | wpb 510.9 | bsz 1 | num_updates 17436 | best_loss 8.224
2022-03-05 02:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17436 updates
2022-03-05 02:15:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:15:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:15:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 181 @ 17436 updates, score 12.904) (writing took 2.3114700252190232 seconds)
2022-03-05 02:15:54 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-05 02:15:54 | INFO | train | epoch 181 | loss 2.463 | nll_loss 1.014 | ppl 2.02 | wps 24553.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17436 | lr 0.000239484 | gnorm 0.95 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46423
2022-03-05 02:15:54 | INFO | fairseq.trainer | begin training epoch 182
2022-03-05 02:15:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:18:38 | INFO | train_inner | epoch 182:     64 / 97 loss=2.463, nll_loss=1.014, ppl=2.02, wps=24598.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=0.946, loss_scale=16, train_wall=236, gb_free=21, wall=46587
2022-03-05 02:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:20:07 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 12.897 | nll_loss 12.209 | ppl 4733.58 | wps 44912.4 | wpb 510.9 | bsz 1 | num_updates 17533 | best_loss 8.224
2022-03-05 02:20:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17533 updates
2022-03-05 02:20:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:20:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:20:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 182 @ 17533 updates, score 12.897) (writing took 2.408320521004498 seconds)
2022-03-05 02:20:10 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-05 02:20:10 | INFO | train | epoch 182 | loss 2.461 | nll_loss 1.012 | ppl 2.02 | wps 24809.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17533 | lr 0.000238821 | gnorm 0.943 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46679
2022-03-05 02:20:10 | INFO | fairseq.trainer | begin training epoch 183
2022-03-05 02:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:21:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:23:04 | INFO | train_inner | epoch 183:     68 / 97 loss=2.459, nll_loss=1.01, ppl=2.01, wps=24585.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=0.942, loss_scale=16, train_wall=236, gb_free=21, wall=46854
2022-03-05 02:24:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:24:23 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 12.941 | nll_loss 12.265 | ppl 4921.76 | wps 45073.3 | wpb 510.9 | bsz 1 | num_updates 17629 | best_loss 8.224
2022-03-05 02:24:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17629 updates
2022-03-05 02:24:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:24:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:24:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 183 @ 17629 updates, score 12.941) (writing took 2.420308501459658 seconds)
2022-03-05 02:24:26 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-05 02:24:26 | INFO | train | epoch 183 | loss 2.457 | nll_loss 1.008 | ppl 2.01 | wps 24556.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17629 | lr 0.00023817 | gnorm 0.939 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46935
2022-03-05 02:24:26 | INFO | fairseq.trainer | begin training epoch 184
2022-03-05 02:24:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:27:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:27:30 | INFO | train_inner | epoch 184:     72 / 97 loss=2.455, nll_loss=1.006, ppl=2.01, wps=24611.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.94, loss_scale=16, train_wall=236, gb_free=21, wall=47120
2022-03-05 02:28:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:28:39 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 12.946 | nll_loss 12.26 | ppl 4906.41 | wps 45069.7 | wpb 510.9 | bsz 1 | num_updates 17725 | best_loss 8.224
2022-03-05 02:28:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17725 updates
2022-03-05 02:28:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:28:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 184 @ 17725 updates, score 12.946) (writing took 2.412087131291628 seconds)
2022-03-05 02:28:42 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-05 02:28:42 | INFO | train | epoch 184 | loss 2.452 | nll_loss 1.003 | ppl 2 | wps 24571.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 17725 | lr 0.000237524 | gnorm 0.947 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47191
2022-03-05 02:28:42 | INFO | fairseq.trainer | begin training epoch 185
2022-03-05 02:28:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:31:54 | INFO | train_inner | epoch 185:     75 / 97 loss=2.452, nll_loss=1.003, ppl=2, wps=24836.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=0.954, loss_scale=16, train_wall=234, gb_free=21, wall=47383
2022-03-05 02:32:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:32:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:32:55 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 12.899 | nll_loss 12.219 | ppl 4767.54 | wps 44854.5 | wpb 510.9 | bsz 1 | num_updates 17821 | best_loss 8.224
2022-03-05 02:32:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17821 updates
2022-03-05 02:32:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:32:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:32:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 185 @ 17821 updates, score 12.899) (writing took 2.370282660238445 seconds)
2022-03-05 02:32:58 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-05 02:32:58 | INFO | train | epoch 185 | loss 2.449 | nll_loss 1 | ppl 2 | wps 24557.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17821 | lr 0.000236883 | gnorm 0.95 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47447
2022-03-05 02:32:58 | INFO | fairseq.trainer | begin training epoch 186
2022-03-05 02:32:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:36:20 | INFO | train_inner | epoch 186:     79 / 97 loss=2.446, nll_loss=0.996, ppl=2, wps=24590.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17900, lr=0.00023636, gnorm=0.96, loss_scale=16, train_wall=236, gb_free=21, wall=47650
2022-03-05 02:37:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:37:11 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 12.906 | nll_loss 12.224 | ppl 4784.11 | wps 45049.1 | wpb 510.9 | bsz 1 | num_updates 17918 | best_loss 8.224
2022-03-05 02:37:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17918 updates
2022-03-05 02:37:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:37:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:37:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 186 @ 17918 updates, score 12.906) (writing took 2.396127258427441 seconds)
2022-03-05 02:37:14 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-05 02:37:14 | INFO | train | epoch 186 | loss 2.446 | nll_loss 0.997 | ppl 2 | wps 24809.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17918 | lr 0.000236241 | gnorm 0.957 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47703
2022-03-05 02:37:14 | INFO | fairseq.trainer | begin training epoch 187
2022-03-05 02:37:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:38:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:38:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:40:49 | INFO | train_inner | epoch 187:     84 / 97 loss=2.444, nll_loss=0.994, ppl=1.99, wps=24367, ups=0.37, wpb=65495, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=0.945, loss_scale=8, train_wall=239, gb_free=21, wall=47919
2022-03-05 02:41:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:41:27 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 12.857 | nll_loss 12.181 | ppl 4644.01 | wps 44866.8 | wpb 510.9 | bsz 1 | num_updates 18013 | best_loss 8.224
2022-03-05 02:41:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18013 updates
2022-03-05 02:41:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:41:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:41:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 187 @ 18013 updates, score 12.857) (writing took 2.3577437410131097 seconds)
2022-03-05 02:41:30 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-05 02:41:30 | INFO | train | epoch 187 | loss 2.441 | nll_loss 0.991 | ppl 1.99 | wps 24306.6 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 18013 | lr 0.000235617 | gnorm 0.95 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 47959
2022-03-05 02:41:30 | INFO | fairseq.trainer | begin training epoch 188
2022-03-05 02:41:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:45:13 | INFO | train_inner | epoch 188:     87 / 97 loss=2.44, nll_loss=0.99, ppl=1.99, wps=24813.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.955, loss_scale=16, train_wall=234, gb_free=21, wall=48182
2022-03-05 02:45:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:45:44 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 12.951 | nll_loss 12.267 | ppl 4927.74 | wps 44876 | wpb 510.9 | bsz 1 | num_updates 18110 | best_loss 8.224
2022-03-05 02:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18110 updates
2022-03-05 02:45:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:45:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:45:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 188 @ 18110 updates, score 12.951) (writing took 2.3022199692204595 seconds)
2022-03-05 02:45:46 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-05 02:45:46 | INFO | train | epoch 188 | loss 2.439 | nll_loss 0.99 | ppl 1.99 | wps 24794.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18110 | lr 0.000234985 | gnorm 0.95 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 48215
2022-03-05 02:45:46 | INFO | fairseq.trainer | begin training epoch 189
2022-03-05 02:45:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:48:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:49:39 | INFO | train_inner | epoch 189:     91 / 97 loss=2.436, nll_loss=0.987, ppl=1.98, wps=24617.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.949, loss_scale=8, train_wall=236, gb_free=21, wall=48449
2022-03-05 02:49:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:49:59 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 12.965 | nll_loss 12.292 | ppl 5013.65 | wps 44863.4 | wpb 510.9 | bsz 1 | num_updates 18206 | best_loss 8.224
2022-03-05 02:49:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18206 updates
2022-03-05 02:49:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:50:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:50:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 189 @ 18206 updates, score 12.965) (writing took 2.367224092595279 seconds)
2022-03-05 02:50:02 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 02:50:02 | INFO | train | epoch 189 | loss 2.434 | nll_loss 0.984 | ppl 1.98 | wps 24577.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 18206 | lr 0.000234365 | gnorm 0.95 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 48471
2022-03-05 02:50:02 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 02:50:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:54:03 | INFO | train_inner | epoch 190:     94 / 97 loss=2.432, nll_loss=0.982, ppl=1.98, wps=24856.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18300, lr=0.000233762, gnorm=0.959, loss_scale=8, train_wall=234, gb_free=21, wall=48712
2022-03-05 02:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:54:15 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 12.962 | nll_loss 12.294 | ppl 5020.35 | wps 45004.5 | wpb 510.9 | bsz 1 | num_updates 18303 | best_loss 8.224
2022-03-05 02:54:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18303 updates
2022-03-05 02:54:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:54:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:54:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 190 @ 18303 updates, score 12.962) (writing took 2.348291006870568 seconds)
2022-03-05 02:54:17 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 02:54:17 | INFO | train | epoch 190 | loss 2.431 | nll_loss 0.981 | ppl 1.97 | wps 24835.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18303 | lr 0.000233743 | gnorm 0.959 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 48727
2022-03-05 02:54:17 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 02:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:54:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:58:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:58:31 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 12.963 | nll_loss 12.288 | ppl 5002.59 | wps 45098.5 | wpb 510.9 | bsz 1 | num_updates 18399 | best_loss 8.224
2022-03-05 02:58:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18399 updates
2022-03-05 02:58:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 02:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 191 @ 18399 updates, score 12.963) (writing took 2.338163890875876 seconds)
2022-03-05 02:58:33 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 02:58:33 | INFO | train | epoch 191 | loss 2.425 | nll_loss 0.975 | ppl 1.97 | wps 24587.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 18399 | lr 0.000233133 | gnorm 0.942 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 48983
2022-03-05 02:58:33 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 02:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:58:36 | INFO | train_inner | epoch 192:      1 / 97 loss=2.426, nll_loss=0.976, ppl=1.97, wps=23958.2, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=18400, lr=0.000233126, gnorm=0.943, loss_scale=8, train_wall=236, gb_free=21, wall=48985
2022-03-05 03:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:02:47 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 12.887 | nll_loss 12.21 | ppl 4738.21 | wps 45038 | wpb 510.9 | bsz 1 | num_updates 18496 | best_loss 8.224
2022-03-05 03:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18496 updates
2022-03-05 03:02:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 192 @ 18496 updates, score 12.887) (writing took 2.3539859233424067 seconds)
2022-03-05 03:02:49 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 03:02:49 | INFO | train | epoch 192 | loss 2.424 | nll_loss 0.974 | ppl 1.96 | wps 24834.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18496 | lr 0.00023252 | gnorm 0.941 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 49238
2022-03-05 03:02:49 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 03:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:02:59 | INFO | train_inner | epoch 193:      4 / 97 loss=2.423, nll_loss=0.972, ppl=1.96, wps=24856.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18500, lr=0.000232495, gnorm=0.94, loss_scale=16, train_wall=234, gb_free=21, wall=49249
2022-03-05 03:05:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:07:02 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 12.957 | nll_loss 12.283 | ppl 4983.12 | wps 44950.3 | wpb 510.9 | bsz 1 | num_updates 18592 | best_loss 8.224
2022-03-05 03:07:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18592 updates
2022-03-05 03:07:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 193 @ 18592 updates, score 12.957) (writing took 2.3899443969130516 seconds)
2022-03-05 03:07:05 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 03:07:05 | INFO | train | epoch 193 | loss 2.42 | nll_loss 0.97 | ppl 1.96 | wps 24574.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 18592 | lr 0.000231919 | gnorm 0.954 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 49494
2022-03-05 03:07:05 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 03:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:07:25 | INFO | train_inner | epoch 194:      8 / 97 loss=2.419, nll_loss=0.969, ppl=1.96, wps=24616.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18600, lr=0.000231869, gnorm=0.953, loss_scale=16, train_wall=236, gb_free=21, wall=49515
2022-03-05 03:08:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:11:18 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 12.941 | nll_loss 12.263 | ppl 4916.52 | wps 45156.9 | wpb 510.9 | bsz 1 | num_updates 18688 | best_loss 8.224
2022-03-05 03:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18688 updates
2022-03-05 03:11:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 194 @ 18688 updates, score 12.941) (writing took 2.4015957303345203 seconds)
2022-03-05 03:11:20 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 03:11:20 | INFO | train | epoch 194 | loss 2.416 | nll_loss 0.966 | ppl 1.95 | wps 24607.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 18688 | lr 0.000231323 | gnorm 0.941 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 49750
2022-03-05 03:11:20 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 03:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:11:51 | INFO | train_inner | epoch 195:     12 / 97 loss=2.414, nll_loss=0.964, ppl=1.95, wps=24638, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18700, lr=0.000231249, gnorm=0.941, loss_scale=8, train_wall=236, gb_free=21, wall=49781
2022-03-05 03:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:15:33 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 12.972 | nll_loss 12.298 | ppl 5037.07 | wps 45178.5 | wpb 510.9 | bsz 1 | num_updates 18785 | best_loss 8.224
2022-03-05 03:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18785 updates
2022-03-05 03:15:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 195 @ 18785 updates, score 12.972) (writing took 2.3530559446662664 seconds)
2022-03-05 03:15:36 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 03:15:36 | INFO | train | epoch 195 | loss 2.412 | nll_loss 0.962 | ppl 1.95 | wps 24860.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18785 | lr 0.000230725 | gnorm 0.935 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50005
2022-03-05 03:15:36 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 03:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:16:14 | INFO | train_inner | epoch 196:     15 / 97 loss=2.411, nll_loss=0.961, ppl=1.95, wps=24890.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.931, loss_scale=16, train_wall=234, gb_free=21, wall=50044
2022-03-05 03:18:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:19:49 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 12.911 | nll_loss 12.229 | ppl 4800.85 | wps 45118.3 | wpb 510.9 | bsz 1 | num_updates 18881 | best_loss 8.224
2022-03-05 03:19:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18881 updates
2022-03-05 03:19:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:19:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:19:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 196 @ 18881 updates, score 12.911) (writing took 2.3570432709529996 seconds)
2022-03-05 03:19:51 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 03:19:51 | INFO | train | epoch 196 | loss 2.41 | nll_loss 0.959 | ppl 1.94 | wps 24603.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 18881 | lr 0.000230138 | gnorm 0.944 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 50261
2022-03-05 03:19:51 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 03:19:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:20:40 | INFO | train_inner | epoch 197:     19 / 97 loss=2.408, nll_loss=0.957, ppl=1.94, wps=24637.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.951, loss_scale=8, train_wall=236, gb_free=21, wall=50310
2022-03-05 03:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:24:05 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 12.923 | nll_loss 12.247 | ppl 4860.32 | wps 44916.9 | wpb 510.9 | bsz 1 | num_updates 18978 | best_loss 8.224
2022-03-05 03:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18978 updates
2022-03-05 03:24:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:24:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 197 @ 18978 updates, score 12.923) (writing took 2.395607433281839 seconds)
2022-03-05 03:24:07 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 03:24:07 | INFO | train | epoch 197 | loss 2.406 | nll_loss 0.955 | ppl 1.94 | wps 24833 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18978 | lr 0.000229549 | gnorm 0.942 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50517
2022-03-05 03:24:07 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 03:24:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:25:04 | INFO | train_inner | epoch 198:     22 / 97 loss=2.403, nll_loss=0.953, ppl=1.94, wps=24861.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.937, loss_scale=16, train_wall=234, gb_free=21, wall=50573
2022-03-05 03:28:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:28:20 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 12.961 | nll_loss 12.29 | ppl 5008.04 | wps 45241.6 | wpb 510.9 | bsz 1 | num_updates 19075 | best_loss 8.224
2022-03-05 03:28:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19075 updates
2022-03-05 03:28:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:28:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:28:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 198 @ 19075 updates, score 12.961) (writing took 2.333300555124879 seconds)
2022-03-05 03:28:23 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 03:28:23 | INFO | train | epoch 198 | loss 2.403 | nll_loss 0.953 | ppl 1.94 | wps 24869 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19075 | lr 0.000228964 | gnorm 0.942 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50772
2022-03-05 03:28:23 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 03:28:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:29:27 | INFO | train_inner | epoch 199:     25 / 97 loss=2.402, nll_loss=0.952, ppl=1.93, wps=24885.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.941, loss_scale=16, train_wall=234, gb_free=21, wall=50836
2022-03-05 03:29:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:30:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:32:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:32:36 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 12.925 | nll_loss 12.247 | ppl 4861.26 | wps 44951.6 | wpb 510.9 | bsz 1 | num_updates 19170 | best_loss 8.224
2022-03-05 03:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19170 updates
2022-03-05 03:32:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:32:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:32:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 199 @ 19170 updates, score 12.925) (writing took 2.4119454314932227 seconds)
2022-03-05 03:32:38 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 03:32:38 | INFO | train | epoch 199 | loss 2.399 | nll_loss 0.948 | ppl 1.93 | wps 24341.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 19170 | lr 0.000228396 | gnorm 0.941 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 51028
2022-03-05 03:32:38 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 03:32:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:33:55 | INFO | train_inner | epoch 200:     30 / 97 loss=2.396, nll_loss=0.945, ppl=1.93, wps=24404.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.933, loss_scale=8, train_wall=238, gb_free=21, wall=51104
2022-03-05 03:36:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:36:51 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 12.967 | nll_loss 12.295 | ppl 5025.28 | wps 45062.4 | wpb 510.9 | bsz 1 | num_updates 19267 | best_loss 8.224
2022-03-05 03:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19267 updates
2022-03-05 03:36:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:36:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:36:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 200 @ 19267 updates, score 12.967) (writing took 2.390992186963558 seconds)
2022-03-05 03:36:54 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 03:36:54 | INFO | train | epoch 200 | loss 2.397 | nll_loss 0.946 | ppl 1.93 | wps 24876.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19267 | lr 0.000227821 | gnorm 0.934 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51283
2022-03-05 03:36:54 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 03:36:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:38:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:38:21 | INFO | train_inner | epoch 201:     34 / 97 loss=2.397, nll_loss=0.946, ppl=1.93, wps=24654.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.938, loss_scale=8, train_wall=236, gb_free=21, wall=51370
2022-03-05 03:41:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:41:07 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 12.947 | nll_loss 12.267 | ppl 4929.58 | wps 45098.1 | wpb 510.9 | bsz 1 | num_updates 19363 | best_loss 8.224
2022-03-05 03:41:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19363 updates
2022-03-05 03:41:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:41:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:41:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 201 @ 19363 updates, score 12.947) (writing took 2.345853352919221 seconds)
2022-03-05 03:41:09 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 03:41:09 | INFO | train | epoch 201 | loss 2.393 | nll_loss 0.942 | ppl 1.92 | wps 24595 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19363 | lr 0.000227255 | gnorm 0.921 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 51539
2022-03-05 03:41:09 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 03:41:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:42:44 | INFO | train_inner | epoch 202:     37 / 97 loss=2.391, nll_loss=0.94, ppl=1.92, wps=24877.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.927, loss_scale=8, train_wall=234, gb_free=21, wall=51633
2022-03-05 03:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:45:22 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 12.957 | nll_loss 12.284 | ppl 4986.04 | wps 45124.1 | wpb 510.9 | bsz 1 | num_updates 19460 | best_loss 8.224
2022-03-05 03:45:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19460 updates
2022-03-05 03:45:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:45:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:45:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 202 @ 19460 updates, score 12.957) (writing took 2.5768176168203354 seconds)
2022-03-05 03:45:25 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 03:45:25 | INFO | train | epoch 202 | loss 2.391 | nll_loss 0.94 | ppl 1.92 | wps 24840.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19460 | lr 0.000226688 | gnorm 0.944 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51794
2022-03-05 03:45:25 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 03:45:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:47:08 | INFO | train_inner | epoch 203:     40 / 97 loss=2.39, nll_loss=0.939, ppl=1.92, wps=24855.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.937, loss_scale=16, train_wall=234, gb_free=21, wall=51897
2022-03-05 03:48:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:49:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:49:38 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 12.983 | nll_loss 12.314 | ppl 5093.25 | wps 45156.1 | wpb 510.9 | bsz 1 | num_updates 19556 | best_loss 8.224
2022-03-05 03:49:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19556 updates
2022-03-05 03:49:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:49:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:49:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 203 @ 19556 updates, score 12.983) (writing took 2.298220341093838 seconds)
2022-03-05 03:49:41 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 03:49:41 | INFO | train | epoch 203 | loss 2.386 | nll_loss 0.935 | ppl 1.91 | wps 24601.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19556 | lr 0.000226131 | gnorm 0.929 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 52050
2022-03-05 03:49:41 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 03:49:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:51:33 | INFO | train_inner | epoch 204:     44 / 97 loss=2.383, nll_loss=0.931, ppl=1.91, wps=24638.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.931, loss_scale=8, train_wall=236, gb_free=21, wall=52163
2022-03-05 03:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:53:54 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 12.985 | nll_loss 12.317 | ppl 5102.96 | wps 45130.4 | wpb 510.9 | bsz 1 | num_updates 19653 | best_loss 8.224
2022-03-05 03:53:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19653 updates
2022-03-05 03:53:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:53:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:53:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 204 @ 19653 updates, score 12.985) (writing took 2.35005786921829 seconds)
2022-03-05 03:53:56 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 03:53:56 | INFO | train | epoch 204 | loss 2.385 | nll_loss 0.934 | ppl 1.91 | wps 24847.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19653 | lr 0.000225572 | gnorm 0.951 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 52306
2022-03-05 03:53:56 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 03:53:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:55:57 | INFO | train_inner | epoch 205:     47 / 97 loss=2.385, nll_loss=0.934, ppl=1.91, wps=24861.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.942, loss_scale=16, train_wall=234, gb_free=21, wall=52426
2022-03-05 03:57:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:58:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:58:09 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 13.011 | nll_loss 12.341 | ppl 5187.68 | wps 45146 | wpb 510.9 | bsz 1 | num_updates 19749 | best_loss 8.224
2022-03-05 03:58:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19749 updates
2022-03-05 03:58:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:58:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 03:58:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 205 @ 19749 updates, score 13.011) (writing took 2.4764980413019657 seconds)
2022-03-05 03:58:12 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 03:58:12 | INFO | train | epoch 205 | loss 2.38 | nll_loss 0.929 | ppl 1.9 | wps 24586 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 19749 | lr 0.000225023 | gnorm 0.918 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 52561
2022-03-05 03:58:12 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 03:58:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:00:23 | INFO | train_inner | epoch 206:     51 / 97 loss=2.379, nll_loss=0.927, ppl=1.9, wps=24634.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.917, loss_scale=8, train_wall=236, gb_free=21, wall=52692
2022-03-05 04:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:02:25 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 12.994 | nll_loss 12.333 | ppl 5159.72 | wps 45107.3 | wpb 510.9 | bsz 1 | num_updates 19846 | best_loss 8.224
2022-03-05 04:02:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19846 updates
2022-03-05 04:02:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:02:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:02:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 206 @ 19846 updates, score 12.994) (writing took 2.5314486073330045 seconds)
2022-03-05 04:02:28 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 04:02:28 | INFO | train | epoch 206 | loss 2.378 | nll_loss 0.927 | ppl 1.9 | wps 24841.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19846 | lr 0.000224473 | gnorm 0.913 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 52817
2022-03-05 04:02:28 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 04:02:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:04:46 | INFO | train_inner | epoch 207:     54 / 97 loss=2.378, nll_loss=0.927, ppl=1.9, wps=24859.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.917, loss_scale=16, train_wall=234, gb_free=21, wall=52955
2022-03-05 04:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:06:41 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 13.013 | nll_loss 12.348 | ppl 5213.97 | wps 44906.9 | wpb 510.9 | bsz 1 | num_updates 19943 | best_loss 8.224
2022-03-05 04:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19943 updates
2022-03-05 04:06:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:06:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:06:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 207 @ 19943 updates, score 13.013) (writing took 2.5402492750436068 seconds)
2022-03-05 04:06:44 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 04:06:44 | INFO | train | epoch 207 | loss 2.376 | nll_loss 0.925 | ppl 1.9 | wps 24821.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19943 | lr 0.000223926 | gnorm 0.927 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53073
2022-03-05 04:06:44 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 04:06:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:08:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:09:12 | INFO | train_inner | epoch 208:     58 / 97 loss=2.375, nll_loss=0.924, ppl=1.9, wps=24614.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.932, loss_scale=16, train_wall=236, gb_free=21, wall=53222
2022-03-05 04:10:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:10:57 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 12.972 | nll_loss 12.301 | ppl 5046.66 | wps 45295.3 | wpb 510.9 | bsz 1 | num_updates 20038 | best_loss 8.224
2022-03-05 04:10:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20038 updates
2022-03-05 04:10:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:10:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 208 @ 20038 updates, score 12.972) (writing took 2.2869652276858687 seconds)
2022-03-05 04:10:59 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 04:10:59 | INFO | train | epoch 208 | loss 2.373 | nll_loss 0.921 | ppl 1.89 | wps 24367.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 20038 | lr 0.000223395 | gnorm 0.929 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 53328
2022-03-05 04:10:59 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 04:10:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:13:38 | INFO | train_inner | epoch 209:     62 / 97 loss=2.369, nll_loss=0.918, ppl=1.89, wps=24662.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.927, loss_scale=8, train_wall=236, gb_free=21, wall=53487
2022-03-05 04:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:15:12 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 13.015 | nll_loss 12.348 | ppl 5214 | wps 44945.9 | wpb 510.9 | bsz 1 | num_updates 20135 | best_loss 8.224
2022-03-05 04:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20135 updates
2022-03-05 04:15:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 209 @ 20135 updates, score 13.015) (writing took 2.335674445144832 seconds)
2022-03-05 04:15:14 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 04:15:14 | INFO | train | epoch 209 | loss 2.369 | nll_loss 0.918 | ppl 1.89 | wps 24861.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20135 | lr 0.000222856 | gnorm 0.929 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 53584
2022-03-05 04:15:14 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 04:15:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:17:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:18:03 | INFO | train_inner | epoch 210:     66 / 97 loss=2.369, nll_loss=0.917, ppl=1.89, wps=24653.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.948, loss_scale=8, train_wall=236, gb_free=21, wall=53753
2022-03-05 04:19:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:19:27 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 12.979 | nll_loss 12.308 | ppl 5072.4 | wps 45016.4 | wpb 510.9 | bsz 1 | num_updates 20231 | best_loss 8.224
2022-03-05 04:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20231 updates
2022-03-05 04:19:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 210 @ 20231 updates, score 12.979) (writing took 2.278545081615448 seconds)
2022-03-05 04:19:30 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 04:19:30 | INFO | train | epoch 210 | loss 2.367 | nll_loss 0.916 | ppl 1.89 | wps 24626.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 20231 | lr 0.000222327 | gnorm 0.953 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 53839
2022-03-05 04:19:30 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 04:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:22:26 | INFO | train_inner | epoch 211:     69 / 97 loss=2.364, nll_loss=0.913, ppl=1.88, wps=24903.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.922, loss_scale=8, train_wall=234, gb_free=21, wall=54016
2022-03-05 04:23:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:23:43 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 13.007 | nll_loss 12.344 | ppl 5200.7 | wps 45009.5 | wpb 510.9 | bsz 1 | num_updates 20328 | best_loss 8.224
2022-03-05 04:23:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20328 updates
2022-03-05 04:23:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:23:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:23:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 211 @ 20328 updates, score 13.007) (writing took 2.224559422582388 seconds)
2022-03-05 04:23:45 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 04:23:45 | INFO | train | epoch 211 | loss 2.364 | nll_loss 0.912 | ppl 1.88 | wps 24881.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20328 | lr 0.000221795 | gnorm 0.915 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54094
2022-03-05 04:23:45 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 04:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:25:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:26:52 | INFO | train_inner | epoch 212:     73 / 97 loss=2.362, nll_loss=0.91, ppl=1.88, wps=24659, ups=0.38, wpb=65495, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.941, loss_scale=8, train_wall=236, gb_free=21, wall=54281
2022-03-05 04:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:27:58 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 13.063 | nll_loss 12.402 | ppl 5412.37 | wps 45125.4 | wpb 510.9 | bsz 1 | num_updates 20424 | best_loss 8.224
2022-03-05 04:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20424 updates
2022-03-05 04:27:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:28:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:28:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 212 @ 20424 updates, score 13.063) (writing took 2.56261536385864 seconds)
2022-03-05 04:28:01 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 04:28:01 | INFO | train | epoch 212 | loss 2.36 | nll_loss 0.909 | ppl 1.88 | wps 24595.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 20424 | lr 0.000221274 | gnorm 0.944 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 54350
2022-03-05 04:28:01 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 04:28:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:31:15 | INFO | train_inner | epoch 213:     76 / 97 loss=2.359, nll_loss=0.908, ppl=1.88, wps=24882.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.923, loss_scale=16, train_wall=234, gb_free=21, wall=54545
2022-03-05 04:32:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:32:14 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 13.039 | nll_loss 12.372 | ppl 5299.19 | wps 44974.9 | wpb 510.9 | bsz 1 | num_updates 20521 | best_loss 8.224
2022-03-05 04:32:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20521 updates
2022-03-05 04:32:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:32:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:32:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 213 @ 20521 updates, score 13.039) (writing took 2.287034639157355 seconds)
2022-03-05 04:32:16 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 04:32:16 | INFO | train | epoch 213 | loss 2.357 | nll_loss 0.905 | ppl 1.87 | wps 24881.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20521 | lr 0.00022075 | gnorm 0.917 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54605
2022-03-05 04:32:16 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 04:32:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:35:38 | INFO | train_inner | epoch 214:     79 / 97 loss=2.356, nll_loss=0.905, ppl=1.87, wps=24897.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.921, loss_scale=16, train_wall=234, gb_free=21, wall=54808
2022-03-05 04:36:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:36:29 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 12.999 | nll_loss 12.336 | ppl 5168.98 | wps 45066.3 | wpb 510.9 | bsz 1 | num_updates 20617 | best_loss 8.224
2022-03-05 04:36:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20617 updates
2022-03-05 04:36:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 214 @ 20617 updates, score 12.999) (writing took 2.2665828447788954 seconds)
2022-03-05 04:36:31 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 04:36:31 | INFO | train | epoch 214 | loss 2.356 | nll_loss 0.904 | ppl 1.87 | wps 24625.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 20617 | lr 0.000220235 | gnorm 0.922 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54861
2022-03-05 04:36:31 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 04:36:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:40:04 | INFO | train_inner | epoch 215:     83 / 97 loss=2.354, nll_loss=0.902, ppl=1.87, wps=24659.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.928, loss_scale=16, train_wall=236, gb_free=21, wall=55073
2022-03-05 04:40:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:40:44 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 13.022 | nll_loss 12.36 | ppl 5255.29 | wps 44865.3 | wpb 510.9 | bsz 1 | num_updates 20714 | best_loss 8.224
2022-03-05 04:40:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20714 updates
2022-03-05 04:40:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:40:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 215 @ 20714 updates, score 13.022) (writing took 2.3579039592295885 seconds)
2022-03-05 04:40:47 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 04:40:47 | INFO | train | epoch 215 | loss 2.352 | nll_loss 0.9 | ppl 1.87 | wps 24864.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20714 | lr 0.000219719 | gnorm 0.926 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55116
2022-03-05 04:40:47 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 04:40:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:40:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:44:29 | INFO | train_inner | epoch 216:     87 / 97 loss=2.35, nll_loss=0.898, ppl=1.86, wps=24654.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.914, loss_scale=8, train_wall=236, gb_free=21, wall=55339
2022-03-05 04:44:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:45:00 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 12.994 | nll_loss 12.331 | ppl 5152.02 | wps 45207.6 | wpb 510.9 | bsz 1 | num_updates 20810 | best_loss 8.224
2022-03-05 04:45:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20810 updates
2022-03-05 04:45:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:45:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:45:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 216 @ 20810 updates, score 12.994) (writing took 2.4349085008725524 seconds)
2022-03-05 04:45:02 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 04:45:02 | INFO | train | epoch 216 | loss 2.349 | nll_loss 0.897 | ppl 1.86 | wps 24613.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 20810 | lr 0.000219212 | gnorm 0.917 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 55372
2022-03-05 04:45:02 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 04:45:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:48:53 | INFO | train_inner | epoch 217:     90 / 97 loss=2.348, nll_loss=0.897, ppl=1.86, wps=24877.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20900, lr=0.000218739, gnorm=0.929, loss_scale=16, train_wall=234, gb_free=21, wall=55602
2022-03-05 04:49:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:49:15 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 13 | nll_loss 12.337 | ppl 5173.38 | wps 44997.8 | wpb 510.9 | bsz 1 | num_updates 20907 | best_loss 8.224
2022-03-05 04:49:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20907 updates
2022-03-05 04:49:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:49:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:49:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 217 @ 20907 updates, score 13.0) (writing took 2.253296623006463 seconds)
2022-03-05 04:49:18 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 04:49:18 | INFO | train | epoch 217 | loss 2.346 | nll_loss 0.895 | ppl 1.86 | wps 24870.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20907 | lr 0.000218703 | gnorm 0.923 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55627
2022-03-05 04:49:18 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 04:49:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:50:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:53:18 | INFO | train_inner | epoch 218:     94 / 97 loss=2.343, nll_loss=0.892, ppl=1.86, wps=24661.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21000, lr=0.000218218, gnorm=0.908, loss_scale=8, train_wall=236, gb_free=21, wall=55868
2022-03-05 04:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:53:31 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 13.053 | nll_loss 12.391 | ppl 5371.2 | wps 45023.5 | wpb 510.9 | bsz 1 | num_updates 21003 | best_loss 8.224
2022-03-05 04:53:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 21003 updates
2022-03-05 04:53:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:53:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:53:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 218 @ 21003 updates, score 13.053) (writing took 2.1919200671836734 seconds)
2022-03-05 04:53:33 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 04:53:33 | INFO | train | epoch 218 | loss 2.343 | nll_loss 0.891 | ppl 1.85 | wps 24629 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 21003 | lr 0.000218202 | gnorm 0.909 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 55882
2022-03-05 04:53:33 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 04:53:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:56:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:57:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:57:46 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 13.021 | nll_loss 12.359 | ppl 5253.08 | wps 45086.3 | wpb 510.9 | bsz 1 | num_updates 21099 | best_loss 8.224
2022-03-05 04:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21099 updates
2022-03-05 04:57:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:57:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 04:57:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 219 @ 21099 updates, score 13.021) (writing took 2.3286376716569066 seconds)
2022-03-05 04:57:48 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 04:57:48 | INFO | train | epoch 219 | loss 2.341 | nll_loss 0.89 | ppl 1.85 | wps 24603.5 | ups 0.38 | wpb 65493.3 | bsz 127.9 | num_updates 21099 | lr 0.000217705 | gnorm 0.918 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 56138
2022-03-05 04:57:48 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 04:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:57:51 | INFO | train_inner | epoch 220:      1 / 97 loss=2.343, nll_loss=0.891, ppl=1.85, wps=23987.7, ups=0.37, wpb=65454.1, bsz=127.8, num_updates=21100, lr=0.0002177, gnorm=0.919, loss_scale=8, train_wall=236, gb_free=21, wall=56141
2022-03-05 05:01:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:02:02 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 13.044 | nll_loss 12.386 | ppl 5354.19 | wps 44957.7 | wpb 510.9 | bsz 1 | num_updates 21196 | best_loss 8.224
2022-03-05 05:02:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21196 updates
2022-03-05 05:02:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:02:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:02:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 220 @ 21196 updates, score 13.044) (writing took 3.244822546839714 seconds)
2022-03-05 05:02:05 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 05:02:05 | INFO | train | epoch 220 | loss 2.338 | nll_loss 0.886 | ppl 1.85 | wps 24738.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21196 | lr 0.000217207 | gnorm 0.914 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 56395
2022-03-05 05:02:05 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 05:02:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:02:16 | INFO | train_inner | epoch 221:      4 / 97 loss=2.337, nll_loss=0.885, ppl=1.85, wps=24765.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21200, lr=0.000217186, gnorm=0.914, loss_scale=8, train_wall=234, gb_free=21, wall=56405
2022-03-05 05:06:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:06:19 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 13.039 | nll_loss 12.385 | ppl 5349.62 | wps 44860.7 | wpb 510.9 | bsz 1 | num_updates 21293 | best_loss 8.224
2022-03-05 05:06:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21293 updates
2022-03-05 05:06:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:06:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:06:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 221 @ 21293 updates, score 13.039) (writing took 2.514928948134184 seconds)
2022-03-05 05:06:21 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 05:06:21 | INFO | train | epoch 221 | loss 2.336 | nll_loss 0.884 | ppl 1.85 | wps 24825.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21293 | lr 0.000216711 | gnorm 0.919 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56651
2022-03-05 05:06:21 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 05:06:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:06:39 | INFO | train_inner | epoch 222:      7 / 97 loss=2.335, nll_loss=0.882, ppl=1.84, wps=24846.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21300, lr=0.000216676, gnorm=0.919, loss_scale=16, train_wall=234, gb_free=21, wall=56669
2022-03-05 05:08:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:10:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:10:35 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 13.06 | nll_loss 12.402 | ppl 5412.04 | wps 44911.9 | wpb 510.9 | bsz 1 | num_updates 21389 | best_loss 8.224
2022-03-05 05:10:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21389 updates
2022-03-05 05:10:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:10:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 222 @ 21389 updates, score 13.06) (writing took 2.5799114424735308 seconds)
2022-03-05 05:10:37 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 05:10:37 | INFO | train | epoch 222 | loss 2.333 | nll_loss 0.881 | ppl 1.84 | wps 24558.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21389 | lr 0.000216224 | gnorm 0.915 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56907
2022-03-05 05:10:37 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 05:10:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:11:05 | INFO | train_inner | epoch 223:     11 / 97 loss=2.332, nll_loss=0.88, ppl=1.84, wps=24597.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.913, loss_scale=16, train_wall=236, gb_free=21, wall=56935
2022-03-05 05:13:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:14:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:14:51 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 12.985 | nll_loss 12.319 | ppl 5110.3 | wps 45112.7 | wpb 510.9 | bsz 1 | num_updates 21485 | best_loss 8.224
2022-03-05 05:14:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21485 updates
2022-03-05 05:14:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:14:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 223 @ 21485 updates, score 12.985) (writing took 2.536536129191518 seconds)
2022-03-05 05:14:53 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 05:14:53 | INFO | train | epoch 223 | loss 2.331 | nll_loss 0.879 | ppl 1.84 | wps 24570.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 21485 | lr 0.000215741 | gnorm 0.913 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57162
2022-03-05 05:14:53 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 05:14:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:15:32 | INFO | train_inner | epoch 224:     15 / 97 loss=2.328, nll_loss=0.876, ppl=1.83, wps=24611.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.912, loss_scale=16, train_wall=236, gb_free=21, wall=57201
2022-03-05 05:19:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:19:07 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 13.015 | nll_loss 12.361 | ppl 5261.2 | wps 44573.7 | wpb 510.9 | bsz 1 | num_updates 21582 | best_loss 8.224
2022-03-05 05:19:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21582 updates
2022-03-05 05:19:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:19:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:19:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 224 @ 21582 updates, score 13.015) (writing took 2.2891761111095548 seconds)
2022-03-05 05:19:09 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 05:19:09 | INFO | train | epoch 224 | loss 2.328 | nll_loss 0.876 | ppl 1.84 | wps 24834.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21582 | lr 0.000215255 | gnorm 0.913 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57418
2022-03-05 05:19:09 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 05:19:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:19:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:19:58 | INFO | train_inner | epoch 225:     19 / 97 loss=2.327, nll_loss=0.875, ppl=1.83, wps=24606.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.911, loss_scale=8, train_wall=236, gb_free=21, wall=57467
2022-03-05 05:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:23:23 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 13.011 | nll_loss 12.351 | ppl 5224.56 | wps 44547 | wpb 510.9 | bsz 1 | num_updates 21678 | best_loss 8.224
2022-03-05 05:23:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21678 updates
2022-03-05 05:23:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:23:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:23:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 225 @ 21678 updates, score 13.011) (writing took 2.4892578125 seconds)
2022-03-05 05:23:25 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 05:23:25 | INFO | train | epoch 225 | loss 2.326 | nll_loss 0.874 | ppl 1.83 | wps 24499 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21678 | lr 0.000214778 | gnorm 0.91 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 57675
2022-03-05 05:23:25 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 05:23:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:24:22 | INFO | train_inner | epoch 226:     22 / 97 loss=2.326, nll_loss=0.874, ppl=1.83, wps=24773.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.911, loss_scale=8, train_wall=234, gb_free=21, wall=57731
2022-03-05 05:26:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:27:40 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 12.998 | nll_loss 12.339 | ppl 5180.16 | wps 44081 | wpb 510.9 | bsz 1 | num_updates 21774 | best_loss 8.224
2022-03-05 05:27:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21774 updates
2022-03-05 05:27:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:27:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:27:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 226 @ 21774 updates, score 12.998) (writing took 2.444691433571279 seconds)
2022-03-05 05:27:42 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 05:27:42 | INFO | train | epoch 226 | loss 2.323 | nll_loss 0.871 | ppl 1.83 | wps 24476.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21774 | lr 0.000214304 | gnorm 0.91 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 57932
2022-03-05 05:27:42 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 05:27:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:28:49 | INFO | train_inner | epoch 227:     26 / 97 loss=2.322, nll_loss=0.87, ppl=1.83, wps=24522.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.916, loss_scale=8, train_wall=237, gb_free=21, wall=57999
2022-03-05 05:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:31:56 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 13.012 | nll_loss 12.348 | ppl 5213.16 | wps 43130.7 | wpb 510.9 | bsz 1 | num_updates 21871 | best_loss 8.224
2022-03-05 05:31:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21871 updates
2022-03-05 05:31:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:31:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:31:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 227 @ 21871 updates, score 13.012) (writing took 2.503705160692334 seconds)
2022-03-05 05:31:59 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 05:31:59 | INFO | train | epoch 227 | loss 2.322 | nll_loss 0.869 | ppl 1.83 | wps 24752.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21871 | lr 0.000213829 | gnorm 0.918 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58188
2022-03-05 05:31:59 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 05:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:33:14 | INFO | train_inner | epoch 228:     29 / 97 loss=2.321, nll_loss=0.869, ppl=1.83, wps=24766.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.912, loss_scale=16, train_wall=234, gb_free=21, wall=58263
2022-03-05 05:33:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:36:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:36:13 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 13.023 | nll_loss 12.368 | ppl 5284.98 | wps 44548 | wpb 510.9 | bsz 1 | num_updates 21967 | best_loss 8.224
2022-03-05 05:36:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21967 updates
2022-03-05 05:36:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 228 @ 21967 updates, score 13.023) (writing took 2.4941711593419313 seconds)
2022-03-05 05:36:16 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 05:36:16 | INFO | train | epoch 228 | loss 2.318 | nll_loss 0.866 | ppl 1.82 | wps 24505.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21967 | lr 0.000213361 | gnorm 0.917 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 58445
2022-03-05 05:36:16 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 05:36:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:37:41 | INFO | train_inner | epoch 229:     33 / 97 loss=2.316, nll_loss=0.864, ppl=1.82, wps=24527.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.915, loss_scale=8, train_wall=237, gb_free=21, wall=58530
2022-03-05 05:40:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:40:30 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 13.062 | nll_loss 12.408 | ppl 5434.5 | wps 44890.1 | wpb 510.9 | bsz 1 | num_updates 22064 | best_loss 8.224
2022-03-05 05:40:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22064 updates
2022-03-05 05:40:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:40:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:40:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 229 @ 22064 updates, score 13.062) (writing took 2.4181638471782207 seconds)
2022-03-05 05:40:32 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 05:40:32 | INFO | train | epoch 229 | loss 2.316 | nll_loss 0.863 | ppl 1.82 | wps 24736.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22064 | lr 0.000212891 | gnorm 0.907 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 58702
2022-03-05 05:40:32 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 05:40:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:42:05 | INFO | train_inner | epoch 230:     36 / 97 loss=2.314, nll_loss=0.861, ppl=1.82, wps=24767.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.908, loss_scale=16, train_wall=235, gb_free=21, wall=58794
2022-03-05 05:44:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:44:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:44:46 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 13.045 | nll_loss 12.384 | ppl 5345.8 | wps 44670 | wpb 510.9 | bsz 1 | num_updates 22160 | best_loss 8.224
2022-03-05 05:44:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22160 updates
2022-03-05 05:44:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:44:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:44:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 230 @ 22160 updates, score 13.045) (writing took 2.486965687945485 seconds)
2022-03-05 05:44:49 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 05:44:49 | INFO | train | epoch 230 | loss 2.313 | nll_loss 0.86 | ppl 1.82 | wps 24521.3 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 22160 | lr 0.00021243 | gnorm 0.908 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58958
2022-03-05 05:44:49 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 05:44:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:46:34 | INFO | train_inner | epoch 231:     41 / 97 loss=2.312, nll_loss=0.86, ppl=1.82, wps=24325.1, ups=0.37, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.906, loss_scale=8, train_wall=239, gb_free=21, wall=59064
2022-03-05 05:48:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:49:03 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 13.025 | nll_loss 12.368 | ppl 5286.88 | wps 44948.7 | wpb 510.9 | bsz 1 | num_updates 22256 | best_loss 8.224
2022-03-05 05:49:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22256 updates
2022-03-05 05:49:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:49:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:49:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 231 @ 22256 updates, score 13.025) (writing took 2.4674405371770263 seconds)
2022-03-05 05:49:05 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 05:49:05 | INFO | train | epoch 231 | loss 2.31 | nll_loss 0.858 | ppl 1.81 | wps 24508.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22256 | lr 0.000211971 | gnorm 0.906 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 59215
2022-03-05 05:49:05 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 05:49:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:50:58 | INFO | train_inner | epoch 232:     44 / 97 loss=2.31, nll_loss=0.857, ppl=1.81, wps=24805.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.91, loss_scale=8, train_wall=234, gb_free=21, wall=59328
2022-03-05 05:53:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:53:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:53:20 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 13.056 | nll_loss 12.399 | ppl 5400.52 | wps 44068.4 | wpb 510.9 | bsz 1 | num_updates 22352 | best_loss 8.224
2022-03-05 05:53:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22352 updates
2022-03-05 05:53:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:53:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 232 @ 22352 updates, score 13.056) (writing took 2.530169847421348 seconds)
2022-03-05 05:53:22 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 05:53:22 | INFO | train | epoch 232 | loss 2.309 | nll_loss 0.857 | ppl 1.81 | wps 24484.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22352 | lr 0.000211515 | gnorm 0.907 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 59472
2022-03-05 05:53:22 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 05:53:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:26 | INFO | train_inner | epoch 233:     48 / 97 loss=2.308, nll_loss=0.856, ppl=1.81, wps=24499.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.909, loss_scale=8, train_wall=237, gb_free=21, wall=59595
2022-03-05 05:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:57:37 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 13.072 | nll_loss 12.417 | ppl 5469.08 | wps 43492.6 | wpb 510.9 | bsz 1 | num_updates 22449 | best_loss 8.224
2022-03-05 05:57:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22449 updates
2022-03-05 05:57:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:57:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 05:57:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 233 @ 22449 updates, score 13.072) (writing took 2.670772662386298 seconds)
2022-03-05 05:57:39 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 05:57:39 | INFO | train | epoch 233 | loss 2.307 | nll_loss 0.854 | ppl 1.81 | wps 24699 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22449 | lr 0.000211058 | gnorm 0.909 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 59729
2022-03-05 05:57:39 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 05:57:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:59:51 | INFO | train_inner | epoch 234:     51 / 97 loss=2.305, nll_loss=0.853, ppl=1.81, wps=24732.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.913, loss_scale=16, train_wall=235, gb_free=21, wall=59860
2022-03-05 06:00:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:01:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:01:54 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 12.979 | nll_loss 12.319 | ppl 5107.91 | wps 44540.4 | wpb 510.9 | bsz 1 | num_updates 22545 | best_loss 8.224
2022-03-05 06:01:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22545 updates
2022-03-05 06:01:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:01:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:01:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 234 @ 22545 updates, score 12.979) (writing took 2.5262096859514713 seconds)
2022-03-05 06:01:56 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 06:01:56 | INFO | train | epoch 234 | loss 2.304 | nll_loss 0.852 | ppl 1.81 | wps 24499.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22545 | lr 0.000210608 | gnorm 0.914 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 59985
2022-03-05 06:01:56 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 06:01:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:04:18 | INFO | train_inner | epoch 235:     55 / 97 loss=2.303, nll_loss=0.851, ppl=1.8, wps=24536.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.902, loss_scale=8, train_wall=237, gb_free=21, wall=60127
2022-03-05 06:06:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:06:10 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 13.102 | nll_loss 12.452 | ppl 5604.85 | wps 44596.8 | wpb 510.9 | bsz 1 | num_updates 22642 | best_loss 8.224
2022-03-05 06:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22642 updates
2022-03-05 06:06:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:06:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:06:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 235 @ 22642 updates, score 13.102) (writing took 2.5243382984772325 seconds)
2022-03-05 06:06:13 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 06:06:13 | INFO | train | epoch 235 | loss 2.303 | nll_loss 0.851 | ppl 1.8 | wps 24750.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22642 | lr 0.000210156 | gnorm 0.897 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 60242
2022-03-05 06:06:13 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 06:06:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:08:42 | INFO | train_inner | epoch 236:     58 / 97 loss=2.301, nll_loss=0.849, ppl=1.8, wps=24776, ups=0.38, wpb=65495, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.895, loss_scale=16, train_wall=235, gb_free=21, wall=60391
2022-03-05 06:08:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:10:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:10:27 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 13.061 | nll_loss 12.409 | ppl 5439.95 | wps 44467.3 | wpb 510.9 | bsz 1 | num_updates 22738 | best_loss 8.224
2022-03-05 06:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22738 updates
2022-03-05 06:10:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 236 @ 22738 updates, score 13.061) (writing took 2.5256109554320574 seconds)
2022-03-05 06:10:29 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 06:10:29 | INFO | train | epoch 236 | loss 2.299 | nll_loss 0.846 | ppl 1.8 | wps 24517.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22738 | lr 0.000209712 | gnorm 0.899 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 60499
2022-03-05 06:10:29 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 06:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:13:08 | INFO | train_inner | epoch 237:     62 / 97 loss=2.298, nll_loss=0.845, ppl=1.8, wps=24574, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.91, loss_scale=8, train_wall=237, gb_free=21, wall=60658
2022-03-05 06:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:14:43 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 13.045 | nll_loss 12.387 | ppl 5357.13 | wps 42491.4 | wpb 510.9 | bsz 1 | num_updates 22835 | best_loss 8.224
2022-03-05 06:14:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22835 updates
2022-03-05 06:14:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:14:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:14:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 237 @ 22835 updates, score 13.045) (writing took 2.529593932442367 seconds)
2022-03-05 06:14:46 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 06:14:46 | INFO | train | epoch 237 | loss 2.298 | nll_loss 0.846 | ppl 1.8 | wps 24750.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22835 | lr 0.000209266 | gnorm 0.911 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 60755
2022-03-05 06:14:46 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 06:14:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:17:33 | INFO | train_inner | epoch 238:     65 / 97 loss=2.299, nll_loss=0.847, ppl=1.8, wps=24764.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.901, loss_scale=16, train_wall=234, gb_free=21, wall=60922
2022-03-05 06:18:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:18:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:19:00 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 13.058 | nll_loss 12.405 | ppl 5423.42 | wps 42413.6 | wpb 510.9 | bsz 1 | num_updates 22931 | best_loss 8.224
2022-03-05 06:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22931 updates
2022-03-05 06:19:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:19:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:19:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 238 @ 22931 updates, score 13.058) (writing took 2.508411008864641 seconds)
2022-03-05 06:19:03 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 06:19:03 | INFO | train | epoch 238 | loss 2.294 | nll_loss 0.842 | ppl 1.79 | wps 24478.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22931 | lr 0.000208828 | gnorm 0.898 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 61012
2022-03-05 06:19:03 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 06:19:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:22:00 | INFO | train_inner | epoch 239:     69 / 97 loss=2.291, nll_loss=0.838, ppl=1.79, wps=24505.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.901, loss_scale=8, train_wall=237, gb_free=21, wall=61189
2022-03-05 06:23:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:23:17 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 13.048 | nll_loss 12.396 | ppl 5388.89 | wps 45190.4 | wpb 510.9 | bsz 1 | num_updates 23028 | best_loss 8.224
2022-03-05 06:23:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23028 updates
2022-03-05 06:23:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:23:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:23:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 239 @ 23028 updates, score 13.048) (writing took 2.464419578202069 seconds)
2022-03-05 06:23:19 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 06:23:19 | INFO | train | epoch 239 | loss 2.293 | nll_loss 0.841 | ppl 1.79 | wps 24749.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23028 | lr 0.000208388 | gnorm 0.906 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 61269
2022-03-05 06:23:19 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 06:23:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:26:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:26:27 | INFO | train_inner | epoch 240:     73 / 97 loss=2.292, nll_loss=0.84, ppl=1.79, wps=24576.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.895, loss_scale=8, train_wall=237, gb_free=21, wall=61456
2022-03-05 06:27:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:27:33 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 13.025 | nll_loss 12.373 | ppl 5305.89 | wps 44160.3 | wpb 510.9 | bsz 1 | num_updates 23124 | best_loss 8.224
2022-03-05 06:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23124 updates
2022-03-05 06:27:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 240 @ 23124 updates, score 13.025) (writing took 2.4567402051761746 seconds)
2022-03-05 06:27:36 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 06:27:36 | INFO | train | epoch 240 | loss 2.291 | nll_loss 0.839 | ppl 1.79 | wps 24533.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23124 | lr 0.000207955 | gnorm 0.891 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 61525
2022-03-05 06:27:36 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 06:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:30:51 | INFO | train_inner | epoch 241:     76 / 97 loss=2.291, nll_loss=0.838, ppl=1.79, wps=24805.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.9, loss_scale=8, train_wall=234, gb_free=21, wall=61720
2022-03-05 06:31:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:31:49 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 13.075 | nll_loss 12.425 | ppl 5498.71 | wps 44891.2 | wpb 510.9 | bsz 1 | num_updates 23221 | best_loss 8.224
2022-03-05 06:31:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23221 updates
2022-03-05 06:31:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:31:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:31:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 241 @ 23221 updates, score 13.075) (writing took 2.546550165861845 seconds)
2022-03-05 06:31:52 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 06:31:52 | INFO | train | epoch 241 | loss 2.289 | nll_loss 0.836 | ppl 1.79 | wps 24801.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23221 | lr 0.00020752 | gnorm 0.893 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61781
2022-03-05 06:31:52 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 06:31:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:35:14 | INFO | train_inner | epoch 242:     79 / 97 loss=2.288, nll_loss=0.835, ppl=1.78, wps=24823.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.892, loss_scale=16, train_wall=234, gb_free=21, wall=61984
2022-03-05 06:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:36:06 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 13.014 | nll_loss 12.363 | ppl 5266.15 | wps 42986.5 | wpb 510.9 | bsz 1 | num_updates 23318 | best_loss 8.224
2022-03-05 06:36:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23318 updates
2022-03-05 06:36:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:36:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:36:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 242 @ 23318 updates, score 13.014) (writing took 2.4932512026280165 seconds)
2022-03-05 06:36:08 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 06:36:08 | INFO | train | epoch 242 | loss 2.286 | nll_loss 0.834 | ppl 1.78 | wps 24775 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23318 | lr 0.000207088 | gnorm 0.901 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62038
2022-03-05 06:36:08 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 06:36:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:37:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:39:41 | INFO | train_inner | epoch 243:     83 / 97 loss=2.286, nll_loss=0.834, ppl=1.78, wps=24571.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.901, loss_scale=16, train_wall=236, gb_free=21, wall=62250
2022-03-05 06:39:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:40:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:40:22 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 13.09 | nll_loss 12.444 | ppl 5571.18 | wps 45087.4 | wpb 510.9 | bsz 1 | num_updates 23413 | best_loss 8.224
2022-03-05 06:40:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23413 updates
2022-03-05 06:40:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:40:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:40:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 243 @ 23413 updates, score 13.09) (writing took 2.5248335991054773 seconds)
2022-03-05 06:40:24 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 06:40:24 | INFO | train | epoch 243 | loss 2.284 | nll_loss 0.832 | ppl 1.78 | wps 24288.2 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 23413 | lr 0.000206667 | gnorm 0.899 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 62294
2022-03-05 06:40:24 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 06:40:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:44:07 | INFO | train_inner | epoch 244:     87 / 97 loss=2.285, nll_loss=0.832, ppl=1.78, wps=24590.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23500, lr=0.000206284, gnorm=0.896, loss_scale=8, train_wall=236, gb_free=21, wall=62517
2022-03-05 06:44:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:44:38 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 13.036 | nll_loss 12.386 | ppl 5351.75 | wps 42717.9 | wpb 510.9 | bsz 1 | num_updates 23510 | best_loss 8.224
2022-03-05 06:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23510 updates
2022-03-05 06:44:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:44:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:44:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 244 @ 23510 updates, score 13.036) (writing took 2.478551436215639 seconds)
2022-03-05 06:44:41 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 06:44:41 | INFO | train | epoch 244 | loss 2.283 | nll_loss 0.831 | ppl 1.78 | wps 24796.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23510 | lr 0.00020624 | gnorm 0.891 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 62550
2022-03-05 06:44:41 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 06:44:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:48:31 | INFO | train_inner | epoch 245:     90 / 97 loss=2.28, nll_loss=0.828, ppl=1.78, wps=24802.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23600, lr=0.000205847, gnorm=0.889, loss_scale=16, train_wall=234, gb_free=21, wall=62781
2022-03-05 06:48:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:48:55 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 13.07 | nll_loss 12.421 | ppl 5485.73 | wps 42979.2 | wpb 510.9 | bsz 1 | num_updates 23607 | best_loss 8.224
2022-03-05 06:48:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23607 updates
2022-03-05 06:48:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:48:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:48:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 245 @ 23607 updates, score 13.07) (writing took 2.5130407148972154 seconds)
2022-03-05 06:48:57 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 06:48:57 | INFO | train | epoch 245 | loss 2.28 | nll_loss 0.828 | ppl 1.77 | wps 24767 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23607 | lr 0.000205816 | gnorm 0.889 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62806
2022-03-05 06:48:57 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 06:48:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:50:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:52:58 | INFO | train_inner | epoch 246:     94 / 97 loss=2.28, nll_loss=0.828, ppl=1.77, wps=24546.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=0.902, loss_scale=8, train_wall=237, gb_free=21, wall=63048
2022-03-05 06:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:53:11 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 13.082 | nll_loss 12.433 | ppl 5528.27 | wps 44544.2 | wpb 510.9 | bsz 1 | num_updates 23703 | best_loss 8.224
2022-03-05 06:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23703 updates
2022-03-05 06:53:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:53:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:53:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 246 @ 23703 updates, score 13.082) (writing took 2.461413591168821 seconds)
2022-03-05 06:53:13 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 06:53:13 | INFO | train | epoch 246 | loss 2.278 | nll_loss 0.826 | ppl 1.77 | wps 24544.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23703 | lr 0.000205399 | gnorm 0.901 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 63063
2022-03-05 06:53:13 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 06:53:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:57:22 | INFO | train_inner | epoch 247:     97 / 97 loss=2.276, nll_loss=0.824, ppl=1.77, wps=24784, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=23800, lr=0.00020498, gnorm=0.901, loss_scale=16, train_wall=234, gb_free=21, wall=63312
2022-03-05 06:57:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:27 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 13.072 | nll_loss 12.422 | ppl 5486.5 | wps 44878.9 | wpb 510.9 | bsz 1 | num_updates 23800 | best_loss 8.224
2022-03-05 06:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23800 updates
2022-03-05 06:57:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:57:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 06:57:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 247 @ 23800 updates, score 13.072) (writing took 2.472016505897045 seconds)
2022-03-05 06:57:30 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 06:57:30 | INFO | train | epoch 247 | loss 2.276 | nll_loss 0.823 | ppl 1.77 | wps 24765.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23800 | lr 0.00020498 | gnorm 0.901 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 63319
2022-03-05 06:57:30 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 06:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:01:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:01:44 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 13.094 | nll_loss 12.453 | ppl 5605.44 | wps 44035.6 | wpb 510.9 | bsz 1 | num_updates 23896 | best_loss 8.224
2022-03-05 07:01:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23896 updates
2022-03-05 07:01:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:01:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:01:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 248 @ 23896 updates, score 13.094) (writing took 2.4726624451577663 seconds)
2022-03-05 07:01:46 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 07:01:46 | INFO | train | epoch 248 | loss 2.274 | nll_loss 0.821 | ppl 1.77 | wps 24505.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23896 | lr 0.000204568 | gnorm 0.899 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 63576
2022-03-05 07:01:46 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 07:01:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:01:57 | INFO | train_inner | epoch 249:      4 / 97 loss=2.272, nll_loss=0.82, ppl=1.77, wps=23871, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=23900, lr=0.000204551, gnorm=0.898, loss_scale=8, train_wall=237, gb_free=21, wall=63586
2022-03-05 07:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:06:00 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 13.085 | nll_loss 12.439 | ppl 5553.89 | wps 44971.3 | wpb 510.9 | bsz 1 | num_updates 23993 | best_loss 8.224
2022-03-05 07:06:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23993 updates
2022-03-05 07:06:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:06:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:06:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 249 @ 23993 updates, score 13.085) (writing took 2.4928001230582595 seconds)
2022-03-05 07:06:02 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 07:06:02 | INFO | train | epoch 249 | loss 2.272 | nll_loss 0.819 | ppl 1.76 | wps 24801.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23993 | lr 0.000204154 | gnorm 0.894 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 63832
2022-03-05 07:06:02 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 07:06:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:06:20 | INFO | train_inner | epoch 250:      7 / 97 loss=2.271, nll_loss=0.818, ppl=1.76, wps=24828.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.893, loss_scale=8, train_wall=234, gb_free=21, wall=63850
2022-03-05 07:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:10:16 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 13.121 | nll_loss 12.479 | ppl 5709.77 | wps 45093.6 | wpb 510.9 | bsz 1 | num_updates 24090 | best_loss 8.224
2022-03-05 07:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24090 updates
2022-03-05 07:10:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:10:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:10:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 250 @ 24090 updates, score 13.121) (writing took 2.530960962176323 seconds)
2022-03-05 07:10:18 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 07:10:18 | INFO | train | epoch 250 | loss 2.27 | nll_loss 0.817 | ppl 1.76 | wps 24805.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24090 | lr 0.000203742 | gnorm 0.891 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64088
2022-03-05 07:10:19 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 07:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:10:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:10:47 | INFO | train_inner | epoch 251:     11 / 97 loss=2.269, nll_loss=0.816, ppl=1.76, wps=24587.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.894, loss_scale=8, train_wall=236, gb_free=21, wall=64116
2022-03-05 07:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:14:32 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 13.082 | nll_loss 12.437 | ppl 5545.99 | wps 45122.1 | wpb 510.9 | bsz 1 | num_updates 24186 | best_loss 8.224
2022-03-05 07:14:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24186 updates
2022-03-05 07:14:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:14:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 251 @ 24186 updates, score 13.082) (writing took 2.554314974695444 seconds)
2022-03-05 07:14:35 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 07:14:35 | INFO | train | epoch 251 | loss 2.268 | nll_loss 0.815 | ppl 1.76 | wps 24541.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24186 | lr 0.000203338 | gnorm 0.899 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 64344
2022-03-05 07:14:35 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 07:14:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:15:11 | INFO | train_inner | epoch 252:     14 / 97 loss=2.267, nll_loss=0.815, ppl=1.76, wps=24823.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.894, loss_scale=8, train_wall=234, gb_free=21, wall=64380
2022-03-05 07:17:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:18:48 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 13.08 | nll_loss 12.431 | ppl 5523.5 | wps 45120.2 | wpb 510.9 | bsz 1 | num_updates 24282 | best_loss 8.224
2022-03-05 07:18:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24282 updates
2022-03-05 07:18:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:18:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 252 @ 24282 updates, score 13.08) (writing took 2.565453728660941 seconds)
2022-03-05 07:18:51 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 07:18:51 | INFO | train | epoch 252 | loss 2.267 | nll_loss 0.814 | ppl 1.76 | wps 24559.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 24282 | lr 0.000202935 | gnorm 0.897 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 64600
2022-03-05 07:18:51 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 07:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:19:37 | INFO | train_inner | epoch 253:     18 / 97 loss=2.265, nll_loss=0.812, ppl=1.76, wps=24599.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.894, loss_scale=8, train_wall=236, gb_free=21, wall=64646
2022-03-05 07:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:23:04 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 13.099 | nll_loss 12.452 | ppl 5604.03 | wps 45124.2 | wpb 510.9 | bsz 1 | num_updates 24379 | best_loss 8.224
2022-03-05 07:23:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24379 updates
2022-03-05 07:23:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 253 @ 24379 updates, score 13.099) (writing took 2.4383541950955987 seconds)
2022-03-05 07:23:07 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 07:23:07 | INFO | train | epoch 253 | loss 2.264 | nll_loss 0.812 | ppl 1.76 | wps 24817.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24379 | lr 0.000202531 | gnorm 0.89 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 64856
2022-03-05 07:23:07 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 07:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:24:01 | INFO | train_inner | epoch 254:     21 / 97 loss=2.263, nll_loss=0.811, ppl=1.75, wps=24837.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.896, loss_scale=16, train_wall=234, gb_free=21, wall=64910
2022-03-05 07:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:27:21 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 13.065 | nll_loss 12.423 | ppl 5492.05 | wps 45039.9 | wpb 510.9 | bsz 1 | num_updates 24476 | best_loss 8.224
2022-03-05 07:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24476 updates
2022-03-05 07:27:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:27:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:27:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 254 @ 24476 updates, score 13.065) (writing took 2.459537505172193 seconds)
2022-03-05 07:27:23 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 07:27:23 | INFO | train | epoch 254 | loss 2.262 | nll_loss 0.81 | ppl 1.75 | wps 24775 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24476 | lr 0.00020213 | gnorm 0.896 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 65112
2022-03-05 07:27:23 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 07:27:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:28:25 | INFO | train_inner | epoch 255:     24 / 97 loss=2.261, nll_loss=0.809, ppl=1.75, wps=24783.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.894, loss_scale=16, train_wall=234, gb_free=21, wall=65174
2022-03-05 07:29:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:29:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:31:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:31:37 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 13.127 | nll_loss 12.483 | ppl 5725.97 | wps 44732.3 | wpb 510.9 | bsz 1 | num_updates 24571 | best_loss 8.224
2022-03-05 07:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24571 updates
2022-03-05 07:31:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:31:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:31:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 255 @ 24571 updates, score 13.127) (writing took 2.530005228705704 seconds)
2022-03-05 07:31:40 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 07:31:40 | INFO | train | epoch 255 | loss 2.259 | nll_loss 0.806 | ppl 1.75 | wps 24249.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 24571 | lr 0.000201738 | gnorm 0.901 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 65369
2022-03-05 07:31:40 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 07:31:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:32:54 | INFO | train_inner | epoch 256:     29 / 97 loss=2.258, nll_loss=0.806, ppl=1.75, wps=24320.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.896, loss_scale=8, train_wall=239, gb_free=21, wall=65443
2022-03-05 07:35:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:35:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:35:54 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 13.044 | nll_loss 12.392 | ppl 5374.86 | wps 44072.9 | wpb 510.9 | bsz 1 | num_updates 24667 | best_loss 8.224
2022-03-05 07:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24667 updates
2022-03-05 07:35:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:35:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:35:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 256 @ 24667 updates, score 13.044) (writing took 2.4512120792642236 seconds)
2022-03-05 07:35:56 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 07:35:56 | INFO | train | epoch 256 | loss 2.258 | nll_loss 0.805 | ppl 1.75 | wps 24506.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24667 | lr 0.000201345 | gnorm 0.891 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 65626
2022-03-05 07:35:56 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 07:35:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:37:21 | INFO | train_inner | epoch 257:     33 / 97 loss=2.256, nll_loss=0.804, ppl=1.75, wps=24539, ups=0.37, wpb=65495, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.893, loss_scale=8, train_wall=237, gb_free=21, wall=65710
2022-03-05 07:40:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:40:10 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 13.088 | nll_loss 12.44 | ppl 5556.83 | wps 44802.4 | wpb 510.9 | bsz 1 | num_updates 24764 | best_loss 8.224
2022-03-05 07:40:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24764 updates
2022-03-05 07:40:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:40:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 257 @ 24764 updates, score 13.088) (writing took 2.4226109944283962 seconds)
2022-03-05 07:40:12 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 07:40:12 | INFO | train | epoch 257 | loss 2.256 | nll_loss 0.803 | ppl 1.75 | wps 24795.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24764 | lr 0.000200951 | gnorm 0.889 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 65882
2022-03-05 07:40:12 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 07:40:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:41:45 | INFO | train_inner | epoch 258:     36 / 97 loss=2.254, nll_loss=0.801, ppl=1.74, wps=24809.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.886, loss_scale=16, train_wall=234, gb_free=21, wall=65974
2022-03-05 07:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:44:27 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 13.097 | nll_loss 12.449 | ppl 5593.1 | wps 44434.7 | wpb 510.9 | bsz 1 | num_updates 24861 | best_loss 8.224
2022-03-05 07:44:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24861 updates
2022-03-05 07:44:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:44:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:44:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 258 @ 24861 updates, score 13.097) (writing took 2.441814242862165 seconds)
2022-03-05 07:44:29 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 07:44:29 | INFO | train | epoch 258 | loss 2.254 | nll_loss 0.802 | ppl 1.74 | wps 24761.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24861 | lr 0.000200558 | gnorm 0.884 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66138
2022-03-05 07:44:29 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 07:44:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:46:09 | INFO | train_inner | epoch 259:     39 / 97 loss=2.254, nll_loss=0.802, ppl=1.74, wps=24783.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.888, loss_scale=16, train_wall=234, gb_free=21, wall=66239
2022-03-05 07:47:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:48:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:48:43 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 13.076 | nll_loss 12.432 | ppl 5527.3 | wps 44059.3 | wpb 510.9 | bsz 1 | num_updates 24957 | best_loss 8.224
2022-03-05 07:48:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24957 updates
2022-03-05 07:48:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:48:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:48:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 259 @ 24957 updates, score 13.076) (writing took 2.465640712529421 seconds)
2022-03-05 07:48:46 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 07:48:46 | INFO | train | epoch 259 | loss 2.252 | nll_loss 0.8 | ppl 1.74 | wps 24491.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24957 | lr 0.000200172 | gnorm 0.886 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66395
2022-03-05 07:48:46 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 07:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:50:36 | INFO | train_inner | epoch 260:     43 / 97 loss=2.252, nll_loss=0.8, ppl=1.74, wps=24545.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=25000, lr=0.0002, gnorm=0.883, loss_scale=16, train_wall=237, gb_free=21, wall=66505
2022-03-05 07:52:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:52:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:53:00 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 13.118 | nll_loss 12.471 | ppl 5677.63 | wps 44909.3 | wpb 510.9 | bsz 1 | num_updates 25053 | best_loss 8.224
2022-03-05 07:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25053 updates
2022-03-05 07:53:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:53:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:53:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 260 @ 25053 updates, score 13.118) (writing took 2.859330862760544 seconds)
2022-03-05 07:53:02 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 07:53:02 | INFO | train | epoch 260 | loss 2.25 | nll_loss 0.798 | ppl 1.74 | wps 24484.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25053 | lr 0.000199788 | gnorm 0.879 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66652
2022-03-05 07:53:02 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 07:53:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:54:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:55:05 | INFO | train_inner | epoch 261:     48 / 97 loss=2.249, nll_loss=0.797, ppl=1.74, wps=24309.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.884, loss_scale=8, train_wall=239, gb_free=21, wall=66775
2022-03-05 07:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:57:16 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 13.102 | nll_loss 12.458 | ppl 5624.79 | wps 45064 | wpb 510.9 | bsz 1 | num_updates 25149 | best_loss 8.224
2022-03-05 07:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25149 updates
2022-03-05 07:57:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:57:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 07:57:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 261 @ 25149 updates, score 13.102) (writing took 2.627747029066086 seconds)
2022-03-05 07:57:19 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 07:57:19 | INFO | train | epoch 261 | loss 2.249 | nll_loss 0.797 | ppl 1.74 | wps 24549.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25149 | lr 0.000199407 | gnorm 0.892 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 66908
2022-03-05 07:57:19 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 07:57:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:59:29 | INFO | train_inner | epoch 262:     51 / 97 loss=2.247, nll_loss=0.795, ppl=1.73, wps=24824.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.885, loss_scale=8, train_wall=234, gb_free=21, wall=67039
2022-03-05 08:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:01:32 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 13.12 | nll_loss 12.478 | ppl 5705.25 | wps 45119.5 | wpb 510.9 | bsz 1 | num_updates 25246 | best_loss 8.224
2022-03-05 08:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25246 updates
2022-03-05 08:01:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:01:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:01:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 262 @ 25246 updates, score 13.12) (writing took 2.529843522235751 seconds)
2022-03-05 08:01:35 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 08:01:35 | INFO | train | epoch 262 | loss 2.247 | nll_loss 0.795 | ppl 1.73 | wps 24795.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25246 | lr 0.000199023 | gnorm 0.881 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67164
2022-03-05 08:01:35 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 08:01:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:03:53 | INFO | train_inner | epoch 263:     54 / 97 loss=2.247, nll_loss=0.795, ppl=1.74, wps=24801.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.878, loss_scale=16, train_wall=234, gb_free=21, wall=67303
2022-03-05 08:05:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:05:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:05:48 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 13.098 | nll_loss 12.454 | ppl 5609.29 | wps 43664.4 | wpb 510.9 | bsz 1 | num_updates 25342 | best_loss 8.224
2022-03-05 08:05:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25342 updates
2022-03-05 08:05:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 263 @ 25342 updates, score 13.098) (writing took 2.6218429123982787 seconds)
2022-03-05 08:05:51 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 08:05:51 | INFO | train | epoch 263 | loss 2.245 | nll_loss 0.793 | ppl 1.73 | wps 24530 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25342 | lr 0.000198646 | gnorm 0.876 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67420
2022-03-05 08:05:51 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 08:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:08:20 | INFO | train_inner | epoch 264:     58 / 97 loss=2.242, nll_loss=0.79, ppl=1.73, wps=24574.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.869, loss_scale=16, train_wall=236, gb_free=21, wall=67569
2022-03-05 08:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:10:05 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 13.078 | nll_loss 12.432 | ppl 5525.13 | wps 45074.7 | wpb 510.9 | bsz 1 | num_updates 25439 | best_loss 8.224
2022-03-05 08:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25439 updates
2022-03-05 08:10:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 264 @ 25439 updates, score 13.078) (writing took 2.5045046685263515 seconds)
2022-03-05 08:10:07 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 08:10:07 | INFO | train | epoch 264 | loss 2.243 | nll_loss 0.791 | ppl 1.73 | wps 24813.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25439 | lr 0.000198267 | gnorm 0.874 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67676
2022-03-05 08:10:07 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 08:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:11:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:12:48 | INFO | train_inner | epoch 265:     63 / 97 loss=2.243, nll_loss=0.791, ppl=1.73, wps=24385.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.89, loss_scale=8, train_wall=239, gb_free=21, wall=67838
2022-03-05 08:14:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:14:20 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 13.059 | nll_loss 12.418 | ppl 5472.44 | wps 45170.7 | wpb 510.9 | bsz 1 | num_updates 25534 | best_loss 8.224
2022-03-05 08:14:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25534 updates
2022-03-05 08:14:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:14:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 265 @ 25534 updates, score 13.059) (writing took 2.520729623734951 seconds)
2022-03-05 08:14:23 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 08:14:23 | INFO | train | epoch 265 | loss 2.24 | nll_loss 0.788 | ppl 1.73 | wps 24319.8 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 25534 | lr 0.000197898 | gnorm 0.88 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 67932
2022-03-05 08:14:23 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 08:14:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:17:12 | INFO | train_inner | epoch 266:     66 / 97 loss=2.24, nll_loss=0.788, ppl=1.73, wps=24829.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.875, loss_scale=8, train_wall=234, gb_free=21, wall=68102
2022-03-05 08:18:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:18:36 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 13.077 | nll_loss 12.438 | ppl 5547.35 | wps 44841.7 | wpb 510.9 | bsz 1 | num_updates 25631 | best_loss 8.224
2022-03-05 08:18:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25631 updates
2022-03-05 08:18:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:18:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:18:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 266 @ 25631 updates, score 13.077) (writing took 2.505006205290556 seconds)
2022-03-05 08:18:39 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 08:18:39 | INFO | train | epoch 266 | loss 2.24 | nll_loss 0.787 | ppl 1.73 | wps 24807 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25631 | lr 0.000197523 | gnorm 0.877 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68188
2022-03-05 08:18:39 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 08:18:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:21:36 | INFO | train_inner | epoch 267:     69 / 97 loss=2.238, nll_loss=0.786, ppl=1.72, wps=24840.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.87, loss_scale=16, train_wall=234, gb_free=21, wall=68365
2022-03-05 08:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:22:53 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 13.113 | nll_loss 12.472 | ppl 5683.01 | wps 44864.1 | wpb 510.9 | bsz 1 | num_updates 25728 | best_loss 8.224
2022-03-05 08:22:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25728 updates
2022-03-05 08:22:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:22:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:22:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 267 @ 25728 updates, score 13.113) (writing took 2.6250861017033458 seconds)
2022-03-05 08:22:55 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 08:22:55 | INFO | train | epoch 267 | loss 2.238 | nll_loss 0.786 | ppl 1.72 | wps 24797.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25728 | lr 0.00019715 | gnorm 0.873 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68445
2022-03-05 08:22:55 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 08:22:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:23:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:23:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:26:05 | INFO | train_inner | epoch 268:     74 / 97 loss=2.238, nll_loss=0.786, ppl=1.72, wps=24342.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.889, loss_scale=8, train_wall=239, gb_free=21, wall=68634
2022-03-05 08:27:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:27:09 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 13.102 | nll_loss 12.463 | ppl 5645.95 | wps 44900.1 | wpb 510.9 | bsz 1 | num_updates 25823 | best_loss 8.224
2022-03-05 08:27:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25823 updates
2022-03-05 08:27:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:27:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:27:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 268 @ 25823 updates, score 13.102) (writing took 2.5391602562740445 seconds)
2022-03-05 08:27:11 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 08:27:11 | INFO | train | epoch 268 | loss 2.235 | nll_loss 0.783 | ppl 1.72 | wps 24301.4 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 25823 | lr 0.000196787 | gnorm 0.891 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 68701
2022-03-05 08:27:11 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 08:27:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:30:29 | INFO | train_inner | epoch 269:     77 / 97 loss=2.235, nll_loss=0.783, ppl=1.72, wps=24841.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.876, loss_scale=16, train_wall=234, gb_free=21, wall=68898
2022-03-05 08:31:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:31:25 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 13.143 | nll_loss 12.505 | ppl 5811.67 | wps 44974 | wpb 510.9 | bsz 1 | num_updates 25920 | best_loss 8.224
2022-03-05 08:31:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25920 updates
2022-03-05 08:31:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:31:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 269 @ 25920 updates, score 13.143) (writing took 2.5856255926191807 seconds)
2022-03-05 08:31:27 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 08:31:27 | INFO | train | epoch 269 | loss 2.235 | nll_loss 0.783 | ppl 1.72 | wps 24803.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25920 | lr 0.000196419 | gnorm 0.873 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68957
2022-03-05 08:31:27 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 08:31:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:34:55 | INFO | train_inner | epoch 270:     81 / 97 loss=2.233, nll_loss=0.781, ppl=1.72, wps=24587.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.875, loss_scale=8, train_wall=236, gb_free=21, wall=69164
2022-03-05 08:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:35:41 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 13.075 | nll_loss 12.436 | ppl 5542.82 | wps 45260.7 | wpb 510.9 | bsz 1 | num_updates 26016 | best_loss 8.224
2022-03-05 08:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26016 updates
2022-03-05 08:35:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:35:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:35:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 270 @ 26016 updates, score 13.075) (writing took 2.5191408274695277 seconds)
2022-03-05 08:35:43 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 08:35:43 | INFO | train | epoch 270 | loss 2.232 | nll_loss 0.78 | ppl 1.72 | wps 24568.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26016 | lr 0.000196056 | gnorm 0.874 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 69213
2022-03-05 08:35:43 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 08:35:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:39:19 | INFO | train_inner | epoch 271:     84 / 97 loss=2.232, nll_loss=0.78, ppl=1.72, wps=24836.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26100, lr=0.00019574, gnorm=0.887, loss_scale=16, train_wall=234, gb_free=21, wall=69428
2022-03-05 08:39:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:39:57 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 13.051 | nll_loss 12.408 | ppl 5433.08 | wps 45089.4 | wpb 510.9 | bsz 1 | num_updates 26113 | best_loss 8.224
2022-03-05 08:39:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26113 updates
2022-03-05 08:39:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:39:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:39:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 271 @ 26113 updates, score 13.051) (writing took 2.5435256250202656 seconds)
2022-03-05 08:39:59 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 08:39:59 | INFO | train | epoch 271 | loss 2.232 | nll_loss 0.78 | ppl 1.72 | wps 24803.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26113 | lr 0.000195691 | gnorm 0.886 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69469
2022-03-05 08:39:59 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 08:39:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:42:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:43:45 | INFO | train_inner | epoch 272:     88 / 97 loss=2.229, nll_loss=0.777, ppl=1.71, wps=24608.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26200, lr=0.000195366, gnorm=0.861, loss_scale=8, train_wall=236, gb_free=21, wall=69694
2022-03-05 08:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:44:13 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 13.115 | nll_loss 12.477 | ppl 5699.21 | wps 44985.1 | wpb 510.9 | bsz 1 | num_updates 26209 | best_loss 8.224
2022-03-05 08:44:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26209 updates
2022-03-05 08:44:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:44:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:44:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 272 @ 26209 updates, score 13.115) (writing took 2.584280412644148 seconds)
2022-03-05 08:44:15 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 08:44:15 | INFO | train | epoch 272 | loss 2.228 | nll_loss 0.776 | ppl 1.71 | wps 24562.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26209 | lr 0.000195333 | gnorm 0.865 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 69725
2022-03-05 08:44:15 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 08:44:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:48:09 | INFO | train_inner | epoch 273:     91 / 97 loss=2.229, nll_loss=0.777, ppl=1.71, wps=24781.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26300, lr=0.000194994, gnorm=0.879, loss_scale=8, train_wall=234, gb_free=21, wall=69958
2022-03-05 08:48:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:48:29 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 13.076 | nll_loss 12.436 | ppl 5541.39 | wps 44431 | wpb 510.9 | bsz 1 | num_updates 26306 | best_loss 8.224
2022-03-05 08:48:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26306 updates
2022-03-05 08:48:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:48:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:48:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 273 @ 26306 updates, score 13.076) (writing took 2.548167148604989 seconds)
2022-03-05 08:48:32 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 08:48:32 | INFO | train | epoch 273 | loss 2.228 | nll_loss 0.777 | ppl 1.71 | wps 24753.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26306 | lr 0.000194972 | gnorm 0.876 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69981
2022-03-05 08:48:32 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 08:48:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:52:33 | INFO | train_inner | epoch 274:     94 / 97 loss=2.227, nll_loss=0.776, ppl=1.71, wps=24818.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.872, loss_scale=16, train_wall=234, gb_free=21, wall=70222
2022-03-05 08:52:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:52:46 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 13.078 | nll_loss 12.436 | ppl 5542.19 | wps 43059.7 | wpb 510.9 | bsz 1 | num_updates 26403 | best_loss 8.224
2022-03-05 08:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26403 updates
2022-03-05 08:52:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:52:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:52:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 274 @ 26403 updates, score 13.078) (writing took 2.485040602274239 seconds)
2022-03-05 08:52:48 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 08:52:48 | INFO | train | epoch 274 | loss 2.226 | nll_loss 0.774 | ppl 1.71 | wps 24783 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26403 | lr 0.000194614 | gnorm 0.871 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70238
2022-03-05 08:52:48 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 08:52:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:54:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:56:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:57:02 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 13.132 | nll_loss 12.492 | ppl 5762.17 | wps 45016.5 | wpb 510.9 | bsz 1 | num_updates 26499 | best_loss 8.224
2022-03-05 08:57:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26499 updates
2022-03-05 08:57:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:57:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 08:57:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 275 @ 26499 updates, score 13.132) (writing took 2.5546789467334747 seconds)
2022-03-05 08:57:04 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 08:57:04 | INFO | train | epoch 275 | loss 2.223 | nll_loss 0.771 | ppl 1.71 | wps 24561 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26499 | lr 0.000194261 | gnorm 0.864 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70494
2022-03-05 08:57:04 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 08:57:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:57:07 | INFO | train_inner | epoch 276:      1 / 97 loss=2.223, nll_loss=0.771, ppl=1.71, wps=23892.4, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=26500, lr=0.000194257, gnorm=0.865, loss_scale=16, train_wall=236, gb_free=21, wall=70496
2022-03-05 08:59:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:00:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:01:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:01:18 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 13.076 | nll_loss 12.439 | ppl 5552.58 | wps 45188.9 | wpb 510.9 | bsz 1 | num_updates 26594 | best_loss 8.224
2022-03-05 09:01:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26594 updates
2022-03-05 09:01:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:01:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:01:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 276 @ 26594 updates, score 13.076) (writing took 2.515422586351633 seconds)
2022-03-05 09:01:20 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 09:01:20 | INFO | train | epoch 276 | loss 2.221 | nll_loss 0.769 | ppl 1.7 | wps 24322.4 | ups 0.37 | wpb 65492.9 | bsz 127.9 | num_updates 26594 | lr 0.000193914 | gnorm 0.878 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 70749
2022-03-05 09:01:20 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 09:01:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:01:36 | INFO | train_inner | epoch 277:      6 / 97 loss=2.22, nll_loss=0.768, ppl=1.7, wps=24383.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=26600, lr=0.000193892, gnorm=0.878, loss_scale=8, train_wall=239, gb_free=21, wall=70765
2022-03-05 09:05:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:05:34 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 13.078 | nll_loss 12.44 | ppl 5557.85 | wps 45032.4 | wpb 510.9 | bsz 1 | num_updates 26691 | best_loss 8.224
2022-03-05 09:05:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26691 updates
2022-03-05 09:05:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:05:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:05:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 277 @ 26691 updates, score 13.078) (writing took 2.5047684544697404 seconds)
2022-03-05 09:05:36 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 09:05:36 | INFO | train | epoch 277 | loss 2.222 | nll_loss 0.771 | ppl 1.71 | wps 24816.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26691 | lr 0.000193561 | gnorm 0.89 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 71005
2022-03-05 09:05:36 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 09:05:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:05:59 | INFO | train_inner | epoch 278:      9 / 97 loss=2.221, nll_loss=0.769, ppl=1.7, wps=24840.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.886, loss_scale=16, train_wall=234, gb_free=21, wall=71029
2022-03-05 09:09:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:09:49 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 13.096 | nll_loss 12.459 | ppl 5628.54 | wps 45055.9 | wpb 510.9 | bsz 1 | num_updates 26788 | best_loss 8.224
2022-03-05 09:09:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26788 updates
2022-03-05 09:09:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:09:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:09:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 278 @ 26788 updates, score 13.096) (writing took 2.5216986974701285 seconds)
2022-03-05 09:09:52 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 09:09:52 | INFO | train | epoch 278 | loss 2.219 | nll_loss 0.767 | ppl 1.7 | wps 24827.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26788 | lr 0.00019321 | gnorm 0.864 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 71261
2022-03-05 09:09:52 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 09:09:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:10:23 | INFO | train_inner | epoch 279:     12 / 97 loss=2.218, nll_loss=0.767, ppl=1.7, wps=24846, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.862, loss_scale=16, train_wall=234, gb_free=21, wall=71292
2022-03-05 09:11:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:14:05 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 13.104 | nll_loss 12.463 | ppl 5646.08 | wps 45151.6 | wpb 510.9 | bsz 1 | num_updates 26884 | best_loss 8.224
2022-03-05 09:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26884 updates
2022-03-05 09:14:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:14:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:14:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 279 @ 26884 updates, score 13.104) (writing took 2.5187939796596766 seconds)
2022-03-05 09:14:08 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 09:14:08 | INFO | train | epoch 279 | loss 2.217 | nll_loss 0.766 | ppl 1.7 | wps 24558.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26884 | lr 0.000192865 | gnorm 0.868 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 71517
2022-03-05 09:14:08 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 09:14:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:14:49 | INFO | train_inner | epoch 280:     16 / 97 loss=2.216, nll_loss=0.764, ppl=1.7, wps=24601.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.869, loss_scale=16, train_wall=236, gb_free=21, wall=71558
2022-03-05 09:17:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:18:22 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 13.09 | nll_loss 12.452 | ppl 5603.57 | wps 44452.4 | wpb 510.9 | bsz 1 | num_updates 26980 | best_loss 8.224
2022-03-05 09:18:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26980 updates
2022-03-05 09:18:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:18:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:18:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 280 @ 26980 updates, score 13.09) (writing took 2.4545124173164368 seconds)
2022-03-05 09:18:24 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 09:18:24 | INFO | train | epoch 280 | loss 2.215 | nll_loss 0.764 | ppl 1.7 | wps 24545.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26980 | lr 0.000192521 | gnorm 0.861 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 71773
2022-03-05 09:18:24 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 09:18:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:18:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:19:18 | INFO | train_inner | epoch 281:     21 / 97 loss=2.215, nll_loss=0.763, ppl=1.7, wps=24342.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.86, loss_scale=8, train_wall=239, gb_free=21, wall=71827
2022-03-05 09:22:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:22:38 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 13.101 | nll_loss 12.464 | ppl 5649.1 | wps 44090.4 | wpb 510.9 | bsz 1 | num_updates 27076 | best_loss 8.224
2022-03-05 09:22:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27076 updates
2022-03-05 09:22:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:22:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:22:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 281 @ 27076 updates, score 13.101) (writing took 2.515218515880406 seconds)
2022-03-05 09:22:40 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 09:22:40 | INFO | train | epoch 281 | loss 2.214 | nll_loss 0.762 | ppl 1.7 | wps 24531.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27076 | lr 0.00019218 | gnorm 0.865 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 72030
2022-03-05 09:22:40 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 09:22:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:23:42 | INFO | train_inner | epoch 282:     24 / 97 loss=2.213, nll_loss=0.761, ppl=1.69, wps=24818.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.86, loss_scale=8, train_wall=234, gb_free=21, wall=72091
2022-03-05 09:24:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:26:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:26:54 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 13.118 | nll_loss 12.48 | ppl 5714.56 | wps 45013.8 | wpb 510.9 | bsz 1 | num_updates 27172 | best_loss 8.224
2022-03-05 09:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27172 updates
2022-03-05 09:26:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:26:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:26:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 282 @ 27172 updates, score 13.118) (writing took 2.808143920265138 seconds)
2022-03-05 09:26:57 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 09:26:57 | INFO | train | epoch 282 | loss 2.213 | nll_loss 0.761 | ppl 1.7 | wps 24543 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27172 | lr 0.00019184 | gnorm 0.861 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 72286
2022-03-05 09:26:57 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 09:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:28:08 | INFO | train_inner | epoch 283:     28 / 97 loss=2.212, nll_loss=0.76, ppl=1.69, wps=24583.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.865, loss_scale=8, train_wall=236, gb_free=21, wall=72358
2022-03-05 09:31:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:31:10 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 13.22 | nll_loss 12.593 | ppl 6177.49 | wps 44832.6 | wpb 510.9 | bsz 1 | num_updates 27269 | best_loss 8.224
2022-03-05 09:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27269 updates
2022-03-05 09:31:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:31:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:31:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 283 @ 27269 updates, score 13.22) (writing took 2.532400288619101 seconds)
2022-03-05 09:31:13 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 09:31:13 | INFO | train | epoch 283 | loss 2.211 | nll_loss 0.759 | ppl 1.69 | wps 24812.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27269 | lr 0.000191499 | gnorm 0.882 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 72542
2022-03-05 09:31:13 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 09:31:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:32:32 | INFO | train_inner | epoch 284:     31 / 97 loss=2.21, nll_loss=0.759, ppl=1.69, wps=24826.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.878, loss_scale=16, train_wall=234, gb_free=21, wall=72621
2022-03-05 09:33:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:35:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:35:27 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 13.179 | nll_loss 12.551 | ppl 5999.14 | wps 44405 | wpb 510.9 | bsz 1 | num_updates 27365 | best_loss 8.224
2022-03-05 09:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27365 updates
2022-03-05 09:35:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:35:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:35:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 284 @ 27365 updates, score 13.179) (writing took 2.482534701935947 seconds)
2022-03-05 09:35:29 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 09:35:29 | INFO | train | epoch 284 | loss 2.21 | nll_loss 0.758 | ppl 1.69 | wps 24488.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27365 | lr 0.000191162 | gnorm 0.866 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 72799
2022-03-05 09:35:29 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 09:35:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:36:59 | INFO | train_inner | epoch 285:     35 / 97 loss=2.208, nll_loss=0.756, ppl=1.69, wps=24513.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.865, loss_scale=8, train_wall=237, gb_free=21, wall=72889
2022-03-05 09:39:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:39:43 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 13.125 | nll_loss 12.491 | ppl 5757.54 | wps 44463.4 | wpb 510.9 | bsz 1 | num_updates 27462 | best_loss 8.224
2022-03-05 09:39:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27462 updates
2022-03-05 09:39:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:39:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:39:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 285 @ 27462 updates, score 13.125) (writing took 2.4375405041500926 seconds)
2022-03-05 09:39:46 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 09:39:46 | INFO | train | epoch 285 | loss 2.208 | nll_loss 0.757 | ppl 1.69 | wps 24766.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27462 | lr 0.000190824 | gnorm 0.867 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 73055
2022-03-05 09:39:46 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 09:39:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:41:24 | INFO | train_inner | epoch 286:     38 / 97 loss=2.208, nll_loss=0.757, ppl=1.69, wps=24784.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27500, lr=0.000190693, gnorm=0.872, loss_scale=16, train_wall=235, gb_free=21, wall=73153
2022-03-05 09:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:44:00 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 13.144 | nll_loss 12.509 | ppl 5828.7 | wps 44506.3 | wpb 510.9 | bsz 1 | num_updates 27559 | best_loss 8.224
2022-03-05 09:44:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27559 updates
2022-03-05 09:44:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:44:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:44:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 286 @ 27559 updates, score 13.144) (writing took 2.5205703545361757 seconds)
2022-03-05 09:44:02 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 09:44:02 | INFO | train | epoch 286 | loss 2.206 | nll_loss 0.755 | ppl 1.69 | wps 24752.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27559 | lr 0.000190488 | gnorm 0.87 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 73312
2022-03-05 09:44:02 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 09:44:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:44:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:45:51 | INFO | train_inner | epoch 287:     42 / 97 loss=2.205, nll_loss=0.754, ppl=1.69, wps=24530, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.875, loss_scale=8, train_wall=237, gb_free=21, wall=73420
2022-03-05 09:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:48:17 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 13.152 | nll_loss 12.521 | ppl 5877.66 | wps 45150.4 | wpb 510.9 | bsz 1 | num_updates 27655 | best_loss 8.224
2022-03-05 09:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27655 updates
2022-03-05 09:48:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:48:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 287 @ 27655 updates, score 13.152) (writing took 2.490913087502122 seconds)
2022-03-05 09:48:19 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 09:48:19 | INFO | train | epoch 287 | loss 2.205 | nll_loss 0.753 | ppl 1.69 | wps 24493.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27655 | lr 0.000190157 | gnorm 0.868 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 73568
2022-03-05 09:48:19 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 09:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:50:15 | INFO | train_inner | epoch 288:     45 / 97 loss=2.205, nll_loss=0.754, ppl=1.69, wps=24788.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.857, loss_scale=16, train_wall=234, gb_free=21, wall=73684
2022-03-05 09:51:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:52:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:52:33 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 13.065 | nll_loss 12.422 | ppl 5487.56 | wps 45121.4 | wpb 510.9 | bsz 1 | num_updates 27751 | best_loss 8.224
2022-03-05 09:52:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27751 updates
2022-03-05 09:52:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:52:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 288 @ 27751 updates, score 13.065) (writing took 2.7481610821560025 seconds)
2022-03-05 09:52:35 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 09:52:35 | INFO | train | epoch 288 | loss 2.204 | nll_loss 0.752 | ppl 1.68 | wps 24529 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27751 | lr 0.000189828 | gnorm 0.865 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 73825
2022-03-05 09:52:35 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 09:52:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:54:41 | INFO | train_inner | epoch 289:     49 / 97 loss=2.201, nll_loss=0.749, ppl=1.68, wps=24568.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.866, loss_scale=8, train_wall=237, gb_free=21, wall=73951
2022-03-05 09:56:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:56:49 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 13.115 | nll_loss 12.48 | ppl 5713.43 | wps 44936.9 | wpb 510.9 | bsz 1 | num_updates 27848 | best_loss 8.224
2022-03-05 09:56:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27848 updates
2022-03-05 09:56:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:56:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 09:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 289 @ 27848 updates, score 13.115) (writing took 2.6464374577626586 seconds)
2022-03-05 09:56:52 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 09:56:52 | INFO | train | epoch 289 | loss 2.202 | nll_loss 0.75 | ppl 1.68 | wps 24796.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27848 | lr 0.000189497 | gnorm 0.864 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 74081
2022-03-05 09:56:52 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 09:56:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:59:05 | INFO | train_inner | epoch 290:     52 / 97 loss=2.201, nll_loss=0.75, ppl=1.68, wps=24842.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.864, loss_scale=16, train_wall=234, gb_free=21, wall=74214
2022-03-05 09:59:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:01:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:01:05 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 13.171 | nll_loss 12.54 | ppl 5954.2 | wps 45056.9 | wpb 510.9 | bsz 1 | num_updates 27944 | best_loss 8.224
2022-03-05 10:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27944 updates
2022-03-05 10:01:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:01:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 290 @ 27944 updates, score 13.171) (writing took 2.5999110313132405 seconds)
2022-03-05 10:01:07 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 10:01:07 | INFO | train | epoch 290 | loss 2.199 | nll_loss 0.748 | ppl 1.68 | wps 24573.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27944 | lr 0.000189172 | gnorm 0.861 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 74337
2022-03-05 10:01:08 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 10:01:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:03:31 | INFO | train_inner | epoch 291:     56 / 97 loss=2.2, nll_loss=0.748, ppl=1.68, wps=24574.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.863, loss_scale=8, train_wall=236, gb_free=21, wall=74481
2022-03-05 10:05:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:05:21 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 13.087 | nll_loss 12.452 | ppl 5602.13 | wps 45147.9 | wpb 510.9 | bsz 1 | num_updates 28041 | best_loss 8.224
2022-03-05 10:05:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28041 updates
2022-03-05 10:05:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:05:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:05:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 291 @ 28041 updates, score 13.087) (writing took 2.5025899335741997 seconds)
2022-03-05 10:05:24 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 10:05:24 | INFO | train | epoch 291 | loss 2.199 | nll_loss 0.748 | ppl 1.68 | wps 24772.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28041 | lr 0.000188844 | gnorm 0.858 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 74593
2022-03-05 10:05:24 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 10:05:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:07:55 | INFO | train_inner | epoch 292:     59 / 97 loss=2.199, nll_loss=0.748, ppl=1.68, wps=24829, ups=0.38, wpb=65495, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.86, loss_scale=16, train_wall=234, gb_free=21, wall=74745
2022-03-05 10:09:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:09:37 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 13.128 | nll_loss 12.496 | ppl 5776.91 | wps 45090.2 | wpb 510.9 | bsz 1 | num_updates 28138 | best_loss 8.224
2022-03-05 10:09:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28138 updates
2022-03-05 10:09:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:09:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:09:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 292 @ 28138 updates, score 13.128) (writing took 2.518977750092745 seconds)
2022-03-05 10:09:40 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 10:09:40 | INFO | train | epoch 292 | loss 2.197 | nll_loss 0.746 | ppl 1.68 | wps 24809.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28138 | lr 0.000188518 | gnorm 0.853 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 74849
2022-03-05 10:09:40 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 10:09:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:11:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:11:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:12:24 | INFO | train_inner | epoch 293:     64 / 97 loss=2.196, nll_loss=0.745, ppl=1.68, wps=24341.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.856, loss_scale=8, train_wall=239, gb_free=21, wall=75014
2022-03-05 10:13:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:13:54 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 13.119 | nll_loss 12.487 | ppl 5739.63 | wps 45139.8 | wpb 510.9 | bsz 1 | num_updates 28233 | best_loss 8.224
2022-03-05 10:13:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28233 updates
2022-03-05 10:13:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 293 @ 28233 updates, score 13.119) (writing took 2.53449818585068 seconds)
2022-03-05 10:13:56 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 10:13:56 | INFO | train | epoch 293 | loss 2.196 | nll_loss 0.745 | ppl 1.68 | wps 24274.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 28233 | lr 0.000188201 | gnorm 0.868 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 75106
2022-03-05 10:13:56 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 10:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:16:48 | INFO | train_inner | epoch 294:     67 / 97 loss=2.196, nll_loss=0.745, ppl=1.68, wps=24825.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.862, loss_scale=8, train_wall=234, gb_free=21, wall=75277
2022-03-05 10:18:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:18:10 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 13.138 | nll_loss 12.504 | ppl 5808.49 | wps 44941.9 | wpb 510.9 | bsz 1 | num_updates 28330 | best_loss 8.224
2022-03-05 10:18:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28330 updates
2022-03-05 10:18:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:18:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 294 @ 28330 updates, score 13.138) (writing took 2.5826679719612002 seconds)
2022-03-05 10:18:12 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 10:18:12 | INFO | train | epoch 294 | loss 2.195 | nll_loss 0.744 | ppl 1.67 | wps 24800.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28330 | lr 0.000187878 | gnorm 0.855 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 75362
2022-03-05 10:18:12 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 10:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:21:12 | INFO | train_inner | epoch 295:     70 / 97 loss=2.194, nll_loss=0.743, ppl=1.67, wps=24804.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.86, loss_scale=16, train_wall=234, gb_free=21, wall=75542
2022-03-05 10:22:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:22:26 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 13.086 | nll_loss 12.451 | ppl 5599.38 | wps 44966.8 | wpb 510.9 | bsz 1 | num_updates 28427 | best_loss 8.224
2022-03-05 10:22:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28427 updates
2022-03-05 10:22:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:22:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:22:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 295 @ 28427 updates, score 13.086) (writing took 2.5012005595490336 seconds)
2022-03-05 10:22:29 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 10:22:29 | INFO | train | epoch 295 | loss 2.193 | nll_loss 0.742 | ppl 1.67 | wps 24789.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28427 | lr 0.000187558 | gnorm 0.866 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 75618
2022-03-05 10:22:29 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 10:22:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:22:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:25:38 | INFO | train_inner | epoch 296:     74 / 97 loss=2.192, nll_loss=0.74, ppl=1.67, wps=24605.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.859, loss_scale=16, train_wall=236, gb_free=21, wall=75808
2022-03-05 10:26:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:26:42 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 13.132 | nll_loss 12.5 | ppl 5794.42 | wps 45068.7 | wpb 510.9 | bsz 1 | num_updates 28523 | best_loss 8.224
2022-03-05 10:26:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28523 updates
2022-03-05 10:26:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:26:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:26:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 296 @ 28523 updates, score 13.132) (writing took 2.5039592552930117 seconds)
2022-03-05 10:26:45 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 10:26:45 | INFO | train | epoch 296 | loss 2.19 | nll_loss 0.739 | ppl 1.67 | wps 24573.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28523 | lr 0.000187242 | gnorm 0.855 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 75874
2022-03-05 10:26:45 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 10:26:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:28:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:30:05 | INFO | train_inner | epoch 297:     78 / 97 loss=2.19, nll_loss=0.739, ppl=1.67, wps=24597.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28600, lr=0.000186989, gnorm=0.862, loss_scale=8, train_wall=236, gb_free=21, wall=76074
2022-03-05 10:30:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:30:58 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 13.106 | nll_loss 12.476 | ppl 5698.9 | wps 45081.7 | wpb 510.9 | bsz 1 | num_updates 28619 | best_loss 8.224
2022-03-05 10:30:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28619 updates
2022-03-05 10:30:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:31:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 297 @ 28619 updates, score 13.106) (writing took 2.522117767482996 seconds)
2022-03-05 10:31:01 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 10:31:01 | INFO | train | epoch 297 | loss 2.191 | nll_loss 0.739 | ppl 1.67 | wps 24553.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28619 | lr 0.000186927 | gnorm 0.866 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 76130
2022-03-05 10:31:01 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 10:31:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:34:28 | INFO | train_inner | epoch 298:     81 / 97 loss=2.19, nll_loss=0.739, ppl=1.67, wps=24829.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.859, loss_scale=16, train_wall=234, gb_free=21, wall=76338
2022-03-05 10:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:35:14 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 13.149 | nll_loss 12.514 | ppl 5847.68 | wps 45013.3 | wpb 510.9 | bsz 1 | num_updates 28716 | best_loss 8.224
2022-03-05 10:35:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28716 updates
2022-03-05 10:35:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:35:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 298 @ 28716 updates, score 13.149) (writing took 2.527360150590539 seconds)
2022-03-05 10:35:17 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 10:35:17 | INFO | train | epoch 298 | loss 2.189 | nll_loss 0.738 | ppl 1.67 | wps 24814.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28716 | lr 0.000186611 | gnorm 0.857 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 76386
2022-03-05 10:35:17 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 10:35:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:36:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:38:55 | INFO | train_inner | epoch 299:     85 / 97 loss=2.187, nll_loss=0.736, ppl=1.67, wps=24605.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28800, lr=0.000186339, gnorm=0.854, loss_scale=8, train_wall=236, gb_free=21, wall=76604
2022-03-05 10:39:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:39:30 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 13.204 | nll_loss 12.578 | ppl 6116.46 | wps 44996.5 | wpb 510.9 | bsz 1 | num_updates 28812 | best_loss 8.224
2022-03-05 10:39:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28812 updates
2022-03-05 10:39:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:39:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:39:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 299 @ 28812 updates, score 13.204) (writing took 2.5208991914987564 seconds)
2022-03-05 10:39:33 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 10:39:33 | INFO | train | epoch 299 | loss 2.187 | nll_loss 0.735 | ppl 1.66 | wps 24562.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28812 | lr 0.0001863 | gnorm 0.849 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 76642
2022-03-05 10:39:33 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 10:39:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:43:19 | INFO | train_inner | epoch 300:     88 / 97 loss=2.186, nll_loss=0.735, ppl=1.66, wps=24810.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.849, loss_scale=16, train_wall=234, gb_free=21, wall=76868
2022-03-05 10:43:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:43:46 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 13.119 | nll_loss 12.486 | ppl 5736.69 | wps 44992.8 | wpb 510.9 | bsz 1 | num_updates 28909 | best_loss 8.224
2022-03-05 10:43:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28909 updates
2022-03-05 10:43:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:43:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:43:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 300 @ 28909 updates, score 13.119) (writing took 2.545630161650479 seconds)
2022-03-05 10:43:49 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 10:43:49 | INFO | train | epoch 300 | loss 2.186 | nll_loss 0.734 | ppl 1.66 | wps 24784 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28909 | lr 0.000185987 | gnorm 0.848 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 76898
2022-03-05 10:43:49 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 10:43:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:47:42 | INFO | train_inner | epoch 301:     91 / 97 loss=2.186, nll_loss=0.735, ppl=1.66, wps=24825.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29000, lr=0.000185695, gnorm=0.865, loss_scale=16, train_wall=234, gb_free=21, wall=77132
2022-03-05 10:47:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:48:02 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 13.143 | nll_loss 12.511 | ppl 5838.48 | wps 45047.5 | wpb 510.9 | bsz 1 | num_updates 29006 | best_loss 8.224
2022-03-05 10:48:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 29006 updates
2022-03-05 10:48:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:48:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:48:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 301 @ 29006 updates, score 13.143) (writing took 2.5226133251562715 seconds)
2022-03-05 10:48:05 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 10:48:05 | INFO | train | epoch 301 | loss 2.185 | nll_loss 0.734 | ppl 1.66 | wps 24805.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29006 | lr 0.000185676 | gnorm 0.865 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 77154
2022-03-05 10:48:05 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 10:48:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:48:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:52:09 | INFO | train_inner | epoch 302:     95 / 97 loss=2.183, nll_loss=0.732, ppl=1.66, wps=24572.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29100, lr=0.000185376, gnorm=0.86, loss_scale=16, train_wall=237, gb_free=21, wall=77398
2022-03-05 10:52:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:52:19 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 13.185 | nll_loss 12.564 | ppl 6055.92 | wps 45084.5 | wpb 510.9 | bsz 1 | num_updates 29102 | best_loss 8.224
2022-03-05 10:52:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29102 updates
2022-03-05 10:52:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:52:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:52:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 302 @ 29102 updates, score 13.185) (writing took 2.5323365945369005 seconds)
2022-03-05 10:52:21 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 10:52:21 | INFO | train | epoch 302 | loss 2.182 | nll_loss 0.731 | ppl 1.66 | wps 24531.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29102 | lr 0.00018537 | gnorm 0.86 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 77411
2022-03-05 10:52:21 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 10:52:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:53:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:56:35 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 13.186 | nll_loss 12.557 | ppl 6027.86 | wps 45051.9 | wpb 510.9 | bsz 1 | num_updates 29198 | best_loss 8.224
2022-03-05 10:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29198 updates
2022-03-05 10:56:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:56:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 10:56:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 303 @ 29198 updates, score 13.186) (writing took 2.5258785849437118 seconds)
2022-03-05 10:56:37 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 10:56:37 | INFO | train | epoch 303 | loss 2.182 | nll_loss 0.731 | ppl 1.66 | wps 24564.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29198 | lr 0.000185065 | gnorm 0.852 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 77667
2022-03-05 10:56:37 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 10:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:56:42 | INFO | train_inner | epoch 304:      2 / 97 loss=2.182, nll_loss=0.731, ppl=1.66, wps=23920.3, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=29200, lr=0.000185058, gnorm=0.851, loss_scale=16, train_wall=236, gb_free=21, wall=77672
2022-03-05 10:59:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:00:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:00:51 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 13.141 | nll_loss 12.513 | ppl 5846.13 | wps 44324 | wpb 510.9 | bsz 1 | num_updates 29293 | best_loss 8.224
2022-03-05 11:00:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29293 updates
2022-03-05 11:00:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:00:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:00:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 304 @ 29293 updates, score 13.141) (writing took 2.559048871509731 seconds)
2022-03-05 11:00:53 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 11:00:53 | INFO | train | epoch 304 | loss 2.18 | nll_loss 0.729 | ppl 1.66 | wps 24300.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 29293 | lr 0.000184764 | gnorm 0.847 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 77923
2022-03-05 11:00:53 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 11:00:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:01:11 | INFO | train_inner | epoch 305:      7 / 97 loss=2.179, nll_loss=0.728, ppl=1.66, wps=24363.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=29300, lr=0.000184742, gnorm=0.848, loss_scale=8, train_wall=239, gb_free=21, wall=77941
2022-03-05 11:05:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:05:07 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 13.16 | nll_loss 12.533 | ppl 5925.53 | wps 45029 | wpb 510.9 | bsz 1 | num_updates 29390 | best_loss 8.224
2022-03-05 11:05:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29390 updates
2022-03-05 11:05:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:05:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:05:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 305 @ 29390 updates, score 13.16) (writing took 2.462848453782499 seconds)
2022-03-05 11:05:09 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 11:05:09 | INFO | train | epoch 305 | loss 2.178 | nll_loss 0.727 | ppl 1.65 | wps 24817.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29390 | lr 0.000184459 | gnorm 0.853 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 78179
2022-03-05 11:05:09 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 11:05:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:05:35 | INFO | train_inner | epoch 306:     10 / 97 loss=2.177, nll_loss=0.726, ppl=1.65, wps=24835.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.852, loss_scale=8, train_wall=234, gb_free=21, wall=78204
2022-03-05 11:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:09:23 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 13.154 | nll_loss 12.522 | ppl 5879.8 | wps 45058.4 | wpb 510.9 | bsz 1 | num_updates 29487 | best_loss 8.224
2022-03-05 11:09:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29487 updates
2022-03-05 11:09:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:09:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:09:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 306 @ 29487 updates, score 13.154) (writing took 2.5795464366674423 seconds)
2022-03-05 11:09:26 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 11:09:26 | INFO | train | epoch 306 | loss 2.178 | nll_loss 0.727 | ppl 1.66 | wps 24790.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29487 | lr 0.000184156 | gnorm 0.861 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 78435
2022-03-05 11:09:26 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 11:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:09:59 | INFO | train_inner | epoch 307:     13 / 97 loss=2.176, nll_loss=0.725, ppl=1.65, wps=24816.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.858, loss_scale=16, train_wall=234, gb_free=21, wall=78468
2022-03-05 11:11:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:11:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:13:39 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 13.156 | nll_loss 12.526 | ppl 5896.94 | wps 45038.9 | wpb 510.9 | bsz 1 | num_updates 29582 | best_loss 8.224
2022-03-05 11:13:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29582 updates
2022-03-05 11:13:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:13:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:13:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 307 @ 29582 updates, score 13.156) (writing took 2.4729196317493916 seconds)
2022-03-05 11:13:41 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 11:13:41 | INFO | train | epoch 307 | loss 2.175 | nll_loss 0.725 | ppl 1.65 | wps 24315.8 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 29582 | lr 0.00018386 | gnorm 0.855 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 78691
2022-03-05 11:13:41 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 11:13:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:14:28 | INFO | train_inner | epoch 308:     18 / 97 loss=2.177, nll_loss=0.726, ppl=1.65, wps=24376.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.857, loss_scale=8, train_wall=239, gb_free=21, wall=78737
2022-03-05 11:17:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:17:55 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 13.163 | nll_loss 12.536 | ppl 5937.24 | wps 44427.1 | wpb 510.9 | bsz 1 | num_updates 29679 | best_loss 8.224
2022-03-05 11:17:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29679 updates
2022-03-05 11:17:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:17:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 308 @ 29679 updates, score 13.163) (writing took 2.4967648396268487 seconds)
2022-03-05 11:17:57 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 11:17:57 | INFO | train | epoch 308 | loss 2.175 | nll_loss 0.724 | ppl 1.65 | wps 24812.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29679 | lr 0.000183559 | gnorm 0.847 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 78947
2022-03-05 11:17:57 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 11:17:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:18:51 | INFO | train_inner | epoch 309:     21 / 97 loss=2.173, nll_loss=0.722, ppl=1.65, wps=24822.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.847, loss_scale=16, train_wall=234, gb_free=21, wall=79001
2022-03-05 11:22:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:22:11 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 13.18 | nll_loss 12.554 | ppl 6015.39 | wps 45472.6 | wpb 510.9 | bsz 1 | num_updates 29776 | best_loss 8.224
2022-03-05 11:22:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 29776 updates
2022-03-05 11:22:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:22:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt
2022-03-05 11:22:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.06_#1/checkpoint_last.pt (epoch 309 @ 29776 updates, score 13.18) (writing took 2.524508439935744 seconds)
2022-03-05 11:22:13 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-03-05 11:22:13 | INFO | train | epoch 309 | loss 2.173 | nll_loss 0.723 | ppl 1.65 | wps 24821.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29776 | lr 0.00018326 | gnorm 0.854 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 79203
2022-03-05 11:22:13 | INFO | fairseq.trainer | begin training epoch 310
2022-03-05 11:22:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:23:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:23:17 | INFO | train_inner | epoch 310:     25 / 97 loss=2.172, nll_loss=0.722, ppl=1.65, wps=24623.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=29800, lr=0.000183186, gnorm=0.846, loss_scale=16, train_wall=236, gb_free=21, wall=79267
2022-03-05 11:24:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 324, in train
    for i, samples in enumerate(progress):
  File "/cluster/home/andriusb/fq/fairseq/fairseq/logging/progress_bar.py", line 261, in __iter__
    for i, obj in enumerate(self.iterable, start=self.n):
  File "/cluster/home/andriusb/fq/fairseq/fairseq/data/iterators.py", line 56, in __next__
    x = next(self._itr)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/data/iterators.py", line 509, in _chunk_iterator
    for x in itr:
  File "/cluster/home/andriusb/fq/fairseq/fairseq/data/iterators.py", line 56, in __next__
    x = next(self._itr)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/data/iterators.py", line 635, in __next__
    item = self._queue.get(True)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/queue.py", line 170, in get
    self.not_empty.wait()
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/threading.py", line 302, in wait
    waiter.acquire()
KeyboardInterrupt
Exception in thread Thread-1238:
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/threading.py", line 932, in _bootstrap_inner
