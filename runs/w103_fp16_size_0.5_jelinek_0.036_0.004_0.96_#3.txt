Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 206457060: <w103_fp16_size_0.5_jelinek_0.036_0.004_0.96_#3> in cluster <euler> Exited

Job <w103_fp16_size_0.5_jelinek_0.036_0.004_0.96_#3> was submitted from host <eu-login-05> by user <andriusb> in cluster <euler> at Fri Feb 25 10:44:17 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Feb 25 10:44:48 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 25 10:44:48 2022
Terminated at Sun Feb 27 10:45:20 2022
Results reported at Sun Feb 27 10:45:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.036, 0.004, 0.96)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321673 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   172744.00 sec.
    Max Memory :                                 10910 MB
    Average Memory :                             3072.10 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               9090.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   172832 sec.
    Turnaround time :                            172863 sec.

The output (if any) follows:

2022-02-25 10:44:55 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321673, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321673, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.036, 0.004, 0.96)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-25 10:44:56 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-25 10:45:01 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
Calculating frequency stats:
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 667/900675 [00:00<02:15, 6655.04it/s]  0%|          | 1333/900675 [00:00<02:33, 5868.53it/s]  0%|          | 1926/900675 [00:00<02:38, 5674.47it/s]  0%|          | 2497/900675 [00:00<02:40, 5611.98it/s]  0%|          | 3214/900675 [00:00<02:26, 6145.04it/s]  0%|          | 3833/900675 [00:00<02:26, 6104.93it/s]  1%|          | 4555/900675 [00:00<02:18, 6457.86it/s]  1%|          | 5261/900675 [00:00<02:14, 6638.56it/s]  1%|          | 5963/900675 [00:00<02:12, 6755.71it/s]  1%|          | 6641/900675 [00:01<02:25, 6140.07it/s]  1%|          | 7267/900675 [00:01<02:24, 6166.58it/s]  1%|          | 7892/900675 [00:01<02:26, 6089.06it/s]  1%|          | 8507/900675 [00:01<02:32, 5862.63it/s]  1%|          | 9160/900675 [00:01<02:27, 6049.13it/s]  1%|          | 9770/900675 [00:01<02:29, 5977.42it/s]  1%|          | 10402/900675 [00:01<02:26, 6070.91it/s]  1%|          | 11012/900675 [00:01<02:26, 6053.57it/s]  1%|▏         | 11620/900675 [00:01<02:31, 5872.70it/s]  1%|▏         | 12259/900675 [00:02<02:27, 6018.37it/s]  1%|▏         | 12871/900675 [00:02<02:26, 6045.19it/s]  2%|▏         | 13556/900675 [00:02<02:21, 6277.95it/s]  2%|▏         | 14186/900675 [00:02<02:23, 6163.35it/s]  2%|▏         | 14817/900675 [00:02<02:22, 6203.86it/s]  2%|▏         | 15439/900675 [00:02<02:24, 6130.96it/s]  2%|▏         | 16053/900675 [00:02<02:28, 5965.43it/s]  2%|▏         | 16651/900675 [00:02<02:30, 5890.49it/s]  2%|▏         | 17289/900675 [00:02<02:26, 6029.88it/s]  2%|▏         | 17894/900675 [00:02<02:27, 5968.62it/s]  2%|▏         | 18492/900675 [00:03<02:31, 5822.35it/s]  2%|▏         | 19225/900675 [00:03<02:20, 6256.77it/s]  2%|▏         | 19882/900675 [00:03<02:18, 6348.03it/s]  2%|▏         | 20519/900675 [00:03<02:23, 6134.80it/s]  2%|▏         | 21136/900675 [00:03<02:24, 6100.14it/s]  2%|▏         | 21748/900675 [00:03<02:28, 5918.29it/s]  2%|▏         | 22398/900675 [00:03<02:24, 6083.15it/s]  3%|▎         | 23039/900675 [00:03<02:22, 6174.75it/s]  3%|▎         | 23721/900675 [00:03<02:17, 6363.77it/s]  3%|▎         | 24483/900675 [00:03<02:10, 6725.95it/s]  3%|▎         | 25173/900675 [00:04<02:09, 6777.24it/s]  3%|▎         | 25852/900675 [00:04<02:13, 6566.73it/s]  3%|▎         | 26511/900675 [00:04<02:21, 6160.92it/s]  3%|▎         | 27133/900675 [00:04<02:27, 5909.02it/s]  3%|▎         | 27729/900675 [00:04<02:29, 5848.92it/s]  3%|▎         | 28385/900675 [00:04<02:24, 6047.78it/s]  3%|▎         | 29068/900675 [00:04<02:19, 6270.23it/s]  3%|▎         | 29699/900675 [00:04<02:23, 6063.69it/s]  3%|▎         | 30309/900675 [00:04<02:24, 6018.72it/s]  3%|▎         | 30914/900675 [00:05<02:31, 5727.36it/s]  3%|▎         | 31509/900675 [00:05<02:30, 5780.87it/s]  4%|▎         | 32091/900675 [00:05<02:34, 5631.99it/s]  4%|▎         | 32684/900675 [00:05<02:31, 5715.19it/s]  4%|▎         | 33258/900675 [00:05<02:34, 5600.00it/s]  4%|▍         | 33882/900675 [00:05<02:29, 5782.80it/s]  4%|▍         | 34547/900675 [00:05<02:23, 6027.19it/s]  4%|▍         | 35152/900675 [00:05<02:25, 5940.12it/s]  4%|▍         | 35757/900675 [00:05<02:24, 5967.61it/s]  4%|▍         | 36396/900675 [00:05<02:22, 6083.44it/s]  4%|▍         | 37006/900675 [00:06<02:27, 5838.51it/s]  4%|▍         | 37593/900675 [00:06<02:33, 5609.57it/s]  4%|▍         | 38207/900675 [00:06<02:29, 5751.20it/s]  4%|▍         | 38815/900675 [00:06<02:27, 5839.22it/s]  4%|▍         | 39402/900675 [00:06<02:30, 5728.47it/s]  4%|▍         | 40029/900675 [00:06<02:26, 5876.88it/s]  5%|▍         | 40727/900675 [00:06<02:18, 6198.82it/s]  5%|▍         | 41349/900675 [00:06<02:20, 6132.46it/s]  5%|▍         | 41964/900675 [00:06<02:29, 5758.04it/s]  5%|▍         | 42546/900675 [00:07<02:32, 5628.54it/s]  5%|▍         | 43113/900675 [00:07<02:32, 5636.60it/s]  5%|▍         | 43774/900675 [00:07<02:24, 5914.84it/s]  5%|▍         | 44369/900675 [00:07<02:28, 5781.41it/s]  5%|▍         | 45013/900675 [00:07<02:23, 5967.87it/s]  5%|▌         | 45633/900675 [00:07<02:21, 6032.57it/s]  5%|▌         | 46371/900675 [00:07<02:13, 6420.26it/s]  5%|▌         | 47016/900675 [00:07<02:15, 6282.38it/s]  5%|▌         | 47965/900675 [00:07<01:58, 7218.98it/s]  5%|▌         | 48691/900675 [00:07<02:00, 7092.43it/s]  5%|▌         | 49404/900675 [00:08<02:00, 7085.66it/s]  6%|▌         | 50115/900675 [00:08<02:06, 6745.22it/s]  6%|▌         | 50794/900675 [00:08<02:14, 6301.98it/s]  6%|▌         | 51432/900675 [00:08<02:15, 6244.63it/s]  6%|▌         | 52124/900675 [00:08<02:11, 6430.36it/s]  6%|▌         | 52891/900675 [00:08<02:04, 6783.06it/s]  6%|▌         | 53575/900675 [00:08<02:11, 6443.95it/s]  6%|▌         | 54226/900675 [00:08<02:15, 6226.45it/s]  6%|▌         | 54854/900675 [00:08<02:18, 6108.62it/s]  6%|▌         | 55469/900675 [00:09<02:20, 6024.68it/s]  6%|▌         | 56158/900675 [00:09<02:14, 6269.53it/s]  6%|▋         | 56788/900675 [00:09<02:21, 5954.72it/s]  6%|▋         | 57388/900675 [00:09<02:27, 5708.84it/s]  6%|▋         | 58035/900675 [00:09<02:22, 5912.16it/s]  7%|▋         | 58784/900675 [00:09<02:12, 6353.03it/s]  7%|▋         | 59425/900675 [00:09<02:20, 5990.22it/s]  7%|▋         | 60032/900675 [00:09<02:21, 5948.90it/s]  7%|▋         | 60653/900675 [00:09<02:19, 6022.57it/s]  7%|▋         | 61440/900675 [00:10<02:08, 6547.22it/s]  7%|▋         | 62100/900675 [00:10<02:13, 6277.82it/s]  7%|▋         | 62733/900675 [00:10<02:13, 6269.19it/s]  7%|▋         | 63364/900675 [00:10<02:17, 6071.40it/s]  7%|▋         | 63997/900675 [00:10<02:16, 6144.03it/s]  7%|▋         | 64615/900675 [00:10<02:18, 6044.77it/s]  7%|▋         | 65222/900675 [00:10<02:18, 6011.25it/s]  7%|▋         | 65881/900675 [00:10<02:15, 6179.25it/s]  7%|▋         | 66501/900675 [00:10<02:15, 6166.73it/s]  7%|▋         | 67119/900675 [00:10<02:18, 6035.18it/s]  8%|▊         | 67724/900675 [00:11<02:23, 5818.26it/s]  8%|▊         | 68318/900675 [00:11<02:22, 5843.96it/s]  8%|▊         | 69057/900675 [00:11<02:12, 6291.08it/s]  8%|▊         | 69696/900675 [00:11<02:11, 6312.03it/s]  8%|▊         | 70330/900675 [00:11<02:11, 6312.50it/s]  8%|▊         | 70963/900675 [00:11<02:16, 6098.89it/s]  8%|▊         | 71592/900675 [00:11<02:14, 6148.04it/s]  8%|▊         | 72210/900675 [00:11<02:14, 6150.76it/s]  8%|▊         | 72975/900675 [00:11<02:05, 6585.10it/s]  8%|▊         | 73638/900675 [00:12<02:05, 6598.26it/s]  8%|▊         | 74470/900675 [00:12<01:56, 7101.03it/s]  8%|▊         | 75182/900675 [00:12<02:10, 6311.94it/s]  8%|▊         | 75830/900675 [00:12<02:12, 6214.19it/s]  8%|▊         | 76491/900675 [00:12<02:10, 6321.03it/s]  9%|▊         | 77132/900675 [00:12<02:13, 6155.24it/s]  9%|▊         | 77807/900675 [00:12<02:10, 6322.12it/s]  9%|▊         | 78445/900675 [00:12<02:14, 6107.06it/s]  9%|▉         | 79061/900675 [00:12<02:22, 5760.66it/s]  9%|▉         | 79644/900675 [00:13<02:22, 5755.37it/s]  9%|▉         | 80224/900675 [00:13<02:23, 5723.44it/s]  9%|▉         | 80865/900675 [00:13<02:18, 5916.38it/s]  9%|▉         | 81528/900675 [00:13<02:13, 6121.22it/s]  9%|▉         | 82143/900675 [00:13<02:14, 6068.67it/s]  9%|▉         | 82806/900675 [00:13<02:11, 6230.15it/s]  9%|▉         | 83431/900675 [00:13<02:11, 6225.38it/s]  9%|▉         | 84078/900675 [00:13<02:09, 6292.34it/s]  9%|▉         | 84722/900675 [00:13<02:08, 6332.75it/s]  9%|▉         | 85356/900675 [00:13<02:15, 5998.76it/s] 10%|▉         | 86034/900675 [00:14<02:10, 6220.71it/s] 10%|▉         | 86660/900675 [00:14<02:11, 6193.02it/s] 10%|▉         | 87355/900675 [00:14<02:06, 6413.29it/s] 10%|▉         | 87999/900675 [00:14<02:07, 6382.75it/s] 10%|▉         | 88639/900675 [00:14<02:17, 5906.77it/s] 10%|▉         | 89252/900675 [00:14<02:16, 5963.65it/s] 10%|▉         | 89876/900675 [00:14<02:14, 6038.60it/s] 10%|█         | 90485/900675 [00:14<02:19, 5817.31it/s] 10%|█         | 91072/900675 [00:14<02:19, 5785.22it/s] 10%|█         | 91755/900675 [00:14<02:13, 6079.51it/s] 10%|█         | 92456/900675 [00:15<02:07, 6344.50it/s] 10%|█         | 93094/900675 [00:15<02:10, 6169.77it/s] 10%|█         | 93714/900675 [00:15<02:10, 6165.23it/s] 10%|█         | 94333/900675 [00:15<02:14, 5994.14it/s] 11%|█         | 95045/900675 [00:15<02:07, 6316.42it/s] 11%|█         | 95680/900675 [00:15<02:13, 6045.37it/s] 11%|█         | 96356/900675 [00:15<02:08, 6245.82it/s] 11%|█         | 97145/900675 [00:15<01:59, 6720.33it/s] 11%|█         | 97822/900675 [00:15<02:07, 6275.87it/s] 11%|█         | 98473/900675 [00:16<02:06, 6340.09it/s] 11%|█         | 99114/900675 [00:16<02:09, 6194.91it/s] 11%|█         | 99739/900675 [00:16<02:13, 6001.83it/s] 11%|█         | 100343/900675 [00:16<02:17, 5835.82it/s] 11%|█         | 100968/900675 [00:16<02:14, 5938.21it/s] 11%|█▏        | 101568/900675 [00:16<02:14, 5949.93it/s] 11%|█▏        | 102232/900675 [00:16<02:09, 6144.18it/s] 11%|█▏        | 102849/900675 [00:16<02:09, 6139.88it/s] 11%|█▏        | 103465/900675 [00:16<02:12, 6008.55it/s] 12%|█▏        | 104068/900675 [00:17<02:25, 5474.44it/s] 12%|█▏        | 104712/900675 [00:17<02:18, 5732.37it/s] 12%|█▏        | 105295/900675 [00:17<02:18, 5759.58it/s] 12%|█▏        | 105966/900675 [00:17<02:11, 6027.56it/s] 12%|█▏        | 106575/900675 [00:17<02:13, 5948.04it/s] 12%|█▏        | 107174/900675 [00:17<02:17, 5789.07it/s] 12%|█▏        | 107806/900675 [00:17<02:13, 5930.34it/s] 12%|█▏        | 108433/900675 [00:17<02:11, 6026.74it/s] 12%|█▏        | 109039/900675 [00:17<02:18, 5728.68it/s] 12%|█▏        | 109628/900675 [00:17<02:17, 5772.38it/s] 12%|█▏        | 110241/900675 [00:18<02:14, 5875.36it/s] 12%|█▏        | 110899/900675 [00:18<02:09, 6079.33it/s] 12%|█▏        | 111556/900675 [00:18<02:06, 6223.60it/s] 12%|█▏        | 112181/900675 [00:18<02:08, 6127.57it/s] 13%|█▎        | 112834/900675 [00:18<02:06, 6243.12it/s] 13%|█▎        | 113460/900675 [00:18<02:10, 6033.30it/s] 13%|█▎        | 114066/900675 [00:18<02:10, 6030.53it/s] 13%|█▎        | 114671/900675 [00:18<02:11, 5958.32it/s] 13%|█▎        | 115309/900675 [00:18<02:09, 6076.89it/s] 13%|█▎        | 115918/900675 [00:19<02:16, 5728.86it/s] 13%|█▎        | 116496/900675 [00:19<02:18, 5668.98it/s] 13%|█▎        | 117077/900675 [00:19<02:17, 5701.42it/s] 13%|█▎        | 117675/900675 [00:19<02:15, 5774.62it/s] 13%|█▎        | 118256/900675 [00:19<02:15, 5776.75it/s] 13%|█▎        | 118906/900675 [00:19<02:10, 5985.92it/s] 13%|█▎        | 119506/900675 [00:19<02:12, 5878.92it/s] 13%|█▎        | 120257/900675 [00:19<02:02, 6356.44it/s] 13%|█▎        | 120895/900675 [00:19<02:04, 6259.22it/s] 13%|█▎        | 121523/900675 [00:19<02:10, 5979.15it/s] 14%|█▎        | 122127/900675 [00:20<02:09, 5996.42it/s] 14%|█▎        | 122759/900675 [00:20<02:07, 6087.98it/s] 14%|█▎        | 123370/900675 [00:20<02:13, 5835.72it/s] 14%|█▍        | 123978/900675 [00:20<02:11, 5895.95it/s] 14%|█▍        | 124589/900675 [00:20<02:10, 5955.42it/s] 14%|█▍        | 125187/900675 [00:20<02:10, 5936.77it/s] 14%|█▍        | 125847/900675 [00:20<02:06, 6117.97it/s] 14%|█▍        | 126482/900675 [00:20<02:05, 6184.75it/s] 14%|█▍        | 127144/900675 [00:20<02:02, 6310.64it/s] 14%|█▍        | 127776/900675 [00:20<02:06, 6128.49it/s] 14%|█▍        | 128391/900675 [00:21<02:08, 6024.33it/s] 14%|█▍        | 129008/900675 [00:21<02:07, 6063.47it/s] 14%|█▍        | 129665/900675 [00:21<02:04, 6211.64it/s] 14%|█▍        | 130288/900675 [00:21<02:11, 5872.23it/s] 15%|█▍        | 130880/900675 [00:21<02:11, 5834.36it/s] 15%|█▍        | 131488/900675 [00:21<02:10, 5902.05it/s] 15%|█▍        | 132081/900675 [00:21<02:20, 5471.21it/s] 15%|█▍        | 132785/900675 [00:21<02:10, 5902.42it/s] 15%|█▍        | 133387/900675 [00:21<02:09, 5928.51it/s] 15%|█▍        | 134070/900675 [00:22<02:03, 6182.98it/s] 15%|█▍        | 134782/900675 [00:22<01:58, 6450.65it/s] 15%|█▌        | 135525/900675 [00:22<01:53, 6737.97it/s] 15%|█▌        | 136203/900675 [00:22<01:56, 6565.75it/s] 15%|█▌        | 136924/900675 [00:22<01:53, 6750.20it/s] 15%|█▌        | 137614/900675 [00:22<01:52, 6775.88it/s] 15%|█▌        | 138294/900675 [00:22<01:59, 6368.98it/s] 15%|█▌        | 138940/900675 [00:22<01:59, 6393.60it/s] 15%|█▌        | 139584/900675 [00:22<02:00, 6312.10it/s] 16%|█▌        | 140219/900675 [00:22<02:03, 6173.71it/s] 16%|█▌        | 140839/900675 [00:23<02:03, 6175.17it/s] 16%|█▌        | 141459/900675 [00:23<02:06, 5981.52it/s] 16%|█▌        | 142060/900675 [00:23<02:10, 5832.72it/s] 16%|█▌        | 142677/900675 [00:23<02:07, 5925.12it/s] 16%|█▌        | 143272/900675 [00:23<02:10, 5818.15it/s] 16%|█▌        | 144049/900675 [00:23<01:58, 6372.81it/s] 16%|█▌        | 144690/900675 [00:23<01:59, 6334.75it/s] 16%|█▌        | 145424/900675 [00:23<01:54, 6624.86it/s] 16%|█▌        | 146089/900675 [00:23<01:54, 6565.63it/s] 16%|█▋        | 146748/900675 [00:24<02:01, 6189.76it/s] 16%|█▋        | 147372/900675 [00:24<02:06, 5976.23it/s] 16%|█▋        | 147974/900675 [00:24<02:16, 5527.82it/s] 16%|█▋        | 148596/900675 [00:24<02:11, 5709.93it/s] 17%|█▋        | 149215/900675 [00:24<02:08, 5841.20it/s] 17%|█▋        | 149806/900675 [00:24<02:09, 5813.88it/s] 17%|█▋        | 150392/900675 [00:24<02:11, 5723.19it/s] 17%|█▋        | 150968/900675 [00:24<02:13, 5634.34it/s] 17%|█▋        | 151610/900675 [00:24<02:08, 5850.47it/s] 17%|█▋        | 152198/900675 [00:25<02:14, 5580.16it/s] 17%|█▋        | 152868/900675 [00:25<02:06, 5893.64it/s] 17%|█▋        | 153562/900675 [00:25<02:00, 6189.82it/s] 17%|█▋        | 154186/900675 [00:25<02:06, 5898.46it/s] 17%|█▋        | 154782/900675 [00:25<02:06, 5883.24it/s] 17%|█▋        | 155451/900675 [00:25<02:01, 6113.75it/s] 17%|█▋        | 156098/900675 [00:25<01:59, 6215.26it/s] 17%|█▋        | 156723/900675 [00:25<02:01, 6115.12it/s] 17%|█▋        | 157399/900675 [00:25<01:57, 6300.60it/s] 18%|█▊        | 158078/900675 [00:25<01:55, 6442.69it/s] 18%|█▊        | 158731/900675 [00:26<01:54, 6465.74it/s] 18%|█▊        | 159379/900675 [00:26<02:00, 6157.30it/s] 18%|█▊        | 160073/900675 [00:26<01:56, 6381.82it/s] 18%|█▊        | 160715/900675 [00:26<02:01, 6076.46it/s] 18%|█▊        | 161379/900675 [00:26<01:58, 6232.96it/s] 18%|█▊        | 162007/900675 [00:26<02:01, 6083.99it/s] 18%|█▊        | 162665/900675 [00:26<01:58, 6218.33it/s] 18%|█▊        | 163322/900675 [00:26<01:56, 6319.97it/s] 18%|█▊        | 163957/900675 [00:26<02:03, 5960.40it/s] 18%|█▊        | 164585/900675 [00:27<02:01, 6048.12it/s] 18%|█▊        | 165278/900675 [00:27<01:56, 6298.92it/s] 18%|█▊        | 165970/900675 [00:27<01:53, 6478.62it/s] 18%|█▊        | 166622/900675 [00:27<01:54, 6408.52it/s] 19%|█▊        | 167276/900675 [00:27<01:53, 6445.61it/s] 19%|█▊        | 167923/900675 [00:27<01:59, 6120.70it/s] 19%|█▊        | 168546/900675 [00:27<01:59, 6144.37it/s] 19%|█▉        | 169187/900675 [00:27<01:57, 6217.00it/s] 19%|█▉        | 169812/900675 [00:27<02:03, 5932.84it/s] 19%|█▉        | 170410/900675 [00:27<02:06, 5754.22it/s] 19%|█▉        | 171002/900675 [00:28<02:05, 5792.91it/s] 19%|█▉        | 171668/900675 [00:28<02:00, 6030.26it/s] 19%|█▉        | 172274/900675 [00:28<02:05, 5799.39it/s] 19%|█▉        | 172921/900675 [00:28<02:01, 5990.28it/s] 19%|█▉        | 173524/900675 [00:28<02:03, 5894.12it/s] 19%|█▉        | 174138/900675 [00:28<02:01, 5963.25it/s] 19%|█▉        | 174837/900675 [00:28<01:55, 6259.17it/s] 19%|█▉        | 175466/900675 [00:28<02:01, 5965.21it/s] 20%|█▉        | 176067/900675 [00:28<02:05, 5781.44it/s] 20%|█▉        | 176662/900675 [00:29<02:04, 5822.72it/s] 20%|█▉        | 177262/900675 [00:29<02:03, 5869.13it/s] 20%|█▉        | 177851/900675 [00:29<02:12, 5470.91it/s] 20%|█▉        | 178473/900675 [00:29<02:07, 5673.79it/s] 20%|█▉        | 179110/900675 [00:29<02:02, 5869.13it/s] 20%|█▉        | 179702/900675 [00:29<02:05, 5760.04it/s] 20%|██        | 180304/900675 [00:29<02:03, 5830.62it/s] 20%|██        | 180890/900675 [00:29<02:03, 5817.60it/s] 20%|██        | 181474/900675 [00:29<02:05, 5753.29it/s] 20%|██        | 182063/900675 [00:29<02:04, 5792.06it/s] 20%|██        | 182648/900675 [00:30<02:03, 5805.93it/s] 20%|██        | 183230/900675 [00:30<02:04, 5752.21it/s] 20%|██        | 183806/900675 [00:30<02:11, 5463.14it/s] 20%|██        | 184382/900675 [00:30<02:09, 5537.42it/s] 21%|██        | 185023/900675 [00:30<02:03, 5786.67it/s] 21%|██        | 185605/900675 [00:30<02:10, 5496.29it/s] 21%|██        | 186212/900675 [00:30<02:06, 5652.06it/s] 21%|██        | 186782/900675 [00:30<02:06, 5661.72it/s] 21%|██        | 187419/900675 [00:30<02:01, 5862.57it/s] 21%|██        | 188008/900675 [00:30<02:02, 5826.25it/s] 21%|██        | 188717/900675 [00:31<01:54, 6191.60it/s] 21%|██        | 189339/900675 [00:31<01:59, 5939.50it/s] 21%|██        | 189992/900675 [00:31<01:56, 6104.39it/s] 21%|██        | 190637/900675 [00:31<01:54, 6198.87it/s] 21%|██        | 191260/900675 [00:31<01:55, 6164.59it/s] 21%|██▏       | 191879/900675 [00:31<01:55, 6137.74it/s] 21%|██▏       | 192499/900675 [00:31<01:55, 6151.36it/s] 21%|██▏       | 193119/900675 [00:31<01:54, 6152.76it/s] 22%|██▏       | 193735/900675 [00:31<01:55, 6095.00it/s] 22%|██▏       | 194387/900675 [00:32<01:53, 6218.04it/s] 22%|██▏       | 195010/900675 [00:32<01:55, 6086.84it/s] 22%|██▏       | 195655/900675 [00:32<01:53, 6191.76it/s] 22%|██▏       | 196276/900675 [00:32<01:56, 6045.94it/s] 22%|██▏       | 196882/900675 [00:32<02:03, 5714.32it/s] 22%|██▏       | 197805/900675 [00:32<01:44, 6702.78it/s] 22%|██▏       | 198485/900675 [00:32<01:53, 6203.95it/s] 22%|██▏       | 199118/900675 [00:32<01:53, 6180.56it/s] 22%|██▏       | 199745/900675 [00:32<01:56, 6017.15it/s] 22%|██▏       | 200353/900675 [00:33<01:58, 5930.60it/s] 22%|██▏       | 200951/900675 [00:33<01:57, 5934.24it/s] 22%|██▏       | 201548/900675 [00:33<01:59, 5855.43it/s] 22%|██▏       | 202162/900675 [00:33<01:57, 5933.31it/s] 23%|██▎       | 202757/900675 [00:33<01:59, 5843.75it/s] 23%|██▎       | 203469/900675 [00:33<01:52, 6213.35it/s] 23%|██▎       | 204106/900675 [00:33<01:51, 6253.33it/s] 23%|██▎       | 204733/900675 [00:33<01:55, 6013.88it/s] 23%|██▎       | 205338/900675 [00:33<01:56, 5943.51it/s] 23%|██▎       | 205935/900675 [00:33<01:57, 5925.53it/s] 23%|██▎       | 206562/900675 [00:34<01:55, 6021.55it/s] 23%|██▎       | 207166/900675 [00:34<01:59, 5826.41it/s] 23%|██▎       | 207764/900675 [00:34<01:58, 5867.39it/s] 23%|██▎       | 208353/900675 [00:34<01:59, 5807.41it/s] 23%|██▎       | 208935/900675 [00:34<02:04, 5539.57it/s] 23%|██▎       | 209525/900675 [00:34<02:02, 5641.12it/s] 23%|██▎       | 210211/900675 [00:34<01:55, 5988.49it/s] 23%|██▎       | 210856/900675 [00:34<01:52, 6119.78it/s] 23%|██▎       | 211471/900675 [00:34<01:52, 6120.49it/s] 24%|██▎       | 212085/900675 [00:34<01:55, 5980.37it/s] 24%|██▎       | 212809/900675 [00:35<01:48, 6343.61it/s] 24%|██▎       | 213446/900675 [00:35<01:50, 6219.27it/s] 24%|██▍       | 214070/900675 [00:35<01:56, 5906.23it/s] 24%|██▍       | 214665/900675 [00:35<01:58, 5775.53it/s] 24%|██▍       | 215246/900675 [00:35<02:03, 5541.41it/s] 24%|██▍       | 215804/900675 [00:35<02:05, 5438.67it/s] 24%|██▍       | 216390/900675 [00:35<02:03, 5550.41it/s] 24%|██▍       | 216985/900675 [00:35<02:00, 5661.55it/s] 24%|██▍       | 217568/900675 [00:35<01:59, 5708.03it/s] 24%|██▍       | 218244/900675 [00:36<01:53, 6010.38it/s] 24%|██▍       | 218847/900675 [00:36<01:53, 5993.47it/s] 24%|██▍       | 219494/900675 [00:36<01:51, 6128.81it/s] 24%|██▍       | 220108/900675 [00:36<01:52, 6070.47it/s] 25%|██▍       | 220716/900675 [00:36<01:54, 5940.72it/s] 25%|██▍       | 221363/900675 [00:36<01:51, 6087.80it/s] 25%|██▍       | 221973/900675 [00:36<01:54, 5912.06it/s] 25%|██▍       | 222566/900675 [00:36<01:59, 5684.74it/s] 25%|██▍       | 223190/900675 [00:36<01:56, 5834.70it/s] 25%|██▍       | 223776/900675 [00:36<02:02, 5531.11it/s] 25%|██▍       | 224492/900675 [00:37<01:52, 5984.35it/s] 25%|██▍       | 225115/900675 [00:37<01:51, 6051.56it/s] 25%|██▌       | 225864/900675 [00:37<01:44, 6466.18it/s] 25%|██▌       | 226516/900675 [00:37<01:45, 6414.38it/s] 25%|██▌       | 227161/900675 [00:37<01:47, 6236.73it/s] 25%|██▌       | 227788/900675 [00:37<01:52, 5976.84it/s] 25%|██▌       | 228390/900675 [00:37<01:52, 5970.65it/s] 25%|██▌       | 228990/900675 [00:37<01:54, 5864.94it/s] 25%|██▌       | 229579/900675 [00:37<01:57, 5699.03it/s] 26%|██▌       | 230151/900675 [00:38<01:58, 5681.05it/s] 26%|██▌       | 230763/900675 [00:38<01:55, 5806.76it/s] 26%|██▌       | 231345/900675 [00:38<01:55, 5806.97it/s] 26%|██▌       | 231927/900675 [00:38<01:58, 5651.83it/s] 26%|██▌       | 232550/900675 [00:38<01:54, 5819.00it/s] 26%|██▌       | 233292/900675 [00:38<01:46, 6286.25it/s] 26%|██▌       | 233923/900675 [00:38<01:46, 6269.96it/s] 26%|██▌       | 234633/900675 [00:38<01:42, 6512.30it/s] 26%|██▌       | 235305/900675 [00:38<01:41, 6563.69it/s] 26%|██▌       | 235963/900675 [00:38<01:43, 6443.59it/s] 26%|██▋       | 236609/900675 [00:39<01:50, 6009.80it/s] 26%|██▋       | 237227/900675 [00:39<01:49, 6054.88it/s] 26%|██▋       | 237838/900675 [00:39<01:51, 5954.65it/s] 26%|██▋       | 238578/900675 [00:39<01:44, 6360.70it/s] 27%|██▋       | 239219/900675 [00:39<01:46, 6184.27it/s] 27%|██▋       | 239841/900675 [00:39<01:51, 5940.55it/s] 27%|██▋       | 240607/900675 [00:39<01:42, 6424.80it/s] 27%|██▋       | 241307/900675 [00:39<01:40, 6589.61it/s] 27%|██▋       | 241971/900675 [00:39<01:42, 6411.29it/s] 27%|██▋       | 242617/900675 [00:40<01:44, 6290.48it/s] 27%|██▋       | 243249/900675 [00:40<01:46, 6198.41it/s] 27%|██▋       | 243871/900675 [00:40<01:47, 6127.68it/s] 27%|██▋       | 244495/900675 [00:40<01:46, 6160.02it/s] 27%|██▋       | 245113/900675 [00:40<01:51, 5891.44it/s] 27%|██▋       | 245764/900675 [00:40<01:48, 6060.42it/s] 27%|██▋       | 246488/900675 [00:40<01:42, 6398.30it/s] 27%|██▋       | 247196/900675 [00:40<01:39, 6594.32it/s] 28%|██▊       | 247859/900675 [00:40<01:41, 6435.31it/s] 28%|██▊       | 248506/900675 [00:40<01:44, 6226.03it/s] 28%|██▊       | 249184/900675 [00:41<01:42, 6384.25it/s] 28%|██▊       | 249893/900675 [00:41<01:38, 6580.12it/s] 28%|██▊       | 250554/900675 [00:41<01:41, 6385.72it/s] 28%|██▊       | 251196/900675 [00:41<01:41, 6378.87it/s] 28%|██▊       | 251836/900675 [00:41<01:41, 6365.22it/s] 28%|██▊       | 252474/900675 [00:41<01:41, 6362.88it/s] 28%|██▊       | 253112/900675 [00:41<01:46, 6072.06it/s] 28%|██▊       | 253826/900675 [00:41<01:41, 6370.64it/s] 28%|██▊       | 254467/900675 [00:41<01:45, 6111.29it/s] 28%|██▊       | 255109/900675 [00:42<01:44, 6197.12it/s] 28%|██▊       | 255735/900675 [00:42<01:43, 6214.65it/s] 28%|██▊       | 256374/900675 [00:42<01:42, 6265.72it/s] 29%|██▊       | 257030/900675 [00:42<01:41, 6349.12it/s] 29%|██▊       | 257723/900675 [00:42<01:38, 6520.56it/s] 29%|██▊       | 258377/900675 [00:42<01:50, 5825.59it/s] 29%|██▉       | 258974/900675 [00:42<01:51, 5761.26it/s] 29%|██▉       | 259560/900675 [00:42<01:52, 5711.74it/s] 29%|██▉       | 260220/900675 [00:42<01:47, 5959.46it/s] 29%|██▉       | 260954/900675 [00:42<01:40, 6348.95it/s] 29%|██▉       | 261595/900675 [00:43<01:41, 6291.93it/s] 29%|██▉       | 262229/900675 [00:43<01:46, 5981.41it/s] 29%|██▉       | 262833/900675 [00:43<01:48, 5902.99it/s] 29%|██▉       | 263428/900675 [00:43<01:51, 5736.37it/s] 29%|██▉       | 264045/900675 [00:43<01:48, 5857.20it/s] 29%|██▉       | 264644/900675 [00:43<01:48, 5887.49it/s] 29%|██▉       | 265235/900675 [00:43<01:48, 5872.23it/s] 30%|██▉       | 265824/900675 [00:43<01:49, 5804.04it/s] 30%|██▉       | 266425/900675 [00:43<01:48, 5859.99it/s] 30%|██▉       | 267012/900675 [00:44<01:50, 5740.06it/s] 30%|██▉       | 267616/900675 [00:44<01:48, 5820.66it/s] 30%|██▉       | 268199/900675 [00:44<01:49, 5758.47it/s] 30%|██▉       | 268835/900675 [00:44<01:46, 5934.74it/s] 30%|██▉       | 269496/900675 [00:44<01:42, 6129.33it/s] 30%|██▉       | 270110/900675 [00:44<01:43, 6110.72it/s] 30%|███       | 270727/900675 [00:44<01:42, 6124.76it/s] 30%|███       | 271340/900675 [00:44<01:43, 6056.43it/s] 30%|███       | 272008/900675 [00:44<01:40, 6227.85it/s] 30%|███       | 272789/900675 [00:44<01:33, 6692.57it/s] 30%|███       | 273460/900675 [00:45<01:34, 6669.39it/s] 30%|███       | 274128/900675 [00:45<01:35, 6565.31it/s] 31%|███       | 274786/900675 [00:45<01:41, 6138.01it/s] 31%|███       | 275406/900675 [00:45<01:45, 5904.73it/s] 31%|███       | 276009/900675 [00:45<01:45, 5935.46it/s] 31%|███       | 276607/900675 [00:45<01:49, 5673.40it/s] 31%|███       | 277219/900675 [00:45<01:47, 5792.25it/s] 31%|███       | 277816/900675 [00:45<01:46, 5841.83it/s] 31%|███       | 278525/900675 [00:45<01:40, 6192.22it/s] 31%|███       | 279148/900675 [00:46<01:43, 6012.32it/s] 31%|███       | 279753/900675 [00:46<01:43, 5974.10it/s] 31%|███       | 280452/900675 [00:46<01:38, 6266.29it/s] 31%|███       | 281082/900675 [00:46<01:46, 5828.22it/s] 31%|███▏      | 281732/900675 [00:46<01:42, 6010.61it/s] 31%|███▏      | 282340/900675 [00:46<01:47, 5731.22it/s] 31%|███▏      | 282970/900675 [00:46<01:44, 5884.18it/s] 31%|███▏      | 283664/900675 [00:46<01:39, 6182.28it/s] 32%|███▏      | 284288/900675 [00:46<01:41, 6064.18it/s] 32%|███▏      | 284913/900675 [00:46<01:40, 6116.53it/s] 32%|███▏      | 285528/900675 [00:47<01:42, 6004.22it/s] 32%|███▏      | 286161/900675 [00:47<01:40, 6090.52it/s] 32%|███▏      | 286772/900675 [00:47<01:42, 5970.04it/s] 32%|███▏      | 287371/900675 [00:47<01:43, 5911.33it/s] 32%|███▏      | 288082/900675 [00:47<01:37, 6252.79it/s] 32%|███▏      | 288709/900675 [00:47<01:38, 6206.30it/s] 32%|███▏      | 289331/900675 [00:47<01:39, 6137.87it/s] 32%|███▏      | 289946/900675 [00:47<01:45, 5796.01it/s] 32%|███▏      | 290564/900675 [00:47<01:43, 5904.42it/s] 32%|███▏      | 291158/900675 [00:48<01:43, 5861.80it/s] 32%|███▏      | 291765/900675 [00:48<01:42, 5918.76it/s] 32%|███▏      | 292359/900675 [00:48<01:46, 5732.91it/s] 33%|███▎      | 292935/900675 [00:48<01:47, 5661.33it/s] 33%|███▎      | 293578/900675 [00:48<01:43, 5881.79it/s] 33%|███▎      | 294174/900675 [00:48<01:42, 5890.83it/s] 33%|███▎      | 294834/900675 [00:48<01:39, 6093.63it/s] 33%|███▎      | 295484/900675 [00:48<01:37, 6208.48it/s] 33%|███▎      | 296135/900675 [00:48<01:36, 6296.89it/s] 33%|███▎      | 296766/900675 [00:48<01:38, 6122.38it/s] 33%|███▎      | 297380/900675 [00:49<01:38, 6100.67it/s] 33%|███▎      | 298033/900675 [00:49<01:36, 6226.24it/s] 33%|███▎      | 298657/900675 [00:49<01:38, 6100.66it/s] 33%|███▎      | 299278/900675 [00:49<01:38, 6129.56it/s] 33%|███▎      | 299966/900675 [00:49<01:34, 6350.31it/s] 33%|███▎      | 300603/900675 [00:49<01:36, 6212.73it/s] 33%|███▎      | 301226/900675 [00:49<01:39, 6022.07it/s] 34%|███▎      | 301830/900675 [00:49<01:43, 5764.48it/s] 34%|███▎      | 302491/900675 [00:49<01:39, 6001.64it/s] 34%|███▎      | 303146/900675 [00:50<01:37, 6157.74it/s] 34%|███▎      | 303787/900675 [00:50<01:35, 6228.39it/s] 34%|███▍      | 304413/900675 [00:50<01:38, 6033.96it/s] 34%|███▍      | 305048/900675 [00:50<01:37, 6123.49it/s] 34%|███▍      | 305663/900675 [00:50<01:41, 5857.57it/s] 34%|███▍      | 306321/900675 [00:50<01:38, 6058.69it/s] 34%|███▍      | 306931/900675 [00:50<01:42, 5802.64it/s] 34%|███▍      | 307597/900675 [00:50<01:38, 6044.97it/s] 34%|███▍      | 308206/900675 [00:50<01:42, 5777.78it/s] 34%|███▍      | 308789/900675 [00:50<01:43, 5697.23it/s] 34%|███▍      | 309723/900675 [00:51<01:27, 6727.78it/s] 34%|███▍      | 310404/900675 [00:51<01:31, 6476.05it/s] 35%|███▍      | 311059/900675 [00:51<01:34, 6257.18it/s] 35%|███▍      | 311691/900675 [00:51<01:35, 6184.91it/s] 35%|███▍      | 312339/900675 [00:51<01:34, 6256.12it/s] 35%|███▍      | 313003/900675 [00:51<01:32, 6360.91it/s] 35%|███▍      | 313673/900675 [00:51<01:30, 6453.29it/s] 35%|███▍      | 314369/900675 [00:51<01:28, 6598.94it/s] 35%|███▍      | 315031/900675 [00:51<01:38, 5966.57it/s] 35%|███▌      | 315640/900675 [00:52<01:37, 5980.70it/s] 35%|███▌      | 316247/900675 [00:52<01:37, 5987.63it/s] 35%|███▌      | 316852/900675 [00:52<01:38, 5912.00it/s] 35%|███▌      | 317450/900675 [00:52<01:38, 5930.76it/s] 35%|███▌      | 318047/900675 [00:52<01:41, 5729.05it/s] 35%|███▌      | 318624/900675 [00:52<01:42, 5701.59it/s] 35%|███▌      | 319286/900675 [00:52<01:37, 5962.01it/s] 36%|███▌      | 319898/900675 [00:52<01:36, 5993.68it/s] 36%|███▌      | 320500/900675 [00:52<01:41, 5716.29it/s] 36%|███▌      | 321244/900675 [00:52<01:33, 6208.30it/s] 36%|███▌      | 321870/900675 [00:53<01:37, 5936.15it/s] 36%|███▌      | 322470/900675 [00:53<01:39, 5789.97it/s] 36%|███▌      | 323067/900675 [00:53<01:38, 5835.16it/s] 36%|███▌      | 323712/900675 [00:53<01:36, 6006.63it/s] 36%|███▌      | 324335/900675 [00:53<01:34, 6071.14it/s] 36%|███▌      | 324945/900675 [00:53<01:34, 6062.14it/s] 36%|███▌      | 325553/900675 [00:53<01:42, 5627.94it/s] 36%|███▌      | 326143/900675 [00:53<01:40, 5701.59it/s] 36%|███▋      | 326872/900675 [00:53<01:33, 6154.44it/s] 36%|███▋      | 327530/900675 [00:54<01:31, 6275.29it/s] 36%|███▋      | 328318/900675 [00:54<01:24, 6743.03it/s] 37%|███▋      | 328997/900675 [00:54<01:25, 6682.66it/s] 37%|███▋      | 329669/900675 [00:54<01:31, 6208.52it/s] 37%|███▋      | 330299/900675 [00:54<01:37, 5876.17it/s] 37%|███▋      | 330932/900675 [00:54<01:34, 5997.78it/s] 37%|███▋      | 331539/900675 [00:54<01:38, 5807.33it/s] 37%|███▋      | 332154/900675 [00:54<01:36, 5901.58it/s] 37%|███▋      | 332781/900675 [00:54<01:34, 6005.68it/s] 37%|███▋      | 333386/900675 [00:55<01:35, 5941.04it/s] 37%|███▋      | 334033/900675 [00:55<01:33, 6085.73it/s] 37%|███▋      | 334644/900675 [00:55<01:34, 6021.43it/s] 37%|███▋      | 335301/900675 [00:55<01:31, 6180.70it/s] 37%|███▋      | 335921/900675 [00:55<01:31, 6183.52it/s] 37%|███▋      | 336544/900675 [00:55<01:31, 6191.81it/s] 37%|███▋      | 337188/900675 [00:55<01:29, 6262.66it/s] 38%|███▊      | 337855/900675 [00:55<01:28, 6383.76it/s] 38%|███▊      | 338494/900675 [00:55<01:29, 6269.42it/s] 38%|███▊      | 339122/900675 [00:55<01:30, 6234.32it/s] 38%|███▊      | 339746/900675 [00:56<01:31, 6146.31it/s] 38%|███▊      | 340362/900675 [00:56<01:44, 5356.18it/s] 38%|███▊      | 340991/900675 [00:56<01:39, 5603.10it/s] 38%|███▊      | 341631/900675 [00:56<01:36, 5821.94it/s] 38%|███▊      | 342250/900675 [00:56<01:34, 5925.67it/s] 38%|███▊      | 342854/900675 [00:56<01:33, 5957.89it/s] 38%|███▊      | 343488/900675 [00:56<01:31, 6066.24it/s] 38%|███▊      | 344175/900675 [00:56<01:28, 6296.95it/s] 38%|███▊      | 344809/900675 [00:56<01:31, 6067.40it/s] 38%|███▊      | 345431/900675 [00:56<01:30, 6109.71it/s] 38%|███▊      | 346046/900675 [00:57<01:38, 5636.15it/s] 38%|███▊      | 346703/900675 [00:57<01:34, 5888.66it/s] 39%|███▊      | 347331/900675 [00:57<01:32, 5994.01it/s] 39%|███▊      | 347951/900675 [00:57<01:31, 6052.91it/s] 39%|███▊      | 348561/900675 [00:57<01:32, 5980.72it/s] 39%|███▉      | 349168/900675 [00:57<01:31, 6006.39it/s] 39%|███▉      | 349772/900675 [00:57<01:32, 5938.77it/s] 39%|███▉      | 350651/900675 [00:57<01:21, 6770.24it/s] 39%|███▉      | 351332/900675 [00:57<01:25, 6448.55it/s] 39%|███▉      | 351982/900675 [00:58<01:29, 6116.60it/s] 39%|███▉      | 352600/900675 [00:58<01:37, 5609.22it/s] 39%|███▉      | 353220/900675 [00:58<01:34, 5764.92it/s] 39%|███▉      | 353806/900675 [00:58<01:35, 5697.33it/s] 39%|███▉      | 354569/900675 [00:58<01:27, 6236.54it/s] 39%|███▉      | 355201/900675 [00:58<01:33, 5821.09it/s] 40%|███▉      | 355829/900675 [00:58<01:31, 5947.05it/s] 40%|███▉      | 356469/900675 [00:58<01:29, 6069.68it/s] 40%|███▉      | 357083/900675 [00:58<01:35, 5685.01it/s] 40%|███▉      | 357693/900675 [00:59<01:33, 5795.24it/s] 40%|███▉      | 358280/900675 [00:59<01:33, 5812.89it/s] 40%|███▉      | 358944/900675 [00:59<01:29, 6047.76it/s] 40%|███▉      | 359554/900675 [00:59<01:31, 5895.40it/s] 40%|███▉      | 360148/900675 [00:59<01:31, 5905.96it/s] 40%|████      | 360845/900675 [00:59<01:26, 6213.19it/s] 40%|████      | 361469/900675 [00:59<01:26, 6211.85it/s] 40%|████      | 362250/900675 [00:59<01:20, 6679.20it/s] 40%|████      | 362920/900675 [00:59<01:22, 6537.12it/s] 40%|████      | 363576/900675 [00:59<01:22, 6501.75it/s] 40%|████      | 364228/900675 [01:00<01:22, 6465.16it/s] 41%|████      | 364908/900675 [01:00<01:21, 6561.41it/s] 41%|████      | 365573/900675 [01:00<01:21, 6584.35it/s] 41%|████      | 366233/900675 [01:00<01:21, 6555.37it/s] 41%|████      | 366889/900675 [01:00<01:27, 6129.00it/s] 41%|████      | 367537/900675 [01:00<01:25, 6221.70it/s] 41%|████      | 368164/900675 [01:00<01:32, 5739.57it/s] 41%|████      | 368845/900675 [01:00<01:28, 6022.49it/s] 41%|████      | 369482/900675 [01:00<01:26, 6114.93it/s] 41%|████      | 370112/900675 [01:01<01:26, 6162.43it/s] 41%|████      | 370734/900675 [01:01<01:29, 5928.31it/s] 41%|████      | 371349/900675 [01:01<01:28, 5990.99it/s] 41%|████▏     | 371952/900675 [01:01<01:30, 5833.86it/s] 41%|████▏     | 372595/900675 [01:01<01:27, 6002.64it/s] 41%|████▏     | 373207/900675 [01:01<01:27, 6034.39it/s] 42%|████▏     | 373813/900675 [01:01<01:29, 5910.43it/s] 42%|████▏     | 374507/900675 [01:01<01:24, 6204.17it/s] 42%|████▏     | 375130/900675 [01:01<01:24, 6200.33it/s] 42%|████▏     | 375752/900675 [01:01<01:30, 5775.74it/s] 42%|████▏     | 376364/900675 [01:02<01:29, 5871.13it/s] 42%|████▏     | 376957/900675 [01:02<01:33, 5572.83it/s] 42%|████▏     | 377521/900675 [01:02<01:33, 5570.60it/s] 42%|████▏     | 378235/900675 [01:02<01:26, 6014.48it/s] 42%|████▏     | 378842/900675 [01:02<01:27, 5988.93it/s] 42%|████▏     | 379520/900675 [01:02<01:23, 6215.98it/s] 42%|████▏     | 380229/900675 [01:02<01:20, 6469.23it/s] 42%|████▏     | 380897/900675 [01:02<01:19, 6526.08it/s] 42%|████▏     | 381552/900675 [01:02<01:21, 6342.94it/s] 42%|████▏     | 382189/900675 [01:03<01:26, 5964.23it/s] 43%|████▎     | 382792/900675 [01:03<01:28, 5825.79it/s] 43%|████▎     | 383407/900675 [01:03<01:27, 5909.77it/s] 43%|████▎     | 384006/900675 [01:03<01:27, 5927.34it/s] 43%|████▎     | 384718/900675 [01:03<01:22, 6268.72it/s] 43%|████▎     | 385349/900675 [01:03<01:22, 6277.95it/s] 43%|████▎     | 385979/900675 [01:03<01:26, 5952.45it/s] 43%|████▎     | 386579/900675 [01:03<01:26, 5915.12it/s] 43%|████▎     | 387347/900675 [01:03<01:19, 6419.48it/s] 43%|████▎     | 388014/900675 [01:03<01:18, 6491.67it/s] 43%|████▎     | 388667/900675 [01:04<01:22, 6204.90it/s] 43%|████▎     | 389318/900675 [01:04<01:21, 6290.81it/s] 43%|████▎     | 389951/900675 [01:04<01:21, 6279.13it/s] 43%|████▎     | 390582/900675 [01:04<01:22, 6158.95it/s] 43%|████▎     | 391200/900675 [01:04<01:23, 6137.00it/s] 44%|████▎     | 391816/900675 [01:04<01:27, 5827.03it/s] 44%|████▎     | 392406/900675 [01:04<01:26, 5844.05it/s] 44%|████▎     | 392994/900675 [01:04<01:28, 5766.45it/s] 44%|████▎     | 393573/900675 [01:04<01:30, 5587.62it/s] 44%|████▍     | 394274/900675 [01:05<01:24, 5991.75it/s] 44%|████▍     | 394877/900675 [01:05<01:26, 5864.45it/s] 44%|████▍     | 395514/900675 [01:05<01:24, 6007.92it/s] 44%|████▍     | 396132/900675 [01:05<01:23, 6052.04it/s] 44%|████▍     | 396740/900675 [01:05<01:25, 5879.03it/s] 44%|████▍     | 397330/900675 [01:05<01:29, 5630.89it/s] 44%|████▍     | 397968/900675 [01:05<01:26, 5832.46it/s] 44%|████▍     | 398700/900675 [01:05<01:20, 6256.92it/s] 44%|████▍     | 399330/900675 [01:05<01:20, 6222.25it/s] 44%|████▍     | 400049/900675 [01:05<01:16, 6504.76it/s] 44%|████▍     | 400703/900675 [01:06<01:18, 6403.30it/s] 45%|████▍     | 401346/900675 [01:06<01:19, 6244.18it/s] 45%|████▍     | 401973/900675 [01:06<01:26, 5755.01it/s] 45%|████▍     | 402557/900675 [01:06<01:30, 5492.47it/s] 45%|████▍     | 403174/900675 [01:06<01:27, 5672.91it/s] 45%|████▍     | 403790/900675 [01:06<01:25, 5802.44it/s] 45%|████▍     | 404376/900675 [01:06<01:26, 5707.35it/s] 45%|████▍     | 405025/900675 [01:06<01:23, 5925.90it/s] 45%|████▌     | 405622/900675 [01:06<01:25, 5767.87it/s] 45%|████▌     | 406215/900675 [01:07<01:25, 5811.11it/s] 45%|████▌     | 406799/900675 [01:07<01:26, 5695.03it/s] 45%|████▌     | 407439/900675 [01:07<01:23, 5896.61it/s] 45%|████▌     | 408031/900675 [01:07<01:28, 5585.40it/s] 45%|████▌     | 408723/900675 [01:07<01:22, 5962.07it/s] 45%|████▌     | 409351/900675 [01:07<01:21, 6043.63it/s] 46%|████▌     | 409960/900675 [01:07<01:24, 5787.99it/s] 46%|████▌     | 410617/900675 [01:07<01:21, 6010.20it/s] 46%|████▌     | 411223/900675 [01:07<01:23, 5887.16it/s] 46%|████▌     | 411816/900675 [01:08<01:22, 5896.58it/s] 46%|████▌     | 412409/900675 [01:08<01:29, 5425.47it/s] 46%|████▌     | 412960/900675 [01:08<01:30, 5402.64it/s] 46%|████▌     | 413580/900675 [01:08<01:26, 5618.09it/s] 46%|████▌     | 414148/900675 [01:08<01:29, 5421.92it/s] 46%|████▌     | 414695/900675 [01:08<01:31, 5303.90it/s] 46%|████▌     | 415316/900675 [01:08<01:27, 5557.44it/s] 46%|████▌     | 415888/900675 [01:08<01:26, 5603.21it/s] 46%|████▌     | 416452/900675 [01:08<01:26, 5570.20it/s] 46%|████▋     | 417025/900675 [01:08<01:26, 5616.64it/s] 46%|████▋     | 417634/900675 [01:09<01:23, 5755.36it/s] 46%|████▋     | 418282/900675 [01:09<01:20, 5966.57it/s] 47%|████▋     | 418880/900675 [01:09<01:21, 5945.21it/s] 47%|████▋     | 419476/900675 [01:09<01:20, 5941.75it/s] 47%|████▋     | 420091/900675 [01:09<01:20, 6002.38it/s] 47%|████▋     | 420699/900675 [01:09<01:19, 6022.06it/s] 47%|████▋     | 421302/900675 [01:09<01:25, 5629.81it/s] 47%|████▋     | 421892/900675 [01:09<01:23, 5706.70it/s] 47%|████▋     | 422539/900675 [01:09<01:20, 5922.57it/s] 47%|████▋     | 423135/900675 [01:10<01:23, 5691.08it/s] 47%|████▋     | 423713/900675 [01:10<01:23, 5712.97it/s] 47%|████▋     | 424302/900675 [01:10<01:22, 5759.48it/s] 47%|████▋     | 424881/900675 [01:10<01:23, 5691.64it/s] 47%|████▋     | 425452/900675 [01:10<01:23, 5657.77it/s] 47%|████▋     | 426110/900675 [01:10<01:20, 5925.55it/s] 47%|████▋     | 426845/900675 [01:10<01:14, 6345.45it/s] 47%|████▋     | 427482/900675 [01:10<01:16, 6197.23it/s] 48%|████▊     | 428104/900675 [01:10<01:18, 6028.58it/s] 48%|████▊     | 428836/900675 [01:10<01:13, 6391.51it/s] 48%|████▊     | 429478/900675 [01:11<01:14, 6360.34it/s] 48%|████▊     | 430116/900675 [01:11<01:14, 6354.80it/s] 48%|████▊     | 430753/900675 [01:11<01:15, 6185.16it/s] 48%|████▊     | 431374/900675 [01:11<01:20, 5839.68it/s] 48%|████▊     | 431963/900675 [01:11<01:23, 5622.87it/s] 48%|████▊     | 432876/900675 [01:11<01:10, 6598.17it/s] 48%|████▊     | 433546/900675 [01:11<01:14, 6293.74it/s] 48%|████▊     | 434185/900675 [01:11<01:17, 6033.89it/s] 48%|████▊     | 434796/900675 [01:11<01:19, 5868.58it/s] 48%|████▊     | 435388/900675 [01:12<01:19, 5862.00it/s] 48%|████▊     | 435992/900675 [01:12<01:18, 5904.41it/s] 48%|████▊     | 436591/900675 [01:12<01:18, 5928.22it/s] 49%|████▊     | 437276/900675 [01:12<01:14, 6184.77it/s] 49%|████▊     | 437897/900675 [01:12<01:16, 6046.37it/s] 49%|████▊     | 438504/900675 [01:12<01:17, 5951.15it/s] 49%|████▉     | 439101/900675 [01:12<01:18, 5845.61it/s] 49%|████▉     | 439687/900675 [01:12<01:21, 5675.72it/s] 49%|████▉     | 440256/900675 [01:12<01:22, 5608.67it/s] 49%|████▉     | 440876/900675 [01:12<01:19, 5772.69it/s] 49%|████▉     | 441509/900675 [01:13<01:17, 5931.69it/s] 49%|████▉     | 442111/900675 [01:13<01:17, 5952.16it/s] 49%|████▉     | 442708/900675 [01:13<01:17, 5921.80it/s] 49%|████▉     | 443356/900675 [01:13<01:15, 6081.41it/s] 49%|████▉     | 444052/900675 [01:13<01:12, 6339.78it/s] 49%|████▉     | 444687/900675 [01:13<01:18, 5792.54it/s] 49%|████▉     | 445276/900675 [01:13<01:21, 5585.31it/s] 50%|████▉     | 445873/900675 [01:13<01:19, 5691.61it/s] 50%|████▉     | 446449/900675 [01:13<01:20, 5659.25it/s] 50%|████▉     | 447029/900675 [01:14<01:19, 5698.06it/s] 50%|████▉     | 447731/900675 [01:14<01:14, 6076.75it/s] 50%|████▉     | 448370/900675 [01:14<01:13, 6162.82it/s] 50%|████▉     | 448989/900675 [01:14<01:13, 6116.47it/s] 50%|████▉     | 449603/900675 [01:14<01:17, 5844.11it/s] 50%|████▉     | 450239/900675 [01:14<01:15, 5990.38it/s] 50%|█████     | 450897/900675 [01:14<01:13, 6160.20it/s] 50%|█████     | 451516/900675 [01:14<01:12, 6154.57it/s] 50%|█████     | 452134/900675 [01:14<01:17, 5800.99it/s] 50%|█████     | 452720/900675 [01:14<01:17, 5773.07it/s] 50%|█████     | 453374/900675 [01:15<01:14, 5992.94it/s] 50%|█████     | 453977/900675 [01:15<01:16, 5821.88it/s] 50%|█████     | 454563/900675 [01:15<01:18, 5704.34it/s] 51%|█████     | 455136/900675 [01:15<01:18, 5709.19it/s] 51%|█████     | 455712/900675 [01:15<01:17, 5723.69it/s] 51%|█████     | 456387/900675 [01:15<01:13, 6024.06it/s] 51%|█████     | 457092/900675 [01:15<01:10, 6325.23it/s] 51%|█████     | 457727/900675 [01:15<01:11, 6226.29it/s] 51%|█████     | 458351/900675 [01:15<01:14, 5912.09it/s] 51%|█████     | 458971/900675 [01:16<01:13, 5992.70it/s] 51%|█████     | 459692/900675 [01:16<01:09, 6344.78it/s] 51%|█████     | 460330/900675 [01:16<01:14, 5931.28it/s] 51%|█████     | 460938/900675 [01:16<01:13, 5970.35it/s] 51%|█████     | 461554/900675 [01:16<01:12, 6024.02it/s] 51%|█████▏    | 462161/900675 [01:16<01:15, 5825.80it/s] 51%|█████▏    | 462813/900675 [01:16<01:12, 6018.93it/s] 51%|█████▏    | 463421/900675 [01:16<01:12, 6031.64it/s] 52%|█████▏    | 464027/900675 [01:16<01:15, 5811.38it/s] 52%|█████▏    | 464647/900675 [01:16<01:13, 5918.34it/s] 52%|█████▏    | 465277/900675 [01:17<01:12, 6026.89it/s] 52%|█████▏    | 465903/900675 [01:17<01:11, 6092.83it/s] 52%|█████▏    | 466514/900675 [01:17<01:16, 5670.81it/s] 52%|█████▏    | 467096/900675 [01:17<01:16, 5703.83it/s] 52%|█████▏    | 467733/900675 [01:17<01:13, 5893.16it/s] 52%|█████▏    | 468327/900675 [01:17<01:13, 5902.92it/s] 52%|█████▏    | 468957/900675 [01:17<01:11, 6013.91it/s] 52%|█████▏    | 469561/900675 [01:17<01:13, 5851.53it/s] 52%|█████▏    | 470149/900675 [01:17<01:15, 5685.01it/s] 52%|█████▏    | 470866/900675 [01:18<01:10, 6111.06it/s] 52%|█████▏    | 471500/900675 [01:18<01:09, 6176.40it/s] 52%|█████▏    | 472121/900675 [01:18<01:13, 5820.44it/s] 52%|█████▏    | 472749/900675 [01:18<01:11, 5949.10it/s] 53%|█████▎    | 473389/900675 [01:18<01:10, 6078.76it/s] 53%|█████▎    | 474001/900675 [01:18<01:12, 5865.30it/s] 53%|█████▎    | 474596/900675 [01:18<01:12, 5887.48it/s] 53%|█████▎    | 475353/900675 [01:18<01:06, 6369.95it/s] 53%|█████▎    | 476073/900675 [01:18<01:04, 6612.84it/s] 53%|█████▎    | 476738/900675 [01:18<01:09, 6097.30it/s] 53%|█████▎    | 477358/900675 [01:19<01:10, 6027.50it/s] 53%|█████▎    | 477968/900675 [01:19<01:10, 5955.60it/s] 53%|█████▎    | 478689/900675 [01:19<01:06, 6311.24it/s] 53%|█████▎    | 479364/900675 [01:19<01:05, 6431.23it/s] 53%|█████▎    | 480012/900675 [01:19<01:08, 6141.47it/s] 53%|█████▎    | 480658/900675 [01:19<01:07, 6224.94it/s] 53%|█████▎    | 481285/900675 [01:19<01:09, 6068.70it/s] 54%|█████▎    | 481896/900675 [01:19<01:10, 5934.62it/s] 54%|█████▎    | 482492/900675 [01:19<01:12, 5735.08it/s] 54%|█████▎    | 483170/900675 [01:20<01:09, 6026.71it/s] 54%|█████▎    | 483777/900675 [01:20<01:10, 5906.75it/s] 54%|█████▍    | 484373/900675 [01:20<01:10, 5913.98it/s] 54%|█████▍    | 485150/900675 [01:20<01:04, 6447.62it/s] 54%|█████▍    | 485798/900675 [01:20<01:04, 6455.68it/s] 54%|█████▍    | 486446/900675 [01:20<01:07, 6154.24it/s] 54%|█████▍    | 487066/900675 [01:20<01:10, 5858.84it/s] 54%|█████▍    | 487754/900675 [01:20<01:07, 6142.65it/s] 54%|█████▍    | 488383/900675 [01:20<01:06, 6181.86it/s] 54%|█████▍    | 489006/900675 [01:21<01:08, 6032.62it/s] 54%|█████▍    | 489624/900675 [01:21<01:07, 6071.67it/s] 54%|█████▍    | 490234/900675 [01:21<01:10, 5824.24it/s] 54%|█████▍    | 490820/900675 [01:21<01:11, 5770.34it/s] 55%|█████▍    | 491414/900675 [01:21<01:10, 5812.92it/s] 55%|█████▍    | 492053/900675 [01:21<01:08, 5980.46it/s] 55%|█████▍    | 492689/900675 [01:21<01:07, 6087.20it/s] 55%|█████▍    | 493300/900675 [01:21<01:08, 5918.72it/s] 55%|█████▍    | 493920/900675 [01:21<01:07, 5990.77it/s] 55%|█████▍    | 494631/900675 [01:21<01:04, 6315.95it/s] 55%|█████▍    | 495265/900675 [01:22<01:04, 6281.53it/s] 55%|█████▌    | 495895/900675 [01:22<01:04, 6246.06it/s] 55%|█████▌    | 496521/900675 [01:22<01:06, 6073.33it/s] 55%|█████▌    | 497156/900675 [01:22<01:05, 6152.87it/s] 55%|█████▌    | 497788/900675 [01:22<01:04, 6201.03it/s] 55%|█████▌    | 498410/900675 [01:22<01:08, 5912.35it/s] 55%|█████▌    | 499116/900675 [01:22<01:04, 6240.05it/s] 55%|█████▌    | 499757/900675 [01:22<01:03, 6283.64it/s] 56%|█████▌    | 500448/900675 [01:22<01:01, 6464.83it/s] 56%|█████▌    | 501097/900675 [01:22<01:02, 6363.46it/s] 56%|█████▌    | 501736/900675 [01:23<01:06, 6005.75it/s] 56%|█████▌    | 502342/900675 [01:23<01:07, 5930.41it/s] 56%|█████▌    | 502946/900675 [01:23<01:06, 5954.67it/s] 56%|█████▌    | 503563/900675 [01:23<01:06, 6012.24it/s] 56%|█████▌    | 504167/900675 [01:23<01:09, 5699.73it/s] 56%|█████▌    | 504762/900675 [01:23<01:08, 5768.12it/s] 56%|█████▌    | 505438/900675 [01:23<01:05, 6038.37it/s] 56%|█████▌    | 506060/900675 [01:23<01:04, 6085.84it/s] 56%|█████▋    | 506732/900675 [01:23<01:02, 6269.43it/s] 56%|█████▋    | 507362/900675 [01:24<01:04, 6125.08it/s] 56%|█████▋    | 507977/900675 [01:24<01:08, 5722.67it/s] 56%|█████▋    | 508556/900675 [01:24<01:09, 5668.72it/s] 57%|█████▋    | 509243/900675 [01:24<01:05, 6008.28it/s] 57%|█████▋    | 509919/900675 [01:24<01:02, 6217.74it/s] 57%|█████▋    | 510545/900675 [01:24<01:08, 5693.56it/s] 57%|█████▋    | 511213/900675 [01:24<01:05, 5965.14it/s] 57%|█████▋    | 511868/900675 [01:24<01:03, 6127.89it/s] 57%|█████▋    | 512489/900675 [01:24<01:05, 5968.77it/s] 57%|█████▋    | 513212/900675 [01:24<01:01, 6322.84it/s] 57%|█████▋    | 513851/900675 [01:25<01:02, 6215.81it/s] 57%|█████▋    | 514477/900675 [01:25<01:03, 6082.71it/s] 57%|█████▋    | 515089/900675 [01:25<01:03, 6026.13it/s] 57%|█████▋    | 515694/900675 [01:25<01:04, 5997.30it/s] 57%|█████▋    | 516336/900675 [01:25<01:02, 6115.55it/s] 57%|█████▋    | 516949/900675 [01:25<01:06, 5773.52it/s] 57%|█████▋    | 517564/900675 [01:25<01:05, 5878.95it/s] 58%|█████▊    | 518258/900675 [01:25<01:01, 6182.04it/s] 58%|█████▊    | 518885/900675 [01:25<01:01, 6205.16it/s] 58%|█████▊    | 519509/900675 [01:26<01:02, 6135.70it/s] 58%|█████▊    | 520125/900675 [01:26<01:02, 6052.82it/s] 58%|█████▊    | 520732/900675 [01:26<01:02, 6042.82it/s] 58%|█████▊    | 521347/900675 [01:26<01:02, 6073.54it/s] 58%|█████▊    | 522046/900675 [01:26<00:59, 6339.43it/s] 58%|█████▊    | 522681/900675 [01:26<01:03, 5981.19it/s] 58%|█████▊    | 523284/900675 [01:26<01:03, 5936.21it/s] 58%|█████▊    | 523881/900675 [01:26<01:03, 5895.18it/s] 58%|█████▊    | 524520/900675 [01:26<01:02, 6037.11it/s] 58%|█████▊    | 525126/900675 [01:26<01:03, 5942.75it/s] 58%|█████▊    | 525722/900675 [01:27<01:04, 5787.44it/s] 58%|█████▊    | 526361/900675 [01:27<01:02, 5956.52it/s] 59%|█████▊    | 527127/900675 [01:27<00:57, 6448.92it/s] 59%|█████▊    | 527775/900675 [01:27<01:03, 5876.13it/s] 59%|█████▊    | 528457/900675 [01:27<01:00, 6133.00it/s] 59%|█████▊    | 529107/900675 [01:27<00:59, 6232.10it/s] 59%|█████▉    | 529738/900675 [01:27<01:00, 6112.85it/s] 59%|█████▉    | 530355/900675 [01:27<01:01, 5984.97it/s] 59%|█████▉    | 530958/900675 [01:27<01:03, 5798.95it/s] 59%|█████▉    | 531563/900675 [01:28<01:02, 5862.25it/s] 59%|█████▉    | 532186/900675 [01:28<01:01, 5964.32it/s] 59%|█████▉    | 532842/900675 [01:28<00:59, 6135.58it/s] 59%|█████▉    | 533466/900675 [01:28<00:59, 6161.52it/s] 59%|█████▉    | 534084/900675 [01:28<01:02, 5838.23it/s] 59%|█████▉    | 534673/900675 [01:28<01:03, 5795.79it/s] 59%|█████▉    | 535285/900675 [01:28<01:02, 5883.32it/s] 59%|█████▉    | 535876/900675 [01:28<01:03, 5722.62it/s] 60%|█████▉    | 536508/900675 [01:28<01:01, 5890.89it/s] 60%|█████▉    | 537136/900675 [01:28<01:00, 5995.37it/s] 60%|█████▉    | 537738/900675 [01:29<01:02, 5852.12it/s] 60%|█████▉    | 538347/900675 [01:29<01:01, 5913.92it/s] 60%|█████▉    | 539055/900675 [01:29<00:57, 6254.30it/s] 60%|█████▉    | 539683/900675 [01:29<01:00, 5934.85it/s] 60%|█████▉    | 540295/900675 [01:29<01:00, 5984.08it/s] 60%|██████    | 540907/900675 [01:29<00:59, 6018.72it/s] 60%|██████    | 541512/900675 [01:29<01:01, 5823.44it/s] 60%|██████    | 542112/900675 [01:29<01:01, 5871.43it/s] 60%|██████    | 542791/900675 [01:29<00:58, 6138.47it/s] 60%|██████    | 543449/900675 [01:30<00:57, 6260.94it/s] 60%|██████    | 544077/900675 [01:30<00:58, 6145.60it/s] 60%|██████    | 544694/900675 [01:30<01:00, 5839.25it/s] 61%|██████    | 545339/900675 [01:30<00:59, 6005.13it/s] 61%|██████    | 545944/900675 [01:30<00:59, 5922.58it/s] 61%|██████    | 546644/900675 [01:30<00:56, 6226.46it/s] 61%|██████    | 547270/900675 [01:30<01:00, 5881.46it/s] 61%|██████    | 547913/900675 [01:30<00:58, 6033.32it/s] 61%|██████    | 548673/900675 [01:30<00:54, 6483.01it/s] 61%|██████    | 549327/900675 [01:31<00:57, 6091.26it/s] 61%|██████    | 549960/900675 [01:31<00:57, 6151.55it/s] 61%|██████    | 550582/900675 [01:31<00:57, 6075.28it/s] 61%|██████    | 551210/900675 [01:31<00:57, 6124.75it/s] 61%|██████▏   | 551839/900675 [01:31<00:56, 6170.37it/s] 61%|██████▏   | 552459/900675 [01:31<00:58, 5907.67it/s] 61%|██████▏   | 553054/900675 [01:31<00:59, 5882.39it/s] 61%|██████▏   | 553645/900675 [01:31<01:02, 5563.19it/s] 62%|██████▏   | 554252/900675 [01:31<01:00, 5704.16it/s] 62%|██████▏   | 554827/900675 [01:31<01:01, 5618.47it/s] 62%|██████▏   | 555423/900675 [01:32<01:00, 5714.44it/s] 62%|██████▏   | 556101/900675 [01:32<00:57, 6022.66it/s] 62%|██████▏   | 556743/900675 [01:32<00:56, 6138.69it/s] 62%|██████▏   | 557360/900675 [01:32<00:57, 5926.22it/s] 62%|██████▏   | 557981/900675 [01:32<00:57, 6007.86it/s] 62%|██████▏   | 558585/900675 [01:32<00:58, 5872.31it/s] 62%|██████▏   | 559175/900675 [01:32<01:01, 5585.18it/s] 62%|██████▏   | 559782/900675 [01:32<00:59, 5718.48it/s] 62%|██████▏   | 560365/900675 [01:32<00:59, 5745.20it/s] 62%|██████▏   | 560968/900675 [01:32<00:58, 5826.45it/s] 62%|██████▏   | 561642/900675 [01:33<00:55, 6091.00it/s] 62%|██████▏   | 562270/900675 [01:33<00:55, 6137.75it/s] 63%|██████▎   | 562924/900675 [01:33<00:54, 6252.50it/s] 63%|██████▎   | 563551/900675 [01:33<00:56, 5956.13it/s] 63%|██████▎   | 564178/900675 [01:33<00:55, 6046.17it/s] 63%|██████▎   | 564786/900675 [01:33<00:59, 5637.57it/s] 63%|██████▎   | 565473/900675 [01:33<00:56, 5981.39it/s] 63%|██████▎   | 566094/900675 [01:33<00:55, 6041.82it/s] 63%|██████▎   | 566704/900675 [01:33<00:58, 5670.71it/s] 63%|██████▎   | 567307/900675 [01:34<00:57, 5763.86it/s] 63%|██████▎   | 568020/900675 [01:34<00:54, 6147.08it/s] 63%|██████▎   | 568655/900675 [01:34<00:53, 6196.18it/s] 63%|██████▎   | 569292/900675 [01:34<00:53, 6244.71it/s] 63%|██████▎   | 569920/900675 [01:34<00:53, 6173.50it/s] 63%|██████▎   | 570588/900675 [01:34<00:52, 6311.61it/s] 63%|██████▎   | 571400/900675 [01:34<00:48, 6839.04it/s] 64%|██████▎   | 572087/900675 [01:34<00:50, 6562.75it/s] 64%|██████▎   | 572747/900675 [01:34<00:52, 6228.08it/s] 64%|██████▎   | 573376/900675 [01:35<00:53, 6148.64it/s] 64%|██████▎   | 574012/900675 [01:35<00:52, 6203.91it/s] 64%|██████▍   | 574636/900675 [01:35<00:55, 5901.93it/s] 64%|██████▍   | 575231/900675 [01:35<00:55, 5851.44it/s] 64%|██████▍   | 575827/900675 [01:35<00:55, 5880.85it/s] 64%|██████▍   | 576418/900675 [01:35<00:55, 5841.96it/s] 64%|██████▍   | 577073/900675 [01:35<00:53, 6047.26it/s] 64%|██████▍   | 577680/900675 [01:35<00:53, 6029.10it/s] 64%|██████▍   | 578285/900675 [01:35<00:57, 5617.38it/s] 64%|██████▍   | 578893/900675 [01:35<00:55, 5747.32it/s] 64%|██████▍   | 579552/900675 [01:36<00:53, 5978.62it/s] 64%|██████▍   | 580204/900675 [01:36<00:52, 6132.56it/s] 64%|██████▍   | 580835/900675 [01:36<00:51, 6184.27it/s] 65%|██████▍   | 581457/900675 [01:36<00:52, 6111.76it/s] 65%|██████▍   | 582096/900675 [01:36<00:51, 6187.17it/s] 65%|██████▍   | 582731/900675 [01:36<00:51, 6232.15it/s] 65%|██████▍   | 583406/900675 [01:36<00:49, 6378.29it/s] 65%|██████▍   | 584203/900675 [01:36<00:46, 6847.89it/s] 65%|██████▍   | 584889/900675 [01:36<00:46, 6810.35it/s] 65%|██████▌   | 585571/900675 [01:36<00:47, 6609.72it/s] 65%|██████▌   | 586234/900675 [01:37<00:49, 6351.56it/s] 65%|██████▌   | 586872/900675 [01:37<00:50, 6216.96it/s] 65%|██████▌   | 587496/900675 [01:37<00:55, 5659.76it/s] 65%|██████▌   | 588125/900675 [01:37<00:53, 5829.72it/s] 65%|██████▌   | 588718/900675 [01:37<00:53, 5855.41it/s] 65%|██████▌   | 589358/900675 [01:37<00:51, 6006.01it/s] 66%|██████▌   | 590000/900675 [01:37<00:50, 6125.44it/s] 66%|██████▌   | 590644/900675 [01:37<00:49, 6211.16it/s] 66%|██████▌   | 591269/900675 [01:37<00:51, 6054.35it/s] 66%|██████▌   | 591929/900675 [01:38<00:49, 6201.63it/s] 66%|██████▌   | 592552/900675 [01:38<00:52, 5887.55it/s] 66%|██████▌   | 593146/900675 [01:38<00:52, 5805.09it/s] 66%|██████▌   | 593731/900675 [01:38<00:52, 5814.52it/s] 66%|██████▌   | 594370/900675 [01:38<00:51, 5980.20it/s] 66%|██████▌   | 595034/900675 [01:38<00:49, 6167.52it/s] 66%|██████▌   | 595653/900675 [01:38<00:52, 5793.48it/s] 66%|██████▌   | 596325/900675 [01:38<00:50, 6051.35it/s] 66%|██████▋   | 596951/900675 [01:38<00:49, 6111.16it/s] 66%|██████▋   | 597567/900675 [01:39<00:53, 5632.67it/s] 66%|██████▋   | 598159/900675 [01:39<00:52, 5710.84it/s] 66%|██████▋   | 598746/900675 [01:39<00:52, 5754.49it/s] 67%|██████▋   | 599369/900675 [01:39<00:51, 5884.43it/s] 67%|██████▋   | 600034/900675 [01:39<00:49, 6107.14it/s] 67%|██████▋   | 600649/900675 [01:39<00:49, 6067.54it/s] 67%|██████▋   | 601259/900675 [01:39<00:52, 5735.35it/s] 67%|██████▋   | 601838/900675 [01:39<00:53, 5567.75it/s] 67%|██████▋   | 602429/900675 [01:39<00:52, 5661.49it/s] 67%|██████▋   | 603023/900675 [01:39<00:51, 5736.30it/s] 67%|██████▋   | 603719/900675 [01:40<00:48, 6087.97it/s] 67%|██████▋   | 604331/900675 [01:40<00:49, 5974.06it/s] 67%|██████▋   | 604955/900675 [01:40<00:48, 6049.52it/s] 67%|██████▋   | 605562/900675 [01:40<00:51, 5786.00it/s] 67%|██████▋   | 606210/900675 [01:40<00:49, 5983.92it/s] 67%|██████▋   | 606834/900675 [01:40<00:48, 6051.39it/s] 67%|██████▋   | 607446/900675 [01:40<00:48, 6071.36it/s] 68%|██████▊   | 608055/900675 [01:40<00:48, 6070.36it/s] 68%|██████▊   | 608664/900675 [01:40<00:50, 5788.12it/s] 68%|██████▊   | 609247/900675 [01:41<00:51, 5658.40it/s] 68%|██████▊   | 609960/900675 [01:41<00:47, 6079.01it/s] 68%|██████▊   | 610658/900675 [01:41<00:45, 6330.16it/s] 68%|██████▊   | 611295/900675 [01:41<00:47, 6108.36it/s] 68%|██████▊   | 611934/900675 [01:41<00:46, 6188.87it/s] 68%|██████▊   | 612556/900675 [01:41<00:47, 6079.99it/s] 68%|██████▊   | 613167/900675 [01:41<00:47, 6078.44it/s] 68%|██████▊   | 613777/900675 [01:41<00:48, 5856.67it/s] 68%|██████▊   | 614366/900675 [01:41<00:50, 5702.45it/s] 68%|██████▊   | 614939/900675 [01:41<00:50, 5664.13it/s] 68%|██████▊   | 615598/900675 [01:42<00:48, 5926.49it/s] 68%|██████▊   | 616193/900675 [01:42<00:50, 5662.37it/s] 69%|██████▊   | 617042/900675 [01:42<00:43, 6462.63it/s] 69%|██████▊   | 617706/900675 [01:42<00:43, 6512.94it/s] 69%|██████▊   | 618363/900675 [01:42<00:43, 6475.06it/s] 69%|██████▊   | 619055/900675 [01:42<00:42, 6604.87it/s] 69%|██████▉   | 619799/900675 [01:42<00:41, 6850.44it/s] 69%|██████▉   | 620562/900675 [01:42<00:39, 7073.84it/s] 69%|██████▉   | 621272/900675 [01:42<00:43, 6492.24it/s] 69%|██████▉   | 621932/900675 [01:43<00:43, 6432.11it/s] 69%|██████▉   | 622618/900675 [01:43<00:42, 6546.55it/s] 69%|██████▉   | 623302/900675 [01:43<00:41, 6629.60it/s] 69%|██████▉   | 623970/900675 [01:43<00:44, 6236.17it/s] 69%|██████▉   | 624601/900675 [01:43<00:45, 6023.45it/s] 69%|██████▉   | 625451/900675 [01:43<00:41, 6708.68it/s] 70%|██████▉   | 626131/900675 [01:43<00:42, 6460.03it/s] 70%|██████▉   | 626785/900675 [01:43<00:44, 6158.34it/s] 70%|██████▉   | 627444/900675 [01:43<00:43, 6275.52it/s] 70%|██████▉   | 628078/900675 [01:43<00:44, 6140.24it/s] 70%|██████▉   | 628697/900675 [01:44<00:44, 6133.58it/s] 70%|██████▉   | 629379/900675 [01:44<00:42, 6325.84it/s] 70%|██████▉   | 630015/900675 [01:44<00:45, 5996.52it/s] 70%|███████   | 630648/900675 [01:44<00:44, 6085.22it/s] 70%|███████   | 631377/900675 [01:44<00:41, 6430.73it/s] 70%|███████   | 632072/900675 [01:44<00:40, 6572.57it/s] 70%|███████   | 632733/900675 [01:44<00:41, 6386.10it/s] 70%|███████   | 633511/900675 [01:44<00:39, 6787.79it/s] 70%|███████   | 634194/900675 [01:44<00:41, 6449.38it/s] 70%|███████   | 634845/900675 [01:45<00:42, 6293.53it/s] 71%|███████   | 635479/900675 [01:45<00:42, 6284.28it/s] 71%|███████   | 636165/900675 [01:45<00:41, 6441.55it/s] 71%|███████   | 636812/900675 [01:45<00:44, 5987.11it/s] 71%|███████   | 637419/900675 [01:45<00:43, 6005.04it/s] 71%|███████   | 638137/900675 [01:45<00:41, 6334.13it/s] 71%|███████   | 638776/900675 [01:45<00:42, 6214.59it/s] 71%|███████   | 639402/900675 [01:45<00:42, 6138.44it/s] 71%|███████   | 640019/900675 [01:45<00:43, 5960.92it/s] 71%|███████   | 640744/900675 [01:46<00:41, 6323.51it/s] 71%|███████   | 641496/900675 [01:46<00:38, 6669.69it/s] 71%|███████▏  | 642167/900675 [01:46<00:42, 6118.28it/s] 71%|███████▏  | 642790/900675 [01:46<00:42, 6132.63it/s] 71%|███████▏  | 643411/900675 [01:46<00:44, 5725.41it/s] 72%|███████▏  | 644013/900675 [01:46<00:44, 5803.05it/s] 72%|███████▏  | 644601/900675 [01:46<00:44, 5744.93it/s] 72%|███████▏  | 645181/900675 [01:46<00:44, 5706.51it/s] 72%|███████▏  | 645940/900675 [01:46<00:40, 6242.60it/s] 72%|███████▏  | 646570/900675 [01:46<00:42, 5927.65it/s] 72%|███████▏  | 647169/900675 [01:47<00:43, 5844.28it/s] 72%|███████▏  | 647758/900675 [01:47<00:43, 5808.70it/s] 72%|███████▏  | 648342/900675 [01:47<00:47, 5346.87it/s] 72%|███████▏  | 648990/900675 [01:47<00:44, 5649.82it/s] 72%|███████▏  | 649564/900675 [01:47<00:46, 5432.31it/s] 72%|███████▏  | 650149/900675 [01:47<00:45, 5545.53it/s] 72%|███████▏  | 650710/900675 [01:47<00:44, 5559.98it/s] 72%|███████▏  | 651304/900675 [01:47<00:43, 5668.43it/s] 72%|███████▏  | 651972/900675 [01:47<00:41, 5957.79it/s] 72%|███████▏  | 652571/900675 [01:48<00:42, 5890.67it/s] 73%|███████▎  | 653287/900675 [01:48<00:39, 6258.25it/s] 73%|███████▎  | 653916/900675 [01:48<00:40, 6129.52it/s] 73%|███████▎  | 654592/900675 [01:48<00:38, 6313.29it/s] 73%|███████▎  | 655226/900675 [01:48<00:39, 6155.17it/s] 73%|███████▎  | 655844/900675 [01:48<00:39, 6126.89it/s] 73%|███████▎  | 656459/900675 [01:48<00:40, 5997.28it/s] 73%|███████▎  | 657064/900675 [01:48<00:40, 6010.73it/s] 73%|███████▎  | 657783/900675 [01:48<00:38, 6355.43it/s] 73%|███████▎  | 658421/900675 [01:48<00:39, 6158.56it/s] 73%|███████▎  | 659039/900675 [01:49<00:39, 6136.49it/s] 73%|███████▎  | 659662/900675 [01:49<00:39, 6163.47it/s] 73%|███████▎  | 660292/900675 [01:49<00:38, 6198.64it/s] 73%|███████▎  | 660913/900675 [01:49<00:39, 6028.32it/s] 73%|███████▎  | 661518/900675 [01:49<00:39, 5997.93it/s] 74%|███████▎  | 662205/900675 [01:49<00:38, 6238.25it/s] 74%|███████▎  | 662830/900675 [01:49<00:38, 6168.74it/s] 74%|███████▎  | 663448/900675 [01:49<00:40, 5896.09it/s] 74%|███████▎  | 664124/900675 [01:49<00:38, 6142.80it/s] 74%|███████▍  | 664742/900675 [01:50<00:39, 6030.68it/s] 74%|███████▍  | 665416/900675 [01:50<00:37, 6232.81it/s] 74%|███████▍  | 666086/900675 [01:50<00:36, 6367.45it/s] 74%|███████▍  | 666725/900675 [01:50<00:36, 6324.76it/s] 74%|███████▍  | 667359/900675 [01:50<00:39, 5969.67it/s] 74%|███████▍  | 668014/900675 [01:50<00:37, 6128.09it/s] 74%|███████▍  | 668670/900675 [01:50<00:37, 6251.46it/s] 74%|███████▍  | 669327/900675 [01:50<00:36, 6342.83it/s] 74%|███████▍  | 669964/900675 [01:50<00:36, 6335.98it/s] 74%|███████▍  | 670600/900675 [01:50<00:36, 6285.97it/s] 75%|███████▍  | 671251/900675 [01:51<00:36, 6350.98it/s] 75%|███████▍  | 671909/900675 [01:51<00:35, 6416.77it/s] 75%|███████▍  | 672552/900675 [01:51<00:36, 6278.94it/s] 75%|███████▍  | 673235/900675 [01:51<00:35, 6433.76it/s] 75%|███████▍  | 673880/900675 [01:51<00:36, 6207.12it/s] 75%|███████▍  | 674503/900675 [01:51<00:36, 6139.44it/s] 75%|███████▍  | 675154/900675 [01:51<00:36, 6239.31it/s] 75%|███████▌  | 675831/900675 [01:51<00:35, 6389.02it/s] 75%|███████▌  | 676472/900675 [01:51<00:37, 6048.21it/s] 75%|███████▌  | 677082/900675 [01:52<00:37, 5968.14it/s] 75%|███████▌  | 677703/900675 [01:52<00:36, 6035.72it/s] 75%|███████▌  | 678310/900675 [01:52<00:36, 6039.33it/s] 75%|███████▌  | 678916/900675 [01:52<00:36, 6042.99it/s] 75%|███████▌  | 679522/900675 [01:52<00:38, 5789.01it/s] 76%|███████▌  | 680277/900675 [01:52<00:35, 6288.57it/s] 76%|███████▌  | 680916/900675 [01:52<00:34, 6311.94it/s] 76%|███████▌  | 681551/900675 [01:52<00:35, 6151.26it/s] 76%|███████▌  | 682169/900675 [01:52<00:36, 5972.24it/s] 76%|███████▌  | 682769/900675 [01:52<00:36, 5891.25it/s] 76%|███████▌  | 683413/900675 [01:53<00:35, 6045.53it/s] 76%|███████▌  | 684123/900675 [01:53<00:34, 6352.09it/s] 76%|███████▌  | 684793/900675 [01:53<00:33, 6444.20it/s] 76%|███████▌  | 685440/900675 [01:53<00:34, 6172.58it/s] 76%|███████▌  | 686065/900675 [01:53<00:34, 6190.90it/s] 76%|███████▌  | 686687/900675 [01:53<00:37, 5757.88it/s] 76%|███████▋  | 687270/900675 [01:53<00:37, 5634.67it/s] 76%|███████▋  | 687880/900675 [01:53<00:36, 5757.55it/s] 76%|███████▋  | 688511/900675 [01:53<00:35, 5911.88it/s] 77%|███████▋  | 689106/900675 [01:54<00:35, 5911.86it/s] 77%|███████▋  | 689700/900675 [01:54<00:36, 5806.08it/s] 77%|███████▋  | 690283/900675 [01:54<00:36, 5720.75it/s] 77%|███████▋  | 690865/900675 [01:54<00:36, 5748.09it/s] 77%|███████▋  | 691441/900675 [01:54<00:38, 5400.63it/s] 77%|███████▋  | 692061/900675 [01:54<00:37, 5624.63it/s] 77%|███████▋  | 692629/900675 [01:54<00:38, 5434.00it/s] 77%|███████▋  | 693326/900675 [01:54<00:35, 5863.61it/s] 77%|███████▋  | 693918/900675 [01:54<00:36, 5730.90it/s] 77%|███████▋  | 694516/900675 [01:54<00:35, 5798.05it/s] 77%|███████▋  | 695099/900675 [01:55<00:35, 5730.90it/s] 77%|███████▋  | 695685/900675 [01:55<00:35, 5765.86it/s] 77%|███████▋  | 696264/900675 [01:55<00:36, 5669.02it/s] 77%|███████▋  | 696858/900675 [01:55<00:35, 5742.68it/s] 77%|███████▋  | 697482/900675 [01:55<00:34, 5887.44it/s] 78%|███████▊  | 698105/900675 [01:55<00:33, 5984.48it/s] 78%|███████▊  | 698705/900675 [01:55<00:34, 5841.45it/s] 78%|███████▊  | 699291/900675 [01:55<00:35, 5650.40it/s] 78%|███████▊  | 699930/900675 [01:55<00:34, 5859.21it/s] 78%|███████▊  | 700519/900675 [01:56<00:34, 5830.97it/s] 78%|███████▊  | 701176/900675 [01:56<00:32, 6046.85it/s] 78%|███████▊  | 701856/900675 [01:56<00:31, 6267.92it/s] 78%|███████▊  | 702485/900675 [01:56<00:31, 6216.25it/s] 78%|███████▊  | 703133/900675 [01:56<00:31, 6293.98it/s] 78%|███████▊  | 703764/900675 [01:56<00:31, 6216.64it/s] 78%|███████▊  | 704387/900675 [01:56<00:32, 6062.09it/s] 78%|███████▊  | 704995/900675 [01:56<00:33, 5843.43it/s] 78%|███████▊  | 705582/900675 [01:56<00:35, 5529.05it/s] 78%|███████▊  | 706139/900675 [01:56<00:35, 5445.86it/s] 78%|███████▊  | 706773/900675 [01:57<00:34, 5690.91it/s] 79%|███████▊  | 707346/900675 [01:57<00:34, 5635.64it/s] 79%|███████▊  | 707929/900675 [01:57<00:33, 5691.34it/s] 79%|███████▊  | 708631/900675 [01:57<00:31, 6073.59it/s] 79%|███████▊  | 709241/900675 [01:57<00:31, 6058.36it/s] 79%|███████▉  | 709850/900675 [01:57<00:31, 6066.06it/s] 79%|███████▉  | 710509/900675 [01:57<00:30, 6220.49it/s] 79%|███████▉  | 711193/900675 [01:57<00:29, 6401.49it/s] 79%|███████▉  | 711834/900675 [01:57<00:30, 6288.02it/s] 79%|███████▉  | 712464/900675 [01:57<00:30, 6260.85it/s] 79%|███████▉  | 713126/900675 [01:58<00:29, 6366.78it/s] 79%|███████▉  | 713764/900675 [01:58<00:29, 6359.32it/s] 79%|███████▉  | 714401/900675 [01:58<00:30, 6131.01it/s] 79%|███████▉  | 715017/900675 [01:58<00:30, 6115.13it/s] 79%|███████▉  | 715650/900675 [01:58<00:29, 6174.39it/s] 80%|███████▉  | 716303/900675 [01:58<00:29, 6277.09it/s] 80%|███████▉  | 716939/900675 [01:58<00:29, 6301.08it/s] 80%|███████▉  | 717570/900675 [01:58<00:30, 6094.37it/s] 80%|███████▉  | 718182/900675 [01:58<00:31, 5808.27it/s] 80%|███████▉  | 718864/900675 [01:59<00:29, 6090.69it/s] 80%|███████▉  | 719478/900675 [01:59<00:31, 5714.46it/s] 80%|███████▉  | 720057/900675 [01:59<00:31, 5680.44it/s] 80%|████████  | 720640/900675 [01:59<00:31, 5720.47it/s] 80%|████████  | 721265/900675 [01:59<00:30, 5867.99it/s] 80%|████████  | 721855/900675 [01:59<00:30, 5876.24it/s] 80%|████████  | 722545/900675 [01:59<00:28, 6173.46it/s] 80%|████████  | 723165/900675 [01:59<00:30, 5798.43it/s] 80%|████████  | 723889/900675 [01:59<00:28, 6200.10it/s] 80%|████████  | 724516/900675 [01:59<00:28, 6086.71it/s] 81%|████████  | 725130/900675 [02:00<00:28, 6061.53it/s] 81%|████████  | 725740/900675 [02:00<00:31, 5616.12it/s] 81%|████████  | 726382/900675 [02:00<00:29, 5827.66it/s] 81%|████████  | 727012/900675 [02:00<00:29, 5956.50it/s] 81%|████████  | 727662/900675 [02:00<00:28, 6112.69it/s] 81%|████████  | 728278/900675 [02:00<00:29, 5829.05it/s] 81%|████████  | 728891/900675 [02:00<00:29, 5914.12it/s] 81%|████████  | 729487/900675 [02:00<00:30, 5598.27it/s] 81%|████████  | 730053/900675 [02:00<00:30, 5613.39it/s] 81%|████████  | 730637/900675 [02:01<00:29, 5678.00it/s] 81%|████████  | 731209/900675 [02:01<00:29, 5658.93it/s] 81%|████████▏ | 731829/900675 [02:01<00:29, 5815.19it/s] 81%|████████▏ | 732413/900675 [02:01<00:29, 5755.98it/s] 81%|████████▏ | 733068/900675 [02:01<00:27, 5987.93it/s] 81%|████████▏ | 733809/900675 [02:01<00:26, 6402.93it/s] 82%|████████▏ | 734468/900675 [02:01<00:25, 6456.85it/s] 82%|████████▏ | 735115/900675 [02:01<00:26, 6172.36it/s] 82%|████████▏ | 735736/900675 [02:01<00:28, 5840.89it/s] 82%|████████▏ | 736414/900675 [02:01<00:26, 6090.07it/s] 82%|████████▏ | 737277/900675 [02:02<00:23, 6809.00it/s] 82%|████████▏ | 737966/900675 [02:02<00:25, 6493.93it/s] 82%|████████▏ | 738623/900675 [02:02<00:25, 6336.78it/s] 82%|████████▏ | 739262/900675 [02:02<00:25, 6324.92it/s] 82%|████████▏ | 739899/900675 [02:02<00:26, 6124.29it/s] 82%|████████▏ | 740515/900675 [02:02<00:26, 6088.15it/s] 82%|████████▏ | 741142/900675 [02:02<00:25, 6137.31it/s] 82%|████████▏ | 741758/900675 [02:02<00:27, 5841.54it/s] 82%|████████▏ | 742346/900675 [02:02<00:27, 5797.25it/s] 82%|████████▏ | 742954/900675 [02:03<00:26, 5871.51it/s] 83%|████████▎ | 743645/900675 [02:03<00:25, 6159.38it/s] 83%|████████▎ | 744264/900675 [02:03<00:27, 5784.41it/s] 83%|████████▎ | 744881/900675 [02:03<00:26, 5891.75it/s] 83%|████████▎ | 745485/900675 [02:03<00:26, 5929.66it/s] 83%|████████▎ | 746082/900675 [02:03<00:26, 5826.26it/s] 83%|████████▎ | 746725/900675 [02:03<00:25, 5997.76it/s] 83%|████████▎ | 747328/900675 [02:03<00:26, 5875.29it/s] 83%|████████▎ | 747918/900675 [02:03<00:26, 5771.23it/s] 83%|████████▎ | 748497/900675 [02:04<00:26, 5742.79it/s] 83%|████████▎ | 749167/900675 [02:04<00:25, 6020.94it/s] 83%|████████▎ | 749771/900675 [02:04<00:25, 5979.85it/s] 83%|████████▎ | 750371/900675 [02:04<00:25, 5950.70it/s] 83%|████████▎ | 750967/900675 [02:04<00:25, 5936.40it/s] 83%|████████▎ | 751562/900675 [02:04<00:25, 5799.86it/s] 84%|████████▎ | 752143/900675 [02:04<00:25, 5764.72it/s] 84%|████████▎ | 752806/900675 [02:04<00:24, 6014.26it/s] 84%|████████▎ | 753409/900675 [02:04<00:24, 6015.96it/s] 84%|████████▎ | 754091/900675 [02:04<00:23, 6253.67it/s] 84%|████████▍ | 754718/900675 [02:05<00:25, 5763.37it/s] 84%|████████▍ | 755309/900675 [02:05<00:25, 5797.26it/s] 84%|████████▍ | 755895/900675 [02:05<00:25, 5621.03it/s] 84%|████████▍ | 756462/900675 [02:05<00:25, 5611.66it/s] 84%|████████▍ | 757067/900675 [02:05<00:25, 5728.97it/s] 84%|████████▍ | 757703/900675 [02:05<00:24, 5912.04it/s] 84%|████████▍ | 758345/900675 [02:05<00:23, 6059.55it/s] 84%|████████▍ | 758954/900675 [02:05<00:23, 5929.40it/s] 84%|████████▍ | 759628/900675 [02:05<00:22, 6157.66it/s] 84%|████████▍ | 760275/900675 [02:05<00:22, 6248.54it/s] 84%|████████▍ | 760919/900675 [02:06<00:22, 6302.23it/s] 85%|████████▍ | 761551/900675 [02:06<00:22, 6253.75it/s] 85%|████████▍ | 762178/900675 [02:06<00:22, 6142.85it/s] 85%|████████▍ | 762794/900675 [02:06<00:23, 5891.12it/s] 85%|████████▍ | 763386/900675 [02:06<00:23, 5742.73it/s] 85%|████████▍ | 763994/900675 [02:06<00:23, 5838.66it/s] 85%|████████▍ | 764602/900675 [02:06<00:23, 5902.65it/s] 85%|████████▍ | 765194/900675 [02:06<00:22, 5891.97it/s] 85%|████████▌ | 765785/900675 [02:06<00:23, 5723.73it/s] 85%|████████▌ | 766489/900675 [02:07<00:21, 6102.68it/s] 85%|████████▌ | 767146/900675 [02:07<00:21, 6233.12it/s] 85%|████████▌ | 767772/900675 [02:07<00:21, 6168.95it/s] 85%|████████▌ | 768391/900675 [02:07<00:21, 6022.21it/s] 85%|████████▌ | 768995/900675 [02:07<00:23, 5701.50it/s] 85%|████████▌ | 769570/900675 [02:07<00:23, 5646.20it/s] 86%|████████▌ | 770138/900675 [02:07<00:23, 5611.96it/s] 86%|████████▌ | 770701/900675 [02:07<00:23, 5452.03it/s] 86%|████████▌ | 771280/900675 [02:07<00:23, 5544.67it/s] 86%|████████▌ | 771964/900675 [02:07<00:21, 5913.07it/s] 86%|████████▌ | 772558/900675 [02:08<00:22, 5800.15it/s] 86%|████████▌ | 773226/900675 [02:08<00:21, 6039.80it/s] 86%|████████▌ | 773865/900675 [02:08<00:20, 6136.49it/s] 86%|████████▌ | 774490/900675 [02:08<00:20, 6165.38it/s] 86%|████████▌ | 775108/900675 [02:08<00:21, 5884.30it/s] 86%|████████▌ | 775700/900675 [02:08<00:21, 5883.14it/s] 86%|████████▌ | 776291/900675 [02:08<00:21, 5836.27it/s] 86%|████████▋ | 776877/900675 [02:08<00:22, 5598.60it/s] 86%|████████▋ | 777544/900675 [02:08<00:20, 5899.84it/s] 86%|████████▋ | 778164/900675 [02:09<00:20, 5983.57it/s] 86%|████████▋ | 778774/900675 [02:09<00:20, 6013.20it/s] 87%|████████▋ | 779378/900675 [02:09<00:20, 5947.19it/s] 87%|████████▋ | 779975/900675 [02:09<00:20, 5882.81it/s] 87%|████████▋ | 780565/900675 [02:09<00:21, 5627.44it/s] 87%|████████▋ | 781191/900675 [02:09<00:20, 5805.75it/s] 87%|████████▋ | 781973/900675 [02:09<00:18, 6386.72it/s] 87%|████████▋ | 782616/900675 [02:09<00:19, 6016.24it/s] 87%|████████▋ | 783225/900675 [02:09<00:20, 5647.91it/s] 87%|████████▋ | 783798/900675 [02:09<00:20, 5633.31it/s] 87%|████████▋ | 784379/900675 [02:10<00:20, 5679.97it/s] 87%|████████▋ | 784955/900675 [02:10<00:20, 5696.38it/s] 87%|████████▋ | 785538/900675 [02:10<00:20, 5729.13it/s] 87%|████████▋ | 786235/900675 [02:10<00:18, 6081.97it/s] 87%|████████▋ | 786870/900675 [02:10<00:18, 6158.73it/s] 87%|████████▋ | 787488/900675 [02:10<00:18, 6044.41it/s] 88%|████████▊ | 788132/900675 [02:10<00:18, 6155.40it/s] 88%|████████▊ | 788749/900675 [02:10<00:18, 5931.47it/s] 88%|████████▊ | 789345/900675 [02:10<00:19, 5797.71it/s] 88%|████████▊ | 789988/900675 [02:11<00:18, 5978.14it/s] 88%|████████▊ | 790671/900675 [02:11<00:17, 6224.98it/s] 88%|████████▊ | 791296/900675 [02:11<00:18, 6072.78it/s] 88%|████████▊ | 791906/900675 [02:11<00:19, 5557.47it/s] 88%|████████▊ | 792563/900675 [02:11<00:18, 5832.00it/s] 88%|████████▊ | 793233/900675 [02:11<00:17, 6071.57it/s] 88%|████████▊ | 793848/900675 [02:11<00:18, 5888.66it/s] 88%|████████▊ | 794443/900675 [02:11<00:18, 5759.48it/s] 88%|████████▊ | 795074/900675 [02:11<00:17, 5915.06it/s] 88%|████████▊ | 795764/900675 [02:11<00:16, 6198.35it/s] 88%|████████▊ | 796388/900675 [02:12<00:17, 6094.54it/s] 88%|████████▊ | 797023/900675 [02:12<00:16, 6166.94it/s] 89%|████████▊ | 797642/900675 [02:12<00:17, 5864.35it/s] 89%|████████▊ | 798316/900675 [02:12<00:16, 6110.33it/s] 89%|████████▊ | 798932/900675 [02:12<00:17, 5965.82it/s] 89%|████████▉ | 799554/900675 [02:12<00:16, 6035.54it/s] 89%|████████▉ | 800210/900675 [02:12<00:16, 6176.48it/s] 89%|████████▉ | 800830/900675 [02:12<00:16, 5970.26it/s] 89%|████████▉ | 801430/900675 [02:12<00:17, 5810.70it/s] 89%|████████▉ | 802082/900675 [02:13<00:16, 6005.07it/s] 89%|████████▉ | 802686/900675 [02:13<00:17, 5617.05it/s] 89%|████████▉ | 803382/900675 [02:13<00:16, 5986.84it/s] 89%|████████▉ | 803988/900675 [02:13<00:16, 5800.61it/s] 89%|████████▉ | 804656/900675 [02:13<00:15, 6048.11it/s] 89%|████████▉ | 805277/900675 [02:13<00:15, 6090.06it/s] 89%|████████▉ | 805890/900675 [02:13<00:16, 5832.33it/s] 90%|████████▉ | 806499/900675 [02:13<00:15, 5902.93it/s] 90%|████████▉ | 807115/900675 [02:13<00:15, 5972.60it/s] 90%|████████▉ | 807716/900675 [02:14<00:16, 5803.19it/s] 90%|████████▉ | 808299/900675 [02:14<00:17, 5411.92it/s] 90%|████████▉ | 808985/900675 [02:14<00:15, 5812.66it/s] 90%|████████▉ | 809574/900675 [02:14<00:15, 5758.85it/s] 90%|████████▉ | 810166/900675 [02:14<00:15, 5804.46it/s] 90%|█████████ | 810751/900675 [02:14<00:16, 5440.47it/s] 90%|█████████ | 811302/900675 [02:14<00:16, 5442.18it/s] 90%|█████████ | 811961/900675 [02:14<00:15, 5645.91it/s] 90%|█████████ | 812529/900675 [02:14<00:16, 5460.24it/s] 90%|█████████ | 813207/900675 [02:14<00:15, 5830.47it/s] 90%|█████████ | 813903/900675 [02:15<00:14, 6145.11it/s] 90%|█████████ | 814536/900675 [02:15<00:13, 6197.33it/s] 91%|█████████ | 815250/900675 [02:15<00:13, 6472.81it/s] 91%|█████████ | 815901/900675 [02:15<00:13, 6391.30it/s] 91%|█████████ | 816626/900675 [02:15<00:12, 6632.81it/s] 91%|█████████ | 817325/900675 [02:15<00:12, 6735.83it/s] 91%|█████████ | 818058/900675 [02:15<00:11, 6911.38it/s] 91%|█████████ | 818751/900675 [02:15<00:12, 6442.64it/s] 91%|█████████ | 819403/900675 [02:15<00:13, 6246.95it/s] 91%|█████████ | 820108/900675 [02:16<00:12, 6472.20it/s] 91%|█████████ | 820761/900675 [02:16<00:12, 6398.81it/s] 91%|█████████ | 821408/900675 [02:16<00:12, 6410.20it/s] 91%|█████████▏| 822052/900675 [02:16<00:13, 5869.58it/s] 91%|█████████▏| 822666/900675 [02:16<00:13, 5940.85it/s] 91%|█████████▏| 823314/900675 [02:16<00:12, 6091.53it/s] 91%|█████████▏| 823947/900675 [02:16<00:12, 6159.23it/s] 92%|█████████▏| 824568/900675 [02:16<00:12, 5872.74it/s] 92%|█████████▏| 825161/900675 [02:16<00:13, 5693.99it/s] 92%|█████████▏| 825735/900675 [02:16<00:13, 5610.79it/s] 92%|█████████▏| 826340/900675 [02:17<00:12, 5735.04it/s] 92%|█████████▏| 826956/900675 [02:17<00:12, 5854.94it/s] 92%|█████████▏| 827643/900675 [02:17<00:11, 6146.01it/s] 92%|█████████▏| 828261/900675 [02:17<00:11, 6057.23it/s] 92%|█████████▏| 828869/900675 [02:17<00:12, 5720.14it/s] 92%|█████████▏| 829446/900675 [02:17<00:13, 5353.56it/s] 92%|█████████▏| 830101/900675 [02:17<00:12, 5676.53it/s] 92%|█████████▏| 830677/900675 [02:17<00:12, 5572.43it/s] 92%|█████████▏| 831261/900675 [02:17<00:12, 5646.30it/s] 92%|█████████▏| 831848/900675 [02:18<00:12, 5704.76it/s] 92%|█████████▏| 832422/900675 [02:18<00:11, 5694.13it/s] 92%|█████████▏| 833073/900675 [02:18<00:11, 5929.37it/s] 93%|█████████▎| 833713/900675 [02:18<00:11, 6065.18it/s] 93%|█████████▎| 834322/900675 [02:18<00:11, 5880.60it/s] 93%|█████████▎| 834979/900675 [02:18<00:10, 6072.51it/s] 93%|█████████▎| 835617/900675 [02:18<00:10, 6161.08it/s] 93%|█████████▎| 836235/900675 [02:18<00:10, 6123.18it/s] 93%|█████████▎| 836861/900675 [02:18<00:10, 6149.29it/s] 93%|█████████▎| 837481/900675 [02:18<00:10, 6160.02it/s] 93%|█████████▎| 838120/900675 [02:19<00:10, 6227.33it/s] 93%|█████████▎| 838744/900675 [02:19<00:10, 6009.97it/s] 93%|█████████▎| 839347/900675 [02:19<00:10, 5827.34it/s] 93%|█████████▎| 839932/900675 [02:19<00:10, 5797.09it/s] 93%|█████████▎| 840710/900675 [02:19<00:09, 6369.64it/s] 93%|█████████▎| 841350/900675 [02:19<00:09, 6146.31it/s] 93%|█████████▎| 841969/900675 [02:19<00:09, 6141.59it/s] 94%|█████████▎| 842586/900675 [02:19<00:09, 5936.69it/s] 94%|█████████▎| 843183/900675 [02:19<00:10, 5587.90it/s] 94%|█████████▎| 843747/900675 [02:20<00:10, 5403.69it/s] 94%|█████████▎| 844291/900675 [02:20<00:10, 5217.35it/s] 94%|█████████▍| 844840/900675 [02:20<00:10, 5290.68it/s] 94%|█████████▍| 845418/900675 [02:20<00:10, 5426.24it/s] 94%|█████████▍| 846029/900675 [02:20<00:09, 5621.77it/s] 94%|█████████▍| 846594/900675 [02:20<00:09, 5547.93it/s] 94%|█████████▍| 847244/900675 [02:20<00:09, 5821.52it/s] 94%|█████████▍| 847876/900675 [02:20<00:08, 5964.06it/s] 94%|█████████▍| 848475/900675 [02:20<00:08, 5811.56it/s] 94%|█████████▍| 849081/900675 [02:20<00:08, 5877.57it/s] 94%|█████████▍| 849671/900675 [02:21<00:08, 5861.78it/s] 94%|█████████▍| 850306/900675 [02:21<00:08, 5999.40it/s] 94%|█████████▍| 850907/900675 [02:21<00:08, 5631.38it/s] 95%|█████████▍| 851532/900675 [02:21<00:08, 5806.48it/s] 95%|█████████▍| 852118/900675 [02:21<00:08, 5787.02it/s] 95%|█████████▍| 852700/900675 [02:21<00:08, 5549.13it/s] 95%|█████████▍| 853298/900675 [02:21<00:08, 5664.05it/s] 95%|█████████▍| 853868/900675 [02:21<00:08, 5458.98it/s] 95%|█████████▍| 854508/900675 [02:21<00:08, 5724.03it/s] 95%|█████████▍| 855085/900675 [02:22<00:08, 5643.64it/s] 95%|█████████▌| 855700/900675 [02:22<00:07, 5785.08it/s] 95%|█████████▌| 856306/900675 [02:22<00:07, 5861.45it/s] 95%|█████████▌| 856974/900675 [02:22<00:07, 6097.06it/s] 95%|█████████▌| 857586/900675 [02:22<00:07, 5998.98it/s] 95%|█████████▌| 858188/900675 [02:22<00:07, 5989.01it/s] 95%|█████████▌| 858788/900675 [02:22<00:07, 5827.51it/s] 95%|█████████▌| 859373/900675 [02:22<00:07, 5825.71it/s] 95%|█████████▌| 859958/900675 [02:22<00:06, 5825.02it/s] 96%|█████████▌| 860791/900675 [02:22<00:06, 6562.47it/s] 96%|█████████▌| 861450/900675 [02:23<00:06, 6238.89it/s] 96%|█████████▌| 862079/900675 [02:23<00:06, 6135.92it/s] 96%|█████████▌| 862749/900675 [02:23<00:06, 6297.12it/s] 96%|█████████▌| 863404/900675 [02:23<00:05, 6367.63it/s] 96%|█████████▌| 864043/900675 [02:23<00:06, 6005.01it/s] 96%|█████████▌| 864649/900675 [02:23<00:06, 5959.55it/s] 96%|█████████▌| 865257/900675 [02:23<00:05, 5992.29it/s] 96%|█████████▌| 865859/900675 [02:23<00:06, 5781.11it/s] 96%|█████████▌| 866531/900675 [02:23<00:05, 6047.13it/s] 96%|█████████▋| 867140/900675 [02:24<00:05, 5780.14it/s] 96%|█████████▋| 867768/900675 [02:24<00:05, 5916.05it/s] 96%|█████████▋| 868407/900675 [02:24<00:05, 6046.02it/s] 96%|█████████▋| 869015/900675 [02:24<00:05, 5850.85it/s] 97%|█████████▋| 869604/900675 [02:24<00:05, 5761.55it/s] 97%|█████████▋| 870334/900675 [02:24<00:04, 6195.36it/s] 97%|█████████▋| 870957/900675 [02:24<00:04, 6182.42it/s] 97%|█████████▋| 871596/900675 [02:24<00:04, 6237.96it/s] 97%|█████████▋| 872222/900675 [02:24<00:04, 6200.39it/s] 97%|█████████▋| 872844/900675 [02:24<00:04, 5798.77it/s] 97%|█████████▋| 873521/900675 [02:25<00:04, 6071.46it/s] 97%|█████████▋| 874215/900675 [02:25<00:04, 6319.00it/s] 97%|█████████▋| 874852/900675 [02:25<00:04, 6154.15it/s] 97%|█████████▋| 875490/900675 [02:25<00:04, 6217.55it/s] 97%|█████████▋| 876129/900675 [02:25<00:03, 6262.24it/s] 97%|█████████▋| 876758/900675 [02:25<00:03, 6254.40it/s] 97%|█████████▋| 877386/900675 [02:25<00:03, 5987.81it/s] 97%|█████████▋| 877989/900675 [02:25<00:03, 5893.55it/s] 98%|█████████▊| 878581/900675 [02:25<00:03, 5896.50it/s] 98%|█████████▊| 879210/900675 [02:26<00:03, 6009.43it/s] 98%|█████████▊| 879813/900675 [02:26<00:03, 5754.42it/s] 98%|█████████▊| 880402/900675 [02:26<00:03, 5791.59it/s] 98%|█████████▊| 880984/900675 [02:26<00:03, 5729.00it/s] 98%|█████████▊| 881559/900675 [02:26<00:03, 5653.68it/s] 98%|█████████▊| 882126/900675 [02:26<00:03, 5295.52it/s] 98%|█████████▊| 882785/900675 [02:26<00:03, 5656.76it/s] 98%|█████████▊| 883435/900675 [02:26<00:02, 5895.16it/s] 98%|█████████▊| 884030/900675 [02:26<00:02, 5828.55it/s] 98%|█████████▊| 884617/900675 [02:26<00:02, 5726.82it/s] 98%|█████████▊| 885225/900675 [02:27<00:02, 5827.67it/s] 98%|█████████▊| 885894/900675 [02:27<00:02, 6075.94it/s] 98%|█████████▊| 886621/900675 [02:27<00:02, 6422.40it/s] 99%|█████████▊| 887266/900675 [02:27<00:02, 6195.80it/s] 99%|█████████▊| 887889/900675 [02:27<00:02, 5954.75it/s] 99%|█████████▊| 888496/900675 [02:27<00:02, 5987.38it/s] 99%|█████████▊| 889098/900675 [02:27<00:01, 5971.70it/s] 99%|█████████▉| 889697/900675 [02:27<00:01, 5930.13it/s] 99%|█████████▉| 890298/900675 [02:27<00:01, 5944.21it/s] 99%|█████████▉| 890998/900675 [02:28<00:01, 6248.65it/s] 99%|█████████▉| 891625/900675 [02:28<00:01, 5999.89it/s] 99%|█████████▉| 892228/900675 [02:28<00:01, 5858.79it/s] 99%|█████████▉| 892824/900675 [02:28<00:01, 5885.03it/s] 99%|█████████▉| 893415/900675 [02:28<00:01, 5742.21it/s] 99%|█████████▉| 893991/900675 [02:28<00:01, 5545.94it/s] 99%|█████████▉| 894548/900675 [02:28<00:01, 5421.83it/s] 99%|█████████▉| 895161/900675 [02:28<00:00, 5619.43it/s] 99%|█████████▉| 895725/900675 [02:28<00:00, 5521.24it/s]100%|█████████▉| 896383/900675 [02:28<00:00, 5822.92it/s]100%|█████████▉| 896983/900675 [02:29<00:00, 5872.38it/s]100%|█████████▉| 897572/900675 [02:29<00:00, 5874.19it/s]100%|█████████▉| 898237/900675 [02:29<00:00, 6102.14it/s]100%|█████████▉| 898888/900675 [02:29<00:00, 6222.49it/s]100%|█████████▉| 899512/900675 [02:29<00:00, 6211.75it/s]100%|█████████▉| 900134/900675 [02:29<00:00, 6110.40it/s]100%|██████████| 900675/900675 [02:29<00:00, 6017.44it/s]

gathering stats for n=1
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 1890/900675 [00:00<00:47, 18896.55it/s]  0%|          | 3908/900675 [00:00<00:45, 19648.50it/s]  1%|          | 6130/900675 [00:00<00:42, 20815.77it/s]  1%|          | 8212/900675 [00:00<00:45, 19807.15it/s]  1%|          | 10240/900675 [00:00<00:44, 19967.20it/s]  1%|▏         | 12242/900675 [00:00<00:45, 19717.37it/s]  2%|▏         | 14294/900675 [00:00<00:44, 19972.97it/s]  2%|▏         | 16295/900675 [00:00<00:44, 19888.02it/s]  2%|▏         | 18286/900675 [00:00<00:44, 19886.52it/s]  2%|▏         | 20369/900675 [00:01<00:43, 20172.59it/s]  2%|▏         | 22388/900675 [00:01<00:43, 20128.13it/s]  3%|▎         | 24673/900675 [00:01<00:41, 20949.77it/s]  3%|▎         | 26770/900675 [00:01<00:42, 20402.79it/s]  3%|▎         | 28814/900675 [00:01<00:42, 20411.15it/s]  3%|▎         | 30858/900675 [00:01<00:43, 19824.11it/s]  4%|▎         | 32846/900675 [00:01<00:44, 19695.25it/s]  4%|▍         | 34881/900675 [00:01<00:43, 19886.48it/s]  4%|▍         | 36873/900675 [00:01<00:44, 19621.42it/s]  4%|▍         | 38838/900675 [00:01<00:44, 19377.62it/s]  5%|▍         | 40887/900675 [00:02<00:43, 19698.60it/s]  5%|▍         | 42859/900675 [00:02<00:44, 19240.03it/s]  5%|▍         | 44882/900675 [00:02<00:43, 19526.76it/s]  5%|▌         | 46929/900675 [00:02<00:43, 19791.34it/s]  5%|▌         | 49473/900675 [00:02<00:39, 21460.27it/s]  6%|▌         | 51624/900675 [00:02<00:40, 20758.46it/s]  6%|▌         | 53769/900675 [00:02<00:40, 20952.28it/s]  6%|▌         | 55871/900675 [00:02<00:41, 20432.38it/s]  6%|▋         | 57921/900675 [00:02<00:42, 19969.45it/s]  7%|▋         | 59936/900675 [00:02<00:42, 20015.59it/s]  7%|▋         | 62052/900675 [00:03<00:41, 20346.74it/s]  7%|▋         | 64091/900675 [00:03<00:41, 20196.46it/s]  7%|▋         | 66114/900675 [00:03<00:41, 20118.69it/s]  8%|▊         | 68128/900675 [00:03<00:41, 19872.41it/s]  8%|▊         | 70275/900675 [00:03<00:40, 20338.51it/s]  8%|▊         | 72311/900675 [00:03<00:40, 20228.01it/s]  8%|▊         | 74712/900675 [00:03<00:38, 21342.58it/s]  9%|▊         | 76849/900675 [00:03<00:39, 21008.31it/s]  9%|▉         | 78953/900675 [00:03<00:40, 20359.81it/s]  9%|▉         | 80995/900675 [00:04<00:40, 20131.63it/s]  9%|▉         | 83041/900675 [00:04<00:40, 20223.22it/s]  9%|▉         | 85098/900675 [00:04<00:40, 20320.80it/s] 10%|▉         | 87202/900675 [00:04<00:39, 20531.11it/s] 10%|▉         | 89257/900675 [00:04<00:40, 20152.72it/s] 10%|█         | 91275/900675 [00:04<00:40, 20038.93it/s] 10%|█         | 93281/900675 [00:04<00:40, 19992.35it/s] 11%|█         | 95340/900675 [00:04<00:39, 20167.80it/s] 11%|█         | 97513/900675 [00:04<00:38, 20619.67it/s] 11%|█         | 99577/900675 [00:04<00:39, 20156.28it/s] 11%|█▏        | 101596/900675 [00:05<00:40, 19873.77it/s] 12%|█▏        | 103586/900675 [00:05<00:40, 19812.88it/s] 12%|█▏        | 105569/900675 [00:05<00:40, 19770.28it/s] 12%|█▏        | 107548/900675 [00:05<00:40, 19564.79it/s] 12%|█▏        | 109506/900675 [00:05<00:40, 19384.26it/s] 12%|█▏        | 111602/900675 [00:05<00:39, 19843.32it/s] 13%|█▎        | 113588/900675 [00:05<00:40, 19627.04it/s] 13%|█▎        | 115552/900675 [00:05<00:40, 19564.73it/s] 13%|█▎        | 117510/900675 [00:05<00:40, 19394.89it/s] 13%|█▎        | 119479/900675 [00:05<00:40, 19481.67it/s] 13%|█▎        | 121551/900675 [00:06<00:39, 19847.52it/s] 14%|█▎        | 123537/900675 [00:06<00:39, 19745.43it/s] 14%|█▍        | 125513/900675 [00:06<00:39, 19630.08it/s] 14%|█▍        | 127582/900675 [00:06<00:38, 19944.16it/s] 14%|█▍        | 129648/900675 [00:06<00:38, 20156.35it/s] 15%|█▍        | 131665/900675 [00:06<00:39, 19492.33it/s] 15%|█▍        | 133694/900675 [00:06<00:38, 19721.96it/s] 15%|█▌        | 135905/900675 [00:06<00:37, 20422.62it/s] 15%|█▌        | 138110/900675 [00:06<00:36, 20899.79it/s] 16%|█▌        | 140204/900675 [00:06<00:36, 20703.00it/s] 16%|█▌        | 142278/900675 [00:07<00:37, 20283.09it/s] 16%|█▌        | 144381/900675 [00:07<00:36, 20495.87it/s] 16%|█▋        | 146501/900675 [00:07<00:36, 20695.90it/s] 16%|█▋        | 148573/900675 [00:07<00:37, 20158.45it/s] 17%|█▋        | 150594/900675 [00:07<00:37, 19808.03it/s] 17%|█▋        | 152579/900675 [00:07<00:38, 19621.68it/s] 17%|█▋        | 154579/900675 [00:07<00:37, 19730.16it/s] 17%|█▋        | 156668/900675 [00:07<00:37, 20060.14it/s] 18%|█▊        | 158811/900675 [00:07<00:36, 20457.26it/s] 18%|█▊        | 160859/900675 [00:08<00:36, 20373.42it/s] 18%|█▊        | 162915/900675 [00:08<00:36, 20428.60it/s] 18%|█▊        | 164959/900675 [00:08<00:36, 20334.62it/s] 19%|█▊        | 167135/900675 [00:08<00:35, 20758.04it/s] 19%|█▉        | 169212/900675 [00:08<00:35, 20330.36it/s] 19%|█▉        | 171248/900675 [00:08<00:36, 19789.90it/s] 19%|█▉        | 173232/900675 [00:08<00:36, 19799.39it/s] 19%|█▉        | 175215/900675 [00:08<00:36, 19622.44it/s] 20%|█▉        | 177180/900675 [00:08<00:37, 19528.99it/s] 20%|█▉        | 179135/900675 [00:08<00:37, 19383.79it/s] 20%|██        | 181075/900675 [00:09<00:37, 19231.79it/s] 20%|██        | 183019/900675 [00:09<00:37, 19289.45it/s] 21%|██        | 184949/900675 [00:09<00:37, 18918.18it/s] 21%|██        | 186843/900675 [00:09<00:38, 18641.86it/s] 21%|██        | 188902/900675 [00:09<00:37, 19207.94it/s] 21%|██        | 190920/900675 [00:09<00:36, 19493.28it/s] 21%|██▏       | 192970/900675 [00:09<00:35, 19786.78it/s] 22%|██▏       | 194992/900675 [00:09<00:35, 19914.89it/s] 22%|██▏       | 197191/900675 [00:09<00:34, 20531.80it/s] 22%|██▏       | 199246/900675 [00:09<00:34, 20464.01it/s] 22%|██▏       | 201294/900675 [00:10<00:35, 19963.27it/s] 23%|██▎       | 203327/900675 [00:10<00:34, 20065.31it/s] 23%|██▎       | 205336/900675 [00:10<00:34, 19892.90it/s] 23%|██▎       | 207328/900675 [00:10<00:35, 19614.04it/s] 23%|██▎       | 209292/900675 [00:10<00:35, 19333.22it/s] 23%|██▎       | 211330/900675 [00:10<00:35, 19632.90it/s] 24%|██▎       | 213312/900675 [00:10<00:34, 19686.71it/s] 24%|██▍       | 215283/900675 [00:10<00:35, 19226.57it/s] 24%|██▍       | 217209/900675 [00:10<00:36, 18916.95it/s] 24%|██▍       | 219270/900675 [00:10<00:35, 19405.49it/s] 25%|██▍       | 221214/900675 [00:11<00:35, 19264.99it/s] 25%|██▍       | 223143/900675 [00:11<00:36, 18717.21it/s] 25%|██▍       | 225046/900675 [00:11<00:35, 18807.44it/s] 25%|██▌       | 227119/900675 [00:11<00:34, 19362.31it/s] 25%|██▌       | 229059/900675 [00:11<00:35, 19114.24it/s] 26%|██▌       | 230974/900675 [00:11<00:35, 18854.03it/s] 26%|██▌       | 232973/900675 [00:11<00:34, 19182.37it/s] 26%|██▌       | 235142/900675 [00:11<00:33, 19916.12it/s] 26%|██▋       | 237137/900675 [00:11<00:33, 19782.93it/s] 27%|██▋       | 239186/900675 [00:12<00:33, 19991.71it/s] 27%|██▋       | 241360/900675 [00:12<00:32, 20507.87it/s] 27%|██▋       | 243413/900675 [00:12<00:32, 20274.31it/s] 27%|██▋       | 245443/900675 [00:12<00:32, 20260.20it/s] 27%|██▋       | 247680/900675 [00:12<00:31, 20885.99it/s] 28%|██▊       | 249848/900675 [00:12<00:30, 21121.73it/s] 28%|██▊       | 251962/900675 [00:12<00:30, 20966.57it/s] 28%|██▊       | 254060/900675 [00:12<00:31, 20839.86it/s] 28%|██▊       | 256145/900675 [00:12<00:31, 20683.54it/s] 29%|██▊       | 258215/900675 [00:12<00:31, 20415.59it/s] 29%|██▉       | 260258/900675 [00:13<00:31, 20109.30it/s] 29%|██▉       | 262300/900675 [00:13<00:31, 20193.69it/s] 29%|██▉       | 264321/900675 [00:13<00:32, 19847.46it/s] 30%|██▉       | 266308/900675 [00:13<00:32, 19823.52it/s] 30%|██▉       | 268292/900675 [00:13<00:32, 19651.64it/s] 30%|███       | 270376/900675 [00:13<00:31, 19997.55it/s] 30%|███       | 272528/900675 [00:13<00:30, 20446.23it/s] 30%|███       | 274611/900675 [00:13<00:30, 20552.99it/s] 31%|███       | 276668/900675 [00:13<00:31, 19824.22it/s] 31%|███       | 278745/900675 [00:13<00:30, 20093.99it/s] 31%|███       | 280760/900675 [00:14<00:31, 19804.05it/s] 31%|███▏      | 282745/900675 [00:14<00:31, 19659.66it/s] 32%|███▏      | 284822/900675 [00:14<00:30, 19982.93it/s] 32%|███▏      | 286823/900675 [00:14<00:30, 19917.55it/s] 32%|███▏      | 288935/900675 [00:14<00:30, 20271.99it/s] 32%|███▏      | 290964/900675 [00:14<00:30, 19727.56it/s] 33%|███▎      | 292941/900675 [00:14<00:31, 19599.24it/s] 33%|███▎      | 295053/900675 [00:14<00:30, 20041.55it/s] 33%|███▎      | 297082/900675 [00:14<00:30, 20113.76it/s] 33%|███▎      | 299096/900675 [00:14<00:29, 20068.90it/s] 33%|███▎      | 301119/900675 [00:15<00:29, 20113.25it/s] 34%|███▎      | 303167/900675 [00:15<00:29, 20220.51it/s] 34%|███▍      | 305190/900675 [00:15<00:29, 20203.26it/s] 34%|███▍      | 307211/900675 [00:15<00:29, 19966.44it/s] 34%|███▍      | 309238/900675 [00:15<00:29, 20055.90it/s] 35%|███▍      | 311324/900675 [00:15<00:29, 20294.50it/s] 35%|███▍      | 313411/900675 [00:15<00:28, 20462.04it/s] 35%|███▌      | 315458/900675 [00:15<00:28, 20218.22it/s] 35%|███▌      | 317481/900675 [00:15<00:29, 19915.46it/s] 35%|███▌      | 319474/900675 [00:15<00:29, 19800.00it/s] 36%|███▌      | 321478/900675 [00:16<00:29, 19863.03it/s] 36%|███▌      | 323466/900675 [00:16<00:29, 19449.92it/s] 36%|███▌      | 325413/900675 [00:16<00:29, 19193.61it/s] 36%|███▋      | 327585/900675 [00:16<00:28, 19930.99it/s] 37%|███▋      | 329699/900675 [00:16<00:28, 20282.57it/s] 37%|███▋      | 331731/900675 [00:16<00:28, 19910.62it/s] 37%|███▋      | 333726/900675 [00:16<00:28, 19868.24it/s] 37%|███▋      | 335790/900675 [00:16<00:28, 20092.12it/s] 38%|███▊      | 337904/900675 [00:16<00:27, 20400.97it/s] 38%|███▊      | 339946/900675 [00:17<00:27, 20044.60it/s] 38%|███▊      | 341953/900675 [00:17<00:28, 19954.18it/s] 38%|███▊      | 344056/900675 [00:17<00:27, 20266.65it/s] 38%|███▊      | 346085/900675 [00:17<00:28, 19565.78it/s] 39%|███▊      | 348173/900675 [00:17<00:27, 19946.25it/s] 39%|███▉      | 350380/900675 [00:17<00:26, 20564.29it/s] 39%|███▉      | 352442/900675 [00:17<00:27, 19697.79it/s] 39%|███▉      | 354509/900675 [00:17<00:27, 19973.19it/s] 40%|███▉      | 356515/900675 [00:17<00:27, 19742.37it/s] 40%|███▉      | 358496/900675 [00:17<00:28, 19362.33it/s] 40%|████      | 360540/900675 [00:18<00:27, 19672.35it/s] 40%|████      | 362743/900675 [00:18<00:26, 20360.31it/s] 41%|████      | 364890/900675 [00:18<00:25, 20685.56it/s] 41%|████      | 366963/900675 [00:18<00:25, 20536.66it/s] 41%|████      | 369020/900675 [00:18<00:26, 20336.14it/s] 41%|████      | 371056/900675 [00:18<00:26, 20183.94it/s] 41%|████▏     | 373076/900675 [00:18<00:26, 20055.99it/s] 42%|████▏     | 375165/900675 [00:18<00:25, 20287.86it/s] 42%|████▏     | 377195/900675 [00:18<00:26, 19662.01it/s] 42%|████▏     | 379256/900675 [00:18<00:26, 19937.15it/s] 42%|████▏     | 381449/900675 [00:19<00:25, 20521.62it/s] 43%|████▎     | 383506/900675 [00:19<00:26, 19803.69it/s] 43%|████▎     | 385635/900675 [00:19<00:25, 20233.40it/s] 43%|████▎     | 387776/900675 [00:19<00:24, 20575.28it/s] 43%|████▎     | 389840/900675 [00:19<00:25, 20370.28it/s] 44%|████▎     | 391882/900675 [00:19<00:25, 19878.80it/s] 44%|████▎     | 393875/900675 [00:19<00:25, 19698.77it/s] 44%|████▍     | 395914/900675 [00:19<00:25, 19878.91it/s] 44%|████▍     | 397905/900675 [00:19<00:25, 19611.57it/s] 44%|████▍     | 400109/900675 [00:20<00:24, 20306.60it/s] 45%|████▍     | 402143/900675 [00:20<00:25, 19782.95it/s] 45%|████▍     | 404126/900675 [00:20<00:25, 19645.23it/s] 45%|████▌     | 406094/900675 [00:20<00:25, 19577.55it/s] 45%|████▌     | 408054/900675 [00:20<00:25, 19282.64it/s] 46%|████▌     | 410097/900675 [00:20<00:25, 19611.51it/s] 46%|████▌     | 412082/900675 [00:20<00:24, 19674.27it/s] 46%|████▌     | 414052/900675 [00:20<00:25, 18781.86it/s] 46%|████▌     | 415952/900675 [00:20<00:25, 18843.59it/s] 46%|████▋     | 417945/900675 [00:20<00:25, 19154.08it/s] 47%|████▋     | 419903/900675 [00:21<00:24, 19278.27it/s] 47%|████▋     | 421835/900675 [00:21<00:25, 19002.48it/s] 47%|████▋     | 423878/900675 [00:21<00:24, 19419.56it/s] 47%|████▋     | 425824/900675 [00:21<00:24, 19348.58it/s] 48%|████▊     | 427913/900675 [00:21<00:23, 19804.46it/s] 48%|████▊     | 430071/900675 [00:21<00:23, 20331.55it/s] 48%|████▊     | 432107/900675 [00:21<00:23, 19699.03it/s] 48%|████▊     | 434252/900675 [00:21<00:23, 20206.63it/s] 48%|████▊     | 436278/900675 [00:21<00:23, 19800.13it/s] 49%|████▊     | 438314/900675 [00:21<00:23, 19961.39it/s] 49%|████▉     | 440314/900675 [00:22<00:23, 19490.99it/s] 49%|████▉     | 442377/900675 [00:22<00:23, 19812.45it/s] 49%|████▉     | 444380/900675 [00:22<00:22, 19871.74it/s] 50%|████▉     | 446371/900675 [00:22<00:23, 19351.30it/s] 50%|████▉     | 448440/900675 [00:22<00:22, 19739.06it/s] 50%|█████     | 450472/900675 [00:22<00:22, 19904.32it/s] 50%|█████     | 452466/900675 [00:22<00:22, 19615.36it/s] 50%|█████     | 454431/900675 [00:22<00:22, 19408.09it/s] 51%|█████     | 456417/900675 [00:22<00:22, 19540.20it/s] 51%|█████     | 458415/900675 [00:23<00:22, 19668.53it/s] 51%|█████     | 460419/900675 [00:23<00:22, 19778.22it/s] 51%|█████▏    | 462398/900675 [00:23<00:22, 19641.88it/s] 52%|█████▏    | 464376/900675 [00:23<00:22, 19681.40it/s] 52%|█████▏    | 466396/900675 [00:23<00:21, 19835.43it/s] 52%|█████▏    | 468381/900675 [00:23<00:21, 19812.31it/s] 52%|█████▏    | 470363/900675 [00:23<00:21, 19656.64it/s] 52%|█████▏    | 472369/900675 [00:23<00:21, 19773.60it/s] 53%|█████▎    | 474365/900675 [00:23<00:21, 19815.72it/s] 53%|█████▎    | 476505/900675 [00:23<00:20, 20286.45it/s] 53%|█████▎    | 478577/900675 [00:24<00:20, 20414.86it/s] 53%|█████▎    | 480649/900675 [00:24<00:20, 20505.93it/s] 54%|█████▎    | 482700/900675 [00:24<00:20, 19922.27it/s] 54%|█████▍    | 484727/900675 [00:24<00:20, 20020.79it/s] 54%|█████▍    | 486778/900675 [00:24<00:20, 20164.43it/s] 54%|█████▍    | 488827/900675 [00:24<00:20, 20260.83it/s] 54%|█████▍    | 490855/900675 [00:24<00:20, 19829.21it/s] 55%|█████▍    | 492903/900675 [00:24<00:20, 20020.03it/s] 55%|█████▍    | 494978/900675 [00:24<00:20, 20234.09it/s] 55%|█████▌    | 497004/900675 [00:24<00:20, 20120.72it/s] 55%|█████▌    | 499045/900675 [00:25<00:19, 20203.78it/s] 56%|█████▌    | 501156/900675 [00:25<00:19, 20472.30it/s] 56%|█████▌    | 503205/900675 [00:25<00:19, 19956.57it/s] 56%|█████▌    | 505204/900675 [00:25<00:19, 19845.24it/s] 56%|█████▋    | 507270/900675 [00:25<00:19, 20084.22it/s] 57%|█████▋    | 509281/900675 [00:25<00:19, 19771.60it/s] 57%|█████▋    | 511437/900675 [00:25<00:19, 20294.60it/s] 57%|█████▋    | 513470/900675 [00:25<00:19, 20251.41it/s] 57%|█████▋    | 515497/900675 [00:25<00:19, 20070.58it/s] 57%|█████▋    | 517506/900675 [00:25<00:19, 19781.03it/s] 58%|█████▊    | 519622/900675 [00:26<00:18, 20181.33it/s] 58%|█████▊    | 521643/900675 [00:26<00:18, 20179.26it/s] 58%|█████▊    | 523663/900675 [00:26<00:18, 19943.45it/s] 58%|█████▊    | 525659/900675 [00:26<00:19, 19603.40it/s] 59%|█████▊    | 527716/900675 [00:26<00:18, 19885.35it/s] 59%|█████▉    | 529799/900675 [00:26<00:18, 20158.38it/s] 59%|█████▉    | 531817/900675 [00:26<00:18, 19869.07it/s] 59%|█████▉    | 533806/900675 [00:26<00:18, 19718.52it/s] 59%|█████▉    | 535780/900675 [00:26<00:18, 19554.48it/s] 60%|█████▉    | 537737/900675 [00:26<00:18, 19533.33it/s] 60%|█████▉    | 539733/900675 [00:27<00:18, 19658.79it/s] 60%|██████    | 541707/900675 [00:27<00:18, 19679.01it/s] 60%|██████    | 543706/900675 [00:27<00:18, 19770.29it/s] 61%|██████    | 545684/900675 [00:27<00:18, 19669.93it/s] 61%|██████    | 547696/900675 [00:27<00:17, 19798.13it/s] 61%|██████    | 549738/900675 [00:27<00:17, 19972.55it/s] 61%|██████▏   | 551755/900675 [00:27<00:17, 20028.31it/s] 61%|██████▏   | 553759/900675 [00:27<00:17, 19704.51it/s] 62%|██████▏   | 555816/900675 [00:27<00:17, 19958.40it/s] 62%|██████▏   | 557814/900675 [00:27<00:17, 19817.96it/s] 62%|██████▏   | 559797/900675 [00:28<00:17, 19443.06it/s] 62%|██████▏   | 561805/900675 [00:28<00:17, 19625.00it/s] 63%|██████▎   | 563801/900675 [00:28<00:17, 19718.70it/s] 63%|██████▎   | 565808/900675 [00:28<00:16, 19822.37it/s] 63%|██████▎   | 567792/900675 [00:28<00:16, 19670.96it/s] 63%|██████▎   | 569839/900675 [00:28<00:16, 19904.10it/s] 64%|██████▎   | 572095/900675 [00:28<00:15, 20693.85it/s] 64%|██████▎   | 574166/900675 [00:28<00:16, 20228.56it/s] 64%|██████▍   | 576192/900675 [00:28<00:16, 20010.47it/s] 64%|██████▍   | 578196/900675 [00:29<00:16, 19595.68it/s] 64%|██████▍   | 580246/900675 [00:29<00:16, 19856.92it/s] 65%|██████▍   | 582275/900675 [00:29<00:15, 19981.59it/s] 65%|██████▍   | 584570/900675 [00:29<00:15, 20853.88it/s] 65%|██████▌   | 586659/900675 [00:29<00:15, 20600.86it/s] 65%|██████▌   | 588722/900675 [00:29<00:15, 19963.31it/s] 66%|██████▌   | 590777/900675 [00:29<00:15, 20129.52it/s] 66%|██████▌   | 592795/900675 [00:29<00:15, 19731.58it/s] 66%|██████▌   | 594807/900675 [00:29<00:15, 19842.98it/s] 66%|██████▋   | 596795/900675 [00:29<00:15, 19797.29it/s] 66%|██████▋   | 598777/900675 [00:30<00:15, 19725.59it/s] 67%|██████▋   | 600823/900675 [00:30<00:15, 19938.04it/s] 67%|██████▋   | 602819/900675 [00:30<00:15, 19335.47it/s] 67%|██████▋   | 604854/900675 [00:30<00:15, 19628.01it/s] 67%|██████▋   | 606831/900675 [00:30<00:14, 19668.54it/s] 68%|██████▊   | 608801/900675 [00:30<00:15, 19383.33it/s] 68%|██████▊   | 610936/900675 [00:30<00:14, 19960.58it/s] 68%|██████▊   | 612936/900675 [00:30<00:14, 19906.94it/s] 68%|██████▊   | 614929/900675 [00:30<00:14, 19453.05it/s] 69%|██████▊   | 617101/900675 [00:30<00:14, 20107.07it/s] 69%|██████▉   | 619274/900675 [00:31<00:13, 20584.27it/s] 69%|██████▉   | 621437/900675 [00:31<00:13, 20891.05it/s] 69%|██████▉   | 623609/900675 [00:31<00:13, 21128.73it/s] 69%|██████▉   | 625725/900675 [00:31<00:13, 20732.43it/s] 70%|██████▉   | 627802/900675 [00:31<00:13, 20354.45it/s] 70%|██████▉   | 629841/900675 [00:31<00:13, 20260.51it/s] 70%|███████   | 632024/900675 [00:31<00:12, 20717.96it/s] 70%|███████   | 634122/900675 [00:31<00:12, 20794.36it/s] 71%|███████   | 636220/900675 [00:31<00:12, 20843.44it/s] 71%|███████   | 638306/900675 [00:31<00:12, 20538.00it/s] 71%|███████   | 640362/900675 [00:32<00:12, 20203.85it/s] 71%|███████▏  | 642611/900675 [00:32<00:12, 20869.32it/s] 72%|███████▏  | 644701/900675 [00:32<00:12, 20007.43it/s] 72%|███████▏  | 646713/900675 [00:32<00:12, 20038.88it/s] 72%|███████▏  | 648723/900675 [00:32<00:13, 19253.34it/s] 72%|███████▏  | 650658/900675 [00:32<00:13, 18983.00it/s] 72%|███████▏  | 652635/900675 [00:32<00:12, 19204.55it/s] 73%|███████▎  | 654719/900675 [00:32<00:12, 19675.28it/s] 73%|███████▎  | 656692/900675 [00:32<00:12, 19676.36it/s] 73%|███████▎  | 658715/900675 [00:33<00:12, 19839.39it/s] 73%|███████▎  | 660796/900675 [00:33<00:11, 20123.54it/s] 74%|███████▎  | 662811/900675 [00:33<00:11, 20078.06it/s] 74%|███████▍  | 664821/900675 [00:33<00:11, 19837.69it/s] 74%|███████▍  | 666926/900675 [00:33<00:11, 20196.06it/s] 74%|███████▍  | 668980/900675 [00:33<00:11, 20294.83it/s] 75%|███████▍  | 671096/900675 [00:33<00:11, 20544.02it/s] 75%|███████▍  | 673194/900675 [00:33<00:11, 20671.93it/s] 75%|███████▍  | 675262/900675 [00:33<00:11, 20462.37it/s] 75%|███████▌  | 677310/900675 [00:33<00:11, 20237.20it/s] 75%|███████▌  | 679335/900675 [00:34<00:11, 19980.89it/s] 76%|███████▌  | 681399/900675 [00:34<00:10, 20174.55it/s] 76%|███████▌  | 683418/900675 [00:34<00:10, 20006.83it/s] 76%|███████▌  | 685538/900675 [00:34<00:10, 20356.15it/s] 76%|███████▋  | 687575/900675 [00:34<00:10, 20037.45it/s] 77%|███████▋  | 689581/900675 [00:34<00:10, 19829.43it/s] 77%|███████▋  | 691566/900675 [00:34<00:10, 19350.58it/s] 77%|███████▋  | 693531/900675 [00:34<00:10, 19437.40it/s] 77%|███████▋  | 695477/900675 [00:34<00:10, 19191.09it/s] 77%|███████▋  | 697489/900675 [00:34<00:10, 19457.58it/s] 78%|███████▊  | 699437/900675 [00:35<00:10, 19416.58it/s] 78%|███████▊  | 701494/900675 [00:35<00:10, 19754.60it/s] 78%|███████▊  | 703581/900675 [00:35<00:09, 20083.47it/s] 78%|███████▊  | 705591/900675 [00:35<00:09, 19641.13it/s] 79%|███████▊  | 707558/900675 [00:35<00:09, 19533.63it/s] 79%|███████▉  | 709674/900675 [00:35<00:09, 20012.13it/s] 79%|███████▉  | 711744/900675 [00:35<00:09, 20203.66it/s] 79%|███████▉  | 713852/900675 [00:35<00:09, 20459.44it/s] 79%|███████▉  | 715900/900675 [00:35<00:09, 20325.31it/s] 80%|███████▉  | 717934/900675 [00:35<00:09, 20141.56it/s] 80%|███████▉  | 719950/900675 [00:36<00:09, 19714.95it/s] 80%|████████  | 721958/900675 [00:36<00:09, 19821.09it/s] 80%|████████  | 723995/900675 [00:36<00:08, 19847.63it/s] 81%|████████  | 725982/900675 [00:36<00:08, 19612.74it/s] 81%|████████  | 728076/900675 [00:36<00:08, 20000.62it/s] 81%|████████  | 730078/900675 [00:36<00:08, 19433.60it/s] 81%|████████▏ | 732054/900675 [00:36<00:08, 19528.00it/s] 82%|████████▏ | 734199/900675 [00:36<00:08, 20091.39it/s] 82%|████████▏ | 736212/900675 [00:36<00:08, 19818.50it/s] 82%|████████▏ | 738378/900675 [00:37<00:07, 20359.00it/s] 82%|████████▏ | 740418/900675 [00:37<00:08, 20022.14it/s] 82%|████████▏ | 742424/900675 [00:37<00:08, 19697.91it/s] 83%|████████▎ | 744397/900675 [00:37<00:07, 19634.29it/s] 83%|████████▎ | 746392/900675 [00:37<00:07, 19724.26it/s] 83%|████████▎ | 748366/900675 [00:37<00:07, 19362.45it/s] 83%|████████▎ | 750381/900675 [00:37<00:07, 19585.89it/s] 84%|████████▎ | 752342/900675 [00:37<00:07, 19428.43it/s] 84%|████████▍ | 754399/900675 [00:37<00:07, 19763.40it/s] 84%|████████▍ | 756377/900675 [00:37<00:07, 19174.01it/s] 84%|████████▍ | 758449/900675 [00:38<00:07, 19617.63it/s] 84%|████████▍ | 760553/900675 [00:38<00:06, 20031.81it/s] 85%|████████▍ | 762561/900675 [00:38<00:06, 19952.90it/s] 85%|████████▍ | 764560/900675 [00:38<00:06, 19681.59it/s] 85%|████████▌ | 766531/900675 [00:38<00:06, 19661.24it/s] 85%|████████▌ | 768511/900675 [00:38<00:06, 19695.33it/s] 86%|████████▌ | 770482/900675 [00:38<00:06, 19234.60it/s] 86%|████████▌ | 772414/900675 [00:38<00:06, 19258.19it/s] 86%|████████▌ | 774507/900675 [00:38<00:06, 19738.74it/s] 86%|████████▌ | 776484/900675 [00:38<00:06, 19426.33it/s] 86%|████████▋ | 778494/900675 [00:39<00:06, 19621.22it/s] 87%|████████▋ | 780459/900675 [00:39<00:06, 19329.70it/s] 87%|████████▋ | 782515/900675 [00:39<00:06, 19690.17it/s] 87%|████████▋ | 784487/900675 [00:39<00:06, 19229.17it/s] 87%|████████▋ | 786539/900675 [00:39<00:05, 19597.33it/s] 88%|████████▊ | 788503/900675 [00:39<00:05, 19504.13it/s] 88%|████████▊ | 790556/900675 [00:39<00:05, 19805.99it/s] 88%|████████▊ | 792539/900675 [00:39<00:05, 19454.56it/s] 88%|████████▊ | 794487/900675 [00:39<00:05, 19447.25it/s] 88%|████████▊ | 796550/900675 [00:39<00:05, 19794.40it/s] 89%|████████▊ | 798532/900675 [00:40<00:05, 19787.18it/s] 89%|████████▉ | 800512/900675 [00:40<00:05, 19771.68it/s] 89%|████████▉ | 802491/900675 [00:40<00:05, 19458.47it/s] 89%|████████▉ | 804504/900675 [00:40<00:04, 19655.86it/s] 90%|████████▉ | 806471/900675 [00:40<00:04, 19638.67it/s] 90%|████████▉ | 808436/900675 [00:40<00:04, 19143.51it/s] 90%|████████▉ | 810390/900675 [00:40<00:04, 19258.99it/s] 90%|█████████ | 812319/900675 [00:40<00:04, 18951.59it/s] 90%|█████████ | 814505/900675 [00:40<00:04, 19803.58it/s] 91%|█████████ | 816763/900675 [00:41<00:04, 20622.09it/s] 91%|█████████ | 818854/900675 [00:41<00:03, 20701.54it/s] 91%|█████████ | 820951/900675 [00:41<00:03, 20775.00it/s] 91%|█████████▏| 823031/900675 [00:41<00:03, 20261.18it/s] 92%|█████████▏| 825062/900675 [00:41<00:03, 19852.01it/s] 92%|█████████▏| 827052/900675 [00:41<00:03, 19610.78it/s] 92%|█████████▏| 829016/900675 [00:41<00:03, 19106.51it/s] 92%|█████████▏| 830956/900675 [00:41<00:03, 19188.44it/s] 92%|█████████▏| 832952/900675 [00:41<00:03, 19411.50it/s] 93%|█████████▎| 835010/900675 [00:41<00:03, 19754.41it/s] 93%|█████████▎| 837020/900675 [00:42<00:03, 19850.35it/s] 93%|█████████▎| 839018/900675 [00:42<00:03, 19884.16it/s] 93%|█████████▎| 841050/900675 [00:42<00:02, 20011.74it/s] 94%|█████████▎| 843053/900675 [00:42<00:02, 19624.73it/s] 94%|█████████▍| 845018/900675 [00:42<00:02, 18713.84it/s] 94%|█████████▍| 846954/900675 [00:42<00:02, 18898.38it/s] 94%|█████████▍| 848976/900675 [00:42<00:02, 19278.63it/s] 94%|█████████▍| 850911/900675 [00:42<00:02, 19077.31it/s] 95%|█████████▍| 852824/900675 [00:42<00:02, 19027.21it/s] 95%|█████████▍| 854822/900675 [00:42<00:02, 19304.39it/s] 95%|█████████▌| 856756/900675 [00:43<00:02, 19253.81it/s] 95%|█████████▌| 858696/900675 [00:43<00:02, 19296.80it/s] 96%|█████████▌| 860854/900675 [00:43<00:01, 19969.32it/s] 96%|█████████▌| 862853/900675 [00:43<00:01, 19933.76it/s] 96%|█████████▌| 864848/900675 [00:43<00:01, 19879.89it/s] 96%|█████████▌| 866837/900675 [00:43<00:01, 19687.40it/s] 96%|█████████▋| 868807/900675 [00:43<00:01, 19612.45it/s] 97%|█████████▋| 870878/900675 [00:43<00:01, 19931.91it/s] 97%|█████████▋| 872872/900675 [00:43<00:01, 19833.21it/s] 97%|█████████▋| 874973/900675 [00:43<00:01, 20180.19it/s] 97%|█████████▋| 876992/900675 [00:44<00:01, 20094.54it/s] 98%|█████████▊| 879002/900675 [00:44<00:01, 19987.41it/s] 98%|█████████▊| 881002/900675 [00:44<00:01, 19525.21it/s] 98%|█████████▊| 882957/900675 [00:44<00:00, 19185.75it/s] 98%|█████████▊| 884890/900675 [00:44<00:00, 19226.68it/s] 98%|█████████▊| 887061/900675 [00:44<00:00, 19956.21it/s] 99%|█████████▊| 889060/900675 [00:44<00:00, 19667.97it/s] 99%|█████████▉| 891130/900675 [00:44<00:00, 19970.06it/s] 99%|█████████▉| 893130/900675 [00:44<00:00, 19428.10it/s] 99%|█████████▉| 895078/900675 [00:45<00:00, 19123.27it/s]100%|█████████▉| 897093/900675 [00:45<00:00, 19413.87it/s]100%|█████████▉| 899185/900675 [00:45<00:00, 19854.37it/s]100%|██████████| 900675/900675 [00:45<00:00, 19888.28it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 12.01it/s]2022-02-25 10:48:27 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-25 10:48:27 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-25 10:48:27 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-25 10:48:27 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-02-25 10:48:27 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-25 10:48:27 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-25 10:48:27 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-25 10:48:27 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-25 10:48:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 10:48:27 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-02-25 10:48:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 10:48:27 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-25 10:48:27 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-02-25 10:48:27 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_last.pt
2022-02-25 10:48:27 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_last.pt
2022-02-25 10:48:27 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-25 10:48:27 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-25 10:48:28 | INFO | fairseq.trainer | begin training epoch 1
2022-02-25 10:48:28 | INFO | fairseq_cli.train | Start iterating over samples

2022-02-25 10:48:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-25 10:48:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 10:49:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 10:49:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 10:49:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-25 11:00:32 | INFO | train_inner | epoch 001:    105 / 788 loss=17.525, ppl=188661, wps=10173.6, ups=0.16, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.594, loss_scale=4, train_wall=719, gb_free=4, wall=725
2022-02-25 11:11:16 | INFO | train_inner | epoch 001:    205 / 788 loss=14.966, ppl=32004.8, wps=10178.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.486, loss_scale=4, train_wall=639, gb_free=4, wall=1369
2022-02-25 11:22:00 | INFO | train_inner | epoch 001:    305 / 788 loss=12.87, ppl=7486.2, wps=10183, ups=0.16, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.011, loss_scale=4, train_wall=639, gb_free=4, wall=2012
2022-02-25 11:32:43 | INFO | train_inner | epoch 001:    405 / 788 loss=11.261, ppl=2453.57, wps=10185.1, ups=0.16, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.657, loss_scale=4, train_wall=639, gb_free=4, wall=2656
2022-02-25 11:43:26 | INFO | train_inner | epoch 001:    505 / 788 loss=10.597, ppl=1548.84, wps=10188.4, ups=0.16, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.468, loss_scale=4, train_wall=638, gb_free=4, wall=3299
2022-02-25 11:54:10 | INFO | train_inner | epoch 001:    605 / 788 loss=10.263, ppl=1228.72, wps=10184.7, ups=0.16, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.576, loss_scale=8, train_wall=639, gb_free=4, wall=3943
2022-02-25 12:04:54 | INFO | train_inner | epoch 001:    705 / 788 loss=9.991, ppl=1017.82, wps=10179.9, ups=0.16, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.628, loss_scale=8, train_wall=639, gb_free=4, wall=4586
2022-02-25 12:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 12:13:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.598 | ppl 775.15 | wps 23550 | wpb 2034.1 | bsz 4 | num_updates 783
2022-02-25 12:13:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 783 updates
2022-02-25 12:13:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 12:14:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 12:14:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 1 @ 783 updates, score 9.598) (writing took 7.093699692981318 seconds)
2022-02-25 12:14:01 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-25 12:14:01 | INFO | train | epoch 001 | loss 12.21 | ppl 4736.9 | wps 10148.7 | ups 0.15 | wpb 65497.3 | bsz 127.9 | num_updates 783 | lr 9.79554e-05 | gnorm 1.152 | loss_scale 8 | train_wall 5079 | gb_free 4 | wall 5134
2022-02-25 12:14:01 | INFO | fairseq.trainer | begin training epoch 2
2022-02-25 12:14:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 12:15:51 | INFO | train_inner | epoch 002:     17 / 788 loss=9.759, ppl=866.45, wps=9925.9, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.707, loss_scale=8, train_wall=636, gb_free=4, wall=5244
2022-02-25 12:26:34 | INFO | train_inner | epoch 002:    117 / 788 loss=9.542, ppl=745.21, wps=10181.2, ups=0.16, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.821, loss_scale=8, train_wall=639, gb_free=4, wall=5887
2022-02-25 12:37:18 | INFO | train_inner | epoch 002:    217 / 788 loss=9.345, ppl=650.18, wps=10179.6, ups=0.16, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.882, loss_scale=8, train_wall=639, gb_free=4, wall=6531
2022-02-25 12:48:02 | INFO | train_inner | epoch 002:    317 / 788 loss=9.183, ppl=581.14, wps=10182.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.882, loss_scale=16, train_wall=639, gb_free=4, wall=7175
2022-02-25 12:58:45 | INFO | train_inner | epoch 002:    417 / 788 loss=9.028, ppl=522.18, wps=10181.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.859, loss_scale=16, train_wall=639, gb_free=4, wall=7818
2022-02-25 13:09:29 | INFO | train_inner | epoch 002:    517 / 788 loss=8.885, ppl=472.87, wps=10187.8, ups=0.16, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.889, loss_scale=16, train_wall=638, gb_free=4, wall=8461
2022-02-25 13:20:12 | INFO | train_inner | epoch 002:    617 / 788 loss=8.749, ppl=430.29, wps=10185.2, ups=0.16, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.894, loss_scale=16, train_wall=639, gb_free=4, wall=9105
2022-02-25 13:30:55 | INFO | train_inner | epoch 002:    717 / 788 loss=8.615, ppl=392.15, wps=10186.7, ups=0.16, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.893, loss_scale=16, train_wall=639, gb_free=4, wall=9748
2022-02-25 13:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 13:38:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.331 | ppl 322.05 | wps 23519.5 | wpb 2034.1 | bsz 4 | num_updates 1571 | best_loss 8.331
2022-02-25 13:38:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1571 updates
2022-02-25 13:38:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 13:38:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 13:38:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 2 @ 1571 updates, score 8.331) (writing took 6.581468869931996 seconds)
2022-02-25 13:38:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-25 13:38:45 | INFO | train | epoch 002 | loss 9.015 | ppl 517.32 | wps 10151.6 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 1571 | lr 0.000196436 | gnorm 0.872 | loss_scale 32 | train_wall 5030 | gb_free 4 | wall 10218
2022-02-25 13:38:45 | INFO | fairseq.trainer | begin training epoch 3
2022-02-25 13:38:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 13:41:52 | INFO | train_inner | epoch 003:     29 / 788 loss=8.488, ppl=359.16, wps=9936.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.891, loss_scale=32, train_wall=636, gb_free=4, wall=10405
2022-02-25 13:52:36 | INFO | train_inner | epoch 003:    129 / 788 loss=8.355, ppl=327.34, wps=10180.8, ups=0.16, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.889, loss_scale=32, train_wall=639, gb_free=4, wall=11049
2022-02-25 14:03:20 | INFO | train_inner | epoch 003:    229 / 788 loss=8.249, ppl=304.12, wps=10179.6, ups=0.16, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.891, loss_scale=32, train_wall=639, gb_free=4, wall=11692
2022-02-25 14:14:03 | INFO | train_inner | epoch 003:    329 / 788 loss=8.165, ppl=286.92, wps=10179.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.866, loss_scale=32, train_wall=639, gb_free=4, wall=12336
2022-02-25 14:24:47 | INFO | train_inner | epoch 003:    429 / 788 loss=8.057, ppl=266.32, wps=10179.9, ups=0.16, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.831, loss_scale=32, train_wall=639, gb_free=4, wall=12980
2022-02-25 14:30:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 14:35:37 | INFO | train_inner | epoch 003:    530 / 788 loss=7.991, ppl=254.47, wps=10079.4, ups=0.15, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.848, loss_scale=32, train_wall=645, gb_free=4, wall=13630
2022-02-25 14:46:21 | INFO | train_inner | epoch 003:    630 / 788 loss=7.909, ppl=240.33, wps=10178.8, ups=0.16, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.805, loss_scale=32, train_wall=639, gb_free=4, wall=14274
2022-02-25 14:57:05 | INFO | train_inner | epoch 003:    730 / 788 loss=7.823, ppl=226.43, wps=10176.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.811, loss_scale=32, train_wall=639, gb_free=4, wall=14918
2022-02-25 15:03:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 15:03:25 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.598 | ppl 193.72 | wps 23583.4 | wpb 2034.1 | bsz 4 | num_updates 2358 | best_loss 7.598
2022-02-25 15:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2358 updates
2022-02-25 15:03:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 15:03:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 15:03:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 3 @ 2358 updates, score 7.598) (writing took 6.30009081796743 seconds)
2022-02-25 15:03:31 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-25 15:03:31 | INFO | train | epoch 003 | loss 8.068 | ppl 268.34 | wps 10135.3 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 2358 | lr 0.000294791 | gnorm 0.846 | loss_scale 32 | train_wall 5032 | gb_free 4 | wall 15304
2022-02-25 15:03:31 | INFO | fairseq.trainer | begin training epoch 4
2022-02-25 15:03:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 15:08:02 | INFO | train_inner | epoch 004:     42 / 788 loss=7.717, ppl=210.33, wps=9936.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.811, loss_scale=32, train_wall=636, gb_free=4, wall=15574
2022-02-25 15:18:46 | INFO | train_inner | epoch 004:    142 / 788 loss=7.615, ppl=196.07, wps=10179.4, ups=0.16, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.783, loss_scale=32, train_wall=639, gb_free=4, wall=16218
2022-02-25 15:26:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 15:29:36 | INFO | train_inner | epoch 004:    243 / 788 loss=7.565, ppl=189.33, wps=10079.2, ups=0.15, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.789, loss_scale=32, train_wall=645, gb_free=4, wall=16869
2022-02-25 15:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 15:40:26 | INFO | train_inner | epoch 004:    344 / 788 loss=7.508, ppl=182.05, wps=10080.7, ups=0.15, wpb=65520.6, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.774, loss_scale=16, train_wall=645, gb_free=4, wall=17518
2022-02-25 15:51:09 | INFO | train_inner | epoch 004:    444 / 788 loss=7.456, ppl=175.59, wps=10186.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.756, loss_scale=16, train_wall=638, gb_free=4, wall=18162
2022-02-25 16:01:52 | INFO | train_inner | epoch 004:    544 / 788 loss=7.399, ppl=168.82, wps=10188.4, ups=0.16, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.761, loss_scale=16, train_wall=638, gb_free=4, wall=18805
2022-02-25 16:12:36 | INFO | train_inner | epoch 004:    644 / 788 loss=7.343, ppl=162.32, wps=10184.7, ups=0.16, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.735, loss_scale=16, train_wall=639, gb_free=4, wall=19449
2022-02-25 16:23:19 | INFO | train_inner | epoch 004:    744 / 788 loss=7.291, ppl=156.6, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.747, loss_scale=16, train_wall=639, gb_free=4, wall=20092
2022-02-25 16:27:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 16:28:09 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.154 | ppl 142.39 | wps 23551.4 | wpb 2034.1 | bsz 4 | num_updates 3144 | best_loss 7.154
2022-02-25 16:28:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3144 updates
2022-02-25 16:28:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 16:28:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 16:28:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 4 @ 3144 updates, score 7.154) (writing took 6.3558825419750065 seconds)
2022-02-25 16:28:15 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-25 16:28:15 | INFO | train | epoch 004 | loss 7.454 | ppl 175.28 | wps 10126.3 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 3144 | lr 0.000393021 | gnorm 0.765 | loss_scale 16 | train_wall 5030 | gb_free 4 | wall 20388
2022-02-25 16:28:15 | INFO | fairseq.trainer | begin training epoch 5
2022-02-25 16:28:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 16:34:16 | INFO | train_inner | epoch 005:     56 / 788 loss=7.19, ppl=146.04, wps=9938.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.722, loss_scale=32, train_wall=636, gb_free=4, wall=20748
2022-02-25 16:36:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 16:45:06 | INFO | train_inner | epoch 005:    157 / 788 loss=7.116, ppl=138.73, wps=10082, ups=0.15, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.737, loss_scale=16, train_wall=645, gb_free=4, wall=21398
2022-02-25 16:55:49 | INFO | train_inner | epoch 005:    257 / 788 loss=7.083, ppl=135.61, wps=10185.2, ups=0.16, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.697, loss_scale=16, train_wall=639, gb_free=4, wall=22042
2022-02-25 17:06:32 | INFO | train_inner | epoch 005:    357 / 788 loss=7.063, ppl=133.69, wps=10186.9, ups=0.16, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.704, loss_scale=16, train_wall=638, gb_free=4, wall=22685
2022-02-25 17:17:16 | INFO | train_inner | epoch 005:    457 / 788 loss=7.021, ppl=129.9, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.696, loss_scale=16, train_wall=638, gb_free=4, wall=23329
2022-02-25 17:27:59 | INFO | train_inner | epoch 005:    557 / 788 loss=6.999, ppl=127.89, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.706, loss_scale=16, train_wall=639, gb_free=4, wall=23972
2022-02-25 17:31:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 17:38:49 | INFO | train_inner | epoch 005:    658 / 788 loss=6.96, ppl=124.53, wps=10087.2, ups=0.15, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.675, loss_scale=16, train_wall=645, gb_free=4, wall=24622
2022-02-25 17:49:32 | INFO | train_inner | epoch 005:    758 / 788 loss=6.947, ppl=123.37, wps=10184.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.659, loss_scale=16, train_wall=639, gb_free=4, wall=25265
2022-02-25 17:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 17:52:51 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.86 | ppl 116.13 | wps 23550.1 | wpb 2034.1 | bsz 4 | num_updates 3930 | best_loss 6.86
2022-02-25 17:52:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3930 updates
2022-02-25 17:52:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 17:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 17:52:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 5 @ 3930 updates, score 6.86) (writing took 6.302467894041911 seconds)
2022-02-25 17:52:58 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-25 17:52:58 | INFO | train | epoch 005 | loss 7.032 | ppl 130.85 | wps 10128.7 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 3930 | lr 0.000491252 | gnorm 0.697 | loss_scale 16 | train_wall 5029 | gb_free 4 | wall 25471
2022-02-25 17:52:58 | INFO | fairseq.trainer | begin training epoch 6
2022-02-25 17:52:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 18:00:28 | INFO | train_inner | epoch 006:     70 / 788 loss=6.822, ppl=113.14, wps=9945.3, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.674, loss_scale=16, train_wall=635, gb_free=4, wall=25921
2022-02-25 18:11:11 | INFO | train_inner | epoch 006:    170 / 788 loss=6.779, ppl=109.84, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.654, loss_scale=16, train_wall=638, gb_free=4, wall=26564
2022-02-25 18:21:55 | INFO | train_inner | epoch 006:    270 / 788 loss=6.766, ppl=108.84, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.643, loss_scale=16, train_wall=638, gb_free=4, wall=27208
2022-02-25 18:29:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 18:32:45 | INFO | train_inner | epoch 006:    371 / 788 loss=6.733, ppl=106.41, wps=10083, ups=0.15, wpb=65534.7, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.613, loss_scale=16, train_wall=645, gb_free=4, wall=27858
2022-02-25 18:43:28 | INFO | train_inner | epoch 006:    471 / 788 loss=6.72, ppl=105.42, wps=10187.2, ups=0.16, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.618, loss_scale=16, train_wall=638, gb_free=4, wall=28501
2022-02-25 18:54:11 | INFO | train_inner | epoch 006:    571 / 788 loss=6.7, ppl=103.98, wps=10186.4, ups=0.16, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.61, loss_scale=16, train_wall=638, gb_free=4, wall=29144
2022-02-25 19:04:55 | INFO | train_inner | epoch 006:    671 / 788 loss=6.689, ppl=103.16, wps=10187.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.596, loss_scale=16, train_wall=638, gb_free=4, wall=29787
2022-02-25 19:15:38 | INFO | train_inner | epoch 006:    771 / 788 loss=6.67, ppl=101.8, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.587, loss_scale=16, train_wall=639, gb_free=4, wall=30431
2022-02-25 19:17:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 19:17:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.636 | ppl 99.44 | wps 23585.4 | wpb 2034.1 | bsz 4 | num_updates 4717 | best_loss 6.636
2022-02-25 19:17:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4717 updates
2022-02-25 19:17:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 19:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 19:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 6 @ 4717 updates, score 6.636) (writing took 6.285072216996923 seconds)
2022-02-25 19:17:40 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-25 19:17:40 | INFO | train | epoch 006 | loss 6.725 | ppl 105.77 | wps 10142.3 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 4717 | lr 0.000460434 | gnorm 0.62 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 30553
2022-02-25 19:17:40 | INFO | fairseq.trainer | begin training epoch 7
2022-02-25 19:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 19:26:34 | INFO | train_inner | epoch 007:     83 / 788 loss=6.513, ppl=91.36, wps=9940.1, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.584, loss_scale=32, train_wall=636, gb_free=4, wall=31087
2022-02-25 19:30:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 19:37:24 | INFO | train_inner | epoch 007:    184 / 788 loss=6.518, ppl=91.67, wps=10080.3, ups=0.15, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.578, loss_scale=16, train_wall=645, gb_free=4, wall=31737
2022-02-25 19:48:08 | INFO | train_inner | epoch 007:    284 / 788 loss=6.503, ppl=90.73, wps=10183.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.574, loss_scale=16, train_wall=639, gb_free=4, wall=32381
2022-02-25 19:58:51 | INFO | train_inner | epoch 007:    384 / 788 loss=6.503, ppl=90.67, wps=10189.1, ups=0.16, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.564, loss_scale=16, train_wall=638, gb_free=4, wall=33024
2022-02-25 20:09:34 | INFO | train_inner | epoch 007:    484 / 788 loss=6.499, ppl=90.45, wps=10188.4, ups=0.16, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.561, loss_scale=16, train_wall=638, gb_free=4, wall=33667
2022-02-25 20:20:18 | INFO | train_inner | epoch 007:    584 / 788 loss=6.479, ppl=89.22, wps=10185, ups=0.16, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.559, loss_scale=16, train_wall=639, gb_free=4, wall=34311
2022-02-25 20:25:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 20:26:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 20:31:14 | INFO | train_inner | epoch 007:    686 / 788 loss=6.478, ppl=89.15, wps=9986.3, ups=0.15, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.563, loss_scale=8, train_wall=651, gb_free=4, wall=34967
2022-02-25 20:41:57 | INFO | train_inner | epoch 007:    786 / 788 loss=6.472, ppl=88.77, wps=10187.6, ups=0.16, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.559, loss_scale=8, train_wall=638, gb_free=4, wall=35610
2022-02-25 20:42:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 20:42:17 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.509 | ppl 91.08 | wps 23550 | wpb 2034.1 | bsz 4 | num_updates 5502 | best_loss 6.509
2022-02-25 20:42:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5502 updates
2022-02-25 20:42:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 20:42:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 20:42:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 7 @ 5502 updates, score 6.509) (writing took 6.4691119629424065 seconds)
2022-02-25 20:42:23 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-25 20:42:23 | INFO | train | epoch 007 | loss 6.493 | ppl 90.08 | wps 10115 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 5502 | lr 0.000426324 | gnorm 0.568 | loss_scale 8 | train_wall 5029 | gb_free 4 | wall 35636
2022-02-25 20:42:23 | INFO | fairseq.trainer | begin training epoch 8
2022-02-25 20:42:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 20:52:54 | INFO | train_inner | epoch 008:     98 / 788 loss=6.314, ppl=79.58, wps=9942.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=5600, lr=0.000422577, gnorm=0.553, loss_scale=8, train_wall=636, gb_free=4, wall=36266
2022-02-25 21:03:37 | INFO | train_inner | epoch 008:    198 / 788 loss=6.326, ppl=80.21, wps=10186.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.549, loss_scale=8, train_wall=638, gb_free=4, wall=36910
2022-02-25 21:14:20 | INFO | train_inner | epoch 008:    298 / 788 loss=6.33, ppl=80.44, wps=10186.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.539, loss_scale=8, train_wall=638, gb_free=4, wall=37553
2022-02-25 21:25:04 | INFO | train_inner | epoch 008:    398 / 788 loss=6.331, ppl=80.52, wps=10185.8, ups=0.16, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.548, loss_scale=16, train_wall=639, gb_free=4, wall=38196
2022-02-25 21:35:47 | INFO | train_inner | epoch 008:    498 / 788 loss=6.331, ppl=80.52, wps=10188.7, ups=0.16, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.529, loss_scale=16, train_wall=638, gb_free=4, wall=38840
2022-02-25 21:46:30 | INFO | train_inner | epoch 008:    598 / 788 loss=6.342, ppl=81.12, wps=10189.8, ups=0.16, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.543, loss_scale=16, train_wall=638, gb_free=4, wall=39483
2022-02-25 21:57:13 | INFO | train_inner | epoch 008:    698 / 788 loss=6.329, ppl=80.38, wps=10186.6, ups=0.16, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.536, loss_scale=16, train_wall=638, gb_free=4, wall=40126
2022-02-25 22:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 22:06:59 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.411 | ppl 85.12 | wps 23577.4 | wpb 2034.1 | bsz 4 | num_updates 6290 | best_loss 6.411
2022-02-25 22:06:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6290 updates
2022-02-25 22:06:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 22:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 22:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 8 @ 6290 updates, score 6.411) (writing took 6.289180773077533 seconds)
2022-02-25 22:07:05 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-25 22:07:05 | INFO | train | epoch 008 | loss 6.33 | ppl 80.45 | wps 10156 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 6290 | lr 0.000398726 | gnorm 0.543 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 40718
2022-02-25 22:07:05 | INFO | fairseq.trainer | begin training epoch 9
2022-02-25 22:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 22:08:10 | INFO | train_inner | epoch 009:     10 / 788 loss=6.324, ppl=80.14, wps=9943, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.545, loss_scale=16, train_wall=636, gb_free=4, wall=40782
2022-02-25 22:17:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 22:18:59 | INFO | train_inner | epoch 009:    111 / 788 loss=6.177, ppl=72.37, wps=10085.5, ups=0.15, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.538, loss_scale=16, train_wall=645, gb_free=4, wall=41432
2022-02-25 22:29:43 | INFO | train_inner | epoch 009:    211 / 788 loss=6.186, ppl=72.8, wps=10186.7, ups=0.16, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.543, loss_scale=16, train_wall=638, gb_free=4, wall=42076
2022-02-25 22:40:26 | INFO | train_inner | epoch 009:    311 / 788 loss=6.208, ppl=73.93, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.534, loss_scale=16, train_wall=639, gb_free=4, wall=42719
2022-02-25 22:51:09 | INFO | train_inner | epoch 009:    411 / 788 loss=6.209, ppl=74, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.56, loss_scale=16, train_wall=638, gb_free=4, wall=43362
2022-02-25 23:01:53 | INFO | train_inner | epoch 009:    511 / 788 loss=6.209, ppl=73.96, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.523, loss_scale=16, train_wall=638, gb_free=4, wall=44006
2022-02-25 23:12:36 | INFO | train_inner | epoch 009:    611 / 788 loss=6.225, ppl=74.81, wps=10189.7, ups=0.16, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.538, loss_scale=32, train_wall=638, gb_free=4, wall=44649
2022-02-25 23:13:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 23:23:26 | INFO | train_inner | epoch 009:    712 / 788 loss=6.223, ppl=74.71, wps=10085.7, ups=0.15, wpb=65520.6, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.525, loss_scale=16, train_wall=645, gb_free=4, wall=45298
2022-02-25 23:31:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 23:31:41 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.366 | ppl 82.47 | wps 23498.6 | wpb 2034.1 | bsz 4 | num_updates 7076 | best_loss 6.366
2022-02-25 23:31:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7076 updates
2022-02-25 23:31:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 23:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-25 23:31:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 9 @ 7076 updates, score 6.366) (writing took 6.218588524032384 seconds)
2022-02-25 23:31:47 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-25 23:31:47 | INFO | train | epoch 009 | loss 6.206 | ppl 73.84 | wps 10130 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 7076 | lr 0.000375929 | gnorm 0.537 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 45800
2022-02-25 23:31:47 | INFO | fairseq.trainer | begin training epoch 10
2022-02-25 23:31:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 23:34:22 | INFO | train_inner | epoch 010:     24 / 788 loss=6.184, ppl=72.69, wps=9944.3, ups=0.15, wpb=65248, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.544, loss_scale=16, train_wall=636, gb_free=4, wall=45954
2022-02-25 23:45:05 | INFO | train_inner | epoch 010:    124 / 788 loss=6.07, ppl=67.2, wps=10185.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.519, loss_scale=16, train_wall=638, gb_free=4, wall=46598
2022-02-25 23:55:48 | INFO | train_inner | epoch 010:    224 / 788 loss=6.09, ppl=68.11, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.555, loss_scale=16, train_wall=638, gb_free=4, wall=47241
2022-02-26 00:06:32 | INFO | train_inner | epoch 010:    324 / 788 loss=6.1, ppl=68.61, wps=10185.4, ups=0.16, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.532, loss_scale=16, train_wall=639, gb_free=4, wall=47885
2022-02-26 00:10:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 00:17:22 | INFO | train_inner | epoch 010:    425 / 788 loss=6.117, ppl=69.43, wps=10084.2, ups=0.15, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.524, loss_scale=16, train_wall=645, gb_free=4, wall=48534
2022-02-26 00:28:05 | INFO | train_inner | epoch 010:    525 / 788 loss=6.114, ppl=69.28, wps=10188.4, ups=0.16, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.558, loss_scale=16, train_wall=638, gb_free=4, wall=49178
2022-02-26 00:38:48 | INFO | train_inner | epoch 010:    625 / 788 loss=6.128, ppl=69.92, wps=10186.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.527, loss_scale=16, train_wall=638, gb_free=4, wall=49821
2022-02-26 00:49:31 | INFO | train_inner | epoch 010:    725 / 788 loss=6.137, ppl=70.39, wps=10188.9, ups=0.16, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.524, loss_scale=16, train_wall=638, gb_free=4, wall=50464
2022-02-26 00:56:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 00:56:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.305 | ppl 79.08 | wps 23483.9 | wpb 2034.1 | bsz 4 | num_updates 7863 | best_loss 6.305
2022-02-26 00:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7863 updates
2022-02-26 00:56:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 00:56:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 00:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 10 @ 7863 updates, score 6.305) (writing took 6.288557829102501 seconds)
2022-02-26 00:56:29 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-26 00:56:29 | INFO | train | epoch 010 | loss 6.109 | ppl 69.03 | wps 10142.4 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 7863 | lr 0.00035662 | gnorm 0.534 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 50882
2022-02-26 00:56:29 | INFO | fairseq.trainer | begin training epoch 11
2022-02-26 00:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 01:00:28 | INFO | train_inner | epoch 011:     37 / 788 loss=6.073, ppl=67.34, wps=9944.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.518, loss_scale=16, train_wall=636, gb_free=4, wall=51120
2022-02-26 01:08:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 01:11:17 | INFO | train_inner | epoch 011:    138 / 788 loss=5.981, ppl=63.17, wps=10083.7, ups=0.15, wpb=65534.7, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.532, loss_scale=16, train_wall=645, gb_free=4, wall=51770
2022-02-26 01:22:01 | INFO | train_inner | epoch 011:    238 / 788 loss=6.001, ppl=64.06, wps=10185.8, ups=0.16, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.522, loss_scale=16, train_wall=638, gb_free=4, wall=52414
2022-02-26 01:32:44 | INFO | train_inner | epoch 011:    338 / 788 loss=6.034, ppl=65.54, wps=10187.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.539, loss_scale=16, train_wall=638, gb_free=4, wall=53057
2022-02-26 01:43:27 | INFO | train_inner | epoch 011:    438 / 788 loss=6.034, ppl=65.54, wps=10187.2, ups=0.16, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.522, loss_scale=16, train_wall=638, gb_free=4, wall=53700
2022-02-26 01:54:11 | INFO | train_inner | epoch 011:    538 / 788 loss=6.038, ppl=65.7, wps=10188.7, ups=0.16, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.539, loss_scale=16, train_wall=638, gb_free=4, wall=54343
2022-02-26 02:03:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 02:05:00 | INFO | train_inner | epoch 011:    639 / 788 loss=6.055, ppl=66.48, wps=10086.8, ups=0.15, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.523, loss_scale=16, train_wall=645, gb_free=4, wall=54993
2022-02-26 02:15:43 | INFO | train_inner | epoch 011:    739 / 788 loss=6.062, ppl=66.81, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.526, loss_scale=16, train_wall=638, gb_free=4, wall=55636
2022-02-26 02:20:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 02:21:05 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.276 | ppl 77.51 | wps 23528.6 | wpb 2034.1 | bsz 4 | num_updates 8649 | best_loss 6.276
2022-02-26 02:21:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8649 updates
2022-02-26 02:21:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 02:21:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 02:21:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 11 @ 8649 updates, score 6.276) (writing took 6.3021308339666575 seconds)
2022-02-26 02:21:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-26 02:21:12 | INFO | train | epoch 011 | loss 6.029 | ppl 65.31 | wps 10129.9 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 8649 | lr 0.00034003 | gnorm 0.533 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 55964
2022-02-26 02:21:12 | INFO | fairseq.trainer | begin training epoch 12
2022-02-26 02:21:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 02:26:40 | INFO | train_inner | epoch 012:     51 / 788 loss=5.981, ppl=63.17, wps=9943.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.573, loss_scale=16, train_wall=636, gb_free=4, wall=56292
2022-02-26 02:37:23 | INFO | train_inner | epoch 012:    151 / 788 loss=5.917, ppl=60.44, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.53, loss_scale=16, train_wall=639, gb_free=4, wall=56936
2022-02-26 02:48:06 | INFO | train_inner | epoch 012:    251 / 788 loss=5.944, ppl=61.57, wps=10187.2, ups=0.16, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.527, loss_scale=16, train_wall=638, gb_free=4, wall=57579
2022-02-26 02:58:50 | INFO | train_inner | epoch 012:    351 / 788 loss=5.957, ppl=62.14, wps=10187.6, ups=0.16, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.565, loss_scale=16, train_wall=638, gb_free=4, wall=58222
2022-02-26 02:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 03:09:39 | INFO | train_inner | epoch 012:    452 / 788 loss=5.961, ppl=62.3, wps=10087.1, ups=0.15, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.535, loss_scale=16, train_wall=645, gb_free=4, wall=58872
2022-02-26 03:20:23 | INFO | train_inner | epoch 012:    552 / 788 loss=5.978, ppl=63.01, wps=10188.6, ups=0.16, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.548, loss_scale=16, train_wall=638, gb_free=4, wall=59515
2022-02-26 03:31:06 | INFO | train_inner | epoch 012:    652 / 788 loss=5.997, ppl=63.88, wps=10183.9, ups=0.16, wpb=65519.3, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.524, loss_scale=16, train_wall=639, gb_free=4, wall=60159
2022-02-26 03:41:49 | INFO | train_inner | epoch 012:    752 / 788 loss=5.998, ppl=63.91, wps=10188.2, ups=0.16, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.544, loss_scale=16, train_wall=638, gb_free=4, wall=60802
2022-02-26 03:45:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 03:45:47 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.247 | ppl 75.93 | wps 23531.8 | wpb 2034.1 | bsz 4 | num_updates 9436 | best_loss 6.247
2022-02-26 03:45:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9436 updates
2022-02-26 03:45:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 03:45:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 03:45:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 12 @ 9436 updates, score 6.247) (writing took 6.560220269020647 seconds)
2022-02-26 03:45:54 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-26 03:45:54 | INFO | train | epoch 012 | loss 5.962 | ppl 62.35 | wps 10142.3 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 9436 | lr 0.000325541 | gnorm 0.538 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 61047
2022-02-26 03:45:54 | INFO | fairseq.trainer | begin training epoch 13
2022-02-26 03:45:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 03:52:46 | INFO | train_inner | epoch 013:     64 / 788 loss=5.901, ppl=59.74, wps=9940.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.523, loss_scale=16, train_wall=636, gb_free=4, wall=61458
2022-02-26 03:54:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 04:03:35 | INFO | train_inner | epoch 013:    165 / 788 loss=5.87, ppl=58.47, wps=10085.9, ups=0.15, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.553, loss_scale=16, train_wall=645, gb_free=4, wall=62108
2022-02-26 04:14:19 | INFO | train_inner | epoch 013:    265 / 788 loss=5.886, ppl=59.12, wps=10186, ups=0.16, wpb=65534.7, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.555, loss_scale=16, train_wall=638, gb_free=4, wall=62752
2022-02-26 04:25:02 | INFO | train_inner | epoch 013:    365 / 788 loss=5.902, ppl=59.81, wps=10184.6, ups=0.16, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.557, loss_scale=16, train_wall=639, gb_free=4, wall=63395
2022-02-26 04:35:46 | INFO | train_inner | epoch 013:    465 / 788 loss=5.917, ppl=60.43, wps=10185.3, ups=0.16, wpb=65520.6, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.541, loss_scale=16, train_wall=638, gb_free=4, wall=64038
2022-02-26 04:46:29 | INFO | train_inner | epoch 013:    565 / 788 loss=5.932, ppl=61.07, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.53, loss_scale=16, train_wall=639, gb_free=4, wall=64682
2022-02-26 04:51:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 04:57:19 | INFO | train_inner | epoch 013:    666 / 788 loss=5.931, ppl=60.99, wps=10086.1, ups=0.15, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.523, loss_scale=16, train_wall=645, gb_free=4, wall=65331
2022-02-26 05:08:02 | INFO | train_inner | epoch 013:    766 / 788 loss=5.933, ppl=61.09, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.541, loss_scale=16, train_wall=638, gb_free=4, wall=65975
2022-02-26 05:10:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 05:10:30 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.233 | ppl 75.24 | wps 23526.8 | wpb 2034.1 | bsz 4 | num_updates 10222 | best_loss 6.233
2022-02-26 05:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10222 updates
2022-02-26 05:10:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 05:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 05:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 13 @ 10222 updates, score 6.233) (writing took 6.317635946208611 seconds)
2022-02-26 05:10:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 05:10:36 | INFO | train | epoch 013 | loss 5.905 | ppl 59.9 | wps 10129 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 10222 | lr 0.000312775 | gnorm 0.544 | loss_scale 16 | train_wall 5029 | gb_free 4 | wall 66129
2022-02-26 05:10:36 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 05:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 05:18:58 | INFO | train_inner | epoch 014:     78 / 788 loss=5.818, ppl=56.41, wps=9942.8, ups=0.15, wpb=65248, bsz=127.4, num_updates=10300, lr=0.000311588, gnorm=0.589, loss_scale=16, train_wall=636, gb_free=4, wall=66631
2022-02-26 05:29:42 | INFO | train_inner | epoch 014:    178 / 788 loss=5.815, ppl=56.32, wps=10184.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.53, loss_scale=16, train_wall=638, gb_free=4, wall=67274
2022-02-26 05:40:25 | INFO | train_inner | epoch 014:    278 / 788 loss=5.835, ppl=57.09, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.565, loss_scale=16, train_wall=638, gb_free=4, wall=67918
2022-02-26 05:46:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 05:51:15 | INFO | train_inner | epoch 014:    379 / 788 loss=5.858, ppl=58, wps=10087.3, ups=0.15, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.52, loss_scale=16, train_wall=645, gb_free=4, wall=68567
2022-02-26 06:01:58 | INFO | train_inner | epoch 014:    479 / 788 loss=5.86, ppl=58.07, wps=10188.7, ups=0.16, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.547, loss_scale=16, train_wall=638, gb_free=4, wall=69211
2022-02-26 06:12:41 | INFO | train_inner | epoch 014:    579 / 788 loss=5.881, ppl=58.92, wps=10188.3, ups=0.16, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.538, loss_scale=16, train_wall=638, gb_free=4, wall=69854
2022-02-26 06:23:24 | INFO | train_inner | epoch 014:    679 / 788 loss=5.887, ppl=59.19, wps=10187.7, ups=0.16, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.55, loss_scale=16, train_wall=638, gb_free=4, wall=70497
2022-02-26 06:34:08 | INFO | train_inner | epoch 014:    779 / 788 loss=5.895, ppl=59.52, wps=10187.1, ups=0.16, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.525, loss_scale=16, train_wall=638, gb_free=4, wall=71141
2022-02-26 06:35:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 06:35:12 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.217 | ppl 74.38 | wps 23551.3 | wpb 2034.1 | bsz 4 | num_updates 11009 | best_loss 6.217
2022-02-26 06:35:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 11009 updates
2022-02-26 06:35:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 06:35:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 06:35:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 14 @ 11009 updates, score 6.217) (writing took 6.324589889962226 seconds)
2022-02-26 06:35:18 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 06:35:18 | INFO | train | epoch 014 | loss 5.855 | ppl 57.87 | wps 10143 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 11009 | lr 0.000301388 | gnorm 0.544 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 71211
2022-02-26 06:35:18 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 06:35:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 06:43:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 06:45:10 | INFO | train_inner | epoch 015:     92 / 788 loss=5.761, ppl=54.22, wps=9846.5, ups=0.15, wpb=65248, bsz=127.4, num_updates=11100, lr=0.00030015, gnorm=0.541, loss_scale=16, train_wall=642, gb_free=4, wall=71803
2022-02-26 06:51:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 06:56:00 | INFO | train_inner | epoch 015:    193 / 788 loss=5.766, ppl=54.42, wps=10087.3, ups=0.15, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.589, loss_scale=8, train_wall=645, gb_free=4, wall=72453
2022-02-26 07:06:43 | INFO | train_inner | epoch 015:    293 / 788 loss=5.797, ppl=55.61, wps=10185.5, ups=0.16, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.521, loss_scale=8, train_wall=639, gb_free=4, wall=73096
2022-02-26 07:17:27 | INFO | train_inner | epoch 015:    393 / 788 loss=5.811, ppl=56.15, wps=10187.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.563, loss_scale=8, train_wall=638, gb_free=4, wall=73739
2022-02-26 07:28:10 | INFO | train_inner | epoch 015:    493 / 788 loss=5.819, ppl=56.47, wps=10186.9, ups=0.16, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.545, loss_scale=8, train_wall=638, gb_free=4, wall=74383
2022-02-26 07:38:53 | INFO | train_inner | epoch 015:    593 / 788 loss=5.835, ppl=57.07, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.566, loss_scale=8, train_wall=639, gb_free=4, wall=75026
2022-02-26 07:49:37 | INFO | train_inner | epoch 015:    693 / 788 loss=5.849, ppl=57.64, wps=10185.3, ups=0.16, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.528, loss_scale=16, train_wall=638, gb_free=4, wall=75670
2022-02-26 07:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 07:59:54 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.198 | ppl 73.43 | wps 23535.1 | wpb 2034.1 | bsz 4 | num_updates 11795 | best_loss 6.198
2022-02-26 07:59:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11795 updates
2022-02-26 07:59:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 08:00:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 08:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 15 @ 11795 updates, score 6.198) (writing took 6.277713370975107 seconds)
2022-02-26 08:00:01 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 08:00:01 | INFO | train | epoch 015 | loss 5.811 | ppl 56.14 | wps 10129.3 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 11795 | lr 0.000291173 | gnorm 0.549 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 76294
2022-02-26 08:00:01 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 08:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 08:00:33 | INFO | train_inner | epoch 016:      5 / 788 loss=5.848, ppl=57.6, wps=9942.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.548, loss_scale=16, train_wall=636, gb_free=4, wall=76326
2022-02-26 08:01:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:11:23 | INFO | train_inner | epoch 016:    106 / 788 loss=5.705, ppl=52.16, wps=10086.6, ups=0.15, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.546, loss_scale=8, train_wall=645, gb_free=4, wall=76976
2022-02-26 08:22:06 | INFO | train_inner | epoch 016:    206 / 788 loss=5.74, ppl=53.44, wps=10183.9, ups=0.16, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.562, loss_scale=8, train_wall=639, gb_free=4, wall=77619
2022-02-26 08:32:50 | INFO | train_inner | epoch 016:    306 / 788 loss=5.755, ppl=54.01, wps=10185.9, ups=0.16, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.563, loss_scale=8, train_wall=638, gb_free=4, wall=78263
2022-02-26 08:43:33 | INFO | train_inner | epoch 016:    406 / 788 loss=5.772, ppl=54.63, wps=10186.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.563, loss_scale=8, train_wall=638, gb_free=4, wall=78906
2022-02-26 08:54:16 | INFO | train_inner | epoch 016:    506 / 788 loss=5.79, ppl=55.32, wps=10188, ups=0.16, wpb=65520.6, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.543, loss_scale=8, train_wall=638, gb_free=4, wall=79549
2022-02-26 09:05:00 | INFO | train_inner | epoch 016:    606 / 788 loss=5.801, ppl=55.74, wps=10186.6, ups=0.16, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.542, loss_scale=16, train_wall=638, gb_free=4, wall=80192
2022-02-26 09:05:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:15:49 | INFO | train_inner | epoch 016:    707 / 788 loss=5.805, ppl=55.91, wps=10087.9, ups=0.15, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.564, loss_scale=8, train_wall=645, gb_free=4, wall=80842
2022-02-26 09:24:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 09:24:37 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.191 | ppl 73.05 | wps 23520.1 | wpb 2034.1 | bsz 4 | num_updates 12581 | best_loss 6.191
2022-02-26 09:24:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12581 updates
2022-02-26 09:24:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 09:24:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 09:24:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 16 @ 12581 updates, score 6.191) (writing took 6.283829563995823 seconds)
2022-02-26 09:24:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 09:24:43 | INFO | train | epoch 016 | loss 5.772 | ppl 54.63 | wps 10129.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 12581 | lr 0.000281931 | gnorm 0.556 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 81376
2022-02-26 09:24:43 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 09:24:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 09:26:45 | INFO | train_inner | epoch 017:     19 / 788 loss=5.791, ppl=55.37, wps=9946.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.554, loss_scale=8, train_wall=636, gb_free=4, wall=81498
2022-02-26 09:37:29 | INFO | train_inner | epoch 017:    119 / 788 loss=5.677, ppl=51.16, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=82141
2022-02-26 09:48:12 | INFO | train_inner | epoch 017:    219 / 788 loss=5.697, ppl=51.87, wps=10186.9, ups=0.16, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.553, loss_scale=8, train_wall=638, gb_free=4, wall=82785
2022-02-26 09:58:55 | INFO | train_inner | epoch 017:    319 / 788 loss=5.718, ppl=52.63, wps=10188, ups=0.16, wpb=65534.7, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.574, loss_scale=8, train_wall=638, gb_free=4, wall=83428
2022-02-26 10:09:38 | INFO | train_inner | epoch 017:    419 / 788 loss=5.74, ppl=53.45, wps=10188.3, ups=0.16, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.559, loss_scale=16, train_wall=638, gb_free=4, wall=84071
2022-02-26 10:20:22 | INFO | train_inner | epoch 017:    519 / 788 loss=5.755, ppl=54.02, wps=10187.6, ups=0.16, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.578, loss_scale=16, train_wall=638, gb_free=4, wall=84715
2022-02-26 10:29:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 10:31:11 | INFO | train_inner | epoch 017:    620 / 788 loss=5.768, ppl=54.5, wps=10087.6, ups=0.15, wpb=65520.6, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.553, loss_scale=8, train_wall=645, gb_free=4, wall=85364
2022-02-26 10:41:54 | INFO | train_inner | epoch 017:    720 / 788 loss=5.776, ppl=54.81, wps=10188.5, ups=0.16, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.558, loss_scale=8, train_wall=638, gb_free=4, wall=86007
2022-02-26 10:49:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 10:49:18 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.185 | ppl 72.73 | wps 23545.4 | wpb 2034.1 | bsz 4 | num_updates 13368 | best_loss 6.185
2022-02-26 10:49:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13368 updates
2022-02-26 10:49:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 10:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 10:49:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 17 @ 13368 updates, score 6.185) (writing took 6.472170597873628 seconds)
2022-02-26 10:49:25 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 10:49:25 | INFO | train | epoch 017 | loss 5.737 | ppl 53.32 | wps 10143.3 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 13368 | lr 0.000273506 | gnorm 0.559 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 86458
2022-02-26 10:49:25 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 10:49:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 10:52:51 | INFO | train_inner | epoch 018:     32 / 788 loss=5.745, ppl=53.64, wps=9943.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.567, loss_scale=8, train_wall=636, gb_free=4, wall=86663
2022-02-26 11:03:34 | INFO | train_inner | epoch 018:    132 / 788 loss=5.647, ppl=50.1, wps=10189.3, ups=0.16, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.564, loss_scale=8, train_wall=638, gb_free=4, wall=87307
2022-02-26 11:14:17 | INFO | train_inner | epoch 018:    232 / 788 loss=5.67, ppl=50.92, wps=10185.7, ups=0.16, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.575, loss_scale=8, train_wall=638, gb_free=4, wall=87950
2022-02-26 11:25:01 | INFO | train_inner | epoch 018:    332 / 788 loss=5.688, ppl=51.57, wps=10186.2, ups=0.16, wpb=65519.3, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.554, loss_scale=16, train_wall=638, gb_free=4, wall=88593
2022-02-26 11:35:44 | INFO | train_inner | epoch 018:    432 / 788 loss=5.704, ppl=52.14, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.566, loss_scale=16, train_wall=638, gb_free=4, wall=89237
2022-02-26 11:46:27 | INFO | train_inner | epoch 018:    532 / 788 loss=5.724, ppl=52.85, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.546, loss_scale=16, train_wall=638, gb_free=4, wall=89880
2022-02-26 11:57:10 | INFO | train_inner | epoch 018:    632 / 788 loss=5.741, ppl=53.47, wps=10189.7, ups=0.16, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.572, loss_scale=16, train_wall=638, gb_free=4, wall=90523
2022-02-26 12:07:54 | INFO | train_inner | epoch 018:    732 / 788 loss=5.748, ppl=53.74, wps=10189.1, ups=0.16, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.555, loss_scale=16, train_wall=638, gb_free=4, wall=91166
2022-02-26 12:13:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 12:14:00 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.181 | ppl 72.53 | wps 23522.7 | wpb 2034.1 | bsz 4 | num_updates 14156 | best_loss 6.181
2022-02-26 12:14:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14156 updates
2022-02-26 12:14:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 12:14:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 12:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 18 @ 14156 updates, score 6.181) (writing took 6.200995618943125 seconds)
2022-02-26 12:14:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 12:14:06 | INFO | train | epoch 018 | loss 5.705 | ppl 52.16 | wps 10156.6 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 14156 | lr 0.000265785 | gnorm 0.562 | loss_scale 16 | train_wall 5027 | gb_free 4 | wall 91539
2022-02-26 12:14:06 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 12:14:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 12:14:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 12:18:56 | INFO | train_inner | epoch 019:     45 / 788 loss=5.689, ppl=51.59, wps=9850.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.569, loss_scale=8, train_wall=642, gb_free=4, wall=91829
2022-02-26 12:29:39 | INFO | train_inner | epoch 019:    145 / 788 loss=5.62, ppl=49.18, wps=10188.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=92472
2022-02-26 12:40:22 | INFO | train_inner | epoch 019:    245 / 788 loss=5.645, ppl=50.04, wps=10187.1, ups=0.16, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.544, loss_scale=8, train_wall=638, gb_free=4, wall=93115
2022-02-26 12:51:06 | INFO | train_inner | epoch 019:    345 / 788 loss=5.66, ppl=50.55, wps=10188.3, ups=0.16, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.582, loss_scale=8, train_wall=638, gb_free=4, wall=93758
2022-02-26 13:01:49 | INFO | train_inner | epoch 019:    445 / 788 loss=5.685, ppl=51.45, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.569, loss_scale=8, train_wall=638, gb_free=4, wall=94402
2022-02-26 13:12:32 | INFO | train_inner | epoch 019:    545 / 788 loss=5.698, ppl=51.92, wps=10187.4, ups=0.16, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.551, loss_scale=16, train_wall=638, gb_free=4, wall=95045
2022-02-26 13:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 13:23:22 | INFO | train_inner | epoch 019:    646 / 788 loss=5.716, ppl=52.56, wps=10087.3, ups=0.15, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.579, loss_scale=8, train_wall=645, gb_free=4, wall=95695
2022-02-26 13:34:05 | INFO | train_inner | epoch 019:    746 / 788 loss=5.727, ppl=52.95, wps=10187.7, ups=0.16, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.567, loss_scale=8, train_wall=638, gb_free=4, wall=96338
2022-02-26 13:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 13:38:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.17 | ppl 72 | wps 23487 | wpb 2034.1 | bsz 4 | num_updates 14942 | best_loss 6.17
2022-02-26 13:38:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14942 updates
2022-02-26 13:38:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 13:38:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 13:38:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 19 @ 14942 updates, score 6.17) (writing took 6.175420731073245 seconds)
2022-02-26 13:38:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 13:38:48 | INFO | train | epoch 019 | loss 5.676 | ppl 51.14 | wps 10131.1 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 14942 | lr 0.0002587 | gnorm 0.566 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 96621
2022-02-26 13:38:48 | INFO | fairseq.trainer | begin training epoch 20
2022-02-26 13:38:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 13:45:01 | INFO | train_inner | epoch 020:     58 / 788 loss=5.637, ppl=49.77, wps=9946.7, ups=0.15, wpb=65248, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.559, loss_scale=8, train_wall=636, gb_free=4, wall=96994
2022-02-26 13:55:44 | INFO | train_inner | epoch 020:    158 / 788 loss=5.599, ppl=48.47, wps=10189.7, ups=0.16, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.559, loss_scale=8, train_wall=638, gb_free=4, wall=97637
2022-02-26 14:06:27 | INFO | train_inner | epoch 020:    258 / 788 loss=5.623, ppl=49.27, wps=10189, ups=0.16, wpb=65534.7, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.551, loss_scale=8, train_wall=638, gb_free=4, wall=98280
2022-02-26 14:17:11 | INFO | train_inner | epoch 020:    358 / 788 loss=5.634, ppl=49.67, wps=10189.4, ups=0.16, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.558, loss_scale=16, train_wall=638, gb_free=4, wall=98923
2022-02-26 14:27:54 | INFO | train_inner | epoch 020:    458 / 788 loss=5.651, ppl=50.25, wps=10188.2, ups=0.16, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.591, loss_scale=16, train_wall=638, gb_free=4, wall=99567
2022-02-26 14:38:37 | INFO | train_inner | epoch 020:    558 / 788 loss=5.676, ppl=51.14, wps=10185.7, ups=0.16, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.553, loss_scale=16, train_wall=638, gb_free=4, wall=100210
2022-02-26 14:39:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 14:49:27 | INFO | train_inner | epoch 020:    659 / 788 loss=5.696, ppl=51.84, wps=10089.5, ups=0.15, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.579, loss_scale=8, train_wall=645, gb_free=4, wall=100860
2022-02-26 15:00:10 | INFO | train_inner | epoch 020:    759 / 788 loss=5.704, ppl=52.11, wps=10188.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.564, loss_scale=8, train_wall=638, gb_free=4, wall=101503
2022-02-26 15:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 15:03:23 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.165 | ppl 71.77 | wps 23521.2 | wpb 2034.1 | bsz 4 | num_updates 15729 | best_loss 6.165
2022-02-26 15:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15729 updates
2022-02-26 15:03:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 15:03:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 15:03:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 20 @ 15729 updates, score 6.165) (writing took 6.312501461012289 seconds)
2022-02-26 15:03:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-26 15:03:29 | INFO | train | epoch 020 | loss 5.65 | ppl 50.22 | wps 10144.2 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 15729 | lr 0.000252144 | gnorm 0.565 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 101702
2022-02-26 15:03:29 | INFO | fairseq.trainer | begin training epoch 21
2022-02-26 15:03:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 15:11:06 | INFO | train_inner | epoch 021:     71 / 788 loss=5.59, ppl=48.17, wps=9943.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=15800, lr=0.000251577, gnorm=0.592, loss_scale=8, train_wall=636, gb_free=4, wall=102159
2022-02-26 15:21:49 | INFO | train_inner | epoch 021:    171 / 788 loss=5.573, ppl=47.61, wps=10187.8, ups=0.16, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.592, loss_scale=8, train_wall=638, gb_free=4, wall=102802
2022-02-26 15:32:33 | INFO | train_inner | epoch 021:    271 / 788 loss=5.597, ppl=48.4, wps=10186.6, ups=0.16, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.576, loss_scale=8, train_wall=638, gb_free=4, wall=103446
2022-02-26 15:43:16 | INFO | train_inner | epoch 021:    371 / 788 loss=5.625, ppl=49.34, wps=10187.1, ups=0.16, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.565, loss_scale=16, train_wall=638, gb_free=4, wall=104089
2022-02-26 15:53:59 | INFO | train_inner | epoch 021:    471 / 788 loss=5.636, ppl=49.72, wps=10185.5, ups=0.16, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.564, loss_scale=16, train_wall=639, gb_free=4, wall=104732
2022-02-26 15:55:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:04:49 | INFO | train_inner | epoch 021:    572 / 788 loss=5.66, ppl=50.55, wps=10086.3, ups=0.15, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.579, loss_scale=8, train_wall=645, gb_free=4, wall=105382
2022-02-26 16:15:32 | INFO | train_inner | epoch 021:    672 / 788 loss=5.661, ppl=50.59, wps=10187.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.57, loss_scale=8, train_wall=638, gb_free=4, wall=106025
2022-02-26 16:26:16 | INFO | train_inner | epoch 021:    772 / 788 loss=5.676, ppl=51.14, wps=10187.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.564, loss_scale=8, train_wall=638, gb_free=4, wall=106668
2022-02-26 16:27:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 16:28:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.163 | ppl 71.65 | wps 23577.6 | wpb 2034.1 | bsz 4 | num_updates 16516 | best_loss 6.163
2022-02-26 16:28:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16516 updates
2022-02-26 16:28:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 16:28:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 16:28:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 21 @ 16516 updates, score 6.163) (writing took 6.197624190011993 seconds)
2022-02-26 16:28:11 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-26 16:28:11 | INFO | train | epoch 021 | loss 5.626 | ppl 49.39 | wps 10142.9 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 16516 | lr 0.000246064 | gnorm 0.574 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 106784
2022-02-26 16:28:11 | INFO | fairseq.trainer | begin training epoch 22
2022-02-26 16:28:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 16:37:12 | INFO | train_inner | epoch 022:     84 / 788 loss=5.556, ppl=47.06, wps=9946, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=16600, lr=0.00024544, gnorm=0.562, loss_scale=8, train_wall=636, gb_free=4, wall=107325
2022-02-26 16:47:55 | INFO | train_inner | epoch 022:    184 / 788 loss=5.558, ppl=47.1, wps=10187.7, ups=0.16, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.584, loss_scale=8, train_wall=638, gb_free=4, wall=107968
2022-02-26 16:57:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:58:45 | INFO | train_inner | epoch 022:    285 / 788 loss=5.586, ppl=48.03, wps=10087.2, ups=0.15, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.599, loss_scale=8, train_wall=645, gb_free=4, wall=108617
2022-02-26 17:09:28 | INFO | train_inner | epoch 022:    385 / 788 loss=5.607, ppl=48.74, wps=10187.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.581, loss_scale=8, train_wall=638, gb_free=4, wall=109261
2022-02-26 17:20:11 | INFO | train_inner | epoch 022:    485 / 788 loss=5.62, ppl=49.16, wps=10186.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.595, loss_scale=8, train_wall=638, gb_free=4, wall=109904
2022-02-26 17:30:55 | INFO | train_inner | epoch 022:    585 / 788 loss=5.632, ppl=49.6, wps=10186.7, ups=0.16, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.572, loss_scale=8, train_wall=638, gb_free=4, wall=110547
2022-02-26 17:41:38 | INFO | train_inner | epoch 022:    685 / 788 loss=5.637, ppl=49.77, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.6, loss_scale=8, train_wall=638, gb_free=4, wall=111191
2022-02-26 17:52:21 | INFO | train_inner | epoch 022:    785 / 788 loss=5.646, ppl=50.09, wps=10188, ups=0.16, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.562, loss_scale=16, train_wall=638, gb_free=4, wall=111834
2022-02-26 17:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 17:52:47 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.161 | ppl 71.56 | wps 23572 | wpb 2034.1 | bsz 4 | num_updates 17303 | best_loss 6.161
2022-02-26 17:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17303 updates
2022-02-26 17:52:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 17:52:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 17:52:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 22 @ 17303 updates, score 6.161) (writing took 6.17904176004231 seconds)
2022-02-26 17:52:53 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-26 17:52:53 | INFO | train | epoch 022 | loss 5.604 | ppl 48.63 | wps 10143.3 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 17303 | lr 0.000240403 | gnorm 0.582 | loss_scale 16 | train_wall 5027 | gb_free 4 | wall 111866
2022-02-26 17:52:53 | INFO | fairseq.trainer | begin training epoch 23
2022-02-26 17:52:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 18:03:17 | INFO | train_inner | epoch 023:     97 / 788 loss=5.508, ppl=45.5, wps=9947.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=17400, lr=0.000239732, gnorm=0.573, loss_scale=16, train_wall=636, gb_free=4, wall=112490
2022-02-26 18:14:00 | INFO | train_inner | epoch 023:    197 / 788 loss=5.541, ppl=46.56, wps=10187.7, ups=0.16, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.58, loss_scale=16, train_wall=638, gb_free=4, wall=113133
2022-02-26 18:16:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 18:24:50 | INFO | train_inner | epoch 023:    298 / 788 loss=5.561, ppl=47.2, wps=10088.5, ups=0.15, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.585, loss_scale=8, train_wall=645, gb_free=4, wall=113783
2022-02-26 18:35:33 | INFO | train_inner | epoch 023:    398 / 788 loss=5.579, ppl=47.82, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.582, loss_scale=8, train_wall=638, gb_free=4, wall=114426
2022-02-26 18:46:17 | INFO | train_inner | epoch 023:    498 / 788 loss=5.604, ppl=48.65, wps=10186.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.568, loss_scale=8, train_wall=638, gb_free=4, wall=115069
2022-02-26 18:57:00 | INFO | train_inner | epoch 023:    598 / 788 loss=5.611, ppl=48.89, wps=10187, ups=0.16, wpb=65534.7, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.586, loss_scale=8, train_wall=638, gb_free=4, wall=115713
2022-02-26 19:07:43 | INFO | train_inner | epoch 023:    698 / 788 loss=5.636, ppl=49.71, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.568, loss_scale=8, train_wall=638, gb_free=4, wall=116356
2022-02-26 19:16:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 19:17:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.162 | ppl 71.59 | wps 23536.2 | wpb 2034.1 | bsz 4 | num_updates 18089 | best_loss 6.161
2022-02-26 19:17:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18089 updates
2022-02-26 19:17:28 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-26 19:17:28 | INFO | train | epoch 023 | loss 5.584 | ppl 47.96 | wps 10143.2 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 18089 | lr 0.000235122 | gnorm 0.578 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 116941
2022-02-26 19:17:28 | INFO | fairseq.trainer | begin training epoch 24
2022-02-26 19:17:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:39 | INFO | train_inner | epoch 024:     11 / 788 loss=5.621, ppl=49.22, wps=9942.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.594, loss_scale=8, train_wall=642, gb_free=4, wall=117012
2022-02-26 19:29:23 | INFO | train_inner | epoch 024:    111 / 788 loss=5.49, ppl=44.93, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.566, loss_scale=8, train_wall=638, gb_free=4, wall=117655
2022-02-26 19:40:06 | INFO | train_inner | epoch 024:    211 / 788 loss=5.518, ppl=45.83, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.605, loss_scale=8, train_wall=638, gb_free=4, wall=118299
2022-02-26 19:50:49 | INFO | train_inner | epoch 024:    311 / 788 loss=5.54, ppl=46.53, wps=10187.4, ups=0.16, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.581, loss_scale=8, train_wall=638, gb_free=4, wall=118942
2022-02-26 20:01:32 | INFO | train_inner | epoch 024:    411 / 788 loss=5.563, ppl=47.28, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.588, loss_scale=8, train_wall=638, gb_free=4, wall=119585
2022-02-26 20:12:16 | INFO | train_inner | epoch 024:    511 / 788 loss=5.584, ppl=47.96, wps=10185.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.578, loss_scale=16, train_wall=639, gb_free=4, wall=120229
2022-02-26 20:22:59 | INFO | train_inner | epoch 024:    611 / 788 loss=5.602, ppl=48.56, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.577, loss_scale=16, train_wall=638, gb_free=4, wall=120872
2022-02-26 20:33:43 | INFO | train_inner | epoch 024:    711 / 788 loss=5.614, ppl=48.99, wps=10187.3, ups=0.16, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.594, loss_scale=16, train_wall=638, gb_free=4, wall=121515
2022-02-26 20:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 20:42:04 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.155 | ppl 71.28 | wps 23530.9 | wpb 2034.1 | bsz 4 | num_updates 18877 | best_loss 6.155
2022-02-26 20:42:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18877 updates
2022-02-26 20:42:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 20:42:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 20:42:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 24 @ 18877 updates, score 6.155) (writing took 6.1612261950504035 seconds)
2022-02-26 20:42:10 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-26 20:42:10 | INFO | train | epoch 024 | loss 5.565 | ppl 47.33 | wps 10156 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 18877 | lr 0.000230162 | gnorm 0.587 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 122023
2022-02-26 20:42:10 | INFO | fairseq.trainer | begin training epoch 25
2022-02-26 20:42:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 20:44:38 | INFO | train_inner | epoch 025:     23 / 788 loss=5.593, ppl=48.28, wps=9945.5, ups=0.15, wpb=65218.6, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.595, loss_scale=16, train_wall=635, gb_free=4, wall=122171
2022-02-26 20:55:21 | INFO | train_inner | epoch 025:    123 / 788 loss=5.48, ppl=44.64, wps=10189.9, ups=0.16, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.576, loss_scale=16, train_wall=638, gb_free=4, wall=122814
2022-02-26 21:06:05 | INFO | train_inner | epoch 025:    223 / 788 loss=5.5, ppl=45.24, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.575, loss_scale=16, train_wall=638, gb_free=4, wall=123458
2022-02-26 21:06:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 21:08:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 21:17:01 | INFO | train_inner | epoch 025:    325 / 788 loss=5.542, ppl=46.6, wps=9990, ups=0.15, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.591, loss_scale=8, train_wall=651, gb_free=4, wall=124114
2022-02-26 21:27:44 | INFO | train_inner | epoch 025:    425 / 788 loss=5.546, ppl=46.73, wps=10190, ups=0.16, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.596, loss_scale=8, train_wall=638, gb_free=4, wall=124757
2022-02-26 21:38:27 | INFO | train_inner | epoch 025:    525 / 788 loss=5.565, ppl=47.35, wps=10187.4, ups=0.16, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.596, loss_scale=8, train_wall=638, gb_free=4, wall=125400
2022-02-26 21:49:10 | INFO | train_inner | epoch 025:    625 / 788 loss=5.584, ppl=47.97, wps=10190.3, ups=0.16, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.593, loss_scale=8, train_wall=638, gb_free=4, wall=126043
2022-02-26 21:59:54 | INFO | train_inner | epoch 025:    725 / 788 loss=5.591, ppl=48.2, wps=10187.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.622, loss_scale=8, train_wall=638, gb_free=4, wall=126686
2022-02-26 22:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 22:06:45 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.15 | ppl 71 | wps 23566.2 | wpb 2034.1 | bsz 4 | num_updates 19663 | best_loss 6.15
2022-02-26 22:06:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19663 updates
2022-02-26 22:06:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 22:06:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-26 22:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 25 @ 19663 updates, score 6.15) (writing took 6.163231820100918 seconds)
2022-02-26 22:06:51 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-26 22:06:51 | INFO | train | epoch 025 | loss 5.547 | ppl 46.75 | wps 10132 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 19663 | lr 0.000225515 | gnorm 0.591 | loss_scale 16 | train_wall 5027 | gb_free 4 | wall 127104
2022-02-26 22:06:51 | INFO | fairseq.trainer | begin training epoch 26
2022-02-26 22:06:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 22:10:50 | INFO | train_inner | epoch 026:     37 / 788 loss=5.546, ppl=46.71, wps=9946.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.571, loss_scale=16, train_wall=636, gb_free=4, wall=127342
2022-02-26 22:21:33 | INFO | train_inner | epoch 026:    137 / 788 loss=5.481, ppl=44.67, wps=10183.7, ups=0.16, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.597, loss_scale=16, train_wall=639, gb_free=4, wall=127986
2022-02-26 22:32:17 | INFO | train_inner | epoch 026:    237 / 788 loss=5.488, ppl=44.89, wps=10185.3, ups=0.16, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.585, loss_scale=16, train_wall=639, gb_free=4, wall=128629
2022-02-26 22:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 22:43:06 | INFO | train_inner | epoch 026:    338 / 788 loss=5.512, ppl=45.63, wps=10085.1, ups=0.15, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.609, loss_scale=8, train_wall=645, gb_free=4, wall=129279
2022-02-26 22:53:50 | INFO | train_inner | epoch 026:    438 / 788 loss=5.532, ppl=46.28, wps=10186.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.601, loss_scale=8, train_wall=638, gb_free=4, wall=129922
2022-02-26 23:04:33 | INFO | train_inner | epoch 026:    538 / 788 loss=5.553, ppl=46.94, wps=10187.3, ups=0.16, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.581, loss_scale=8, train_wall=638, gb_free=4, wall=130566
2022-02-26 23:15:16 | INFO | train_inner | epoch 026:    638 / 788 loss=5.565, ppl=47.35, wps=10186, ups=0.16, wpb=65534.7, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.602, loss_scale=8, train_wall=638, gb_free=4, wall=131209
2022-02-26 23:26:00 | INFO | train_inner | epoch 026:    738 / 788 loss=5.586, ppl=48.05, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.592, loss_scale=8, train_wall=638, gb_free=4, wall=131852
2022-02-26 23:31:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 23:31:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.156 | ppl 71.31 | wps 23524.3 | wpb 2034.1 | bsz 4 | num_updates 20450 | best_loss 6.15
2022-02-26 23:31:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20450 updates
2022-02-26 23:31:28 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-26 23:31:28 | INFO | train | epoch 026 | loss 5.531 | ppl 46.23 | wps 10154.4 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 20450 | lr 0.000221133 | gnorm 0.593 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 132181
2022-02-26 23:31:28 | INFO | fairseq.trainer | begin training epoch 27
2022-02-26 23:31:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 23:36:49 | INFO | train_inner | epoch 027:     50 / 788 loss=5.508, ppl=45.5, wps=10040.4, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=20500, lr=0.000220863, gnorm=0.582, loss_scale=16, train_wall=635, gb_free=4, wall=132502
2022-02-26 23:43:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 23:47:39 | INFO | train_inner | epoch 027:    151 / 788 loss=5.455, ppl=43.86, wps=10085.6, ups=0.15, wpb=65534.7, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.598, loss_scale=8, train_wall=645, gb_free=4, wall=133152
2022-02-26 23:58:22 | INFO | train_inner | epoch 027:    251 / 788 loss=5.489, ppl=44.92, wps=10189.2, ups=0.16, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.586, loss_scale=8, train_wall=638, gb_free=4, wall=133795
2022-02-27 00:09:06 | INFO | train_inner | epoch 027:    351 / 788 loss=5.517, ppl=45.79, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.599, loss_scale=8, train_wall=638, gb_free=4, wall=134438
2022-02-27 00:19:49 | INFO | train_inner | epoch 027:    451 / 788 loss=5.523, ppl=45.97, wps=10187.8, ups=0.16, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.596, loss_scale=8, train_wall=638, gb_free=4, wall=135082
2022-02-27 00:30:32 | INFO | train_inner | epoch 027:    551 / 788 loss=5.538, ppl=46.45, wps=10186.7, ups=0.16, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.603, loss_scale=8, train_wall=638, gb_free=4, wall=135725
2022-02-27 00:40:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 00:41:22 | INFO | train_inner | epoch 027:    652 / 788 loss=5.552, ppl=46.93, wps=10087, ups=0.15, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.594, loss_scale=8, train_wall=645, gb_free=4, wall=136375
2022-02-27 00:52:05 | INFO | train_inner | epoch 027:    752 / 788 loss=5.548, ppl=46.8, wps=10188.1, ups=0.16, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.593, loss_scale=8, train_wall=638, gb_free=4, wall=137018
2022-02-27 00:55:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 00:56:03 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.148 | ppl 70.93 | wps 23539 | wpb 2034.1 | bsz 4 | num_updates 21236 | best_loss 6.148
2022-02-27 00:56:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21236 updates
2022-02-27 00:56:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-27 00:56:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt
2022-02-27 00:56:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#3/checkpoint_best.pt (epoch 27 @ 21236 updates, score 6.148) (writing took 6.195304189110175 seconds)
2022-02-27 00:56:09 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-27 00:56:09 | INFO | train | epoch 027 | loss 5.514 | ppl 45.71 | wps 10130.7 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 21236 | lr 0.000217002 | gnorm 0.596 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 137262
2022-02-27 00:56:09 | INFO | fairseq.trainer | begin training epoch 28
2022-02-27 00:56:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 01:03:01 | INFO | train_inner | epoch 028:     64 / 788 loss=5.474, ppl=44.45, wps=9947, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=21300, lr=0.000216676, gnorm=0.602, loss_scale=8, train_wall=636, gb_free=4, wall=137674
2022-02-27 01:13:44 | INFO | train_inner | epoch 028:    164 / 788 loss=5.444, ppl=43.53, wps=10188.5, ups=0.16, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.603, loss_scale=8, train_wall=638, gb_free=4, wall=138317
2022-02-27 01:24:28 | INFO | train_inner | epoch 028:    264 / 788 loss=5.464, ppl=44.14, wps=10189.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.604, loss_scale=8, train_wall=638, gb_free=4, wall=138960
2022-02-27 01:35:11 | INFO | train_inner | epoch 028:    364 / 788 loss=5.494, ppl=45.06, wps=10187.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.609, loss_scale=8, train_wall=638, gb_free=4, wall=139604
2022-02-27 01:44:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 01:46:01 | INFO | train_inner | epoch 028:    465 / 788 loss=5.519, ppl=45.84, wps=10085.5, ups=0.15, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.611, loss_scale=8, train_wall=645, gb_free=4, wall=140253
2022-02-27 01:56:44 | INFO | train_inner | epoch 028:    565 / 788 loss=5.535, ppl=46.36, wps=10189.3, ups=0.16, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.584, loss_scale=8, train_wall=638, gb_free=4, wall=140896
2022-02-27 02:07:27 | INFO | train_inner | epoch 028:    665 / 788 loss=5.53, ppl=46.21, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.619, loss_scale=8, train_wall=638, gb_free=4, wall=141540
2022-02-27 02:18:10 | INFO | train_inner | epoch 028:    765 / 788 loss=5.56, ppl=47.18, wps=10190, ups=0.16, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.594, loss_scale=8, train_wall=638, gb_free=4, wall=142183
2022-02-27 02:20:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 02:20:44 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.149 | ppl 70.96 | wps 23591.3 | wpb 2034.1 | bsz 4 | num_updates 22023 | best_loss 6.148
2022-02-27 02:20:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 22023 updates
2022-02-27 02:20:44 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-27 02:20:44 | INFO | train | epoch 028 | loss 5.501 | ppl 45.28 | wps 10156.8 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 22023 | lr 0.000213089 | gnorm 0.604 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 142337
2022-02-27 02:20:45 | INFO | fairseq.trainer | begin training epoch 29
2022-02-27 02:20:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 02:29:00 | INFO | train_inner | epoch 029:     77 / 788 loss=5.444, ppl=43.53, wps=10041.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22100, lr=0.000212718, gnorm=0.603, loss_scale=8, train_wall=636, gb_free=4, wall=142833
2022-02-27 02:39:43 | INFO | train_inner | epoch 029:    177 / 788 loss=5.427, ppl=43.03, wps=10188.1, ups=0.16, wpb=65520.6, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.615, loss_scale=16, train_wall=638, gb_free=4, wall=143476
2022-02-27 02:50:26 | INFO | train_inner | epoch 029:    277 / 788 loss=5.46, ppl=44.02, wps=10188.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.604, loss_scale=16, train_wall=638, gb_free=4, wall=144119
2022-02-27 02:51:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 03:01:16 | INFO | train_inner | epoch 029:    378 / 788 loss=5.485, ppl=44.78, wps=10088.2, ups=0.15, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.623, loss_scale=8, train_wall=645, gb_free=4, wall=144769
2022-02-27 03:11:59 | INFO | train_inner | epoch 029:    478 / 788 loss=5.495, ppl=45.11, wps=10187.6, ups=0.16, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.599, loss_scale=8, train_wall=638, gb_free=4, wall=145412
2022-02-27 03:22:43 | INFO | train_inner | epoch 029:    578 / 788 loss=5.517, ppl=45.79, wps=10186.5, ups=0.16, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.603, loss_scale=8, train_wall=638, gb_free=4, wall=146055
2022-02-27 03:33:26 | INFO | train_inner | epoch 029:    678 / 788 loss=5.522, ppl=45.94, wps=10190.4, ups=0.16, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.602, loss_scale=8, train_wall=638, gb_free=4, wall=146698
2022-02-27 03:44:09 | INFO | train_inner | epoch 029:    778 / 788 loss=5.554, ppl=46.97, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.604, loss_scale=8, train_wall=638, gb_free=4, wall=147342
2022-02-27 03:45:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 03:45:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.152 | ppl 71.13 | wps 23533.8 | wpb 2034.1 | bsz 4 | num_updates 22810 | best_loss 6.148
2022-02-27 03:45:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22810 updates
2022-02-27 03:45:20 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-27 03:45:20 | INFO | train | epoch 029 | loss 5.486 | ppl 44.83 | wps 10156.6 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 22810 | lr 0.000209381 | gnorm 0.606 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 147412
2022-02-27 03:45:20 | INFO | fairseq.trainer | begin training epoch 30
2022-02-27 03:45:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 03:46:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 03:55:05 | INFO | train_inner | epoch 030:     91 / 788 loss=5.408, ppl=42.45, wps=9944.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22900, lr=0.000208969, gnorm=0.602, loss_scale=8, train_wall=642, gb_free=4, wall=147998
2022-02-27 04:05:48 | INFO | train_inner | epoch 030:    191 / 788 loss=5.424, ppl=42.93, wps=10188.3, ups=0.16, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.601, loss_scale=8, train_wall=638, gb_free=4, wall=148641
2022-02-27 04:16:32 | INFO | train_inner | epoch 030:    291 / 788 loss=5.449, ppl=43.69, wps=10187.6, ups=0.16, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.605, loss_scale=8, train_wall=638, gb_free=4, wall=149284
2022-02-27 04:27:15 | INFO | train_inner | epoch 030:    391 / 788 loss=5.465, ppl=44.17, wps=10188.2, ups=0.16, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.609, loss_scale=8, train_wall=638, gb_free=4, wall=149928
2022-02-27 04:37:58 | INFO | train_inner | epoch 030:    491 / 788 loss=5.489, ppl=44.91, wps=10187.9, ups=0.16, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.611, loss_scale=8, train_wall=638, gb_free=4, wall=150571
2022-02-27 04:48:41 | INFO | train_inner | epoch 030:    591 / 788 loss=5.506, ppl=45.46, wps=10187.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.62, loss_scale=16, train_wall=638, gb_free=4, wall=151214
2022-02-27 04:58:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 04:59:31 | INFO | train_inner | epoch 030:    692 / 788 loss=5.519, ppl=45.84, wps=10086.3, ups=0.15, wpb=65520.6, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.601, loss_scale=8, train_wall=645, gb_free=4, wall=151864
2022-02-27 05:09:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 05:09:55 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.152 | ppl 71.13 | wps 23337.3 | wpb 2034.1 | bsz 4 | num_updates 23596 | best_loss 6.148
2022-02-27 05:09:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23596 updates
2022-02-27 05:09:55 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-27 05:09:55 | INFO | train | epoch 030 | loss 5.473 | ppl 44.43 | wps 10143 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 23596 | lr 0.000205864 | gnorm 0.606 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 152488
2022-02-27 05:09:55 | INFO | fairseq.trainer | begin training epoch 31
2022-02-27 05:09:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 05:10:21 | INFO | train_inner | epoch 031:      4 / 788 loss=5.529, ppl=46.18, wps=10039.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.599, loss_scale=8, train_wall=636, gb_free=4, wall=152514
2022-02-27 05:21:04 | INFO | train_inner | epoch 031:    104 / 788 loss=5.391, ppl=41.96, wps=10187.6, ups=0.16, wpb=65534.7, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.605, loss_scale=8, train_wall=638, gb_free=4, wall=153157
2022-02-27 05:31:48 | INFO | train_inner | epoch 031:    204 / 788 loss=5.418, ppl=42.76, wps=10188.2, ups=0.16, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.631, loss_scale=8, train_wall=638, gb_free=4, wall=153800
2022-02-27 05:42:31 | INFO | train_inner | epoch 031:    304 / 788 loss=5.439, ppl=43.39, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.599, loss_scale=8, train_wall=638, gb_free=4, wall=154444
2022-02-27 05:53:14 | INFO | train_inner | epoch 031:    404 / 788 loss=5.466, ppl=44.21, wps=10186.4, ups=0.16, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.608, loss_scale=8, train_wall=638, gb_free=4, wall=155087
2022-02-27 05:56:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 06:04:04 | INFO | train_inner | epoch 031:    505 / 788 loss=5.468, ppl=44.27, wps=10088, ups=0.15, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.614, loss_scale=8, train_wall=645, gb_free=4, wall=155737
2022-02-27 06:14:47 | INFO | train_inner | epoch 031:    605 / 788 loss=5.493, ppl=45.04, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.613, loss_scale=8, train_wall=638, gb_free=4, wall=156380
2022-02-27 06:25:30 | INFO | train_inner | epoch 031:    705 / 788 loss=5.507, ppl=45.49, wps=10189, ups=0.16, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.614, loss_scale=8, train_wall=638, gb_free=4, wall=157023
2022-02-27 06:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 06:34:30 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.157 | ppl 71.38 | wps 23527.9 | wpb 2034.1 | bsz 4 | num_updates 24383 | best_loss 6.148
2022-02-27 06:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24383 updates
2022-02-27 06:34:30 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-27 06:34:30 | INFO | train | epoch 031 | loss 5.461 | ppl 44.05 | wps 10156.4 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 24383 | lr 0.000202515 | gnorm 0.612 | loss_scale 8 | train_wall 5027 | gb_free 4 | wall 157563
2022-02-27 06:34:30 | INFO | fairseq.trainer | begin training epoch 32
2022-02-27 06:34:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 06:36:20 | INFO | train_inner | epoch 032:     17 / 788 loss=5.492, ppl=45.01, wps=10041, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.614, loss_scale=8, train_wall=635, gb_free=4, wall=157673
2022-02-27 06:47:03 | INFO | train_inner | epoch 032:    117 / 788 loss=5.384, ppl=41.76, wps=10189.3, ups=0.16, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.595, loss_scale=8, train_wall=638, gb_free=4, wall=158316
2022-02-27 06:57:46 | INFO | train_inner | epoch 032:    217 / 788 loss=5.411, ppl=42.55, wps=10187.4, ups=0.16, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.62, loss_scale=16, train_wall=638, gb_free=4, wall=158959
2022-02-27 07:08:30 | INFO | train_inner | epoch 032:    317 / 788 loss=5.428, ppl=43.06, wps=10188.8, ups=0.16, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.595, loss_scale=16, train_wall=638, gb_free=4, wall=159602
2022-02-27 07:19:13 | INFO | train_inner | epoch 032:    417 / 788 loss=5.453, ppl=43.8, wps=10185.5, ups=0.16, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.612, loss_scale=16, train_wall=639, gb_free=4, wall=160246
2022-02-27 07:21:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 07:30:03 | INFO | train_inner | epoch 032:    518 / 788 loss=5.47, ppl=44.31, wps=10084.4, ups=0.15, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.646, loss_scale=8, train_wall=645, gb_free=4, wall=160896
2022-02-27 07:40:46 | INFO | train_inner | epoch 032:    618 / 788 loss=5.477, ppl=44.52, wps=10183.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.606, loss_scale=8, train_wall=638, gb_free=4, wall=161539
2022-02-27 07:51:29 | INFO | train_inner | epoch 032:    718 / 788 loss=5.499, ppl=45.22, wps=10188.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.608, loss_scale=8, train_wall=638, gb_free=4, wall=162182
2022-02-27 07:58:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 07:59:06 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.156 | ppl 71.33 | wps 23569 | wpb 2034.1 | bsz 4 | num_updates 25170 | best_loss 6.148
2022-02-27 07:59:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25170 updates
2022-02-27 07:59:06 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-27 07:59:06 | INFO | train | epoch 032 | loss 5.45 | ppl 43.71 | wps 10155.7 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 25170 | lr 0.000199323 | gnorm 0.612 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 162639
2022-02-27 07:59:06 | INFO | fairseq.trainer | begin training epoch 33
2022-02-27 07:59:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 08:02:19 | INFO | train_inner | epoch 033:     30 / 788 loss=5.466, ppl=44.21, wps=10043.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.612, loss_scale=8, train_wall=635, gb_free=4, wall=162832
2022-02-27 08:13:02 | INFO | train_inner | epoch 033:    130 / 788 loss=5.373, ppl=41.45, wps=10190.2, ups=0.16, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.614, loss_scale=8, train_wall=638, gb_free=4, wall=163475
2022-02-27 08:23:46 | INFO | train_inner | epoch 033:    230 / 788 loss=5.399, ppl=42.21, wps=10186.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.612, loss_scale=16, train_wall=638, gb_free=4, wall=164118
2022-02-27 08:34:29 | INFO | train_inner | epoch 033:    330 / 788 loss=5.425, ppl=42.97, wps=10185.4, ups=0.16, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.619, loss_scale=16, train_wall=639, gb_free=4, wall=164762
2022-02-27 08:45:12 | INFO | train_inner | epoch 033:    430 / 788 loss=5.439, ppl=43.4, wps=10186.2, ups=0.16, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.625, loss_scale=16, train_wall=638, gb_free=4, wall=165405
2022-02-27 08:55:56 | INFO | train_inner | epoch 033:    530 / 788 loss=5.458, ppl=43.97, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.628, loss_scale=16, train_wall=638, gb_free=4, wall=166049
2022-02-27 09:06:39 | INFO | train_inner | epoch 033:    630 / 788 loss=5.477, ppl=44.54, wps=10187.8, ups=0.16, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.603, loss_scale=16, train_wall=638, gb_free=4, wall=166692
2022-02-27 09:11:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-27 09:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 09:17:35 | INFO | train_inner | epoch 033:    732 / 788 loss=5.493, ppl=45.05, wps=9987, ups=0.15, wpb=65520.6, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.627, loss_scale=8, train_wall=651, gb_free=4, wall=167348
2022-02-27 09:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 09:23:42 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.157 | ppl 71.38 | wps 23544 | wpb 2034.1 | bsz 4 | num_updates 25956 | best_loss 6.148
2022-02-27 09:23:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25956 updates
2022-02-27 09:23:42 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-27 09:23:42 | INFO | train | epoch 033 | loss 5.439 | ppl 43.37 | wps 10142.7 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 25956 | lr 0.000196282 | gnorm 0.618 | loss_scale 8 | train_wall 5028 | gb_free 4 | wall 167715
2022-02-27 09:23:42 | INFO | fairseq.trainer | begin training epoch 34
2022-02-27 09:23:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 09:28:25 | INFO | train_inner | epoch 034:     44 / 788 loss=5.429, ppl=43.09, wps=10042.1, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=26000, lr=0.000196116, gnorm=0.619, loss_scale=8, train_wall=636, gb_free=4, wall=167998
2022-02-27 09:39:08 | INFO | train_inner | epoch 034:    144 / 788 loss=5.377, ppl=41.56, wps=10187.4, ups=0.16, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.609, loss_scale=8, train_wall=638, gb_free=4, wall=168641
2022-02-27 09:49:51 | INFO | train_inner | epoch 034:    244 / 788 loss=5.388, ppl=41.87, wps=10190.6, ups=0.16, wpb=65534.7, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.624, loss_scale=8, train_wall=638, gb_free=4, wall=169284
2022-02-27 10:00:34 | INFO | train_inner | epoch 034:    344 / 788 loss=5.41, ppl=42.53, wps=10191, ups=0.16, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.624, loss_scale=8, train_wall=638, gb_free=4, wall=169927
2022-02-27 10:11:18 | INFO | train_inner | epoch 034:    444 / 788 loss=5.429, ppl=43.09, wps=10188.7, ups=0.16, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.642, loss_scale=16, train_wall=638, gb_free=4, wall=170570
2022-02-27 10:22:01 | INFO | train_inner | epoch 034:    544 / 788 loss=5.451, ppl=43.75, wps=10185.8, ups=0.16, wpb=65520.6, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.62, loss_scale=16, train_wall=638, gb_free=4, wall=171214
2022-02-27 10:32:44 | INFO | train_inner | epoch 034:    644 / 788 loss=5.467, ppl=44.22, wps=10186.9, ups=0.16, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.613, loss_scale=16, train_wall=638, gb_free=4, wall=171857
2022-02-27 10:43:27 | INFO | train_inner | epoch 034:    744 / 788 loss=5.481, ppl=44.67, wps=10187.1, ups=0.16, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.621, loss_scale=16, train_wall=638, gb_free=4, wall=172500
User defined signal 2
