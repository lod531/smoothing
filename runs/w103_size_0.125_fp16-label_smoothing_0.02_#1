Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 207132138: <w103_size_0.125_fp16_label_smoothing_0.02_#1> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.02_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:20:37 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:20:42 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:20:42 2022
Terminated at Sat Mar  5 11:25:21 2022
Results reported at Sat Mar  5 11:25:21 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.02 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   93770.45 sec.
    Max Memory :                                 6693 MB
    Average Memory :                             3740.33 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13307.00 MB
    Max Swap :                                   105 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   93880 sec.
    Turnaround time :                            93884 sec.

The output (if any) follows:

2022-03-04 09:20:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.02, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:20:49 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-04 09:20:51 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-04 09:20:51 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:20:51 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:20:51 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 09:20:51 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-04 09:20:51 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:20:51 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-04 09:20:54 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:20:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:20:54 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-04 09:20:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:20:54 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:20:54 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:20:54 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 09:20:54 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 09:20:54 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:20:54 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-04 09:20:54 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:20:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:21:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:21:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:21:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:21:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:23:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:26:46 | INFO | train_inner | epoch 001:    105 / 196 loss=16.39, nll_loss=16.35, ppl=83519, wps=20489.1, ups=0.31, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.476, loss_scale=4, train_wall=328, gb_free=19.9, wall=352
2022-03-04 09:31:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:31:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.118 | nll_loss 13.009 | ppl 8241.06 | wps 40465.1 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-04 09:31:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-04 09:31:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 09:31:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 09:31:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.118) (writing took 7.483892859891057 seconds)
2022-03-04 09:31:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:31:46 | INFO | train | epoch 001 | loss 15.309 | nll_loss 15.246 | ppl 38873.2 | wps 20156.6 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.55 | loss_scale 8 | train_wall 594 | gb_free 19.9 | wall 652
2022-03-04 09:31:46 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:31:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:32:15 | INFO | train_inner | epoch 002:      9 / 196 loss=14.027, nll_loss=13.938, ppl=15700, wps=19883.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.508, loss_scale=8, train_wall=293, gb_free=19.9, wall=681
2022-03-04 09:37:31 | INFO | train_inner | epoch 002:    109 / 196 loss=12.099, nll_loss=11.964, ppl=3995.38, wps=20748.4, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.938, loss_scale=16, train_wall=293, gb_free=19.9, wall=997
2022-03-04 09:42:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:42:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.511 | nll_loss 10.317 | ppl 1276.02 | wps 40403.9 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.511
2022-03-04 09:42:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-04 09:42:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 09:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 09:42:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.511) (writing took 6.975027109030634 seconds)
2022-03-04 09:42:16 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 09:42:16 | INFO | train | epoch 002 | loss 11.593 | nll_loss 11.441 | ppl 2780.09 | wps 20350.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.799 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 1283
2022-03-04 09:42:17 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 09:42:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:42:57 | INFO | train_inner | epoch 003:     13 / 196 loss=10.802, nll_loss=10.623, ppl=1577.57, wps=19989.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.576, loss_scale=16, train_wall=292, gb_free=19.9, wall=1324
2022-03-04 09:48:13 | INFO | train_inner | epoch 003:    113 / 196 loss=10.236, nll_loss=10.028, ppl=1044.16, wps=20767.7, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.495, loss_scale=32, train_wall=293, gb_free=19.9, wall=1639
2022-03-04 09:51:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:52:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.798 | nll_loss 9.568 | ppl 759.13 | wps 40504.6 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.798
2022-03-04 09:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-04 09:52:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 09:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 09:52:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.798) (writing took 6.684076055884361 seconds)
2022-03-04 09:52:46 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 09:52:46 | INFO | train | epoch 003 | loss 10.128 | nll_loss 9.914 | ppl 965.01 | wps 20270.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.532 | loss_scale 32 | train_wall 574 | gb_free 19.9 | wall 1912
2022-03-04 09:52:46 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 09:52:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:53:43 | INFO | train_inner | epoch 004:     18 / 196 loss=9.908, nll_loss=9.684, ppl=822.65, wps=19804.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.606, loss_scale=32, train_wall=295, gb_free=19.9, wall=1969
2022-03-04 09:58:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:59:02 | INFO | train_inner | epoch 004:    119 / 196 loss=9.622, nll_loss=9.387, ppl=669.55, wps=20545.4, ups=0.31, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.619, loss_scale=32, train_wall=296, gb_free=19.9, wall=2288
2022-03-04 10:03:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:03:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.32 | nll_loss 9.076 | ppl 539.7 | wps 38158 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.32
2022-03-04 10:03:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-04 10:03:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:03:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:03:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.32) (writing took 6.694660012843087 seconds)
2022-03-04 10:03:21 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 10:03:21 | INFO | train | epoch 004 | loss 9.56 | nll_loss 9.322 | ppl 640.13 | wps 20095.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.675 | loss_scale 32 | train_wall 577 | gb_free 19.9 | wall 2547
2022-03-04 10:03:21 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 10:03:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:04:36 | INFO | train_inner | epoch 005:     23 / 196 loss=9.396, nll_loss=9.152, ppl=569.03, wps=19597.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.757, loss_scale=32, train_wall=297, gb_free=19.9, wall=2622
2022-03-04 10:06:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:10:02 | INFO | train_inner | epoch 005:    124 / 196 loss=9.148, nll_loss=8.895, ppl=475.92, wps=20097.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.835, loss_scale=32, train_wall=302, gb_free=19.9, wall=2948
2022-03-04 10:13:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:13:59 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.916 | nll_loss 8.656 | ppl 403.39 | wps 37857.7 | wpb 510.9 | bsz 1 | num_updates 971 | best_loss 8.916
2022-03-04 10:13:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 971 updates
2022-03-04 10:13:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:14:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 5 @ 971 updates, score 8.916) (writing took 6.656152036972344 seconds)
2022-03-04 10:14:06 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:14:06 | INFO | train | epoch 005 | loss 9.107 | nll_loss 8.852 | ppl 462.19 | wps 19689.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 971 | lr 0.000121451 | gnorm 0.835 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 3192
2022-03-04 10:14:06 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:14:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:15:40 | INFO | train_inner | epoch 006:     29 / 196 loss=8.951, nll_loss=8.692, ppl=413.44, wps=19328.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.887, loss_scale=32, train_wall=301, gb_free=19.9, wall=3286
2022-03-04 10:20:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:21:07 | INFO | train_inner | epoch 006:    130 / 196 loss=8.757, nll_loss=8.49, ppl=359.49, wps=20055, ups=0.31, wpb=65532.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.933, loss_scale=32, train_wall=302, gb_free=19.9, wall=3613
2022-03-04 10:24:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:24:45 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.598 | nll_loss 8.322 | ppl 320.07 | wps 37992.5 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.598
2022-03-04 10:24:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-04 10:24:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:24:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:24:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.598) (writing took 6.618743007071316 seconds)
2022-03-04 10:24:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 10:24:51 | INFO | train | epoch 006 | loss 8.731 | nll_loss 8.463 | ppl 352.88 | wps 19772.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.942 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 3838
2022-03-04 10:24:51 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 10:24:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:26:41 | INFO | train_inner | epoch 007:     34 / 196 loss=8.596, nll_loss=8.323, ppl=320.33, wps=19520.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.959, loss_scale=32, train_wall=298, gb_free=19.9, wall=3948
2022-03-04 10:28:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:32:08 | INFO | train_inner | epoch 007:    135 / 196 loss=8.432, nll_loss=8.154, ppl=284.87, wps=20068.3, ups=0.31, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.944, loss_scale=32, train_wall=302, gb_free=19.9, wall=4274
2022-03-04 10:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:35:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.337 | nll_loss 8.052 | ppl 265.37 | wps 38251.5 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.337
2022-03-04 10:35:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-04 10:35:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:35:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 7 @ 1361 updates, score 8.337) (writing took 6.601476771989837 seconds)
2022-03-04 10:35:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 10:35:37 | INFO | train | epoch 007 | loss 8.413 | nll_loss 8.134 | ppl 280.98 | wps 19772.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.96 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 4483
2022-03-04 10:35:37 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 10:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:36:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:37:46 | INFO | train_inner | epoch 008:     40 / 196 loss=8.279, nll_loss=7.996, ppl=255.34, wps=19318.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.956, loss_scale=32, train_wall=301, gb_free=19.9, wall=4613
2022-03-04 10:43:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:43:13 | INFO | train_inner | epoch 008:    141 / 196 loss=8.137, nll_loss=7.849, ppl=230.64, wps=20082.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.96, loss_scale=32, train_wall=302, gb_free=19.9, wall=4939
2022-03-04 10:46:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:46:15 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.098 | nll_loss 7.802 | ppl 223.21 | wps 37751 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 8.098
2022-03-04 10:46:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-04 10:46:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:46:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:46:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 8 @ 1555 updates, score 8.098) (writing took 6.626837863819674 seconds)
2022-03-04 10:46:22 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 10:46:22 | INFO | train | epoch 008 | loss 8.128 | nll_loss 7.84 | ppl 229.15 | wps 19683.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 0.952 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 5128
2022-03-04 10:46:22 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 10:46:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:48:48 | INFO | train_inner | epoch 009:     45 / 196 loss=7.993, nll_loss=7.7, ppl=207.98, wps=19515, ups=0.3, wpb=65367, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.934, loss_scale=32, train_wall=298, gb_free=19.9, wall=5274
2022-03-04 10:50:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:54:14 | INFO | train_inner | epoch 009:    146 / 196 loss=7.86, nll_loss=7.564, ppl=189.18, wps=20080.7, ups=0.31, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.948, loss_scale=32, train_wall=302, gb_free=19.9, wall=5600
2022-03-04 10:56:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:57:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.89 | nll_loss 7.587 | ppl 192.29 | wps 38076.3 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 7.89
2022-03-04 10:57:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1750 updates
2022-03-04 10:57:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:57:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 10:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 9 @ 1750 updates, score 7.89) (writing took 6.668336878996342 seconds)
2022-03-04 10:57:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 10:57:07 | INFO | train | epoch 009 | loss 7.86 | nll_loss 7.563 | ppl 189.16 | wps 19776.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1750 | lr 0.000218806 | gnorm 0.944 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 5774
2022-03-04 10:57:07 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 10:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:58:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:59:52 | INFO | train_inner | epoch 010:     51 / 196 loss=7.735, nll_loss=7.434, ppl=172.94, wps=19314, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.932, loss_scale=32, train_wall=301, gb_free=19.9, wall=5939
2022-03-04 11:05:16 | INFO | train_inner | epoch 010:    151 / 196 loss=7.613, nll_loss=7.307, ppl=158.39, wps=20260.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.938, loss_scale=64, train_wall=299, gb_free=19.9, wall=6262
2022-03-04 11:06:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:07:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:07:46 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.713 | nll_loss 7.402 | ppl 169.14 | wps 38294.6 | wpb 510.9 | bsz 1 | num_updates 1944 | best_loss 7.713
2022-03-04 11:07:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1944 updates
2022-03-04 11:07:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:07:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:07:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 10 @ 1944 updates, score 7.713) (writing took 6.675334178842604 seconds)
2022-03-04 11:07:53 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 11:07:53 | INFO | train | epoch 010 | loss 7.611 | nll_loss 7.306 | ppl 158.22 | wps 19666.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 1944 | lr 0.000243051 | gnorm 0.919 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 6419
2022-03-04 11:07:53 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 11:07:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:10:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:10:57 | INFO | train_inner | epoch 011:     57 / 196 loss=7.471, nll_loss=7.162, ppl=143.17, wps=19139.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.911, loss_scale=16, train_wall=304, gb_free=19.9, wall=6604
2022-03-04 11:16:20 | INFO | train_inner | epoch 011:    157 / 196 loss=7.377, nll_loss=7.064, ppl=133.77, wps=20289.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.924, loss_scale=16, train_wall=299, gb_free=19.9, wall=6927
2022-03-04 11:18:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:18:32 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.573 | nll_loss 7.254 | ppl 152.64 | wps 38000.9 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 7.573
2022-03-04 11:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2139 updates
2022-03-04 11:18:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:18:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:18:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 11 @ 2139 updates, score 7.573) (writing took 6.622184745967388 seconds)
2022-03-04 11:18:38 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 11:18:38 | INFO | train | epoch 011 | loss 7.379 | nll_loss 7.065 | ppl 133.93 | wps 19777.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2139 | lr 0.000267422 | gnorm 0.918 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 7064
2022-03-04 11:18:38 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 11:18:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:21:56 | INFO | train_inner | epoch 012:     61 / 196 loss=7.251, nll_loss=6.934, ppl=122.24, wps=19506.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.904, loss_scale=32, train_wall=298, gb_free=19.9, wall=7262
2022-03-04 11:23:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:27:22 | INFO | train_inner | epoch 012:    162 / 196 loss=7.153, nll_loss=6.832, ppl=113.94, wps=20076.5, ups=0.31, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.899, loss_scale=16, train_wall=302, gb_free=19.9, wall=7588
2022-03-04 11:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:29:17 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.443 | nll_loss 7.119 | ppl 138.96 | wps 38303.6 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.443
2022-03-04 11:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-04 11:29:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:29:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:29:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.443) (writing took 6.647461018059403 seconds)
2022-03-04 11:29:24 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 11:29:24 | INFO | train | epoch 012 | loss 7.162 | nll_loss 6.842 | ppl 114.69 | wps 19776.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.892 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 7710
2022-03-04 11:29:24 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 11:29:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:32:57 | INFO | train_inner | epoch 013:     66 / 196 loss=7.023, nll_loss=6.697, ppl=103.77, wps=19510.1, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.88, loss_scale=32, train_wall=298, gb_free=19.9, wall=7923
2022-03-04 11:37:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:38:23 | INFO | train_inner | epoch 013:    167 / 196 loss=6.967, nll_loss=6.639, ppl=99.63, wps=20074.4, ups=0.31, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.88, loss_scale=32, train_wall=302, gb_free=19.9, wall=8250
2022-03-04 11:39:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:40:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.332 | nll_loss 7.002 | ppl 128.21 | wps 38231.7 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 7.332
2022-03-04 11:40:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-04 11:40:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:40:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:40:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 13 @ 2529 updates, score 7.332) (writing took 6.689818526152521 seconds)
2022-03-04 11:40:09 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 11:40:09 | INFO | train | epoch 013 | loss 6.963 | nll_loss 6.635 | ppl 99.37 | wps 19780.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.885 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 8355
2022-03-04 11:40:09 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 11:40:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:43:58 | INFO | train_inner | epoch 014:     71 / 196 loss=6.827, nll_loss=6.494, ppl=90.16, wps=19520.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.869, loss_scale=32, train_wall=298, gb_free=19.9, wall=8585
2022-03-04 11:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:49:25 | INFO | train_inner | epoch 014:    172 / 196 loss=6.774, nll_loss=6.439, ppl=86.75, wps=20087.8, ups=0.31, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.868, loss_scale=32, train_wall=302, gb_free=19.9, wall=8911
2022-03-04 11:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:50:47 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.257 | nll_loss 6.924 | ppl 121.45 | wps 38166.5 | wpb 510.9 | bsz 1 | num_updates 2724 | best_loss 7.257
2022-03-04 11:50:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2724 updates
2022-03-04 11:50:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:50:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 14 @ 2724 updates, score 7.257) (writing took 6.632124735042453 seconds)
2022-03-04 11:50:54 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 11:50:54 | INFO | train | epoch 014 | loss 6.778 | nll_loss 6.444 | ppl 87.04 | wps 19788 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2724 | lr 0.000340532 | gnorm 0.875 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 9000
2022-03-04 11:50:54 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 11:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:52:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:55:03 | INFO | train_inner | epoch 015:     77 / 196 loss=6.639, nll_loss=6.3, ppl=78.8, wps=19327.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.864, loss_scale=32, train_wall=301, gb_free=19.9, wall=9249
2022-03-04 12:00:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:00:29 | INFO | train_inner | epoch 015:    178 / 196 loss=6.612, nll_loss=6.271, ppl=77.22, wps=20066.6, ups=0.31, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.869, loss_scale=32, train_wall=302, gb_free=19.9, wall=9576
2022-03-04 12:00:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:01:32 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.184 | nll_loss 6.852 | ppl 115.51 | wps 38400.6 | wpb 510.9 | bsz 1 | num_updates 2917 | best_loss 7.184
2022-03-04 12:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2917 updates
2022-03-04 12:01:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:01:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:01:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 15 @ 2917 updates, score 7.184) (writing took 6.655955504858866 seconds)
2022-03-04 12:01:39 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 12:01:39 | INFO | train | epoch 015 | loss 6.607 | nll_loss 6.266 | ppl 76.97 | wps 19571.5 | ups 0.3 | wpb 65446.6 | bsz 127.8 | num_updates 2917 | lr 0.000364652 | gnorm 0.865 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 9645
2022-03-04 12:01:39 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 12:01:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:06:08 | INFO | train_inner | epoch 016:     83 / 196 loss=6.475, nll_loss=6.129, ppl=69.99, wps=19325.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.866, loss_scale=16, train_wall=301, gb_free=19.9, wall=9914
2022-03-04 12:11:31 | INFO | train_inner | epoch 016:    183 / 196 loss=6.455, nll_loss=6.108, ppl=69, wps=20268, ups=0.31, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.847, loss_scale=32, train_wall=299, gb_free=19.9, wall=10237
2022-03-04 12:12:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:12:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.149 | nll_loss 6.814 | ppl 112.52 | wps 38248.8 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 7.149
2022-03-04 12:12:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3113 updates
2022-03-04 12:12:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 16 @ 3113 updates, score 7.149) (writing took 6.701030342141166 seconds)
2022-03-04 12:12:25 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 12:12:25 | INFO | train | epoch 016 | loss 6.45 | nll_loss 6.103 | ppl 68.74 | wps 19873.9 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 3113 | lr 0.000389147 | gnorm 0.846 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 10291
2022-03-04 12:12:25 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 12:12:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:15:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:17:09 | INFO | train_inner | epoch 017:     88 / 196 loss=6.306, nll_loss=5.955, ppl=62.02, wps=19332.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.845, loss_scale=32, train_wall=301, gb_free=19.9, wall=10575
2022-03-04 12:17:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:22:35 | INFO | train_inner | epoch 017:    189 / 196 loss=6.313, nll_loss=5.96, ppl=62.25, wps=20083, ups=0.31, wpb=65532.4, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.844, loss_scale=16, train_wall=302, gb_free=19.9, wall=10902
2022-03-04 12:22:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:23:03 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.124 | nll_loss 6.789 | ppl 110.55 | wps 38044.1 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 7.124
2022-03-04 12:23:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3307 updates
2022-03-04 12:23:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:23:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 17 @ 3307 updates, score 7.124) (writing took 6.676523144822568 seconds)
2022-03-04 12:23:10 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 12:23:10 | INFO | train | epoch 017 | loss 6.302 | nll_loss 5.95 | ppl 61.81 | wps 19681.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 3307 | lr 0.000413392 | gnorm 0.853 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 10936
2022-03-04 12:23:10 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 12:23:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:27:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:28:14 | INFO | train_inner | epoch 018:     94 / 196 loss=6.162, nll_loss=5.805, ppl=55.91, wps=19304.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.864, loss_scale=16, train_wall=301, gb_free=19.9, wall=11240
2022-03-04 12:33:37 | INFO | train_inner | epoch 018:    194 / 196 loss=6.176, nll_loss=5.818, ppl=56.41, wps=20276.8, ups=0.31, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.806, loss_scale=16, train_wall=299, gb_free=19.9, wall=11563
2022-03-04 12:33:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:33:49 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.126 | nll_loss 6.782 | ppl 110.09 | wps 37795.6 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 7.124
2022-03-04 12:33:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3502 updates
2022-03-04 12:33:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 12:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 12:33:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 18 @ 3502 updates, score 7.126) (writing took 3.2176772279199213 seconds)
2022-03-04 12:33:52 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 12:33:52 | INFO | train | epoch 018 | loss 6.162 | nll_loss 5.805 | ppl 55.89 | wps 19872.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3502 | lr 0.000437762 | gnorm 0.83 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 11578
2022-03-04 12:33:52 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 12:33:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:34:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:39:12 | INFO | train_inner | epoch 019:     99 / 196 loss=6.01, nll_loss=5.646, ppl=50.09, wps=19540.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.859, loss_scale=16, train_wall=301, gb_free=19.9, wall=11898
2022-03-04 12:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:44:30 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.12 | nll_loss 6.778 | ppl 109.73 | wps 38265.6 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 7.12
2022-03-04 12:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3697 updates
2022-03-04 12:44:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:44:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 19 @ 3697 updates, score 7.12) (writing took 6.606321424013004 seconds)
2022-03-04 12:44:37 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 12:44:37 | INFO | train | epoch 019 | loss 6.031 | nll_loss 5.668 | ppl 50.85 | wps 19784 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3697 | lr 0.000462133 | gnorm 0.853 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 12223
2022-03-04 12:44:37 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 12:44:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:44:47 | INFO | train_inner | epoch 020:      3 / 196 loss=6.05, nll_loss=5.688, ppl=51.54, wps=19509.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.844, loss_scale=32, train_wall=298, gb_free=19.9, wall=12233
2022-03-04 12:48:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:50:13 | INFO | train_inner | epoch 020:    104 / 196 loss=5.877, nll_loss=5.509, ppl=45.54, wps=20060.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.845, loss_scale=32, train_wall=302, gb_free=19.9, wall=12560
2022-03-04 12:53:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:55:16 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.132 | nll_loss 6.785 | ppl 110.31 | wps 37891.8 | wpb 510.9 | bsz 1 | num_updates 3891 | best_loss 7.12
2022-03-04 12:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3891 updates
2022-03-04 12:55:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 12:55:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 12:55:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 20 @ 3891 updates, score 7.132) (writing took 3.2137397641781718 seconds)
2022-03-04 12:55:19 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 12:55:19 | INFO | train | epoch 020 | loss 5.905 | nll_loss 5.537 | ppl 46.42 | wps 19769.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 3891 | lr 0.000486378 | gnorm 0.847 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 12865
2022-03-04 12:55:19 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 12:55:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:55:48 | INFO | train_inner | epoch 021:      9 / 196 loss=5.915, nll_loss=5.547, ppl=46.75, wps=19517.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.851, loss_scale=16, train_wall=301, gb_free=19.9, wall=12895
2022-03-04 13:01:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:01:15 | INFO | train_inner | epoch 021:    110 / 196 loss=5.76, nll_loss=5.387, ppl=41.85, wps=20062, ups=0.31, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.846, loss_scale=16, train_wall=302, gb_free=19.9, wall=13221
2022-03-04 13:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:05:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.147 | nll_loss 6.804 | ppl 111.71 | wps 38384.2 | wpb 510.9 | bsz 1 | num_updates 4086 | best_loss 7.12
2022-03-04 13:05:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4086 updates
2022-03-04 13:05:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:06:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:06:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 21 @ 4086 updates, score 7.147) (writing took 3.2172930068336427 seconds)
2022-03-04 13:06:01 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 13:06:01 | INFO | train | epoch 021 | loss 5.786 | nll_loss 5.413 | ppl 42.62 | wps 19887 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4086 | lr 0.00049471 | gnorm 0.838 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 13507
2022-03-04 13:06:01 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 13:06:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:06:46 | INFO | train_inner | epoch 022:     14 / 196 loss=5.792, nll_loss=5.419, ppl=42.79, wps=19734.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.829, loss_scale=16, train_wall=298, gb_free=19.9, wall=13552
2022-03-04 13:12:10 | INFO | train_inner | epoch 022:    114 / 196 loss=5.634, nll_loss=5.256, ppl=38.2, wps=20255.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.818, loss_scale=32, train_wall=299, gb_free=19.9, wall=13876
2022-03-04 13:15:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:16:40 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.182 | nll_loss 6.838 | ppl 114.43 | wps 37789.4 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 7.12
2022-03-04 13:16:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-04 13:16:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:16:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:16:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 22 @ 4281 updates, score 7.182) (writing took 3.1552216201089323 seconds)
2022-03-04 13:16:43 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 13:16:43 | INFO | train | epoch 022 | loss 5.658 | nll_loss 5.28 | ppl 38.84 | wps 19877.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.814 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 14149
2022-03-04 13:16:43 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 13:16:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:17:44 | INFO | train_inner | epoch 023:     19 / 196 loss=5.651, nll_loss=5.272, ppl=38.64, wps=19526.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.794, loss_scale=32, train_wall=301, gb_free=19.9, wall=14211
2022-03-04 13:22:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:23:11 | INFO | train_inner | epoch 023:    120 / 196 loss=5.514, nll_loss=5.13, ppl=35.03, wps=20091.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.793, loss_scale=32, train_wall=302, gb_free=19.9, wall=14537
2022-03-04 13:27:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:27:21 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.214 | nll_loss 6.865 | ppl 116.59 | wps 38050 | wpb 510.9 | bsz 1 | num_updates 4476 | best_loss 7.12
2022-03-04 13:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4476 updates
2022-03-04 13:27:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:27:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:27:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 23 @ 4476 updates, score 7.214) (writing took 3.183721896028146 seconds)
2022-03-04 13:27:24 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 13:27:24 | INFO | train | epoch 023 | loss 5.535 | nll_loss 5.152 | ppl 35.55 | wps 19895.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4476 | lr 0.000472667 | gnorm 0.795 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 14791
2022-03-04 13:27:24 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 13:27:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:28:42 | INFO | train_inner | epoch 024:     24 / 196 loss=5.52, nll_loss=5.136, ppl=35.17, wps=19720.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.801, loss_scale=32, train_wall=298, gb_free=19.9, wall=14868
2022-03-04 13:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:34:08 | INFO | train_inner | epoch 024:    125 / 196 loss=5.408, nll_loss=5.02, ppl=32.44, wps=20086.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.809, loss_scale=32, train_wall=302, gb_free=19.9, wall=15195
2022-03-04 13:36:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:37:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:38:03 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.245 | nll_loss 6.889 | ppl 118.56 | wps 37976 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 7.12
2022-03-04 13:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-04 13:38:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:38:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:38:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 24 @ 4670 updates, score 7.245) (writing took 3.168792305048555 seconds)
2022-03-04 13:38:06 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 13:38:06 | INFO | train | epoch 024 | loss 5.42 | nll_loss 5.032 | ppl 32.73 | wps 19794.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.794 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 15432
2022-03-04 13:38:06 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 13:38:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:39:43 | INFO | train_inner | epoch 025:     30 / 196 loss=5.402, nll_loss=5.013, ppl=32.3, wps=19537.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.788, loss_scale=32, train_wall=301, gb_free=19.9, wall=15529
2022-03-04 13:40:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:45:09 | INFO | train_inner | epoch 025:    131 / 196 loss=5.303, nll_loss=4.911, ppl=30.08, wps=20088.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.777, loss_scale=16, train_wall=302, gb_free=19.9, wall=15855
2022-03-04 13:48:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:48:44 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.273 | nll_loss 6.921 | ppl 121.2 | wps 38300 | wpb 510.9 | bsz 1 | num_updates 4865 | best_loss 7.12
2022-03-04 13:48:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4865 updates
2022-03-04 13:48:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:48:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:48:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 25 @ 4865 updates, score 7.273) (writing took 3.141130347037688 seconds)
2022-03-04 13:48:47 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 13:48:47 | INFO | train | epoch 025 | loss 5.313 | nll_loss 4.921 | ppl 30.3 | wps 19900 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4865 | lr 0.000453376 | gnorm 0.799 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 16073
2022-03-04 13:48:47 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 13:48:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:50:40 | INFO | train_inner | epoch 026:     35 / 196 loss=5.285, nll_loss=4.891, ppl=29.67, wps=19737.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.788, loss_scale=32, train_wall=298, gb_free=19.9, wall=16187
2022-03-04 13:51:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:56:07 | INFO | train_inner | epoch 026:    136 / 196 loss=5.203, nll_loss=4.806, ppl=27.98, wps=20089.9, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.801, loss_scale=16, train_wall=302, gb_free=19.9, wall=16513
2022-03-04 13:59:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:59:25 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.315 | nll_loss 6.956 | ppl 124.14 | wps 38185.8 | wpb 510.9 | bsz 1 | num_updates 5060 | best_loss 7.12
2022-03-04 13:59:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5060 updates
2022-03-04 13:59:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:59:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:59:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 26 @ 5060 updates, score 7.315) (writing took 3.220142652047798 seconds)
2022-03-04 13:59:29 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 13:59:29 | INFO | train | epoch 026 | loss 5.211 | nll_loss 4.815 | ppl 28.14 | wps 19896.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5060 | lr 0.000444554 | gnorm 0.784 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 16715
2022-03-04 13:59:29 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 13:59:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:01:38 | INFO | train_inner | epoch 027:     40 / 196 loss=5.174, nll_loss=4.776, ppl=27.39, wps=19721.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.788, loss_scale=32, train_wall=298, gb_free=19.9, wall=16844
2022-03-04 14:04:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:07:05 | INFO | train_inner | epoch 027:    141 / 196 loss=5.116, nll_loss=4.715, ppl=26.27, wps=20064.1, ups=0.31, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.773, loss_scale=16, train_wall=302, gb_free=19.9, wall=17171
2022-03-04 14:10:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:10:07 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.353 | nll_loss 6.997 | ppl 127.71 | wps 38211.1 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 7.12
2022-03-04 14:10:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5255 updates
2022-03-04 14:10:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 27 @ 5255 updates, score 7.353) (writing took 3.176699586212635 seconds)
2022-03-04 14:10:10 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 14:10:10 | INFO | train | epoch 027 | loss 5.116 | nll_loss 4.715 | ppl 26.27 | wps 19883.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5255 | lr 0.000436228 | gnorm 0.783 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 17357
2022-03-04 14:10:10 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 14:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:12:36 | INFO | train_inner | epoch 028:     45 / 196 loss=5.07, nll_loss=4.667, ppl=25.41, wps=19723.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.781, loss_scale=32, train_wall=298, gb_free=19.9, wall=17502
2022-03-04 14:17:59 | INFO | train_inner | epoch 028:    145 / 196 loss=5.031, nll_loss=4.626, ppl=24.7, wps=20297.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.797, loss_scale=32, train_wall=299, gb_free=19.9, wall=17825
2022-03-04 14:18:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:20:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:20:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.374 | nll_loss 7.015 | ppl 129.32 | wps 37819.5 | wpb 510.9 | bsz 1 | num_updates 5450 | best_loss 7.12
2022-03-04 14:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5450 updates
2022-03-04 14:20:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:20:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:20:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 28 @ 5450 updates, score 7.374) (writing took 3.1772722071036696 seconds)
2022-03-04 14:20:52 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 14:20:52 | INFO | train | epoch 028 | loss 5.025 | nll_loss 4.62 | ppl 24.59 | wps 19896.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5450 | lr 0.000428353 | gnorm 0.785 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 17998
2022-03-04 14:20:52 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 14:20:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:23:34 | INFO | train_inner | epoch 029:     50 / 196 loss=4.97, nll_loss=4.563, ppl=23.63, wps=19526.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.793, loss_scale=32, train_wall=301, gb_free=19.9, wall=18160
2022-03-04 14:25:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:28:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:29:03 | INFO | train_inner | epoch 029:    152 / 196 loss=4.946, nll_loss=4.537, ppl=23.21, wps=19897.5, ups=0.3, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.79, loss_scale=16, train_wall=305, gb_free=19.9, wall=18489
2022-03-04 14:31:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:31:30 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.407 | nll_loss 7.045 | ppl 132.04 | wps 37881.4 | wpb 510.9 | bsz 1 | num_updates 5644 | best_loss 7.12
2022-03-04 14:31:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5644 updates
2022-03-04 14:31:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:31:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 29 @ 5644 updates, score 7.407) (writing took 3.131277740933001 seconds)
2022-03-04 14:31:33 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 14:31:33 | INFO | train | epoch 029 | loss 4.938 | nll_loss 4.529 | ppl 23.09 | wps 19798.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 5644 | lr 0.000420927 | gnorm 0.788 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 18639
2022-03-04 14:31:33 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 14:31:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:34:34 | INFO | train_inner | epoch 030:     56 / 196 loss=4.891, nll_loss=4.48, ppl=22.32, wps=19726.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.804, loss_scale=16, train_wall=298, gb_free=19.9, wall=18821
2022-03-04 14:35:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:40:01 | INFO | train_inner | epoch 030:    157 / 196 loss=4.869, nll_loss=4.457, ppl=21.96, wps=20080.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.809, loss_scale=16, train_wall=302, gb_free=19.9, wall=19147
2022-03-04 14:42:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:42:12 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.49 | nll_loss 7.138 | ppl 140.84 | wps 38121.8 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 7.12
2022-03-04 14:42:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-04 14:42:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:42:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:42:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.49) (writing took 3.1948560040909797 seconds)
2022-03-04 14:42:15 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 14:42:15 | INFO | train | epoch 030 | loss 4.859 | nll_loss 4.446 | ppl 21.8 | wps 19889.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.814 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 19281
2022-03-04 14:42:15 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 14:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:45:32 | INFO | train_inner | epoch 031:     61 / 196 loss=4.795, nll_loss=4.379, ppl=20.81, wps=19733.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.797, loss_scale=32, train_wall=298, gb_free=19.9, wall=19478
2022-03-04 14:47:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:50:58 | INFO | train_inner | epoch 031:    162 / 196 loss=4.804, nll_loss=4.388, ppl=20.94, wps=20082.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.82, loss_scale=16, train_wall=302, gb_free=19.9, wall=19805
2022-03-04 14:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:52:53 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.513 | nll_loss 7.157 | ppl 142.7 | wps 38006.6 | wpb 510.9 | bsz 1 | num_updates 6034 | best_loss 7.12
2022-03-04 14:52:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6034 updates
2022-03-04 14:52:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:52:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:52:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 31 @ 6034 updates, score 7.513) (writing took 3.139740697108209 seconds)
2022-03-04 14:52:56 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 14:52:56 | INFO | train | epoch 031 | loss 4.782 | nll_loss 4.366 | ppl 20.62 | wps 19898.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6034 | lr 0.000407096 | gnorm 0.81 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 19922
2022-03-04 14:52:56 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 14:52:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:56:30 | INFO | train_inner | epoch 032:     66 / 196 loss=4.712, nll_loss=4.292, ppl=19.6, wps=19736.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.812, loss_scale=32, train_wall=298, gb_free=19.9, wall=20136
2022-03-04 14:58:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:01:56 | INFO | train_inner | epoch 032:    167 / 196 loss=4.728, nll_loss=4.309, ppl=19.81, wps=20086, ups=0.31, wpb=65532.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.811, loss_scale=16, train_wall=302, gb_free=19.9, wall=20462
2022-03-04 15:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:03:34 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.586 | nll_loss 7.226 | ppl 149.71 | wps 37792.9 | wpb 510.9 | bsz 1 | num_updates 6229 | best_loss 7.12
2022-03-04 15:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6229 updates
2022-03-04 15:03:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:03:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 32 @ 6229 updates, score 7.586) (writing took 3.214001219952479 seconds)
2022-03-04 15:03:38 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 15:03:38 | INFO | train | epoch 032 | loss 4.708 | nll_loss 4.288 | ppl 19.54 | wps 19895.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6229 | lr 0.000400674 | gnorm 0.814 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 20564
2022-03-04 15:03:38 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 15:03:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:05:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:07:30 | INFO | train_inner | epoch 033:     72 / 196 loss=4.635, nll_loss=4.212, ppl=18.53, wps=19538.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.809, loss_scale=16, train_wall=301, gb_free=19.9, wall=20797
2022-03-04 15:12:54 | INFO | train_inner | epoch 033:    172 / 196 loss=4.666, nll_loss=4.243, ppl=18.94, wps=20276.4, ups=0.31, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.831, loss_scale=32, train_wall=299, gb_free=19.9, wall=21120
2022-03-04 15:14:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:14:16 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.622 | nll_loss 7.261 | ppl 153.39 | wps 37993 | wpb 510.9 | bsz 1 | num_updates 6424 | best_loss 7.12
2022-03-04 15:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6424 updates
2022-03-04 15:14:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:14:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 33 @ 6424 updates, score 7.622) (writing took 3.162295959191397 seconds)
2022-03-04 15:14:19 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 15:14:19 | INFO | train | epoch 033 | loss 4.638 | nll_loss 4.215 | ppl 18.57 | wps 19894.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6424 | lr 0.000394546 | gnorm 0.827 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 21205
2022-03-04 15:14:19 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 15:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:18:28 | INFO | train_inner | epoch 034:     77 / 196 loss=4.565, nll_loss=4.139, ppl=17.62, wps=19535.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.841, loss_scale=16, train_wall=301, gb_free=19.9, wall=21454
2022-03-04 15:22:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:23:54 | INFO | train_inner | epoch 034:    178 / 196 loss=4.597, nll_loss=4.171, ppl=18.02, wps=20091.7, ups=0.31, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.813, loss_scale=16, train_wall=302, gb_free=19.9, wall=21781
2022-03-04 15:24:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:24:57 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.68 | nll_loss 7.317 | ppl 159.46 | wps 38398 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 7.12
2022-03-04 15:24:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-04 15:24:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:25:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:25:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 34 @ 6618 updates, score 7.68) (writing took 3.19287922908552 seconds)
2022-03-04 15:25:01 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 15:25:01 | INFO | train | epoch 034 | loss 4.57 | nll_loss 4.143 | ppl 17.67 | wps 19796.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.824 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 21847
2022-03-04 15:25:01 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 15:25:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:29:26 | INFO | train_inner | epoch 035:     82 / 196 loss=4.484, nll_loss=4.054, ppl=16.62, wps=19720.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.837, loss_scale=16, train_wall=298, gb_free=19.9, wall=22112
2022-03-04 15:32:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:34:52 | INFO | train_inner | epoch 035:    183 / 196 loss=4.544, nll_loss=4.115, ppl=17.33, wps=20085.5, ups=0.31, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.843, loss_scale=16, train_wall=302, gb_free=19.9, wall=22438
2022-03-04 15:35:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:35:39 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.717 | nll_loss 7.356 | ppl 163.79 | wps 38231.8 | wpb 510.9 | bsz 1 | num_updates 6813 | best_loss 7.12
2022-03-04 15:35:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6813 updates
2022-03-04 15:35:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:35:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:35:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 35 @ 6813 updates, score 7.717) (writing took 3.1976372071076185 seconds)
2022-03-04 15:35:42 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 15:35:42 | INFO | train | epoch 035 | loss 4.509 | nll_loss 4.079 | ppl 16.9 | wps 19890.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6813 | lr 0.000383116 | gnorm 0.837 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 22488
2022-03-04 15:35:42 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 15:35:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:39:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:40:27 | INFO | train_inner | epoch 036:     88 / 196 loss=4.417, nll_loss=3.984, ppl=15.82, wps=19534, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.854, loss_scale=16, train_wall=301, gb_free=19.9, wall=22773
2022-03-04 15:45:50 | INFO | train_inner | epoch 036:    188 / 196 loss=4.493, nll_loss=4.061, ppl=16.69, wps=20282.1, ups=0.31, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.857, loss_scale=16, train_wall=299, gb_free=19.9, wall=23096
2022-03-04 15:46:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:46:21 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.778 | nll_loss 7.417 | ppl 170.94 | wps 37921.9 | wpb 510.9 | bsz 1 | num_updates 7008 | best_loss 7.12
2022-03-04 15:46:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7008 updates
2022-03-04 15:46:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:46:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:46:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 36 @ 7008 updates, score 7.778) (writing took 3.1072061150334775 seconds)
2022-03-04 15:46:24 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 15:46:24 | INFO | train | epoch 036 | loss 4.447 | nll_loss 4.014 | ppl 16.16 | wps 19891.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7008 | lr 0.000377749 | gnorm 0.857 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 23130
2022-03-04 15:46:24 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 15:46:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:47:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:51:24 | INFO | train_inner | epoch 037:     93 / 196 loss=4.349, nll_loss=3.912, ppl=15.06, wps=19559.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.852, loss_scale=16, train_wall=301, gb_free=19.9, wall=23430
2022-03-04 15:54:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:56:44 | INFO | train_inner | epoch 037:    194 / 196 loss=4.434, nll_loss=4, ppl=16, wps=20483.1, ups=0.31, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.856, loss_scale=16, train_wall=297, gb_free=19.9, wall=23750
2022-03-04 15:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:56:55 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.848 | nll_loss 7.484 | ppl 179.06 | wps 40121.4 | wpb 510.9 | bsz 1 | num_updates 7202 | best_loss 7.12
2022-03-04 15:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7202 updates
2022-03-04 15:56:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:56:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:56:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 37 @ 7202 updates, score 7.848) (writing took 3.074131859932095 seconds)
2022-03-04 15:56:58 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 15:56:58 | INFO | train | epoch 037 | loss 4.388 | nll_loss 3.952 | ppl 15.47 | wps 20018.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7202 | lr 0.000372626 | gnorm 0.854 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 23764
2022-03-04 15:56:58 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 15:56:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:01:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:02:11 | INFO | train_inner | epoch 038:     99 / 196 loss=4.285, nll_loss=3.845, ppl=14.37, wps=19975.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.855, loss_scale=16, train_wall=296, gb_free=19.9, wall=24077
2022-03-04 16:07:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:07:22 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.886 | nll_loss 7.525 | ppl 184.17 | wps 40332.6 | wpb 510.9 | bsz 1 | num_updates 7397 | best_loss 7.12
2022-03-04 16:07:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7397 updates
2022-03-04 16:07:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:07:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:07:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 38 @ 7397 updates, score 7.886) (writing took 3.050209693843499 seconds)
2022-03-04 16:07:26 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 16:07:26 | INFO | train | epoch 038 | loss 4.332 | nll_loss 3.894 | ppl 14.86 | wps 20337.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7397 | lr 0.000367682 | gnorm 0.853 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 24392
2022-03-04 16:07:26 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 16:07:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:07:35 | INFO | train_inner | epoch 039:      3 / 196 loss=4.377, nll_loss=3.94, ppl=15.34, wps=20171.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.852, loss_scale=16, train_wall=293, gb_free=19.9, wall=24401
2022-03-04 16:09:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:12:55 | INFO | train_inner | epoch 039:    104 / 196 loss=4.23, nll_loss=3.788, ppl=13.81, wps=20515.9, ups=0.31, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.852, loss_scale=16, train_wall=297, gb_free=19.9, wall=24721
2022-03-04 16:16:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:17:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.912 | nll_loss 7.553 | ppl 187.84 | wps 40415.6 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 7.12
2022-03-04 16:17:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-04 16:17:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:17:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:17:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 39 @ 7591 updates, score 7.912) (writing took 3.077577044023201 seconds)
2022-03-04 16:17:53 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 16:17:53 | INFO | train | epoch 039 | loss 4.278 | nll_loss 3.837 | ppl 14.29 | wps 20228.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.866 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 25019
2022-03-04 16:17:53 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 16:17:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:18:22 | INFO | train_inner | epoch 040:      9 / 196 loss=4.316, nll_loss=3.875, ppl=14.67, wps=19985.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.888, loss_scale=16, train_wall=295, gb_free=19.9, wall=25048
2022-03-04 16:23:38 | INFO | train_inner | epoch 040:    109 / 196 loss=4.192, nll_loss=3.747, ppl=13.43, wps=20704.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.863, loss_scale=32, train_wall=294, gb_free=19.9, wall=25364
2022-03-04 16:24:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:28:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:28:18 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.985 | nll_loss 7.616 | ppl 196.11 | wps 40635.2 | wpb 510.9 | bsz 1 | num_updates 7786 | best_loss 7.12
2022-03-04 16:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7786 updates
2022-03-04 16:28:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:28:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 40 @ 7786 updates, score 7.985) (writing took 3.164533199975267 seconds)
2022-03-04 16:28:21 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:28:21 | INFO | train | epoch 040 | loss 4.228 | nll_loss 3.784 | ppl 13.78 | wps 20329 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7786 | lr 0.000358379 | gnorm 0.876 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 25647
2022-03-04 16:28:21 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:28:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:29:05 | INFO | train_inner | epoch 041:     14 / 196 loss=4.252, nll_loss=3.809, ppl=14.02, wps=19978.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.883, loss_scale=16, train_wall=296, gb_free=19.9, wall=25692
2022-03-04 16:31:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:34:24 | INFO | train_inner | epoch 041:    115 / 196 loss=4.14, nll_loss=3.692, ppl=12.93, wps=20541.3, ups=0.31, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.874, loss_scale=16, train_wall=296, gb_free=19.9, wall=26011
2022-03-04 16:38:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:38:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:38:48 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.022 | nll_loss 7.652 | ppl 201.11 | wps 38150.3 | wpb 510.9 | bsz 1 | num_updates 7980 | best_loss 7.12
2022-03-04 16:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7980 updates
2022-03-04 16:38:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:38:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 41 @ 7980 updates, score 8.022) (writing took 3.552029959857464 seconds)
2022-03-04 16:38:52 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:38:52 | INFO | train | epoch 041 | loss 4.178 | nll_loss 3.731 | ppl 13.28 | wps 20130.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7980 | lr 0.000353996 | gnorm 0.881 | loss_scale 16 | train_wall 576 | gb_free 19.9 | wall 26278
2022-03-04 16:38:52 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:38:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:39:56 | INFO | train_inner | epoch 042:     20 / 196 loss=4.2, nll_loss=3.754, ppl=13.49, wps=19684, ups=0.3, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.881, loss_scale=16, train_wall=299, gb_free=19.9, wall=26343
2022-03-04 16:43:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:45:23 | INFO | train_inner | epoch 042:    121 / 196 loss=4.105, nll_loss=3.655, ppl=12.6, wps=20083.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.903, loss_scale=8, train_wall=302, gb_free=19.9, wall=26669
2022-03-04 16:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:49:30 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.07 | nll_loss 7.705 | ppl 208.68 | wps 38121.9 | wpb 510.9 | bsz 1 | num_updates 8175 | best_loss 7.12
2022-03-04 16:49:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8175 updates
2022-03-04 16:49:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:49:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:49:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 42 @ 8175 updates, score 8.07) (writing took 3.2511047169100493 seconds)
2022-03-04 16:49:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 16:49:33 | INFO | train | epoch 042 | loss 4.131 | nll_loss 3.682 | ppl 12.84 | wps 19893.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8175 | lr 0.000349749 | gnorm 0.888 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 26919
2022-03-04 16:49:33 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 16:49:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:50:54 | INFO | train_inner | epoch 043:     25 / 196 loss=4.142, nll_loss=3.693, ppl=12.93, wps=19724.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.882, loss_scale=16, train_wall=298, gb_free=19.9, wall=27000
2022-03-04 16:56:17 | INFO | train_inner | epoch 043:    125 / 196 loss=4.065, nll_loss=3.613, ppl=12.23, wps=20281.5, ups=0.31, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.887, loss_scale=16, train_wall=299, gb_free=19.9, wall=27324
2022-03-04 16:57:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:00:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:00:11 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.148 | nll_loss 7.781 | ppl 219.96 | wps 38136.4 | wpb 510.9 | bsz 1 | num_updates 8370 | best_loss 7.12
2022-03-04 17:00:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8370 updates
2022-03-04 17:00:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:00:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:00:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 43 @ 8370 updates, score 8.148) (writing took 3.564322445075959 seconds)
2022-03-04 17:00:15 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 17:00:15 | INFO | train | epoch 043 | loss 4.086 | nll_loss 3.635 | ppl 12.42 | wps 19885 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8370 | lr 0.000345651 | gnorm 0.898 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 27561
2022-03-04 17:00:15 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 17:00:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:01:52 | INFO | train_inner | epoch 044:     30 / 196 loss=4.088, nll_loss=3.636, ppl=12.43, wps=19531.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.907, loss_scale=16, train_wall=301, gb_free=19.9, wall=27658
2022-03-04 17:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:07:18 | INFO | train_inner | epoch 044:    131 / 196 loss=4.025, nll_loss=3.571, ppl=11.88, wps=20092.5, ups=0.31, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.896, loss_scale=16, train_wall=302, gb_free=19.9, wall=27984
2022-03-04 17:10:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:10:53 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.184 | nll_loss 7.822 | ppl 226.34 | wps 37952.2 | wpb 510.9 | bsz 1 | num_updates 8565 | best_loss 7.12
2022-03-04 17:10:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8565 updates
2022-03-04 17:10:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:10:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:10:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 44 @ 8565 updates, score 8.184) (writing took 3.174445282900706 seconds)
2022-03-04 17:10:56 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 17:10:56 | INFO | train | epoch 044 | loss 4.04 | nll_loss 3.587 | ppl 12.01 | wps 19900.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8565 | lr 0.000341693 | gnorm 0.9 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 28203
2022-03-04 17:10:56 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 17:10:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:11:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:12:53 | INFO | train_inner | epoch 045:     36 / 196 loss=4.045, nll_loss=3.591, ppl=12.05, wps=19535.4, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.924, loss_scale=8, train_wall=301, gb_free=19.9, wall=28319
2022-03-04 17:18:16 | INFO | train_inner | epoch 045:    136 / 196 loss=3.989, nll_loss=3.533, ppl=11.57, wps=20295.5, ups=0.31, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.897, loss_scale=16, train_wall=299, gb_free=19.9, wall=28642
2022-03-04 17:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:21:34 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.239 | nll_loss 7.874 | ppl 234.62 | wps 37664.5 | wpb 510.9 | bsz 1 | num_updates 8760 | best_loss 7.12
2022-03-04 17:21:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8760 updates
2022-03-04 17:21:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:21:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:21:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 45 @ 8760 updates, score 8.239) (writing took 3.151947665028274 seconds)
2022-03-04 17:21:38 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 17:21:38 | INFO | train | epoch 045 | loss 3.999 | nll_loss 3.543 | ppl 11.65 | wps 19904.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8760 | lr 0.000337869 | gnorm 0.907 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 28844
2022-03-04 17:21:38 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 17:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:23:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:23:50 | INFO | train_inner | epoch 046:     41 / 196 loss=3.983, nll_loss=3.527, ppl=11.52, wps=19539, ups=0.3, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.906, loss_scale=8, train_wall=301, gb_free=19.9, wall=28976
2022-03-04 17:29:13 | INFO | train_inner | epoch 046:    141 / 196 loss=3.953, nll_loss=3.495, ppl=11.27, wps=20297.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.914, loss_scale=8, train_wall=299, gb_free=19.9, wall=29299
2022-03-04 17:32:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:32:16 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.303 | nll_loss 7.941 | ppl 245.66 | wps 37859 | wpb 510.9 | bsz 1 | num_updates 8955 | best_loss 7.12
2022-03-04 17:32:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8955 updates
2022-03-04 17:32:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:32:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 46 @ 8955 updates, score 8.303) (writing took 3.1445217318832874 seconds)
2022-03-04 17:32:19 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 17:32:19 | INFO | train | epoch 046 | loss 3.958 | nll_loss 3.5 | ppl 11.31 | wps 19899.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8955 | lr 0.00033417 | gnorm 0.916 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 29485
2022-03-04 17:32:19 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 17:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:34:44 | INFO | train_inner | epoch 047:     45 / 196 loss=3.94, nll_loss=3.481, ppl=11.17, wps=19725.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.901, loss_scale=16, train_wall=298, gb_free=19.9, wall=29631
2022-03-04 17:37:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:40:11 | INFO | train_inner | epoch 047:    146 / 196 loss=3.922, nll_loss=3.462, ppl=11.02, wps=20093, ups=0.31, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.938, loss_scale=16, train_wall=302, gb_free=19.9, wall=29957
2022-03-04 17:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:42:57 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.308 | nll_loss 7.939 | ppl 245.4 | wps 38150.6 | wpb 510.9 | bsz 1 | num_updates 9150 | best_loss 7.12
2022-03-04 17:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9150 updates
2022-03-04 17:42:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:43:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:43:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 47 @ 9150 updates, score 8.308) (writing took 3.127309118863195 seconds)
2022-03-04 17:43:00 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 17:43:00 | INFO | train | epoch 047 | loss 3.919 | nll_loss 3.458 | ppl 10.99 | wps 19901.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9150 | lr 0.00033059 | gnorm 0.923 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 30126
2022-03-04 17:43:00 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 17:43:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:43:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:45:45 | INFO | train_inner | epoch 048:     51 / 196 loss=3.9, nll_loss=3.439, ppl=10.85, wps=19556.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.937, loss_scale=8, train_wall=301, gb_free=19.9, wall=30291
2022-03-04 17:51:08 | INFO | train_inner | epoch 048:    151 / 196 loss=3.884, nll_loss=3.422, ppl=10.72, wps=20286.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.931, loss_scale=16, train_wall=299, gb_free=19.9, wall=30614
2022-03-04 17:53:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:53:38 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.38 | nll_loss 8.011 | ppl 257.92 | wps 37968.9 | wpb 510.9 | bsz 1 | num_updates 9345 | best_loss 7.12
2022-03-04 17:53:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9345 updates
2022-03-04 17:53:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:53:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 48 @ 9345 updates, score 8.38) (writing took 3.1315493271686137 seconds)
2022-03-04 17:53:41 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 17:53:41 | INFO | train | epoch 048 | loss 3.881 | nll_loss 3.419 | ppl 10.7 | wps 19906.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9345 | lr 0.000327122 | gnorm 0.937 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 30767
2022-03-04 17:53:41 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 17:53:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:56:39 | INFO | train_inner | epoch 049:     55 / 196 loss=3.852, nll_loss=3.388, ppl=10.47, wps=19740.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.925, loss_scale=16, train_wall=298, gb_free=19.9, wall=30945
2022-03-04 17:57:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:02:05 | INFO | train_inner | epoch 049:    156 / 196 loss=3.856, nll_loss=3.392, ppl=10.5, wps=20084.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.929, loss_scale=16, train_wall=302, gb_free=19.9, wall=31271
2022-03-04 18:04:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:04:19 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.438 | nll_loss 8.067 | ppl 268.16 | wps 37937.5 | wpb 510.9 | bsz 1 | num_updates 9540 | best_loss 7.12
2022-03-04 18:04:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9540 updates
2022-03-04 18:04:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:04:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:04:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 49 @ 9540 updates, score 8.438) (writing took 3.1377742011100054 seconds)
2022-03-04 18:04:23 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 18:04:23 | INFO | train | epoch 049 | loss 3.844 | nll_loss 3.38 | ppl 10.41 | wps 19899.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9540 | lr 0.000323762 | gnorm 0.925 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 31409
2022-03-04 18:04:23 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 18:04:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:04:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:07:40 | INFO | train_inner | epoch 050:     61 / 196 loss=3.809, nll_loss=3.344, ppl=10.15, wps=19547.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.929, loss_scale=16, train_wall=301, gb_free=19.9, wall=31606
2022-03-04 18:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:13:06 | INFO | train_inner | epoch 050:    162 / 196 loss=3.828, nll_loss=3.363, ppl=10.29, wps=20101.6, ups=0.31, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.947, loss_scale=16, train_wall=302, gb_free=19.9, wall=31932
2022-03-04 18:13:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:14:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:15:00 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.51 | nll_loss 8.141 | ppl 282.28 | wps 38112 | wpb 510.9 | bsz 1 | num_updates 9733 | best_loss 7.12
2022-03-04 18:15:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9733 updates
2022-03-04 18:15:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:15:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 50 @ 9733 updates, score 8.51) (writing took 3.179384328890592 seconds)
2022-03-04 18:15:04 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 18:15:04 | INFO | train | epoch 050 | loss 3.809 | nll_loss 3.343 | ppl 10.15 | wps 19706.6 | ups 0.3 | wpb 65446.6 | bsz 127.8 | num_updates 9733 | lr 0.000320536 | gnorm 0.945 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 32050
2022-03-04 18:15:04 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 18:15:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:18:40 | INFO | train_inner | epoch 051:     67 / 196 loss=3.765, nll_loss=3.298, ppl=9.83, wps=19552.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.962, loss_scale=8, train_wall=301, gb_free=19.9, wall=32266
2022-03-04 18:24:03 | INFO | train_inner | epoch 051:    167 / 196 loss=3.794, nll_loss=3.327, ppl=10.03, wps=20303.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.965, loss_scale=16, train_wall=299, gb_free=19.9, wall=32589
2022-03-04 18:25:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:25:41 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.545 | nll_loss 8.18 | ppl 290.08 | wps 38095.6 | wpb 510.9 | bsz 1 | num_updates 9929 | best_loss 7.12
2022-03-04 18:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9929 updates
2022-03-04 18:25:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:25:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:25:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 51 @ 9929 updates, score 8.545) (writing took 3.15424219192937 seconds)
2022-03-04 18:25:44 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 18:25:44 | INFO | train | epoch 051 | loss 3.775 | nll_loss 3.308 | ppl 9.9 | wps 20014.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 9929 | lr 0.000317356 | gnorm 0.962 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 32691
2022-03-04 18:25:44 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 18:25:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:27:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:29:37 | INFO | train_inner | epoch 052:     72 / 196 loss=3.732, nll_loss=3.263, ppl=9.6, wps=19539.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.955, loss_scale=16, train_wall=301, gb_free=19.9, wall=32923
2022-03-04 18:34:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:34:58 | INFO | train_inner | epoch 052:    173 / 196 loss=3.768, nll_loss=3.299, ppl=9.85, wps=20416.5, ups=0.31, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.944, loss_scale=16, train_wall=298, gb_free=19.9, wall=33244
2022-03-04 18:36:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:36:16 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.601 | nll_loss 8.235 | ppl 301.23 | wps 37743.3 | wpb 510.9 | bsz 1 | num_updates 10123 | best_loss 7.12
2022-03-04 18:36:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10123 updates
2022-03-04 18:36:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 52 @ 10123 updates, score 8.601) (writing took 3.1350693968124688 seconds)
2022-03-04 18:36:19 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 18:36:19 | INFO | train | epoch 052 | loss 3.741 | nll_loss 3.272 | ppl 9.66 | wps 20002 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10123 | lr 0.000314301 | gnorm 0.942 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 33325
2022-03-04 18:36:19 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 18:36:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:40:23 | INFO | train_inner | epoch 053:     77 / 196 loss=3.685, nll_loss=3.214, ppl=9.28, wps=20129.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.949, loss_scale=16, train_wall=293, gb_free=19.9, wall=33569
2022-03-04 18:40:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:45:46 | INFO | train_inner | epoch 053:    178 / 196 loss=3.746, nll_loss=3.276, ppl=9.69, wps=20257.3, ups=0.31, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.959, loss_scale=8, train_wall=300, gb_free=19.9, wall=33893
2022-03-04 18:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:46:49 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.629 | nll_loss 8.257 | ppl 306.02 | wps 38491.5 | wpb 510.9 | bsz 1 | num_updates 10318 | best_loss 7.12
2022-03-04 18:46:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10318 updates
2022-03-04 18:46:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:46:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:46:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 53 @ 10318 updates, score 8.629) (writing took 3.1836083692032844 seconds)
2022-03-04 18:46:52 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 18:46:52 | INFO | train | epoch 053 | loss 3.711 | nll_loss 3.24 | ppl 9.45 | wps 20160.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10318 | lr 0.000311317 | gnorm 0.959 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 33958
2022-03-04 18:46:52 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 18:46:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:48:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:51:19 | INFO | train_inner | epoch 054:     83 / 196 loss=3.655, nll_loss=3.182, ppl=9.08, wps=19662.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.954, loss_scale=8, train_wall=300, gb_free=19.9, wall=34225
2022-03-04 18:54:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:56:43 | INFO | train_inner | epoch 054:    184 / 196 loss=3.715, nll_loss=3.244, ppl=9.47, wps=20215.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.978, loss_scale=8, train_wall=300, gb_free=19.9, wall=34549
2022-03-04 18:57:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:57:27 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.68 | nll_loss 8.314 | ppl 318.35 | wps 38549.1 | wpb 510.9 | bsz 1 | num_updates 10512 | best_loss 7.12
2022-03-04 18:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10512 updates
2022-03-04 18:57:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:57:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:57:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 54 @ 10512 updates, score 8.68) (writing took 3.1280090000946075 seconds)
2022-03-04 18:57:30 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 18:57:30 | INFO | train | epoch 054 | loss 3.679 | nll_loss 3.207 | ppl 9.23 | wps 19919.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 10512 | lr 0.000308431 | gnorm 0.966 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 34596
2022-03-04 18:57:30 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 18:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:02:12 | INFO | train_inner | epoch 055:     88 / 196 loss=3.615, nll_loss=3.14, ppl=8.81, wps=19861.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.962, loss_scale=16, train_wall=297, gb_free=19.9, wall=34878
2022-03-04 19:07:33 | INFO | train_inner | epoch 055:    188 / 196 loss=3.69, nll_loss=3.218, ppl=9.3, wps=20418.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.979, loss_scale=16, train_wall=297, gb_free=19.9, wall=35199
2022-03-04 19:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:08:04 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.749 | nll_loss 8.384 | ppl 334.13 | wps 38312.1 | wpb 510.9 | bsz 1 | num_updates 10708 | best_loss 7.12
2022-03-04 19:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10708 updates
2022-03-04 19:08:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:08:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 55 @ 10708 updates, score 8.749) (writing took 3.129425717983395 seconds)
2022-03-04 19:08:07 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 19:08:07 | INFO | train | epoch 055 | loss 3.65 | nll_loss 3.176 | ppl 9.04 | wps 20130.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 10708 | lr 0.000305595 | gnorm 0.971 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 35233
2022-03-04 19:08:07 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 19:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:08:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:12:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:13:09 | INFO | train_inner | epoch 056:     94 / 196 loss=3.581, nll_loss=3.104, ppl=8.6, wps=19484.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.971, loss_scale=8, train_wall=302, gb_free=19.9, wall=35535
2022-03-04 19:18:29 | INFO | train_inner | epoch 056:    194 / 196 loss=3.667, nll_loss=3.193, ppl=9.14, wps=20430.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=10900, lr=0.000302891, gnorm=1, loss_scale=8, train_wall=297, gb_free=19.9, wall=35856
2022-03-04 19:18:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:18:41 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.792 | nll_loss 8.425 | ppl 343.71 | wps 38508.5 | wpb 510.9 | bsz 1 | num_updates 10902 | best_loss 7.12
2022-03-04 19:18:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10902 updates
2022-03-04 19:18:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:18:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:18:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 56 @ 10902 updates, score 8.792) (writing took 3.0490741340909153 seconds)
2022-03-04 19:18:44 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 19:18:44 | INFO | train | epoch 056 | loss 3.619 | nll_loss 3.144 | ppl 8.84 | wps 19937.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 10902 | lr 0.000302863 | gnorm 0.987 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 35870
2022-03-04 19:18:44 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 19:18:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:20:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:24:02 | INFO | train_inner | epoch 057:     99 / 196 loss=3.545, nll_loss=3.066, ppl=8.38, wps=19672, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.974, loss_scale=8, train_wall=300, gb_free=19.9, wall=36188
2022-03-04 19:29:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:29:18 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.808 | nll_loss 8.442 | ppl 347.8 | wps 38957.8 | wpb 510.9 | bsz 1 | num_updates 11097 | best_loss 7.12
2022-03-04 19:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11097 updates
2022-03-04 19:29:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:29:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:29:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 57 @ 11097 updates, score 8.808) (writing took 3.096118127927184 seconds)
2022-03-04 19:29:21 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 19:29:21 | INFO | train | epoch 057 | loss 3.593 | nll_loss 3.116 | ppl 8.67 | wps 20028.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11097 | lr 0.000300191 | gnorm 0.987 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 36507
2022-03-04 19:29:21 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 19:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:29:31 | INFO | train_inner | epoch 058:      3 / 196 loss=3.638, nll_loss=3.163, ppl=8.96, wps=19863.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=1.003, loss_scale=16, train_wall=297, gb_free=19.9, wall=36517
2022-03-04 19:33:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:34:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:34:58 | INFO | train_inner | epoch 058:    105 / 196 loss=3.522, nll_loss=3.042, ppl=8.23, wps=20020.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.971, loss_scale=8, train_wall=303, gb_free=19.9, wall=36844
2022-03-04 19:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:39:55 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.851 | nll_loss 8.484 | ppl 358.08 | wps 38947 | wpb 510.9 | bsz 1 | num_updates 11291 | best_loss 7.12
2022-03-04 19:39:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11291 updates
2022-03-04 19:39:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:39:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:39:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 58 @ 11291 updates, score 8.851) (writing took 3.110510051017627 seconds)
2022-03-04 19:39:58 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 19:39:58 | INFO | train | epoch 058 | loss 3.566 | nll_loss 3.087 | ppl 8.5 | wps 19933.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 11291 | lr 0.000297601 | gnorm 0.99 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 37144
2022-03-04 19:39:58 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 19:39:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:40:27 | INFO | train_inner | epoch 059:      9 / 196 loss=3.604, nll_loss=3.127, ppl=8.73, wps=19876.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=1.01, loss_scale=8, train_wall=296, gb_free=19.9, wall=37173
2022-03-04 19:42:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:45:51 | INFO | train_inner | epoch 059:    110 / 196 loss=3.502, nll_loss=3.021, ppl=8.12, wps=20238.2, ups=0.31, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.985, loss_scale=8, train_wall=300, gb_free=19.9, wall=37497
2022-03-04 19:49:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:50:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:50:31 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.901 | nll_loss 8.533 | ppl 370.42 | wps 38586.6 | wpb 510.9 | bsz 1 | num_updates 11485 | best_loss 7.12
2022-03-04 19:50:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11485 updates
2022-03-04 19:50:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:50:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:50:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 59 @ 11485 updates, score 8.901) (writing took 3.1398752161767334 seconds)
2022-03-04 19:50:35 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 19:50:35 | INFO | train | epoch 059 | loss 3.538 | nll_loss 3.059 | ppl 8.33 | wps 19941.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 11485 | lr 0.000295076 | gnorm 0.998 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 37781
2022-03-04 19:50:35 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 19:50:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:51:23 | INFO | train_inner | epoch 060:     15 / 196 loss=3.565, nll_loss=3.086, ppl=8.49, wps=19684.1, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=1.002, loss_scale=8, train_wall=299, gb_free=19.9, wall=37829
2022-03-04 19:56:44 | INFO | train_inner | epoch 060:    115 / 196 loss=3.484, nll_loss=3.002, ppl=8.01, wps=20402.9, ups=0.31, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=1.002, loss_scale=16, train_wall=298, gb_free=19.9, wall=38150
2022-03-04 20:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:01:09 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.931 | nll_loss 8.564 | ppl 378.48 | wps 38711.4 | wpb 510.9 | bsz 1 | num_updates 11681 | best_loss 7.12
2022-03-04 20:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11681 updates
2022-03-04 20:01:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:01:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 60 @ 11681 updates, score 8.931) (writing took 3.9350797520019114 seconds)
2022-03-04 20:01:13 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 20:01:13 | INFO | train | epoch 060 | loss 3.514 | nll_loss 3.033 | ppl 8.19 | wps 20101.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 11681 | lr 0.00029259 | gnorm 0.992 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 38419
2022-03-04 20:01:13 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 20:01:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:02:14 | INFO | train_inner | epoch 061:     19 / 196 loss=3.535, nll_loss=3.055, ppl=8.31, wps=19814.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.981, loss_scale=16, train_wall=297, gb_free=19.9, wall=38480
2022-03-04 20:03:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:05:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:07:41 | INFO | train_inner | epoch 061:    121 / 196 loss=3.463, nll_loss=2.98, ppl=7.89, wps=20018.5, ups=0.31, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=1.001, loss_scale=8, train_wall=303, gb_free=19.9, wall=38807
2022-03-04 20:11:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:11:47 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.004 | nll_loss 8.639 | ppl 398.74 | wps 38366.6 | wpb 510.9 | bsz 1 | num_updates 11875 | best_loss 7.12
2022-03-04 20:11:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11875 updates
2022-03-04 20:11:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:11:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 61 @ 11875 updates, score 9.004) (writing took 3.061248352052644 seconds)
2022-03-04 20:11:50 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 20:11:50 | INFO | train | epoch 061 | loss 3.489 | nll_loss 3.007 | ppl 8.04 | wps 19930.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 11875 | lr 0.000290191 | gnorm 1 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 39056
2022-03-04 20:11:50 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 20:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:13:10 | INFO | train_inner | epoch 062:     25 / 196 loss=3.505, nll_loss=3.023, ppl=8.13, wps=19879, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.998, loss_scale=16, train_wall=297, gb_free=19.9, wall=39136
2022-03-04 20:13:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:18:34 | INFO | train_inner | epoch 062:    126 / 196 loss=3.443, nll_loss=2.959, ppl=7.78, wps=20224.4, ups=0.31, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=1.007, loss_scale=8, train_wall=300, gb_free=19.9, wall=39460
2022-03-04 20:21:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:22:24 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.025 | nll_loss 8.662 | ppl 405.16 | wps 38492.8 | wpb 510.9 | bsz 1 | num_updates 12069 | best_loss 7.12
2022-03-04 20:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12069 updates
2022-03-04 20:22:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:22:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:22:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 62 @ 12069 updates, score 9.025) (writing took 3.075718860840425 seconds)
2022-03-04 20:22:27 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 20:22:27 | INFO | train | epoch 062 | loss 3.464 | nll_loss 2.981 | ppl 7.9 | wps 19935.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 12069 | lr 0.000287849 | gnorm 0.999 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 39693
2022-03-04 20:22:27 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 20:22:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:24:06 | INFO | train_inner | epoch 063:     31 / 196 loss=3.473, nll_loss=2.99, ppl=7.94, wps=19669.6, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=1.001, loss_scale=8, train_wall=300, gb_free=19.9, wall=39793
2022-03-04 20:29:27 | INFO | train_inner | epoch 063:    131 / 196 loss=3.427, nll_loss=2.942, ppl=7.69, wps=20434, ups=0.31, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=1.016, loss_scale=16, train_wall=297, gb_free=19.9, wall=40113
2022-03-04 20:30:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:32:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:33:00 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.038 | nll_loss 8.675 | ppl 408.79 | wps 38608.6 | wpb 510.9 | bsz 1 | num_updates 12264 | best_loss 7.12
2022-03-04 20:33:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12264 updates
2022-03-04 20:33:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:33:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:33:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 63 @ 12264 updates, score 9.038) (writing took 3.10146566783078 seconds)
2022-03-04 20:33:04 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 20:33:04 | INFO | train | epoch 063 | loss 3.441 | nll_loss 2.956 | ppl 7.76 | wps 20039.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12264 | lr 0.000285551 | gnorm 1.016 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 40330
2022-03-04 20:33:04 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 20:33:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:34:59 | INFO | train_inner | epoch 064:     36 / 196 loss=3.443, nll_loss=2.959, ppl=7.77, wps=19680, ups=0.3, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=1.02, loss_scale=8, train_wall=300, gb_free=19.9, wall=40445
2022-03-04 20:38:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:40:23 | INFO | train_inner | epoch 064:    137 / 196 loss=3.407, nll_loss=2.921, ppl=7.57, wps=20219.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=1.009, loss_scale=8, train_wall=300, gb_free=19.9, wall=40770
2022-03-04 20:43:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:43:38 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.081 | nll_loss 8.714 | ppl 420.02 | wps 38940.5 | wpb 510.9 | bsz 1 | num_updates 12459 | best_loss 7.12
2022-03-04 20:43:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12459 updates
2022-03-04 20:43:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:43:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:43:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 64 @ 12459 updates, score 9.081) (writing took 3.1023952029645443 seconds)
2022-03-04 20:43:41 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 20:43:41 | INFO | train | epoch 064 | loss 3.418 | nll_loss 2.932 | ppl 7.63 | wps 20031.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12459 | lr 0.000283308 | gnorm 1.009 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 40967
2022-03-04 20:43:41 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 20:43:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:45:52 | INFO | train_inner | epoch 065:     41 / 196 loss=3.42, nll_loss=2.934, ppl=7.64, wps=19864.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=1.015, loss_scale=16, train_wall=297, gb_free=19.9, wall=41099
2022-03-04 20:47:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:51:17 | INFO | train_inner | epoch 065:    142 / 196 loss=3.392, nll_loss=2.904, ppl=7.49, wps=20217.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=12600, lr=0.000281718, gnorm=1.031, loss_scale=8, train_wall=300, gb_free=19.9, wall=41423
2022-03-04 20:54:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:54:15 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.108 | nll_loss 8.744 | ppl 428.72 | wps 38824.1 | wpb 510.9 | bsz 1 | num_updates 12654 | best_loss 7.12
2022-03-04 20:54:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12654 updates
2022-03-04 20:54:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:54:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:54:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 65 @ 12654 updates, score 9.108) (writing took 3.1236204211600125 seconds)
2022-03-04 20:54:18 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 20:54:18 | INFO | train | epoch 065 | loss 3.396 | nll_loss 2.909 | ppl 7.51 | wps 20031.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12654 | lr 0.000281116 | gnorm 1.031 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 41604
2022-03-04 20:54:18 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 20:54:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:56:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:56:49 | INFO | train_inner | epoch 066:     47 / 196 loss=3.388, nll_loss=2.901, ppl=7.47, wps=19675.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=1.014, loss_scale=8, train_wall=300, gb_free=19.9, wall=41755
2022-03-04 21:02:10 | INFO | train_inner | epoch 066:    147 / 196 loss=3.372, nll_loss=2.884, ppl=7.38, wps=20419.6, ups=0.31, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=1.035, loss_scale=8, train_wall=297, gb_free=19.9, wall=42076
2022-03-04 21:04:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:04:52 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.179 | nll_loss 8.815 | ppl 450.51 | wps 39052.2 | wpb 510.9 | bsz 1 | num_updates 12849 | best_loss 7.12
2022-03-04 21:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12849 updates
2022-03-04 21:04:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 66 @ 12849 updates, score 9.179) (writing took 3.06334803882055 seconds)
2022-03-04 21:04:55 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 21:04:55 | INFO | train | epoch 066 | loss 3.375 | nll_loss 2.887 | ppl 7.4 | wps 20035.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12849 | lr 0.000278975 | gnorm 1.023 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 42241
2022-03-04 21:04:55 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 21:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:05:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:07:42 | INFO | train_inner | epoch 067:     52 / 196 loss=3.36, nll_loss=2.872, ppl=7.32, wps=19680.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=1.022, loss_scale=8, train_wall=299, gb_free=19.9, wall=42408
2022-03-04 21:12:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:13:06 | INFO | train_inner | epoch 067:    153 / 196 loss=3.363, nll_loss=2.875, ppl=7.33, wps=20217.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=1.036, loss_scale=8, train_wall=300, gb_free=19.9, wall=42732
2022-03-04 21:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:15:29 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.247 | nll_loss 8.887 | ppl 473.55 | wps 38546.2 | wpb 510.9 | bsz 1 | num_updates 13043 | best_loss 7.12
2022-03-04 21:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13043 updates
2022-03-04 21:15:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 67 @ 13043 updates, score 9.247) (writing took 3.1196322420146316 seconds)
2022-03-04 21:15:32 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 21:15:32 | INFO | train | epoch 067 | loss 3.353 | nll_loss 2.864 | ppl 7.28 | wps 19927.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 13043 | lr 0.000276893 | gnorm 1.032 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 42878
2022-03-04 21:15:32 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 21:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:18:35 | INFO | train_inner | epoch 068:     57 / 196 loss=3.336, nll_loss=2.846, ppl=7.19, wps=19872.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=1.03, loss_scale=8, train_wall=297, gb_free=19.9, wall=43061
2022-03-04 21:21:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:23:59 | INFO | train_inner | epoch 068:    158 / 196 loss=3.34, nll_loss=2.85, ppl=7.21, wps=20222.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=1.035, loss_scale=8, train_wall=300, gb_free=19.9, wall=43385
2022-03-04 21:26:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:26:06 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.287 | nll_loss 8.927 | ppl 486.74 | wps 38919.1 | wpb 510.9 | bsz 1 | num_updates 13238 | best_loss 7.12
2022-03-04 21:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13238 updates
2022-03-04 21:26:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 68 @ 13238 updates, score 9.287) (writing took 3.0491753229871392 seconds)
2022-03-04 21:26:09 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 21:26:09 | INFO | train | epoch 068 | loss 3.333 | nll_loss 2.843 | ppl 7.17 | wps 20034.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13238 | lr 0.000274846 | gnorm 1.034 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 43515
2022-03-04 21:26:09 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 21:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:29:28 | INFO | train_inner | epoch 069:     62 / 196 loss=3.308, nll_loss=2.817, ppl=7.05, wps=19871.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=1.027, loss_scale=16, train_wall=297, gb_free=19.9, wall=43714
2022-03-04 21:30:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:34:52 | INFO | train_inner | epoch 069:    163 / 196 loss=3.327, nll_loss=2.837, ppl=7.14, wps=20231.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=1.048, loss_scale=8, train_wall=300, gb_free=19.9, wall=44038
2022-03-04 21:36:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:36:42 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.283 | nll_loss 8.918 | ppl 483.74 | wps 39196.1 | wpb 510.9 | bsz 1 | num_updates 13433 | best_loss 7.12
2022-03-04 21:36:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13433 updates
2022-03-04 21:36:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:36:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:36:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 69 @ 13433 updates, score 9.283) (writing took 3.058742901077494 seconds)
2022-03-04 21:36:46 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 21:36:46 | INFO | train | epoch 069 | loss 3.313 | nll_loss 2.822 | ppl 7.07 | wps 20049.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13433 | lr 0.000272843 | gnorm 1.04 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 44152
2022-03-04 21:36:46 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 21:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:40:21 | INFO | train_inner | epoch 070:     67 / 196 loss=3.282, nll_loss=2.79, ppl=6.92, wps=19879.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=1.028, loss_scale=16, train_wall=296, gb_free=19.9, wall=44367
2022-03-04 21:43:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:45:45 | INFO | train_inner | epoch 070:    168 / 196 loss=3.314, nll_loss=2.823, ppl=7.08, wps=20205.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=1.029, loss_scale=8, train_wall=300, gb_free=19.9, wall=44691
2022-03-04 21:47:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:47:20 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.322 | nll_loss 8.957 | ppl 496.84 | wps 38690.5 | wpb 510.9 | bsz 1 | num_updates 13628 | best_loss 7.12
2022-03-04 21:47:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13628 updates
2022-03-04 21:47:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:47:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:47:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 70 @ 13628 updates, score 9.322) (writing took 3.025237383088097 seconds)
2022-03-04 21:47:23 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 21:47:23 | INFO | train | epoch 070 | loss 3.292 | nll_loss 2.8 | ppl 6.96 | wps 20024.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13628 | lr 0.000270884 | gnorm 1.027 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 44789
2022-03-04 21:47:23 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 21:47:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:51:14 | INFO | train_inner | epoch 071:     72 / 196 loss=3.254, nll_loss=2.761, ppl=6.78, wps=19857.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=1.035, loss_scale=16, train_wall=297, gb_free=19.9, wall=45020
2022-03-04 21:53:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:56:38 | INFO | train_inner | epoch 071:    173 / 196 loss=3.299, nll_loss=2.807, ppl=7, wps=20223.3, ups=0.31, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=1.054, loss_scale=8, train_wall=300, gb_free=19.9, wall=45344
2022-03-04 21:57:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:57:57 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.368 | nll_loss 9.007 | ppl 514.32 | wps 38911.7 | wpb 510.9 | bsz 1 | num_updates 13823 | best_loss 7.12
2022-03-04 21:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13823 updates
2022-03-04 21:57:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:58:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:58:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 71 @ 13823 updates, score 9.368) (writing took 3.0299936961382627 seconds)
2022-03-04 21:58:00 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 21:58:00 | INFO | train | epoch 071 | loss 3.274 | nll_loss 2.781 | ppl 6.87 | wps 20030.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13823 | lr 0.000268967 | gnorm 1.044 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 45426
2022-03-04 21:58:00 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 21:58:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:02:07 | INFO | train_inner | epoch 072:     77 / 196 loss=3.235, nll_loss=2.741, ppl=6.68, wps=19860.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=1.041, loss_scale=16, train_wall=297, gb_free=19.9, wall=45674
2022-03-04 22:03:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:07:31 | INFO | train_inner | epoch 072:    178 / 196 loss=3.283, nll_loss=2.791, ppl=6.92, wps=20226, ups=0.31, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=1.055, loss_scale=8, train_wall=300, gb_free=19.9, wall=45998
2022-03-04 22:08:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:08:34 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.401 | nll_loss 9.041 | ppl 526.61 | wps 38689.2 | wpb 510.9 | bsz 1 | num_updates 14018 | best_loss 7.12
2022-03-04 22:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14018 updates
2022-03-04 22:08:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:08:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:08:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 72 @ 14018 updates, score 9.401) (writing took 3.084607477998361 seconds)
2022-03-04 22:08:37 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 22:08:37 | INFO | train | epoch 072 | loss 3.256 | nll_loss 2.762 | ppl 6.78 | wps 20032.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14018 | lr 0.00026709 | gnorm 1.049 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 46063
2022-03-04 22:08:37 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 22:08:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:10:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:13:03 | INFO | train_inner | epoch 073:     83 / 196 loss=3.212, nll_loss=2.716, ppl=6.57, wps=19685.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=1.054, loss_scale=8, train_wall=299, gb_free=19.9, wall=46330
2022-03-04 22:18:24 | INFO | train_inner | epoch 073:    183 / 196 loss=3.268, nll_loss=2.774, ppl=6.84, wps=20421, ups=0.31, wpb=65532.4, bsz=128, num_updates=14200, lr=0.000265372, gnorm=1.06, loss_scale=16, train_wall=297, gb_free=19.9, wall=46651
2022-03-04 22:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:19:11 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.418 | nll_loss 9.054 | ppl 531.69 | wps 38883.5 | wpb 510.9 | bsz 1 | num_updates 14213 | best_loss 7.12
2022-03-04 22:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14213 updates
2022-03-04 22:19:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 73 @ 14213 updates, score 9.418) (writing took 3.052077694097534 seconds)
2022-03-04 22:19:14 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 22:19:14 | INFO | train | epoch 073 | loss 3.237 | nll_loss 2.742 | ppl 6.69 | wps 20037.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14213 | lr 0.000265251 | gnorm 1.058 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 46700
2022-03-04 22:19:14 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 22:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:23:53 | INFO | train_inner | epoch 074:     87 / 196 loss=3.188, nll_loss=2.691, ppl=6.46, wps=19858.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=1.036, loss_scale=16, train_wall=297, gb_free=19.9, wall=46980
2022-03-04 22:24:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:27:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:29:21 | INFO | train_inner | epoch 074:    189 / 196 loss=3.258, nll_loss=2.764, ppl=6.79, wps=20017.5, ups=0.31, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=1.051, loss_scale=8, train_wall=303, gb_free=19.9, wall=47307
2022-03-04 22:29:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:29:48 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.461 | nll_loss 9.1 | ppl 548.82 | wps 38943 | wpb 510.9 | bsz 1 | num_updates 14407 | best_loss 7.12
2022-03-04 22:29:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14407 updates
2022-03-04 22:29:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:29:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:29:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 74 @ 14407 updates, score 9.461) (writing took 3.09706894797273 seconds)
2022-03-04 22:29:51 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 22:29:51 | INFO | train | epoch 074 | loss 3.219 | nll_loss 2.723 | ppl 6.6 | wps 19922.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 14407 | lr 0.000263459 | gnorm 1.041 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 47337
2022-03-04 22:29:51 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 22:29:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:34:50 | INFO | train_inner | epoch 075:     93 / 196 loss=3.156, nll_loss=2.657, ppl=6.31, wps=19867, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=1.068, loss_scale=16, train_wall=297, gb_free=19.9, wall=47636
2022-03-04 22:35:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:40:13 | INFO | train_inner | epoch 075:    194 / 196 loss=3.255, nll_loss=2.76, ppl=6.78, wps=20283.2, ups=0.31, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=1.07, loss_scale=8, train_wall=299, gb_free=19.9, wall=47959
2022-03-04 22:40:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:40:24 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.441 | nll_loss 9.076 | ppl 539.82 | wps 39721.3 | wpb 510.9 | bsz 1 | num_updates 14602 | best_loss 7.12
2022-03-04 22:40:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14602 updates
2022-03-04 22:40:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:40:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 75 @ 14602 updates, score 9.441) (writing took 3.052824313985184 seconds)
2022-03-04 22:40:27 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 22:40:27 | INFO | train | epoch 075 | loss 3.203 | nll_loss 2.707 | ppl 6.53 | wps 20071.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14602 | lr 0.000261694 | gnorm 1.07 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 47973
2022-03-04 22:40:27 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 22:40:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:43:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:45:40 | INFO | train_inner | epoch 076:     99 / 196 loss=3.151, nll_loss=2.652, ppl=6.28, wps=19960.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=1.064, loss_scale=8, train_wall=296, gb_free=19.9, wall=48287
2022-03-04 22:50:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:50:52 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.54 | nll_loss 9.179 | ppl 579.75 | wps 40278.5 | wpb 510.9 | bsz 1 | num_updates 14797 | best_loss 7.12
2022-03-04 22:50:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14797 updates
2022-03-04 22:50:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:50:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:50:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 76 @ 14797 updates, score 9.54) (writing took 3.017114450922236 seconds)
2022-03-04 22:50:55 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 22:50:55 | INFO | train | epoch 076 | loss 3.186 | nll_loss 2.689 | ppl 6.45 | wps 20335.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14797 | lr 0.000259964 | gnorm 1.059 | loss_scale 16 | train_wall 574 | gb_free 19.9 | wall 48601
2022-03-04 22:50:55 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 22:50:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:51:04 | INFO | train_inner | epoch 077:      3 / 196 loss=3.221, nll_loss=2.726, ppl=6.61, wps=20184.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.055, loss_scale=16, train_wall=293, gb_free=19.9, wall=48611
2022-03-04 22:53:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:56:24 | INFO | train_inner | epoch 077:    104 / 196 loss=3.128, nll_loss=2.628, ppl=6.18, wps=20514.8, ups=0.31, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=1.068, loss_scale=8, train_wall=296, gb_free=19.9, wall=48930
2022-03-04 23:00:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:01:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:01:19 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.562 | nll_loss 9.202 | ppl 589 | wps 40029.9 | wpb 510.9 | bsz 1 | num_updates 14991 | best_loss 7.12
2022-03-04 23:01:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14991 updates
2022-03-04 23:01:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:01:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:01:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 77 @ 14991 updates, score 9.562) (writing took 3.0816208138130605 seconds)
2022-03-04 23:01:22 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 23:01:22 | INFO | train | epoch 077 | loss 3.168 | nll_loss 2.67 | ppl 6.37 | wps 20225.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14991 | lr 0.000258276 | gnorm 1.072 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 49229
2022-03-04 23:01:22 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 23:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:01:51 | INFO | train_inner | epoch 078:      9 / 196 loss=3.201, nll_loss=2.704, ppl=6.52, wps=19973.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.072, loss_scale=8, train_wall=296, gb_free=19.9, wall=49257
2022-03-04 23:07:07 | INFO | train_inner | epoch 078:    109 / 196 loss=3.117, nll_loss=2.617, ppl=6.13, wps=20717.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=1.075, loss_scale=8, train_wall=294, gb_free=19.9, wall=49574
2022-03-04 23:08:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:11:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:11:47 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.563 | nll_loss 9.199 | ppl 587.87 | wps 40244.1 | wpb 510.9 | bsz 1 | num_updates 15186 | best_loss 7.12
2022-03-04 23:11:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15186 updates
2022-03-04 23:11:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:11:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 78 @ 15186 updates, score 9.563) (writing took 3.064742579124868 seconds)
2022-03-04 23:11:50 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 23:11:50 | INFO | train | epoch 078 | loss 3.155 | nll_loss 2.656 | ppl 6.3 | wps 20329.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15186 | lr 0.000256613 | gnorm 1.069 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 49856
2022-03-04 23:11:50 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 23:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:12:35 | INFO | train_inner | epoch 079:     14 / 196 loss=3.187, nll_loss=2.69, ppl=6.45, wps=19975.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.068, loss_scale=8, train_wall=296, gb_free=19.9, wall=49901
2022-03-04 23:17:51 | INFO | train_inner | epoch 079:    114 / 196 loss=3.108, nll_loss=2.607, ppl=6.09, wps=20709.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.071, loss_scale=16, train_wall=294, gb_free=19.9, wall=50217
2022-03-04 23:20:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:22:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:22:15 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.637 | nll_loss 9.274 | ppl 619.28 | wps 40121.6 | wpb 510.9 | bsz 1 | num_updates 15381 | best_loss 7.12
2022-03-04 23:22:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15381 updates
2022-03-04 23:22:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:22:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:22:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 79 @ 15381 updates, score 9.637) (writing took 3.009439652087167 seconds)
2022-03-04 23:22:18 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 23:22:18 | INFO | train | epoch 079 | loss 3.138 | nll_loss 2.639 | ppl 6.23 | wps 20331.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15381 | lr 0.000254981 | gnorm 1.069 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 50484
2022-03-04 23:22:18 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 23:22:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:23:18 | INFO | train_inner | epoch 080:     19 / 196 loss=3.165, nll_loss=2.667, ppl=6.35, wps=19977.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.064, loss_scale=8, train_wall=296, gb_free=19.9, wall=50544
2022-03-04 23:28:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:28:43 | INFO | train_inner | epoch 080:    120 / 196 loss=3.1, nll_loss=2.598, ppl=6.06, wps=20162.2, ups=0.31, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=1.086, loss_scale=8, train_wall=301, gb_free=19.9, wall=50869
2022-03-04 23:32:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:32:53 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.657 | nll_loss 9.295 | ppl 628.04 | wps 38797.9 | wpb 510.9 | bsz 1 | num_updates 15576 | best_loss 7.12
2022-03-04 23:32:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15576 updates
2022-03-04 23:32:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:32:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:32:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 80 @ 15576 updates, score 9.657) (writing took 3.062742498004809 seconds)
2022-03-04 23:32:56 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 23:32:56 | INFO | train | epoch 080 | loss 3.124 | nll_loss 2.623 | ppl 6.16 | wps 20013.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15576 | lr 0.00025338 | gnorm 1.076 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 51122
2022-03-04 23:32:56 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 23:32:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:34:13 | INFO | train_inner | epoch 081:     24 / 196 loss=3.137, nll_loss=2.637, ppl=6.22, wps=19820.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.078, loss_scale=8, train_wall=297, gb_free=19.9, wall=51199
2022-03-04 23:35:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:39:38 | INFO | train_inner | epoch 081:    125 / 196 loss=3.087, nll_loss=2.585, ppl=6, wps=20160.9, ups=0.31, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=1.091, loss_scale=8, train_wall=301, gb_free=19.9, wall=51524
2022-03-04 23:42:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:43:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:43:31 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.678 | nll_loss 9.317 | ppl 637.83 | wps 38205.2 | wpb 510.9 | bsz 1 | num_updates 15770 | best_loss 7.12
2022-03-04 23:43:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15770 updates
2022-03-04 23:43:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:43:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:43:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 81 @ 15770 updates, score 9.678) (writing took 3.1238340260460973 seconds)
2022-03-04 23:43:35 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 23:43:35 | INFO | train | epoch 081 | loss 3.108 | nll_loss 2.607 | ppl 6.09 | wps 19872.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 15770 | lr 0.000251816 | gnorm 1.093 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 51761
2022-03-04 23:43:35 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 23:43:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:45:11 | INFO | train_inner | epoch 082:     30 / 196 loss=3.119, nll_loss=2.618, ppl=6.14, wps=19629.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.098, loss_scale=8, train_wall=300, gb_free=19.9, wall=51857
2022-03-04 23:50:33 | INFO | train_inner | epoch 082:    130 / 196 loss=3.082, nll_loss=2.58, ppl=5.98, wps=20360.3, ups=0.31, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.083, loss_scale=16, train_wall=298, gb_free=19.9, wall=52179
2022-03-04 23:51:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:54:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:54:10 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.713 | nll_loss 9.353 | ppl 654.1 | wps 38697.2 | wpb 510.9 | bsz 1 | num_updates 15965 | best_loss 7.12
2022-03-04 23:54:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15965 updates
2022-03-04 23:54:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:54:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:54:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 82 @ 15965 updates, score 9.713) (writing took 3.10011911788024 seconds)
2022-03-04 23:54:13 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 23:54:13 | INFO | train | epoch 082 | loss 3.095 | nll_loss 2.593 | ppl 6.03 | wps 19979.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15965 | lr 0.000250274 | gnorm 1.092 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 52399
2022-03-04 23:54:13 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 23:54:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:56:06 | INFO | train_inner | epoch 083:     35 / 196 loss=3.102, nll_loss=2.6, ppl=6.06, wps=19630.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.084, loss_scale=8, train_wall=300, gb_free=19.9, wall=52512
2022-03-04 23:59:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:01:31 | INFO | train_inner | epoch 083:    136 / 196 loss=3.069, nll_loss=2.566, ppl=5.92, wps=20165.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.089, loss_scale=8, train_wall=301, gb_free=19.9, wall=52837
2022-03-05 00:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:04:49 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.692 | nll_loss 9.331 | ppl 643.92 | wps 38160.5 | wpb 510.9 | bsz 1 | num_updates 16160 | best_loss 7.12
2022-03-05 00:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16160 updates
2022-03-05 00:04:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 83 @ 16160 updates, score 9.692) (writing took 3.104969231877476 seconds)
2022-03-05 00:04:52 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-05 00:04:52 | INFO | train | epoch 083 | loss 3.08 | nll_loss 2.578 | ppl 5.97 | wps 19977.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16160 | lr 0.000248759 | gnorm 1.086 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 53038
2022-03-05 00:04:52 | INFO | fairseq.trainer | begin training epoch 84
2022-03-05 00:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:07:01 | INFO | train_inner | epoch 084:     40 / 196 loss=3.082, nll_loss=2.58, ppl=5.98, wps=19807.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.092, loss_scale=16, train_wall=297, gb_free=19.9, wall=53167
2022-03-05 00:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:12:26 | INFO | train_inner | epoch 084:    141 / 196 loss=3.062, nll_loss=2.558, ppl=5.89, wps=20187.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.08, loss_scale=8, train_wall=301, gb_free=19.9, wall=53492
2022-03-05 00:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:15:27 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.766 | nll_loss 9.405 | ppl 678.04 | wps 38723.6 | wpb 510.9 | bsz 1 | num_updates 16355 | best_loss 7.12
2022-03-05 00:15:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16355 updates
2022-03-05 00:15:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:15:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:15:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 84 @ 16355 updates, score 9.766) (writing took 3.1715832019690424 seconds)
2022-03-05 00:15:30 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-05 00:15:30 | INFO | train | epoch 084 | loss 3.066 | nll_loss 2.562 | ppl 5.91 | wps 19997.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16355 | lr 0.000247272 | gnorm 1.085 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 53677
2022-03-05 00:15:30 | INFO | fairseq.trainer | begin training epoch 85
2022-03-05 00:15:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:17:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:17:58 | INFO | train_inner | epoch 085:     46 / 196 loss=3.064, nll_loss=2.56, ppl=5.9, wps=19643.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.085, loss_scale=8, train_wall=300, gb_free=19.9, wall=53825
2022-03-05 00:23:20 | INFO | train_inner | epoch 085:    146 / 196 loss=3.051, nll_loss=2.547, ppl=5.85, wps=20376.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.083, loss_scale=8, train_wall=298, gb_free=19.9, wall=54146
2022-03-05 00:26:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:26:06 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.8 | nll_loss 9.438 | ppl 693.77 | wps 38086.1 | wpb 510.9 | bsz 1 | num_updates 16550 | best_loss 7.12
2022-03-05 00:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16550 updates
2022-03-05 00:26:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 85 @ 16550 updates, score 9.8) (writing took 3.0694042118266225 seconds)
2022-03-05 00:26:09 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-05 00:26:09 | INFO | train | epoch 085 | loss 3.052 | nll_loss 2.548 | ppl 5.85 | wps 19990 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16550 | lr 0.000245811 | gnorm 1.083 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 54315
2022-03-05 00:26:09 | INFO | fairseq.trainer | begin training epoch 86
2022-03-05 00:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:28:50 | INFO | train_inner | epoch 086:     50 / 196 loss=3.04, nll_loss=2.536, ppl=5.8, wps=19810.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.075, loss_scale=16, train_wall=297, gb_free=19.9, wall=54476
2022-03-05 00:29:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:34:15 | INFO | train_inner | epoch 086:    151 / 196 loss=3.044, nll_loss=2.539, ppl=5.81, wps=20158.5, ups=0.31, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.095, loss_scale=8, train_wall=301, gb_free=19.9, wall=54801
2022-03-05 00:36:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:36:45 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.829 | nll_loss 9.468 | ppl 708.02 | wps 38648.5 | wpb 510.9 | bsz 1 | num_updates 16745 | best_loss 7.12
2022-03-05 00:36:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16745 updates
2022-03-05 00:36:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:36:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:36:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 86 @ 16745 updates, score 9.829) (writing took 3.1127296700142324 seconds)
2022-03-05 00:36:48 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-05 00:36:48 | INFO | train | epoch 086 | loss 3.038 | nll_loss 2.533 | ppl 5.79 | wps 19973.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16745 | lr 0.000244375 | gnorm 1.09 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 54954
2022-03-05 00:36:48 | INFO | fairseq.trainer | begin training epoch 87
2022-03-05 00:36:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:37:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:39:48 | INFO | train_inner | epoch 087:     56 / 196 loss=3.03, nll_loss=2.525, ppl=5.76, wps=19625.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.096, loss_scale=8, train_wall=300, gb_free=19.9, wall=55134
2022-03-05 00:45:10 | INFO | train_inner | epoch 087:    156 / 196 loss=3.03, nll_loss=2.525, ppl=5.76, wps=20365.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.102, loss_scale=16, train_wall=298, gb_free=19.9, wall=55456
2022-03-05 00:47:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:47:23 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.867 | nll_loss 9.507 | ppl 727.72 | wps 38564.3 | wpb 510.9 | bsz 1 | num_updates 16940 | best_loss 7.12
2022-03-05 00:47:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16940 updates
2022-03-05 00:47:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:47:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:47:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 87 @ 16940 updates, score 9.867) (writing took 3.0905173290520906 seconds)
2022-03-05 00:47:27 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-05 00:47:27 | INFO | train | epoch 087 | loss 3.026 | nll_loss 2.52 | ppl 5.74 | wps 19977.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16940 | lr 0.000242965 | gnorm 1.093 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 55593
2022-03-05 00:47:27 | INFO | fairseq.trainer | begin training epoch 88
2022-03-05 00:47:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:48:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:50:43 | INFO | train_inner | epoch 088:     61 / 196 loss=3.004, nll_loss=2.498, ppl=5.65, wps=19614.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.085, loss_scale=8, train_wall=300, gb_free=19.9, wall=55789
2022-03-05 00:55:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:56:08 | INFO | train_inner | epoch 088:    162 / 196 loss=3.026, nll_loss=2.52, ppl=5.74, wps=20183.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.095, loss_scale=8, train_wall=301, gb_free=19.9, wall=56114
2022-03-05 00:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:58:02 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.881 | nll_loss 9.522 | ppl 735.44 | wps 38244.8 | wpb 510.9 | bsz 1 | num_updates 17134 | best_loss 7.12
2022-03-05 00:58:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17134 updates
2022-03-05 00:58:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:58:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:58:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 88 @ 17134 updates, score 9.881) (writing took 3.2282013159710914 seconds)
2022-03-05 00:58:05 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-05 00:58:05 | INFO | train | epoch 088 | loss 3.013 | nll_loss 2.507 | ppl 5.68 | wps 19876.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 17134 | lr 0.000241585 | gnorm 1.1 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 56232
2022-03-05 00:58:05 | INFO | fairseq.trainer | begin training epoch 89
2022-03-05 00:58:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:01:38 | INFO | train_inner | epoch 089:     66 / 196 loss=2.99, nll_loss=2.483, ppl=5.59, wps=19806.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.117, loss_scale=8, train_wall=297, gb_free=19.9, wall=56444
2022-03-05 01:04:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:07:03 | INFO | train_inner | epoch 089:    167 / 196 loss=3.018, nll_loss=2.513, ppl=5.71, wps=20175.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.106, loss_scale=8, train_wall=301, gb_free=19.9, wall=56769
2022-03-05 01:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:08:41 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.95 | nll_loss 9.595 | ppl 773.44 | wps 38877.2 | wpb 510.9 | bsz 1 | num_updates 17329 | best_loss 7.12
2022-03-05 01:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17329 updates
2022-03-05 01:08:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:08:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 89 @ 17329 updates, score 9.95) (writing took 3.0881212330423295 seconds)
2022-03-05 01:08:44 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-05 01:08:44 | INFO | train | epoch 089 | loss 3 | nll_loss 2.493 | ppl 5.63 | wps 19985.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17329 | lr 0.000240222 | gnorm 1.108 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 56870
2022-03-05 01:08:44 | INFO | fairseq.trainer | begin training epoch 90
2022-03-05 01:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:11:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:12:36 | INFO | train_inner | epoch 090:     72 / 196 loss=2.968, nll_loss=2.46, ppl=5.5, wps=19622.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.101, loss_scale=8, train_wall=300, gb_free=19.9, wall=57102
2022-03-05 01:17:57 | INFO | train_inner | epoch 090:    172 / 196 loss=3.012, nll_loss=2.506, ppl=5.68, wps=20372.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.11, loss_scale=8, train_wall=298, gb_free=19.9, wall=57424
2022-03-05 01:19:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:19:20 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.868 | nll_loss 9.51 | ppl 729.06 | wps 38330.8 | wpb 510.9 | bsz 1 | num_updates 17524 | best_loss 7.12
2022-03-05 01:19:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17524 updates
2022-03-05 01:19:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:19:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 90 @ 17524 updates, score 9.868) (writing took 3.133162901038304 seconds)
2022-03-05 01:19:23 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-05 01:19:23 | INFO | train | epoch 090 | loss 2.988 | nll_loss 2.481 | ppl 5.58 | wps 19977.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17524 | lr 0.000238882 | gnorm 1.105 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 57509
2022-03-05 01:19:23 | INFO | fairseq.trainer | begin training epoch 91
2022-03-05 01:19:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:20:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:23:30 | INFO | train_inner | epoch 091:     77 / 196 loss=2.963, nll_loss=2.455, ppl=5.48, wps=19671.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.104, loss_scale=8, train_wall=300, gb_free=19.9, wall=57756
2022-03-05 01:27:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:28:53 | INFO | train_inner | epoch 091:    178 / 196 loss=3.001, nll_loss=2.494, ppl=5.64, wps=20257.1, ups=0.31, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.109, loss_scale=8, train_wall=300, gb_free=19.9, wall=58079
2022-03-05 01:29:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:29:56 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.959 | nll_loss 9.599 | ppl 775.53 | wps 38838.6 | wpb 510.9 | bsz 1 | num_updates 17718 | best_loss 7.12
2022-03-05 01:29:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17718 updates
2022-03-05 01:29:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:29:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:29:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 91 @ 17718 updates, score 9.959) (writing took 3.0513320302125067 seconds)
2022-03-05 01:29:59 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-05 01:29:59 | INFO | train | epoch 091 | loss 2.976 | nll_loss 2.468 | ppl 5.53 | wps 19960.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 17718 | lr 0.000237571 | gnorm 1.107 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 58145
2022-03-05 01:29:59 | INFO | fairseq.trainer | begin training epoch 92
2022-03-05 01:29:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:22 | INFO | train_inner | epoch 092:     82 / 196 loss=2.937, nll_loss=2.427, ppl=5.38, wps=19896.8, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.107, loss_scale=16, train_wall=296, gb_free=19.9, wall=58408
2022-03-05 01:36:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:39:46 | INFO | train_inner | epoch 092:    183 / 196 loss=2.994, nll_loss=2.487, ppl=5.61, wps=20234.8, ups=0.31, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.108, loss_scale=8, train_wall=300, gb_free=19.9, wall=58732
2022-03-05 01:40:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:40:32 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.972 | nll_loss 9.616 | ppl 784.5 | wps 38794.2 | wpb 510.9 | bsz 1 | num_updates 17913 | best_loss 7.12
2022-03-05 01:40:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17913 updates
2022-03-05 01:40:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:40:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:40:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 92 @ 17913 updates, score 9.972) (writing took 3.136145832017064 seconds)
2022-03-05 01:40:35 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-05 01:40:35 | INFO | train | epoch 092 | loss 2.965 | nll_loss 2.457 | ppl 5.49 | wps 20053.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17913 | lr 0.000236274 | gnorm 1.111 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 58781
2022-03-05 01:40:35 | INFO | fairseq.trainer | begin training epoch 93
2022-03-05 01:40:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:45:14 | INFO | train_inner | epoch 093:     87 / 196 loss=2.926, nll_loss=2.415, ppl=5.33, wps=19888.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.113, loss_scale=16, train_wall=296, gb_free=19.9, wall=59060
2022-03-05 01:49:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:50:38 | INFO | train_inner | epoch 093:    188 / 196 loss=2.985, nll_loss=2.477, ppl=5.57, wps=20249.3, ups=0.31, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.119, loss_scale=8, train_wall=300, gb_free=19.9, wall=59384
2022-03-05 01:51:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:51:09 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.05 | nll_loss 9.692 | ppl 827 | wps 38260 | wpb 510.9 | bsz 1 | num_updates 18108 | best_loss 7.12
2022-03-05 01:51:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18108 updates
2022-03-05 01:51:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:51:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:51:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 93 @ 18108 updates, score 10.05) (writing took 3.0738725350238383 seconds)
2022-03-05 01:51:12 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-05 01:51:12 | INFO | train | epoch 093 | loss 2.952 | nll_loss 2.443 | ppl 5.44 | wps 20053.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18108 | lr 0.000234998 | gnorm 1.112 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 59418
2022-03-05 01:51:12 | INFO | fairseq.trainer | begin training epoch 94
2022-03-05 01:51:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:56:08 | INFO | train_inner | epoch 094:     92 / 196 loss=2.908, nll_loss=2.397, ppl=5.27, wps=19828.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.108, loss_scale=8, train_wall=297, gb_free=19.9, wall=59714
2022-03-05 01:56:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:01:32 | INFO | train_inner | epoch 094:    193 / 196 loss=2.982, nll_loss=2.474, ppl=5.56, wps=20183.2, ups=0.31, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.135, loss_scale=8, train_wall=301, gb_free=19.9, wall=60038
2022-03-05 02:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:01:47 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.015 | nll_loss 9.656 | ppl 807.03 | wps 38676.5 | wpb 510.9 | bsz 1 | num_updates 18303 | best_loss 7.12
2022-03-05 02:01:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18303 updates
2022-03-05 02:01:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:01:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 94 @ 18303 updates, score 10.015) (writing took 3.1364165819250047 seconds)
2022-03-05 02:01:50 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-05 02:01:50 | INFO | train | epoch 094 | loss 2.942 | nll_loss 2.432 | ppl 5.4 | wps 19996 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18303 | lr 0.000233743 | gnorm 1.122 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 60056
2022-03-05 02:01:50 | INFO | fairseq.trainer | begin training epoch 95
2022-03-05 02:01:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:07:02 | INFO | train_inner | epoch 095:     97 / 196 loss=2.885, nll_loss=2.373, ppl=5.18, wps=19822.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=1.109, loss_scale=16, train_wall=297, gb_free=19.9, wall=60368
2022-03-05 02:08:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:12:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:12:25 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.047 | nll_loss 9.69 | ppl 826.16 | wps 38182.4 | wpb 510.9 | bsz 1 | num_updates 18498 | best_loss 7.12
2022-03-05 02:12:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18498 updates
2022-03-05 02:12:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:12:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 95 @ 18498 updates, score 10.047) (writing took 3.0706137360539287 seconds)
2022-03-05 02:12:28 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-05 02:12:28 | INFO | train | epoch 095 | loss 2.931 | nll_loss 2.421 | ppl 5.35 | wps 19996 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18498 | lr 0.000232508 | gnorm 1.117 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 60694
2022-03-05 02:12:28 | INFO | fairseq.trainer | begin training epoch 96
2022-03-05 02:12:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:12:35 | INFO | train_inner | epoch 096:      2 / 196 loss=2.976, nll_loss=2.468, ppl=5.53, wps=19651, ups=0.3, wpb=65367, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.126, loss_scale=8, train_wall=300, gb_free=19.9, wall=60701
2022-03-05 02:16:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:18:00 | INFO | train_inner | epoch 096:    103 / 196 loss=2.877, nll_loss=2.365, ppl=5.15, wps=20172.1, ups=0.31, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.113, loss_scale=8, train_wall=301, gb_free=19.9, wall=61026
2022-03-05 02:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:23:03 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.08 | nll_loss 9.721 | ppl 844.15 | wps 38235 | wpb 510.9 | bsz 1 | num_updates 18693 | best_loss 7.12
2022-03-05 02:23:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18693 updates
2022-03-05 02:23:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 96 @ 18693 updates, score 10.08) (writing took 3.0948452909942716 seconds)
2022-03-05 02:23:07 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-05 02:23:07 | INFO | train | epoch 096 | loss 2.92 | nll_loss 2.409 | ppl 5.31 | wps 19989 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18693 | lr 0.000231292 | gnorm 1.119 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 61333
2022-03-05 02:23:07 | INFO | fairseq.trainer | begin training epoch 97
2022-03-05 02:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:23:29 | INFO | train_inner | epoch 097:      7 / 196 loss=2.955, nll_loss=2.447, ppl=5.45, wps=19826.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.125, loss_scale=8, train_wall=297, gb_free=19.9, wall=61355
2022-03-05 02:27:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:28:54 | INFO | train_inner | epoch 097:    108 / 196 loss=2.874, nll_loss=2.361, ppl=5.14, wps=20159.4, ups=0.31, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.104, loss_scale=8, train_wall=301, gb_free=19.9, wall=61680
2022-03-05 02:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:33:42 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.11 | nll_loss 9.749 | ppl 860.31 | wps 38489.3 | wpb 510.9 | bsz 1 | num_updates 18888 | best_loss 7.12
2022-03-05 02:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18888 updates
2022-03-05 02:33:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:33:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:33:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 97 @ 18888 updates, score 10.11) (writing took 3.086576825939119 seconds)
2022-03-05 02:33:45 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-05 02:33:45 | INFO | train | epoch 097 | loss 2.91 | nll_loss 2.399 | ppl 5.27 | wps 19989 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18888 | lr 0.000230095 | gnorm 1.112 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 61971
2022-03-05 02:33:45 | INFO | fairseq.trainer | begin training epoch 98
2022-03-05 02:33:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:34:24 | INFO | train_inner | epoch 098:     12 / 196 loss=2.941, nll_loss=2.431, ppl=5.39, wps=19839.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.124, loss_scale=8, train_wall=297, gb_free=19.9, wall=62010
2022-03-05 02:39:45 | INFO | train_inner | epoch 098:    112 / 196 loss=2.868, nll_loss=2.355, ppl=5.12, wps=20368.5, ups=0.31, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.121, loss_scale=16, train_wall=298, gb_free=19.9, wall=62332
2022-03-05 02:40:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:44:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:44:21 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.144 | nll_loss 9.783 | ppl 881.26 | wps 38055.3 | wpb 510.9 | bsz 1 | num_updates 19083 | best_loss 7.12
2022-03-05 02:44:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19083 updates
2022-03-05 02:44:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:44:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:44:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 98 @ 19083 updates, score 10.144) (writing took 3.115358646027744 seconds)
2022-03-05 02:44:24 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-05 02:44:24 | INFO | train | epoch 098 | loss 2.899 | nll_loss 2.387 | ppl 5.23 | wps 19982.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19083 | lr 0.000228916 | gnorm 1.125 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 62610
2022-03-05 02:44:24 | INFO | fairseq.trainer | begin training epoch 99
2022-03-05 02:44:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:45:19 | INFO | train_inner | epoch 099:     17 / 196 loss=2.924, nll_loss=2.414, ppl=5.33, wps=19627.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.129, loss_scale=8, train_wall=300, gb_free=19.9, wall=62665
2022-03-05 02:49:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:50:43 | INFO | train_inner | epoch 099:    118 / 196 loss=2.87, nll_loss=2.357, ppl=5.12, wps=20169, ups=0.31, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.12, loss_scale=8, train_wall=301, gb_free=19.9, wall=62990
2022-03-05 02:54:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:54:59 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.159 | nll_loss 9.799 | ppl 890.62 | wps 38940.7 | wpb 510.9 | bsz 1 | num_updates 19278 | best_loss 7.12
2022-03-05 02:54:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19278 updates
2022-03-05 02:54:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:55:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:55:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 99 @ 19278 updates, score 10.159) (writing took 3.116104149026796 seconds)
2022-03-05 02:55:02 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-05 02:55:02 | INFO | train | epoch 099 | loss 2.888 | nll_loss 2.376 | ppl 5.19 | wps 19987.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19278 | lr 0.000227756 | gnorm 1.134 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 63248
2022-03-05 02:55:02 | INFO | fairseq.trainer | begin training epoch 100
2022-03-05 02:55:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:56:13 | INFO | train_inner | epoch 100:     22 / 196 loss=2.898, nll_loss=2.386, ppl=5.23, wps=19845.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.135, loss_scale=8, train_wall=297, gb_free=19.9, wall=63319
2022-03-05 03:01:30 | INFO | train_inner | epoch 100:    122 / 196 loss=2.862, nll_loss=2.348, ppl=5.09, wps=20674.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.121, loss_scale=16, train_wall=294, gb_free=19.9, wall=63636
2022-03-05 03:03:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:04:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:05:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:05:29 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.202 | nll_loss 9.846 | ppl 920.47 | wps 40332.7 | wpb 510.9 | bsz 1 | num_updates 19472 | best_loss 7.12
2022-03-05 03:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19472 updates
2022-03-05 03:05:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:05:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:05:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 100 @ 19472 updates, score 10.202) (writing took 3.0452922449912876 seconds)
2022-03-05 03:05:32 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-05 03:05:32 | INFO | train | epoch 100 | loss 2.878 | nll_loss 2.365 | ppl 5.15 | wps 20170.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19472 | lr 0.000226618 | gnorm 1.12 | loss_scale 8 | train_wall 576 | gb_free 19.9 | wall 63878
2022-03-05 03:05:32 | INFO | fairseq.trainer | begin training epoch 101
2022-03-05 03:05:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:07:00 | INFO | train_inner | epoch 101:     28 / 196 loss=2.886, nll_loss=2.374, ppl=5.19, wps=19776.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.126, loss_scale=8, train_wall=299, gb_free=19.9, wall=63967
2022-03-05 03:12:17 | INFO | train_inner | epoch 101:    128 / 196 loss=2.861, nll_loss=2.347, ppl=5.09, wps=20693.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.142, loss_scale=16, train_wall=294, gb_free=19.9, wall=64283
2022-03-05 03:15:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:15:57 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.195 | nll_loss 9.837 | ppl 914.47 | wps 39554.3 | wpb 510.9 | bsz 1 | num_updates 19668 | best_loss 7.12
2022-03-05 03:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19668 updates
2022-03-05 03:15:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:16:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 101 @ 19668 updates, score 10.195) (writing took 3.035087753087282 seconds)
2022-03-05 03:16:00 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-05 03:16:00 | INFO | train | epoch 101 | loss 2.869 | nll_loss 2.356 | ppl 5.12 | wps 20423 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 19668 | lr 0.000225486 | gnorm 1.136 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 64506
2022-03-05 03:16:00 | INFO | fairseq.trainer | begin training epoch 102
2022-03-05 03:16:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:17:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:17:44 | INFO | train_inner | epoch 102:     33 / 196 loss=2.877, nll_loss=2.364, ppl=5.15, wps=19972, ups=0.31, wpb=65367, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.127, loss_scale=8, train_wall=296, gb_free=19.9, wall=64610
2022-03-05 03:23:01 | INFO | train_inner | epoch 102:    133 / 196 loss=2.842, nll_loss=2.328, ppl=5.02, wps=20716, ups=0.32, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.124, loss_scale=8, train_wall=294, gb_free=19.9, wall=64927
2022-03-05 03:26:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:26:24 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.198 | nll_loss 9.84 | ppl 916.78 | wps 40331.5 | wpb 510.9 | bsz 1 | num_updates 19863 | best_loss 7.12
2022-03-05 03:26:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19863 updates
2022-03-05 03:26:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:26:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 102 @ 19863 updates, score 10.198) (writing took 3.0970976899843663 seconds)
2022-03-05 03:26:28 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-05 03:26:28 | INFO | train | epoch 102 | loss 2.858 | nll_loss 2.345 | ppl 5.08 | wps 20329.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19863 | lr 0.000224377 | gnorm 1.122 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 65134
2022-03-05 03:26:28 | INFO | fairseq.trainer | begin training epoch 103
2022-03-05 03:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:26:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:28:28 | INFO | train_inner | epoch 103:     38 / 196 loss=2.872, nll_loss=2.359, ppl=5.13, wps=19975.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.133, loss_scale=8, train_wall=296, gb_free=19.9, wall=65254
2022-03-05 03:33:44 | INFO | train_inner | epoch 103:    138 / 196 loss=2.841, nll_loss=2.326, ppl=5.01, wps=20731.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.124, loss_scale=16, train_wall=293, gb_free=19.9, wall=65570
2022-03-05 03:34:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:36:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:36:52 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.227 | nll_loss 9.868 | ppl 934.74 | wps 40460.6 | wpb 510.9 | bsz 1 | num_updates 20057 | best_loss 7.12
2022-03-05 03:36:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20057 updates
2022-03-05 03:36:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:36:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:36:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 103 @ 20057 updates, score 10.227) (writing took 3.0777835447806865 seconds)
2022-03-05 03:36:55 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-05 03:36:55 | INFO | train | epoch 103 | loss 2.848 | nll_loss 2.334 | ppl 5.04 | wps 20239 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20057 | lr 0.000223289 | gnorm 1.136 | loss_scale 8 | train_wall 574 | gb_free 19.9 | wall 65761
2022-03-05 03:36:55 | INFO | fairseq.trainer | begin training epoch 104
2022-03-05 03:36:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:39:11 | INFO | train_inner | epoch 104:     43 / 196 loss=2.846, nll_loss=2.332, ppl=5.03, wps=19988.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.158, loss_scale=8, train_wall=296, gb_free=19.9, wall=65897
2022-03-05 03:42:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:44:34 | INFO | train_inner | epoch 104:    144 / 196 loss=2.84, nll_loss=2.325, ppl=5.01, wps=20285.2, ups=0.31, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.151, loss_scale=8, train_wall=299, gb_free=19.9, wall=66220
2022-03-05 03:47:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:47:26 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.268 | nll_loss 9.908 | ppl 960.57 | wps 38766.7 | wpb 510.9 | bsz 1 | num_updates 20252 | best_loss 7.12
2022-03-05 03:47:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20252 updates
2022-03-05 03:47:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:47:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 104 @ 20252 updates, score 10.268) (writing took 3.2398486740421504 seconds)
2022-03-05 03:47:29 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-05 03:47:29 | INFO | train | epoch 104 | loss 2.84 | nll_loss 2.326 | ppl 5.01 | wps 20125.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20252 | lr 0.000222211 | gnorm 1.156 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 66395
2022-03-05 03:47:29 | INFO | fairseq.trainer | begin training epoch 105
2022-03-05 03:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:50:03 | INFO | train_inner | epoch 105:     48 / 196 loss=2.832, nll_loss=2.317, ppl=4.98, wps=19892.7, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.147, loss_scale=16, train_wall=296, gb_free=19.9, wall=66549
2022-03-05 03:52:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:55:25 | INFO | train_inner | epoch 105:    149 / 196 loss=2.833, nll_loss=2.318, ppl=4.99, wps=20300.1, ups=0.31, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.141, loss_scale=8, train_wall=299, gb_free=19.9, wall=66872
2022-03-05 03:57:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:58:01 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.255 | nll_loss 9.897 | ppl 953.68 | wps 38869.7 | wpb 510.9 | bsz 1 | num_updates 20447 | best_loss 7.12
2022-03-05 03:58:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20447 updates
2022-03-05 03:58:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:58:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:58:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 105 @ 20447 updates, score 10.255) (writing took 3.234521596925333 seconds)
2022-03-05 03:58:04 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-05 03:58:04 | INFO | train | epoch 105 | loss 2.831 | nll_loss 2.316 | ppl 4.98 | wps 20105.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20447 | lr 0.000221149 | gnorm 1.139 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 67030
2022-03-05 03:58:04 | INFO | fairseq.trainer | begin training epoch 106
2022-03-05 03:58:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:59:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:00:56 | INFO | train_inner | epoch 106:     54 / 196 loss=2.825, nll_loss=2.309, ppl=4.96, wps=19747.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.128, loss_scale=8, train_wall=298, gb_free=19.9, wall=67203
2022-03-05 04:06:16 | INFO | train_inner | epoch 106:    154 / 196 loss=2.825, nll_loss=2.309, ppl=4.96, wps=20503.5, ups=0.31, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.137, loss_scale=8, train_wall=296, gb_free=19.9, wall=67522
2022-03-05 04:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:08:35 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.277 | nll_loss 9.918 | ppl 967.36 | wps 38830.4 | wpb 510.9 | bsz 1 | num_updates 20642 | best_loss 7.12
2022-03-05 04:08:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20642 updates
2022-03-05 04:08:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:08:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 106 @ 20642 updates, score 10.277) (writing took 3.1984657009597868 seconds)
2022-03-05 04:08:38 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-05 04:08:38 | INFO | train | epoch 106 | loss 2.821 | nll_loss 2.306 | ppl 4.94 | wps 20109.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20642 | lr 0.000220102 | gnorm 1.134 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 67665
2022-03-05 04:08:38 | INFO | fairseq.trainer | begin training epoch 107
2022-03-05 04:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:09:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:11:47 | INFO | train_inner | epoch 107:     59 / 196 loss=2.807, nll_loss=2.291, ppl=4.89, wps=19733.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.129, loss_scale=8, train_wall=299, gb_free=19.9, wall=67854
2022-03-05 04:17:07 | INFO | train_inner | epoch 107:    159 / 196 loss=2.824, nll_loss=2.309, ppl=4.96, wps=20504.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.154, loss_scale=16, train_wall=296, gb_free=19.9, wall=68173
2022-03-05 04:17:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:19:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:19:10 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.348 | nll_loss 9.993 | ppl 1019.2 | wps 38768.2 | wpb 510.9 | bsz 1 | num_updates 20836 | best_loss 7.12
2022-03-05 04:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20836 updates
2022-03-05 04:19:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 107 @ 20836 updates, score 10.348) (writing took 3.3103606931399554 seconds)
2022-03-05 04:19:14 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-05 04:19:14 | INFO | train | epoch 107 | loss 2.812 | nll_loss 2.296 | ppl 4.91 | wps 19990.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20836 | lr 0.000219075 | gnorm 1.147 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 68300
2022-03-05 04:19:14 | INFO | fairseq.trainer | begin training epoch 108
2022-03-05 04:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:22:38 | INFO | train_inner | epoch 108:     64 / 196 loss=2.795, nll_loss=2.278, ppl=4.85, wps=19733.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.152, loss_scale=8, train_wall=298, gb_free=19.9, wall=68504
2022-03-05 04:25:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:28:01 | INFO | train_inner | epoch 108:    165 / 196 loss=2.823, nll_loss=2.308, ppl=4.95, wps=20306.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.154, loss_scale=8, train_wall=299, gb_free=19.9, wall=68827
2022-03-05 04:29:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:29:45 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.362 | nll_loss 10.011 | ppl 1031.58 | wps 38688.2 | wpb 510.9 | bsz 1 | num_updates 21031 | best_loss 7.12
2022-03-05 04:29:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21031 updates
2022-03-05 04:29:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:29:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:29:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 108 @ 21031 updates, score 10.362) (writing took 3.2209960299078375 seconds)
2022-03-05 04:29:48 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-05 04:29:48 | INFO | train | epoch 108 | loss 2.805 | nll_loss 2.289 | ppl 4.89 | wps 20114.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21031 | lr 0.000218057 | gnorm 1.153 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 68934
2022-03-05 04:29:48 | INFO | fairseq.trainer | begin training epoch 109
2022-03-05 04:29:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:33:29 | INFO | train_inner | epoch 109:     69 / 196 loss=2.779, nll_loss=2.262, ppl=4.8, wps=19937.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.155, loss_scale=16, train_wall=296, gb_free=19.9, wall=69155
2022-03-05 04:37:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:38:52 | INFO | train_inner | epoch 109:    170 / 196 loss=2.816, nll_loss=2.301, ppl=4.93, wps=20299.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.163, loss_scale=8, train_wall=299, gb_free=19.9, wall=69478
2022-03-05 04:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:40:20 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.354 | nll_loss 9.996 | ppl 1021.3 | wps 38926.6 | wpb 510.9 | bsz 1 | num_updates 21226 | best_loss 7.12
2022-03-05 04:40:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21226 updates
2022-03-05 04:40:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:40:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 109 @ 21226 updates, score 10.354) (writing took 3.1677360830362886 seconds)
2022-03-05 04:40:23 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-05 04:40:23 | INFO | train | epoch 109 | loss 2.795 | nll_loss 2.278 | ppl 4.85 | wps 20109.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21226 | lr 0.000217053 | gnorm 1.154 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 69569
2022-03-05 04:40:23 | INFO | fairseq.trainer | begin training epoch 110
2022-03-05 04:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:44:19 | INFO | train_inner | epoch 110:     74 / 196 loss=2.769, nll_loss=2.251, ppl=4.76, wps=19939.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.146, loss_scale=8, train_wall=296, gb_free=19.9, wall=69806
2022-03-05 04:45:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:49:42 | INFO | train_inner | epoch 110:    175 / 196 loss=2.813, nll_loss=2.297, ppl=4.91, wps=20302.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.154, loss_scale=8, train_wall=299, gb_free=19.9, wall=70128
2022-03-05 04:50:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:50:54 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.394 | nll_loss 10.04 | ppl 1052.51 | wps 38677.7 | wpb 510.9 | bsz 1 | num_updates 21421 | best_loss 7.12
2022-03-05 04:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21421 updates
2022-03-05 04:50:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 110 @ 21421 updates, score 10.394) (writing took 3.092349654994905 seconds)
2022-03-05 04:50:57 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-05 04:50:57 | INFO | train | epoch 110 | loss 2.788 | nll_loss 2.271 | ppl 4.83 | wps 20111.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21421 | lr 0.000216063 | gnorm 1.151 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 70204
2022-03-05 04:50:57 | INFO | fairseq.trainer | begin training epoch 111
2022-03-05 04:50:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:54:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:55:13 | INFO | train_inner | epoch 111:     80 / 196 loss=2.76, nll_loss=2.242, ppl=4.73, wps=19744.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.152, loss_scale=8, train_wall=299, gb_free=19.9, wall=70459
2022-03-05 05:00:33 | INFO | train_inner | epoch 111:    180 / 196 loss=2.8, nll_loss=2.284, ppl=4.87, wps=20487.7, ups=0.31, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.152, loss_scale=8, train_wall=297, gb_free=19.9, wall=70779
2022-03-05 05:01:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:01:29 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.379 | nll_loss 10.022 | ppl 1039.74 | wps 38918.7 | wpb 510.9 | bsz 1 | num_updates 21616 | best_loss 7.12
2022-03-05 05:01:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21616 updates
2022-03-05 05:01:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:01:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:01:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 111 @ 21616 updates, score 10.379) (writing took 3.097071076044813 seconds)
2022-03-05 05:01:32 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-05 05:01:32 | INFO | train | epoch 111 | loss 2.779 | nll_loss 2.262 | ppl 4.79 | wps 20098.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21616 | lr 0.000215086 | gnorm 1.154 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 70839
2022-03-05 05:01:32 | INFO | fairseq.trainer | begin training epoch 112
2022-03-05 05:01:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:03:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:06:04 | INFO | train_inner | epoch 112:     85 / 196 loss=2.749, nll_loss=2.23, ppl=4.69, wps=19745.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.149, loss_scale=8, train_wall=299, gb_free=19.9, wall=71110
2022-03-05 05:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:11:27 | INFO | train_inner | epoch 112:    186 / 196 loss=2.794, nll_loss=2.277, ppl=4.85, wps=20290.4, ups=0.31, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.15, loss_scale=8, train_wall=299, gb_free=19.9, wall=71433
2022-03-05 05:11:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:12:04 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.422 | nll_loss 10.069 | ppl 1074.24 | wps 38815.1 | wpb 510.9 | bsz 1 | num_updates 21810 | best_loss 7.12
2022-03-05 05:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21810 updates
2022-03-05 05:12:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:12:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 112 @ 21810 updates, score 10.422) (writing took 3.0901765301823616 seconds)
2022-03-05 05:12:07 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-05 05:12:07 | INFO | train | epoch 112 | loss 2.77 | nll_loss 2.252 | ppl 4.76 | wps 20000.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21810 | lr 0.000214127 | gnorm 1.147 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 71473
2022-03-05 05:12:07 | INFO | fairseq.trainer | begin training epoch 113
2022-03-05 05:12:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:16:55 | INFO | train_inner | epoch 113:     90 / 196 loss=2.739, nll_loss=2.219, ppl=4.66, wps=19934.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.14, loss_scale=8, train_wall=296, gb_free=19.9, wall=71761
2022-03-05 05:18:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:22:18 | INFO | train_inner | epoch 113:    191 / 196 loss=2.797, nll_loss=2.281, ppl=4.86, wps=20297.3, ups=0.31, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.185, loss_scale=8, train_wall=299, gb_free=19.9, wall=72084
2022-03-05 05:22:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:22:39 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.432 | nll_loss 10.077 | ppl 1079.78 | wps 38651 | wpb 510.9 | bsz 1 | num_updates 22005 | best_loss 7.12
2022-03-05 05:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22005 updates
2022-03-05 05:22:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 113 @ 22005 updates, score 10.432) (writing took 3.081884511979297 seconds)
2022-03-05 05:22:42 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-05 05:22:42 | INFO | train | epoch 113 | loss 2.764 | nll_loss 2.246 | ppl 4.74 | wps 20105.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22005 | lr 0.000213176 | gnorm 1.163 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 72108
2022-03-05 05:22:42 | INFO | fairseq.trainer | begin training epoch 114
2022-03-05 05:22:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:26:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:27:49 | INFO | train_inner | epoch 114:     96 / 196 loss=2.718, nll_loss=2.197, ppl=4.59, wps=19754.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.13, loss_scale=8, train_wall=299, gb_free=19.9, wall=72415
2022-03-05 05:33:08 | INFO | train_inner | epoch 114:    196 / 196 loss=2.796, nll_loss=2.28, ppl=4.86, wps=20499.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.165, loss_scale=8, train_wall=296, gb_free=19.9, wall=72734
2022-03-05 05:33:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:33:13 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.481 | nll_loss 10.123 | ppl 1115.1 | wps 39134 | wpb 510.9 | bsz 1 | num_updates 22200 | best_loss 7.12
2022-03-05 05:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22200 updates
2022-03-05 05:33:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:33:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:33:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 114 @ 22200 updates, score 10.481) (writing took 3.0602735488209873 seconds)
2022-03-05 05:33:16 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-05 05:33:16 | INFO | train | epoch 114 | loss 2.755 | nll_loss 2.236 | ppl 4.71 | wps 20113.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22200 | lr 0.000212238 | gnorm 1.146 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 72743
2022-03-05 05:33:16 | INFO | fairseq.trainer | begin training epoch 115
2022-03-05 05:33:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:34:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:38:39 | INFO | train_inner | epoch 115:    101 / 196 loss=2.705, nll_loss=2.183, ppl=4.54, wps=19759.9, ups=0.3, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.15, loss_scale=8, train_wall=299, gb_free=19.9, wall=73066
2022-03-05 05:41:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:43:48 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.468 | nll_loss 10.109 | ppl 1104.74 | wps 38875.4 | wpb 510.9 | bsz 1 | num_updates 22394 | best_loss 7.12
2022-03-05 05:43:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22394 updates
2022-03-05 05:43:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:43:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 115 @ 22394 updates, score 10.468) (writing took 3.0716562860179693 seconds)
2022-03-05 05:43:51 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-05 05:43:51 | INFO | train | epoch 115 | loss 2.747 | nll_loss 2.227 | ppl 4.68 | wps 20010.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22394 | lr 0.000211317 | gnorm 1.156 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 73377
2022-03-05 05:43:51 | INFO | fairseq.trainer | begin training epoch 116
2022-03-05 05:43:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:44:10 | INFO | train_inner | epoch 116:      6 / 196 loss=2.785, nll_loss=2.268, ppl=4.82, wps=19758.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.164, loss_scale=8, train_wall=299, gb_free=19.9, wall=73396
2022-03-05 05:49:30 | INFO | train_inner | epoch 116:    106 / 196 loss=2.707, nll_loss=2.186, ppl=4.55, wps=20496.5, ups=0.31, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.156, loss_scale=16, train_wall=296, gb_free=19.9, wall=73716
2022-03-05 05:54:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:54:23 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.5 | nll_loss 10.146 | ppl 1133.06 | wps 38702.4 | wpb 510.9 | bsz 1 | num_updates 22589 | best_loss 7.12
2022-03-05 05:54:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22589 updates
2022-03-05 05:54:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:54:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:54:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 116 @ 22589 updates, score 10.5) (writing took 3.026831960072741 seconds)
2022-03-05 05:54:26 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-05 05:54:26 | INFO | train | epoch 116 | loss 2.739 | nll_loss 2.22 | ppl 4.66 | wps 20104.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22589 | lr 0.000210403 | gnorm 1.161 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 74012
2022-03-05 05:54:26 | INFO | fairseq.trainer | begin training epoch 117
2022-03-05 05:54:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:01 | INFO | train_inner | epoch 117:     11 / 196 loss=2.76, nll_loss=2.242, ppl=4.73, wps=19744.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.166, loss_scale=8, train_wall=299, gb_free=19.9, wall=74047
2022-03-05 06:00:21 | INFO | train_inner | epoch 117:    111 / 196 loss=2.704, nll_loss=2.182, ppl=4.54, wps=20500.4, ups=0.31, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.17, loss_scale=8, train_wall=296, gb_free=19.9, wall=74367
2022-03-05 06:03:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:04:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:04:57 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.541 | nll_loss 10.185 | ppl 1164.28 | wps 39011.1 | wpb 510.9 | bsz 1 | num_updates 22784 | best_loss 7.12
2022-03-05 06:04:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22784 updates
2022-03-05 06:04:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 117 @ 22784 updates, score 10.541) (writing took 3.092070922954008 seconds)
2022-03-05 06:05:00 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-05 06:05:00 | INFO | train | epoch 117 | loss 2.732 | nll_loss 2.212 | ppl 4.63 | wps 20104.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22784 | lr 0.0002095 | gnorm 1.174 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 74647
2022-03-05 06:05:01 | INFO | fairseq.trainer | begin training epoch 118
2022-03-05 06:05:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:05:52 | INFO | train_inner | epoch 118:     16 / 196 loss=2.759, nll_loss=2.241, ppl=4.73, wps=19746.5, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.18, loss_scale=8, train_wall=299, gb_free=19.9, wall=74698
2022-03-05 06:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:11:15 | INFO | train_inner | epoch 118:    117 / 196 loss=2.702, nll_loss=2.18, ppl=4.53, wps=20282.3, ups=0.31, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.16, loss_scale=8, train_wall=299, gb_free=19.9, wall=75021
2022-03-05 06:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:15:32 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.526 | nll_loss 10.174 | ppl 1155.38 | wps 38744.1 | wpb 510.9 | bsz 1 | num_updates 22979 | best_loss 7.12
2022-03-05 06:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22979 updates
2022-03-05 06:15:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:15:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 118 @ 22979 updates, score 10.526) (writing took 3.0535175129771233 seconds)
2022-03-05 06:15:35 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-05 06:15:35 | INFO | train | epoch 118 | loss 2.725 | nll_loss 2.205 | ppl 4.61 | wps 20108 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22979 | lr 0.00020861 | gnorm 1.17 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 75281
2022-03-05 06:15:35 | INFO | fairseq.trainer | begin training epoch 119
2022-03-05 06:15:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:16:42 | INFO | train_inner | epoch 119:     21 / 196 loss=2.742, nll_loss=2.223, ppl=4.67, wps=19950.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.172, loss_scale=8, train_wall=296, gb_free=19.9, wall=75349
2022-03-05 06:18:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:22:05 | INFO | train_inner | epoch 119:    122 / 196 loss=2.699, nll_loss=2.177, ppl=4.52, wps=20300.6, ups=0.31, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.17, loss_scale=8, train_wall=299, gb_free=19.9, wall=75671
2022-03-05 06:25:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:26:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:26:07 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.554 | nll_loss 10.199 | ppl 1175.66 | wps 38118.1 | wpb 510.9 | bsz 1 | num_updates 23173 | best_loss 7.12
2022-03-05 06:26:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23173 updates
2022-03-05 06:26:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:26:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:26:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 119 @ 23173 updates, score 10.554) (writing took 3.0384649930056185 seconds)
2022-03-05 06:26:10 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-05 06:26:10 | INFO | train | epoch 119 | loss 2.717 | nll_loss 2.196 | ppl 4.58 | wps 20009.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23173 | lr 0.000207735 | gnorm 1.171 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 75916
2022-03-05 06:26:10 | INFO | fairseq.trainer | begin training epoch 120
2022-03-05 06:26:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:27:36 | INFO | train_inner | epoch 120:     27 / 196 loss=2.733, nll_loss=2.214, ppl=4.64, wps=19757.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.178, loss_scale=8, train_wall=298, gb_free=19.9, wall=76002
2022-03-05 06:32:55 | INFO | train_inner | epoch 120:    127 / 196 loss=2.695, nll_loss=2.174, ppl=4.51, wps=20523.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.179, loss_scale=16, train_wall=296, gb_free=19.9, wall=76322
2022-03-05 06:33:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:36:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:36:40 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.541 | nll_loss 10.187 | ppl 1165.59 | wps 39076 | wpb 510.9 | bsz 1 | num_updates 23368 | best_loss 7.12
2022-03-05 06:36:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23368 updates
2022-03-05 06:36:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:36:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:36:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 120 @ 23368 updates, score 10.541) (writing took 3.0459886321332306 seconds)
2022-03-05 06:36:43 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-05 06:36:43 | INFO | train | epoch 120 | loss 2.71 | nll_loss 2.189 | ppl 4.56 | wps 20150.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23368 | lr 0.000206866 | gnorm 1.176 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 76549
2022-03-05 06:36:43 | INFO | fairseq.trainer | begin training epoch 121
2022-03-05 06:36:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:38:25 | INFO | train_inner | epoch 121:     32 / 196 loss=2.719, nll_loss=2.199, ppl=4.59, wps=19822.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.171, loss_scale=8, train_wall=298, gb_free=19.9, wall=76651
2022-03-05 06:40:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:43:47 | INFO | train_inner | epoch 121:    133 / 196 loss=2.685, nll_loss=2.163, ppl=4.48, wps=20352.4, ups=0.31, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.162, loss_scale=8, train_wall=299, gb_free=19.9, wall=76973
2022-03-05 06:47:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:47:13 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.589 | nll_loss 10.235 | ppl 1205.23 | wps 39156.3 | wpb 510.9 | bsz 1 | num_updates 23563 | best_loss 7.12
2022-03-05 06:47:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23563 updates
2022-03-05 06:47:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:47:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:47:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 121 @ 23563 updates, score 10.589) (writing took 3.040588742122054 seconds)
2022-03-05 06:47:16 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-05 06:47:16 | INFO | train | epoch 121 | loss 2.703 | nll_loss 2.182 | ppl 4.54 | wps 20168.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23563 | lr 0.000206008 | gnorm 1.177 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 77182
2022-03-05 06:47:16 | INFO | fairseq.trainer | begin training epoch 122
2022-03-05 06:47:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:48:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:49:17 | INFO | train_inner | epoch 122:     38 / 196 loss=2.711, nll_loss=2.191, ppl=4.57, wps=19811.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.196, loss_scale=8, train_wall=298, gb_free=19.9, wall=77303
2022-03-05 06:54:36 | INFO | train_inner | epoch 122:    138 / 196 loss=2.694, nll_loss=2.172, ppl=4.51, wps=20557.6, ups=0.31, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.169, loss_scale=8, train_wall=296, gb_free=19.9, wall=77622
2022-03-05 06:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:57:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:46 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.642 | nll_loss 10.291 | ppl 1252.49 | wps 39122.1 | wpb 510.9 | bsz 1 | num_updates 23757 | best_loss 7.12
2022-03-05 06:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23757 updates
2022-03-05 06:57:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:57:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:57:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 122 @ 23757 updates, score 10.642) (writing took 3.0461277160793543 seconds)
2022-03-05 06:57:49 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-05 06:57:49 | INFO | train | epoch 122 | loss 2.697 | nll_loss 2.175 | ppl 4.52 | wps 20065.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23757 | lr 0.000205165 | gnorm 1.185 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 77815
2022-03-05 06:57:49 | INFO | fairseq.trainer | begin training epoch 123
2022-03-05 06:57:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:00:06 | INFO | train_inner | epoch 123:     43 / 196 loss=2.697, nll_loss=2.176, ppl=4.52, wps=19807, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.192, loss_scale=8, train_wall=298, gb_free=19.9, wall=77952
2022-03-05 07:04:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:05:28 | INFO | train_inner | epoch 123:    144 / 196 loss=2.69, nll_loss=2.168, ppl=4.49, wps=20355.8, ups=0.31, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.167, loss_scale=8, train_wall=299, gb_free=19.9, wall=78274
2022-03-05 07:08:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:08:18 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.594 | nll_loss 10.24 | ppl 1209.27 | wps 39082.3 | wpb 510.9 | bsz 1 | num_updates 23952 | best_loss 7.12
2022-03-05 07:08:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23952 updates
2022-03-05 07:08:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:08:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:08:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 123 @ 23952 updates, score 10.594) (writing took 3.034859461011365 seconds)
2022-03-05 07:08:21 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-05 07:08:21 | INFO | train | epoch 123 | loss 2.689 | nll_loss 2.167 | ppl 4.49 | wps 20170.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23952 | lr 0.000204329 | gnorm 1.176 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 78448
2022-03-05 07:08:21 | INFO | fairseq.trainer | begin training epoch 124
2022-03-05 07:08:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:10:54 | INFO | train_inner | epoch 124:     48 / 196 loss=2.679, nll_loss=2.157, ppl=4.46, wps=20009.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.183, loss_scale=8, train_wall=295, gb_free=19.9, wall=78601
2022-03-05 07:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:16:14 | INFO | train_inner | epoch 124:    149 / 196 loss=2.691, nll_loss=2.169, ppl=4.5, wps=20536.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.178, loss_scale=8, train_wall=296, gb_free=19.9, wall=78920
2022-03-05 07:18:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:18:46 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.626 | nll_loss 10.275 | ppl 1238.96 | wps 40843 | wpb 510.9 | bsz 1 | num_updates 24147 | best_loss 7.12
2022-03-05 07:18:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24147 updates
2022-03-05 07:18:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:18:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:18:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 124 @ 24147 updates, score 10.626) (writing took 2.9934429849963635 seconds)
2022-03-05 07:18:49 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-05 07:18:49 | INFO | train | epoch 124 | loss 2.683 | nll_loss 2.161 | ppl 4.47 | wps 20337.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24147 | lr 0.000203502 | gnorm 1.18 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 79075
2022-03-05 07:18:49 | INFO | fairseq.trainer | begin training epoch 125
2022-03-05 07:18:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:19:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:21:39 | INFO | train_inner | epoch 125:     54 / 196 loss=2.673, nll_loss=2.15, ppl=4.44, wps=20107.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.2, loss_scale=8, train_wall=294, gb_free=19.9, wall=79245
2022-03-05 07:26:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:26:56 | INFO | train_inner | epoch 125:    155 / 196 loss=2.682, nll_loss=2.16, ppl=4.47, wps=20658.9, ups=0.32, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.179, loss_scale=8, train_wall=295, gb_free=19.9, wall=79562
2022-03-05 07:29:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:29:09 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.644 | nll_loss 10.289 | ppl 1251.2 | wps 40837.4 | wpb 510.9 | bsz 1 | num_updates 24341 | best_loss 7.12
2022-03-05 07:29:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24341 updates
2022-03-05 07:29:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:29:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:29:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 125 @ 24341 updates, score 10.644) (writing took 3.020202116109431 seconds)
2022-03-05 07:29:12 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-05 07:29:12 | INFO | train | epoch 125 | loss 2.675 | nll_loss 2.152 | ppl 4.44 | wps 20365.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24341 | lr 0.000202689 | gnorm 1.186 | loss_scale 8 | train_wall 571 | gb_free 19.9 | wall 79699
2022-03-05 07:29:12 | INFO | fairseq.trainer | begin training epoch 126
2022-03-05 07:29:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:32:18 | INFO | train_inner | epoch 126:     59 / 196 loss=2.662, nll_loss=2.139, ppl=4.4, wps=20304.8, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.174, loss_scale=8, train_wall=291, gb_free=19.9, wall=79884
2022-03-05 07:33:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:37:35 | INFO | train_inner | epoch 126:    160 / 196 loss=2.681, nll_loss=2.159, ppl=4.47, wps=20652, ups=0.32, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.189, loss_scale=8, train_wall=295, gb_free=19.9, wall=80201
2022-03-05 07:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:39:33 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.669 | nll_loss 10.317 | ppl 1275.7 | wps 40954.2 | wpb 510.9 | bsz 1 | num_updates 24536 | best_loss 7.12
2022-03-05 07:39:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24536 updates
2022-03-05 07:39:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:39:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:39:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 126 @ 24536 updates, score 10.669) (writing took 2.9829004909843206 seconds)
2022-03-05 07:39:36 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-05 07:39:36 | INFO | train | epoch 126 | loss 2.67 | nll_loss 2.148 | ppl 4.43 | wps 20473.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24536 | lr 0.000201882 | gnorm 1.182 | loss_scale 8 | train_wall 571 | gb_free 19.9 | wall 80322
2022-03-05 07:39:36 | INFO | fairseq.trainer | begin training epoch 127
2022-03-05 07:39:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:42:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:43:00 | INFO | train_inner | epoch 127:     65 / 196 loss=2.657, nll_loss=2.134, ppl=4.39, wps=20107.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.173, loss_scale=8, train_wall=294, gb_free=19.9, wall=80526
2022-03-05 07:48:14 | INFO | train_inner | epoch 127:    165 / 196 loss=2.676, nll_loss=2.153, ppl=4.45, wps=20868.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.172, loss_scale=8, train_wall=292, gb_free=19.9, wall=80840
2022-03-05 07:49:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:49:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:49:56 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.699 | nll_loss 10.347 | ppl 1302.67 | wps 40685.1 | wpb 510.9 | bsz 1 | num_updates 24730 | best_loss 7.12
2022-03-05 07:49:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24730 updates
2022-03-05 07:49:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:49:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 127 @ 24730 updates, score 10.699) (writing took 3.0168642508797348 seconds)
2022-03-05 07:49:59 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-05 07:49:59 | INFO | train | epoch 127 | loss 2.662 | nll_loss 2.139 | ppl 4.41 | wps 20361.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24730 | lr 0.000201089 | gnorm 1.172 | loss_scale 8 | train_wall 572 | gb_free 19.9 | wall 80945
2022-03-05 07:49:59 | INFO | fairseq.trainer | begin training epoch 128
2022-03-05 07:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:53:39 | INFO | train_inner | epoch 128:     70 / 196 loss=2.642, nll_loss=2.118, ppl=4.34, wps=20109.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.179, loss_scale=8, train_wall=294, gb_free=19.9, wall=81165
2022-03-05 07:58:57 | INFO | train_inner | epoch 128:    170 / 196 loss=2.678, nll_loss=2.156, ppl=4.46, wps=20627, ups=0.31, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.17, loss_scale=16, train_wall=295, gb_free=19.9, wall=81483
2022-03-05 07:59:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:00:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:00:25 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.662 | nll_loss 10.31 | ppl 1269.82 | wps 38948.7 | wpb 510.9 | bsz 1 | num_updates 24925 | best_loss 7.12
2022-03-05 08:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24925 updates
2022-03-05 08:00:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:00:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:00:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 128 @ 24925 updates, score 10.662) (writing took 3.0351236180868 seconds)
2022-03-05 08:00:28 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-05 08:00:28 | INFO | train | epoch 128 | loss 2.657 | nll_loss 2.134 | ppl 4.39 | wps 20305.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24925 | lr 0.000200301 | gnorm 1.174 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 81574
2022-03-05 08:00:28 | INFO | fairseq.trainer | begin training epoch 129
2022-03-05 08:00:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:04:27 | INFO | train_inner | epoch 129:     75 / 196 loss=2.627, nll_loss=2.102, ppl=4.29, wps=19803.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.175, loss_scale=8, train_wall=298, gb_free=19.9, wall=81813
2022-03-05 08:09:46 | INFO | train_inner | epoch 129:    175 / 196 loss=2.678, nll_loss=2.156, ppl=4.46, wps=20554.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.186, loss_scale=16, train_wall=296, gb_free=19.9, wall=82132
2022-03-05 08:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:10:58 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.693 | nll_loss 10.341 | ppl 1296.68 | wps 39067.9 | wpb 510.9 | bsz 1 | num_updates 25121 | best_loss 7.12
2022-03-05 08:10:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25121 updates
2022-03-05 08:10:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 129 @ 25121 updates, score 10.693) (writing took 3.0308934589847922 seconds)
2022-03-05 08:11:01 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-05 08:11:01 | INFO | train | epoch 129 | loss 2.652 | nll_loss 2.128 | ppl 4.37 | wps 20264.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 25121 | lr 0.000199518 | gnorm 1.179 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 82207
2022-03-05 08:11:01 | INFO | fairseq.trainer | begin training epoch 130
2022-03-05 08:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:12:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:15:16 | INFO | train_inner | epoch 130:     80 / 196 loss=2.622, nll_loss=2.097, ppl=4.28, wps=19796.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.182, loss_scale=8, train_wall=298, gb_free=19.9, wall=82462
2022-03-05 08:20:35 | INFO | train_inner | epoch 130:    180 / 196 loss=2.673, nll_loss=2.15, ppl=4.44, wps=20539.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.197, loss_scale=16, train_wall=296, gb_free=19.9, wall=82781
2022-03-05 08:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:21:31 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.752 | nll_loss 10.401 | ppl 1352.44 | wps 39108.6 | wpb 510.9 | bsz 1 | num_updates 25316 | best_loss 7.12
2022-03-05 08:21:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25316 updates
2022-03-05 08:21:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 130 @ 25316 updates, score 10.752) (writing took 3.0376657471060753 seconds)
2022-03-05 08:21:34 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-05 08:21:34 | INFO | train | epoch 130 | loss 2.645 | nll_loss 2.121 | ppl 4.35 | wps 20151.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25316 | lr 0.000198748 | gnorm 1.19 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 82840
2022-03-05 08:21:34 | INFO | fairseq.trainer | begin training epoch 131
2022-03-05 08:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:21:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:26:05 | INFO | train_inner | epoch 131:     85 / 196 loss=2.611, nll_loss=2.085, ppl=4.24, wps=19801.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.162, loss_scale=8, train_wall=298, gb_free=19.9, wall=83111
2022-03-05 08:29:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:31:27 | INFO | train_inner | epoch 131:    186 / 196 loss=2.67, nll_loss=2.148, ppl=4.43, wps=20338.5, ups=0.31, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.182, loss_scale=8, train_wall=299, gb_free=19.9, wall=83434
2022-03-05 08:31:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:32:04 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.75 | nll_loss 10.397 | ppl 1348.19 | wps 39175.9 | wpb 510.9 | bsz 1 | num_updates 25510 | best_loss 7.12
2022-03-05 08:32:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25510 updates
2022-03-05 08:32:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:32:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 131 @ 25510 updates, score 10.75) (writing took 3.0494566231500357 seconds)
2022-03-05 08:32:07 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-05 08:32:07 | INFO | train | epoch 131 | loss 2.637 | nll_loss 2.112 | ppl 4.32 | wps 20051 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25510 | lr 0.000197991 | gnorm 1.175 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 83474
2022-03-05 08:32:07 | INFO | fairseq.trainer | begin training epoch 132
2022-03-05 08:32:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:36:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:36:58 | INFO | train_inner | epoch 132:     91 / 196 loss=2.603, nll_loss=2.077, ppl=4.22, wps=19796, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.179, loss_scale=8, train_wall=298, gb_free=19.9, wall=83764
2022-03-05 08:42:17 | INFO | train_inner | epoch 132:    191 / 196 loss=2.665, nll_loss=2.142, ppl=4.42, wps=20552.8, ups=0.31, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=1.205, loss_scale=8, train_wall=296, gb_free=19.9, wall=84083
2022-03-05 08:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:42:37 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.718 | nll_loss 10.368 | ppl 1321.14 | wps 38902.9 | wpb 510.9 | bsz 1 | num_updates 25705 | best_loss 7.12
2022-03-05 08:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25705 updates
2022-03-05 08:42:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:42:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 132 @ 25705 updates, score 10.718) (writing took 3.049052125075832 seconds)
2022-03-05 08:42:40 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-05 08:42:40 | INFO | train | epoch 132 | loss 2.633 | nll_loss 2.109 | ppl 4.31 | wps 20157.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25705 | lr 0.000197238 | gnorm 1.191 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 84107
2022-03-05 08:42:40 | INFO | fairseq.trainer | begin training epoch 133
2022-03-05 08:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:44:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:47:48 | INFO | train_inner | epoch 133:     96 / 196 loss=2.594, nll_loss=2.067, ppl=4.19, wps=19742.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25800, lr=0.000196875, gnorm=1.196, loss_scale=8, train_wall=299, gb_free=19.9, wall=84414
2022-03-05 08:51:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:53:12 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.749 | nll_loss 10.4 | ppl 1351.09 | wps 38903.9 | wpb 510.9 | bsz 1 | num_updates 25899 | best_loss 7.12
2022-03-05 08:53:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25899 updates
2022-03-05 08:53:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:53:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:53:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 133 @ 25899 updates, score 10.749) (writing took 3.078514294931665 seconds)
2022-03-05 08:53:15 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-05 08:53:15 | INFO | train | epoch 133 | loss 2.626 | nll_loss 2.102 | ppl 4.29 | wps 20002.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25899 | lr 0.000196498 | gnorm 1.189 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 84741
2022-03-05 08:53:15 | INFO | fairseq.trainer | begin training epoch 134
2022-03-05 08:53:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:53:18 | INFO | train_inner | epoch 134:      1 / 196 loss=2.662, nll_loss=2.139, ppl=4.4, wps=19756, ups=0.3, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.186, loss_scale=8, train_wall=299, gb_free=19.9, wall=84745
2022-03-05 08:58:39 | INFO | train_inner | epoch 134:    101 / 196 loss=2.589, nll_loss=2.062, ppl=4.18, wps=20476.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.187, loss_scale=16, train_wall=297, gb_free=19.9, wall=85065
2022-03-05 08:59:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:03:47 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.767 | nll_loss 10.417 | ppl 1366.75 | wps 39008.6 | wpb 510.9 | bsz 1 | num_updates 26094 | best_loss 7.12
2022-03-05 09:03:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26094 updates
2022-03-05 09:03:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:03:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:03:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 134 @ 26094 updates, score 10.767) (writing took 3.0666906340047717 seconds)
2022-03-05 09:03:50 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-05 09:03:50 | INFO | train | epoch 134 | loss 2.621 | nll_loss 2.096 | ppl 4.28 | wps 20095.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26094 | lr 0.000195763 | gnorm 1.196 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 85376
2022-03-05 09:03:50 | INFO | fairseq.trainer | begin training epoch 135
2022-03-05 09:03:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:04:10 | INFO | train_inner | epoch 135:      6 / 196 loss=2.649, nll_loss=2.126, ppl=4.36, wps=19746.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.205, loss_scale=8, train_wall=299, gb_free=19.9, wall=85396
2022-03-05 09:06:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:09:33 | INFO | train_inner | epoch 135:    107 / 196 loss=2.582, nll_loss=2.055, ppl=4.15, wps=20283, ups=0.31, wpb=65532.4, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.183, loss_scale=8, train_wall=299, gb_free=19.9, wall=85719
2022-03-05 09:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:14:22 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.809 | nll_loss 10.459 | ppl 1407.65 | wps 38879.4 | wpb 510.9 | bsz 1 | num_updates 26289 | best_loss 7.12
2022-03-05 09:14:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26289 updates
2022-03-05 09:14:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:14:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:14:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 135 @ 26289 updates, score 10.809) (writing took 3.0983734410256147 seconds)
2022-03-05 09:14:25 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-05 09:14:25 | INFO | train | epoch 135 | loss 2.615 | nll_loss 2.09 | ppl 4.26 | wps 20101.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26289 | lr 0.000195035 | gnorm 1.201 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 86011
2022-03-05 09:14:25 | INFO | fairseq.trainer | begin training epoch 136
2022-03-05 09:14:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:15:00 | INFO | train_inner | epoch 136:     11 / 196 loss=2.643, nll_loss=2.119, ppl=4.34, wps=19943, ups=0.31, wpb=65367, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.214, loss_scale=16, train_wall=296, gb_free=19.9, wall=86047
2022-03-05 09:15:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:20:24 | INFO | train_inner | epoch 136:    112 / 196 loss=2.583, nll_loss=2.056, ppl=4.16, wps=20272.8, ups=0.31, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.18, loss_scale=8, train_wall=300, gb_free=19.9, wall=86370
2022-03-05 09:24:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:24:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:25:01 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.794 | nll_loss 10.445 | ppl 1393.54 | wps 36130.6 | wpb 510.9 | bsz 1 | num_updates 26483 | best_loss 7.12
2022-03-05 09:25:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26483 updates
2022-03-05 09:25:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:25:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:25:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 136 @ 26483 updates, score 10.794) (writing took 4.025443556951359 seconds)
2022-03-05 09:25:05 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-05 09:25:05 | INFO | train | epoch 136 | loss 2.61 | nll_loss 2.084 | ppl 4.24 | wps 19827.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 26483 | lr 0.00019432 | gnorm 1.196 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 86652
2022-03-05 09:25:06 | INFO | fairseq.trainer | begin training epoch 137
2022-03-05 09:25:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:26:00 | INFO | train_inner | epoch 137:     17 / 196 loss=2.633, nll_loss=2.109, ppl=4.31, wps=19422.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.211, loss_scale=8, train_wall=299, gb_free=19.9, wall=86706
2022-03-05 09:31:45 | INFO | train_inner | epoch 137:    117 / 196 loss=2.584, nll_loss=2.057, ppl=4.16, wps=18981, ups=0.29, wpb=65532.4, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.194, loss_scale=8, train_wall=322, gb_free=19.9, wall=87052
2022-03-05 09:34:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:36:03 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.837 | nll_loss 10.486 | ppl 1434.25 | wps 39090.3 | wpb 510.9 | bsz 1 | num_updates 26678 | best_loss 7.12
2022-03-05 09:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26678 updates
2022-03-05 09:36:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:36:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:36:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 137 @ 26678 updates, score 10.837) (writing took 3.9459170459304005 seconds)
2022-03-05 09:36:07 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-05 09:36:07 | INFO | train | epoch 137 | loss 2.604 | nll_loss 2.078 | ppl 4.22 | wps 19301.7 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 26678 | lr 0.000193608 | gnorm 1.2 | loss_scale 8 | train_wall 606 | gb_free 19.9 | wall 87313
2022-03-05 09:36:07 | INFO | fairseq.trainer | begin training epoch 138
2022-03-05 09:36:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:37:17 | INFO | train_inner | epoch 138:     22 / 196 loss=2.617, nll_loss=2.092, ppl=4.26, wps=19716, ups=0.3, wpb=65367, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.204, loss_scale=8, train_wall=298, gb_free=19.9, wall=87383
2022-03-05 09:41:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:42:40 | INFO | train_inner | epoch 138:    123 / 196 loss=2.573, nll_loss=2.045, ppl=4.13, wps=20309.2, ups=0.31, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.184, loss_scale=8, train_wall=299, gb_free=19.9, wall=87706
2022-03-05 09:46:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:46:38 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.791 | nll_loss 10.441 | ppl 1389.81 | wps 38824.9 | wpb 510.9 | bsz 1 | num_updates 26873 | best_loss 7.12
2022-03-05 09:46:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26873 updates
2022-03-05 09:46:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:46:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:46:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 138 @ 26873 updates, score 10.791) (writing took 3.9912592398468405 seconds)
2022-03-05 09:46:42 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-05 09:46:42 | INFO | train | epoch 138 | loss 2.598 | nll_loss 2.072 | ppl 4.2 | wps 20096.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26873 | lr 0.000192904 | gnorm 1.185 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 87948
2022-03-05 09:46:42 | INFO | fairseq.trainer | begin training epoch 139
2022-03-05 09:46:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:48:08 | INFO | train_inner | epoch 139:     27 / 196 loss=2.623, nll_loss=2.099, ppl=4.28, wps=19905.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.189, loss_scale=8, train_wall=295, gb_free=19.9, wall=88034
2022-03-05 09:48:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:53:31 | INFO | train_inner | epoch 139:    128 / 196 loss=2.579, nll_loss=2.052, ppl=4.15, wps=20295.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.181, loss_scale=8, train_wall=299, gb_free=19.9, wall=88357
2022-03-05 09:56:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:57:13 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.844 | nll_loss 10.494 | ppl 1441.85 | wps 38943.4 | wpb 510.9 | bsz 1 | num_updates 27067 | best_loss 7.12
2022-03-05 09:57:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27067 updates
2022-03-05 09:57:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:57:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:57:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 139 @ 27067 updates, score 10.844) (writing took 3.567710150964558 seconds)
2022-03-05 09:57:17 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-05 09:57:17 | INFO | train | epoch 139 | loss 2.593 | nll_loss 2.067 | ppl 4.19 | wps 19996.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27067 | lr 0.000192212 | gnorm 1.192 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 88583
2022-03-05 09:57:17 | INFO | fairseq.trainer | begin training epoch 140
2022-03-05 09:57:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:59:02 | INFO | train_inner | epoch 140:     33 / 196 loss=2.601, nll_loss=2.075, ppl=4.21, wps=19727.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.209, loss_scale=8, train_wall=299, gb_free=19.9, wall=88689
2022-03-05 10:03:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:04:25 | INFO | train_inner | epoch 140:    134 / 196 loss=2.583, nll_loss=2.056, ppl=4.16, wps=20305.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.192, loss_scale=8, train_wall=299, gb_free=19.9, wall=89011
2022-03-05 10:07:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:07:48 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.859 | nll_loss 10.511 | ppl 1459.56 | wps 39135.9 | wpb 510.9 | bsz 1 | num_updates 27262 | best_loss 7.12
2022-03-05 10:07:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27262 updates
2022-03-05 10:07:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:07:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:07:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 140 @ 27262 updates, score 10.859) (writing took 3.7565522759687155 seconds)
2022-03-05 10:07:52 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-05 10:07:52 | INFO | train | epoch 140 | loss 2.588 | nll_loss 2.062 | ppl 4.17 | wps 20099 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27262 | lr 0.000191523 | gnorm 1.2 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 89218
2022-03-05 10:07:52 | INFO | fairseq.trainer | begin training epoch 141
2022-03-05 10:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:09:53 | INFO | train_inner | epoch 141:     38 / 196 loss=2.589, nll_loss=2.063, ppl=4.18, wps=19916.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.197, loss_scale=8, train_wall=296, gb_free=19.9, wall=89339
2022-03-05 10:11:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:15:16 | INFO | train_inner | epoch 141:    139 / 196 loss=2.575, nll_loss=2.048, ppl=4.14, wps=20308.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.202, loss_scale=8, train_wall=299, gb_free=19.9, wall=89662
2022-03-05 10:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:18:23 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.858 | nll_loss 10.506 | ppl 1453.84 | wps 38928.7 | wpb 510.9 | bsz 1 | num_updates 27457 | best_loss 7.12
2022-03-05 10:18:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27457 updates
2022-03-05 10:18:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:18:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:18:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 141 @ 27457 updates, score 10.858) (writing took 3.31021616095677 seconds)
2022-03-05 10:18:26 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-05 10:18:26 | INFO | train | epoch 141 | loss 2.582 | nll_loss 2.055 | ppl 4.16 | wps 20112.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27457 | lr 0.000190842 | gnorm 1.197 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 89852
2022-03-05 10:18:26 | INFO | fairseq.trainer | begin training epoch 142
2022-03-05 10:18:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:19:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:20:47 | INFO | train_inner | epoch 142:     44 / 196 loss=2.58, nll_loss=2.053, ppl=4.15, wps=19752.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.195, loss_scale=8, train_wall=298, gb_free=19.9, wall=89993
2022-03-05 10:26:06 | INFO | train_inner | epoch 142:    144 / 196 loss=2.582, nll_loss=2.055, ppl=4.16, wps=20510.6, ups=0.31, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.197, loss_scale=8, train_wall=296, gb_free=19.9, wall=90313
2022-03-05 10:27:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:28:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:28:57 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.9 | nll_loss 10.553 | ppl 1501.94 | wps 38915.7 | wpb 510.9 | bsz 1 | num_updates 27651 | best_loss 7.12
2022-03-05 10:28:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27651 updates
2022-03-05 10:28:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:29:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 142 @ 27651 updates, score 10.9) (writing took 3.8715468540322036 seconds)
2022-03-05 10:29:01 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-05 10:29:01 | INFO | train | epoch 142 | loss 2.577 | nll_loss 2.05 | ppl 4.14 | wps 19993.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27651 | lr 0.000190171 | gnorm 1.199 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 90487
2022-03-05 10:29:01 | INFO | fairseq.trainer | begin training epoch 143
2022-03-05 10:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:31:38 | INFO | train_inner | epoch 143:     49 / 196 loss=2.573, nll_loss=2.046, ppl=4.13, wps=19715.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.203, loss_scale=8, train_wall=298, gb_free=19.9, wall=90644
2022-03-05 10:34:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:37:01 | INFO | train_inner | epoch 143:    150 / 196 loss=2.572, nll_loss=2.045, ppl=4.13, wps=20314.2, ups=0.31, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.218, loss_scale=8, train_wall=299, gb_free=19.9, wall=90967
2022-03-05 10:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:39:32 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.923 | nll_loss 10.572 | ppl 1522.42 | wps 39017.7 | wpb 510.9 | bsz 1 | num_updates 27846 | best_loss 7.12
2022-03-05 10:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27846 updates
2022-03-05 10:39:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:39:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:39:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 143 @ 27846 updates, score 10.923) (writing took 4.21094606583938 seconds)
2022-03-05 10:39:37 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-05 10:39:37 | INFO | train | epoch 143 | loss 2.572 | nll_loss 2.045 | ppl 4.13 | wps 20087.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27846 | lr 0.000189504 | gnorm 1.204 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 91123
2022-03-05 10:39:37 | INFO | fairseq.trainer | begin training epoch 144
2022-03-05 10:39:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:41:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:42:33 | INFO | train_inner | epoch 144:     55 / 196 loss=2.564, nll_loss=2.036, ppl=4.1, wps=19689.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.208, loss_scale=8, train_wall=298, gb_free=19.9, wall=91299
2022-03-05 10:47:52 | INFO | train_inner | epoch 144:    155 / 196 loss=2.575, nll_loss=2.047, ppl=4.13, wps=20501.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.217, loss_scale=8, train_wall=296, gb_free=19.9, wall=91618
2022-03-05 10:48:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:50:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:50:08 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.934 | nll_loss 10.587 | ppl 1538.26 | wps 39053.3 | wpb 510.9 | bsz 1 | num_updates 28040 | best_loss 7.12
2022-03-05 10:50:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28040 updates
2022-03-05 10:50:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:50:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:50:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 144 @ 28040 updates, score 10.934) (writing took 4.200066094985232 seconds)
2022-03-05 10:50:12 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-05 10:50:12 | INFO | train | epoch 144 | loss 2.565 | nll_loss 2.037 | ppl 4.1 | wps 19978.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28040 | lr 0.000188847 | gnorm 1.216 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 91758
2022-03-05 10:50:12 | INFO | fairseq.trainer | begin training epoch 145
2022-03-05 10:50:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:53:24 | INFO | train_inner | epoch 145:     60 / 196 loss=2.551, nll_loss=2.022, ppl=4.06, wps=19701.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.191, loss_scale=8, train_wall=298, gb_free=19.9, wall=91950
2022-03-05 10:56:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:58:47 | INFO | train_inner | epoch 145:    161 / 196 loss=2.577, nll_loss=2.05, ppl=4.14, wps=20316.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.215, loss_scale=8, train_wall=299, gb_free=19.9, wall=92273
2022-03-05 11:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:00:43 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.931 | nll_loss 10.583 | ppl 1533.66 | wps 39170.3 | wpb 510.9 | bsz 1 | num_updates 28235 | best_loss 7.12
2022-03-05 11:00:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28235 updates
2022-03-05 11:00:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:00:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:00:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 145 @ 28235 updates, score 10.931) (writing took 3.9708526879549026 seconds)
2022-03-05 11:00:47 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-05 11:00:47 | INFO | train | epoch 145 | loss 2.562 | nll_loss 2.034 | ppl 4.1 | wps 20096.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28235 | lr 0.000188194 | gnorm 1.202 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 92393
2022-03-05 11:00:47 | INFO | fairseq.trainer | begin training epoch 146
2022-03-05 11:00:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:04:15 | INFO | train_inner | epoch 146:     65 / 196 loss=2.543, nll_loss=2.014, ppl=4.04, wps=19892.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.208, loss_scale=16, train_wall=296, gb_free=19.9, wall=92601
2022-03-05 11:05:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:09:38 | INFO | train_inner | epoch 146:    166 / 196 loss=2.571, nll_loss=2.044, ppl=4.12, wps=20318.2, ups=0.31, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.193, loss_scale=8, train_wall=299, gb_free=19.9, wall=92924
2022-03-05 11:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:11:18 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.943 | nll_loss 10.595 | ppl 1546.79 | wps 39006.7 | wpb 510.9 | bsz 1 | num_updates 28430 | best_loss 7.12
2022-03-05 11:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28430 updates
2022-03-05 11:11:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:11:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:11:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 146 @ 28430 updates, score 10.943) (writing took 3.973428091034293 seconds)
2022-03-05 11:11:22 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-05 11:11:22 | INFO | train | epoch 146 | loss 2.556 | nll_loss 2.028 | ppl 4.08 | wps 20092.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28430 | lr 0.000187548 | gnorm 1.201 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 93029
2022-03-05 11:11:22 | INFO | fairseq.trainer | begin training epoch 147
2022-03-05 11:11:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:13:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:15:09 | INFO | train_inner | epoch 147:     71 / 196 loss=2.539, nll_loss=2.01, ppl=4.03, wps=19715.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.205, loss_scale=8, train_wall=298, gb_free=19.9, wall=93255
2022-03-05 11:20:28 | INFO | train_inner | epoch 147:    171 / 196 loss=2.569, nll_loss=2.041, ppl=4.12, wps=20528.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.217, loss_scale=16, train_wall=296, gb_free=19.9, wall=93575
2022-03-05 11:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:21:53 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.93 | nll_loss 10.58 | ppl 1530.81 | wps 39011.5 | wpb 510.9 | bsz 1 | num_updates 28625 | best_loss 7.12
2022-03-05 11:21:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28625 updates
2022-03-05 11:21:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:21:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:21:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 147 @ 28625 updates, score 10.93) (writing took 3.8746444198768586 seconds)
2022-03-05 11:21:57 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-05 11:21:57 | INFO | train | epoch 147 | loss 2.551 | nll_loss 2.023 | ppl 4.06 | wps 20108.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28625 | lr 0.000186908 | gnorm 1.209 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 93663
2022-03-05 11:21:57 | INFO | fairseq.trainer | begin training epoch 148
2022-03-05 11:21:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:23:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 729, in train_step
    sample, is_dummy_batch = self._prepare_sample(sample)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 1238, in _prepare_sample
    if self.cfg.common.on_cpu_convert_precision:
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/omegaconf/dictconfig.py", line 295, in __getattr__
    try:
KeyboardInterrupt
