Sender: LSF System <lsfadmin@eu-g2-03>
Subject: Job 207129955: <w103_size_0.125_fp16_cross_entropy_#4> in cluster <euler> Exited

Job <w103_size_0.125_fp16_cross_entropy_#4> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:14:53 2022
Job was executed on host(s) <eu-g2-03>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:15:13 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:15:13 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575614 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   97455.05 sec.
    Max Memory :                                 7778 MB
    Average Memory :                             4484.34 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               12222.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   94208 sec.
    Turnaround time :                            94229 sec.

The output (if any) follows:

2022-03-04 09:15:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575614, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575614, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:15:28 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-04 09:15:31 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-04 09:15:31 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:15:31 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:15:31 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-04 09:15:31 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-04 09:15:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:15:31 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-04 09:15:40 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:15:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:15:40 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-03-04 09:15:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:15:40 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:15:40 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:15:40 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 09:15:40 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 09:15:40 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:15:41 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-04 09:15:41 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:15:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:15:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:16:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:16:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:16:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:21:43 | INFO | train_inner | epoch 001:    105 / 196 loss=16.488, ppl=91911, wps=19814.1, ups=0.3, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.552, loss_scale=4, train_wall=336, gb_free=7.2, wall=362
2022-03-04 09:26:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:26:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.051 | ppl 8486.66 | wps 39447.1 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-04 09:26:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-04 09:26:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 09:26:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 09:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.051) (writing took 1667.711923474446 seconds)
2022-03-04 09:54:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:54:33 | INFO | train | epoch 001 | loss 15.345 | ppl 41608.5 | wps 5410.1 | ups 0.08 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.625 | loss_scale 8 | train_wall 611 | gb_free 7.2 | wall 2333
2022-03-04 09:54:34 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:54:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:55:03 | INFO | train_inner | epoch 002:      9 / 196 loss=13.989, ppl=16263.8, wps=3267.5, ups=0.05, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.583, loss_scale=8, train_wall=303, gb_free=7.2, wall=2363
2022-03-04 10:00:31 | INFO | train_inner | epoch 002:    109 / 196 loss=11.976, ppl=4028.64, wps=20003, ups=0.31, wpb=65532.4, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.037, loss_scale=16, train_wall=303, gb_free=7.2, wall=2690
2022-03-04 10:05:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:05:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.299 | ppl 1260.19 | wps 38968.5 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.299
2022-03-04 10:05:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-04 10:05:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 10:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 10:05:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.299) (writing took 20.051902240142226 seconds)
2022-03-04 10:05:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 10:05:41 | INFO | train | epoch 002 | loss 11.441 | ppl 2780.01 | wps 19207.1 | ups 0.29 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.859 | loss_scale 16 | train_wall 594 | gb_free 7.2 | wall 3001
2022-03-04 10:05:41 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 10:05:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:06:24 | INFO | train_inner | epoch 003:     13 / 196 loss=10.608, ppl=1560.53, wps=18487, ups=0.28, wpb=65367, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.592, loss_scale=32, train_wall=303, gb_free=7.2, wall=3044
2022-03-04 10:11:52 | INFO | train_inner | epoch 003:    113 / 196 loss=10.029, ppl=1044.93, wps=19959.2, ups=0.3, wpb=65532.4, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.494, loss_scale=32, train_wall=304, gb_free=7.2, wall=3372
2022-03-04 10:13:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:16:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:16:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.594 | ppl 772.96 | wps 39104.8 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.594
2022-03-04 10:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-04 10:16:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 10:16:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 10:16:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.594) (writing took 7.477567421272397 seconds)
2022-03-04 10:16:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 10:16:37 | INFO | train | epoch 003 | loss 9.916 | ppl 965.81 | wps 19456.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.534 | loss_scale 32 | train_wall 595 | gb_free 7.2 | wall 3657
2022-03-04 10:16:37 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 10:16:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:17:36 | INFO | train_inner | epoch 004:     18 / 196 loss=9.691, ppl=826.69, wps=19015.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.602, loss_scale=32, train_wall=306, gb_free=7.2, wall=3716
2022-03-04 10:21:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:23:07 | INFO | train_inner | epoch 004:    119 / 196 loss=9.392, ppl=671.65, wps=19808.9, ups=0.3, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.644, loss_scale=32, train_wall=306, gb_free=7.2, wall=4047
2022-03-04 10:27:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:27:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.096 | ppl 547.13 | wps 39129.6 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.096
2022-03-04 10:27:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-04 10:27:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 10:27:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 10:53:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.096) (writing took 1589.0012138448656 seconds)
2022-03-04 10:53:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 10:53:53 | INFO | train | epoch 004 | loss 9.318 | ppl 638.29 | wps 5707.5 | ups 0.09 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.706 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 5893
2022-03-04 10:53:53 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 10:53:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:55:09 | INFO | train_inner | epoch 005:     23 / 196 loss=9.133, ppl=561.61, wps=3401.3, ups=0.05, wpb=65363.4, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.82, loss_scale=32, train_wall=303, gb_free=7.2, wall=5969
2022-03-04 10:55:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:59:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:00:44 | INFO | train_inner | epoch 005:    125 / 196 loss=8.897, ppl=476.81, wps=19538.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.88, loss_scale=16, train_wall=310, gb_free=7.2, wall=6304
2022-03-04 11:04:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:04:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.705 | ppl 417.41 | wps 38992.1 | wpb 510.9 | bsz 1 | num_updates 971 | best_loss 8.705
2022-03-04 11:04:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 971 updates
2022-03-04 11:04:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 11:04:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 11:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 5 @ 971 updates, score 8.705) (writing took 1509.6785944011062 seconds)
2022-03-04 11:29:52 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 11:29:52 | INFO | train | epoch 005 | loss 8.848 | ppl 460.87 | wps 5881.1 | ups 0.09 | wpb 65447.1 | bsz 127.8 | num_updates 971 | lr 0.000121451 | gnorm 0.873 | loss_scale 16 | train_wall 595 | gb_free 7.2 | wall 8052
2022-03-04 11:29:52 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 11:29:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:31:28 | INFO | train_inner | epoch 006:     29 / 196 loss=8.679, ppl=409.73, wps=3545.8, ups=0.05, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.88, loss_scale=16, train_wall=303, gb_free=7.2, wall=8147
2022-03-04 11:35:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:37:00 | INFO | train_inner | epoch 006:    130 / 196 loss=8.477, ppl=356.24, wps=19730.2, ups=0.3, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.942, loss_scale=16, train_wall=307, gb_free=7.2, wall=8480
2022-03-04 11:40:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:40:42 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.381 | ppl 333.31 | wps 38550.5 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.381
2022-03-04 11:40:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-04 11:40:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 11:40:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 11:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.381) (writing took 7.252579674124718 seconds)
2022-03-04 11:40:49 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 11:40:49 | INFO | train | epoch 006 | loss 8.452 | ppl 350.1 | wps 19429.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.947 | loss_scale 16 | train_wall 596 | gb_free 7.2 | wall 8709
2022-03-04 11:40:49 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 11:40:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:42:41 | INFO | train_inner | epoch 007:     34 / 196 loss=8.31, ppl=317.32, wps=19175.3, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.963, loss_scale=16, train_wall=303, gb_free=7.2, wall=8821
2022-03-04 11:48:09 | INFO | train_inner | epoch 007:    134 / 196 loss=8.136, ppl=281.36, wps=19963.1, ups=0.3, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.986, loss_scale=32, train_wall=304, gb_free=7.2, wall=9149
2022-03-04 11:49:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:51:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:51:38 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.097 | ppl 273.8 | wps 39066.8 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.097
2022-03-04 11:51:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-04 11:51:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 11:51:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 11:51:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 7 @ 1361 updates, score 8.097) (writing took 7.436235513538122 seconds)
2022-03-04 11:51:45 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 11:51:45 | INFO | train | epoch 007 | loss 8.117 | ppl 277.64 | wps 19454.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.98 | loss_scale 32 | train_wall 595 | gb_free 7.2 | wall 9365
2022-03-04 11:51:45 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 11:51:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:53:53 | INFO | train_inner | epoch 008:     39 / 196 loss=7.975, ppl=251.68, wps=19003.8, ups=0.29, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.968, loss_scale=32, train_wall=306, gb_free=7.2, wall=9493
2022-03-04 11:58:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:59:24 | INFO | train_inner | epoch 008:    140 / 196 loss=7.832, ppl=227.84, wps=19781.3, ups=0.3, wpb=65532.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.998, loss_scale=32, train_wall=307, gb_free=7.2, wall=9824
2022-03-04 12:02:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:02:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.853 | ppl 231.19 | wps 39196.2 | wpb 510.9 | bsz 1 | num_updates 1556 | best_loss 7.853
2022-03-04 12:02:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1556 updates
2022-03-04 12:02:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 12:02:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 12:30:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 8 @ 1556 updates, score 7.853) (writing took 1665.3221800532192 seconds)
2022-03-04 12:30:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 12:30:18 | INFO | train | epoch 008 | loss 7.818 | ppl 225.66 | wps 5517.9 | ups 0.08 | wpb 65447.5 | bsz 127.8 | num_updates 1556 | lr 0.000194561 | gnorm 0.988 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 11678
2022-03-04 12:30:18 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 12:30:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:32:42 | INFO | train_inner | epoch 009:     44 / 196 loss=7.69, ppl=206.55, wps=3271.8, ups=0.05, wpb=65367, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.968, loss_scale=32, train_wall=302, gb_free=7.2, wall=11822
2022-03-04 12:33:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:38:13 | INFO | train_inner | epoch 009:    145 / 196 loss=7.543, ppl=186.52, wps=19782.4, ups=0.3, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.995, loss_scale=32, train_wall=306, gb_free=7.2, wall=12153
2022-03-04 12:41:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:41:06 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.656 | ppl 201.65 | wps 38990.4 | wpb 510.9 | bsz 1 | num_updates 1751 | best_loss 7.656
2022-03-04 12:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1751 updates
2022-03-04 12:41:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 12:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 13:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 9 @ 1751 updates, score 7.656) (writing took 1576.628201764077 seconds)
2022-03-04 13:07:22 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 13:07:22 | INFO | train | epoch 009 | loss 7.542 | ppl 186.31 | wps 5737.5 | ups 0.09 | wpb 65447.5 | bsz 127.8 | num_updates 1751 | lr 0.000218931 | gnorm 0.988 | loss_scale 64 | train_wall 594 | gb_free 7.2 | wall 13902
2022-03-04 13:07:22 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 13:07:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:07:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:10:06 | INFO | train_inner | epoch 010:     50 / 196 loss=7.403, ppl=169.29, wps=3418.1, ups=0.05, wpb=65359.9, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=1.007, loss_scale=32, train_wall=305, gb_free=7.2, wall=14065
2022-03-04 13:14:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:15:37 | INFO | train_inner | epoch 010:    151 / 196 loss=7.287, ppl=156.13, wps=19806.9, ups=0.3, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.974, loss_scale=32, train_wall=306, gb_free=7.2, wall=14396
2022-03-04 13:18:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:18:09 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.488 | ppl 179.56 | wps 38827 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 7.488
2022-03-04 13:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-04 13:18:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 13:18:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 13:18:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 10 @ 1945 updates, score 7.488) (writing took 7.40609959512949 seconds)
2022-03-04 13:18:16 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 13:18:16 | INFO | train | epoch 010 | loss 7.287 | ppl 156.16 | wps 19411.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.983 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 14556
2022-03-04 13:18:16 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 13:18:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:21:16 | INFO | train_inner | epoch 011:     55 / 196 loss=7.152, ppl=142.2, wps=19226.6, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.957, loss_scale=32, train_wall=302, gb_free=7.2, wall=14736
2022-03-04 13:21:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:26:47 | INFO | train_inner | epoch 011:    156 / 196 loss=7.045, ppl=132.02, wps=19846.5, ups=0.3, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.962, loss_scale=32, train_wall=305, gb_free=7.2, wall=15067
2022-03-04 13:28:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:29:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.346 | ppl 162.65 | wps 39018.3 | wpb 510.9 | bsz 1 | num_updates 2140 | best_loss 7.346
2022-03-04 13:29:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2140 updates
2022-03-04 13:29:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 13:29:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 13:29:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 11 @ 2140 updates, score 7.346) (writing took 7.579794889315963 seconds)
2022-03-04 13:29:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 13:29:10 | INFO | train | epoch 011 | loss 7.047 | ppl 132.28 | wps 19529.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2140 | lr 0.000267547 | gnorm 0.958 | loss_scale 64 | train_wall 592 | gb_free 7.2 | wall 15210
2022-03-04 13:29:10 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 13:29:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:29:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:32:30 | INFO | train_inner | epoch 012:     61 / 196 loss=6.916, ppl=120.76, wps=19055.4, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.974, loss_scale=32, train_wall=305, gb_free=7.2, wall=15410
2022-03-04 13:36:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:38:00 | INFO | train_inner | epoch 012:    162 / 196 loss=6.821, ppl=113.04, wps=19827, ups=0.3, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.948, loss_scale=32, train_wall=306, gb_free=7.2, wall=15740
2022-03-04 13:39:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:39:56 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.229 | ppl 150.05 | wps 38111.4 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.229
2022-03-04 13:39:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-04 13:39:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 13:40:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 14:08:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.229) (writing took 1695.8422569204122 seconds)
2022-03-04 14:08:12 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 14:08:12 | INFO | train | epoch 012 | loss 6.823 | ppl 113.25 | wps 5420.2 | ups 0.08 | wpb 65447.1 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.953 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 17552
2022-03-04 14:08:12 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 14:08:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:11:49 | INFO | train_inner | epoch 013:     66 / 196 loss=6.681, ppl=102.63, wps=3222.4, ups=0.05, wpb=65367, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.939, loss_scale=32, train_wall=302, gb_free=7.2, wall=17769
2022-03-04 14:11:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:17:20 | INFO | train_inner | epoch 013:    167 / 196 loss=6.615, ppl=98, wps=19786.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.93, loss_scale=32, train_wall=306, gb_free=7.2, wall=18100
2022-03-04 14:18:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:19:00 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.122 | ppl 139.3 | wps 38610.8 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 7.122
2022-03-04 14:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-04 14:19:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 14:19:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 14:45:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 13 @ 2529 updates, score 7.122) (writing took 1575.3711303249002 seconds)
2022-03-04 14:45:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 14:45:15 | INFO | train | epoch 013 | loss 6.618 | ppl 98.21 | wps 5741 | ups 0.09 | wpb 65447.5 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.934 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 19775
2022-03-04 14:45:15 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 14:45:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:45:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:49:12 | INFO | train_inner | epoch 014:     72 / 196 loss=6.478, ppl=89.12, wps=3418.9, ups=0.05, wpb=65367, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.909, loss_scale=32, train_wall=306, gb_free=7.2, wall=20012
2022-03-04 14:52:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:54:44 | INFO | train_inner | epoch 014:    173 / 196 loss=6.422, ppl=85.75, wps=19732.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.897, loss_scale=32, train_wall=307, gb_free=7.2, wall=20344
2022-03-04 14:55:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:56:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.055 | ppl 132.97 | wps 38780 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 7.055
2022-03-04 14:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2723 updates
2022-03-04 14:56:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 14:56:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 14:56:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 14 @ 2723 updates, score 7.055) (writing took 7.259257273748517 seconds)
2022-03-04 14:56:12 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 14:56:12 | INFO | train | epoch 014 | loss 6.423 | ppl 85.78 | wps 19340.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2723 | lr 0.000340407 | gnorm 0.903 | loss_scale 32 | train_wall 595 | gb_free 7.2 | wall 20432
2022-03-04 14:56:12 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 14:56:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:00:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:00:28 | INFO | train_inner | epoch 015:     78 / 196 loss=6.279, ppl=77.68, wps=19019.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.922, loss_scale=32, train_wall=306, gb_free=7.2, wall=20687
2022-03-04 15:05:56 | INFO | train_inner | epoch 015:    178 / 196 loss=6.251, ppl=76.17, wps=19971.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.884, loss_scale=32, train_wall=304, gb_free=7.2, wall=21016
2022-03-04 15:06:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:07:00 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.003 | ppl 128.3 | wps 38122.4 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 7.003
2022-03-04 15:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2918 updates
2022-03-04 15:07:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 15:07:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 15:07:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 15 @ 2918 updates, score 7.003) (writing took 12.231712389737368 seconds)
2022-03-04 15:07:12 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 15:07:12 | INFO | train | epoch 015 | loss 6.247 | ppl 75.93 | wps 19323.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2918 | lr 0.000364777 | gnorm 0.9 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 21092
2022-03-04 15:07:12 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 15:07:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:08:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:10:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:11:47 | INFO | train_inner | epoch 016:     84 / 196 loss=6.099, ppl=68.57, wps=18588.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.905, loss_scale=16, train_wall=309, gb_free=7.2, wall=21367
2022-03-04 15:17:15 | INFO | train_inner | epoch 016:    184 / 196 loss=6.086, ppl=67.95, wps=20004.1, ups=0.31, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.903, loss_scale=16, train_wall=303, gb_free=7.2, wall=21695
2022-03-04 15:17:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:17:59 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7 | ppl 128.02 | wps 38691.9 | wpb 510.9 | bsz 1 | num_updates 3112 | best_loss 7
2022-03-04 15:17:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3112 updates
2022-03-04 15:17:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 15:18:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 15:44:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 16 @ 3112 updates, score 7.0) (writing took 1566.9595222268254 seconds)
2022-03-04 15:44:06 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 15:44:06 | INFO | train | epoch 016 | loss 6.08 | ppl 67.63 | wps 5734.7 | ups 0.09 | wpb 65447.1 | bsz 127.8 | num_updates 3112 | lr 0.000389022 | gnorm 0.897 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 23306
2022-03-04 15:44:06 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 15:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:47:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:48:58 | INFO | train_inner | epoch 017:     89 / 196 loss=5.932, ppl=61.05, wps=3435.4, ups=0.05, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.91, loss_scale=16, train_wall=305, gb_free=7.2, wall=23598
2022-03-04 15:54:25 | INFO | train_inner | epoch 017:    189 / 196 loss=5.936, ppl=61.23, wps=20002.4, ups=0.31, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.87, loss_scale=32, train_wall=303, gb_free=7.2, wall=23925
2022-03-04 15:54:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:54:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.971 | ppl 125.5 | wps 39387.9 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 6.971
2022-03-04 15:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3307 updates
2022-03-04 15:54:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 15:54:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt
2022-03-04 16:21:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_best.pt (epoch 17 @ 3307 updates, score 6.971) (writing took 1624.7809028867632 seconds)
2022-03-04 16:21:58 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 16:21:58 | INFO | train | epoch 017 | loss 5.926 | ppl 60.78 | wps 5618.3 | ups 0.09 | wpb 65447.5 | bsz 127.8 | num_updates 3307 | lr 0.000413392 | gnorm 0.892 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 25578
2022-03-04 16:21:58 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 16:21:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:27:02 | INFO | train_inner | epoch 018:     93 / 196 loss=5.764, ppl=54.34, wps=3340.1, ups=0.05, wpb=65367, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.891, loss_scale=32, train_wall=302, gb_free=7.2, wall=25882
2022-03-04 16:29:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:30:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:32:36 | INFO | train_inner | epoch 018:    195 / 196 loss=5.799, ppl=55.69, wps=19617.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.864, loss_scale=16, train_wall=309, gb_free=7.2, wall=26216
2022-03-04 16:32:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:32:44 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.975 | ppl 125.79 | wps 39374 | wpb 510.9 | bsz 1 | num_updates 3501 | best_loss 6.971
2022-03-04 16:32:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3501 updates
2022-03-04 16:32:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 16:32:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 16:32:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 18 @ 3501 updates, score 6.975) (writing took 4.234326438978314 seconds)
2022-03-04 16:32:49 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 16:32:49 | INFO | train | epoch 018 | loss 5.777 | ppl 54.82 | wps 19508 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 3501 | lr 0.000437637 | gnorm 0.88 | loss_scale 16 | train_wall 593 | gb_free 7.2 | wall 26228
2022-03-04 16:32:49 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 16:32:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:38:13 | INFO | train_inner | epoch 019:     99 / 196 loss=5.623, ppl=49.28, wps=19402.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.896, loss_scale=32, train_wall=302, gb_free=7.2, wall=26553
2022-03-04 16:39:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:43:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:43:36 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.974 | ppl 125.68 | wps 39274.9 | wpb 510.9 | bsz 1 | num_updates 3696 | best_loss 6.971
2022-03-04 16:43:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3696 updates
2022-03-04 16:43:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 16:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 16:43:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 19 @ 3696 updates, score 6.974) (writing took 4.24880930967629 seconds)
2022-03-04 16:43:40 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 16:43:40 | INFO | train | epoch 019 | loss 5.637 | ppl 49.75 | wps 19584.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3696 | lr 0.000462008 | gnorm 0.874 | loss_scale 16 | train_wall 594 | gb_free 7.2 | wall 26880
2022-03-04 16:43:40 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 16:43:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:43:54 | INFO | train_inner | epoch 020:      4 / 196 loss=5.641, ppl=49.91, wps=19212.3, ups=0.29, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.853, loss_scale=16, train_wall=306, gb_free=7.2, wall=26893
2022-03-04 16:46:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:49:25 | INFO | train_inner | epoch 020:    105 / 196 loss=5.484, ppl=44.76, wps=19791, ups=0.3, wpb=65532.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.909, loss_scale=16, train_wall=306, gb_free=7.2, wall=27224
2022-03-04 16:53:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:54:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:54:27 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.037 | ppl 131.28 | wps 39333.6 | wpb 510.9 | bsz 1 | num_updates 3890 | best_loss 6.971
2022-03-04 16:54:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3890 updates
2022-03-04 16:54:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 16:54:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 16:54:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 20 @ 3890 updates, score 7.037) (writing took 4.180371856316924 seconds)
2022-03-04 16:54:31 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 16:54:31 | INFO | train | epoch 020 | loss 5.505 | ppl 45.41 | wps 19501.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 3890 | lr 0.000486253 | gnorm 0.903 | loss_scale 16 | train_wall 593 | gb_free 7.2 | wall 27531
2022-03-04 16:54:31 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 16:54:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:55:04 | INFO | train_inner | epoch 021:     10 / 196 loss=5.512, ppl=45.64, wps=19247.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.894, loss_scale=16, train_wall=305, gb_free=7.2, wall=27564
2022-03-04 17:00:32 | INFO | train_inner | epoch 021:    110 / 196 loss=5.35, ppl=40.78, wps=20006.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.887, loss_scale=16, train_wall=303, gb_free=7.2, wall=27892
2022-03-04 17:01:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:05:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:05:18 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.025 | ppl 130.26 | wps 39049 | wpb 510.9 | bsz 1 | num_updates 4085 | best_loss 6.971
2022-03-04 17:05:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4085 updates
2022-03-04 17:05:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:05:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 21 @ 4085 updates, score 7.025) (writing took 4.2092835288494825 seconds)
2022-03-04 17:05:23 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 17:05:23 | INFO | train | epoch 021 | loss 5.377 | ppl 41.56 | wps 19596.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4085 | lr 0.000494771 | gnorm 0.893 | loss_scale 16 | train_wall 593 | gb_free 7.2 | wall 28182
2022-03-04 17:05:23 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 17:05:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:06:12 | INFO | train_inner | epoch 022:     15 / 196 loss=5.382, ppl=41.69, wps=19218.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.899, loss_scale=16, train_wall=306, gb_free=7.2, wall=28232
2022-03-04 17:11:40 | INFO | train_inner | epoch 022:    115 / 196 loss=5.22, ppl=37.26, wps=20000, ups=0.31, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.85, loss_scale=32, train_wall=303, gb_free=7.2, wall=28559
2022-03-04 17:12:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:16:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:16:10 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.145 | ppl 141.49 | wps 38518.8 | wpb 510.9 | bsz 1 | num_updates 4280 | best_loss 6.971
2022-03-04 17:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4280 updates
2022-03-04 17:16:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:16:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:16:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 22 @ 4280 updates, score 7.145) (writing took 4.496266040951014 seconds)
2022-03-04 17:16:14 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 17:16:14 | INFO | train | epoch 022 | loss 5.241 | ppl 37.8 | wps 19587.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4280 | lr 0.000483368 | gnorm 0.85 | loss_scale 16 | train_wall 593 | gb_free 7.2 | wall 28834
2022-03-04 17:16:14 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 17:16:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:17:20 | INFO | train_inner | epoch 023:     20 / 196 loss=5.226, ppl=37.43, wps=19212.8, ups=0.29, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.834, loss_scale=16, train_wall=305, gb_free=7.2, wall=28900
2022-03-04 17:20:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:51 | INFO | train_inner | epoch 023:    121 / 196 loss=5.093, ppl=34.13, wps=19783.1, ups=0.3, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.841, loss_scale=16, train_wall=307, gb_free=7.2, wall=29231
2022-03-04 17:26:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:27:02 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.164 | ppl 143.4 | wps 38799.5 | wpb 510.9 | bsz 1 | num_updates 4475 | best_loss 6.971
2022-03-04 17:27:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4475 updates
2022-03-04 17:27:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 23 @ 4475 updates, score 7.164) (writing took 4.2607292626053095 seconds)
2022-03-04 17:27:07 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 17:27:07 | INFO | train | epoch 023 | loss 5.11 | ppl 34.54 | wps 19562.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4475 | lr 0.000472719 | gnorm 0.833 | loss_scale 16 | train_wall 594 | gb_free 7.2 | wall 29486
2022-03-04 17:27:07 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 17:27:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:28:29 | INFO | train_inner | epoch 024:     25 / 196 loss=5.092, ppl=34.11, wps=19370.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.827, loss_scale=32, train_wall=303, gb_free=7.2, wall=29568
2022-03-04 17:28:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:34:00 | INFO | train_inner | epoch 024:    126 / 196 loss=4.973, ppl=31.41, wps=19752.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.845, loss_scale=16, train_wall=307, gb_free=7.2, wall=29900
2022-03-04 17:37:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:37:55 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.243 | ppl 151.48 | wps 38977.4 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 6.971
2022-03-04 17:37:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-04 17:37:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:37:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 24 @ 4670 updates, score 7.243) (writing took 4.149793885648251 seconds)
2022-03-04 17:37:59 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 17:37:59 | INFO | train | epoch 024 | loss 4.988 | ppl 31.75 | wps 19557.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.836 | loss_scale 32 | train_wall 595 | gb_free 7.2 | wall 30139
2022-03-04 17:37:59 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 17:37:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:39:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:39:41 | INFO | train_inner | epoch 025:     31 / 196 loss=4.971, ppl=31.35, wps=19193.9, ups=0.29, wpb=65367, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.839, loss_scale=16, train_wall=306, gb_free=7.2, wall=30241
2022-03-04 17:45:09 | INFO | train_inner | epoch 025:    131 / 196 loss=4.866, ppl=29.16, wps=19955.5, ups=0.3, wpb=65532.4, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.834, loss_scale=16, train_wall=304, gb_free=7.2, wall=30569
2022-03-04 17:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:48:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:48:48 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.288 | ppl 156.3 | wps 39116.6 | wpb 510.9 | bsz 1 | num_updates 4864 | best_loss 6.971
2022-03-04 17:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4864 updates
2022-03-04 17:48:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:48:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:48:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 25 @ 4864 updates, score 7.288) (writing took 4.146290587261319 seconds)
2022-03-04 17:48:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 17:48:52 | INFO | train | epoch 025 | loss 4.876 | ppl 29.37 | wps 19456.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.847 | loss_scale 16 | train_wall 595 | gb_free 7.2 | wall 30791
2022-03-04 17:48:52 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 17:48:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:50:50 | INFO | train_inner | epoch 026:     36 / 196 loss=4.843, ppl=28.7, wps=19189.4, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.836, loss_scale=16, train_wall=306, gb_free=7.2, wall=30910
2022-03-04 17:56:18 | INFO | train_inner | epoch 026:    136 / 196 loss=4.76, ppl=27.09, wps=19969.9, ups=0.3, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.84, loss_scale=32, train_wall=304, gb_free=7.2, wall=31238
2022-03-04 17:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:59:40 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.361 | ppl 164.42 | wps 38328.8 | wpb 510.9 | bsz 1 | num_updates 5060 | best_loss 6.971
2022-03-04 17:59:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5060 updates
2022-03-04 17:59:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:59:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 17:59:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 26 @ 5060 updates, score 7.361) (writing took 4.141012730076909 seconds)
2022-03-04 17:59:44 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 17:59:44 | INFO | train | epoch 026 | loss 4.768 | ppl 27.25 | wps 19660.8 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 5060 | lr 0.000444554 | gnorm 0.814 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 31444
2022-03-04 17:59:44 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 17:59:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:01:56 | INFO | train_inner | epoch 027:     40 / 196 loss=4.73, ppl=26.53, wps=19368.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.803, loss_scale=32, train_wall=303, gb_free=7.2, wall=31575
2022-03-04 18:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:07:27 | INFO | train_inner | epoch 027:    141 / 196 loss=4.672, ppl=25.49, wps=19760.6, ups=0.3, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.831, loss_scale=32, train_wall=307, gb_free=7.2, wall=31907
2022-03-04 18:09:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:10:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:10:33 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.408 | ppl 169.82 | wps 39166.7 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 6.971
2022-03-04 18:10:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5254 updates
2022-03-04 18:10:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:10:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 27 @ 5254 updates, score 7.408) (writing took 4.127084149047732 seconds)
2022-03-04 18:10:37 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 18:10:37 | INFO | train | epoch 027 | loss 4.668 | ppl 25.42 | wps 19454.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 5254 | lr 0.00043627 | gnorm 0.832 | loss_scale 32 | train_wall 595 | gb_free 7.2 | wall 32097
2022-03-04 18:10:37 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 18:10:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:08 | INFO | train_inner | epoch 028:     46 / 196 loss=4.618, ppl=24.56, wps=19177.8, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.816, loss_scale=32, train_wall=306, gb_free=7.2, wall=32248
2022-03-04 18:16:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:18:39 | INFO | train_inner | epoch 028:    147 / 196 loss=4.575, ppl=23.84, wps=19775, ups=0.3, wpb=65532.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.834, loss_scale=16, train_wall=307, gb_free=7.2, wall=32579
2022-03-04 18:21:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:21:25 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.481 | ppl 178.69 | wps 38738.3 | wpb 510.9 | bsz 1 | num_updates 5449 | best_loss 6.971
2022-03-04 18:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5449 updates
2022-03-04 18:21:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:21:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:21:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 28 @ 5449 updates, score 7.481) (writing took 4.164540112018585 seconds)
2022-03-04 18:21:30 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 18:21:30 | INFO | train | epoch 028 | loss 4.571 | ppl 23.77 | wps 19550.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5449 | lr 0.000428392 | gnorm 0.829 | loss_scale 16 | train_wall 595 | gb_free 7.2 | wall 32749
2022-03-04 18:21:30 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 18:21:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:24:17 | INFO | train_inner | epoch 029:     51 / 196 loss=4.515, ppl=22.87, wps=19354.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.826, loss_scale=32, train_wall=303, gb_free=7.2, wall=32917
2022-03-04 18:25:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:29:49 | INFO | train_inner | epoch 029:    152 / 196 loss=4.49, ppl=22.47, wps=19731.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.839, loss_scale=16, train_wall=307, gb_free=7.2, wall=33249
2022-03-04 18:32:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:32:19 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.524 | ppl 184 | wps 38976.3 | wpb 510.9 | bsz 1 | num_updates 5644 | best_loss 6.971
2022-03-04 18:32:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5644 updates
2022-03-04 18:32:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:32:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:32:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 29 @ 5644 updates, score 7.524) (writing took 4.120698960497975 seconds)
2022-03-04 18:32:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 18:32:23 | INFO | train | epoch 029 | loss 4.48 | ppl 22.32 | wps 19533.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5644 | lr 0.000420927 | gnorm 0.829 | loss_scale 32 | train_wall 595 | gb_free 7.2 | wall 33403
2022-03-04 18:32:23 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 18:32:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:35:27 | INFO | train_inner | epoch 030:     56 / 196 loss=4.418, ppl=21.37, wps=19369.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.831, loss_scale=32, train_wall=303, gb_free=7.2, wall=33587
2022-03-04 18:36:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:40:58 | INFO | train_inner | epoch 030:    157 / 196 loss=4.407, ppl=21.21, wps=19803.3, ups=0.3, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.836, loss_scale=16, train_wall=306, gb_free=7.2, wall=33917
2022-03-04 18:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:43:10 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.599 | ppl 193.93 | wps 39337.8 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 6.971
2022-03-04 18:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-04 18:43:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:43:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.599) (writing took 4.3888129480183125 seconds)
2022-03-04 18:43:15 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 18:43:15 | INFO | train | epoch 030 | loss 4.393 | ppl 21 | wps 19584.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.828 | loss_scale 16 | train_wall 594 | gb_free 7.2 | wall 34054
2022-03-04 18:43:15 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 18:43:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:46:35 | INFO | train_inner | epoch 031:     61 / 196 loss=4.331, ppl=20.12, wps=19399.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.826, loss_scale=32, train_wall=302, gb_free=7.2, wall=34254
2022-03-04 18:51:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:52:06 | INFO | train_inner | epoch 031:    162 / 196 loss=4.328, ppl=20.08, wps=19793.7, ups=0.3, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.837, loss_scale=32, train_wall=306, gb_free=7.2, wall=34586
2022-03-04 18:53:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:54:02 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.677 | ppl 204.68 | wps 39086.9 | wpb 510.9 | bsz 1 | num_updates 6034 | best_loss 6.971
2022-03-04 18:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6034 updates
2022-03-04 18:54:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 18:54:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 31 @ 6034 updates, score 7.677) (writing took 4.337911978363991 seconds)
2022-03-04 18:54:06 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 18:54:06 | INFO | train | epoch 031 | loss 4.314 | ppl 19.89 | wps 19579.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6034 | lr 0.000407096 | gnorm 0.844 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 34706
2022-03-04 18:54:06 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 18:54:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:57:43 | INFO | train_inner | epoch 032:     66 / 196 loss=4.24, ppl=18.9, wps=19403.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.837, loss_scale=32, train_wall=302, gb_free=7.2, wall=34922
2022-03-04 18:58:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:00:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:03:16 | INFO | train_inner | epoch 032:    168 / 196 loss=4.262, ppl=19.19, wps=19631.6, ups=0.3, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.863, loss_scale=16, train_wall=309, gb_free=7.2, wall=35256
2022-03-04 19:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:04:53 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.715 | ppl 210.18 | wps 39280.3 | wpb 510.9 | bsz 1 | num_updates 6228 | best_loss 6.971
2022-03-04 19:04:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6228 updates
2022-03-04 19:04:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:04:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:04:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 32 @ 6228 updates, score 7.715) (writing took 4.344884809106588 seconds)
2022-03-04 19:04:57 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 19:04:57 | INFO | train | epoch 032 | loss 4.234 | ppl 18.82 | wps 19508.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 6228 | lr 0.000400706 | gnorm 0.84 | loss_scale 16 | train_wall 593 | gb_free 7.2 | wall 35357
2022-03-04 19:04:57 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 19:04:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:08:53 | INFO | train_inner | epoch 033:     72 / 196 loss=4.16, ppl=17.88, wps=19410.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.846, loss_scale=32, train_wall=302, gb_free=7.2, wall=35593
2022-03-04 19:14:21 | INFO | train_inner | epoch 033:    172 / 196 loss=4.186, ppl=18.2, wps=19976.7, ups=0.3, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.848, loss_scale=32, train_wall=304, gb_free=7.2, wall=35921
2022-03-04 19:15:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:15:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:15:45 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.806 | ppl 223.77 | wps 39348.3 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 6.971
2022-03-04 19:15:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6423 updates
2022-03-04 19:15:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:15:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 33 @ 6423 updates, score 7.806) (writing took 4.286763647571206 seconds)
2022-03-04 19:15:49 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 19:15:49 | INFO | train | epoch 033 | loss 4.16 | ppl 17.87 | wps 19580 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6423 | lr 0.000394576 | gnorm 0.85 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 36009
2022-03-04 19:15:49 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 19:15:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:20:02 | INFO | train_inner | epoch 034:     77 / 196 loss=4.078, ppl=16.89, wps=19185, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.854, loss_scale=32, train_wall=306, gb_free=7.2, wall=36262
2022-03-04 19:22:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:25:33 | INFO | train_inner | epoch 034:    178 / 196 loss=4.121, ppl=17.4, wps=19791.4, ups=0.3, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.868, loss_scale=32, train_wall=306, gb_free=7.2, wall=36593
2022-03-04 19:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:26:37 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.917 | ppl 241.75 | wps 38821.3 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 6.971
2022-03-04 19:26:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-04 19:26:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 34 @ 6618 updates, score 7.917) (writing took 4.26641490496695 seconds)
2022-03-04 19:26:41 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 19:26:41 | INFO | train | epoch 034 | loss 4.09 | ppl 17.02 | wps 19566.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.857 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 36661
2022-03-04 19:26:41 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 19:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:29:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:31:13 | INFO | train_inner | epoch 035:     83 / 196 loss=4.001, ppl=16.02, wps=19204.1, ups=0.29, wpb=65367, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.861, loss_scale=32, train_wall=306, gb_free=7.2, wall=36933
2022-03-04 19:36:41 | INFO | train_inner | epoch 035:    183 / 196 loss=4.056, ppl=16.64, wps=19994.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.845, loss_scale=64, train_wall=303, gb_free=7.2, wall=37261
2022-03-04 19:37:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:37:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:37:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.973 | ppl 251.18 | wps 39231 | wpb 510.9 | bsz 1 | num_updates 6812 | best_loss 6.971
2022-03-04 19:37:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6812 updates
2022-03-04 19:37:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:37:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:37:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 35 @ 6812 updates, score 7.973) (writing took 4.167843410745263 seconds)
2022-03-04 19:37:33 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 19:37:33 | INFO | train | epoch 035 | loss 4.021 | ppl 16.24 | wps 19489.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 6812 | lr 0.000383145 | gnorm 0.857 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 37313
2022-03-04 19:37:33 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 19:37:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:42:21 | INFO | train_inner | epoch 036:     88 / 196 loss=3.928, ppl=15.22, wps=19228.8, ups=0.29, wpb=65367, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.857, loss_scale=32, train_wall=305, gb_free=7.2, wall=37601
2022-03-04 19:44:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:47:52 | INFO | train_inner | epoch 036:    189 / 196 loss=3.996, ppl=15.95, wps=19806.5, ups=0.3, wpb=65532.4, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.853, loss_scale=32, train_wall=306, gb_free=7.2, wall=37932
2022-03-04 19:48:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:48:20 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.091 | ppl 272.65 | wps 39087.8 | wpb 510.9 | bsz 1 | num_updates 7007 | best_loss 6.971
2022-03-04 19:48:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7007 updates
2022-03-04 19:48:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:48:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:48:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 36 @ 7007 updates, score 8.091) (writing took 4.167933275923133 seconds)
2022-03-04 19:48:24 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 19:48:24 | INFO | train | epoch 036 | loss 3.957 | ppl 15.53 | wps 19598.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7007 | lr 0.000377776 | gnorm 0.857 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 37964
2022-03-04 19:48:24 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 19:48:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:51:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:53:32 | INFO | train_inner | epoch 037:     94 / 196 loss=3.856, ppl=14.48, wps=19223.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.867, loss_scale=32, train_wall=305, gb_free=7.2, wall=38272
2022-03-04 19:58:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:59:03 | INFO | train_inner | epoch 037:    195 / 196 loss=3.943, ppl=15.38, wps=19798.6, ups=0.3, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.869, loss_scale=32, train_wall=306, gb_free=7.2, wall=38603
2022-03-04 19:59:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:59:11 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.136 | ppl 281.32 | wps 38598.2 | wpb 510.9 | bsz 1 | num_updates 7201 | best_loss 6.971
2022-03-04 19:59:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7201 updates
2022-03-04 19:59:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:59:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 19:59:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 37 @ 7201 updates, score 8.136) (writing took 4.206649919971824 seconds)
2022-03-04 19:59:15 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 19:59:15 | INFO | train | epoch 037 | loss 3.895 | ppl 14.88 | wps 19487.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 7201 | lr 0.000372652 | gnorm 0.867 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 38615
2022-03-04 19:59:15 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 19:59:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:04:40 | INFO | train_inner | epoch 038:     99 / 196 loss=3.788, ppl=13.81, wps=19380.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.872, loss_scale=32, train_wall=303, gb_free=7.2, wall=38940
2022-03-04 20:05:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:10:03 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.19 | ppl 292.02 | wps 39160.4 | wpb 510.9 | bsz 1 | num_updates 7396 | best_loss 6.971
2022-03-04 20:10:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7396 updates
2022-03-04 20:10:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 38 @ 7396 updates, score 8.19) (writing took 4.173979017883539 seconds)
2022-03-04 20:10:07 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 20:10:07 | INFO | train | epoch 038 | loss 3.835 | ppl 14.27 | wps 19580.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7396 | lr 0.000367707 | gnorm 0.879 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 39267
2022-03-04 20:10:07 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 20:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:10:21 | INFO | train_inner | epoch 039:      4 / 196 loss=3.876, ppl=14.69, wps=19213.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.89, loss_scale=32, train_wall=306, gb_free=7.2, wall=39280
2022-03-04 20:11:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:15:52 | INFO | train_inner | epoch 039:    105 / 196 loss=3.736, ppl=13.32, wps=19773.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.861, loss_scale=16, train_wall=307, gb_free=7.2, wall=39612
2022-03-04 20:20:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:20:55 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.223 | ppl 298.83 | wps 39485.3 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 6.971
2022-03-04 20:20:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-04 20:20:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:20:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:20:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 39 @ 7591 updates, score 8.223) (writing took 4.259030152112246 seconds)
2022-03-04 20:20:59 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 20:20:59 | INFO | train | epoch 039 | loss 3.779 | ppl 13.72 | wps 19578.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.877 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 39919
2022-03-04 20:20:59 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 20:20:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:21:29 | INFO | train_inner | epoch 040:      9 / 196 loss=3.813, ppl=14.05, wps=19409.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.887, loss_scale=32, train_wall=302, gb_free=7.2, wall=39949
2022-03-04 20:22:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:27:00 | INFO | train_inner | epoch 040:    110 / 196 loss=3.686, ppl=12.87, wps=19785.4, ups=0.3, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.886, loss_scale=16, train_wall=306, gb_free=7.2, wall=40280
2022-03-04 20:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:31:46 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.277 | ppl 310.08 | wps 39491.2 | wpb 510.9 | bsz 1 | num_updates 7786 | best_loss 6.971
2022-03-04 20:31:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7786 updates
2022-03-04 20:31:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:31:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:31:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 40 @ 7786 updates, score 8.277) (writing took 4.1784759648144245 seconds)
2022-03-04 20:31:51 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 20:31:51 | INFO | train | epoch 040 | loss 3.724 | ppl 13.22 | wps 19587.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7786 | lr 0.000358379 | gnorm 0.896 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 40570
2022-03-04 20:31:51 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 20:31:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:32:37 | INFO | train_inner | epoch 041:     14 / 196 loss=3.752, ppl=13.47, wps=19420.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.913, loss_scale=32, train_wall=302, gb_free=7.2, wall=40616
2022-03-04 20:36:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:38:07 | INFO | train_inner | epoch 041:    115 / 196 loss=3.639, ppl=12.46, wps=19801.6, ups=0.3, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.885, loss_scale=32, train_wall=306, gb_free=7.2, wall=40947
2022-03-04 20:40:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:42:37 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.365 | ppl 329.69 | wps 39012.8 | wpb 510.9 | bsz 1 | num_updates 7980 | best_loss 6.971
2022-03-04 20:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7980 updates
2022-03-04 20:42:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:42:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:42:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 41 @ 7980 updates, score 8.365) (writing took 4.1871393620967865 seconds)
2022-03-04 20:42:42 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 20:42:42 | INFO | train | epoch 041 | loss 3.67 | ppl 12.72 | wps 19501.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 7980 | lr 0.000353996 | gnorm 0.89 | loss_scale 16 | train_wall 593 | gb_free 7.2 | wall 41221
2022-03-04 20:42:42 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 20:42:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:43:47 | INFO | train_inner | epoch 042:     20 / 196 loss=3.68, ppl=12.82, wps=19234.3, ups=0.29, wpb=65359.9, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.885, loss_scale=16, train_wall=305, gb_free=7.2, wall=41287
2022-03-04 20:49:15 | INFO | train_inner | epoch 042:    120 / 196 loss=3.595, ppl=12.09, wps=19980.9, ups=0.3, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.881, loss_scale=32, train_wall=303, gb_free=7.2, wall=41615
2022-03-04 20:53:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:53:30 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.418 | ppl 342.08 | wps 38692.4 | wpb 510.9 | bsz 1 | num_updates 8176 | best_loss 6.971
2022-03-04 20:53:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8176 updates
2022-03-04 20:53:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 20:53:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 42 @ 8176 updates, score 8.418) (writing took 4.080655040219426 seconds)
2022-03-04 20:53:34 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 20:53:34 | INFO | train | epoch 042 | loss 3.621 | ppl 12.3 | wps 19665.9 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 8176 | lr 0.000349727 | gnorm 0.891 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 41874
2022-03-04 20:53:34 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 20:53:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:54:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 20:54:56 | INFO | train_inner | epoch 043:     25 / 196 loss=3.633, ppl=12.41, wps=19185.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.911, loss_scale=32, train_wall=306, gb_free=7.2, wall=41956
2022-03-04 21:00:24 | INFO | train_inner | epoch 043:    125 / 196 loss=3.554, ppl=11.75, wps=19974.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.901, loss_scale=32, train_wall=304, gb_free=7.2, wall=42284
2022-03-04 21:02:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 21:04:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:04:22 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.488 | ppl 359.1 | wps 39210.5 | wpb 510.9 | bsz 1 | num_updates 8370 | best_loss 6.971
2022-03-04 21:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8370 updates
2022-03-04 21:04:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:04:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:04:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 43 @ 8370 updates, score 8.488) (writing took 4.109599458053708 seconds)
2022-03-04 21:04:26 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 21:04:26 | INFO | train | epoch 043 | loss 3.57 | ppl 11.88 | wps 19481.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 8370 | lr 0.000345651 | gnorm 0.903 | loss_scale 32 | train_wall 594 | gb_free 7.2 | wall 42525
2022-03-04 21:04:26 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 21:04:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:06:04 | INFO | train_inner | epoch 044:     30 / 196 loss=3.568, ppl=11.86, wps=19227.2, ups=0.29, wpb=65367, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.898, loss_scale=32, train_wall=305, gb_free=7.2, wall=42624
2022-03-04 21:09:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 21:11:35 | INFO | train_inner | epoch 044:    131 / 196 loss=3.507, ppl=11.37, wps=19808.8, ups=0.3, wpb=65532.4, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.912, loss_scale=32, train_wall=306, gb_free=7.2, wall=42955
2022-03-04 21:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:15:12 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.555 | ppl 375.98 | wps 38975.4 | wpb 510.9 | bsz 1 | num_updates 8565 | best_loss 6.971
2022-03-04 21:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8565 updates
2022-03-04 21:15:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:15:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 44 @ 8565 updates, score 8.555) (writing took 4.124061498790979 seconds)
2022-03-04 21:15:16 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 21:15:16 | INFO | train | epoch 044 | loss 3.524 | ppl 11.51 | wps 19619.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8565 | lr 0.000341693 | gnorm 0.904 | loss_scale 32 | train_wall 593 | gb_free 7.2 | wall 43176
2022-03-04 21:15:16 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 21:15:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:15:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:17:14 | INFO | train_inner | epoch 045:     36 / 196 loss=3.526, ppl=11.52, wps=19284.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.902, loss_scale=16, train_wall=304, gb_free=7.2, wall=43294
2022-03-04 21:22:45 | INFO | train_inner | epoch 045:    136 / 196 loss=3.471, ppl=11.09, wps=19773.4, ups=0.3, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.901, loss_scale=16, train_wall=307, gb_free=7.2, wall=43625
2022-03-04 21:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:26:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:26:10 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.677 | ppl 409.36 | wps 37968.5 | wpb 510.9 | bsz 1 | num_updates 8759 | best_loss 6.971
2022-03-04 21:26:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8759 updates
2022-03-04 21:26:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:26:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:26:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 45 @ 8759 updates, score 8.677) (writing took 4.275250004604459 seconds)
2022-03-04 21:26:14 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 21:26:14 | INFO | train | epoch 045 | loss 3.481 | ppl 11.17 | wps 19304.2 | ups 0.29 | wpb 65447.1 | bsz 127.8 | num_updates 8759 | lr 0.000337888 | gnorm 0.912 | loss_scale 16 | train_wall 599 | gb_free 7.2 | wall 43834
2022-03-04 21:26:14 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 21:26:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:28:28 | INFO | train_inner | epoch 046:     41 / 196 loss=3.465, ppl=11.04, wps=19056.4, ups=0.29, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.922, loss_scale=16, train_wall=308, gb_free=7.2, wall=43968
2022-03-04 21:33:55 | INFO | train_inner | epoch 046:    141 / 196 loss=3.432, ppl=10.79, wps=20034, ups=0.31, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.919, loss_scale=32, train_wall=303, gb_free=7.2, wall=44295
2022-03-04 21:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:37:00 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.685 | ppl 411.44 | wps 39387.9 | wpb 510.9 | bsz 1 | num_updates 8955 | best_loss 6.971
2022-03-04 21:37:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8955 updates
2022-03-04 21:37:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:37:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:37:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 46 @ 8955 updates, score 8.685) (writing took 4.158701216802001 seconds)
2022-03-04 21:37:04 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 21:37:04 | INFO | train | epoch 046 | loss 3.436 | ppl 10.82 | wps 19737.3 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 8955 | lr 0.00033417 | gnorm 0.919 | loss_scale 32 | train_wall 592 | gb_free 7.2 | wall 44484
2022-03-04 21:37:04 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 21:37:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:38:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 21:39:35 | INFO | train_inner | epoch 047:     46 / 196 loss=3.418, ppl=10.69, wps=19262.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.911, loss_scale=32, train_wall=305, gb_free=7.2, wall=44635
2022-03-04 21:41:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:45:08 | INFO | train_inner | epoch 047:    147 / 196 loss=3.387, ppl=10.46, wps=19688.2, ups=0.3, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.926, loss_scale=16, train_wall=308, gb_free=7.2, wall=44967
2022-03-04 21:47:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:47:54 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.749 | ppl 430.16 | wps 38691.8 | wpb 510.9 | bsz 1 | num_updates 9149 | best_loss 6.971
2022-03-04 21:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9149 updates
2022-03-04 21:47:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:47:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:47:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 47 @ 9149 updates, score 8.749) (writing took 4.294545207172632 seconds)
2022-03-04 21:47:58 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 21:47:58 | INFO | train | epoch 047 | loss 3.392 | ppl 10.49 | wps 19397.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 9149 | lr 0.000330608 | gnorm 0.921 | loss_scale 16 | train_wall 596 | gb_free 7.2 | wall 45138
2022-03-04 21:47:58 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 21:47:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:50:47 | INFO | train_inner | epoch 048:     51 / 196 loss=3.376, ppl=10.38, wps=19285.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.938, loss_scale=32, train_wall=304, gb_free=7.2, wall=45306
2022-03-04 21:55:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 21:56:19 | INFO | train_inner | epoch 048:    152 / 196 loss=3.359, ppl=10.26, wps=19680, ups=0.3, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.926, loss_scale=32, train_wall=308, gb_free=7.2, wall=45639
2022-03-04 21:58:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:58:49 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.834 | ppl 456.45 | wps 38757.6 | wpb 510.9 | bsz 1 | num_updates 9344 | best_loss 6.971
2022-03-04 21:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9344 updates
2022-03-04 21:58:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:58:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 21:58:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 48 @ 9344 updates, score 8.834) (writing took 4.488267447799444 seconds)
2022-03-04 21:58:53 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 21:58:53 | INFO | train | epoch 048 | loss 3.352 | ppl 10.21 | wps 19480.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.923 | loss_scale 32 | train_wall 597 | gb_free 7.2 | wall 45793
2022-03-04 21:58:53 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 21:58:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:01:58 | INFO | train_inner | epoch 049:     56 / 196 loss=3.317, ppl=9.97, wps=19313.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.907, loss_scale=32, train_wall=304, gb_free=7.2, wall=45978
2022-03-04 22:02:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 22:07:30 | INFO | train_inner | epoch 049:    157 / 196 loss=3.328, ppl=10.04, wps=19730.1, ups=0.3, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.94, loss_scale=32, train_wall=307, gb_free=7.2, wall=46310
2022-03-04 22:09:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 22:09:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:09:43 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.854 | ppl 462.88 | wps 38978.8 | wpb 510.9 | bsz 1 | num_updates 9538 | best_loss 6.971
2022-03-04 22:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9538 updates
2022-03-04 22:09:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:09:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 49 @ 9538 updates, score 8.854) (writing took 4.472268238663673 seconds)
2022-03-04 22:09:48 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 22:09:48 | INFO | train | epoch 049 | loss 3.314 | ppl 9.95 | wps 19433.5 | ups 0.3 | wpb 65534.1 | bsz 128 | num_updates 9538 | lr 0.000323796 | gnorm 0.93 | loss_scale 32 | train_wall 596 | gb_free 7.2 | wall 46447
2022-03-04 22:09:48 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 22:09:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:13:12 | INFO | train_inner | epoch 050:     62 / 196 loss=3.278, ppl=9.7, wps=19183, ups=0.29, wpb=65532.4, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.933, loss_scale=32, train_wall=307, gb_free=7.2, wall=46652
2022-03-04 22:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 22:18:44 | INFO | train_inner | epoch 050:    163 / 196 loss=3.296, ppl=9.82, wps=19732.2, ups=0.3, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.942, loss_scale=32, train_wall=307, gb_free=7.2, wall=46984
2022-03-04 22:19:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:20:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:20:37 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.912 | ppl 481.7 | wps 38876.9 | wpb 510.9 | bsz 1 | num_updates 9732 | best_loss 6.971
2022-03-04 22:20:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9732 updates
2022-03-04 22:20:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:20:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 50 @ 9732 updates, score 8.912) (writing took 4.488496309146285 seconds)
2022-03-04 22:20:42 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 22:20:42 | INFO | train | epoch 050 | loss 3.275 | ppl 9.68 | wps 19415.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 9732 | lr 0.000320552 | gnorm 0.948 | loss_scale 16 | train_wall 596 | gb_free 7.2 | wall 47101
2022-03-04 22:20:42 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 22:20:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:24:25 | INFO | train_inner | epoch 051:     68 / 196 loss=3.221, ppl=9.32, wps=19145.5, ups=0.29, wpb=65367, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.948, loss_scale=16, train_wall=306, gb_free=7.2, wall=47325
2022-03-04 22:28:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:29:57 | INFO | train_inner | epoch 051:    169 / 196 loss=3.264, ppl=9.61, wps=19745.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.959, loss_scale=16, train_wall=307, gb_free=7.2, wall=47657
2022-03-04 22:31:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:31:31 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.026 | ppl 521.27 | wps 38925.8 | wpb 510.9 | bsz 1 | num_updates 9927 | best_loss 6.971
2022-03-04 22:31:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9927 updates
2022-03-04 22:31:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:31:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 51 @ 9927 updates, score 9.026) (writing took 4.443522123619914 seconds)
2022-03-04 22:31:35 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 22:31:35 | INFO | train | epoch 051 | loss 3.237 | ppl 9.43 | wps 19522.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9927 | lr 0.000317388 | gnorm 0.942 | loss_scale 16 | train_wall 595 | gb_free 7.2 | wall 47755
2022-03-04 22:31:35 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 22:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:35:36 | INFO | train_inner | epoch 052:     73 / 196 loss=3.186, ppl=9.1, wps=19313.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.938, loss_scale=32, train_wall=304, gb_free=7.2, wall=47995
2022-03-04 22:41:05 | INFO | train_inner | epoch 052:    173 / 196 loss=3.231, ppl=9.39, wps=19912, ups=0.3, wpb=65532.4, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.959, loss_scale=32, train_wall=305, gb_free=7.2, wall=48324
2022-03-04 22:42:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 22:42:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:42:25 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.999 | ppl 511.69 | wps 38690.3 | wpb 510.9 | bsz 1 | num_updates 10122 | best_loss 6.971
2022-03-04 22:42:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10122 updates
2022-03-04 22:42:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:42:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:42:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 52 @ 10122 updates, score 8.999) (writing took 4.529768597334623 seconds)
2022-03-04 22:42:30 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 22:42:30 | INFO | train | epoch 052 | loss 3.203 | ppl 9.21 | wps 19506.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 10122 | lr 0.000314316 | gnorm 0.956 | loss_scale 32 | train_wall 596 | gb_free 7.2 | wall 48409
2022-03-04 22:42:30 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 22:42:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:45:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:46:50 | INFO | train_inner | epoch 053:     79 / 196 loss=3.145, ppl=8.85, wps=18944.6, ups=0.29, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.963, loss_scale=16, train_wall=310, gb_free=7.2, wall=48670
2022-03-04 22:52:19 | INFO | train_inner | epoch 053:    179 / 196 loss=3.202, ppl=9.2, wps=19931.7, ups=0.3, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.956, loss_scale=32, train_wall=304, gb_free=7.2, wall=48998
2022-03-04 22:53:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:53:19 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.092 | ppl 545.59 | wps 39041 | wpb 510.9 | bsz 1 | num_updates 10317 | best_loss 6.971
2022-03-04 22:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10317 updates
2022-03-04 22:53:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:53:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 22:53:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 53 @ 10317 updates, score 9.092) (writing took 4.358461489900947 seconds)
2022-03-04 22:53:24 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 22:53:24 | INFO | train | epoch 053 | loss 3.168 | ppl 8.99 | wps 19515.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 10317 | lr 0.000311332 | gnorm 0.956 | loss_scale 32 | train_wall 596 | gb_free 7.2 | wall 49063
2022-03-04 22:53:24 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 22:53:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:54:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:58:01 | INFO | train_inner | epoch 054:     84 / 196 loss=3.103, ppl=8.59, wps=19111.1, ups=0.29, wpb=65363.4, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.954, loss_scale=16, train_wall=307, gb_free=7.2, wall=49340
2022-03-04 23:03:44 | INFO | train_inner | epoch 054:    184 / 196 loss=3.176, ppl=9.04, wps=19103.6, ups=0.29, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.966, loss_scale=32, train_wall=318, gb_free=7.2, wall=49683
2022-03-04 23:04:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:04:30 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.186 | ppl 582.58 | wps 35087.7 | wpb 510.9 | bsz 1 | num_updates 10512 | best_loss 6.971
2022-03-04 23:04:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10512 updates
2022-03-04 23:04:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:04:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 54 @ 10512 updates, score 9.186) (writing took 4.919092133641243 seconds)
2022-03-04 23:04:35 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 23:04:35 | INFO | train | epoch 054 | loss 3.134 | ppl 8.78 | wps 19000.7 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 10512 | lr 0.000308431 | gnorm 0.962 | loss_scale 32 | train_wall 611 | gb_free 7.2 | wall 49735
2022-03-04 23:04:35 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 23:04:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:07:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:09:42 | INFO | train_inner | epoch 055:     89 / 196 loss=3.066, ppl=8.37, wps=18231.1, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.948, loss_scale=16, train_wall=321, gb_free=7.2, wall=50042
2022-03-04 23:15:27 | INFO | train_inner | epoch 055:    189 / 196 loss=3.141, ppl=8.82, wps=19018, ups=0.29, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.942, loss_scale=32, train_wall=319, gb_free=7.2, wall=50387
2022-03-04 23:15:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:15:57 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.24 | ppl 604.62 | wps 34186.9 | wpb 510.9 | bsz 1 | num_updates 10707 | best_loss 6.971
2022-03-04 23:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10707 updates
2022-03-04 23:15:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:16:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 55 @ 10707 updates, score 9.24) (writing took 4.865550423040986 seconds)
2022-03-04 23:16:01 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 23:16:01 | INFO | train | epoch 055 | loss 3.1 | ppl 8.57 | wps 18598.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 10707 | lr 0.000305609 | gnorm 0.947 | loss_scale 32 | train_wall 625 | gb_free 7.2 | wall 50421
2022-03-04 23:16:01 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 23:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:21:24 | INFO | train_inner | epoch 056:     93 / 196 loss=3.033, ppl=8.19, wps=18319.1, ups=0.28, wpb=65367, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.97, loss_scale=32, train_wall=320, gb_free=7.2, wall=50743
2022-03-04 23:22:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 23:27:11 | INFO | train_inner | epoch 056:    194 / 196 loss=3.113, ppl=8.65, wps=18855.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.959, loss_scale=32, train_wall=322, gb_free=7.2, wall=51091
2022-03-04 23:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:27:23 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.232 | ppl 601.12 | wps 35626.9 | wpb 510.9 | bsz 1 | num_updates 10902 | best_loss 6.971
2022-03-04 23:27:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10902 updates
2022-03-04 23:27:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:27:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:27:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 56 @ 10902 updates, score 9.232) (writing took 4.8494647443294525 seconds)
2022-03-04 23:27:28 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 23:27:28 | INFO | train | epoch 056 | loss 3.07 | ppl 8.4 | wps 18584.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 10902 | lr 0.000302863 | gnorm 0.963 | loss_scale 32 | train_wall 625 | gb_free 7.2 | wall 51108
2022-03-04 23:27:28 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 23:27:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:29:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 23:32:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:33:11 | INFO | train_inner | epoch 057:    100 / 196 loss=2.994, ppl=7.97, wps=18150.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.978, loss_scale=16, train_wall=323, gb_free=7.2, wall=51451
2022-03-04 23:38:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:38:46 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.336 | ppl 646.49 | wps 36039.2 | wpb 510.9 | bsz 1 | num_updates 11096 | best_loss 6.971
2022-03-04 23:38:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11096 updates
2022-03-04 23:38:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:38:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:38:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 57 @ 11096 updates, score 9.336) (writing took 4.7370519414544106 seconds)
2022-03-04 23:38:51 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 23:38:51 | INFO | train | epoch 057 | loss 3.038 | ppl 8.21 | wps 18602.2 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 11096 | lr 0.000300204 | gnorm 0.975 | loss_scale 16 | train_wall 622 | gb_free 7.2 | wall 51790
2022-03-04 23:38:51 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 23:38:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:39:05 | INFO | train_inner | epoch 058:      4 / 196 loss=3.08, ppl=8.46, wps=18501.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.972, loss_scale=16, train_wall=317, gb_free=7.2, wall=51804
2022-03-04 23:42:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:44:54 | INFO | train_inner | epoch 058:    105 / 196 loss=2.963, ppl=7.8, wps=18760.8, ups=0.29, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.963, loss_scale=16, train_wall=323, gb_free=7.2, wall=52154
2022-03-04 23:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:50:16 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.405 | ppl 677.72 | wps 35074.9 | wpb 510.9 | bsz 1 | num_updates 11291 | best_loss 6.971
2022-03-04 23:50:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11291 updates
2022-03-04 23:50:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:50:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-04 23:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 58 @ 11291 updates, score 9.405) (writing took 4.818485751748085 seconds)
2022-03-04 23:50:21 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 23:50:21 | INFO | train | epoch 058 | loss 3.01 | ppl 8.05 | wps 18498.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 11291 | lr 0.000297601 | gnorm 0.968 | loss_scale 32 | train_wall 628 | gb_free 7.2 | wall 52480
2022-03-04 23:50:21 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 23:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:50:52 | INFO | train_inner | epoch 059:      9 / 196 loss=3.048, ppl=8.27, wps=18267.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.974, loss_scale=32, train_wall=321, gb_free=7.2, wall=52511
2022-03-04 23:51:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:56:41 | INFO | train_inner | epoch 059:    110 / 196 loss=2.949, ppl=7.72, wps=18737.8, ups=0.29, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.972, loss_scale=16, train_wall=324, gb_free=7.2, wall=52861
2022-03-05 00:01:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:01:44 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.439 | ppl 693.92 | wps 34332.3 | wpb 510.9 | bsz 1 | num_updates 11486 | best_loss 6.971
2022-03-05 00:01:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11486 updates
2022-03-05 00:01:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:01:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:01:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 59 @ 11486 updates, score 9.439) (writing took 4.620147135108709 seconds)
2022-03-05 00:01:49 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-05 00:01:49 | INFO | train | epoch 059 | loss 2.98 | ppl 7.89 | wps 18541.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 11486 | lr 0.000295064 | gnorm 0.981 | loss_scale 32 | train_wall 627 | gb_free 7.2 | wall 53169
2022-03-05 00:01:49 | INFO | fairseq.trainer | begin training epoch 60
2022-03-05 00:01:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:02:41 | INFO | train_inner | epoch 060:     15 / 196 loss=3.008, ppl=8.04, wps=18170.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.987, loss_scale=16, train_wall=323, gb_free=7.2, wall=53221
2022-03-05 00:08:29 | INFO | train_inner | epoch 060:    115 / 196 loss=2.92, ppl=7.57, wps=18849.8, ups=0.29, wpb=65532.4, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.979, loss_scale=16, train_wall=322, gb_free=7.2, wall=53569
2022-03-05 00:12:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:13:18 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.457 | ppl 702.91 | wps 33572 | wpb 510.9 | bsz 1 | num_updates 11680 | best_loss 6.971
2022-03-05 00:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11680 updates
2022-03-05 00:13:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 60 @ 11680 updates, score 9.457) (writing took 5.052520399913192 seconds)
2022-03-05 00:13:23 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-05 00:13:23 | INFO | train | epoch 060 | loss 2.951 | ppl 7.73 | wps 18293.5 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 11680 | lr 0.000292603 | gnorm 0.978 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 53863
2022-03-05 00:13:23 | INFO | fairseq.trainer | begin training epoch 61
2022-03-05 00:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:14:33 | INFO | train_inner | epoch 061:     20 / 196 loss=2.965, ppl=7.81, wps=17942.6, ups=0.27, wpb=65367, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.99, loss_scale=16, train_wall=326, gb_free=7.2, wall=53933
2022-03-05 00:20:22 | INFO | train_inner | epoch 061:    120 / 196 loss=2.9, ppl=7.47, wps=18799.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.98, loss_scale=16, train_wall=322, gb_free=7.2, wall=54281
2022-03-05 00:22:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:24:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:24:52 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.481 | ppl 714.8 | wps 34777.6 | wpb 510.9 | bsz 1 | num_updates 11875 | best_loss 6.971
2022-03-05 00:24:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11875 updates
2022-03-05 00:24:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:24:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 61 @ 11875 updates, score 9.481) (writing took 4.653912363573909 seconds)
2022-03-05 00:24:56 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-05 00:24:56 | INFO | train | epoch 061 | loss 2.925 | ppl 7.59 | wps 18407.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 11875 | lr 0.000290191 | gnorm 0.989 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 54556
2022-03-05 00:24:56 | INFO | fairseq.trainer | begin training epoch 62
2022-03-05 00:24:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:26:23 | INFO | train_inner | epoch 062:     25 / 196 loss=2.941, ppl=7.68, wps=18080, ups=0.28, wpb=65367, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.979, loss_scale=16, train_wall=324, gb_free=7.2, wall=54643
2022-03-05 00:32:11 | INFO | train_inner | epoch 062:    125 / 196 loss=2.878, ppl=7.35, wps=18840.3, ups=0.29, wpb=65532.4, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.981, loss_scale=32, train_wall=322, gb_free=7.2, wall=54991
2022-03-05 00:32:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:36:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:36:24 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.546 | ppl 747.66 | wps 34127.8 | wpb 510.9 | bsz 1 | num_updates 12070 | best_loss 6.971
2022-03-05 00:36:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12070 updates
2022-03-05 00:36:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:36:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:36:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 62 @ 12070 updates, score 9.546) (writing took 4.918821765109897 seconds)
2022-03-05 00:36:29 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-05 00:36:29 | INFO | train | epoch 062 | loss 2.898 | ppl 7.45 | wps 18428.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12070 | lr 0.000287837 | gnorm 0.988 | loss_scale 16 | train_wall 630 | gb_free 7.2 | wall 55249
2022-03-05 00:36:29 | INFO | fairseq.trainer | begin training epoch 63
2022-03-05 00:36:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:38:14 | INFO | train_inner | epoch 063:     30 / 196 loss=2.906, ppl=7.49, wps=18020.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=1.004, loss_scale=16, train_wall=325, gb_free=7.2, wall=55354
2022-03-05 00:42:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:44:08 | INFO | train_inner | epoch 063:    131 / 196 loss=2.861, ppl=7.26, wps=18520.2, ups=0.28, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=1.006, loss_scale=16, train_wall=327, gb_free=7.2, wall=55707
2022-03-05 00:47:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:48:01 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.601 | ppl 776.49 | wps 34039.9 | wpb 510.9 | bsz 1 | num_updates 12265 | best_loss 6.971
2022-03-05 00:48:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12265 updates
2022-03-05 00:48:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:48:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:48:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 63 @ 12265 updates, score 9.601) (writing took 4.642664467915893 seconds)
2022-03-05 00:48:05 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-05 00:48:05 | INFO | train | epoch 063 | loss 2.872 | ppl 7.32 | wps 18323.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12265 | lr 0.00028554 | gnorm 1.002 | loss_scale 16 | train_wall 634 | gb_free 7.2 | wall 55945
2022-03-05 00:48:05 | INFO | fairseq.trainer | begin training epoch 64
2022-03-05 00:48:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:50:08 | INFO | train_inner | epoch 064:     35 / 196 loss=2.873, ppl=7.32, wps=18153.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=1.002, loss_scale=32, train_wall=323, gb_free=7.2, wall=56068
2022-03-05 00:52:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:56:00 | INFO | train_inner | epoch 064:    136 / 196 loss=2.838, ppl=7.15, wps=18618.8, ups=0.28, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.997, loss_scale=16, train_wall=326, gb_free=7.2, wall=56420
2022-03-05 00:59:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:59:34 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.625 | ppl 789.42 | wps 34024.6 | wpb 510.9 | bsz 1 | num_updates 12460 | best_loss 6.971
2022-03-05 00:59:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12460 updates
2022-03-05 00:59:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:59:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 00:59:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 64 @ 12460 updates, score 9.625) (writing took 4.858524868264794 seconds)
2022-03-05 00:59:39 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-05 00:59:39 | INFO | train | epoch 064 | loss 2.846 | ppl 7.19 | wps 18391.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12460 | lr 0.000283296 | gnorm 0.993 | loss_scale 16 | train_wall 631 | gb_free 7.2 | wall 56639
2022-03-05 00:59:39 | INFO | fairseq.trainer | begin training epoch 65
2022-03-05 00:59:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:01:59 | INFO | train_inner | epoch 065:     40 / 196 loss=2.838, ppl=7.15, wps=18206.5, ups=0.28, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.983, loss_scale=32, train_wall=322, gb_free=7.2, wall=56779
2022-03-05 01:02:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:07:51 | INFO | train_inner | epoch 065:    141 / 196 loss=2.822, ppl=7.07, wps=18577.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=12600, lr=0.000281718, gnorm=1, loss_scale=16, train_wall=326, gb_free=7.2, wall=57131
2022-03-05 01:10:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:11:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:11:10 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.689 | ppl 825.15 | wps 33545.7 | wpb 510.9 | bsz 1 | num_updates 12654 | best_loss 6.971
2022-03-05 01:11:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12654 updates
2022-03-05 01:11:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 65 @ 12654 updates, score 9.689) (writing took 4.848050499334931 seconds)
2022-03-05 01:11:15 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-05 01:11:15 | INFO | train | epoch 065 | loss 2.821 | ppl 7.07 | wps 18247.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 12654 | lr 0.000281116 | gnorm 0.994 | loss_scale 16 | train_wall 633 | gb_free 7.2 | wall 57335
2022-03-05 01:11:15 | INFO | fairseq.trainer | begin training epoch 66
2022-03-05 01:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:13:57 | INFO | train_inner | epoch 066:     46 / 196 loss=2.809, ppl=7.01, wps=17901.3, ups=0.27, wpb=65367, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=1.012, loss_scale=16, train_wall=327, gb_free=7.2, wall=57496
2022-03-05 01:19:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:19:46 | INFO | train_inner | epoch 066:    147 / 196 loss=2.798, ppl=6.95, wps=18739.8, ups=0.29, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=1.005, loss_scale=16, train_wall=323, gb_free=7.2, wall=57846
2022-03-05 01:22:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:22:38 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.734 | ppl 851.85 | wps 36203.4 | wpb 510.9 | bsz 1 | num_updates 12849 | best_loss 6.971
2022-03-05 01:22:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12849 updates
2022-03-05 01:22:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:22:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:22:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 66 @ 12849 updates, score 9.734) (writing took 4.874807184562087 seconds)
2022-03-05 01:22:43 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-05 01:22:43 | INFO | train | epoch 066 | loss 2.797 | ppl 6.95 | wps 18543.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 12849 | lr 0.000278975 | gnorm 1.01 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 58023
2022-03-05 01:22:43 | INFO | fairseq.trainer | begin training epoch 67
2022-03-05 01:22:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:25:37 | INFO | train_inner | epoch 067:     51 / 196 loss=2.785, ppl=6.89, wps=18624.5, ups=0.28, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.997, loss_scale=16, train_wall=315, gb_free=7.2, wall=58197
2022-03-05 01:27:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:31:21 | INFO | train_inner | epoch 067:    152 / 196 loss=2.778, ppl=6.86, wps=19045.6, ups=0.29, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=1.009, loss_scale=16, train_wall=319, gb_free=7.2, wall=58541
2022-03-05 01:33:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:33:57 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.9 | ppl 955.35 | wps 35498.1 | wpb 510.9 | bsz 1 | num_updates 13044 | best_loss 6.971
2022-03-05 01:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13044 updates
2022-03-05 01:33:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:34:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:34:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 67 @ 13044 updates, score 9.9) (writing took 4.844631703570485 seconds)
2022-03-05 01:34:02 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-05 01:34:02 | INFO | train | epoch 067 | loss 2.773 | ppl 6.84 | wps 18809.9 | ups 0.29 | wpb 65447.5 | bsz 127.8 | num_updates 13044 | lr 0.000276882 | gnorm 1.005 | loss_scale 16 | train_wall 618 | gb_free 7.2 | wall 58702
2022-03-05 01:34:02 | INFO | fairseq.trainer | begin training epoch 68
2022-03-05 01:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:37:15 | INFO | train_inner | epoch 068:     56 / 196 loss=2.749, ppl=6.72, wps=18500.9, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=1.011, loss_scale=32, train_wall=317, gb_free=7.2, wall=58894
2022-03-05 01:40:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:43:02 | INFO | train_inner | epoch 068:    157 / 196 loss=2.764, ppl=6.79, wps=18855.8, ups=0.29, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=1.012, loss_scale=16, train_wall=322, gb_free=7.2, wall=59242
2022-03-05 01:45:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:45:21 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.892 | ppl 950 | wps 35302 | wpb 510.9 | bsz 1 | num_updates 13239 | best_loss 6.971
2022-03-05 01:45:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13239 updates
2022-03-05 01:45:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:45:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:45:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 68 @ 13239 updates, score 9.892) (writing took 4.797754693776369 seconds)
2022-03-05 01:45:26 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-05 01:45:26 | INFO | train | epoch 068 | loss 2.752 | ppl 6.73 | wps 18651 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13239 | lr 0.000274835 | gnorm 1.015 | loss_scale 16 | train_wall 623 | gb_free 7.2 | wall 59386
2022-03-05 01:45:26 | INFO | fairseq.trainer | begin training epoch 69
2022-03-05 01:45:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:47:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:48:57 | INFO | train_inner | epoch 069:     62 / 196 loss=2.722, ppl=6.6, wps=18407.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=1.007, loss_scale=16, train_wall=318, gb_free=7.2, wall=59597
2022-03-05 01:54:41 | INFO | train_inner | epoch 069:    162 / 196 loss=2.744, ppl=6.7, wps=19061.1, ups=0.29, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=1.01, loss_scale=16, train_wall=318, gb_free=7.2, wall=59941
2022-03-05 01:54:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:56:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:56:44 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.915 | ppl 965.22 | wps 34560 | wpb 510.9 | bsz 1 | num_updates 13433 | best_loss 6.971
2022-03-05 01:56:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13433 updates
2022-03-05 01:56:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:56:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 01:56:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 69 @ 13433 updates, score 9.915) (writing took 5.010732606053352 seconds)
2022-03-05 01:56:49 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-05 01:56:49 | INFO | train | epoch 069 | loss 2.728 | ppl 6.62 | wps 18591.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 13433 | lr 0.000272843 | gnorm 1.011 | loss_scale 16 | train_wall 621 | gb_free 7.2 | wall 60069
2022-03-05 01:56:49 | INFO | fairseq.trainer | begin training epoch 70
2022-03-05 01:56:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:00:40 | INFO | train_inner | epoch 070:     67 / 196 loss=2.695, ppl=6.47, wps=18230.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=1.02, loss_scale=16, train_wall=321, gb_free=7.2, wall=60300
2022-03-05 02:03:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:06:30 | INFO | train_inner | epoch 070:    168 / 196 loss=2.731, ppl=6.64, wps=18728.1, ups=0.29, wpb=65532.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=1.023, loss_scale=16, train_wall=324, gb_free=7.2, wall=60649
2022-03-05 02:08:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:08:12 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.94 | ppl 982.13 | wps 34547 | wpb 510.9 | bsz 1 | num_updates 13628 | best_loss 6.971
2022-03-05 02:08:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13628 updates
2022-03-05 02:08:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:08:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:08:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 70 @ 13628 updates, score 9.94) (writing took 4.794645205140114 seconds)
2022-03-05 02:08:17 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-05 02:08:17 | INFO | train | epoch 070 | loss 2.706 | ppl 6.53 | wps 18543.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13628 | lr 0.000270884 | gnorm 1.015 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 60757
2022-03-05 02:08:17 | INFO | fairseq.trainer | begin training epoch 71
2022-03-05 02:08:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:12:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:12:30 | INFO | train_inner | epoch 071:     73 / 196 loss=2.662, ppl=6.33, wps=18135.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=1.017, loss_scale=16, train_wall=323, gb_free=7.2, wall=61010
2022-03-05 02:18:15 | INFO | train_inner | epoch 071:    173 / 196 loss=2.714, ppl=6.56, wps=19002.7, ups=0.29, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=1.034, loss_scale=16, train_wall=319, gb_free=7.2, wall=61355
2022-03-05 02:19:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:19:39 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.953 | ppl 991.27 | wps 35248.4 | wpb 510.9 | bsz 1 | num_updates 13823 | best_loss 6.971
2022-03-05 02:19:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13823 updates
2022-03-05 02:19:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:19:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:19:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 71 @ 13823 updates, score 9.953) (writing took 4.801809510216117 seconds)
2022-03-05 02:19:44 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-05 02:19:44 | INFO | train | epoch 071 | loss 2.684 | ppl 6.43 | wps 18572.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 13823 | lr 0.000268967 | gnorm 1.029 | loss_scale 32 | train_wall 625 | gb_free 7.2 | wall 61444
2022-03-05 02:19:44 | INFO | fairseq.trainer | begin training epoch 72
2022-03-05 02:19:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:21:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:24:14 | INFO | train_inner | epoch 072:     78 / 196 loss=2.643, ppl=6.25, wps=18227.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=1.014, loss_scale=16, train_wall=321, gb_free=7.2, wall=61713
2022-03-05 02:29:58 | INFO | train_inner | epoch 072:    178 / 196 loss=2.692, ppl=6.46, wps=19011, ups=0.29, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=1.019, loss_scale=32, train_wall=319, gb_free=7.2, wall=62058
2022-03-05 02:30:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:31:06 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.962 | ppl 997.25 | wps 35566.3 | wpb 510.9 | bsz 1 | num_updates 14018 | best_loss 6.971
2022-03-05 02:31:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14018 updates
2022-03-05 02:31:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:31:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 72 @ 14018 updates, score 9.962) (writing took 4.887235082685947 seconds)
2022-03-05 02:31:11 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-05 02:31:11 | INFO | train | epoch 072 | loss 2.664 | ppl 6.34 | wps 18596.9 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14018 | lr 0.00026709 | gnorm 1.013 | loss_scale 32 | train_wall 624 | gb_free 7.2 | wall 62130
2022-03-05 02:31:11 | INFO | fairseq.trainer | begin training epoch 73
2022-03-05 02:31:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:31:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:35:58 | INFO | train_inner | epoch 073:     83 / 196 loss=2.618, ppl=6.14, wps=18163, ups=0.28, wpb=65367, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=1.031, loss_scale=16, train_wall=322, gb_free=7.2, wall=62418
2022-03-05 02:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:41:49 | INFO | train_inner | epoch 073:    184 / 196 loss=2.678, ppl=6.4, wps=18698.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=14200, lr=0.000265372, gnorm=1.035, loss_scale=16, train_wall=324, gb_free=7.2, wall=62768
2022-03-05 02:42:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:42:35 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.1 | ppl 1097.42 | wps 35368.4 | wpb 510.9 | bsz 1 | num_updates 14212 | best_loss 6.971
2022-03-05 02:42:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14212 updates
2022-03-05 02:42:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:42:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 73 @ 14212 updates, score 10.1) (writing took 5.0875349920243025 seconds)
2022-03-05 02:42:40 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-05 02:42:40 | INFO | train | epoch 073 | loss 2.643 | ppl 6.25 | wps 18406 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 14212 | lr 0.00026526 | gnorm 1.035 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 62820
2022-03-05 02:42:40 | INFO | fairseq.trainer | begin training epoch 74
2022-03-05 02:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:47:44 | INFO | train_inner | epoch 074:     88 / 196 loss=2.594, ppl=6.04, wps=18388, ups=0.28, wpb=65367, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=1.024, loss_scale=32, train_wall=318, gb_free=7.2, wall=63124
2022-03-05 02:48:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:53:32 | INFO | train_inner | epoch 074:    189 / 196 loss=2.659, ppl=6.31, wps=18844.6, ups=0.29, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=1.028, loss_scale=16, train_wall=322, gb_free=7.2, wall=63472
2022-03-05 02:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:54:01 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.158 | ppl 1142.17 | wps 34969.4 | wpb 510.9 | bsz 1 | num_updates 14407 | best_loss 6.971
2022-03-05 02:54:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14407 updates
2022-03-05 02:54:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 02:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 74 @ 14407 updates, score 10.158) (writing took 5.0670008566230536 seconds)
2022-03-05 02:54:07 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-05 02:54:07 | INFO | train | epoch 074 | loss 2.623 | ppl 6.16 | wps 18599.1 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14407 | lr 0.000263459 | gnorm 1.024 | loss_scale 16 | train_wall 624 | gb_free 7.2 | wall 63506
2022-03-05 02:54:07 | INFO | fairseq.trainer | begin training epoch 75
2022-03-05 02:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:56:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:59:31 | INFO | train_inner | epoch 075:     94 / 196 loss=2.574, ppl=5.95, wps=18203.6, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=1.032, loss_scale=16, train_wall=322, gb_free=7.2, wall=63831
2022-03-05 03:03:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:05:20 | INFO | train_inner | epoch 075:    195 / 196 loss=2.64, ppl=6.23, wps=18749.7, ups=0.29, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=1.041, loss_scale=16, train_wall=323, gb_free=7.2, wall=64180
2022-03-05 03:05:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:05:29 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.117 | ppl 1110.46 | wps 34677.1 | wpb 510.9 | bsz 1 | num_updates 14601 | best_loss 6.971
2022-03-05 03:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14601 updates
2022-03-05 03:05:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 75 @ 14601 updates, score 10.117) (writing took 4.782281495630741 seconds)
2022-03-05 03:05:34 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-05 03:05:34 | INFO | train | epoch 075 | loss 2.605 | ppl 6.08 | wps 18464.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 14601 | lr 0.000261703 | gnorm 1.038 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 64194
2022-03-05 03:05:34 | INFO | fairseq.trainer | begin training epoch 76
2022-03-05 03:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:11:18 | INFO | train_inner | epoch 076:     99 / 196 loss=2.547, ppl=5.84, wps=18271.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=1.026, loss_scale=16, train_wall=320, gb_free=7.2, wall=64538
2022-03-05 03:11:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:16:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:17:00 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.18 | ppl 1159.69 | wps 35505.6 | wpb 510.9 | bsz 1 | num_updates 14796 | best_loss 6.971
2022-03-05 03:17:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14796 updates
2022-03-05 03:17:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:17:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:17:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 76 @ 14796 updates, score 10.18) (writing took 4.776703808456659 seconds)
2022-03-05 03:17:05 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-05 03:17:05 | INFO | train | epoch 076 | loss 2.585 | ppl 6 | wps 18485.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 14796 | lr 0.000259973 | gnorm 1.029 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 64884
2022-03-05 03:17:05 | INFO | fairseq.trainer | begin training epoch 77
2022-03-05 03:17:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:17:18 | INFO | train_inner | epoch 077:      4 / 196 loss=2.619, ppl=6.14, wps=18143.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.034, loss_scale=16, train_wall=323, gb_free=7.2, wall=64898
2022-03-05 03:20:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:23:06 | INFO | train_inner | epoch 077:    105 / 196 loss=2.526, ppl=5.76, wps=18854.7, ups=0.29, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=1.024, loss_scale=16, train_wall=322, gb_free=7.2, wall=65246
2022-03-05 03:27:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:28:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:28:25 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.239 | ppl 1208.69 | wps 34908 | wpb 510.9 | bsz 1 | num_updates 14990 | best_loss 6.971
2022-03-05 03:28:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14990 updates
2022-03-05 03:28:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:28:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 77 @ 14990 updates, score 10.239) (writing took 4.770104790106416 seconds)
2022-03-05 03:28:30 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-05 03:28:30 | INFO | train | epoch 077 | loss 2.567 | ppl 5.93 | wps 18520.8 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 14990 | lr 0.000258285 | gnorm 1.039 | loss_scale 16 | train_wall 624 | gb_free 7.2 | wall 65570
2022-03-05 03:28:30 | INFO | fairseq.trainer | begin training epoch 78
2022-03-05 03:28:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:29:05 | INFO | train_inner | epoch 078:     10 / 196 loss=2.602, ppl=6.07, wps=18223, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.05, loss_scale=16, train_wall=322, gb_free=7.2, wall=65605
2022-03-05 03:34:51 | INFO | train_inner | epoch 078:    110 / 196 loss=2.517, ppl=5.72, wps=18935.5, ups=0.29, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=1.016, loss_scale=16, train_wall=320, gb_free=7.2, wall=65951
2022-03-05 03:35:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:39:55 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.317 | ppl 1275.22 | wps 34074.9 | wpb 510.9 | bsz 1 | num_updates 15185 | best_loss 6.971
2022-03-05 03:39:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15185 updates
2022-03-05 03:39:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:40:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:40:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 78 @ 15185 updates, score 10.317) (writing took 4.907801616936922 seconds)
2022-03-05 03:40:00 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-05 03:40:00 | INFO | train | epoch 078 | loss 2.548 | ppl 5.85 | wps 18501.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15185 | lr 0.000256621 | gnorm 1.027 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 66260
2022-03-05 03:40:00 | INFO | fairseq.trainer | begin training epoch 79
2022-03-05 03:40:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:40:52 | INFO | train_inner | epoch 079:     15 / 196 loss=2.574, ppl=5.95, wps=18088.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.042, loss_scale=16, train_wall=324, gb_free=7.2, wall=66312
2022-03-05 03:43:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:46:40 | INFO | train_inner | epoch 079:    116 / 196 loss=2.509, ppl=5.69, wps=18859.1, ups=0.29, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.029, loss_scale=16, train_wall=322, gb_free=7.2, wall=66660
2022-03-05 03:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:51:20 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.349 | ppl 1304.59 | wps 35260.7 | wpb 510.9 | bsz 1 | num_updates 15380 | best_loss 6.971
2022-03-05 03:51:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15380 updates
2022-03-05 03:51:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:51:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 03:51:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 79 @ 15380 updates, score 10.349) (writing took 4.781504403799772 seconds)
2022-03-05 03:51:25 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-05 03:51:25 | INFO | train | epoch 079 | loss 2.531 | ppl 5.78 | wps 18625.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15380 | lr 0.000254989 | gnorm 1.027 | loss_scale 32 | train_wall 624 | gb_free 7.2 | wall 66945
2022-03-05 03:51:25 | INFO | fairseq.trainer | begin training epoch 80
2022-03-05 03:51:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:51:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:52:37 | INFO | train_inner | epoch 080:     21 / 196 loss=2.548, ppl=5.85, wps=18272, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.027, loss_scale=16, train_wall=321, gb_free=7.2, wall=67017
2022-03-05 03:58:22 | INFO | train_inner | epoch 080:    121 / 196 loss=2.498, ppl=5.65, wps=18994.6, ups=0.29, wpb=65532.4, bsz=128, num_updates=15500, lr=0.000254, gnorm=1.033, loss_scale=16, train_wall=319, gb_free=7.2, wall=67362
2022-03-05 03:59:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:02:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:02:47 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.313 | ppl 1272.28 | wps 34917.4 | wpb 510.9 | bsz 1 | num_updates 15574 | best_loss 6.971
2022-03-05 04:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15574 updates
2022-03-05 04:02:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:02:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 80 @ 15574 updates, score 10.313) (writing took 4.806292133405805 seconds)
2022-03-05 04:02:52 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-05 04:02:52 | INFO | train | epoch 080 | loss 2.514 | ppl 5.71 | wps 18495.6 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 15574 | lr 0.000253396 | gnorm 1.042 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 67631
2022-03-05 04:02:52 | INFO | fairseq.trainer | begin training epoch 81
2022-03-05 04:02:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:04:22 | INFO | train_inner | epoch 081:     26 / 196 loss=2.519, ppl=5.73, wps=18181.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.056, loss_scale=16, train_wall=322, gb_free=7.2, wall=67722
2022-03-05 04:07:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:10:13 | INFO | train_inner | epoch 081:    127 / 196 loss=2.479, ppl=5.57, wps=18683.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=15700, lr=0.000252377, gnorm=1.049, loss_scale=16, train_wall=324, gb_free=7.2, wall=68073
2022-03-05 04:14:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:14:16 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.38 | ppl 1332.57 | wps 35628.8 | wpb 510.9 | bsz 1 | num_updates 15769 | best_loss 6.971
2022-03-05 04:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15769 updates
2022-03-05 04:14:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:14:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 81 @ 15769 updates, score 10.38) (writing took 4.797074940055609 seconds)
2022-03-05 04:14:21 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-05 04:14:21 | INFO | train | epoch 081 | loss 2.496 | ppl 5.64 | wps 18510.5 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 15769 | lr 0.000251824 | gnorm 1.051 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 68321
2022-03-05 04:14:21 | INFO | fairseq.trainer | begin training epoch 82
2022-03-05 04:14:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:15:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:16:11 | INFO | train_inner | epoch 082:     32 / 196 loss=2.506, ppl=5.68, wps=18239, ups=0.28, wpb=65367, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.047, loss_scale=16, train_wall=321, gb_free=7.2, wall=68431
2022-03-05 04:21:55 | INFO | train_inner | epoch 082:    132 / 196 loss=2.464, ppl=5.52, wps=19035.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.07, loss_scale=16, train_wall=318, gb_free=7.2, wall=68775
2022-03-05 04:23:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:25:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:25:41 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.411 | ppl 1361.52 | wps 35144.9 | wpb 510.9 | bsz 1 | num_updates 15963 | best_loss 6.971
2022-03-05 04:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15963 updates
2022-03-05 04:25:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:25:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:25:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 82 @ 15963 updates, score 10.411) (writing took 4.772233044728637 seconds)
2022-03-05 04:25:46 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-05 04:25:46 | INFO | train | epoch 082 | loss 2.479 | ppl 5.57 | wps 18528.9 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 15963 | lr 0.00025029 | gnorm 1.057 | loss_scale 16 | train_wall 624 | gb_free 7.2 | wall 69006
2022-03-05 04:25:46 | INFO | fairseq.trainer | begin training epoch 83
2022-03-05 04:25:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:27:54 | INFO | train_inner | epoch 083:     37 / 196 loss=2.482, ppl=5.59, wps=18206.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.034, loss_scale=16, train_wall=322, gb_free=7.2, wall=69134
2022-03-05 04:30:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:33:43 | INFO | train_inner | epoch 083:    138 / 196 loss=2.463, ppl=5.52, wps=18800.9, ups=0.29, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.053, loss_scale=16, train_wall=322, gb_free=7.2, wall=69483
2022-03-05 04:37:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:37:09 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.392 | ppl 1343.74 | wps 34608 | wpb 510.9 | bsz 1 | num_updates 16158 | best_loss 6.971
2022-03-05 04:37:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16158 updates
2022-03-05 04:37:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:37:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:37:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 83 @ 16158 updates, score 10.392) (writing took 4.82991099357605 seconds)
2022-03-05 04:37:14 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-05 04:37:14 | INFO | train | epoch 083 | loss 2.463 | ppl 5.51 | wps 18552.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16158 | lr 0.000248775 | gnorm 1.052 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 69694
2022-03-05 04:37:14 | INFO | fairseq.trainer | begin training epoch 84
2022-03-05 04:37:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:38:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:39:44 | INFO | train_inner | epoch 084:     43 / 196 loss=2.455, ppl=5.48, wps=18125.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.059, loss_scale=16, train_wall=323, gb_free=7.2, wall=69843
2022-03-05 04:45:29 | INFO | train_inner | epoch 084:    143 / 196 loss=2.443, ppl=5.44, wps=18949.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.063, loss_scale=16, train_wall=320, gb_free=7.2, wall=70189
2022-03-05 04:46:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:48:37 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.467 | ppl 1415.4 | wps 35628.3 | wpb 510.9 | bsz 1 | num_updates 16352 | best_loss 6.971
2022-03-05 04:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16352 updates
2022-03-05 04:48:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:48:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 04:48:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 84 @ 16352 updates, score 10.467) (writing took 4.855482863262296 seconds)
2022-03-05 04:48:41 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-05 04:48:41 | INFO | train | epoch 084 | loss 2.447 | ppl 5.45 | wps 18473.5 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 16352 | lr 0.000247295 | gnorm 1.054 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 70381
2022-03-05 04:48:42 | INFO | fairseq.trainer | begin training epoch 85
2022-03-05 04:48:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:51:26 | INFO | train_inner | epoch 085:     48 / 196 loss=2.44, ppl=5.43, wps=18314.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.053, loss_scale=16, train_wall=320, gb_free=7.2, wall=70546
2022-03-05 04:54:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:57:14 | INFO | train_inner | epoch 085:    149 / 196 loss=2.434, ppl=5.4, wps=18842.4, ups=0.29, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.05, loss_scale=16, train_wall=322, gb_free=7.2, wall=70894
2022-03-05 04:59:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:00:02 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.542 | ppl 1490.62 | wps 34756.1 | wpb 510.9 | bsz 1 | num_updates 16547 | best_loss 6.971
2022-03-05 05:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16547 updates
2022-03-05 05:00:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:00:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:00:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 85 @ 16547 updates, score 10.542) (writing took 4.770632442086935 seconds)
2022-03-05 05:00:07 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-05 05:00:07 | INFO | train | epoch 085 | loss 2.431 | ppl 5.39 | wps 18626.3 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16547 | lr 0.000245833 | gnorm 1.056 | loss_scale 16 | train_wall 624 | gb_free 7.2 | wall 71066
2022-03-05 05:00:07 | INFO | fairseq.trainer | begin training epoch 86
2022-03-05 05:00:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:03:10 | INFO | train_inner | epoch 086:     53 / 196 loss=2.413, ppl=5.33, wps=18375.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.044, loss_scale=32, train_wall=319, gb_free=7.2, wall=71250
2022-03-05 05:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:09:00 | INFO | train_inner | epoch 086:    154 / 196 loss=2.426, ppl=5.37, wps=18739.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.067, loss_scale=16, train_wall=324, gb_free=7.2, wall=71599
2022-03-05 05:10:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:11:31 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.532 | ppl 1480.9 | wps 34339.2 | wpb 510.9 | bsz 1 | num_updates 16741 | best_loss 6.971
2022-03-05 05:11:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16741 updates
2022-03-05 05:11:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:11:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 86 @ 16741 updates, score 10.532) (writing took 4.81585280969739 seconds)
2022-03-05 05:11:36 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-05 05:11:36 | INFO | train | epoch 086 | loss 2.416 | ppl 5.34 | wps 18424.2 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 16741 | lr 0.000244405 | gnorm 1.054 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 71756
2022-03-05 05:11:36 | INFO | fairseq.trainer | begin training epoch 87
2022-03-05 05:11:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:15:01 | INFO | train_inner | epoch 087:     59 / 196 loss=2.399, ppl=5.27, wps=18073.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.057, loss_scale=16, train_wall=324, gb_free=7.2, wall=71961
2022-03-05 05:18:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:20:52 | INFO | train_inner | epoch 087:    160 / 196 loss=2.415, ppl=5.33, wps=18698.5, ups=0.29, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.06, loss_scale=16, train_wall=324, gb_free=7.2, wall=72312
2022-03-05 05:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:23:01 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.571 | ppl 1521.55 | wps 34994 | wpb 510.9 | bsz 1 | num_updates 16936 | best_loss 6.971
2022-03-05 05:23:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16936 updates
2022-03-05 05:23:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:23:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 87 @ 16936 updates, score 10.571) (writing took 4.6568807400763035 seconds)
2022-03-05 05:23:06 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-05 05:23:06 | INFO | train | epoch 087 | loss 2.4 | ppl 5.28 | wps 18491.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 16936 | lr 0.000242993 | gnorm 1.063 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 72446
2022-03-05 05:23:06 | INFO | fairseq.trainer | begin training epoch 88
2022-03-05 05:23:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:26:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:26:50 | INFO | train_inner | epoch 088:     65 / 196 loss=2.373, ppl=5.18, wps=18232, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.072, loss_scale=16, train_wall=321, gb_free=7.2, wall=72670
2022-03-05 05:32:35 | INFO | train_inner | epoch 088:    165 / 196 loss=2.399, ppl=5.28, wps=18996.1, ups=0.29, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.071, loss_scale=16, train_wall=319, gb_free=7.2, wall=73015
2022-03-05 05:33:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:34:28 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.665 | ppl 1623.8 | wps 34096.5 | wpb 510.9 | bsz 1 | num_updates 17130 | best_loss 6.971
2022-03-05 05:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17130 updates
2022-03-05 05:34:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:34:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:34:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 88 @ 17130 updates, score 10.665) (writing took 4.651010852307081 seconds)
2022-03-05 05:34:32 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-05 05:34:32 | INFO | train | epoch 088 | loss 2.385 | ppl 5.22 | wps 18499.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 17130 | lr 0.000241614 | gnorm 1.071 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 73132
2022-03-05 05:34:32 | INFO | fairseq.trainer | begin training epoch 89
2022-03-05 05:34:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:38:36 | INFO | train_inner | epoch 089:     70 / 196 loss=2.358, ppl=5.13, wps=18140.8, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.075, loss_scale=16, train_wall=323, gb_free=7.2, wall=73375
2022-03-05 05:41:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:44:27 | INFO | train_inner | epoch 089:    171 / 196 loss=2.392, ppl=5.25, wps=18673.1, ups=0.28, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.055, loss_scale=16, train_wall=325, gb_free=7.2, wall=73726
2022-03-05 05:45:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:45:58 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.615 | ppl 1567.88 | wps 34814.3 | wpb 510.9 | bsz 1 | num_updates 17325 | best_loss 6.971
2022-03-05 05:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17325 updates
2022-03-05 05:45:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:46:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 89 @ 17325 updates, score 10.615) (writing took 4.851269336417317 seconds)
2022-03-05 05:46:03 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-05 05:46:03 | INFO | train | epoch 089 | loss 2.369 | ppl 5.17 | wps 18483.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17325 | lr 0.00024025 | gnorm 1.061 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 73823
2022-03-05 05:46:03 | INFO | fairseq.trainer | begin training epoch 90
2022-03-05 05:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:49:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:50:22 | INFO | train_inner | epoch 090:     76 / 196 loss=2.336, ppl=5.05, wps=18379.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.059, loss_scale=16, train_wall=319, gb_free=7.2, wall=74082
2022-03-05 05:56:07 | INFO | train_inner | epoch 090:    176 / 196 loss=2.388, ppl=5.23, wps=18995.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.094, loss_scale=16, train_wall=319, gb_free=7.2, wall=74427
2022-03-05 05:56:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:57:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:57:22 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.718 | ppl 1684.75 | wps 34594.8 | wpb 510.9 | bsz 1 | num_updates 17519 | best_loss 6.971
2022-03-05 05:57:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17519 updates
2022-03-05 05:57:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:57:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 05:57:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 90 @ 17519 updates, score 10.718) (writing took 4.812805188819766 seconds)
2022-03-05 05:57:26 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-05 05:57:26 | INFO | train | epoch 090 | loss 2.357 | ppl 5.12 | wps 18570.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 17519 | lr 0.000238916 | gnorm 1.08 | loss_scale 16 | train_wall 622 | gb_free 7.2 | wall 74506
2022-03-05 05:57:27 | INFO | fairseq.trainer | begin training epoch 91
2022-03-05 05:57:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:02:06 | INFO | train_inner | epoch 091:     81 / 196 loss=2.315, ppl=4.98, wps=18226.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.058, loss_scale=16, train_wall=321, gb_free=7.2, wall=74786
2022-03-05 06:04:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:07:55 | INFO | train_inner | epoch 091:    182 / 196 loss=2.374, ppl=5.18, wps=18790.2, ups=0.29, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.07, loss_scale=16, train_wall=323, gb_free=7.2, wall=75134
2022-03-05 06:08:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:08:49 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.702 | ppl 1665.77 | wps 34611.6 | wpb 510.9 | bsz 1 | num_updates 17714 | best_loss 6.971
2022-03-05 06:08:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17714 updates
2022-03-05 06:08:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:08:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 91 @ 17714 updates, score 10.702) (writing took 5.016169063746929 seconds)
2022-03-05 06:08:54 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-05 06:08:54 | INFO | train | epoch 091 | loss 2.342 | ppl 5.07 | wps 18574.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17714 | lr 0.000237597 | gnorm 1.06 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 75193
2022-03-05 06:08:54 | INFO | fairseq.trainer | begin training epoch 92
2022-03-05 06:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:12:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:13:56 | INFO | train_inner | epoch 092:     87 / 196 loss=2.3, ppl=4.93, wps=18107.4, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.061, loss_scale=16, train_wall=323, gb_free=7.2, wall=75495
2022-03-05 06:19:40 | INFO | train_inner | epoch 092:    187 / 196 loss=2.36, ppl=5.13, wps=19003.7, ups=0.29, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.069, loss_scale=16, train_wall=319, gb_free=7.2, wall=75840
2022-03-05 06:20:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:20:17 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.719 | ppl 1685.71 | wps 34931.3 | wpb 510.9 | bsz 1 | num_updates 17909 | best_loss 6.971
2022-03-05 06:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17909 updates
2022-03-05 06:20:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:20:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:20:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 92 @ 17909 updates, score 10.719) (writing took 4.740124128758907 seconds)
2022-03-05 06:20:22 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-05 06:20:22 | INFO | train | epoch 092 | loss 2.328 | ppl 5.02 | wps 18547.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 17909 | lr 0.0002363 | gnorm 1.067 | loss_scale 32 | train_wall 626 | gb_free 7.2 | wall 75881
2022-03-05 06:20:22 | INFO | fairseq.trainer | begin training epoch 93
2022-03-05 06:20:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:20:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:25:39 | INFO | train_inner | epoch 093:     92 / 196 loss=2.283, ppl=4.87, wps=18227.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.055, loss_scale=16, train_wall=321, gb_free=7.2, wall=76199
2022-03-05 06:28:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:31:28 | INFO | train_inner | epoch 093:    193 / 196 loss=2.353, ppl=5.11, wps=18803.2, ups=0.29, wpb=65532.4, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.08, loss_scale=16, train_wall=322, gb_free=7.2, wall=76547
2022-03-05 06:31:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:31:43 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.824 | ppl 1812.98 | wps 35272.4 | wpb 510.9 | bsz 1 | num_updates 18103 | best_loss 6.971
2022-03-05 06:31:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18103 updates
2022-03-05 06:31:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:31:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 93 @ 18103 updates, score 10.824) (writing took 5.138804627582431 seconds)
2022-03-05 06:31:48 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-05 06:31:48 | INFO | train | epoch 093 | loss 2.316 | ppl 4.98 | wps 18487.6 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 18103 | lr 0.000235031 | gnorm 1.067 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 76568
2022-03-05 06:31:48 | INFO | fairseq.trainer | begin training epoch 94
2022-03-05 06:31:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:36:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:37:27 | INFO | train_inner | epoch 094:     98 / 196 loss=2.264, ppl=4.8, wps=18192.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.071, loss_scale=16, train_wall=322, gb_free=7.2, wall=76907
2022-03-05 06:43:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:43:12 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.865 | ppl 1864.6 | wps 34410 | wpb 510.9 | bsz 1 | num_updates 18298 | best_loss 6.971
2022-03-05 06:43:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18298 updates
2022-03-05 06:43:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:43:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:43:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 94 @ 18298 updates, score 10.865) (writing took 4.866680709645152 seconds)
2022-03-05 06:43:17 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-05 06:43:17 | INFO | train | epoch 094 | loss 2.301 | ppl 4.93 | wps 18535.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18298 | lr 0.000233775 | gnorm 1.072 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 77257
2022-03-05 06:43:17 | INFO | fairseq.trainer | begin training epoch 95
2022-03-05 06:43:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:43:24 | INFO | train_inner | epoch 095:      2 / 196 loss=2.34, ppl=5.06, wps=18299.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=18300, lr=0.000233762, gnorm=1.076, loss_scale=16, train_wall=320, gb_free=7.2, wall=77264
2022-03-05 06:44:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:49:13 | INFO | train_inner | epoch 095:    103 / 196 loss=2.253, ppl=4.77, wps=18764.3, ups=0.29, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=1.075, loss_scale=16, train_wall=323, gb_free=7.2, wall=77613
2022-03-05 06:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:54:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:54:38 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.872 | ppl 1873.85 | wps 34987.7 | wpb 510.9 | bsz 1 | num_updates 18492 | best_loss 6.971
2022-03-05 06:54:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18492 updates
2022-03-05 06:54:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 06:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 95 @ 18492 updates, score 10.872) (writing took 4.892081985250115 seconds)
2022-03-05 06:54:43 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-05 06:54:43 | INFO | train | epoch 095 | loss 2.288 | ppl 4.89 | wps 18519.7 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 18492 | lr 0.000232546 | gnorm 1.086 | loss_scale 16 | train_wall 624 | gb_free 7.2 | wall 77942
2022-03-05 06:54:43 | INFO | fairseq.trainer | begin training epoch 96
2022-03-05 06:54:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:55:10 | INFO | train_inner | epoch 096:      8 / 196 loss=2.318, ppl=4.99, wps=18310.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.094, loss_scale=16, train_wall=320, gb_free=7.2, wall=77970
2022-03-05 07:00:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:00:59 | INFO | train_inner | epoch 096:    109 / 196 loss=2.246, ppl=4.75, wps=18797.2, ups=0.29, wpb=65532.4, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.075, loss_scale=16, train_wall=323, gb_free=7.2, wall=78319
2022-03-05 07:05:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:06:05 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.87 | ppl 1871.32 | wps 34632.5 | wpb 510.9 | bsz 1 | num_updates 18687 | best_loss 6.971
2022-03-05 07:06:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18687 updates
2022-03-05 07:06:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:06:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 96 @ 18687 updates, score 10.87) (writing took 4.833913799375296 seconds)
2022-03-05 07:06:10 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-05 07:06:10 | INFO | train | epoch 096 | loss 2.276 | ppl 4.84 | wps 18574 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 18687 | lr 0.000231329 | gnorm 1.081 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 78629
2022-03-05 07:06:10 | INFO | fairseq.trainer | begin training epoch 97
2022-03-05 07:06:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:06:55 | INFO | train_inner | epoch 097:     13 / 196 loss=2.301, ppl=4.93, wps=18372.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.087, loss_scale=16, train_wall=319, gb_free=7.2, wall=78674
2022-03-05 07:08:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:12:45 | INFO | train_inner | epoch 097:    114 / 196 loss=2.239, ppl=4.72, wps=18698.5, ups=0.29, wpb=65532.4, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.08, loss_scale=16, train_wall=324, gb_free=7.2, wall=79025
2022-03-05 07:15:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:17:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:17:35 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.928 | ppl 1947.82 | wps 34291 | wpb 510.9 | bsz 1 | num_updates 18881 | best_loss 6.971
2022-03-05 07:17:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18881 updates
2022-03-05 07:17:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 97 @ 18881 updates, score 10.928) (writing took 4.895351136103272 seconds)
2022-03-05 07:17:40 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-05 07:17:40 | INFO | train | epoch 097 | loss 2.262 | ppl 4.8 | wps 18385.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 18881 | lr 0.000230138 | gnorm 1.081 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 79320
2022-03-05 07:17:40 | INFO | fairseq.trainer | begin training epoch 98
2022-03-05 07:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:18:46 | INFO | train_inner | epoch 098:     19 / 196 loss=2.277, ppl=4.85, wps=18108.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.084, loss_scale=16, train_wall=323, gb_free=7.2, wall=79386
2022-03-05 07:23:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:24:33 | INFO | train_inner | epoch 098:    120 / 196 loss=2.229, ppl=4.69, wps=18883.6, ups=0.29, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.073, loss_scale=16, train_wall=321, gb_free=7.2, wall=79733
2022-03-05 07:28:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:29:01 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.958 | ppl 1989.52 | wps 35086 | wpb 510.9 | bsz 1 | num_updates 19076 | best_loss 6.971
2022-03-05 07:29:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19076 updates
2022-03-05 07:29:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:29:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:29:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 98 @ 19076 updates, score 10.958) (writing took 4.859540270641446 seconds)
2022-03-05 07:29:05 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-05 07:29:05 | INFO | train | epoch 098 | loss 2.25 | ppl 4.76 | wps 18624.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19076 | lr 0.000228958 | gnorm 1.082 | loss_scale 16 | train_wall 623 | gb_free 7.2 | wall 80005
2022-03-05 07:29:06 | INFO | fairseq.trainer | begin training epoch 99
2022-03-05 07:29:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:30:28 | INFO | train_inner | epoch 099:     24 / 196 loss=2.265, ppl=4.81, wps=18400.4, ups=0.28, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.09, loss_scale=16, train_wall=318, gb_free=7.2, wall=80088
2022-03-05 07:30:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:36:17 | INFO | train_inner | epoch 099:    125 / 196 loss=2.22, ppl=4.66, wps=18824.4, ups=0.29, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.092, loss_scale=16, train_wall=322, gb_free=7.2, wall=80436
2022-03-05 07:38:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:40:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:40:27 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.99 | ppl 2033.48 | wps 34493 | wpb 510.9 | bsz 1 | num_updates 19270 | best_loss 6.971
2022-03-05 07:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19270 updates
2022-03-05 07:40:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:40:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:40:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 99 @ 19270 updates, score 10.99) (writing took 4.687191361561418 seconds)
2022-03-05 07:40:32 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-05 07:40:32 | INFO | train | epoch 099 | loss 2.237 | ppl 4.71 | wps 18493.7 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 19270 | lr 0.000227803 | gnorm 1.092 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 80692
2022-03-05 07:40:32 | INFO | fairseq.trainer | begin training epoch 100
2022-03-05 07:40:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:42:17 | INFO | train_inner | epoch 100:     30 / 196 loss=2.249, ppl=4.75, wps=18156.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.092, loss_scale=16, train_wall=323, gb_free=7.2, wall=80796
2022-03-05 07:46:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:48:08 | INFO | train_inner | epoch 100:    131 / 196 loss=2.216, ppl=4.65, wps=18635.3, ups=0.28, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.086, loss_scale=16, train_wall=325, gb_free=7.2, wall=81148
2022-03-05 07:51:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:51:58 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 11.045 | ppl 2112.26 | wps 35000.6 | wpb 510.9 | bsz 1 | num_updates 19465 | best_loss 6.971
2022-03-05 07:51:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19465 updates
2022-03-05 07:51:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:52:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 07:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 100 @ 19465 updates, score 11.045) (writing took 4.723878549411893 seconds)
2022-03-05 07:52:03 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-05 07:52:03 | INFO | train | epoch 100 | loss 2.226 | ppl 4.68 | wps 18478.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19465 | lr 0.000226659 | gnorm 1.085 | loss_scale 16 | train_wall 629 | gb_free 7.2 | wall 81382
2022-03-05 07:52:03 | INFO | fairseq.trainer | begin training epoch 101
2022-03-05 07:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:53:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:54:07 | INFO | train_inner | epoch 101:     36 / 196 loss=2.23, ppl=4.69, wps=18225.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.086, loss_scale=16, train_wall=322, gb_free=7.2, wall=81507
2022-03-05 07:59:52 | INFO | train_inner | epoch 101:    136 / 196 loss=2.208, ppl=4.62, wps=18971.4, ups=0.29, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.096, loss_scale=16, train_wall=320, gb_free=7.2, wall=81852
2022-03-05 08:01:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:03:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:03:25 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 11.038 | ppl 2103.18 | wps 35128.7 | wpb 510.9 | bsz 1 | num_updates 19659 | best_loss 6.971
2022-03-05 08:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19659 updates
2022-03-05 08:03:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:03:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:03:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 101 @ 19659 updates, score 11.038) (writing took 5.100325433537364 seconds)
2022-03-05 08:03:30 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-05 08:03:30 | INFO | train | epoch 101 | loss 2.214 | ppl 4.64 | wps 18470.8 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 19659 | lr 0.000225538 | gnorm 1.09 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 82070
2022-03-05 08:03:30 | INFO | fairseq.trainer | begin training epoch 102
2022-03-05 08:03:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:05:52 | INFO | train_inner | epoch 102:     41 / 196 loss=2.209, ppl=4.62, wps=18186.5, ups=0.28, wpb=65359.9, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.084, loss_scale=16, train_wall=322, gb_free=7.2, wall=82212
2022-03-05 08:09:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:11:42 | INFO | train_inner | epoch 102:    142 / 196 loss=2.204, ppl=4.61, wps=18733.8, ups=0.29, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.084, loss_scale=16, train_wall=324, gb_free=7.2, wall=82561
2022-03-05 08:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:14:55 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 11.075 | ppl 2156.86 | wps 34534.2 | wpb 510.9 | bsz 1 | num_updates 19854 | best_loss 6.971
2022-03-05 08:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19854 updates
2022-03-05 08:14:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:15:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 102 @ 19854 updates, score 11.075) (writing took 4.9646442495286465 seconds)
2022-03-05 08:15:00 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-05 08:15:00 | INFO | train | epoch 102 | loss 2.204 | ppl 4.61 | wps 18499.2 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 19854 | lr 0.000224427 | gnorm 1.087 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 82760
2022-03-05 08:15:00 | INFO | fairseq.trainer | begin training epoch 103
2022-03-05 08:15:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:16:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:17:43 | INFO | train_inner | epoch 103:     47 / 196 loss=2.197, ppl=4.59, wps=18067.5, ups=0.28, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.093, loss_scale=16, train_wall=324, gb_free=7.2, wall=82923
2022-03-05 08:23:28 | INFO | train_inner | epoch 103:    147 / 196 loss=2.195, ppl=4.58, wps=18986.5, ups=0.29, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.096, loss_scale=16, train_wall=319, gb_free=7.2, wall=83268
2022-03-05 08:24:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:26:22 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 11.104 | ppl 2201.52 | wps 35947.5 | wpb 510.9 | bsz 1 | num_updates 20048 | best_loss 6.971
2022-03-05 08:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20048 updates
2022-03-05 08:26:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:26:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:26:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 103 @ 20048 updates, score 11.104) (writing took 4.749939039349556 seconds)
2022-03-05 08:26:27 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-05 08:26:27 | INFO | train | epoch 103 | loss 2.191 | ppl 4.56 | wps 18477.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 20048 | lr 0.000223339 | gnorm 1.097 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 83447
2022-03-05 08:26:27 | INFO | fairseq.trainer | begin training epoch 104
2022-03-05 08:26:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:29:27 | INFO | train_inner | epoch 104:     52 / 196 loss=2.179, ppl=4.53, wps=18230.5, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.091, loss_scale=16, train_wall=322, gb_free=7.2, wall=83627
2022-03-05 08:32:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:35:17 | INFO | train_inner | epoch 104:    153 / 196 loss=2.183, ppl=4.54, wps=18740.3, ups=0.29, wpb=65532.4, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.098, loss_scale=16, train_wall=323, gb_free=7.2, wall=83977
2022-03-05 08:37:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:37:51 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 11.149 | ppl 2270.49 | wps 35253.2 | wpb 510.9 | bsz 1 | num_updates 20243 | best_loss 6.971
2022-03-05 08:37:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20243 updates
2022-03-05 08:37:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:37:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:37:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 104 @ 20243 updates, score 11.149) (writing took 4.858220610767603 seconds)
2022-03-05 08:37:56 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-05 08:37:56 | INFO | train | epoch 104 | loss 2.18 | ppl 4.53 | wps 18531 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20243 | lr 0.000222261 | gnorm 1.092 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 84136
2022-03-05 08:37:56 | INFO | fairseq.trainer | begin training epoch 105
2022-03-05 08:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:39:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:41:16 | INFO | train_inner | epoch 105:     58 / 196 loss=2.169, ppl=4.5, wps=18192, ups=0.28, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.087, loss_scale=16, train_wall=322, gb_free=7.2, wall=84336
2022-03-05 08:47:03 | INFO | train_inner | epoch 105:    158 / 196 loss=2.18, ppl=4.53, wps=18871.4, ups=0.29, wpb=65532.4, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.088, loss_scale=16, train_wall=321, gb_free=7.2, wall=84683
2022-03-05 08:47:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:49:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:49:21 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 11.146 | ppl 2266.18 | wps 34531.5 | wpb 510.9 | bsz 1 | num_updates 20437 | best_loss 6.971
2022-03-05 08:49:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20437 updates
2022-03-05 08:49:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 08:49:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 105 @ 20437 updates, score 11.146) (writing took 4.827279316261411 seconds)
2022-03-05 08:49:25 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-05 08:49:25 | INFO | train | epoch 105 | loss 2.169 | ppl 4.5 | wps 18409.5 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 20437 | lr 0.000221203 | gnorm 1.09 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 84825
2022-03-05 08:49:26 | INFO | fairseq.trainer | begin training epoch 106
2022-03-05 08:49:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:53:03 | INFO | train_inner | epoch 106:     63 / 196 loss=2.145, ppl=4.42, wps=18167.2, ups=0.28, wpb=65367, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.089, loss_scale=16, train_wall=322, gb_free=7.2, wall=85043
2022-03-05 08:55:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:58:51 | INFO | train_inner | epoch 106:    164 / 196 loss=2.176, ppl=4.52, wps=18854, ups=0.29, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.107, loss_scale=16, train_wall=322, gb_free=7.2, wall=85390
2022-03-05 09:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:00:47 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 11.182 | ppl 2322.64 | wps 34497.6 | wpb 510.9 | bsz 1 | num_updates 20632 | best_loss 6.971
2022-03-05 09:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20632 updates
2022-03-05 09:00:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:00:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:00:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 106 @ 20632 updates, score 11.182) (writing took 4.8703003246337175 seconds)
2022-03-05 09:00:52 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-05 09:00:52 | INFO | train | epoch 106 | loss 2.159 | ppl 4.47 | wps 18594 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 20632 | lr 0.000220155 | gnorm 1.1 | loss_scale 16 | train_wall 624 | gb_free 7.2 | wall 85512
2022-03-05 09:00:52 | INFO | fairseq.trainer | begin training epoch 107
2022-03-05 09:00:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:02:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:04:51 | INFO | train_inner | epoch 107:     69 / 196 loss=2.138, ppl=4.4, wps=18154.3, ups=0.28, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.108, loss_scale=16, train_wall=322, gb_free=7.2, wall=85751
2022-03-05 09:10:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:10:40 | INFO | train_inner | epoch 107:    170 / 196 loss=2.168, ppl=4.49, wps=18766.8, ups=0.29, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.107, loss_scale=16, train_wall=323, gb_free=7.2, wall=86100
2022-03-05 09:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:12:15 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 11.244 | ppl 2424.7 | wps 34709.8 | wpb 510.9 | bsz 1 | num_updates 20826 | best_loss 6.971
2022-03-05 09:12:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20826 updates
2022-03-05 09:12:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:12:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:12:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 107 @ 20826 updates, score 11.244) (writing took 4.682677512988448 seconds)
2022-03-05 09:12:20 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-05 09:12:20 | INFO | train | epoch 107 | loss 2.147 | ppl 4.43 | wps 18457.1 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 20826 | lr 0.000219128 | gnorm 1.106 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 86200
2022-03-05 09:12:20 | INFO | fairseq.trainer | begin training epoch 108
2022-03-05 09:12:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:16:37 | INFO | train_inner | epoch 108:     74 / 196 loss=2.115, ppl=4.33, wps=18313, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.094, loss_scale=16, train_wall=320, gb_free=7.2, wall=86457
2022-03-05 09:18:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:22:27 | INFO | train_inner | epoch 108:    175 / 196 loss=2.16, ppl=4.47, wps=18698.9, ups=0.29, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.102, loss_scale=16, train_wall=324, gb_free=7.2, wall=86807
2022-03-05 09:23:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:23:45 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 11.259 | ppl 2450.88 | wps 35454.1 | wpb 510.9 | bsz 1 | num_updates 21021 | best_loss 6.971
2022-03-05 09:23:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21021 updates
2022-03-05 09:23:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:23:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 108 @ 21021 updates, score 11.259) (writing took 5.0045466013252735 seconds)
2022-03-05 09:23:50 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-05 09:23:50 | INFO | train | epoch 108 | loss 2.138 | ppl 4.4 | wps 18498.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21021 | lr 0.000218109 | gnorm 1.098 | loss_scale 16 | train_wall 628 | gb_free 7.2 | wall 86889
2022-03-05 09:23:50 | INFO | fairseq.trainer | begin training epoch 109
2022-03-05 09:23:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:25:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:28:24 | INFO | train_inner | epoch 109:     80 / 196 loss=2.11, ppl=4.32, wps=18316.1, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.09, loss_scale=16, train_wall=320, gb_free=7.2, wall=87164
2022-03-05 09:33:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:34:13 | INFO | train_inner | epoch 109:    181 / 196 loss=2.146, ppl=4.43, wps=18811.1, ups=0.29, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.097, loss_scale=16, train_wall=322, gb_free=7.2, wall=87512
2022-03-05 09:35:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:35:10 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 11.283 | ppl 2491.52 | wps 34899.4 | wpb 510.9 | bsz 1 | num_updates 21215 | best_loss 6.971
2022-03-05 09:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21215 updates
2022-03-05 09:35:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:35:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:35:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 109 @ 21215 updates, score 11.283) (writing took 4.817978406324983 seconds)
2022-03-05 09:35:15 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-05 09:35:15 | INFO | train | epoch 109 | loss 2.126 | ppl 4.37 | wps 18534.5 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 21215 | lr 0.000217109 | gnorm 1.096 | loss_scale 16 | train_wall 623 | gb_free 7.2 | wall 87574
2022-03-05 09:35:15 | INFO | fairseq.trainer | begin training epoch 110
2022-03-05 09:35:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:40:09 | INFO | train_inner | epoch 110:     85 / 196 loss=2.096, ppl=4.27, wps=18358.2, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.102, loss_scale=16, train_wall=319, gb_free=7.2, wall=87868
2022-03-05 09:40:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:45:58 | INFO | train_inner | epoch 110:    186 / 196 loss=2.145, ppl=4.42, wps=18780.8, ups=0.29, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.118, loss_scale=16, train_wall=323, gb_free=7.2, wall=88217
2022-03-05 09:46:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:46:38 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 11.251 | ppl 2436.56 | wps 34463.1 | wpb 510.9 | bsz 1 | num_updates 21410 | best_loss 6.971
2022-03-05 09:46:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21410 updates
2022-03-05 09:46:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:46:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 110 @ 21410 updates, score 11.251) (writing took 4.908446999266744 seconds)
2022-03-05 09:46:43 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-05 09:46:43 | INFO | train | epoch 110 | loss 2.116 | ppl 4.34 | wps 18549.7 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21410 | lr 0.000216118 | gnorm 1.106 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 88262
2022-03-05 09:46:43 | INFO | fairseq.trainer | begin training epoch 111
2022-03-05 09:46:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:48:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:51:59 | INFO | train_inner | epoch 111:     91 / 196 loss=2.071, ppl=4.2, wps=18089.6, ups=0.28, wpb=65367, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.095, loss_scale=16, train_wall=324, gb_free=7.2, wall=88579
2022-03-05 09:56:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:57:50 | INFO | train_inner | epoch 111:    192 / 196 loss=2.143, ppl=4.42, wps=18693.7, ups=0.29, wpb=65532.4, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.087, loss_scale=16, train_wall=324, gb_free=7.2, wall=88929
2022-03-05 09:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:58:09 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 11.314 | ppl 2545.89 | wps 34073.5 | wpb 510.9 | bsz 1 | num_updates 21604 | best_loss 6.971
2022-03-05 09:58:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21604 updates
2022-03-05 09:58:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:58:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 09:58:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 111 @ 21604 updates, score 11.314) (writing took 4.8078545816242695 seconds)
2022-03-05 09:58:14 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-05 09:58:14 | INFO | train | epoch 111 | loss 2.105 | ppl 4.3 | wps 18370.4 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 21604 | lr 0.000215146 | gnorm 1.091 | loss_scale 16 | train_wall 629 | gb_free 7.2 | wall 88954
2022-03-05 09:58:14 | INFO | fairseq.trainer | begin training epoch 112
2022-03-05 09:58:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:03:45 | INFO | train_inner | epoch 112:     96 / 196 loss=2.06, ppl=4.17, wps=18373.8, ups=0.28, wpb=65367, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.082, loss_scale=32, train_wall=319, gb_free=7.2, wall=89285
2022-03-05 10:03:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:09:36 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 11.353 | ppl 2616.17 | wps 34697.3 | wpb 510.9 | bsz 1 | num_updates 21799 | best_loss 6.971
2022-03-05 10:09:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21799 updates
2022-03-05 10:09:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:09:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 112 @ 21799 updates, score 11.353) (writing took 5.139750089496374 seconds)
2022-03-05 10:09:41 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-05 10:09:41 | INFO | train | epoch 112 | loss 2.097 | ppl 4.28 | wps 18564.4 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 21799 | lr 0.000214181 | gnorm 1.097 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 89641
2022-03-05 10:09:41 | INFO | fairseq.trainer | begin training epoch 113
2022-03-05 10:09:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:09:45 | INFO | train_inner | epoch 113:      1 / 196 loss=2.137, ppl=4.4, wps=18177.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=21800, lr=0.000214176, gnorm=1.112, loss_scale=16, train_wall=322, gb_free=7.2, wall=89645
2022-03-05 10:11:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:15:34 | INFO | train_inner | epoch 113:    102 / 196 loss=2.047, ppl=4.13, wps=18778.7, ups=0.29, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=1.11, loss_scale=16, train_wall=323, gb_free=7.2, wall=89994
2022-03-05 10:19:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:20:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:21:06 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 11.361 | ppl 2630.59 | wps 34056.5 | wpb 510.9 | bsz 1 | num_updates 21993 | best_loss 6.971
2022-03-05 10:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 21993 updates
2022-03-05 10:21:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:21:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:21:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 113 @ 21993 updates, score 11.361) (writing took 4.807725952938199 seconds)
2022-03-05 10:21:10 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-05 10:21:10 | INFO | train | epoch 113 | loss 2.087 | ppl 4.25 | wps 18427.4 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 21993 | lr 0.000213235 | gnorm 1.104 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 90330
2022-03-05 10:21:10 | INFO | fairseq.trainer | begin training epoch 114
2022-03-05 10:21:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:21:35 | INFO | train_inner | epoch 114:      7 / 196 loss=2.122, ppl=4.35, wps=18110.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22000, lr=0.000213201, gnorm=1.095, loss_scale=16, train_wall=323, gb_free=7.2, wall=90355
2022-03-05 10:26:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:27:23 | INFO | train_inner | epoch 114:    108 / 196 loss=2.049, ppl=4.14, wps=18838.7, ups=0.29, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=1.109, loss_scale=16, train_wall=322, gb_free=7.2, wall=90702
2022-03-05 10:32:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:32:31 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 11.42 | ppl 2740.36 | wps 35108.8 | wpb 510.9 | bsz 1 | num_updates 22188 | best_loss 6.971
2022-03-05 10:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22188 updates
2022-03-05 10:32:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:32:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:32:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 114 @ 22188 updates, score 11.42) (writing took 4.618230003863573 seconds)
2022-03-05 10:32:36 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-05 10:32:36 | INFO | train | epoch 114 | loss 2.078 | ppl 4.22 | wps 18620.8 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 22188 | lr 0.000212296 | gnorm 1.117 | loss_scale 16 | train_wall 624 | gb_free 7.2 | wall 91016
2022-03-05 10:32:36 | INFO | fairseq.trainer | begin training epoch 115
2022-03-05 10:32:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:33:17 | INFO | train_inner | epoch 115:     12 / 196 loss=2.103, ppl=4.3, wps=18432.6, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.122, loss_scale=16, train_wall=318, gb_free=7.2, wall=91057
2022-03-05 10:34:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:39:06 | INFO | train_inner | epoch 115:    113 / 196 loss=2.035, ppl=4.1, wps=18787.7, ups=0.29, wpb=65532.4, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.097, loss_scale=16, train_wall=323, gb_free=7.2, wall=91406
2022-03-05 10:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:43:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:43:58 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 11.435 | ppl 2769.27 | wps 34362.8 | wpb 510.9 | bsz 1 | num_updates 22382 | best_loss 6.971
2022-03-05 10:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22382 updates
2022-03-05 10:43:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:44:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:44:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 115 @ 22382 updates, score 11.435) (writing took 4.9971744697541 seconds)
2022-03-05 10:44:03 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-05 10:44:03 | INFO | train | epoch 115 | loss 2.067 | ppl 4.19 | wps 18483.7 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 22382 | lr 0.000211374 | gnorm 1.104 | loss_scale 16 | train_wall 625 | gb_free 7.2 | wall 91702
2022-03-05 10:44:03 | INFO | fairseq.trainer | begin training epoch 116
2022-03-05 10:44:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:45:05 | INFO | train_inner | epoch 116:     18 / 196 loss=2.094, ppl=4.27, wps=18211.5, ups=0.28, wpb=65367, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.115, loss_scale=16, train_wall=321, gb_free=7.2, wall=91765
2022-03-05 10:49:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:50:55 | INFO | train_inner | epoch 116:    119 / 196 loss=2.036, ppl=4.1, wps=18740.3, ups=0.29, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.104, loss_scale=16, train_wall=323, gb_free=7.2, wall=92115
2022-03-05 10:55:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:55:26 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 11.456 | ppl 2808.9 | wps 35187.7 | wpb 510.9 | bsz 1 | num_updates 22577 | best_loss 6.971
2022-03-05 10:55:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22577 updates
2022-03-05 10:55:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:55:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 10:55:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 116 @ 22577 updates, score 11.456) (writing took 5.0296303648501635 seconds)
2022-03-05 10:55:31 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-05 10:55:31 | INFO | train | epoch 116 | loss 2.059 | ppl 4.17 | wps 18528.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 22577 | lr 0.000210459 | gnorm 1.108 | loss_scale 16 | train_wall 627 | gb_free 7.2 | wall 92391
2022-03-05 10:55:31 | INFO | fairseq.trainer | begin training epoch 117
2022-03-05 10:55:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:56:51 | INFO | train_inner | epoch 117:     23 / 196 loss=2.07, ppl=4.2, wps=18362.8, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.117, loss_scale=16, train_wall=319, gb_free=7.2, wall=92470
2022-03-05 10:57:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:02:37 | INFO | train_inner | epoch 117:    124 / 196 loss=2.037, ppl=4.1, wps=18907.3, ups=0.29, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.123, loss_scale=16, train_wall=321, gb_free=7.2, wall=92817
2022-03-05 11:04:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:06:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:06:51 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 11.441 | ppl 2780.84 | wps 34452.4 | wpb 510.9 | bsz 1 | num_updates 22771 | best_loss 6.971
2022-03-05 11:06:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22771 updates
2022-03-05 11:06:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 11:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 11:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 117 @ 22771 updates, score 11.441) (writing took 4.667690915986896 seconds)
2022-03-05 11:06:56 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-05 11:06:56 | INFO | train | epoch 117 | loss 2.047 | ppl 4.13 | wps 18543.3 | ups 0.28 | wpb 65447.1 | bsz 127.8 | num_updates 22771 | lr 0.00020956 | gnorm 1.124 | loss_scale 16 | train_wall 623 | gb_free 7.2 | wall 93076
2022-03-05 11:06:56 | INFO | fairseq.trainer | begin training epoch 118
2022-03-05 11:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:08:37 | INFO | train_inner | epoch 118:     29 / 196 loss=2.056, ppl=4.16, wps=18191.9, ups=0.28, wpb=65363.4, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.113, loss_scale=16, train_wall=322, gb_free=7.2, wall=93176
2022-03-05 11:12:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:14:26 | INFO | train_inner | epoch 118:    130 / 196 loss=2.029, ppl=4.08, wps=18761.5, ups=0.29, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.103, loss_scale=16, train_wall=323, gb_free=7.2, wall=93526
2022-03-05 11:18:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:18:19 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 11.455 | ppl 2807.34 | wps 34799.6 | wpb 510.9 | bsz 1 | num_updates 22966 | best_loss 6.971
2022-03-05 11:18:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22966 updates
2022-03-05 11:18:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 11:18:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt
2022-03-05 11:18:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#4/checkpoint_last.pt (epoch 118 @ 22966 updates, score 11.455) (writing took 5.154583148658276 seconds)
2022-03-05 11:18:25 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-05 11:18:25 | INFO | train | epoch 118 | loss 2.04 | ppl 4.11 | wps 18535.6 | ups 0.28 | wpb 65447.5 | bsz 127.8 | num_updates 22966 | lr 0.000208669 | gnorm 1.106 | loss_scale 16 | train_wall 626 | gb_free 7.2 | wall 93764
2022-03-05 11:18:25 | INFO | fairseq.trainer | begin training epoch 119
2022-03-05 11:18:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:19:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:20:26 | INFO | train_inner | epoch 119:     35 / 196 loss=2.044, ppl=4.12, wps=18147.9, ups=0.28, wpb=65367, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.11, loss_scale=16, train_wall=322, gb_free=7.2, wall=93886
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/cross_entropy.py", line 35, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4215, in multi_head_attention_forward
    q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
